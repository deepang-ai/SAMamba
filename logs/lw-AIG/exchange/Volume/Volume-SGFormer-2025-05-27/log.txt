Namespace(seed=15, model='SGFormer', dataset='exchange/Volume', num_heads=8, num_layers=2, dim_hidden=64, dropout=0.8, epochs=1000, lr=0.001, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/exchange/Volume/seed15/khopgnn_gat_1_0.8_0.001_0.0001_2_8_64_BN', warmup=10, layer_norm=False, use_edge_attr=False, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, profile=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=5, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
Data(edge_index=[2, 114], edge_attr=[114, 2], x=[21, 14887], y=[1, 1], num_nodes=21)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b284070feb0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7394;  Loss pred: 0.7394; Loss self: 0.0000; time: 0.55s
Val loss: 0.6937 score: 0.4884 time: 0.23s
Test loss: 0.6927 score: 0.5736 time: 0.21s
Epoch 2/1000, LR 0.000050
Train loss: 0.7244;  Loss pred: 0.7244; Loss self: 0.0000; time: 0.29s
Val loss: 0.6854 score: 0.6124 time: 0.32s
Test loss: 0.6850 score: 0.5969 time: 0.19s
Epoch 3/1000, LR 0.000150
Train loss: 0.7077;  Loss pred: 0.7077; Loss self: 0.0000; time: 0.29s
Val loss: 0.6747 score: 0.6434 time: 0.19s
Test loss: 0.6755 score: 0.5659 time: 0.19s
Epoch 4/1000, LR 0.000250
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.31s
Val loss: 0.6629 score: 0.6279 time: 0.20s
Test loss: 0.6641 score: 0.5426 time: 0.20s
Epoch 5/1000, LR 0.000350
Train loss: 0.6070;  Loss pred: 0.6070; Loss self: 0.0000; time: 0.31s
Val loss: 0.6454 score: 0.6512 time: 0.26s
Test loss: 0.6455 score: 0.6202 time: 0.22s
Epoch 6/1000, LR 0.000450
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.31s
Val loss: 0.6236 score: 0.6667 time: 0.20s
Test loss: 0.6219 score: 0.6822 time: 0.19s
Epoch 7/1000, LR 0.000550
Train loss: 0.5232;  Loss pred: 0.5232; Loss self: 0.0000; time: 0.31s
Val loss: 0.5910 score: 0.7519 time: 0.21s
Test loss: 0.5840 score: 0.7442 time: 0.19s
Epoch 8/1000, LR 0.000650
Train loss: 0.4762;  Loss pred: 0.4762; Loss self: 0.0000; time: 0.44s
Val loss: 0.5510 score: 0.7984 time: 0.20s
Test loss: 0.5369 score: 0.8682 time: 0.19s
Epoch 9/1000, LR 0.000750
Train loss: 0.4282;  Loss pred: 0.4282; Loss self: 0.0000; time: 0.30s
Val loss: 0.5154 score: 0.8682 time: 0.19s
Test loss: 0.4955 score: 0.9070 time: 0.19s
Epoch 10/1000, LR 0.000850
Train loss: 0.3998;  Loss pred: 0.3998; Loss self: 0.0000; time: 0.30s
Val loss: 0.4975 score: 0.9225 time: 0.19s
Test loss: 0.4716 score: 0.9380 time: 0.31s
Epoch 11/1000, LR 0.000950
Train loss: 0.3581;  Loss pred: 0.3581; Loss self: 0.0000; time: 0.29s
Val loss: 0.4670 score: 0.9070 time: 0.18s
Test loss: 0.4441 score: 0.9380 time: 0.18s
Epoch 12/1000, LR 0.000950
Train loss: 0.3266;  Loss pred: 0.3266; Loss self: 0.0000; time: 0.30s
Val loss: 0.4465 score: 0.8295 time: 0.19s
Test loss: 0.4282 score: 0.8992 time: 0.18s
Epoch 13/1000, LR 0.000950
Train loss: 0.3000;  Loss pred: 0.3000; Loss self: 0.0000; time: 0.30s
Val loss: 0.4526 score: 0.8140 time: 0.31s
Test loss: 0.4347 score: 0.8450 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 14/1000, LR 0.000950
Train loss: 0.2773;  Loss pred: 0.2773; Loss self: 0.0000; time: 0.29s
Val loss: 0.4478 score: 0.8140 time: 0.18s
Test loss: 0.4306 score: 0.8450 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 15/1000, LR 0.000950
Train loss: 0.2392;  Loss pred: 0.2392; Loss self: 0.0000; time: 0.29s
Val loss: 0.4405 score: 0.8062 time: 0.18s
Test loss: 0.4235 score: 0.8450 time: 0.17s
Epoch 16/1000, LR 0.000950
Train loss: 0.2233;  Loss pred: 0.2233; Loss self: 0.0000; time: 0.30s
Val loss: 0.4276 score: 0.8062 time: 0.29s
Test loss: 0.4112 score: 0.8527 time: 0.19s
Epoch 17/1000, LR 0.000950
Train loss: 0.2027;  Loss pred: 0.2027; Loss self: 0.0000; time: 0.29s
Val loss: 0.4199 score: 0.8062 time: 0.19s
Test loss: 0.4032 score: 0.8605 time: 0.18s
Epoch 18/1000, LR 0.000950
Train loss: 0.1796;  Loss pred: 0.1796; Loss self: 0.0000; time: 0.29s
Val loss: 0.4116 score: 0.8140 time: 0.19s
Test loss: 0.3939 score: 0.8605 time: 0.18s
Epoch 19/1000, LR 0.000950
Train loss: 0.1660;  Loss pred: 0.1660; Loss self: 0.0000; time: 0.30s
Val loss: 0.4078 score: 0.8062 time: 0.21s
Test loss: 0.3883 score: 0.8605 time: 0.18s
Epoch 20/1000, LR 0.000950
Train loss: 0.1598;  Loss pred: 0.1598; Loss self: 0.0000; time: 0.30s
Val loss: 0.4105 score: 0.8062 time: 0.20s
Test loss: 0.3887 score: 0.8605 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 21/1000, LR 0.000950
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.29s
Val loss: 0.4163 score: 0.7984 time: 0.19s
Test loss: 0.3909 score: 0.8527 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 22/1000, LR 0.000950
Train loss: 0.1366;  Loss pred: 0.1366; Loss self: 0.0000; time: 0.42s
Val loss: 0.4174 score: 0.7984 time: 0.20s
Test loss: 0.3885 score: 0.8527 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 23/1000, LR 0.000950
Train loss: 0.1266;  Loss pred: 0.1266; Loss self: 0.0000; time: 0.31s
Val loss: 0.4194 score: 0.7984 time: 0.20s
Test loss: 0.3873 score: 0.8527 time: 0.19s
     INFO: Early stopping counter 4 of 5
Epoch 24/1000, LR 0.000950
Train loss: 0.1096;  Loss pred: 0.1096; Loss self: 0.0000; time: 0.31s
Val loss: 0.4142 score: 0.8062 time: 0.20s
Test loss: 0.3798 score: 0.8527 time: 0.33s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 018,   Train_Loss: 0.1660,   Val_Loss: 0.4078,   Val_Precision: 0.9756,   Val_Recall: 0.6250,   Val_accuracy: 0.7619,   Val_Score: 0.8062,   Val_Loss: 0.4078,   Test_Precision: 1.0000,   Test_Recall: 0.7231,   Test_accuracy: 0.8393,   Test_Score: 0.8605,   Test_loss: 0.3883


[0.21024983702227473, 0.19815533491782844, 0.19813402905128896, 0.20128642488270998, 0.22301174700260162, 0.1945425160229206, 0.19340062607079744, 0.19910070206969976, 0.19268159288913012, 0.3130925048608333, 0.18097943207249045, 0.1826012779492885, 0.18595743784680963, 0.1855709960218519, 0.1795858289115131, 0.19084129994735122, 0.1833167909644544, 0.18642382696270943, 0.18496814789250493, 0.20108089409768581, 0.18173086713068187, 0.19723692908883095, 0.19390906509943306, 0.3332073569763452]
[0.001629843697847091, 0.0015360878675800655, 0.0015359227058239454, 0.0015603598828117053, 0.0017287732325783072, 0.001508081519557524, 0.001499229659463546, 0.0015434162951139517, 0.0014936557588304661, 0.002427073681091731, 0.001402941333895275, 0.001415513782552624, 0.0014415305259442607, 0.001438534852882573, 0.0013921382086163805, 0.00147938992207249, 0.00142106039507329, 0.0014451459454473598, 0.0014338616115698057, 0.0015587666209122932, 0.001408766411865751, 0.0015289684425490772, 0.0015031710472824269, 0.00258300276725849]
[613.555766924724, 651.0044256617872, 651.0744298578164, 640.8777942932245, 578.4448655007178, 663.0941278913114, 667.0092161582634, 647.9133356086338, 669.4983058097678, 412.01880593513187, 712.7881799757757, 706.4572682554035, 693.7071272527924, 695.1517358068694, 718.3194842370434, 675.9543140587922, 703.6998592508279, 691.9716331421735, 697.4173741252409, 641.5328546198493, 709.8408874439402, 654.0357355792199, 665.2602854531382, 387.1463138467194]
Elapsed: 0.20379439440633482~0.03741556533701153
Time per graph: 0.0015798015070258515~0.0002900431421473762
Speed: 647.8239219453818~81.7026886087531
Total Time: 0.3336
best val loss: 0.40779313156309055 test_score: 0.8605

Testing...
Test loss: 0.4716 score: 0.9380 time: 0.20s
test Score 0.9380
Epoch Time List: [0.9882466758135706, 0.8031412991695106, 0.6795619761105627, 0.7112165957223624, 0.7899676631204784, 0.6946939721237868, 0.7107648469973356, 0.8365233042277396, 0.6786243310198188, 0.7982704290188849, 0.6541324378922582, 0.6744074809830636, 0.7946605999022722, 0.6532258200459182, 0.6523472811095417, 0.7737695630639791, 0.6586999078281224, 0.658978828927502, 0.688142969738692, 0.6951317978091538, 0.6646835568826646, 0.8159557378385216, 0.6905993779655546, 0.8411096897907555]
Total Epoch List: [24]
Total Time List: [0.3336146699730307]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b284070ffd0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.4961 time: 0.20s
Epoch 2/1000, LR 0.000050
Train loss: 0.7118;  Loss pred: 0.7118; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4961 time: 0.19s
Epoch 3/1000, LR 0.000150
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6710 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6801 score: 0.4961 time: 0.18s
Epoch 4/1000, LR 0.000250
Train loss: 0.6352;  Loss pred: 0.6352; Loss self: 0.0000; time: 0.29s
Val loss: 0.6508 score: 0.5659 time: 0.19s
Test loss: 0.6638 score: 0.5349 time: 0.18s
Epoch 5/1000, LR 0.000350
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.29s
Val loss: 0.6257 score: 0.8605 time: 0.19s
Test loss: 0.6440 score: 0.8062 time: 0.18s
Epoch 6/1000, LR 0.000450
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.28s
Val loss: 0.5943 score: 0.9225 time: 0.19s
Test loss: 0.6205 score: 0.8837 time: 0.19s
Epoch 7/1000, LR 0.000550
Train loss: 0.5059;  Loss pred: 0.5059; Loss self: 0.0000; time: 0.29s
Val loss: 0.5574 score: 0.9225 time: 0.20s
Test loss: 0.5919 score: 0.8837 time: 0.20s
Epoch 8/1000, LR 0.000650
Train loss: 0.4705;  Loss pred: 0.4705; Loss self: 0.0000; time: 0.30s
Val loss: 0.5190 score: 0.9380 time: 0.20s
Test loss: 0.5596 score: 0.8837 time: 0.20s
Epoch 9/1000, LR 0.000750
Train loss: 0.4305;  Loss pred: 0.4305; Loss self: 0.0000; time: 0.31s
Val loss: 0.4914 score: 0.9457 time: 0.21s
Test loss: 0.5336 score: 0.9070 time: 0.21s
Epoch 10/1000, LR 0.000850
Train loss: 0.3886;  Loss pred: 0.3886; Loss self: 0.0000; time: 0.30s
Val loss: 0.4867 score: 0.8295 time: 0.20s
Test loss: 0.5300 score: 0.7984 time: 0.20s
Epoch 11/1000, LR 0.000950
Train loss: 0.3591;  Loss pred: 0.3591; Loss self: 0.0000; time: 0.30s
Val loss: 0.5015 score: 0.6899 time: 0.20s
Test loss: 0.5455 score: 0.6434 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 12/1000, LR 0.000950
Train loss: 0.3381;  Loss pred: 0.3381; Loss self: 0.0000; time: 0.30s
Val loss: 0.4893 score: 0.6977 time: 0.20s
Test loss: 0.5347 score: 0.6512 time: 0.21s
     INFO: Early stopping counter 2 of 5
Epoch 13/1000, LR 0.000950
Train loss: 0.3102;  Loss pred: 0.3102; Loss self: 0.0000; time: 0.29s
Val loss: 0.4629 score: 0.7752 time: 0.19s
Test loss: 0.5083 score: 0.7287 time: 0.19s
Epoch 14/1000, LR 0.000950
Train loss: 0.3007;  Loss pred: 0.3007; Loss self: 0.0000; time: 0.29s
Val loss: 0.4279 score: 0.8837 time: 0.19s
Test loss: 0.4735 score: 0.8450 time: 0.19s
Epoch 15/1000, LR 0.000950
Train loss: 0.2758;  Loss pred: 0.2758; Loss self: 0.0000; time: 0.29s
Val loss: 0.4102 score: 0.8992 time: 0.19s
Test loss: 0.4562 score: 0.8527 time: 0.19s
Epoch 16/1000, LR 0.000950
Train loss: 0.2640;  Loss pred: 0.2640; Loss self: 0.0000; time: 0.29s
Val loss: 0.4020 score: 0.8915 time: 0.19s
Test loss: 0.4471 score: 0.8527 time: 0.19s
Epoch 17/1000, LR 0.000950
Train loss: 0.2336;  Loss pred: 0.2336; Loss self: 0.0000; time: 0.29s
Val loss: 0.4065 score: 0.8527 time: 0.19s
Test loss: 0.4500 score: 0.8217 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 18/1000, LR 0.000950
Train loss: 0.2347;  Loss pred: 0.2347; Loss self: 0.0000; time: 0.30s
Val loss: 0.4008 score: 0.8372 time: 0.19s
Test loss: 0.4428 score: 0.8062 time: 0.19s
Epoch 19/1000, LR 0.000950
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.30s
Val loss: 0.3846 score: 0.8605 time: 0.19s
Test loss: 0.4261 score: 0.8527 time: 0.19s
Epoch 20/1000, LR 0.000950
Train loss: 0.1944;  Loss pred: 0.1944; Loss self: 0.0000; time: 0.31s
Val loss: 0.3769 score: 0.8760 time: 0.19s
Test loss: 0.4174 score: 0.8605 time: 0.19s
Epoch 21/1000, LR 0.000950
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 0.30s
Val loss: 0.3601 score: 0.8992 time: 0.19s
Test loss: 0.4003 score: 0.8760 time: 0.19s
Epoch 22/1000, LR 0.000950
Train loss: 0.1773;  Loss pred: 0.1773; Loss self: 0.0000; time: 0.30s
Val loss: 0.3554 score: 0.8992 time: 0.19s
Test loss: 0.3949 score: 0.8682 time: 0.19s
Epoch 23/1000, LR 0.000950
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 0.30s
Val loss: 0.3309 score: 0.9147 time: 0.19s
Test loss: 0.3711 score: 0.9070 time: 0.19s
Epoch 24/1000, LR 0.000950
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.31s
Val loss: 0.3177 score: 0.9302 time: 0.21s
Test loss: 0.3584 score: 0.8992 time: 0.20s
Epoch 25/1000, LR 0.000950
Train loss: 0.1376;  Loss pred: 0.1376; Loss self: 0.0000; time: 0.31s
Val loss: 0.3043 score: 0.9457 time: 0.21s
Test loss: 0.3455 score: 0.9070 time: 0.21s
Epoch 26/1000, LR 0.000949
Train loss: 0.1367;  Loss pred: 0.1367; Loss self: 0.0000; time: 0.31s
Val loss: 0.2979 score: 0.9457 time: 0.19s
Test loss: 0.3393 score: 0.8992 time: 0.19s
Epoch 27/1000, LR 0.000949
Train loss: 0.1355;  Loss pred: 0.1355; Loss self: 0.0000; time: 0.30s
Val loss: 0.2944 score: 0.9380 time: 0.19s
Test loss: 0.3362 score: 0.8992 time: 0.19s
Epoch 28/1000, LR 0.000949
Train loss: 0.1199;  Loss pred: 0.1199; Loss self: 0.0000; time: 0.31s
Val loss: 0.2865 score: 0.9457 time: 0.19s
Test loss: 0.3289 score: 0.8992 time: 0.19s
Epoch 29/1000, LR 0.000949
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.30s
Val loss: 0.2842 score: 0.9225 time: 0.19s
Test loss: 0.3270 score: 0.8992 time: 0.19s
Epoch 30/1000, LR 0.000949
Train loss: 0.1206;  Loss pred: 0.1206; Loss self: 0.0000; time: 0.30s
Val loss: 0.2848 score: 0.9147 time: 0.19s
Test loss: 0.3283 score: 0.8915 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 31/1000, LR 0.000949
Train loss: 0.1017;  Loss pred: 0.1017; Loss self: 0.0000; time: 0.30s
Val loss: 0.3130 score: 0.8837 time: 0.19s
Test loss: 0.3559 score: 0.8527 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 32/1000, LR 0.000949
Train loss: 0.0928;  Loss pred: 0.0928; Loss self: 0.0000; time: 0.30s
Val loss: 0.2824 score: 0.9070 time: 0.19s
Test loss: 0.3278 score: 0.8915 time: 0.19s
Epoch 33/1000, LR 0.000949
Train loss: 0.0918;  Loss pred: 0.0918; Loss self: 0.0000; time: 0.30s
Val loss: 0.2651 score: 0.9147 time: 0.19s
Test loss: 0.3123 score: 0.8992 time: 0.19s
Epoch 34/1000, LR 0.000949
Train loss: 0.0991;  Loss pred: 0.0991; Loss self: 0.0000; time: 0.30s
Val loss: 0.2418 score: 0.9535 time: 0.19s
Test loss: 0.2917 score: 0.9070 time: 0.19s
Epoch 35/1000, LR 0.000949
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 0.30s
Val loss: 0.2258 score: 0.9535 time: 0.19s
Test loss: 0.2789 score: 0.9147 time: 0.19s
Epoch 36/1000, LR 0.000949
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.30s
Val loss: 0.2176 score: 0.9457 time: 0.19s
Test loss: 0.2728 score: 0.9147 time: 0.25s
Epoch 37/1000, LR 0.000948
Train loss: 0.0764;  Loss pred: 0.0764; Loss self: 0.0000; time: 0.30s
Val loss: 0.2250 score: 0.9535 time: 0.19s
Test loss: 0.2798 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 38/1000, LR 0.000948
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.30s
Val loss: 0.2319 score: 0.9380 time: 0.19s
Test loss: 0.2883 score: 0.9070 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 39/1000, LR 0.000948
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.30s
Val loss: 0.2132 score: 0.9535 time: 0.23s
Test loss: 0.2746 score: 0.9070 time: 0.27s
Epoch 40/1000, LR 0.000948
Train loss: 0.0619;  Loss pred: 0.0619; Loss self: 0.0000; time: 0.30s
Val loss: 0.2115 score: 0.9535 time: 0.19s
Test loss: 0.2755 score: 0.9070 time: 0.19s
Epoch 41/1000, LR 0.000948
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.30s
Val loss: 0.2078 score: 0.9535 time: 0.19s
Test loss: 0.2744 score: 0.9070 time: 0.19s
Epoch 42/1000, LR 0.000948
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.41s
Val loss: 0.1996 score: 0.9535 time: 0.20s
Test loss: 0.2688 score: 0.9070 time: 0.20s
Epoch 43/1000, LR 0.000948
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.31s
Val loss: 0.1912 score: 0.9535 time: 0.20s
Test loss: 0.2636 score: 0.9070 time: 0.20s
Epoch 44/1000, LR 0.000947
Train loss: 0.0506;  Loss pred: 0.0506; Loss self: 0.0000; time: 0.31s
Val loss: 0.1837 score: 0.9535 time: 0.20s
Test loss: 0.2598 score: 0.9070 time: 0.33s
Epoch 45/1000, LR 0.000947
Train loss: 0.0446;  Loss pred: 0.0446; Loss self: 0.0000; time: 0.31s
Val loss: 0.1853 score: 0.9535 time: 0.20s
Test loss: 0.2624 score: 0.9147 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 46/1000, LR 0.000947
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.30s
Val loss: 0.1801 score: 0.9535 time: 0.19s
Test loss: 0.2599 score: 0.9147 time: 0.19s
Epoch 47/1000, LR 0.000947
Train loss: 0.0384;  Loss pred: 0.0384; Loss self: 0.0000; time: 0.30s
Val loss: 0.1799 score: 0.9535 time: 0.21s
Test loss: 0.2605 score: 0.9147 time: 0.19s
Epoch 48/1000, LR 0.000947
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.30s
Val loss: 0.1675 score: 0.9535 time: 0.19s
Test loss: 0.2534 score: 0.9147 time: 0.19s
Epoch 49/1000, LR 0.000947
Train loss: 0.0386;  Loss pred: 0.0386; Loss self: 0.0000; time: 0.31s
Val loss: 0.1692 score: 0.9535 time: 0.19s
Test loss: 0.2545 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 50/1000, LR 0.000946
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.40s
Val loss: 0.1744 score: 0.9535 time: 0.19s
Test loss: 0.2600 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 51/1000, LR 0.000946
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.30s
Val loss: 0.1811 score: 0.9457 time: 0.19s
Test loss: 0.2681 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 52/1000, LR 0.000946
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.30s
Val loss: 0.1733 score: 0.9535 time: 0.34s
Test loss: 0.2665 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 4 of 5
Epoch 53/1000, LR 0.000946
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.31s
Val loss: 0.1727 score: 0.9535 time: 0.21s
Test loss: 0.2739 score: 0.9070 time: 0.21s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 047,   Train_Loss: 0.0392,   Val_Loss: 0.1675,   Val_Precision: 0.9538,   Val_Recall: 0.9538,   Val_accuracy: 0.9538,   Val_Score: 0.9535,   Val_Loss: 0.1675,   Test_Precision: 0.9649,   Test_Recall: 0.8594,   Test_accuracy: 0.9091,   Test_Score: 0.9147,   Test_loss: 0.2534


[0.21024983702227473, 0.19815533491782844, 0.19813402905128896, 0.20128642488270998, 0.22301174700260162, 0.1945425160229206, 0.19340062607079744, 0.19910070206969976, 0.19268159288913012, 0.3130925048608333, 0.18097943207249045, 0.1826012779492885, 0.18595743784680963, 0.1855709960218519, 0.1795858289115131, 0.19084129994735122, 0.1833167909644544, 0.18642382696270943, 0.18496814789250493, 0.20108089409768581, 0.18173086713068187, 0.19723692908883095, 0.19390906509943306, 0.3332073569763452, 0.20014727115631104, 0.19550286512821913, 0.18942551710642874, 0.18908555200323462, 0.1888714579399675, 0.19424680597148836, 0.20125553105026484, 0.20076710009016097, 0.21256856387481093, 0.20221495698206127, 0.20200515491887927, 0.21418210095725954, 0.19085420086048543, 0.19094728399068117, 0.19111872604116797, 0.19334752508439124, 0.19644248997792602, 0.19815580802969635, 0.197247255127877, 0.19722814206033945, 0.19586688396520913, 0.19769135606475174, 0.197325699031353, 0.20864535681903362, 0.21073197200894356, 0.1970112919807434, 0.19654888496734202, 0.19544837181456387, 0.19585368409752846, 0.19428114499896765, 0.19454756285995245, 0.19679010403342545, 0.19522369396872818, 0.19546519313007593, 0.198962981114164, 0.25923678698018193, 0.19684510794468224, 0.1966920590493828, 0.27545726811513305, 0.19696526997722685, 0.1956014318857342, 0.20633793296292424, 0.20893248496577144, 0.33877882990054786, 0.20061185397207737, 0.19459690409712493, 0.1953969879541546, 0.19637930998578668, 0.2011004500091076, 0.19200675189495087, 0.19803917198441923, 0.21033617202192545, 0.21058546006679535]
[0.001629843697847091, 0.0015360878675800655, 0.0015359227058239454, 0.0015603598828117053, 0.0017287732325783072, 0.001508081519557524, 0.001499229659463546, 0.0015434162951139517, 0.0014936557588304661, 0.002427073681091731, 0.001402941333895275, 0.001415513782552624, 0.0014415305259442607, 0.001438534852882573, 0.0013921382086163805, 0.00147938992207249, 0.00142106039507329, 0.0014451459454473598, 0.0014338616115698057, 0.0015587666209122932, 0.001408766411865751, 0.0015289684425490772, 0.0015031710472824269, 0.00258300276725849, 0.001551529233769853, 0.0015155260862652646, 0.0014684148612901452, 0.0014657794728932918, 0.0014641198289919962, 0.001505789193577429, 0.001560120395738487, 0.001556334109226054, 0.0016478183246109374, 0.0015675578060624904, 0.0015659314334796842, 0.001660326364009764, 0.0014794899291510498, 0.00148021150380373, 0.0014815405119470385, 0.0014988180239100097, 0.001522809999828884, 0.0015360915351139252, 0.0015290484893633876, 0.0015289003260491431, 0.0015183479377147994, 0.0015324911322848972, 0.0015296565816383953, 0.001617405866814214, 0.0016335811783639036, 0.0015272193176801814, 0.0015236347671886978, 0.0015151036574772394, 0.0015182456131591353, 0.0015060553875888966, 0.0015081206423252127, 0.0015255046824296547, 0.001513361968749831, 0.0015152340552719065, 0.0015423486908074729, 0.0020095874959704025, 0.001525931069338622, 0.0015247446437936653, 0.002135327659807233, 0.0015268625579629989, 0.0015162901696568543, 0.0015995188601777074, 0.001619631666401329, 0.0026261924798492084, 0.0015551306509463362, 0.001508503132535852, 0.0015147053329779426, 0.0015223202324479586, 0.001558918217124865, 0.0014884244332941928, 0.00153518737972418, 0.0016305129614102747, 0.0016324454268743826]
[613.555766924724, 651.0044256617872, 651.0744298578164, 640.8777942932245, 578.4448655007178, 663.0941278913114, 667.0092161582634, 647.9133356086338, 669.4983058097678, 412.01880593513187, 712.7881799757757, 706.4572682554035, 693.7071272527924, 695.1517358068694, 718.3194842370434, 675.9543140587922, 703.6998592508279, 691.9716331421735, 697.4173741252409, 641.5328546198493, 709.8408874439402, 654.0357355792199, 665.2602854531382, 387.1463138467194, 644.5253999953543, 659.8368771495819, 681.0064555744163, 682.2308665751111, 683.0042051192428, 664.1035838650273, 640.9761725643278, 642.5355545907092, 606.8630170356357, 637.9350070106028, 638.5975647591939, 602.291225193193, 675.9086224897878, 675.5791300299176, 674.9731053157644, 667.1924036456883, 656.6807415976837, 651.0028713398478, 654.0014963268728, 654.0648745782643, 658.6105695279945, 652.5323239613339, 653.741507736928, 618.2740031540064, 612.1520088775385, 654.7848029574311, 656.3252700285441, 660.0208474614038, 658.6549576252157, 663.9862041202479, 663.0769262982867, 655.5207673353784, 660.780448200431, 659.9640474821241, 648.3618172466989, 497.61456119984143, 655.3375968898948, 655.8475244168978, 468.31220276998386, 654.9377969776855, 659.5043745659224, 625.1880017775468, 617.4243321766529, 380.77940123315653, 643.0327891688552, 662.908799081485, 660.1944142059491, 656.8920117365552, 641.4704690822807, 671.8513735942859, 651.3862823570536, 613.3039256155762, 612.5779052318362]
Elapsed: 0.20364901543800512~0.028693571164557286
Time per graph: 0.0015786745382791096~0.0002224307842213743
Speed: 642.1614748771482~62.78150236086175
Total Time: 0.2111
best val loss: 0.16754500422078047 test_score: 0.9147

Testing...
Test loss: 0.2917 score: 0.9070 time: 0.21s
test Score 0.9070
Epoch Time List: [0.9882466758135706, 0.8031412991695106, 0.6795619761105627, 0.7112165957223624, 0.7899676631204784, 0.6946939721237868, 0.7107648469973356, 0.8365233042277396, 0.6786243310198188, 0.7982704290188849, 0.6541324378922582, 0.6744074809830636, 0.7946605999022722, 0.6532258200459182, 0.6523472811095417, 0.7737695630639791, 0.6586999078281224, 0.658978828927502, 0.688142969738692, 0.6951317978091538, 0.6646835568826646, 0.8159557378385216, 0.6905993779655546, 0.8411096897907555, 0.6759413729887456, 0.6774655792396516, 0.6547448092605919, 0.6549251212272793, 0.6548475737217814, 0.6626183420885354, 0.685835205251351, 0.6922257742844522, 0.7293981108814478, 0.6956636889372021, 0.6970109271351248, 0.7066722768358886, 0.668542314087972, 0.6639162271749228, 0.6658837841823697, 0.6724781231023371, 0.6766245171893388, 0.6886006118729711, 0.6884862531442195, 0.6891736940015107, 0.6857580828946084, 0.689149378798902, 0.6899841299746186, 0.7208043590653688, 0.7261032732203603, 0.6908101127482951, 0.6880247187800705, 0.6871850348543376, 0.6824190681800246, 0.6820202679373324, 0.6812485130503774, 0.6836999040096998, 0.6824974562041461, 0.6830641052220017, 0.6879306689370424, 0.7505717389285564, 0.6831556791439652, 0.6898122008424252, 0.8002853316720575, 0.6805249841418117, 0.6847520826850086, 0.810714224120602, 0.7171836651396006, 0.8487574972677976, 0.7057375821750611, 0.6825238049495965, 0.7028342890553176, 0.6835646349936724, 0.6910223828162998, 0.7795006581582129, 0.6822786668781191, 0.8433369237463921, 0.7273363168351352]
Total Epoch List: [24, 53]
Total Time List: [0.3336146699730307, 0.21107473107986152]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b28481127a0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.32s
Val loss: 0.6896 score: 0.5116 time: 0.18s
Test loss: 0.6895 score: 0.5312 time: 0.17s
Epoch 2/1000, LR 0.000067
Train loss: 0.7071;  Loss pred: 0.7071; Loss self: 0.0000; time: 0.31s
Val loss: 0.6814 score: 0.5039 time: 0.32s
Test loss: 0.6812 score: 0.5078 time: 0.17s
Epoch 3/1000, LR 0.000167
Train loss: 0.6635;  Loss pred: 0.6635; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6717 score: 0.4961 time: 0.18s
Test loss: 0.6706 score: 0.5078 time: 0.17s
Epoch 4/1000, LR 0.000267
Train loss: 0.6217;  Loss pred: 0.6217; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6536 score: 0.4961 time: 0.18s
Test loss: 0.6522 score: 0.5312 time: 0.17s
Epoch 5/1000, LR 0.000367
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.41s
Val loss: 0.6318 score: 0.5349 time: 0.19s
Test loss: 0.6303 score: 0.5938 time: 0.17s
Epoch 6/1000, LR 0.000467
Train loss: 0.5398;  Loss pred: 0.5398; Loss self: 0.0000; time: 0.30s
Val loss: 0.5974 score: 0.6434 time: 0.18s
Test loss: 0.5964 score: 0.6953 time: 0.17s
Epoch 7/1000, LR 0.000567
Train loss: 0.4964;  Loss pred: 0.4964; Loss self: 0.0000; time: 0.32s
Val loss: 0.5616 score: 0.7674 time: 0.18s
Test loss: 0.5621 score: 0.7578 time: 0.17s
Epoch 8/1000, LR 0.000667
Train loss: 0.4595;  Loss pred: 0.4595; Loss self: 0.0000; time: 0.33s
Val loss: 0.5299 score: 0.8450 time: 0.18s
Test loss: 0.5328 score: 0.8359 time: 0.19s
Epoch 9/1000, LR 0.000767
Train loss: 0.4189;  Loss pred: 0.4189; Loss self: 0.0000; time: 0.37s
Val loss: 0.5025 score: 0.8605 time: 0.21s
Test loss: 0.5071 score: 0.8750 time: 0.21s
Epoch 10/1000, LR 0.000867
Train loss: 0.3975;  Loss pred: 0.3975; Loss self: 0.0000; time: 0.34s
Val loss: 0.4796 score: 0.9070 time: 0.33s
Test loss: 0.4866 score: 0.8906 time: 0.16s
Epoch 11/1000, LR 0.000967
Train loss: 0.3650;  Loss pred: 0.3650; Loss self: 0.0000; time: 0.32s
Val loss: 0.4661 score: 0.9147 time: 0.18s
Test loss: 0.4759 score: 0.9141 time: 0.16s
Epoch 12/1000, LR 0.000967
Train loss: 0.3270;  Loss pred: 0.3270; Loss self: 0.0000; time: 0.31s
Val loss: 0.4559 score: 0.9147 time: 0.18s
Test loss: 0.4645 score: 0.8984 time: 0.17s
Epoch 13/1000, LR 0.000967
Train loss: 0.3148;  Loss pred: 0.3148; Loss self: 0.0000; time: 0.32s
Val loss: 0.4453 score: 0.9147 time: 0.28s
Test loss: 0.4551 score: 0.8906 time: 0.16s
Epoch 14/1000, LR 0.000967
Train loss: 0.3139;  Loss pred: 0.3139; Loss self: 0.0000; time: 0.31s
Val loss: 0.4363 score: 0.9147 time: 0.17s
Test loss: 0.4465 score: 0.8906 time: 0.16s
Epoch 15/1000, LR 0.000967
Train loss: 0.2873;  Loss pred: 0.2873; Loss self: 0.0000; time: 0.32s
Val loss: 0.4300 score: 0.8992 time: 0.18s
Test loss: 0.4407 score: 0.8828 time: 0.16s
Epoch 16/1000, LR 0.000967
Train loss: 0.2811;  Loss pred: 0.2811; Loss self: 0.0000; time: 0.32s
Val loss: 0.4253 score: 0.8682 time: 0.25s
Test loss: 0.4368 score: 0.8750 time: 0.16s
Epoch 17/1000, LR 0.000967
Train loss: 0.2640;  Loss pred: 0.2640; Loss self: 0.0000; time: 0.31s
Val loss: 0.4146 score: 0.8605 time: 0.18s
Test loss: 0.4268 score: 0.8672 time: 0.17s
Epoch 18/1000, LR 0.000967
Train loss: 0.2451;  Loss pred: 0.2451; Loss self: 0.0000; time: 0.29s
Val loss: 0.4079 score: 0.8605 time: 0.18s
Test loss: 0.4204 score: 0.8594 time: 0.16s
Epoch 19/1000, LR 0.000967
Train loss: 0.2326;  Loss pred: 0.2326; Loss self: 0.0000; time: 0.29s
Val loss: 0.4014 score: 0.8527 time: 0.20s
Test loss: 0.4148 score: 0.8594 time: 0.15s
Epoch 20/1000, LR 0.000966
Train loss: 0.2408;  Loss pred: 0.2408; Loss self: 0.0000; time: 0.28s
Val loss: 0.3867 score: 0.8527 time: 0.17s
Test loss: 0.4013 score: 0.8516 time: 0.16s
Epoch 21/1000, LR 0.000966
Train loss: 0.2101;  Loss pred: 0.2101; Loss self: 0.0000; time: 0.32s
Val loss: 0.3671 score: 0.8992 time: 0.18s
Test loss: 0.3834 score: 0.8672 time: 0.16s
Epoch 22/1000, LR 0.000966
Train loss: 0.2044;  Loss pred: 0.2044; Loss self: 0.0000; time: 0.39s
Val loss: 0.3452 score: 0.9147 time: 0.17s
Test loss: 0.3659 score: 0.8906 time: 0.16s
Epoch 23/1000, LR 0.000966
Train loss: 0.2103;  Loss pred: 0.2103; Loss self: 0.0000; time: 0.29s
Val loss: 0.3332 score: 0.9147 time: 0.18s
Test loss: 0.3569 score: 0.8828 time: 0.16s
Epoch 24/1000, LR 0.000966
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 0.32s
Val loss: 0.3167 score: 0.9070 time: 0.18s
Test loss: 0.3439 score: 0.8828 time: 0.16s
Epoch 25/1000, LR 0.000966
Train loss: 0.1891;  Loss pred: 0.1891; Loss self: 0.0000; time: 0.32s
Val loss: 0.3005 score: 0.9225 time: 0.18s
Test loss: 0.3313 score: 0.9062 time: 0.16s
Epoch 26/1000, LR 0.000966
Train loss: 0.1936;  Loss pred: 0.1936; Loss self: 0.0000; time: 0.32s
Val loss: 0.2856 score: 0.9147 time: 0.18s
Test loss: 0.3182 score: 0.9141 time: 0.16s
Epoch 27/1000, LR 0.000966
Train loss: 0.1666;  Loss pred: 0.1666; Loss self: 0.0000; time: 0.31s
Val loss: 0.2712 score: 0.9225 time: 0.17s
Test loss: 0.3036 score: 0.8906 time: 0.16s
Epoch 28/1000, LR 0.000966
Train loss: 0.1616;  Loss pred: 0.1616; Loss self: 0.0000; time: 0.32s
Val loss: 0.2579 score: 0.9225 time: 0.18s
Test loss: 0.2913 score: 0.8906 time: 0.17s
Epoch 29/1000, LR 0.000966
Train loss: 0.1638;  Loss pred: 0.1638; Loss self: 0.0000; time: 0.32s
Val loss: 0.2371 score: 0.9147 time: 0.18s
Test loss: 0.2727 score: 0.9141 time: 0.17s
Epoch 30/1000, LR 0.000966
Train loss: 0.1575;  Loss pred: 0.1575; Loss self: 0.0000; time: 0.32s
Val loss: 0.2238 score: 0.9225 time: 0.18s
Test loss: 0.2606 score: 0.9219 time: 0.17s
Epoch 31/1000, LR 0.000966
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.33s
Val loss: 0.2114 score: 0.9380 time: 0.19s
Test loss: 0.2486 score: 0.9375 time: 0.18s
Epoch 32/1000, LR 0.000966
Train loss: 0.1574;  Loss pred: 0.1574; Loss self: 0.0000; time: 0.35s
Val loss: 0.1966 score: 0.9380 time: 0.19s
Test loss: 0.2343 score: 0.9219 time: 0.19s
Epoch 33/1000, LR 0.000965
Train loss: 0.1328;  Loss pred: 0.1328; Loss self: 0.0000; time: 0.34s
Val loss: 0.1889 score: 0.9457 time: 0.20s
Test loss: 0.2269 score: 0.9219 time: 0.18s
Epoch 34/1000, LR 0.000965
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 0.32s
Val loss: 0.1795 score: 0.9380 time: 0.22s
Test loss: 0.2188 score: 0.9375 time: 0.19s
Epoch 35/1000, LR 0.000965
Train loss: 0.1265;  Loss pred: 0.1265; Loss self: 0.0000; time: 0.31s
Val loss: 0.1838 score: 0.9457 time: 0.18s
Test loss: 0.2282 score: 0.9297 time: 0.17s
     INFO: Early stopping counter 1 of 5
Epoch 36/1000, LR 0.000965
Train loss: 0.1270;  Loss pred: 0.1270; Loss self: 0.0000; time: 0.33s
Val loss: 0.1741 score: 0.9457 time: 0.18s
Test loss: 0.2156 score: 0.9219 time: 0.17s
Epoch 37/1000, LR 0.000965
Train loss: 0.1186;  Loss pred: 0.1186; Loss self: 0.0000; time: 0.33s
Val loss: 0.1698 score: 0.9457 time: 0.18s
Test loss: 0.2114 score: 0.9297 time: 0.17s
Epoch 38/1000, LR 0.000965
Train loss: 0.1259;  Loss pred: 0.1259; Loss self: 0.0000; time: 0.34s
Val loss: 0.1943 score: 0.9302 time: 0.19s
Test loss: 0.2377 score: 0.9219 time: 0.16s
     INFO: Early stopping counter 1 of 5
Epoch 39/1000, LR 0.000965
Train loss: 0.1140;  Loss pred: 0.1140; Loss self: 0.0000; time: 0.34s
Val loss: 0.2007 score: 0.9302 time: 0.18s
Test loss: 0.2423 score: 0.9062 time: 0.16s
     INFO: Early stopping counter 2 of 5
Epoch 40/1000, LR 0.000965
Train loss: 0.1124;  Loss pred: 0.1124; Loss self: 0.0000; time: 0.33s
Val loss: 0.1907 score: 0.9380 time: 0.18s
Test loss: 0.2282 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 3 of 5
Epoch 41/1000, LR 0.000964
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.33s
Val loss: 0.1740 score: 0.9225 time: 0.19s
Test loss: 0.2079 score: 0.9219 time: 0.17s
     INFO: Early stopping counter 4 of 5
Epoch 42/1000, LR 0.000964
Train loss: 0.0989;  Loss pred: 0.0989; Loss self: 0.0000; time: 0.36s
Val loss: 0.1961 score: 0.9380 time: 0.18s
Test loss: 0.2309 score: 0.9141 time: 0.16s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 036,   Train_Loss: 0.1186,   Val_Loss: 0.1698,   Val_Precision: 0.9265,   Val_Recall: 0.9692,   Val_accuracy: 0.9474,   Val_Score: 0.9457,   Val_Loss: 0.1698,   Test_Precision: 0.9365,   Test_Recall: 0.9219,   Test_accuracy: 0.9291,   Test_Score: 0.9297,   Test_loss: 0.2114


[0.21024983702227473, 0.19815533491782844, 0.19813402905128896, 0.20128642488270998, 0.22301174700260162, 0.1945425160229206, 0.19340062607079744, 0.19910070206969976, 0.19268159288913012, 0.3130925048608333, 0.18097943207249045, 0.1826012779492885, 0.18595743784680963, 0.1855709960218519, 0.1795858289115131, 0.19084129994735122, 0.1833167909644544, 0.18642382696270943, 0.18496814789250493, 0.20108089409768581, 0.18173086713068187, 0.19723692908883095, 0.19390906509943306, 0.3332073569763452, 0.20014727115631104, 0.19550286512821913, 0.18942551710642874, 0.18908555200323462, 0.1888714579399675, 0.19424680597148836, 0.20125553105026484, 0.20076710009016097, 0.21256856387481093, 0.20221495698206127, 0.20200515491887927, 0.21418210095725954, 0.19085420086048543, 0.19094728399068117, 0.19111872604116797, 0.19334752508439124, 0.19644248997792602, 0.19815580802969635, 0.197247255127877, 0.19722814206033945, 0.19586688396520913, 0.19769135606475174, 0.197325699031353, 0.20864535681903362, 0.21073197200894356, 0.1970112919807434, 0.19654888496734202, 0.19544837181456387, 0.19585368409752846, 0.19428114499896765, 0.19454756285995245, 0.19679010403342545, 0.19522369396872818, 0.19546519313007593, 0.198962981114164, 0.25923678698018193, 0.19684510794468224, 0.1966920590493828, 0.27545726811513305, 0.19696526997722685, 0.1956014318857342, 0.20633793296292424, 0.20893248496577144, 0.33877882990054786, 0.20061185397207737, 0.19459690409712493, 0.1953969879541546, 0.19637930998578668, 0.2011004500091076, 0.19200675189495087, 0.19803917198441923, 0.21033617202192545, 0.21058546006679535, 0.1732096269261092, 0.17425142601132393, 0.17188937892206013, 0.17208028608001769, 0.17753235902637243, 0.17762739001773298, 0.17659274116158485, 0.19399003707803786, 0.2116954200901091, 0.16945983888581395, 0.16984254191629589, 0.17186007113195956, 0.16799484891816974, 0.16651991684921086, 0.16805956210009754, 0.16117557394318283, 0.17937121307477355, 0.16299164295196533, 0.1569590480066836, 0.16775575093925, 0.1672766210976988, 0.16556210396811366, 0.16714880499057472, 0.16657442110590637, 0.16794327716343105, 0.16754390485584736, 0.16770153515972197, 0.17148588388226926, 0.17224958399310708, 0.17244976991787553, 0.1816445360891521, 0.1905447351746261, 0.18089015898294747, 0.19860850600525737, 0.17358659696765244, 0.17259116005152464, 0.17498165485449135, 0.16992754489183426, 0.16832682397216558, 0.16766882687807083, 0.1796650260221213, 0.1684202600736171]
[0.001629843697847091, 0.0015360878675800655, 0.0015359227058239454, 0.0015603598828117053, 0.0017287732325783072, 0.001508081519557524, 0.001499229659463546, 0.0015434162951139517, 0.0014936557588304661, 0.002427073681091731, 0.001402941333895275, 0.001415513782552624, 0.0014415305259442607, 0.001438534852882573, 0.0013921382086163805, 0.00147938992207249, 0.00142106039507329, 0.0014451459454473598, 0.0014338616115698057, 0.0015587666209122932, 0.001408766411865751, 0.0015289684425490772, 0.0015031710472824269, 0.00258300276725849, 0.001551529233769853, 0.0015155260862652646, 0.0014684148612901452, 0.0014657794728932918, 0.0014641198289919962, 0.001505789193577429, 0.001560120395738487, 0.001556334109226054, 0.0016478183246109374, 0.0015675578060624904, 0.0015659314334796842, 0.001660326364009764, 0.0014794899291510498, 0.00148021150380373, 0.0014815405119470385, 0.0014988180239100097, 0.001522809999828884, 0.0015360915351139252, 0.0015290484893633876, 0.0015289003260491431, 0.0015183479377147994, 0.0015324911322848972, 0.0015296565816383953, 0.001617405866814214, 0.0016335811783639036, 0.0015272193176801814, 0.0015236347671886978, 0.0015151036574772394, 0.0015182456131591353, 0.0015060553875888966, 0.0015081206423252127, 0.0015255046824296547, 0.001513361968749831, 0.0015152340552719065, 0.0015423486908074729, 0.0020095874959704025, 0.001525931069338622, 0.0015247446437936653, 0.002135327659807233, 0.0015268625579629989, 0.0015162901696568543, 0.0015995188601777074, 0.001619631666401329, 0.0026261924798492084, 0.0015551306509463362, 0.001508503132535852, 0.0015147053329779426, 0.0015223202324479586, 0.001558918217124865, 0.0014884244332941928, 0.00153518737972418, 0.0016305129614102747, 0.0016324454268743826, 0.001353200210360228, 0.0013613392657134682, 0.0013428857728285948, 0.0013443772350001382, 0.0013869715548935346, 0.0013877139845135389, 0.0013796307903248817, 0.0015155471646721708, 0.0016538704694539774, 0.0013239049912954215, 0.0013268948587210616, 0.001342656805718434, 0.001312459757173201, 0.0013009368503844598, 0.001312965328907012, 0.0012591841714311158, 0.0014013376021466684, 0.0012733722105622292, 0.0012262425625522155, 0.0013105918042128906, 0.001306848602325772, 0.001293453937250888, 0.001305850038988865, 0.0013013626648898935, 0.001312056852839305, 0.0013089367566863075, 0.001310168243435328, 0.0013397334678302286, 0.001345699874946149, 0.0013472638274834026, 0.0014190979381965008, 0.0014886307435517665, 0.001413204367054277, 0.0015516289531660732, 0.0013561452888097847, 0.0013483684379025362, 0.0013670441785507137, 0.0013275589444674551, 0.0013150533122825436, 0.0013099127099849284, 0.0014036330157978227, 0.0013157832818251336]
[613.555766924724, 651.0044256617872, 651.0744298578164, 640.8777942932245, 578.4448655007178, 663.0941278913114, 667.0092161582634, 647.9133356086338, 669.4983058097678, 412.01880593513187, 712.7881799757757, 706.4572682554035, 693.7071272527924, 695.1517358068694, 718.3194842370434, 675.9543140587922, 703.6998592508279, 691.9716331421735, 697.4173741252409, 641.5328546198493, 709.8408874439402, 654.0357355792199, 665.2602854531382, 387.1463138467194, 644.5253999953543, 659.8368771495819, 681.0064555744163, 682.2308665751111, 683.0042051192428, 664.1035838650273, 640.9761725643278, 642.5355545907092, 606.8630170356357, 637.9350070106028, 638.5975647591939, 602.291225193193, 675.9086224897878, 675.5791300299176, 674.9731053157644, 667.1924036456883, 656.6807415976837, 651.0028713398478, 654.0014963268728, 654.0648745782643, 658.6105695279945, 652.5323239613339, 653.741507736928, 618.2740031540064, 612.1520088775385, 654.7848029574311, 656.3252700285441, 660.0208474614038, 658.6549576252157, 663.9862041202479, 663.0769262982867, 655.5207673353784, 660.780448200431, 659.9640474821241, 648.3618172466989, 497.61456119984143, 655.3375968898948, 655.8475244168978, 468.31220276998386, 654.9377969776855, 659.5043745659224, 625.1880017775468, 617.4243321766529, 380.77940123315653, 643.0327891688552, 662.908799081485, 660.1944142059491, 656.8920117365552, 641.4704690822807, 671.8513735942859, 651.3862823570536, 613.3039256155762, 612.5779052318362, 738.9889480831483, 734.5707460189266, 744.6649746639616, 743.838837764831, 720.9953199629686, 720.6095860960488, 724.831604957523, 659.8277000612586, 604.6422730615344, 755.3412114728224, 753.639215215483, 744.791964514652, 761.9281235363876, 768.6768190973103, 761.6347347362612, 794.1650019817656, 713.6039156218524, 785.3163369714754, 815.4993396401687, 763.0140801930129, 765.1995787578762, 773.1237821467411, 765.7847150460797, 768.4253029378315, 762.1620952140826, 763.9788514546653, 763.2607529686028, 746.4171225188205, 743.1077453581669, 742.2451190335394, 704.672999011525, 671.7582612959286, 707.6117391884572, 644.4839779249521, 737.3841197189469, 741.6370569720223, 731.5052546876433, 753.2622217397254, 760.425444854624, 763.4096473584914, 712.4369324068667, 760.0035764346329]
Elapsed: 0.19298003864584162~0.02786205396445982
Time per graph: 0.0014996761199716163~0.0002135027911742444
Speed: 676.800929380017~72.67461839907065
Total Time: 0.1691
best val loss: 0.16977948429741602 test_score: 0.9297

Testing...
Test loss: 0.2269 score: 0.9219 time: 0.16s
test Score 0.9219
Epoch Time List: [0.9882466758135706, 0.8031412991695106, 0.6795619761105627, 0.7112165957223624, 0.7899676631204784, 0.6946939721237868, 0.7107648469973356, 0.8365233042277396, 0.6786243310198188, 0.7982704290188849, 0.6541324378922582, 0.6744074809830636, 0.7946605999022722, 0.6532258200459182, 0.6523472811095417, 0.7737695630639791, 0.6586999078281224, 0.658978828927502, 0.688142969738692, 0.6951317978091538, 0.6646835568826646, 0.8159557378385216, 0.6905993779655546, 0.8411096897907555, 0.6759413729887456, 0.6774655792396516, 0.6547448092605919, 0.6549251212272793, 0.6548475737217814, 0.6626183420885354, 0.685835205251351, 0.6922257742844522, 0.7293981108814478, 0.6956636889372021, 0.6970109271351248, 0.7066722768358886, 0.668542314087972, 0.6639162271749228, 0.6658837841823697, 0.6724781231023371, 0.6766245171893388, 0.6886006118729711, 0.6884862531442195, 0.6891736940015107, 0.6857580828946084, 0.689149378798902, 0.6899841299746186, 0.7208043590653688, 0.7261032732203603, 0.6908101127482951, 0.6880247187800705, 0.6871850348543376, 0.6824190681800246, 0.6820202679373324, 0.6812485130503774, 0.6836999040096998, 0.6824974562041461, 0.6830641052220017, 0.6879306689370424, 0.7505717389285564, 0.6831556791439652, 0.6898122008424252, 0.8002853316720575, 0.6805249841418117, 0.6847520826850086, 0.810714224120602, 0.7171836651396006, 0.8487574972677976, 0.7057375821750611, 0.6825238049495965, 0.7028342890553176, 0.6835646349936724, 0.6910223828162998, 0.7795006581582129, 0.6822786668781191, 0.8433369237463921, 0.7273363168351352, 0.6687141708098352, 0.8013769232202321, 0.6456333000678569, 0.6690321408677846, 0.7664720360189676, 0.654190202942118, 0.6748587288893759, 0.7074661171063781, 0.7888117877300829, 0.831213589059189, 0.6666463501751423, 0.6610673579853028, 0.7613215839955956, 0.6486863971222192, 0.6558896747883409, 0.7207389699760824, 0.6635696850717068, 0.6313404669053853, 0.6488509587943554, 0.6196072741877288, 0.6593374251388013, 0.723785585956648, 0.6265835370868444, 0.6522394060157239, 0.6543928319588304, 0.6544575749430805, 0.6420382561627775, 0.6643573138862848, 0.6690496599767357, 0.6730235561262816, 0.7037586588412523, 0.7303381792735308, 0.7140494564082474, 0.728736103978008, 0.6591237687971443, 0.6772273750975728, 0.678846089169383, 0.6880250100512058, 0.6811653459444642, 0.6662404290400445, 0.6867624127771705, 0.7087139540817589]
Total Epoch List: [24, 53, 42]
Total Time List: [0.3336146699730307, 0.21107473107986152, 0.16912526404485106]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b284070cf40>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7048;  Loss pred: 0.7048; Loss self: 0.0000; time: 0.33s
Val loss: 0.6823 score: 0.7907 time: 0.24s
Test loss: 0.6826 score: 0.7519 time: 0.22s
Epoch 2/1000, LR 0.000050
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.32s
Val loss: 0.6734 score: 0.7287 time: 0.19s
Test loss: 0.6735 score: 0.7364 time: 0.19s
Epoch 3/1000, LR 0.000150
Train loss: 0.6600;  Loss pred: 0.6600; Loss self: 0.0000; time: 0.32s
Val loss: 0.6605 score: 0.5349 time: 0.19s
Test loss: 0.6597 score: 0.5969 time: 0.19s
Epoch 4/1000, LR 0.000250
Train loss: 0.6272;  Loss pred: 0.6272; Loss self: 0.0000; time: 0.32s
Val loss: 0.6468 score: 0.4961 time: 0.19s
Test loss: 0.6446 score: 0.5659 time: 0.19s
Epoch 5/1000, LR 0.000350
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 0.32s
Val loss: 0.6349 score: 0.5116 time: 0.19s
Test loss: 0.6311 score: 0.5581 time: 0.19s
Epoch 6/1000, LR 0.000450
Train loss: 0.5285;  Loss pred: 0.5285; Loss self: 0.0000; time: 0.33s
Val loss: 0.6250 score: 0.5194 time: 0.20s
Test loss: 0.6180 score: 0.5659 time: 0.19s
Epoch 7/1000, LR 0.000550
Train loss: 0.4893;  Loss pred: 0.4893; Loss self: 0.0000; time: 0.32s
Val loss: 0.6054 score: 0.5194 time: 0.20s
Test loss: 0.5964 score: 0.6124 time: 0.19s
Epoch 8/1000, LR 0.000650
Train loss: 0.4598;  Loss pred: 0.4598; Loss self: 0.0000; time: 0.32s
Val loss: 0.5815 score: 0.5969 time: 0.20s
Test loss: 0.5710 score: 0.6512 time: 0.19s
Epoch 9/1000, LR 0.000750
Train loss: 0.4087;  Loss pred: 0.4087; Loss self: 0.0000; time: 0.33s
Val loss: 0.5471 score: 0.7132 time: 0.20s
Test loss: 0.5387 score: 0.7364 time: 0.19s
Epoch 10/1000, LR 0.000850
Train loss: 0.3752;  Loss pred: 0.3752; Loss self: 0.0000; time: 0.44s
Val loss: 0.5106 score: 0.8450 time: 0.20s
Test loss: 0.5069 score: 0.8837 time: 0.19s
Epoch 11/1000, LR 0.000950
Train loss: 0.3469;  Loss pred: 0.3469; Loss self: 0.0000; time: 0.32s
Val loss: 0.4833 score: 0.8682 time: 0.21s
Test loss: 0.4829 score: 0.9302 time: 0.21s
Epoch 12/1000, LR 0.000950
Train loss: 0.3234;  Loss pred: 0.3234; Loss self: 0.0000; time: 0.33s
Val loss: 0.4630 score: 0.8992 time: 0.20s
Test loss: 0.4649 score: 0.9380 time: 0.30s
Epoch 13/1000, LR 0.000950
Train loss: 0.2911;  Loss pred: 0.2911; Loss self: 0.0000; time: 0.33s
Val loss: 0.4611 score: 0.8605 time: 0.23s
Test loss: 0.4590 score: 0.9225 time: 0.21s
Epoch 14/1000, LR 0.000950
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.35s
Val loss: 0.4629 score: 0.8605 time: 0.23s
Test loss: 0.4552 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 1 of 5
Epoch 15/1000, LR 0.000950
Train loss: 0.2502;  Loss pred: 0.2502; Loss self: 0.0000; time: 0.38s
Val loss: 0.4476 score: 0.8682 time: 0.23s
Test loss: 0.4397 score: 0.8992 time: 0.21s
Epoch 16/1000, LR 0.000950
Train loss: 0.2178;  Loss pred: 0.2178; Loss self: 0.0000; time: 0.33s
Val loss: 0.4309 score: 0.8682 time: 0.23s
Test loss: 0.4223 score: 0.8915 time: 0.21s
Epoch 17/1000, LR 0.000950
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 0.32s
Val loss: 0.4081 score: 0.8682 time: 0.32s
Test loss: 0.4017 score: 0.8992 time: 0.19s
Epoch 18/1000, LR 0.000950
Train loss: 0.1849;  Loss pred: 0.1849; Loss self: 0.0000; time: 0.31s
Val loss: 0.3851 score: 0.8760 time: 0.19s
Test loss: 0.3839 score: 0.9225 time: 0.19s
Epoch 19/1000, LR 0.000950
Train loss: 0.1650;  Loss pred: 0.1650; Loss self: 0.0000; time: 0.32s
Val loss: 0.3666 score: 0.8992 time: 0.19s
Test loss: 0.3721 score: 0.9147 time: 0.19s
Epoch 20/1000, LR 0.000950
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.43s
Val loss: 0.3569 score: 0.8915 time: 0.19s
Test loss: 0.3626 score: 0.9147 time: 0.19s
Epoch 21/1000, LR 0.000950
Train loss: 0.1286;  Loss pred: 0.1286; Loss self: 0.0000; time: 0.32s
Val loss: 0.3495 score: 0.8992 time: 0.19s
Test loss: 0.3602 score: 0.9147 time: 0.19s
Epoch 22/1000, LR 0.000950
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.32s
Val loss: 0.3436 score: 0.8915 time: 0.19s
Test loss: 0.3548 score: 0.9070 time: 0.31s
Epoch 23/1000, LR 0.000950
Train loss: 0.1223;  Loss pred: 0.1223; Loss self: 0.0000; time: 0.32s
Val loss: 0.3358 score: 0.8915 time: 0.19s
Test loss: 0.3434 score: 0.9147 time: 0.19s
Epoch 24/1000, LR 0.000950
Train loss: 0.1020;  Loss pred: 0.1020; Loss self: 0.0000; time: 0.32s
Val loss: 0.3287 score: 0.8992 time: 0.19s
Test loss: 0.3325 score: 0.9147 time: 0.19s
Epoch 25/1000, LR 0.000950
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.31s
Val loss: 0.3236 score: 0.8837 time: 0.29s
Test loss: 0.3217 score: 0.9147 time: 0.20s
Epoch 26/1000, LR 0.000949
Train loss: 0.1076;  Loss pred: 0.1076; Loss self: 0.0000; time: 0.32s
Val loss: 0.3155 score: 0.8837 time: 0.20s
Test loss: 0.3134 score: 0.9147 time: 0.20s
Epoch 27/1000, LR 0.000949
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.32s
Val loss: 0.3088 score: 0.8915 time: 0.20s
Test loss: 0.3239 score: 0.8915 time: 0.20s
Epoch 28/1000, LR 0.000949
Train loss: 0.0770;  Loss pred: 0.0770; Loss self: 0.0000; time: 0.45s
Val loss: 0.3054 score: 0.8915 time: 0.20s
Test loss: 0.3226 score: 0.8915 time: 0.20s
Epoch 29/1000, LR 0.000949
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.33s
Val loss: 0.2976 score: 0.8915 time: 0.20s
Test loss: 0.3129 score: 0.8915 time: 0.20s
Epoch 30/1000, LR 0.000949
Train loss: 0.0654;  Loss pred: 0.0654; Loss self: 0.0000; time: 0.32s
Val loss: 0.2905 score: 0.8915 time: 0.26s
Test loss: 0.3029 score: 0.9070 time: 0.19s
Epoch 31/1000, LR 0.000949
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.32s
Val loss: 0.2841 score: 0.8915 time: 0.19s
Test loss: 0.2914 score: 0.9070 time: 0.19s
Epoch 32/1000, LR 0.000949
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 0.31s
Val loss: 0.2792 score: 0.8915 time: 0.19s
Test loss: 0.2774 score: 0.9070 time: 0.19s
Epoch 33/1000, LR 0.000949
Train loss: 0.0552;  Loss pred: 0.0552; Loss self: 0.0000; time: 0.34s
Val loss: 0.2748 score: 0.8915 time: 0.19s
Test loss: 0.2711 score: 0.9070 time: 0.18s
Epoch 34/1000, LR 0.000949
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 0.31s
Val loss: 0.2702 score: 0.8992 time: 0.19s
Test loss: 0.2673 score: 0.9070 time: 0.19s
Epoch 35/1000, LR 0.000949
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.32s
Val loss: 0.2674 score: 0.8915 time: 0.20s
Test loss: 0.2723 score: 0.9070 time: 0.31s
Epoch 36/1000, LR 0.000949
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.33s
Val loss: 0.2678 score: 0.8915 time: 0.20s
Test loss: 0.2781 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 37/1000, LR 0.000948
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.33s
Val loss: 0.2681 score: 0.8837 time: 0.20s
Test loss: 0.2812 score: 0.8915 time: 0.20s
     INFO: Early stopping counter 2 of 5
Epoch 38/1000, LR 0.000948
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.33s
Val loss: 0.2600 score: 0.8915 time: 0.32s
Test loss: 0.2663 score: 0.8992 time: 0.20s
Epoch 39/1000, LR 0.000948
Train loss: 0.0424;  Loss pred: 0.0424; Loss self: 0.0000; time: 0.33s
Val loss: 0.2553 score: 0.8915 time: 0.20s
Test loss: 0.2557 score: 0.8992 time: 0.20s
Epoch 40/1000, LR 0.000948
Train loss: 0.0472;  Loss pred: 0.0472; Loss self: 0.0000; time: 0.32s
Val loss: 0.2547 score: 0.8837 time: 0.20s
Test loss: 0.2427 score: 0.8992 time: 0.20s
Epoch 41/1000, LR 0.000948
Train loss: 0.0388;  Loss pred: 0.0388; Loss self: 0.0000; time: 0.42s
Val loss: 0.2548 score: 0.8915 time: 0.20s
Test loss: 0.2444 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 42/1000, LR 0.000948
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.32s
Val loss: 0.2553 score: 0.8837 time: 0.20s
Test loss: 0.2507 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 2 of 5
Epoch 43/1000, LR 0.000948
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.33s
Val loss: 0.2659 score: 0.8837 time: 0.20s
Test loss: 0.2697 score: 0.8837 time: 0.31s
     INFO: Early stopping counter 3 of 5
Epoch 44/1000, LR 0.000947
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.32s
Val loss: 0.2605 score: 0.8760 time: 0.20s
Test loss: 0.2567 score: 0.8992 time: 0.20s
     INFO: Early stopping counter 4 of 5
Epoch 45/1000, LR 0.000947
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.31s
Val loss: 0.2584 score: 0.8837 time: 0.19s
Test loss: 0.2469 score: 0.8915 time: 0.18s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 039,   Train_Loss: 0.0472,   Val_Loss: 0.2547,   Val_Precision: 0.9298,   Val_Recall: 0.8281,   Val_accuracy: 0.8760,   Val_Score: 0.8837,   Val_Loss: 0.2547,   Test_Precision: 0.9815,   Test_Recall: 0.8154,   Test_accuracy: 0.8908,   Test_Score: 0.8992,   Test_loss: 0.2427


[0.22652590996585786, 0.19602554896846414, 0.19468481512740254, 0.19580564484931529, 0.1958116739988327, 0.19894798519089818, 0.19649359094910324, 0.19850276410579681, 0.19562333007343113, 0.19769242499023676, 0.21561718196608126, 0.303901364794001, 0.21419476997107267, 0.21859764796681702, 0.2194539860356599, 0.21565315313637257, 0.19652386102825403, 0.19208901608362794, 0.1932221190072596, 0.19303729105740786, 0.19396495795808733, 0.3148338980972767, 0.194849583087489, 0.1939383209683001, 0.20079056802205741, 0.20146789611317217, 0.2028044438920915, 0.20261552999727428, 0.20308944419957697, 0.19266038294881582, 0.19331139815039933, 0.1907357459422201, 0.18988988199271262, 0.193154786946252, 0.3136337208561599, 0.20387075701728463, 0.204564105020836, 0.2029660139232874, 0.20365543011575937, 0.2037341210525483, 0.20248237997293472, 0.20447301189415157, 0.31359465210698545, 0.2012002558913082, 0.18987107905559242]
[0.0017560148059368827, 0.0015195778989803422, 0.0015091846133907173, 0.001517873215886165, 0.0015179199534793232, 0.001542232443340296, 0.0015232061313883972, 0.0015387811170992, 0.0015164599230498537, 0.0015324994185289672, 0.001671451022992878, 0.0023558245332868295, 0.001660424573419168, 0.0016945554105954807, 0.001701193690198914, 0.0016717298692742058, 0.0015234407831647599, 0.0014890621401831624, 0.0014978458837772062, 0.0014964131089721538, 0.001503604325256491, 0.002440572853467261, 0.0015104618843991397, 0.0015033978369635667, 0.0015565160311787397, 0.0015617666365362185, 0.001572127472031717, 0.0015706630232346843, 0.0015743367767409068, 0.0014934913406884948, 0.0014985379701581343, 0.0014785716739706984, 0.0014720145890907955, 0.0014973239298159068, 0.0024312691539237203, 0.0015803934652502685, 0.0015857682559754728, 0.0015733799528937008, 0.0015787242644632509, 0.0015793342717251805, 0.00156963085250337, 0.0015850621077066014, 0.0024309662954029878, 0.001559691906134172, 0.0014718688298883133]
[569.4712804351739, 658.0774836689938, 662.6094588608869, 658.81655301242, 658.7962676871299, 648.4106882319999, 656.5099623702955, 649.8650060673531, 659.4305492682149, 652.5287957106642, 598.2825618242845, 424.47983110389265, 602.2556013735613, 590.1252881713627, 587.8225423485281, 598.1827676705672, 656.4088417815778, 671.5636460121099, 667.6254285108699, 668.2646616794698, 665.0685843361191, 409.73986848182994, 662.0491455815842, 665.1599300021036, 642.4604565381228, 640.3005267277711, 636.0807363207412, 636.6738028508249, 635.1881089064929, 669.5720107349288, 667.3170916680037, 676.3283901648839, 679.3410930917879, 667.8581568671972, 411.3078136109049, 632.7538185825399, 630.6091676585227, 635.57438758568, 633.4228354563164, 633.1781801376686, 637.0924720325941, 630.8901052759899, 411.3590558170315, 641.152266077077, 679.4083682551257]
Elapsed: 0.21045680987752147~0.03264191968101941
Time per graph: 0.0016314481385854378~0.000253038137062166
Speed: 623.7647464122488~70.17339012973831
Total Time: 0.1903
best val loss: 0.2547177018872065 test_score: 0.8992

Testing...
Test loss: 0.4649 score: 0.9380 time: 0.19s
test Score 0.9380
Epoch Time List: [0.7913019149564207, 0.7015031622722745, 0.6988732523750514, 0.6990407477132976, 0.7021887057926506, 0.7161017758771777, 0.710413824999705, 0.7134335679002106, 0.7156150548253208, 0.8236486192326993, 0.7470794720575213, 0.8255815838929266, 0.7705236242618412, 0.7962961918674409, 0.826684718253091, 0.7656399339903146, 0.8320852571632713, 0.6885823500342667, 0.7000659513287246, 0.8105088123120368, 0.6954656320158392, 0.8153401790186763, 0.7002874810714275, 0.7063246911857277, 0.7960680380929261, 0.7212533550336957, 0.7217469532042742, 0.8467924939468503, 0.7258258159272373, 0.7697704019956291, 0.6975509186740965, 0.6896473749075085, 0.7159505770541728, 0.6908233440481126, 0.8291724701412022, 0.7291755999904126, 0.7328123028855771, 0.8481950908899307, 0.7271699919365346, 0.7196201600600034, 0.8176516201347113, 0.7171860749367625, 0.8390071047469974, 0.7213404290378094, 0.6870915619656444]
Total Epoch List: [45]
Total Time List: [0.1903180400840938]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b284070f790>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7126;  Loss pred: 0.7126; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7011 score: 0.4961 time: 0.19s
Epoch 2/1000, LR 0.000050
Train loss: 0.7219;  Loss pred: 0.7219; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5039 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4961 time: 0.21s
Epoch 3/1000, LR 0.000150
Train loss: 0.6608;  Loss pred: 0.6608; Loss self: 0.0000; time: 0.35s
Val loss: 0.6812 score: 0.5271 time: 0.20s
Test loss: 0.6789 score: 0.5116 time: 0.20s
Epoch 4/1000, LR 0.000250
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.33s
Val loss: 0.6687 score: 0.7984 time: 0.20s
Test loss: 0.6628 score: 0.8760 time: 0.20s
Epoch 5/1000, LR 0.000350
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.33s
Val loss: 0.6547 score: 0.7442 time: 0.20s
Test loss: 0.6455 score: 0.7907 time: 0.21s
Epoch 6/1000, LR 0.000450
Train loss: 0.5184;  Loss pred: 0.5184; Loss self: 0.0000; time: 0.35s
Val loss: 0.6364 score: 0.7287 time: 0.22s
Test loss: 0.6230 score: 0.7984 time: 0.21s
Epoch 7/1000, LR 0.000550
Train loss: 0.4649;  Loss pred: 0.4649; Loss self: 0.0000; time: 0.33s
Val loss: 0.6092 score: 0.7674 time: 0.20s
Test loss: 0.5913 score: 0.8217 time: 0.19s
Epoch 8/1000, LR 0.000650
Train loss: 0.4388;  Loss pred: 0.4388; Loss self: 0.0000; time: 0.33s
Val loss: 0.5746 score: 0.8295 time: 0.19s
Test loss: 0.5520 score: 0.8837 time: 0.19s
Epoch 9/1000, LR 0.000750
Train loss: 0.3864;  Loss pred: 0.3864; Loss self: 0.0000; time: 0.33s
Val loss: 0.5418 score: 0.8450 time: 0.19s
Test loss: 0.5158 score: 0.8837 time: 0.19s
Epoch 10/1000, LR 0.000850
Train loss: 0.3502;  Loss pred: 0.3502; Loss self: 0.0000; time: 0.33s
Val loss: 0.5188 score: 0.8837 time: 0.23s
Test loss: 0.4914 score: 0.8992 time: 0.22s
Epoch 11/1000, LR 0.000950
Train loss: 0.3061;  Loss pred: 0.3061; Loss self: 0.0000; time: 0.32s
Val loss: 0.5043 score: 0.8527 time: 0.20s
Test loss: 0.4738 score: 0.8682 time: 0.19s
Epoch 12/1000, LR 0.000950
Train loss: 0.2751;  Loss pred: 0.2751; Loss self: 0.0000; time: 0.32s
Val loss: 0.4734 score: 0.8760 time: 0.20s
Test loss: 0.4377 score: 0.9070 time: 0.19s
Epoch 13/1000, LR 0.000950
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.34s
Val loss: 0.4521 score: 0.8295 time: 0.19s
Test loss: 0.4147 score: 0.8837 time: 0.19s
Epoch 14/1000, LR 0.000950
Train loss: 0.2552;  Loss pred: 0.2552; Loss self: 0.0000; time: 0.32s
Val loss: 0.4395 score: 0.8450 time: 0.19s
Test loss: 0.4011 score: 0.8992 time: 0.19s
Epoch 15/1000, LR 0.000950
Train loss: 0.2174;  Loss pred: 0.2174; Loss self: 0.0000; time: 0.32s
Val loss: 0.4270 score: 0.8450 time: 0.19s
Test loss: 0.3889 score: 0.8992 time: 0.19s
Epoch 16/1000, LR 0.000950
Train loss: 0.2019;  Loss pred: 0.2019; Loss self: 0.0000; time: 0.32s
Val loss: 0.4167 score: 0.8605 time: 0.19s
Test loss: 0.3784 score: 0.9070 time: 0.19s
Epoch 17/1000, LR 0.000950
Train loss: 0.1900;  Loss pred: 0.1900; Loss self: 0.0000; time: 0.32s
Val loss: 0.4159 score: 0.8992 time: 0.19s
Test loss: 0.3756 score: 0.8992 time: 0.18s
Epoch 18/1000, LR 0.000950
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.32s
Val loss: 0.3970 score: 0.8527 time: 0.19s
Test loss: 0.3557 score: 0.9070 time: 0.20s
Epoch 19/1000, LR 0.000950
Train loss: 0.1711;  Loss pred: 0.1711; Loss self: 0.0000; time: 0.35s
Val loss: 0.3862 score: 0.8450 time: 0.20s
Test loss: 0.3438 score: 0.9070 time: 0.19s
Epoch 20/1000, LR 0.000950
Train loss: 0.1548;  Loss pred: 0.1548; Loss self: 0.0000; time: 0.34s
Val loss: 0.3827 score: 0.8450 time: 0.20s
Test loss: 0.3392 score: 0.8837 time: 0.19s
Epoch 21/1000, LR 0.000950
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.33s
Val loss: 0.3779 score: 0.8605 time: 0.20s
Test loss: 0.3347 score: 0.8915 time: 0.19s
Epoch 22/1000, LR 0.000950
Train loss: 0.1362;  Loss pred: 0.1362; Loss self: 0.0000; time: 0.33s
Val loss: 0.3684 score: 0.8450 time: 0.19s
Test loss: 0.3251 score: 0.8837 time: 0.19s
Epoch 23/1000, LR 0.000950
Train loss: 0.1197;  Loss pred: 0.1197; Loss self: 0.0000; time: 0.32s
Val loss: 0.3592 score: 0.8450 time: 0.19s
Test loss: 0.3161 score: 0.8915 time: 0.18s
Epoch 24/1000, LR 0.000950
Train loss: 0.1178;  Loss pred: 0.1178; Loss self: 0.0000; time: 0.32s
Val loss: 0.3509 score: 0.8527 time: 0.19s
Test loss: 0.3076 score: 0.8915 time: 0.18s
Epoch 25/1000, LR 0.000950
Train loss: 0.1123;  Loss pred: 0.1123; Loss self: 0.0000; time: 0.32s
Val loss: 0.3479 score: 0.8605 time: 0.19s
Test loss: 0.3036 score: 0.8992 time: 0.18s
Epoch 26/1000, LR 0.000949
Train loss: 0.1012;  Loss pred: 0.1012; Loss self: 0.0000; time: 0.32s
Val loss: 0.3370 score: 0.8605 time: 0.19s
Test loss: 0.2920 score: 0.8992 time: 0.18s
Epoch 27/1000, LR 0.000949
Train loss: 0.1014;  Loss pred: 0.1014; Loss self: 0.0000; time: 0.32s
Val loss: 0.3287 score: 0.8605 time: 0.19s
Test loss: 0.2833 score: 0.8992 time: 0.19s
Epoch 28/1000, LR 0.000949
Train loss: 0.0820;  Loss pred: 0.0820; Loss self: 0.0000; time: 0.32s
Val loss: 0.3249 score: 0.8605 time: 0.19s
Test loss: 0.2794 score: 0.8992 time: 0.18s
Epoch 29/1000, LR 0.000949
Train loss: 0.0755;  Loss pred: 0.0755; Loss self: 0.0000; time: 0.32s
Val loss: 0.3236 score: 0.8682 time: 0.19s
Test loss: 0.2787 score: 0.8992 time: 0.18s
Epoch 30/1000, LR 0.000949
Train loss: 0.0867;  Loss pred: 0.0867; Loss self: 0.0000; time: 0.32s
Val loss: 0.3206 score: 0.8760 time: 0.19s
Test loss: 0.2768 score: 0.8992 time: 0.18s
Epoch 31/1000, LR 0.000949
Train loss: 0.0875;  Loss pred: 0.0875; Loss self: 0.0000; time: 0.32s
Val loss: 0.3115 score: 0.8605 time: 0.18s
Test loss: 0.2678 score: 0.9070 time: 0.18s
Epoch 32/1000, LR 0.000949
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.32s
Val loss: 0.3054 score: 0.8605 time: 0.19s
Test loss: 0.2618 score: 0.9070 time: 0.18s
Epoch 33/1000, LR 0.000949
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.32s
Val loss: 0.3042 score: 0.8682 time: 0.19s
Test loss: 0.2603 score: 0.9070 time: 0.18s
Epoch 34/1000, LR 0.000949
Train loss: 0.0623;  Loss pred: 0.0623; Loss self: 0.0000; time: 0.32s
Val loss: 0.3017 score: 0.8682 time: 0.19s
Test loss: 0.2570 score: 0.9070 time: 0.18s
Epoch 35/1000, LR 0.000949
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.32s
Val loss: 0.2993 score: 0.8682 time: 0.19s
Test loss: 0.2541 score: 0.8992 time: 0.19s
Epoch 36/1000, LR 0.000949
Train loss: 0.0444;  Loss pred: 0.0444; Loss self: 0.0000; time: 0.32s
Val loss: 0.2953 score: 0.8682 time: 0.19s
Test loss: 0.2499 score: 0.8992 time: 0.19s
Epoch 37/1000, LR 0.000948
Train loss: 0.0438;  Loss pred: 0.0438; Loss self: 0.0000; time: 0.32s
Val loss: 0.2877 score: 0.8605 time: 0.19s
Test loss: 0.2430 score: 0.8992 time: 0.19s
Epoch 38/1000, LR 0.000948
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.32s
Val loss: 0.2847 score: 0.8682 time: 0.33s
Test loss: 0.2408 score: 0.9147 time: 0.20s
Epoch 39/1000, LR 0.000948
Train loss: 0.0392;  Loss pred: 0.0392; Loss self: 0.0000; time: 0.34s
Val loss: 0.2798 score: 0.8682 time: 0.20s
Test loss: 0.2376 score: 0.9147 time: 0.20s
Epoch 40/1000, LR 0.000948
Train loss: 0.0421;  Loss pred: 0.0421; Loss self: 0.0000; time: 0.34s
Val loss: 0.2700 score: 0.8605 time: 0.20s
Test loss: 0.2314 score: 0.9147 time: 0.20s
Epoch 41/1000, LR 0.000948
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.33s
Val loss: 0.2576 score: 0.8682 time: 0.19s
Test loss: 0.2239 score: 0.9147 time: 0.18s
Epoch 42/1000, LR 0.000948
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.32s
Val loss: 0.2439 score: 0.8682 time: 0.19s
Test loss: 0.2153 score: 0.9225 time: 0.18s
Epoch 43/1000, LR 0.000948
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.32s
Val loss: 0.2369 score: 0.8682 time: 0.18s
Test loss: 0.2106 score: 0.9225 time: 0.18s
Epoch 44/1000, LR 0.000947
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.46s
Val loss: 0.2319 score: 0.8682 time: 0.18s
Test loss: 0.2073 score: 0.9302 time: 0.18s
Epoch 45/1000, LR 0.000947
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.32s
Val loss: 0.2271 score: 0.8682 time: 0.19s
Test loss: 0.2041 score: 0.9302 time: 0.18s
Epoch 46/1000, LR 0.000947
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.32s
Val loss: 0.2216 score: 0.8682 time: 0.29s
Test loss: 0.2004 score: 0.9225 time: 0.18s
Epoch 47/1000, LR 0.000947
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.32s
Val loss: 0.2180 score: 0.8605 time: 0.18s
Test loss: 0.1976 score: 0.9225 time: 0.18s
Epoch 48/1000, LR 0.000947
Train loss: 0.0374;  Loss pred: 0.0374; Loss self: 0.0000; time: 0.33s
Val loss: 0.2175 score: 0.8760 time: 0.19s
Test loss: 0.1972 score: 0.9225 time: 0.18s
Epoch 49/1000, LR 0.000947
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.44s
Val loss: 0.2138 score: 0.8837 time: 0.19s
Test loss: 0.1964 score: 0.9225 time: 0.18s
Epoch 50/1000, LR 0.000946
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.32s
Val loss: 0.2027 score: 0.8915 time: 0.19s
Test loss: 0.1909 score: 0.9225 time: 0.18s
Epoch 51/1000, LR 0.000946
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.33s
Val loss: 0.1889 score: 0.9147 time: 0.18s
Test loss: 0.1839 score: 0.9225 time: 0.18s
Epoch 52/1000, LR 0.000946
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.47s
Val loss: 0.1830 score: 0.8992 time: 0.20s
Test loss: 0.1828 score: 0.9302 time: 0.20s
Epoch 53/1000, LR 0.000946
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.34s
Val loss: 0.1794 score: 0.9147 time: 0.20s
Test loss: 0.1830 score: 0.9302 time: 0.20s
Epoch 54/1000, LR 0.000946
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.34s
Val loss: 0.1783 score: 0.9070 time: 0.20s
Test loss: 0.1849 score: 0.9302 time: 0.19s
Epoch 55/1000, LR 0.000945
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.32s
Val loss: 0.1788 score: 0.9070 time: 0.19s
Test loss: 0.1869 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 56/1000, LR 0.000945
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.33s
Val loss: 0.1790 score: 0.8992 time: 0.19s
Test loss: 0.1894 score: 0.9302 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 57/1000, LR 0.000945
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.34s
Val loss: 0.1812 score: 0.8992 time: 0.22s
Test loss: 0.1942 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 58/1000, LR 0.000945
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.32s
Val loss: 0.1854 score: 0.8915 time: 0.19s
Test loss: 0.2010 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 4 of 5
Epoch 59/1000, LR 0.000945
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.33s
Val loss: 0.1912 score: 0.8915 time: 0.19s
Test loss: 0.2071 score: 0.9225 time: 0.18s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 053,   Train_Loss: 0.0205,   Val_Loss: 0.1783,   Val_Precision: 0.9206,   Val_Recall: 0.8923,   Val_accuracy: 0.9062,   Val_Score: 0.9070,   Val_Loss: 0.1783,   Test_Precision: 0.9661,   Test_Recall: 0.8906,   Test_accuracy: 0.9268,   Test_Score: 0.9302,   Test_loss: 0.1849


[0.22652590996585786, 0.19602554896846414, 0.19468481512740254, 0.19580564484931529, 0.1958116739988327, 0.19894798519089818, 0.19649359094910324, 0.19850276410579681, 0.19562333007343113, 0.19769242499023676, 0.21561718196608126, 0.303901364794001, 0.21419476997107267, 0.21859764796681702, 0.2194539860356599, 0.21565315313637257, 0.19652386102825403, 0.19208901608362794, 0.1932221190072596, 0.19303729105740786, 0.19396495795808733, 0.3148338980972767, 0.194849583087489, 0.1939383209683001, 0.20079056802205741, 0.20146789611317217, 0.2028044438920915, 0.20261552999727428, 0.20308944419957697, 0.19266038294881582, 0.19331139815039933, 0.1907357459422201, 0.18988988199271262, 0.193154786946252, 0.3136337208561599, 0.20387075701728463, 0.204564105020836, 0.2029660139232874, 0.20365543011575937, 0.2037341210525483, 0.20248237997293472, 0.20447301189415157, 0.31359465210698545, 0.2012002558913082, 0.18987107905559242, 0.19298566807992756, 0.21585134998895228, 0.20405363896861672, 0.20172541798092425, 0.21258188295178115, 0.2151483171619475, 0.19621939514763653, 0.19739352096803486, 0.19898795895278454, 0.22462046309374273, 0.19434752617962658, 0.19365364289842546, 0.19125790800899267, 0.19133245502598584, 0.19124747905880213, 0.19074076204560697, 0.1885651140473783, 0.2008295590057969, 0.19662743713706732, 0.19361021392978728, 0.19184437207877636, 0.19041766715236008, 0.1894416930153966, 0.18972457107156515, 0.18939628498628736, 0.18912572297267616, 0.19006280205212533, 0.18960720719769597, 0.18851753999479115, 0.18849155702628195, 0.1888643039856106, 0.1893380149267614, 0.18971184617839754, 0.18953914497978985, 0.19010927015915513, 0.19071094691753387, 0.19106179289519787, 0.2028566268272698, 0.20457277190871537, 0.20221248199231923, 0.18754035490565002, 0.18920048396103084, 0.18792630406096578, 0.18604648509062827, 0.1868569350335747, 0.18792181299068034, 0.1890555249992758, 0.18922727392055094, 0.18726339098066092, 0.19035293185152113, 0.18878303817473352, 0.20199246983975172, 0.20315289311110973, 0.19072086596861482, 0.19148453115485609, 0.19015826797112823, 0.1903314609080553, 0.19019884010776877, 0.1896010988857597]
[0.0017560148059368827, 0.0015195778989803422, 0.0015091846133907173, 0.001517873215886165, 0.0015179199534793232, 0.001542232443340296, 0.0015232061313883972, 0.0015387811170992, 0.0015164599230498537, 0.0015324994185289672, 0.001671451022992878, 0.0023558245332868295, 0.001660424573419168, 0.0016945554105954807, 0.001701193690198914, 0.0016717298692742058, 0.0015234407831647599, 0.0014890621401831624, 0.0014978458837772062, 0.0014964131089721538, 0.001503604325256491, 0.002440572853467261, 0.0015104618843991397, 0.0015033978369635667, 0.0015565160311787397, 0.0015617666365362185, 0.001572127472031717, 0.0015706630232346843, 0.0015743367767409068, 0.0014934913406884948, 0.0014985379701581343, 0.0014785716739706984, 0.0014720145890907955, 0.0014973239298159068, 0.0024312691539237203, 0.0015803934652502685, 0.0015857682559754728, 0.0015733799528937008, 0.0015787242644632509, 0.0015793342717251805, 0.00156963085250337, 0.0015850621077066014, 0.0024309662954029878, 0.001559691906134172, 0.0014718688298883133, 0.0014960129308521516, 0.0016732662789841262, 0.0015818111547954785, 0.0015637629300846842, 0.0016479215732696213, 0.0016678164121081201, 0.001521080582539818, 0.0015301823330855415, 0.001542542317463446, 0.001741243899951494, 0.0015065699703847022, 0.0015011910302203525, 0.0014826194419301757, 0.001483197325782836, 0.0014825385973550552, 0.0014786105584930773, 0.0014617450701347155, 0.0015568182868666428, 0.0015242436987369559, 0.0015008543715487386, 0.0014871656750292742, 0.00147610594691752, 0.0014685402559333068, 0.0014707331090819003, 0.0014681882557076539, 0.0014660908757571794, 0.0014733550546676382, 0.0014698233116100462, 0.0014613762790293887, 0.0014611748606688524, 0.0014640643719814775, 0.0014677365498198557, 0.0014706344664992058, 0.0014692956975177508, 0.0014737152725515902, 0.0014783794334692548, 0.001481099169730216, 0.0015725319909090682, 0.0015858354411528322, 0.0015675386200954978, 0.0014538012008189923, 0.0014666704183025647, 0.0014567930547361688, 0.001442220814656033, 0.0014485033723532922, 0.001456758240237832, 0.0014655467054207427, 0.0014668780924073717, 0.0014516541936485342, 0.0014756041228800088, 0.0014634344044552987, 0.0015658330995329591, 0.0015748286287682925, 0.0014784563253380993, 0.0014843762105027603, 0.0014740951005513816, 0.001475437681457793, 0.0014744096132385176, 0.0014697759603547265]
[569.4712804351739, 658.0774836689938, 662.6094588608869, 658.81655301242, 658.7962676871299, 648.4106882319999, 656.5099623702955, 649.8650060673531, 659.4305492682149, 652.5287957106642, 598.2825618242845, 424.47983110389265, 602.2556013735613, 590.1252881713627, 587.8225423485281, 598.1827676705672, 656.4088417815778, 671.5636460121099, 667.6254285108699, 668.2646616794698, 665.0685843361191, 409.73986848182994, 662.0491455815842, 665.1599300021036, 642.4604565381228, 640.3005267277711, 636.0807363207412, 636.6738028508249, 635.1881089064929, 669.5720107349288, 667.3170916680037, 676.3283901648839, 679.3410930917879, 667.8581568671972, 411.3078136109049, 632.7538185825399, 630.6091676585227, 635.57438758568, 633.4228354563164, 633.1781801376686, 637.0924720325941, 630.8901052759899, 411.3590558170315, 641.152266077077, 679.4083682551257, 668.4434200915529, 597.6335103144015, 632.186716453707, 639.4831216173195, 606.8249947210243, 599.5863769777874, 657.4273654392814, 653.5168903587761, 648.2804320366382, 574.3020837160475, 663.7594135403152, 666.13773988059, 674.481914723937, 674.2191228481329, 674.518695016821, 676.3106040708568, 684.1138174030847, 642.335723080866, 656.0630697234546, 666.2871621369203, 672.4200381913168, 677.4581472883103, 680.9483062924048, 679.9330169593082, 681.1115646188089, 682.0859583370222, 678.7230252694125, 680.3538847840144, 684.286459517583, 684.3807862545967, 683.0300765031194, 681.321181326946, 679.9786233627893, 680.5981952369522, 678.5571260780927, 676.4163362671648, 675.1742357550246, 635.9171106095641, 630.5824514005338, 637.9428150478856, 687.8519562624206, 681.8164377770293, 686.4392967476798, 693.3750989015493, 690.3677403079587, 686.4557016933289, 682.3392228314626, 681.7199092249356, 688.8692943369914, 677.6885375247198, 683.324101822116, 638.6376685345775, 634.9897263311257, 676.381157063477, 673.683661139583, 678.3822832230787, 677.7649863272835, 678.2375745662127, 680.375803505898]
Elapsed: 0.20111305516687794~0.02373484941079744
Time per graph: 0.0015590159315261856~0.00018399108070385614
Speed: 647.7427428838942~54.168025522499256
Total Time: 0.1902
best val loss: 0.1783303805850735 test_score: 0.9302

Testing...
Test loss: 0.1839 score: 0.9225 time: 0.31s
test Score 0.9225
Epoch Time List: [0.7913019149564207, 0.7015031622722745, 0.6988732523750514, 0.6990407477132976, 0.7021887057926506, 0.7161017758771777, 0.710413824999705, 0.7134335679002106, 0.7156150548253208, 0.8236486192326993, 0.7470794720575213, 0.8255815838929266, 0.7705236242618412, 0.7962961918674409, 0.826684718253091, 0.7656399339903146, 0.8320852571632713, 0.6885823500342667, 0.7000659513287246, 0.8105088123120368, 0.6954656320158392, 0.8153401790186763, 0.7002874810714275, 0.7063246911857277, 0.7960680380929261, 0.7212533550336957, 0.7217469532042742, 0.8467924939468503, 0.7258258159272373, 0.7697704019956291, 0.6975509186740965, 0.6896473749075085, 0.7159505770541728, 0.6908233440481126, 0.8291724701412022, 0.7291755999904126, 0.7328123028855771, 0.8481950908899307, 0.7271699919365346, 0.7196201600600034, 0.8176516201347113, 0.7171860749367625, 0.8390071047469974, 0.7213404290378094, 0.6870915619656444, 0.7038341017905623, 0.8233719675336033, 0.7512299970258027, 0.724637147737667, 0.7361579539719969, 0.7748029110953212, 0.7255575619637966, 0.7048252979293466, 0.712044871179387, 0.7741246498189867, 0.713219917146489, 0.707027023890987, 0.7213852277491242, 0.6980770651716739, 0.6930909720249474, 0.6981764629017562, 0.6929696523584425, 0.7024497701786458, 0.7370824099052697, 0.7285286330152303, 0.710162719944492, 0.7061117559205741, 0.6916474746540189, 0.691989199956879, 0.6908993648830801, 0.6914221842307597, 0.6937594471964985, 0.69540562806651, 0.6883564658928663, 0.6895911539904773, 0.6900373722892255, 0.6958124968223274, 0.6926173274405301, 0.6938359858468175, 0.6935522987041622, 0.6963195400312543, 0.6979868961498141, 0.8457178790122271, 0.7343404581770301, 0.7329360579606146, 0.7018896790686995, 0.692333857063204, 0.6890242227818817, 0.8209428631234914, 0.6943808221258223, 0.7946467257570475, 0.6895023148972541, 0.6991202961653471, 0.8033327448647469, 0.6913115149363875, 0.6939956969581544, 0.8615617270115763, 0.7387026231735945, 0.7276693552266806, 0.6982902593445033, 0.7040136449504644, 0.7471511110197753, 0.6917303819209337, 0.6967843968886882]
Total Epoch List: [45, 59]
Total Time List: [0.1903180400840938, 0.19018309796229005]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b28407dbca0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.34s
Val loss: 0.6951 score: 0.5039 time: 0.19s
Test loss: 0.6964 score: 0.4922 time: 0.18s
Epoch 2/1000, LR 0.000067
Train loss: 0.7144;  Loss pred: 0.7144; Loss self: 0.0000; time: 0.34s
Val loss: 0.6827 score: 0.5116 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5000 time: 0.18s
Epoch 3/1000, LR 0.000167
Train loss: 0.6705;  Loss pred: 0.6705; Loss self: 0.0000; time: 0.34s
Val loss: 0.6679 score: 0.5194 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6700 score: 0.5000 time: 0.18s
Epoch 4/1000, LR 0.000267
Train loss: 0.5886;  Loss pred: 0.5886; Loss self: 0.0000; time: 0.34s
Val loss: 0.6523 score: 0.5194 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6553 score: 0.5000 time: 0.18s
Epoch 5/1000, LR 0.000367
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 0.38s
Val loss: 0.6382 score: 0.5349 time: 0.19s
Test loss: 0.6423 score: 0.5234 time: 0.18s
Epoch 6/1000, LR 0.000467
Train loss: 0.4863;  Loss pred: 0.4863; Loss self: 0.0000; time: 0.34s
Val loss: 0.6224 score: 0.5271 time: 0.19s
Test loss: 0.6280 score: 0.5234 time: 0.18s
Epoch 7/1000, LR 0.000567
Train loss: 0.4436;  Loss pred: 0.4436; Loss self: 0.0000; time: 0.33s
Val loss: 0.5693 score: 0.6357 time: 0.27s
Test loss: 0.5752 score: 0.5859 time: 0.20s
Epoch 8/1000, LR 0.000667
Train loss: 0.3853;  Loss pred: 0.3853; Loss self: 0.0000; time: 0.33s
Val loss: 0.5224 score: 0.8372 time: 0.19s
Test loss: 0.5261 score: 0.8281 time: 0.18s
Epoch 9/1000, LR 0.000767
Train loss: 0.3572;  Loss pred: 0.3572; Loss self: 0.0000; time: 0.34s
Val loss: 0.4958 score: 0.8760 time: 0.19s
Test loss: 0.4975 score: 0.8906 time: 0.18s
Epoch 10/1000, LR 0.000867
Train loss: 0.3298;  Loss pred: 0.3298; Loss self: 0.0000; time: 0.35s
Val loss: 0.4802 score: 0.8837 time: 0.26s
Test loss: 0.4809 score: 0.8828 time: 0.18s
Epoch 11/1000, LR 0.000967
Train loss: 0.2925;  Loss pred: 0.2925; Loss self: 0.0000; time: 0.33s
Val loss: 0.4924 score: 0.7829 time: 0.19s
Test loss: 0.4938 score: 0.8203 time: 0.18s
     INFO: Early stopping counter 1 of 5
Epoch 12/1000, LR 0.000967
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.33s
Val loss: 0.5259 score: 0.6434 time: 0.19s
Test loss: 0.5285 score: 0.6406 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 13/1000, LR 0.000967
Train loss: 0.2679;  Loss pred: 0.2679; Loss self: 0.0000; time: 0.35s
Val loss: 0.5294 score: 0.6434 time: 0.20s
Test loss: 0.5340 score: 0.6094 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 14/1000, LR 0.000967
Train loss: 0.2478;  Loss pred: 0.2478; Loss self: 0.0000; time: 0.35s
Val loss: 0.5659 score: 0.5969 time: 0.20s
Test loss: 0.5689 score: 0.5938 time: 0.19s
     INFO: Early stopping counter 4 of 5
Epoch 15/1000, LR 0.000967
Train loss: 0.2469;  Loss pred: 0.2469; Loss self: 0.0000; time: 0.33s
Val loss: 0.5870 score: 0.5736 time: 0.19s
Test loss: 0.5883 score: 0.5703 time: 0.31s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 009,   Train_Loss: 0.3298,   Val_Loss: 0.4802,   Val_Precision: 0.9630,   Val_Recall: 0.8000,   Val_accuracy: 0.8739,   Val_Score: 0.8837,   Val_Loss: 0.4802,   Test_Precision: 0.9455,   Test_Recall: 0.8125,   Test_accuracy: 0.8739,   Test_Score: 0.8828,   Test_loss: 0.4809


[0.22652590996585786, 0.19602554896846414, 0.19468481512740254, 0.19580564484931529, 0.1958116739988327, 0.19894798519089818, 0.19649359094910324, 0.19850276410579681, 0.19562333007343113, 0.19769242499023676, 0.21561718196608126, 0.303901364794001, 0.21419476997107267, 0.21859764796681702, 0.2194539860356599, 0.21565315313637257, 0.19652386102825403, 0.19208901608362794, 0.1932221190072596, 0.19303729105740786, 0.19396495795808733, 0.3148338980972767, 0.194849583087489, 0.1939383209683001, 0.20079056802205741, 0.20146789611317217, 0.2028044438920915, 0.20261552999727428, 0.20308944419957697, 0.19266038294881582, 0.19331139815039933, 0.1907357459422201, 0.18988988199271262, 0.193154786946252, 0.3136337208561599, 0.20387075701728463, 0.204564105020836, 0.2029660139232874, 0.20365543011575937, 0.2037341210525483, 0.20248237997293472, 0.20447301189415157, 0.31359465210698545, 0.2012002558913082, 0.18987107905559242, 0.19298566807992756, 0.21585134998895228, 0.20405363896861672, 0.20172541798092425, 0.21258188295178115, 0.2151483171619475, 0.19621939514763653, 0.19739352096803486, 0.19898795895278454, 0.22462046309374273, 0.19434752617962658, 0.19365364289842546, 0.19125790800899267, 0.19133245502598584, 0.19124747905880213, 0.19074076204560697, 0.1885651140473783, 0.2008295590057969, 0.19662743713706732, 0.19361021392978728, 0.19184437207877636, 0.19041766715236008, 0.1894416930153966, 0.18972457107156515, 0.18939628498628736, 0.18912572297267616, 0.19006280205212533, 0.18960720719769597, 0.18851753999479115, 0.18849155702628195, 0.1888643039856106, 0.1893380149267614, 0.18971184617839754, 0.18953914497978985, 0.19010927015915513, 0.19071094691753387, 0.19106179289519787, 0.2028566268272698, 0.20457277190871537, 0.20221248199231923, 0.18754035490565002, 0.18920048396103084, 0.18792630406096578, 0.18604648509062827, 0.1868569350335747, 0.18792181299068034, 0.1890555249992758, 0.18922727392055094, 0.18726339098066092, 0.19035293185152113, 0.18878303817473352, 0.20199246983975172, 0.20315289311110973, 0.19072086596861482, 0.19148453115485609, 0.19015826797112823, 0.1903314609080553, 0.19019884010776877, 0.1896010988857597, 0.18940673815086484, 0.1871906779706478, 0.1904543610289693, 0.18762989295646548, 0.1869632180314511, 0.1881564180366695, 0.2008140350226313, 0.18707156600430608, 0.18857696815393865, 0.18556710495613515, 0.18633619300089777, 0.18580445810221136, 0.1963089460041374, 0.19834130117669702, 0.3198430258780718]
[0.0017560148059368827, 0.0015195778989803422, 0.0015091846133907173, 0.001517873215886165, 0.0015179199534793232, 0.001542232443340296, 0.0015232061313883972, 0.0015387811170992, 0.0015164599230498537, 0.0015324994185289672, 0.001671451022992878, 0.0023558245332868295, 0.001660424573419168, 0.0016945554105954807, 0.001701193690198914, 0.0016717298692742058, 0.0015234407831647599, 0.0014890621401831624, 0.0014978458837772062, 0.0014964131089721538, 0.001503604325256491, 0.002440572853467261, 0.0015104618843991397, 0.0015033978369635667, 0.0015565160311787397, 0.0015617666365362185, 0.001572127472031717, 0.0015706630232346843, 0.0015743367767409068, 0.0014934913406884948, 0.0014985379701581343, 0.0014785716739706984, 0.0014720145890907955, 0.0014973239298159068, 0.0024312691539237203, 0.0015803934652502685, 0.0015857682559754728, 0.0015733799528937008, 0.0015787242644632509, 0.0015793342717251805, 0.00156963085250337, 0.0015850621077066014, 0.0024309662954029878, 0.001559691906134172, 0.0014718688298883133, 0.0014960129308521516, 0.0016732662789841262, 0.0015818111547954785, 0.0015637629300846842, 0.0016479215732696213, 0.0016678164121081201, 0.001521080582539818, 0.0015301823330855415, 0.001542542317463446, 0.001741243899951494, 0.0015065699703847022, 0.0015011910302203525, 0.0014826194419301757, 0.001483197325782836, 0.0014825385973550552, 0.0014786105584930773, 0.0014617450701347155, 0.0015568182868666428, 0.0015242436987369559, 0.0015008543715487386, 0.0014871656750292742, 0.00147610594691752, 0.0014685402559333068, 0.0014707331090819003, 0.0014681882557076539, 0.0014660908757571794, 0.0014733550546676382, 0.0014698233116100462, 0.0014613762790293887, 0.0014611748606688524, 0.0014640643719814775, 0.0014677365498198557, 0.0014706344664992058, 0.0014692956975177508, 0.0014737152725515902, 0.0014783794334692548, 0.001481099169730216, 0.0015725319909090682, 0.0015858354411528322, 0.0015675386200954978, 0.0014538012008189923, 0.0014666704183025647, 0.0014567930547361688, 0.001442220814656033, 0.0014485033723532922, 0.001456758240237832, 0.0014655467054207427, 0.0014668780924073717, 0.0014516541936485342, 0.0014756041228800088, 0.0014634344044552987, 0.0015658330995329591, 0.0015748286287682925, 0.0014784563253380993, 0.0014843762105027603, 0.0014740951005513816, 0.001475437681457793, 0.0014744096132385176, 0.0014697759603547265, 0.0014797401418036316, 0.001462427171645686, 0.0014879246955388226, 0.0014658585387223866, 0.0014606501408707118, 0.0014699720159114804, 0.001568859648614307, 0.0014614966094086412, 0.0014732575637026457, 0.0014497430074698059, 0.0014557515078195138, 0.0014515973289235262, 0.0015336636406573234, 0.0015495414154429454, 0.002498773639672436]
[569.4712804351739, 658.0774836689938, 662.6094588608869, 658.81655301242, 658.7962676871299, 648.4106882319999, 656.5099623702955, 649.8650060673531, 659.4305492682149, 652.5287957106642, 598.2825618242845, 424.47983110389265, 602.2556013735613, 590.1252881713627, 587.8225423485281, 598.1827676705672, 656.4088417815778, 671.5636460121099, 667.6254285108699, 668.2646616794698, 665.0685843361191, 409.73986848182994, 662.0491455815842, 665.1599300021036, 642.4604565381228, 640.3005267277711, 636.0807363207412, 636.6738028508249, 635.1881089064929, 669.5720107349288, 667.3170916680037, 676.3283901648839, 679.3410930917879, 667.8581568671972, 411.3078136109049, 632.7538185825399, 630.6091676585227, 635.57438758568, 633.4228354563164, 633.1781801376686, 637.0924720325941, 630.8901052759899, 411.3590558170315, 641.152266077077, 679.4083682551257, 668.4434200915529, 597.6335103144015, 632.186716453707, 639.4831216173195, 606.8249947210243, 599.5863769777874, 657.4273654392814, 653.5168903587761, 648.2804320366382, 574.3020837160475, 663.7594135403152, 666.13773988059, 674.481914723937, 674.2191228481329, 674.518695016821, 676.3106040708568, 684.1138174030847, 642.335723080866, 656.0630697234546, 666.2871621369203, 672.4200381913168, 677.4581472883103, 680.9483062924048, 679.9330169593082, 681.1115646188089, 682.0859583370222, 678.7230252694125, 680.3538847840144, 684.286459517583, 684.3807862545967, 683.0300765031194, 681.321181326946, 679.9786233627893, 680.5981952369522, 678.5571260780927, 676.4163362671648, 675.1742357550246, 635.9171106095641, 630.5824514005338, 637.9428150478856, 687.8519562624206, 681.8164377770293, 686.4392967476798, 693.3750989015493, 690.3677403079587, 686.4557016933289, 682.3392228314626, 681.7199092249356, 688.8692943369914, 677.6885375247198, 683.324101822116, 638.6376685345775, 634.9897263311257, 676.381157063477, 673.683661139583, 678.3822832230787, 677.7649863272835, 678.2375745662127, 680.375803505898, 675.7943315514277, 683.7947348001532, 672.0770231169997, 682.1940682431609, 684.6266412598211, 680.2850592906923, 637.4056473969795, 684.2301197021767, 678.7679389113488, 689.7774259627372, 686.9304236530329, 688.8962800321345, 652.0334534183801, 645.3522248801231, 400.1963139530678]
Elapsed: 0.20079178690612942~0.02506229607994049
Time per graph: 0.0015580412936548502~0.00019451516754892886
Speed: 648.8034197151026~56.509517418282314
Total Time: 0.3204
best val loss: 0.48021517405214237 test_score: 0.8828

Testing...
Test loss: 0.4809 score: 0.8828 time: 0.18s
test Score 0.8828
Epoch Time List: [0.7913019149564207, 0.7015031622722745, 0.6988732523750514, 0.6990407477132976, 0.7021887057926506, 0.7161017758771777, 0.710413824999705, 0.7134335679002106, 0.7156150548253208, 0.8236486192326993, 0.7470794720575213, 0.8255815838929266, 0.7705236242618412, 0.7962961918674409, 0.826684718253091, 0.7656399339903146, 0.8320852571632713, 0.6885823500342667, 0.7000659513287246, 0.8105088123120368, 0.6954656320158392, 0.8153401790186763, 0.7002874810714275, 0.7063246911857277, 0.7960680380929261, 0.7212533550336957, 0.7217469532042742, 0.8467924939468503, 0.7258258159272373, 0.7697704019956291, 0.6975509186740965, 0.6896473749075085, 0.7159505770541728, 0.6908233440481126, 0.8291724701412022, 0.7291755999904126, 0.7328123028855771, 0.8481950908899307, 0.7271699919365346, 0.7196201600600034, 0.8176516201347113, 0.7171860749367625, 0.8390071047469974, 0.7213404290378094, 0.6870915619656444, 0.7038341017905623, 0.8233719675336033, 0.7512299970258027, 0.724637147737667, 0.7361579539719969, 0.7748029110953212, 0.7255575619637966, 0.7048252979293466, 0.712044871179387, 0.7741246498189867, 0.713219917146489, 0.707027023890987, 0.7213852277491242, 0.6980770651716739, 0.6930909720249474, 0.6981764629017562, 0.6929696523584425, 0.7024497701786458, 0.7370824099052697, 0.7285286330152303, 0.710162719944492, 0.7061117559205741, 0.6916474746540189, 0.691989199956879, 0.6908993648830801, 0.6914221842307597, 0.6937594471964985, 0.69540562806651, 0.6883564658928663, 0.6895911539904773, 0.6900373722892255, 0.6958124968223274, 0.6926173274405301, 0.6938359858468175, 0.6935522987041622, 0.6963195400312543, 0.6979868961498141, 0.8457178790122271, 0.7343404581770301, 0.7329360579606146, 0.7018896790686995, 0.692333857063204, 0.6890242227818817, 0.8209428631234914, 0.6943808221258223, 0.7946467257570475, 0.6895023148972541, 0.6991202961653471, 0.8033327448647469, 0.6913115149363875, 0.6939956969581544, 0.8615617270115763, 0.7387026231735945, 0.7276693552266806, 0.6982902593445033, 0.7040136449504644, 0.7471511110197753, 0.6917303819209337, 0.6967843968886882, 0.7113357677590102, 0.7441778327338398, 0.7081610669847578, 0.7114014970138669, 0.7507294320967048, 0.7098344680853188, 0.8047764101065695, 0.7028969551902264, 0.7100861379876733, 0.7940109160263091, 0.7012066000606865, 0.6990102201234549, 0.7405133880674839, 0.7391942620743066, 0.8370840069837868]
Total Epoch List: [45, 59, 15]
Total Time List: [0.1903180400840938, 0.19018309796229005, 0.3204111398663372]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b2840713490>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.30s
Val loss: 0.7068 score: 0.4574 time: 0.20s
Test loss: 0.7040 score: 0.4651 time: 0.19s
Epoch 2/1000, LR 0.000050
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.31s
Val loss: 0.6969 score: 0.4496 time: 0.20s
Test loss: 0.6956 score: 0.4651 time: 0.19s
Epoch 3/1000, LR 0.000150
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.31s
Val loss: 0.6818 score: 0.6047 time: 0.20s
Test loss: 0.6830 score: 0.6047 time: 0.19s
Epoch 4/1000, LR 0.000250
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.31s
Val loss: 0.6633 score: 0.8450 time: 0.20s
Test loss: 0.6671 score: 0.8295 time: 0.19s
Epoch 5/1000, LR 0.000350
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 0.31s
Val loss: 0.6400 score: 0.8837 time: 0.20s
Test loss: 0.6472 score: 0.8527 time: 0.19s
Epoch 6/1000, LR 0.000450
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.31s
Val loss: 0.6145 score: 0.8217 time: 0.20s
Test loss: 0.6254 score: 0.7907 time: 0.19s
Epoch 7/1000, LR 0.000550
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 0.31s
Val loss: 0.5940 score: 0.7132 time: 0.20s
Test loss: 0.6087 score: 0.6434 time: 0.19s
Epoch 8/1000, LR 0.000650
Train loss: 0.4526;  Loss pred: 0.4526; Loss self: 0.0000; time: 0.31s
Val loss: 0.5584 score: 0.7907 time: 0.20s
Test loss: 0.5798 score: 0.7364 time: 0.19s
Epoch 9/1000, LR 0.000750
Train loss: 0.4159;  Loss pred: 0.4159; Loss self: 0.0000; time: 0.31s
Val loss: 0.5269 score: 0.7984 time: 0.19s
Test loss: 0.5546 score: 0.7674 time: 0.19s
Epoch 10/1000, LR 0.000850
Train loss: 0.3772;  Loss pred: 0.3772; Loss self: 0.0000; time: 0.30s
Val loss: 0.5023 score: 0.8140 time: 0.20s
Test loss: 0.5342 score: 0.7829 time: 0.19s
Epoch 11/1000, LR 0.000950
Train loss: 0.3555;  Loss pred: 0.3555; Loss self: 0.0000; time: 0.31s
Val loss: 0.4719 score: 0.8992 time: 0.20s
Test loss: 0.5054 score: 0.8527 time: 0.19s
Epoch 12/1000, LR 0.000950
Train loss: 0.3117;  Loss pred: 0.3117; Loss self: 0.0000; time: 0.31s
Val loss: 0.4317 score: 0.9225 time: 0.20s
Test loss: 0.4673 score: 0.8837 time: 0.19s
Epoch 13/1000, LR 0.000950
Train loss: 0.3082;  Loss pred: 0.3082; Loss self: 0.0000; time: 0.31s
Val loss: 0.4057 score: 0.9380 time: 0.20s
Test loss: 0.4444 score: 0.8760 time: 0.19s
Epoch 14/1000, LR 0.000950
Train loss: 0.2628;  Loss pred: 0.2628; Loss self: 0.0000; time: 0.30s
Val loss: 0.3985 score: 0.9380 time: 0.20s
Test loss: 0.4354 score: 0.8837 time: 0.19s
Epoch 15/1000, LR 0.000950
Train loss: 0.2488;  Loss pred: 0.2488; Loss self: 0.0000; time: 0.30s
Val loss: 0.3927 score: 0.9302 time: 0.20s
Test loss: 0.4284 score: 0.8760 time: 0.19s
Epoch 16/1000, LR 0.000950
Train loss: 0.2290;  Loss pred: 0.2290; Loss self: 0.0000; time: 0.31s
Val loss: 0.4125 score: 0.8915 time: 0.20s
Test loss: 0.4455 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 17/1000, LR 0.000950
Train loss: 0.2229;  Loss pred: 0.2229; Loss self: 0.0000; time: 0.31s
Val loss: 0.3951 score: 0.9070 time: 0.20s
Test loss: 0.4284 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 18/1000, LR 0.000950
Train loss: 0.2016;  Loss pred: 0.2016; Loss self: 0.0000; time: 0.31s
Val loss: 0.3765 score: 0.9225 time: 0.20s
Test loss: 0.4095 score: 0.8915 time: 0.19s
Epoch 19/1000, LR 0.000950
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 0.31s
Val loss: 0.3536 score: 0.9302 time: 0.20s
Test loss: 0.3870 score: 0.8992 time: 0.19s
Epoch 20/1000, LR 0.000950
Train loss: 0.1754;  Loss pred: 0.1754; Loss self: 0.0000; time: 0.30s
Val loss: 0.3414 score: 0.9380 time: 0.20s
Test loss: 0.3742 score: 0.8915 time: 0.19s
Epoch 21/1000, LR 0.000950
Train loss: 0.1587;  Loss pred: 0.1587; Loss self: 0.0000; time: 0.30s
Val loss: 0.3356 score: 0.9380 time: 0.19s
Test loss: 0.3681 score: 0.9147 time: 0.19s
Epoch 22/1000, LR 0.000950
Train loss: 0.1485;  Loss pred: 0.1485; Loss self: 0.0000; time: 0.30s
Val loss: 0.3298 score: 0.9380 time: 0.19s
Test loss: 0.3622 score: 0.9147 time: 0.19s
Epoch 23/1000, LR 0.000950
Train loss: 0.1358;  Loss pred: 0.1358; Loss self: 0.0000; time: 0.31s
Val loss: 0.3272 score: 0.9457 time: 0.20s
Test loss: 0.3586 score: 0.8915 time: 0.19s
Epoch 24/1000, LR 0.000950
Train loss: 0.1250;  Loss pred: 0.1250; Loss self: 0.0000; time: 0.31s
Val loss: 0.3287 score: 0.9225 time: 0.20s
Test loss: 0.3598 score: 0.8992 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 25/1000, LR 0.000950
Train loss: 0.1219;  Loss pred: 0.1219; Loss self: 0.0000; time: 0.30s
Val loss: 0.3101 score: 0.9457 time: 0.19s
Test loss: 0.3437 score: 0.8837 time: 0.19s
Epoch 26/1000, LR 0.000949
Train loss: 0.1143;  Loss pred: 0.1143; Loss self: 0.0000; time: 0.30s
Val loss: 0.2951 score: 0.9302 time: 0.19s
Test loss: 0.3327 score: 0.8915 time: 0.19s
Epoch 27/1000, LR 0.000949
Train loss: 0.1069;  Loss pred: 0.1069; Loss self: 0.0000; time: 0.31s
Val loss: 0.2841 score: 0.9302 time: 0.20s
Test loss: 0.3262 score: 0.8992 time: 0.19s
Epoch 28/1000, LR 0.000949
Train loss: 0.0952;  Loss pred: 0.0952; Loss self: 0.0000; time: 0.30s
Val loss: 0.2742 score: 0.9225 time: 0.20s
Test loss: 0.3199 score: 0.8992 time: 0.19s
Epoch 29/1000, LR 0.000949
Train loss: 0.1150;  Loss pred: 0.1150; Loss self: 0.0000; time: 0.31s
Val loss: 0.2630 score: 0.9225 time: 0.19s
Test loss: 0.3124 score: 0.8915 time: 0.19s
Epoch 30/1000, LR 0.000949
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.30s
Val loss: 0.2611 score: 0.9225 time: 0.19s
Test loss: 0.3113 score: 0.8837 time: 0.19s
Epoch 31/1000, LR 0.000949
Train loss: 0.1055;  Loss pred: 0.1055; Loss self: 0.0000; time: 0.30s
Val loss: 0.2593 score: 0.9147 time: 0.19s
Test loss: 0.3107 score: 0.8760 time: 0.19s
Epoch 32/1000, LR 0.000949
Train loss: 0.0895;  Loss pred: 0.0895; Loss self: 0.0000; time: 0.31s
Val loss: 0.2333 score: 0.9225 time: 0.20s
Test loss: 0.2951 score: 0.8682 time: 0.31s
Epoch 33/1000, LR 0.000949
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.32s
Val loss: 0.2249 score: 0.9225 time: 0.21s
Test loss: 0.2917 score: 0.8605 time: 0.20s
Epoch 34/1000, LR 0.000949
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.32s
Val loss: 0.2201 score: 0.9302 time: 0.21s
Test loss: 0.2883 score: 0.8527 time: 0.20s
Epoch 35/1000, LR 0.000949
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.43s
Val loss: 0.2230 score: 0.9147 time: 0.20s
Test loss: 0.2886 score: 0.8527 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 36/1000, LR 0.000949
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.31s
Val loss: 0.2189 score: 0.9147 time: 0.21s
Test loss: 0.2857 score: 0.8527 time: 0.20s
Epoch 37/1000, LR 0.000948
Train loss: 0.0736;  Loss pred: 0.0736; Loss self: 0.0000; time: 0.31s
Val loss: 0.2094 score: 0.9302 time: 0.26s
Test loss: 0.2790 score: 0.8527 time: 0.23s
Epoch 38/1000, LR 0.000948
Train loss: 0.0666;  Loss pred: 0.0666; Loss self: 0.0000; time: 0.30s
Val loss: 0.2043 score: 0.9302 time: 0.19s
Test loss: 0.2751 score: 0.8527 time: 0.19s
Epoch 39/1000, LR 0.000948
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.31s
Val loss: 0.2023 score: 0.9225 time: 0.20s
Test loss: 0.2725 score: 0.8605 time: 0.19s
Epoch 40/1000, LR 0.000948
Train loss: 0.0587;  Loss pred: 0.0587; Loss self: 0.0000; time: 0.32s
Val loss: 0.1895 score: 0.9302 time: 0.23s
Test loss: 0.2639 score: 0.8682 time: 0.19s
Epoch 41/1000, LR 0.000948
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.30s
Val loss: 0.1845 score: 0.9302 time: 0.19s
Test loss: 0.2597 score: 0.8760 time: 0.19s
Epoch 42/1000, LR 0.000948
Train loss: 0.0615;  Loss pred: 0.0615; Loss self: 0.0000; time: 0.30s
Val loss: 0.1788 score: 0.9302 time: 0.19s
Test loss: 0.2551 score: 0.8760 time: 0.19s
Epoch 43/1000, LR 0.000948
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.40s
Val loss: 0.1713 score: 0.9380 time: 0.19s
Test loss: 0.2505 score: 0.8760 time: 0.19s
Epoch 44/1000, LR 0.000947
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.30s
Val loss: 0.1753 score: 0.9457 time: 0.19s
Test loss: 0.2509 score: 0.8682 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 45/1000, LR 0.000947
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.30s
Val loss: 0.1636 score: 0.9457 time: 0.32s
Test loss: 0.2442 score: 0.8682 time: 0.20s
Epoch 46/1000, LR 0.000947
Train loss: 0.0593;  Loss pred: 0.0593; Loss self: 0.0000; time: 0.31s
Val loss: 0.1620 score: 0.9535 time: 0.20s
Test loss: 0.2421 score: 0.8682 time: 0.20s
Epoch 47/1000, LR 0.000947
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.31s
Val loss: 0.1842 score: 0.9302 time: 0.21s
Test loss: 0.2595 score: 0.8837 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 48/1000, LR 0.000947
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.35s
Val loss: 0.2040 score: 0.9302 time: 0.21s
Test loss: 0.2780 score: 0.8760 time: 0.21s
     INFO: Early stopping counter 2 of 5
Epoch 49/1000, LR 0.000947
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.32s
Val loss: 0.2166 score: 0.9225 time: 0.21s
Test loss: 0.2933 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 3 of 5
Epoch 50/1000, LR 0.000946
Train loss: 0.0423;  Loss pred: 0.0423; Loss self: 0.0000; time: 0.30s
Val loss: 0.1842 score: 0.9302 time: 0.20s
Test loss: 0.2692 score: 0.8682 time: 0.25s
     INFO: Early stopping counter 4 of 5
Epoch 51/1000, LR 0.000946
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.32s
Val loss: 0.1628 score: 0.9457 time: 0.21s
Test loss: 0.2582 score: 0.8605 time: 0.21s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 045,   Train_Loss: 0.0593,   Val_Loss: 0.1620,   Val_Precision: 0.9531,   Val_Recall: 0.9531,   Val_accuracy: 0.9531,   Val_Score: 0.9535,   Val_Loss: 0.1620,   Test_Precision: 0.9000,   Test_Recall: 0.8308,   Test_accuracy: 0.8640,   Test_Score: 0.8682,   Test_loss: 0.2421


[0.19696645205840468, 0.1976079719606787, 0.19842608994804323, 0.1967304979916662, 0.19650478288531303, 0.19683649414218962, 0.19762168801389635, 0.1988717271015048, 0.19680171110667288, 0.19663211703300476, 0.19720862014219165, 0.1982602570205927, 0.1970206811092794, 0.19670288590714335, 0.19659345992840827, 0.1959595000371337, 0.19777078996412456, 0.19864406902343035, 0.1972805350087583, 0.19630490988492966, 0.19629450817592442, 0.1968258419074118, 0.19585665804333985, 0.19663549400866032, 0.1961518400348723, 0.19678948097862303, 0.19623732985928655, 0.19728468707762659, 0.19620259501971304, 0.19558831211179495, 0.19749596994370222, 0.3138798519503325, 0.20739215100184083, 0.20567252696491778, 0.20536439679563046, 0.20646941591985524, 0.23159329197369516, 0.19505678582936525, 0.19602737994864583, 0.19393540709279478, 0.1949606561101973, 0.19730432983487844, 0.19429570087231696, 0.19450334715656936, 0.20531999599188566, 0.20646808808669448, 0.2065424770116806, 0.21327360696159303, 0.21123632695525885, 0.25290959794074297, 0.2143569909967482]
[0.001526871721382982, 0.0015318447438812301, 0.0015381867437832808, 0.0015250426200904357, 0.001523292890583822, 0.0015258642956758885, 0.0015319510698751655, 0.0015416412953605024, 0.0015255946597416503, 0.001524279977000037, 0.001528748993350323, 0.0015369012172138968, 0.001527292101622321, 0.0015248285729235918, 0.0015239803095225446, 0.0015190658917607262, 0.001533106898946702, 0.0015398765040575996, 0.0015293064729361109, 0.0015217434874800749, 0.0015216628540769335, 0.0015257817202124946, 0.0015182686670026345, 0.0015243061551058939, 0.0015205568994951341, 0.0015254998525474654, 0.0015212196113122988, 0.0015293386595164851, 0.0015209503489900236, 0.0015161884659829067, 0.00153097651119149, 0.0024331771469018027, 0.0016076910930375259, 0.0015943606741466495, 0.0015919720681831818, 0.0016005381079058545, 0.0017952968370053889, 0.0015120681072043817, 0.0015195920926251614, 0.0015033752487813548, 0.0015113229155829249, 0.001529490928952546, 0.0015061682238164105, 0.0015077778849346463, 0.0015916278759060905, 0.0016005278146255385, 0.0016011044729587643, 0.00165328377489607, 0.0016374909066299136, 0.001960539518920488, 0.001661682100749986]
[654.9338664116709, 652.8076712698071, 650.1161214927832, 655.7193791349251, 656.472570824339, 655.3662752538852, 652.7623627571129, 648.659323676301, 655.4821056920478, 656.0474552503919, 654.1296212457054, 650.6599050086028, 654.7535988287896, 655.8114254657985, 656.176456973578, 658.2992913104745, 652.2702367897731, 649.4027263647337, 653.8911707344722, 657.1409756160324, 657.1757977272941, 655.4017437440073, 658.6449564122269, 656.0361884325854, 657.6537848284579, 655.5228427784364, 657.3672812023095, 653.8774088900358, 657.4836585980884, 659.5486131413916, 653.1778852843046, 410.9852836951529, 622.0100393233058, 627.2106532828468, 628.1517245093612, 624.7898722688964, 557.0109518312477, 661.3458714163812, 658.0713369417819, 665.1699240163799, 661.6719628143103, 653.8123117113472, 663.9364608729734, 663.2276610446137, 628.2875634046775, 624.793890404186, 624.5688628625513, 604.8568401772785, 610.6904141886683, 510.0636790788179, 601.7998265424286]
Elapsed: 0.20299353495733263~0.01855822427846477
Time per graph: 0.0015735932942428888~0.00014386220370902924
Speed: 639.3185849319132~42.36391611210565
Total Time: 0.2148
best val loss: 0.16204395153841308 test_score: 0.8682

Testing...
Test loss: 0.2421 score: 0.8682 time: 0.21s
test Score 0.8682
Epoch Time List: [0.6910657770931721, 0.6965502959210426, 0.6987272738479078, 0.696099943947047, 0.6956968288868666, 0.6965404008515179, 0.6964079278986901, 0.6975351367145777, 0.6919458580669016, 0.6914222061168402, 0.6932701792102307, 0.694362583104521, 0.6941537840757519, 0.6910362427588552, 0.6916424320079386, 0.6933367447927594, 0.6954420146066695, 0.696660776855424, 0.6961952080018818, 0.6923121900763363, 0.6909396320115775, 0.6915816373657435, 0.6918948718812317, 0.6924216758925468, 0.6909223929978907, 0.6909104068763554, 0.6925587109290063, 0.6925647677853703, 0.6912493400741369, 0.6891216372605413, 0.690234798938036, 0.8143865743186325, 0.724562959279865, 0.7279491140507162, 0.8304462530650198, 0.7220640769228339, 0.7996774467173964, 0.6892585209570825, 0.6961425531189889, 0.7401868549641222, 0.6866613049060106, 0.6855595619417727, 0.7883581146597862, 0.6882852322887629, 0.8229785030707717, 0.7141559319570661, 0.7179254416842014, 0.7702913088724017, 0.7408669041469693, 0.7487489471677691, 0.7402280259411782]
Total Epoch List: [51]
Total Time List: [0.2147874350193888]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b284070d6c0>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.7645;  Loss pred: 0.7645; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7097 score: 0.5039 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7100 score: 0.4961 time: 0.20s
Epoch 2/1000, LR 0.000050
Train loss: 0.7324;  Loss pred: 0.7324; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5039 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4961 time: 0.19s
Epoch 3/1000, LR 0.000150
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.32s
Val loss: 0.6769 score: 0.6357 time: 0.21s
Test loss: 0.6796 score: 0.5581 time: 0.19s
Epoch 4/1000, LR 0.000250
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.32s
Val loss: 0.6617 score: 0.6899 time: 0.19s
Test loss: 0.6654 score: 0.6822 time: 0.20s
Epoch 5/1000, LR 0.000350
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 0.32s
Val loss: 0.6551 score: 0.5194 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6601 score: 0.5039 time: 0.19s
Epoch 6/1000, LR 0.000450
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.35s
Val loss: 0.6555 score: 0.5039 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6616 score: 0.5039 time: 0.20s
     INFO: Early stopping counter 1 of 5
Epoch 7/1000, LR 0.000550
Train loss: 0.4860;  Loss pred: 0.4860; Loss self: 0.0000; time: 0.32s
Val loss: 0.6474 score: 0.5116 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6562 score: 0.5039 time: 0.21s
Epoch 8/1000, LR 0.000650
Train loss: 0.4338;  Loss pred: 0.4338; Loss self: 0.0000; time: 0.33s
Val loss: 0.6231 score: 0.6202 time: 0.20s
Test loss: 0.6373 score: 0.5891 time: 0.20s
Epoch 9/1000, LR 0.000750
Train loss: 0.4137;  Loss pred: 0.4137; Loss self: 0.0000; time: 0.43s
Val loss: 0.5700 score: 0.7054 time: 0.20s
Test loss: 0.5918 score: 0.6512 time: 0.20s
Epoch 10/1000, LR 0.000850
Train loss: 0.3766;  Loss pred: 0.3766; Loss self: 0.0000; time: 0.33s
Val loss: 0.5161 score: 0.7752 time: 0.20s
Test loss: 0.5412 score: 0.7442 time: 0.20s
Epoch 11/1000, LR 0.000950
Train loss: 0.3473;  Loss pred: 0.3473; Loss self: 0.0000; time: 0.32s
Val loss: 0.4763 score: 0.8450 time: 0.31s
Test loss: 0.5008 score: 0.8140 time: 0.19s
Epoch 12/1000, LR 0.000950
Train loss: 0.3213;  Loss pred: 0.3213; Loss self: 0.0000; time: 0.31s
Val loss: 0.4461 score: 0.9070 time: 0.19s
Test loss: 0.4714 score: 0.8527 time: 0.19s
Epoch 13/1000, LR 0.000950
Train loss: 0.2968;  Loss pred: 0.2968; Loss self: 0.0000; time: 0.31s
Val loss: 0.4267 score: 0.9070 time: 0.19s
Test loss: 0.4522 score: 0.8605 time: 0.19s
Epoch 14/1000, LR 0.000950
Train loss: 0.2692;  Loss pred: 0.2692; Loss self: 0.0000; time: 0.35s
Val loss: 0.4107 score: 0.9147 time: 0.19s
Test loss: 0.4356 score: 0.8605 time: 0.19s
Epoch 15/1000, LR 0.000950
Train loss: 0.2495;  Loss pred: 0.2495; Loss self: 0.0000; time: 0.32s
Val loss: 0.3974 score: 0.9147 time: 0.19s
Test loss: 0.4221 score: 0.8760 time: 0.19s
Epoch 16/1000, LR 0.000950
Train loss: 0.2321;  Loss pred: 0.2321; Loss self: 0.0000; time: 0.32s
Val loss: 0.3887 score: 0.9147 time: 0.19s
Test loss: 0.4141 score: 0.8837 time: 0.33s
Epoch 17/1000, LR 0.000950
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.31s
Val loss: 0.3821 score: 0.9070 time: 0.19s
Test loss: 0.4073 score: 0.8760 time: 0.19s
Epoch 18/1000, LR 0.000950
Train loss: 0.1967;  Loss pred: 0.1967; Loss self: 0.0000; time: 0.32s
Val loss: 0.3740 score: 0.9147 time: 0.19s
Test loss: 0.3980 score: 0.8760 time: 0.19s
Epoch 19/1000, LR 0.000950
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.31s
Val loss: 0.3642 score: 0.9147 time: 0.28s
Test loss: 0.3868 score: 0.8760 time: 0.21s
Epoch 20/1000, LR 0.000950
Train loss: 0.1823;  Loss pred: 0.1823; Loss self: 0.0000; time: 0.33s
Val loss: 0.3528 score: 0.9147 time: 0.20s
Test loss: 0.3737 score: 0.8760 time: 0.20s
Epoch 21/1000, LR 0.000950
Train loss: 0.1741;  Loss pred: 0.1741; Loss self: 0.0000; time: 0.32s
Val loss: 0.3410 score: 0.9147 time: 0.20s
Test loss: 0.3614 score: 0.8992 time: 0.19s
Epoch 22/1000, LR 0.000950
Train loss: 0.1726;  Loss pred: 0.1726; Loss self: 0.0000; time: 0.31s
Val loss: 0.3367 score: 0.9147 time: 0.19s
Test loss: 0.3571 score: 0.8915 time: 0.19s
Epoch 23/1000, LR 0.000950
Train loss: 0.1484;  Loss pred: 0.1484; Loss self: 0.0000; time: 0.31s
Val loss: 0.3296 score: 0.9147 time: 0.19s
Test loss: 0.3503 score: 0.8915 time: 0.19s
Epoch 24/1000, LR 0.000950
Train loss: 0.1427;  Loss pred: 0.1427; Loss self: 0.0000; time: 0.31s
Val loss: 0.3201 score: 0.9147 time: 0.19s
Test loss: 0.3411 score: 0.8915 time: 0.19s
Epoch 25/1000, LR 0.000950
Train loss: 0.1367;  Loss pred: 0.1367; Loss self: 0.0000; time: 0.31s
Val loss: 0.3140 score: 0.9147 time: 0.19s
Test loss: 0.3365 score: 0.8915 time: 0.19s
Epoch 26/1000, LR 0.000949
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 0.31s
Val loss: 0.3063 score: 0.9147 time: 0.19s
Test loss: 0.3317 score: 0.8915 time: 0.19s
Epoch 27/1000, LR 0.000949
Train loss: 0.1153;  Loss pred: 0.1153; Loss self: 0.0000; time: 0.31s
Val loss: 0.2964 score: 0.9147 time: 0.19s
Test loss: 0.3245 score: 0.8915 time: 0.19s
Epoch 28/1000, LR 0.000949
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.31s
Val loss: 0.2861 score: 0.9147 time: 0.19s
Test loss: 0.3160 score: 0.8915 time: 0.19s
Epoch 29/1000, LR 0.000949
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 0.31s
Val loss: 0.2775 score: 0.9147 time: 0.19s
Test loss: 0.3089 score: 0.8992 time: 0.19s
Epoch 30/1000, LR 0.000949
Train loss: 0.1056;  Loss pred: 0.1056; Loss self: 0.0000; time: 0.31s
Val loss: 0.2690 score: 0.9147 time: 0.19s
Test loss: 0.3018 score: 0.8992 time: 0.19s
Epoch 31/1000, LR 0.000949
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.31s
Val loss: 0.2602 score: 0.9225 time: 0.19s
Test loss: 0.2940 score: 0.8992 time: 0.19s
Epoch 32/1000, LR 0.000949
Train loss: 0.0819;  Loss pred: 0.0819; Loss self: 0.0000; time: 0.31s
Val loss: 0.2541 score: 0.9225 time: 0.19s
Test loss: 0.2897 score: 0.8915 time: 0.19s
Epoch 33/1000, LR 0.000949
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 0.31s
Val loss: 0.2462 score: 0.9225 time: 0.19s
Test loss: 0.2834 score: 0.9070 time: 0.19s
Epoch 34/1000, LR 0.000949
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.31s
Val loss: 0.2352 score: 0.9225 time: 0.19s
Test loss: 0.2741 score: 0.9147 time: 0.19s
Epoch 35/1000, LR 0.000949
Train loss: 0.0755;  Loss pred: 0.0755; Loss self: 0.0000; time: 0.31s
Val loss: 0.2252 score: 0.9147 time: 0.19s
Test loss: 0.2670 score: 0.9147 time: 0.19s
Epoch 36/1000, LR 0.000949
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.31s
Val loss: 0.2178 score: 0.9147 time: 0.19s
Test loss: 0.2635 score: 0.9147 time: 0.19s
Epoch 37/1000, LR 0.000948
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.31s
Val loss: 0.2148 score: 0.9225 time: 0.19s
Test loss: 0.2652 score: 0.9147 time: 0.19s
Epoch 38/1000, LR 0.000948
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.31s
Val loss: 0.2058 score: 0.9225 time: 0.19s
Test loss: 0.2600 score: 0.9147 time: 0.19s
Epoch 39/1000, LR 0.000948
Train loss: 0.0713;  Loss pred: 0.0713; Loss self: 0.0000; time: 0.31s
Val loss: 0.1962 score: 0.9302 time: 0.19s
Test loss: 0.2528 score: 0.9147 time: 0.19s
Epoch 40/1000, LR 0.000948
Train loss: 0.0572;  Loss pred: 0.0572; Loss self: 0.0000; time: 0.31s
Val loss: 0.1910 score: 0.9380 time: 0.19s
Test loss: 0.2515 score: 0.9147 time: 0.19s
Epoch 41/1000, LR 0.000948
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.31s
Val loss: 0.1837 score: 0.9457 time: 0.19s
Test loss: 0.2481 score: 0.9147 time: 0.19s
Epoch 42/1000, LR 0.000948
Train loss: 0.0539;  Loss pred: 0.0539; Loss self: 0.0000; time: 0.31s
Val loss: 0.1753 score: 0.9457 time: 0.19s
Test loss: 0.2425 score: 0.9302 time: 0.19s
Epoch 43/1000, LR 0.000948
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.31s
Val loss: 0.1699 score: 0.9457 time: 0.19s
Test loss: 0.2405 score: 0.9302 time: 0.19s
Epoch 44/1000, LR 0.000947
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.31s
Val loss: 0.1661 score: 0.9612 time: 0.19s
Test loss: 0.2412 score: 0.9225 time: 0.19s
Epoch 45/1000, LR 0.000947
Train loss: 0.0452;  Loss pred: 0.0452; Loss self: 0.0000; time: 0.31s
Val loss: 0.1676 score: 0.9302 time: 0.19s
Test loss: 0.2485 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 46/1000, LR 0.000947
Train loss: 0.0469;  Loss pred: 0.0469; Loss self: 0.0000; time: 0.30s
Val loss: 0.1604 score: 0.9302 time: 0.19s
Test loss: 0.2448 score: 0.9225 time: 0.19s
Epoch 47/1000, LR 0.000947
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.31s
Val loss: 0.1506 score: 0.9535 time: 0.19s
Test loss: 0.2387 score: 0.9225 time: 0.19s
Epoch 48/1000, LR 0.000947
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.31s
Val loss: 0.1449 score: 0.9612 time: 0.19s
Test loss: 0.2378 score: 0.9225 time: 0.19s
Epoch 49/1000, LR 0.000947
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.31s
Val loss: 0.1410 score: 0.9612 time: 0.19s
Test loss: 0.2395 score: 0.9225 time: 0.19s
Epoch 50/1000, LR 0.000946
Train loss: 0.0311;  Loss pred: 0.0311; Loss self: 0.0000; time: 0.31s
Val loss: 0.1391 score: 0.9535 time: 0.19s
Test loss: 0.2445 score: 0.9225 time: 0.19s
Epoch 51/1000, LR 0.000946
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.31s
Val loss: 0.1411 score: 0.9457 time: 0.19s
Test loss: 0.2552 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 52/1000, LR 0.000946
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.31s
Val loss: 0.1402 score: 0.9457 time: 0.19s
Test loss: 0.2613 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 53/1000, LR 0.000946
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.31s
Val loss: 0.1368 score: 0.9457 time: 0.19s
Test loss: 0.2615 score: 0.9147 time: 0.19s
Epoch 54/1000, LR 0.000946
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.31s
Val loss: 0.1358 score: 0.9457 time: 0.19s
Test loss: 0.2653 score: 0.9147 time: 0.19s
Epoch 55/1000, LR 0.000945
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.31s
Val loss: 0.1283 score: 0.9457 time: 0.19s
Test loss: 0.2620 score: 0.9225 time: 0.19s
Epoch 56/1000, LR 0.000945
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.31s
Val loss: 0.1256 score: 0.9535 time: 0.19s
Test loss: 0.2638 score: 0.9225 time: 0.19s
Epoch 57/1000, LR 0.000945
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.41s
Val loss: 0.1256 score: 0.9457 time: 0.19s
Test loss: 0.2702 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 58/1000, LR 0.000945
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.31s
Val loss: 0.1414 score: 0.9302 time: 0.19s
Test loss: 0.2908 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 59/1000, LR 0.000945
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.31s
Val loss: 0.1360 score: 0.9457 time: 0.33s
Test loss: 0.2875 score: 0.9147 time: 0.21s
     INFO: Early stopping counter 3 of 5
Epoch 60/1000, LR 0.000944
Train loss: 0.0209;  Loss pred: 0.0209; Loss self: 0.0000; time: 0.32s
Val loss: 0.1225 score: 0.9535 time: 0.20s
Test loss: 0.2722 score: 0.9225 time: 0.21s
Epoch 61/1000, LR 0.000944
Train loss: 0.0217;  Loss pred: 0.0217; Loss self: 0.0000; time: 0.33s
Val loss: 0.1212 score: 0.9535 time: 0.20s
Test loss: 0.2669 score: 0.9225 time: 0.19s
Epoch 62/1000, LR 0.000944
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.38s
Val loss: 0.1197 score: 0.9535 time: 0.19s
Test loss: 0.2610 score: 0.9225 time: 0.19s
Epoch 63/1000, LR 0.000944
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.31s
Val loss: 0.1181 score: 0.9535 time: 0.19s
Test loss: 0.2556 score: 0.9225 time: 0.19s
Epoch 64/1000, LR 0.000943
Train loss: 0.0161;  Loss pred: 0.0161; Loss self: 0.0000; time: 0.32s
Val loss: 0.1166 score: 0.9535 time: 0.19s
Test loss: 0.2539 score: 0.9225 time: 0.19s
Epoch 65/1000, LR 0.000943
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.43s
Val loss: 0.1206 score: 0.9457 time: 0.19s
Test loss: 0.2616 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 66/1000, LR 0.000943
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.31s
Val loss: 0.1334 score: 0.9302 time: 0.19s
Test loss: 0.2777 score: 0.9225 time: 0.19s
     INFO: Early stopping counter 2 of 5
Epoch 67/1000, LR 0.000943
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.31s
Val loss: 0.1559 score: 0.9225 time: 0.30s
Test loss: 0.3016 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 68/1000, LR 0.000942
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.31s
Val loss: 0.1613 score: 0.9225 time: 0.19s
Test loss: 0.3132 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 4 of 5
Epoch 69/1000, LR 0.000942
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.32s
Val loss: 0.1721 score: 0.9147 time: 0.19s
Test loss: 0.3278 score: 0.9147 time: 0.19s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 063,   Train_Loss: 0.0161,   Val_Loss: 0.1166,   Val_Precision: 1.0000,   Val_Recall: 0.9077,   Val_accuracy: 0.9516,   Val_Score: 0.9535,   Val_Loss: 0.1166,   Test_Precision: 0.9355,   Test_Recall: 0.9062,   Test_accuracy: 0.9206,   Test_Score: 0.9225,   Test_loss: 0.2539


[0.19696645205840468, 0.1976079719606787, 0.19842608994804323, 0.1967304979916662, 0.19650478288531303, 0.19683649414218962, 0.19762168801389635, 0.1988717271015048, 0.19680171110667288, 0.19663211703300476, 0.19720862014219165, 0.1982602570205927, 0.1970206811092794, 0.19670288590714335, 0.19659345992840827, 0.1959595000371337, 0.19777078996412456, 0.19864406902343035, 0.1972805350087583, 0.19630490988492966, 0.19629450817592442, 0.1968258419074118, 0.19585665804333985, 0.19663549400866032, 0.1961518400348723, 0.19678948097862303, 0.19623732985928655, 0.19728468707762659, 0.19620259501971304, 0.19558831211179495, 0.19749596994370222, 0.3138798519503325, 0.20739215100184083, 0.20567252696491778, 0.20536439679563046, 0.20646941591985524, 0.23159329197369516, 0.19505678582936525, 0.19602737994864583, 0.19393540709279478, 0.1949606561101973, 0.19730432983487844, 0.19429570087231696, 0.19450334715656936, 0.20531999599188566, 0.20646808808669448, 0.2065424770116806, 0.21327360696159303, 0.21123632695525885, 0.25290959794074297, 0.2143569909967482, 0.20159923797473311, 0.19935291889123619, 0.19827775191515684, 0.2002869830466807, 0.19832143117673695, 0.2075490769930184, 0.2110330311115831, 0.21207956410944462, 0.20896222512237728, 0.20969127397984266, 0.19685412803664804, 0.19850872596725821, 0.19599225721322, 0.19657022994942963, 0.19913004199042916, 0.33230101014487445, 0.19499460002407432, 0.1933408009354025, 0.21083254902623594, 0.2097802241332829, 0.19736794289201498, 0.19485756591893733, 0.19560595299117267, 0.19610633794218302, 0.1951172158587724, 0.19516876200214028, 0.19433401781134307, 0.19478288106620312, 0.1933889330830425, 0.19315946311689913, 0.1943474169820547, 0.1947162258438766, 0.19548895908519626, 0.1943126709666103, 0.19455293589271605, 0.19514246610924602, 0.19400010304525495, 0.19397365394979715, 0.19483441091142595, 0.1937441669870168, 0.194753997027874, 0.19366068998351693, 0.1938352209981531, 0.19326958106830716, 0.19378465111367404, 0.19304057396948338, 0.19335346296429634, 0.19346175086684525, 0.19347008201293647, 0.1931771789677441, 0.19321045000106096, 0.1938515231013298, 0.19347047409974039, 0.19302477105520666, 0.19428038992919028, 0.19999732007272542, 0.1944418391212821, 0.19514849199913442, 0.210928687825799, 0.2101908870972693, 0.19504844909533858, 0.19558135513216257, 0.19630465283989906, 0.20096231112256646, 0.19643819914199412, 0.19701800378970802, 0.19893453503027558, 0.19928394886665046, 0.1975720701739192]
[0.001526871721382982, 0.0015318447438812301, 0.0015381867437832808, 0.0015250426200904357, 0.001523292890583822, 0.0015258642956758885, 0.0015319510698751655, 0.0015416412953605024, 0.0015255946597416503, 0.001524279977000037, 0.001528748993350323, 0.0015369012172138968, 0.001527292101622321, 0.0015248285729235918, 0.0015239803095225446, 0.0015190658917607262, 0.001533106898946702, 0.0015398765040575996, 0.0015293064729361109, 0.0015217434874800749, 0.0015216628540769335, 0.0015257817202124946, 0.0015182686670026345, 0.0015243061551058939, 0.0015205568994951341, 0.0015254998525474654, 0.0015212196113122988, 0.0015293386595164851, 0.0015209503489900236, 0.0015161884659829067, 0.00153097651119149, 0.0024331771469018027, 0.0016076910930375259, 0.0015943606741466495, 0.0015919720681831818, 0.0016005381079058545, 0.0017952968370053889, 0.0015120681072043817, 0.0015195920926251614, 0.0015033752487813548, 0.0015113229155829249, 0.001529490928952546, 0.0015061682238164105, 0.0015077778849346463, 0.0015916278759060905, 0.0016005278146255385, 0.0016011044729587643, 0.00165328377489607, 0.0016374909066299136, 0.001960539518920488, 0.001661682100749986, 0.001562784790501807, 0.0015453714642731488, 0.0015370368365516035, 0.0015526122716796953, 0.001537375435478581, 0.00160890757358929, 0.0016359149698572334, 0.0016440276287553847, 0.0016198622102509867, 0.0016255137517817262, 0.0015260009925321552, 0.001538827333079521, 0.0015193198233582946, 0.0015238002321661212, 0.001543643736359916, 0.002575976822828484, 0.0015115860466982506, 0.0014987658987240505, 0.0016343608451646198, 0.0016262032878549063, 0.0015299840534264727, 0.0015105237668134677, 0.0015163252169858347, 0.001520204170094442, 0.0015125365570447472, 0.001512936139551475, 0.0015064652543514967, 0.0015099448144666907, 0.0014991390161476163, 0.0014973601792007685, 0.0015065691238918969, 0.0015094281073168728, 0.0015154182874821415, 0.0015062997749349636, 0.001508162293741985, 0.0015127322954205118, 0.001503876767792674, 0.0015036717360449392, 0.001510344270631209, 0.0015018927673412154, 0.0015097209071928217, 0.001501245658786953, 0.0015025986123887837, 0.0014982138067310632, 0.001502206597780419, 0.001496438557902972, 0.0014988640539867933, 0.0014997034950918237, 0.0014997680776196625, 0.0014974975113778613, 0.0014977554263648136, 0.0015027249852816265, 0.0014997711170522511, 0.0014963160546915245, 0.0015060495343348084, 0.0015503668222691892, 0.001507301078459551, 0.001512779007745228, 0.0016351061071767363, 0.0016293867216842582, 0.0015120034813592138, 0.001516134535908237, 0.0015217414948829385, 0.0015578473730431508, 0.001522776737534838, 0.001527271347207039, 0.001542128178529268, 0.001544836812919771, 0.001531566435456738]
[654.9338664116709, 652.8076712698071, 650.1161214927832, 655.7193791349251, 656.472570824339, 655.3662752538852, 652.7623627571129, 648.659323676301, 655.4821056920478, 656.0474552503919, 654.1296212457054, 650.6599050086028, 654.7535988287896, 655.8114254657985, 656.176456973578, 658.2992913104745, 652.2702367897731, 649.4027263647337, 653.8911707344722, 657.1409756160324, 657.1757977272941, 655.4017437440073, 658.6449564122269, 656.0361884325854, 657.6537848284579, 655.5228427784364, 657.3672812023095, 653.8774088900358, 657.4836585980884, 659.5486131413916, 653.1778852843046, 410.9852836951529, 622.0100393233058, 627.2106532828468, 628.1517245093612, 624.7898722688964, 557.0109518312477, 661.3458714163812, 658.0713369417819, 665.1699240163799, 661.6719628143103, 653.8123117113472, 663.9364608729734, 663.2276610446137, 628.2875634046775, 624.793890404186, 624.5688628625513, 604.8568401772785, 610.6904141886683, 510.0636790788179, 601.7998265424286, 639.8833710679396, 647.0936102539856, 650.6024945007404, 644.0758058147698, 650.4592026922185, 621.5397431246555, 611.2787146188107, 608.2622837409689, 617.3364584170754, 615.1901199875422, 655.3075685361512, 649.8454885115584, 658.1892664242388, 656.2540016012954, 647.8178717312788, 388.2022505551801, 661.5567814907361, 667.2156077552428, 611.859983649618, 614.9292695866338, 653.6015834677833, 662.0220230692263, 659.4891312220009, 657.8063786904856, 661.1410450494095, 660.966430675957, 663.8055521768274, 662.2758596334514, 667.0495459251877, 667.8419887817241, 663.7597864854122, 662.5025697829217, 659.8838144295421, 663.8784766751866, 663.0586138835528, 661.0554974117337, 664.9481004136777, 665.0387687875738, 662.100700777364, 665.8264969011664, 662.3740820145375, 666.1134999104856, 665.5137251925394, 667.4614767980876, 665.6873971113874, 668.2532969488209, 667.1719141840272, 666.7984726799427, 666.7697592197968, 667.7807424734153, 667.665749959651, 665.4577582687823, 666.7684079457844, 668.3080067640901, 663.9887846993557, 645.008642881272, 663.4374606976274, 661.0350850191155, 611.5811051104534, 613.7278441586438, 661.3741385707996, 659.5720737941981, 657.1418360888727, 641.9114075639944, 656.6950855966317, 654.7624964147506, 648.4545279197886, 647.3175623708636, 652.9262961431926]
Elapsed: 0.20095521646241346~0.017748959183224337
Time per graph: 0.0015577923756776237~0.00013758883087770804
Speed: 645.3438227194108~39.14829114880818
Total Time: 0.1982
best val loss: 0.11661124287187591 test_score: 0.9225

Testing...
Test loss: 0.2412 score: 0.9225 time: 0.33s
test Score 0.9225
Epoch Time List: [0.6910657770931721, 0.6965502959210426, 0.6987272738479078, 0.696099943947047, 0.6956968288868666, 0.6965404008515179, 0.6964079278986901, 0.6975351367145777, 0.6919458580669016, 0.6914222061168402, 0.6932701792102307, 0.694362583104521, 0.6941537840757519, 0.6910362427588552, 0.6916424320079386, 0.6933367447927594, 0.6954420146066695, 0.696660776855424, 0.6961952080018818, 0.6923121900763363, 0.6909396320115775, 0.6915816373657435, 0.6918948718812317, 0.6924216758925468, 0.6909223929978907, 0.6909104068763554, 0.6925587109290063, 0.6925647677853703, 0.6912493400741369, 0.6891216372605413, 0.690234798938036, 0.8143865743186325, 0.724562959279865, 0.7279491140507162, 0.8304462530650198, 0.7220640769228339, 0.7996774467173964, 0.6892585209570825, 0.6961425531189889, 0.7401868549641222, 0.6866613049060106, 0.6855595619417727, 0.7883581146597862, 0.6882852322887629, 0.8229785030707717, 0.7141559319570661, 0.7179254416842014, 0.7702913088724017, 0.7408669041469693, 0.7487489471677691, 0.7402280259411782, 0.7234912391286343, 0.7079399139620364, 0.7258016727864742, 0.7051553309429437, 0.7064094240777194, 0.7538657451514155, 0.7313188319094479, 0.7335588990245014, 0.8414137721993029, 0.7342691970989108, 0.8162669590674341, 0.6983038990292698, 0.694154622964561, 0.7340089979115874, 0.7020497412886471, 0.8352442760951817, 0.6899627540260553, 0.6966115459799767, 0.7945449610706419, 0.7352379520889372, 0.7135139249730855, 0.6873406998347491, 0.6899620769545436, 0.6935106709133834, 0.691165727796033, 0.6887273718602955, 0.6864758310839534, 0.688388881040737, 0.6839069481939077, 0.6834525701124221, 0.6850258919876069, 0.6856383848935366, 0.6846185373142362, 0.6845754778478295, 0.6860885978676379, 0.6843240591697395, 0.6855181627906859, 0.6846347060054541, 0.6852844702079892, 0.682615821948275, 0.6829596939496696, 0.6833172233309597, 0.6823781349230558, 0.6814464870840311, 0.6830546241253614, 0.6809213361702859, 0.6814947710372508, 0.6838376049418002, 0.682176681002602, 0.6854544510133564, 0.6823261608369648, 0.6847366776783019, 0.6836439149919897, 0.6825479799881577, 0.6817551527637988, 0.6858752050902694, 0.7890892741270363, 0.6880558598786592, 0.8476198760326952, 0.726896925130859, 0.720468383980915, 0.7644209319259971, 0.6961081330664456, 0.7002730220556259, 0.8118687961250544, 0.6960814821068197, 0.8093251148238778, 0.6999939540401101, 0.7033357950858772]
Total Epoch List: [51, 69]
Total Time List: [0.2147874350193888, 0.1982174590229988]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7b284070f730>
Training...
Epoch 1/1000, LR 0.001000
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.34s
Val loss: 0.7068 score: 0.4729 time: 0.19s
Test loss: 0.7118 score: 0.4531 time: 0.18s
Epoch 2/1000, LR 0.000067
Train loss: 0.7266;  Loss pred: 0.7266; Loss self: 0.0000; time: 0.34s
Val loss: 0.6959 score: 0.4884 time: 0.19s
Test loss: 0.7007 score: 0.4688 time: 0.18s
Epoch 3/1000, LR 0.000167
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.34s
Val loss: 0.6783 score: 0.5814 time: 0.19s
Test loss: 0.6826 score: 0.5781 time: 0.18s
Epoch 4/1000, LR 0.000267
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.48s
Val loss: 0.6555 score: 0.7984 time: 0.20s
Test loss: 0.6600 score: 0.7109 time: 0.19s
Epoch 5/1000, LR 0.000367
Train loss: 0.5751;  Loss pred: 0.5751; Loss self: 0.0000; time: 0.36s
Val loss: 0.6336 score: 0.7519 time: 0.20s
Test loss: 0.6377 score: 0.7344 time: 0.19s
Epoch 6/1000, LR 0.000467
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.35s
Val loss: 0.6058 score: 0.8682 time: 0.29s
Test loss: 0.6106 score: 0.8438 time: 0.18s
Epoch 7/1000, LR 0.000567
Train loss: 0.4902;  Loss pred: 0.4902; Loss self: 0.0000; time: 0.35s
Val loss: 0.5804 score: 0.9147 time: 0.20s
Test loss: 0.5859 score: 0.8750 time: 0.19s
Epoch 8/1000, LR 0.000667
Train loss: 0.4607;  Loss pred: 0.4607; Loss self: 0.0000; time: 0.35s
Val loss: 0.5739 score: 0.6357 time: 0.20s
Test loss: 0.5798 score: 0.6406 time: 0.19s
Epoch 9/1000, LR 0.000767
Train loss: 0.4261;  Loss pred: 0.4261; Loss self: 0.0000; time: 0.46s
Val loss: 0.5786 score: 0.5581 time: 0.20s
Test loss: 0.5837 score: 0.5625 time: 0.19s
     INFO: Early stopping counter 1 of 5
Epoch 10/1000, LR 0.000867
Train loss: 0.3887;  Loss pred: 0.3887; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6208 score: 0.5039 time: 0.19s
Test loss: 0.6242 score: 0.5156 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 11/1000, LR 0.000967
Train loss: 0.3850;  Loss pred: 0.3850; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6622 score: 0.5039 time: 0.32s
Test loss: 0.6667 score: 0.5078 time: 0.19s
     INFO: Early stopping counter 3 of 5
Epoch 12/1000, LR 0.000967
Train loss: 0.3694;  Loss pred: 0.3694; Loss self: 0.0000; time: 0.34s
Val loss: 0.6055 score: 0.5116 time: 0.19s
Test loss: 0.6100 score: 0.5234 time: 0.18s
     INFO: Early stopping counter 4 of 5
Epoch 13/1000, LR 0.000967
Train loss: 0.3237;  Loss pred: 0.3237; Loss self: 0.0000; time: 0.34s
Val loss: 0.5145 score: 0.6899 time: 0.19s
Test loss: 0.5190 score: 0.6719 time: 0.18s
Epoch 14/1000, LR 0.000967
Train loss: 0.3152;  Loss pred: 0.3152; Loss self: 0.0000; time: 0.35s
Val loss: 0.4501 score: 0.9690 time: 0.20s
Test loss: 0.4571 score: 0.9531 time: 0.18s
Epoch 15/1000, LR 0.000967
Train loss: 0.3121;  Loss pred: 0.3121; Loss self: 0.0000; time: 0.34s
Val loss: 0.4283 score: 0.9380 time: 0.19s
Test loss: 0.4373 score: 0.9297 time: 0.18s
Epoch 16/1000, LR 0.000967
Train loss: 0.3063;  Loss pred: 0.3063; Loss self: 0.0000; time: 0.35s
Val loss: 0.4149 score: 0.9612 time: 0.19s
Test loss: 0.4256 score: 0.9297 time: 0.18s
Epoch 17/1000, LR 0.000967
Train loss: 0.2814;  Loss pred: 0.2814; Loss self: 0.0000; time: 0.45s
Val loss: 0.4022 score: 0.9690 time: 0.19s
Test loss: 0.4149 score: 0.9297 time: 0.18s
Epoch 18/1000, LR 0.000967
Train loss: 0.2699;  Loss pred: 0.2699; Loss self: 0.0000; time: 0.34s
Val loss: 0.3989 score: 0.9690 time: 0.19s
Test loss: 0.4135 score: 0.9375 time: 0.18s
Epoch 19/1000, LR 0.000967
Train loss: 0.2573;  Loss pred: 0.2573; Loss self: 0.0000; time: 0.34s
Val loss: 0.4324 score: 0.8605 time: 0.19s
Test loss: 0.4453 score: 0.8203 time: 0.31s
     INFO: Early stopping counter 1 of 5
Epoch 20/1000, LR 0.000966
Train loss: 0.2390;  Loss pred: 0.2390; Loss self: 0.0000; time: 0.34s
Val loss: 0.4440 score: 0.8062 time: 0.19s
Test loss: 0.4554 score: 0.7734 time: 0.18s
     INFO: Early stopping counter 2 of 5
Epoch 21/1000, LR 0.000966
Train loss: 0.2336;  Loss pred: 0.2336; Loss self: 0.0000; time: 0.34s
Val loss: 0.4595 score: 0.7442 time: 0.19s
Test loss: 0.4691 score: 0.7109 time: 0.18s
     INFO: Early stopping counter 3 of 5
Epoch 22/1000, LR 0.000966
Train loss: 0.2291;  Loss pred: 0.2291; Loss self: 0.0000; time: 0.47s
Val loss: 0.4701 score: 0.7287 time: 0.20s
Test loss: 0.4785 score: 0.6797 time: 0.19s
     INFO: Early stopping counter 4 of 5
Epoch 23/1000, LR 0.000966
Train loss: 0.2277;  Loss pred: 0.2277; Loss self: 0.0000; time: 0.36s
Val loss: 0.4874 score: 0.6899 time: 0.20s
Test loss: 0.4945 score: 0.6719 time: 0.19s
     INFO: Early stopping counter 5 of 5
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 017,   Train_Loss: 0.2699,   Val_Loss: 0.3989,   Val_Precision: 0.9552,   Val_Recall: 0.9846,   Val_accuracy: 0.9697,   Val_Score: 0.9690,   Val_Loss: 0.3989,   Test_Precision: 0.9242,   Test_Recall: 0.9531,   Test_accuracy: 0.9385,   Test_Score: 0.9375,   Test_loss: 0.4135


[0.19696645205840468, 0.1976079719606787, 0.19842608994804323, 0.1967304979916662, 0.19650478288531303, 0.19683649414218962, 0.19762168801389635, 0.1988717271015048, 0.19680171110667288, 0.19663211703300476, 0.19720862014219165, 0.1982602570205927, 0.1970206811092794, 0.19670288590714335, 0.19659345992840827, 0.1959595000371337, 0.19777078996412456, 0.19864406902343035, 0.1972805350087583, 0.19630490988492966, 0.19629450817592442, 0.1968258419074118, 0.19585665804333985, 0.19663549400866032, 0.1961518400348723, 0.19678948097862303, 0.19623732985928655, 0.19728468707762659, 0.19620259501971304, 0.19558831211179495, 0.19749596994370222, 0.3138798519503325, 0.20739215100184083, 0.20567252696491778, 0.20536439679563046, 0.20646941591985524, 0.23159329197369516, 0.19505678582936525, 0.19602737994864583, 0.19393540709279478, 0.1949606561101973, 0.19730432983487844, 0.19429570087231696, 0.19450334715656936, 0.20531999599188566, 0.20646808808669448, 0.2065424770116806, 0.21327360696159303, 0.21123632695525885, 0.25290959794074297, 0.2143569909967482, 0.20159923797473311, 0.19935291889123619, 0.19827775191515684, 0.2002869830466807, 0.19832143117673695, 0.2075490769930184, 0.2110330311115831, 0.21207956410944462, 0.20896222512237728, 0.20969127397984266, 0.19685412803664804, 0.19850872596725821, 0.19599225721322, 0.19657022994942963, 0.19913004199042916, 0.33230101014487445, 0.19499460002407432, 0.1933408009354025, 0.21083254902623594, 0.2097802241332829, 0.19736794289201498, 0.19485756591893733, 0.19560595299117267, 0.19610633794218302, 0.1951172158587724, 0.19516876200214028, 0.19433401781134307, 0.19478288106620312, 0.1933889330830425, 0.19315946311689913, 0.1943474169820547, 0.1947162258438766, 0.19548895908519626, 0.1943126709666103, 0.19455293589271605, 0.19514246610924602, 0.19400010304525495, 0.19397365394979715, 0.19483441091142595, 0.1937441669870168, 0.194753997027874, 0.19366068998351693, 0.1938352209981531, 0.19326958106830716, 0.19378465111367404, 0.19304057396948338, 0.19335346296429634, 0.19346175086684525, 0.19347008201293647, 0.1931771789677441, 0.19321045000106096, 0.1938515231013298, 0.19347047409974039, 0.19302477105520666, 0.19428038992919028, 0.19999732007272542, 0.1944418391212821, 0.19514849199913442, 0.210928687825799, 0.2101908870972693, 0.19504844909533858, 0.19558135513216257, 0.19630465283989906, 0.20096231112256646, 0.19643819914199412, 0.19701800378970802, 0.19893453503027558, 0.19928394886665046, 0.1975720701739192, 0.18117942404933274, 0.18258240213617682, 0.18360239709727466, 0.1916567119769752, 0.19185520499013364, 0.18934335396625102, 0.19111935002729297, 0.19151342986151576, 0.1935959339607507, 0.18482958897948265, 0.1925231688655913, 0.18151483708061278, 0.18117874208837748, 0.18329156399704516, 0.18409360712394118, 0.1834336610045284, 0.1824586819857359, 0.18472797702997923, 0.31220003799535334, 0.18183148303069174, 0.18198359292000532, 0.19368977914564312, 0.19400534499436617]
[0.001526871721382982, 0.0015318447438812301, 0.0015381867437832808, 0.0015250426200904357, 0.001523292890583822, 0.0015258642956758885, 0.0015319510698751655, 0.0015416412953605024, 0.0015255946597416503, 0.001524279977000037, 0.001528748993350323, 0.0015369012172138968, 0.001527292101622321, 0.0015248285729235918, 0.0015239803095225446, 0.0015190658917607262, 0.001533106898946702, 0.0015398765040575996, 0.0015293064729361109, 0.0015217434874800749, 0.0015216628540769335, 0.0015257817202124946, 0.0015182686670026345, 0.0015243061551058939, 0.0015205568994951341, 0.0015254998525474654, 0.0015212196113122988, 0.0015293386595164851, 0.0015209503489900236, 0.0015161884659829067, 0.00153097651119149, 0.0024331771469018027, 0.0016076910930375259, 0.0015943606741466495, 0.0015919720681831818, 0.0016005381079058545, 0.0017952968370053889, 0.0015120681072043817, 0.0015195920926251614, 0.0015033752487813548, 0.0015113229155829249, 0.001529490928952546, 0.0015061682238164105, 0.0015077778849346463, 0.0015916278759060905, 0.0016005278146255385, 0.0016011044729587643, 0.00165328377489607, 0.0016374909066299136, 0.001960539518920488, 0.001661682100749986, 0.001562784790501807, 0.0015453714642731488, 0.0015370368365516035, 0.0015526122716796953, 0.001537375435478581, 0.00160890757358929, 0.0016359149698572334, 0.0016440276287553847, 0.0016198622102509867, 0.0016255137517817262, 0.0015260009925321552, 0.001538827333079521, 0.0015193198233582946, 0.0015238002321661212, 0.001543643736359916, 0.002575976822828484, 0.0015115860466982506, 0.0014987658987240505, 0.0016343608451646198, 0.0016262032878549063, 0.0015299840534264727, 0.0015105237668134677, 0.0015163252169858347, 0.001520204170094442, 0.0015125365570447472, 0.001512936139551475, 0.0015064652543514967, 0.0015099448144666907, 0.0014991390161476163, 0.0014973601792007685, 0.0015065691238918969, 0.0015094281073168728, 0.0015154182874821415, 0.0015062997749349636, 0.001508162293741985, 0.0015127322954205118, 0.001503876767792674, 0.0015036717360449392, 0.001510344270631209, 0.0015018927673412154, 0.0015097209071928217, 0.001501245658786953, 0.0015025986123887837, 0.0014982138067310632, 0.001502206597780419, 0.001496438557902972, 0.0014988640539867933, 0.0014997034950918237, 0.0014997680776196625, 0.0014974975113778613, 0.0014977554263648136, 0.0015027249852816265, 0.0014997711170522511, 0.0014963160546915245, 0.0015060495343348084, 0.0015503668222691892, 0.001507301078459551, 0.001512779007745228, 0.0016351061071767363, 0.0016293867216842582, 0.0015120034813592138, 0.001516134535908237, 0.0015217414948829385, 0.0015578473730431508, 0.001522776737534838, 0.001527271347207039, 0.001542128178529268, 0.001544836812919771, 0.001531566435456738, 0.001415464250385412, 0.0014264250166888814, 0.0014343937273224583, 0.0014973180623201188, 0.001498868788985419, 0.001479244952861336, 0.0014931199220882263, 0.0014961986707930919, 0.0015124682340683648, 0.0014439811639022082, 0.001504087256762432, 0.0014180846646922873, 0.001415458922565449, 0.0014319653437269153, 0.0014382313056557905, 0.0014330754765978782, 0.0014254584530135617, 0.0014431873205467127, 0.002439062796838698, 0.0014205584611772792, 0.0014217468196875416, 0.0015132013995753368, 0.0015156667577684857]
[654.9338664116709, 652.8076712698071, 650.1161214927832, 655.7193791349251, 656.472570824339, 655.3662752538852, 652.7623627571129, 648.659323676301, 655.4821056920478, 656.0474552503919, 654.1296212457054, 650.6599050086028, 654.7535988287896, 655.8114254657985, 656.176456973578, 658.2992913104745, 652.2702367897731, 649.4027263647337, 653.8911707344722, 657.1409756160324, 657.1757977272941, 655.4017437440073, 658.6449564122269, 656.0361884325854, 657.6537848284579, 655.5228427784364, 657.3672812023095, 653.8774088900358, 657.4836585980884, 659.5486131413916, 653.1778852843046, 410.9852836951529, 622.0100393233058, 627.2106532828468, 628.1517245093612, 624.7898722688964, 557.0109518312477, 661.3458714163812, 658.0713369417819, 665.1699240163799, 661.6719628143103, 653.8123117113472, 663.9364608729734, 663.2276610446137, 628.2875634046775, 624.793890404186, 624.5688628625513, 604.8568401772785, 610.6904141886683, 510.0636790788179, 601.7998265424286, 639.8833710679396, 647.0936102539856, 650.6024945007404, 644.0758058147698, 650.4592026922185, 621.5397431246555, 611.2787146188107, 608.2622837409689, 617.3364584170754, 615.1901199875422, 655.3075685361512, 649.8454885115584, 658.1892664242388, 656.2540016012954, 647.8178717312788, 388.2022505551801, 661.5567814907361, 667.2156077552428, 611.859983649618, 614.9292695866338, 653.6015834677833, 662.0220230692263, 659.4891312220009, 657.8063786904856, 661.1410450494095, 660.966430675957, 663.8055521768274, 662.2758596334514, 667.0495459251877, 667.8419887817241, 663.7597864854122, 662.5025697829217, 659.8838144295421, 663.8784766751866, 663.0586138835528, 661.0554974117337, 664.9481004136777, 665.0387687875738, 662.100700777364, 665.8264969011664, 662.3740820145375, 666.1134999104856, 665.5137251925394, 667.4614767980876, 665.6873971113874, 668.2532969488209, 667.1719141840272, 666.7984726799427, 666.7697592197968, 667.7807424734153, 667.665749959651, 665.4577582687823, 666.7684079457844, 668.3080067640901, 663.9887846993557, 645.008642881272, 663.4374606976274, 661.0350850191155, 611.5811051104534, 613.7278441586438, 661.3741385707996, 659.5720737941981, 657.1418360888727, 641.9114075639944, 656.6950855966317, 654.7624964147506, 648.4545279197886, 647.3175623708636, 652.9262961431926, 706.4819897271962, 701.0533244300991, 697.1586538283818, 667.8607739831066, 667.1698065558479, 676.020559046477, 669.7385690236015, 668.360438704259, 661.1709108826144, 692.529809251531, 664.8550444822686, 705.1765137147095, 706.4846489416659, 698.340923110152, 695.2984516937836, 697.7999528496576, 701.5286891637564, 692.9107439921083, 409.9935439530763, 703.9485014726223, 703.3601103604162, 660.8505650871317, 659.7756366130895]
Elapsed: 0.19953032342515156~0.019593371544791086
Time per graph: 0.0015486178520932778~0.0001515745240774352
Speed: 649.9938943160619~44.22478697084863
Total Time: 0.1946
best val loss: 0.3989187077034351 test_score: 0.9375

Testing...
Test loss: 0.4571 score: 0.9531 time: 0.19s
test Score 0.9531
Epoch Time List: [0.6910657770931721, 0.6965502959210426, 0.6987272738479078, 0.696099943947047, 0.6956968288868666, 0.6965404008515179, 0.6964079278986901, 0.6975351367145777, 0.6919458580669016, 0.6914222061168402, 0.6932701792102307, 0.694362583104521, 0.6941537840757519, 0.6910362427588552, 0.6916424320079386, 0.6933367447927594, 0.6954420146066695, 0.696660776855424, 0.6961952080018818, 0.6923121900763363, 0.6909396320115775, 0.6915816373657435, 0.6918948718812317, 0.6924216758925468, 0.6909223929978907, 0.6909104068763554, 0.6925587109290063, 0.6925647677853703, 0.6912493400741369, 0.6891216372605413, 0.690234798938036, 0.8143865743186325, 0.724562959279865, 0.7279491140507162, 0.8304462530650198, 0.7220640769228339, 0.7996774467173964, 0.6892585209570825, 0.6961425531189889, 0.7401868549641222, 0.6866613049060106, 0.6855595619417727, 0.7883581146597862, 0.6882852322887629, 0.8229785030707717, 0.7141559319570661, 0.7179254416842014, 0.7702913088724017, 0.7408669041469693, 0.7487489471677691, 0.7402280259411782, 0.7234912391286343, 0.7079399139620364, 0.7258016727864742, 0.7051553309429437, 0.7064094240777194, 0.7538657451514155, 0.7313188319094479, 0.7335588990245014, 0.8414137721993029, 0.7342691970989108, 0.8162669590674341, 0.6983038990292698, 0.694154622964561, 0.7340089979115874, 0.7020497412886471, 0.8352442760951817, 0.6899627540260553, 0.6966115459799767, 0.7945449610706419, 0.7352379520889372, 0.7135139249730855, 0.6873406998347491, 0.6899620769545436, 0.6935106709133834, 0.691165727796033, 0.6887273718602955, 0.6864758310839534, 0.688388881040737, 0.6839069481939077, 0.6834525701124221, 0.6850258919876069, 0.6856383848935366, 0.6846185373142362, 0.6845754778478295, 0.6860885978676379, 0.6843240591697395, 0.6855181627906859, 0.6846347060054541, 0.6852844702079892, 0.682615821948275, 0.6829596939496696, 0.6833172233309597, 0.6823781349230558, 0.6814464870840311, 0.6830546241253614, 0.6809213361702859, 0.6814947710372508, 0.6838376049418002, 0.682176681002602, 0.6854544510133564, 0.6823261608369648, 0.6847366776783019, 0.6836439149919897, 0.6825479799881577, 0.6817551527637988, 0.6858752050902694, 0.7890892741270363, 0.6880558598786592, 0.8476198760326952, 0.726896925130859, 0.720468383980915, 0.7644209319259971, 0.6961081330664456, 0.7002730220556259, 0.8118687961250544, 0.6960814821068197, 0.8093251148238778, 0.6999939540401101, 0.7033357950858772, 0.708278558915481, 0.708051250083372, 0.7095125131309032, 0.8647803522180766, 0.7440480259247124, 0.8250446517486125, 0.7316177869215608, 0.7392104112077504, 0.845133766066283, 0.7097242558375001, 0.8475021393969655, 0.7082186967600137, 0.7089118950534612, 0.7215016481932253, 0.713504008250311, 0.7163565780501813, 0.819916311185807, 0.7133703453000635, 0.8412407180294394, 0.7066227549221367, 0.7068091472610831, 0.8594427409116179, 0.7461679850239307]
Total Epoch List: [51, 69, 23]
Total Time List: [0.2147874350193888, 0.1982174590229988, 0.19461859297007322]
T-times Epoch Time: 0.7181456956214293 ~ 0.012650830959311854
T-times Total Epoch: 42.333333333333336 ~ 3.7712361663282534
T-times Total Time: 0.22470560333588055 ~ 0.01577066991924529
T-times Inference Elapsed: 0.1977673829923742 ~ 0.0034241127266946376
T-times Time Per Graph: 0.0015354450885732482 ~ 2.5583388475525605e-05
T-times Speed: 658.5327478037271 ~ 12.926694628354783
T-times cross validation test micro f1 score:0.8991268990429359 ~ 0.003243111271233173
T-times cross validation test precision:0.9504649149190153 ~ 0.02163655178739268
T-times cross validation test recall:0.8569978632478633 ~ 0.028150315612847605
T-times cross validation test f1_score:0.8991268990429359 ~ 0.006354631704405511
