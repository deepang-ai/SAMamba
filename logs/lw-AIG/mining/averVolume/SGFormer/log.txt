Namespace(seed=60, model='SGFormer', dataset='mining/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=32, abs_pe='lap', abs_pe_dim=20, num_class=2, outdir='./outdir/mining/averVolume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 390], edge_attr=[390, 2], x=[103, 14887], y=[1, 1], num_nodes=115)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049f550>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7924;  Loss pred: 0.7924; Loss self: 0.0000; time: 0.65s
Val loss: 0.6986 score: 0.3953 time: 0.33s
Test loss: 0.6940 score: 0.5000 time: 0.12s
Epoch 2/1000, LR 0.000015
Train loss: 0.7577;  Loss pred: 0.7577; Loss self: 0.0000; time: 0.26s
Val loss: 0.6979 score: 0.4419 time: 0.10s
Test loss: 0.6930 score: 0.4545 time: 0.10s
Epoch 3/1000, LR 0.000045
Train loss: 0.7993;  Loss pred: 0.7993; Loss self: 0.0000; time: 0.27s
Val loss: 0.6969 score: 0.4884 time: 0.16s
Test loss: 0.6915 score: 0.5909 time: 0.08s
Epoch 4/1000, LR 0.000075
Train loss: 0.7454;  Loss pred: 0.7454; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4884 time: 0.31s
Test loss: 0.6902 score: 0.5000 time: 0.13s
Epoch 5/1000, LR 0.000105
Train loss: 0.7272;  Loss pred: 0.7272; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5000 time: 0.40s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.7394;  Loss pred: 0.7394; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4884 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.7113;  Loss pred: 0.7113; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7013 score: 0.4884 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7073 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7160 score: 0.4884 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7094 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6039;  Loss pred: 0.6039; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7260 score: 0.4884 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7201 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7347 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7297 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7419 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7379 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.6039;  Loss pred: 0.6039; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7478 score: 0.4884 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7448 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5754;  Loss pred: 0.5754; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7481 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7460 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 10 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7490 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7477 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7478 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7474 score: 0.5000 time: 0.45s
     INFO: Early stopping counter 12 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5455;  Loss pred: 0.5455; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7471 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7475 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 13 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7441 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7456 score: 0.5000 time: 0.15s
     INFO: Early stopping counter 14 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5668;  Loss pred: 0.5668; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7403 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7429 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 15 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5558;  Loss pred: 0.5558; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7337 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7373 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 16 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7268 score: 0.4884 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7316 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 17 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7183 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7241 score: 0.5000 time: 0.16s
     INFO: Early stopping counter 18 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5412;  Loss pred: 0.5412; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7101 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7170 score: 0.5000 time: 0.32s
     INFO: Early stopping counter 19 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7027 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7107 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 003,   Train_Loss: 0.7454,   Val_Loss: 0.6961,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6961,   Test_Precision: 0.5000,   Test_Recall: 0.0455,   Test_accuracy: 0.0833,   Test_Score: 0.5000,   Test_loss: 0.6902


[0.12573948699991888, 0.10512629400000151, 0.08509011099977215, 0.1315615120001894, 0.4032009870006732, 0.23139045400057512, 0.19763101799981087, 0.10254254400024365, 0.11870687899954646, 0.08311424499970599, 0.12155232500026614, 0.11163750499963498, 0.11703549899993959, 0.12614515499990375, 0.12146880999989662, 0.45555025999965437, 0.1317049009994662, 0.15253168399976857, 0.11699377300010383, 0.11372926500007452, 0.13993537399983325, 0.16628425300041272, 0.3247410249996392, 0.17735963299946889]
[0.00285771561363452, 0.002389233954545489, 0.0019338661590857307, 0.0029900343636406683, 0.009163658795469846, 0.005258873954558526, 0.0044916140454502465, 0.002330512363641901, 0.002697883613626056, 0.0018889601136296815, 0.0027625528409151393, 0.002537216022718977, 0.0026598977045440815, 0.0028669353409069035, 0.002760654772724923, 0.010353414999992145, 0.0029932932045333223, 0.003466629181812922, 0.002658949386365996, 0.0025847560227289664, 0.003180349409087119, 0.0037791875681911983, 0.0073804778409008905, 0.004030900749987929]
[349.929851392096, 418.54419409096045, 517.0988671071051, 334.4443168146065, 109.12671699369263, 190.15477622033032, 222.6371166091049, 429.09019304119715, 370.66091174183856, 529.3918028149765, 361.984026220014, 394.1327782284624, 375.9543076756798, 348.80451809689856, 362.2329057149522, 96.58648861276774, 334.0802025292767, 288.46465761216376, 376.088392328034, 386.8837101863902, 314.4308600629633, 264.60713631068114, 135.49258212770894, 248.08350838283343]
Elapsed: 0.16503220804160415~0.09463713856808167
Time per graph: 0.003750732000945549~0.0021508440583654924
Speed: 323.28770087144727~110.40936515549365
Total Time: 0.1777
best val loss: 0.696050362531529 test_score: 0.5000

Testing...
Test loss: 0.6915 score: 0.5909 time: 0.41s
test Score 0.5909
Epoch Time List: [1.0991551069992056, 0.45861375000004045, 0.5094064380000418, 0.8094554159997642, 0.82464784900003, 0.7140949290005665, 0.9736223429999882, 0.5261049140008254, 0.6817055850006, 0.5548772789998111, 0.5465628109996032, 0.5299621449994447, 1.173014386999057, 0.7663024109997423, 0.5394408260008277, 0.9711991019994457, 0.7058600190002835, 0.5860561529989354, 0.5183480380001129, 0.6396399600007499, 0.7342172420003408, 0.6822621069995876, 0.7714736440002525, 0.6893293530010851]
Total Epoch List: [24]
Total Time List: [0.17772020600023097]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049f8b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7006;  Loss pred: 0.7006; Loss self: 0.0000; time: 0.39s
Val loss: 0.6941 score: 0.3864 time: 0.10s
Test loss: 0.6981 score: 0.3721 time: 0.22s
Epoch 2/1000, LR 0.000015
Train loss: 0.7188;  Loss pred: 0.7188; Loss self: 0.0000; time: 0.37s
Val loss: 0.6936 score: 0.5227 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4884 time: 0.12s
Epoch 3/1000, LR 0.000045
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.38s
Val loss: 0.6931 score: 0.5227 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4884 time: 0.10s
Epoch 4/1000, LR 0.000075
Train loss: 0.6579;  Loss pred: 0.6579; Loss self: 0.0000; time: 0.37s
Val loss: 0.6924 score: 0.5227 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.4884 time: 0.34s
Epoch 5/1000, LR 0.000105
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.4884 time: 0.15s
Epoch 6/1000, LR 0.000135
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.4884 time: 0.14s
Epoch 7/1000, LR 0.000165
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6989 score: 0.4884 time: 0.16s
Epoch 8/1000, LR 0.000195
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.4884 time: 0.18s
Epoch 9/1000, LR 0.000225
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6995 score: 0.4884 time: 0.16s
Epoch 10/1000, LR 0.000255
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6997 score: 0.4884 time: 0.15s
Epoch 11/1000, LR 0.000285
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.4884 time: 0.13s
Epoch 12/1000, LR 0.000285
Train loss: 0.5732;  Loss pred: 0.5732; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.4884 time: 0.11s
Epoch 13/1000, LR 0.000285
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4884 time: 0.13s
Epoch 14/1000, LR 0.000285
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.34s
Epoch 15/1000, LR 0.000285
Train loss: 0.5471;  Loss pred: 0.5471; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6775 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.10s
Epoch 16/1000, LR 0.000285
Train loss: 0.5322;  Loss pred: 0.5322; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.4884 time: 0.11s
Epoch 17/1000, LR 0.000285
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6671 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.4884 time: 0.08s
Epoch 18/1000, LR 0.000285
Train loss: 0.4917;  Loss pred: 0.4917; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6623 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6757 score: 0.4884 time: 0.13s
Epoch 19/1000, LR 0.000285
Train loss: 0.4821;  Loss pred: 0.4821; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6552 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6686 score: 0.4884 time: 0.37s
Epoch 20/1000, LR 0.000285
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 0.34s
Val loss: 0.6478 score: 0.5227 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6613 score: 0.4884 time: 0.12s
Epoch 21/1000, LR 0.000285
Train loss: 0.4675;  Loss pred: 0.4675; Loss self: 0.0000; time: 0.35s
Val loss: 0.6395 score: 0.5227 time: 0.21s
Test loss: 0.6528 score: 0.5116 time: 0.12s
Epoch 22/1000, LR 0.000285
Train loss: 0.4698;  Loss pred: 0.4698; Loss self: 0.0000; time: 0.33s
Val loss: 0.6319 score: 0.5909 time: 0.14s
Test loss: 0.6449 score: 0.5349 time: 0.12s
Epoch 23/1000, LR 0.000285
Train loss: 0.4583;  Loss pred: 0.4583; Loss self: 0.0000; time: 0.63s
Val loss: 0.6240 score: 0.6136 time: 0.11s
Test loss: 0.6366 score: 0.5349 time: 0.07s
Epoch 24/1000, LR 0.000285
Train loss: 0.4511;  Loss pred: 0.4511; Loss self: 0.0000; time: 0.34s
Val loss: 0.6155 score: 0.6364 time: 0.12s
Test loss: 0.6279 score: 0.5581 time: 0.10s
Epoch 25/1000, LR 0.000285
Train loss: 0.4381;  Loss pred: 0.4381; Loss self: 0.0000; time: 0.29s
Val loss: 0.6085 score: 0.6364 time: 0.10s
Test loss: 0.6202 score: 0.6047 time: 0.13s
Epoch 26/1000, LR 0.000285
Train loss: 0.4300;  Loss pred: 0.4300; Loss self: 0.0000; time: 0.28s
Val loss: 0.6024 score: 0.6591 time: 0.10s
Test loss: 0.6122 score: 0.6744 time: 0.37s
Epoch 27/1000, LR 0.000285
Train loss: 0.4174;  Loss pred: 0.4174; Loss self: 0.0000; time: 0.34s
Val loss: 0.5964 score: 0.6591 time: 0.10s
Test loss: 0.6046 score: 0.6744 time: 0.15s
Epoch 28/1000, LR 0.000285
Train loss: 0.4262;  Loss pred: 0.4262; Loss self: 0.0000; time: 0.35s
Val loss: 0.5912 score: 0.7045 time: 0.12s
Test loss: 0.5981 score: 0.6744 time: 0.14s
Epoch 29/1000, LR 0.000285
Train loss: 0.3927;  Loss pred: 0.3927; Loss self: 0.0000; time: 0.35s
Val loss: 0.5856 score: 0.7273 time: 0.13s
Test loss: 0.5917 score: 0.6744 time: 0.22s
Epoch 30/1000, LR 0.000285
Train loss: 0.3861;  Loss pred: 0.3861; Loss self: 0.0000; time: 0.43s
Val loss: 0.5803 score: 0.7500 time: 0.38s
Test loss: 0.5855 score: 0.6744 time: 0.14s
Epoch 31/1000, LR 0.000285
Train loss: 0.3989;  Loss pred: 0.3989; Loss self: 0.0000; time: 0.38s
Val loss: 0.5742 score: 0.7955 time: 0.14s
Test loss: 0.5784 score: 0.6744 time: 0.12s
Epoch 32/1000, LR 0.000285
Train loss: 0.3875;  Loss pred: 0.3875; Loss self: 0.0000; time: 0.39s
Val loss: 0.5696 score: 0.7955 time: 0.22s
Test loss: 0.5731 score: 0.6977 time: 0.12s
Epoch 33/1000, LR 0.000285
Train loss: 0.3801;  Loss pred: 0.3801; Loss self: 0.0000; time: 0.30s
Val loss: 0.5653 score: 0.7955 time: 0.09s
Test loss: 0.5685 score: 0.7209 time: 0.12s
Epoch 34/1000, LR 0.000285
Train loss: 0.3825;  Loss pred: 0.3825; Loss self: 0.0000; time: 0.49s
Val loss: 0.5602 score: 0.8182 time: 0.09s
Test loss: 0.5630 score: 0.7442 time: 0.10s
Epoch 35/1000, LR 0.000285
Train loss: 0.3445;  Loss pred: 0.3445; Loss self: 0.0000; time: 0.28s
Val loss: 0.5554 score: 0.8182 time: 0.10s
Test loss: 0.5579 score: 0.7674 time: 0.08s
Epoch 36/1000, LR 0.000285
Train loss: 0.3388;  Loss pred: 0.3388; Loss self: 0.0000; time: 0.31s
Val loss: 0.5511 score: 0.8182 time: 0.11s
Test loss: 0.5530 score: 0.7674 time: 0.13s
Epoch 37/1000, LR 0.000285
Train loss: 0.3589;  Loss pred: 0.3589; Loss self: 0.0000; time: 0.40s
Val loss: 0.5477 score: 0.8182 time: 0.11s
Test loss: 0.5488 score: 0.7674 time: 0.22s
Epoch 38/1000, LR 0.000284
Train loss: 0.3411;  Loss pred: 0.3411; Loss self: 0.0000; time: 0.56s
Val loss: 0.5432 score: 0.8182 time: 0.13s
Test loss: 0.5440 score: 0.7674 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.3373;  Loss pred: 0.3373; Loss self: 0.0000; time: 0.33s
Val loss: 0.5387 score: 0.8182 time: 0.14s
Test loss: 0.5388 score: 0.7907 time: 0.13s
Epoch 40/1000, LR 0.000284
Train loss: 0.3297;  Loss pred: 0.3297; Loss self: 0.0000; time: 0.36s
Val loss: 0.5327 score: 0.8409 time: 0.20s
Test loss: 0.5318 score: 0.7907 time: 0.10s
Epoch 41/1000, LR 0.000284
Train loss: 0.3125;  Loss pred: 0.3125; Loss self: 0.0000; time: 0.41s
Val loss: 0.5270 score: 0.8409 time: 0.37s
Test loss: 0.5252 score: 0.7907 time: 0.09s
Epoch 42/1000, LR 0.000284
Train loss: 0.3083;  Loss pred: 0.3083; Loss self: 0.0000; time: 0.29s
Val loss: 0.5215 score: 0.8409 time: 0.14s
Test loss: 0.5189 score: 0.7907 time: 0.12s
Epoch 43/1000, LR 0.000284
Train loss: 0.3174;  Loss pred: 0.3174; Loss self: 0.0000; time: 0.25s
Val loss: 0.5155 score: 0.8409 time: 0.09s
Test loss: 0.5124 score: 0.7907 time: 0.30s
Epoch 44/1000, LR 0.000284
Train loss: 0.2957;  Loss pred: 0.2957; Loss self: 0.0000; time: 0.35s
Val loss: 0.5099 score: 0.8409 time: 0.20s
Test loss: 0.5062 score: 0.7907 time: 0.10s
Epoch 45/1000, LR 0.000284
Train loss: 0.2937;  Loss pred: 0.2937; Loss self: 0.0000; time: 0.34s
Val loss: 0.5036 score: 0.8409 time: 0.12s
Test loss: 0.4991 score: 0.8140 time: 0.10s
Epoch 46/1000, LR 0.000284
Train loss: 0.2975;  Loss pred: 0.2975; Loss self: 0.0000; time: 0.64s
Val loss: 0.4972 score: 0.8409 time: 0.12s
Test loss: 0.4919 score: 0.8372 time: 0.15s
Epoch 47/1000, LR 0.000284
Train loss: 0.3179;  Loss pred: 0.3179; Loss self: 0.0000; time: 0.59s
Val loss: 0.4910 score: 0.8409 time: 0.15s
Test loss: 0.4844 score: 0.8372 time: 0.12s
Epoch 48/1000, LR 0.000284
Train loss: 0.2963;  Loss pred: 0.2963; Loss self: 0.0000; time: 0.31s
Val loss: 0.4867 score: 0.8409 time: 0.13s
Test loss: 0.4782 score: 0.8372 time: 0.13s
Epoch 49/1000, LR 0.000284
Train loss: 0.2829;  Loss pred: 0.2829; Loss self: 0.0000; time: 0.44s
Val loss: 0.4830 score: 0.8409 time: 0.57s
Test loss: 0.4722 score: 0.8372 time: 0.23s
Epoch 50/1000, LR 0.000284
Train loss: 0.2624;  Loss pred: 0.2624; Loss self: 0.0000; time: 0.40s
Val loss: 0.4787 score: 0.8409 time: 0.19s
Test loss: 0.4657 score: 0.8372 time: 0.14s
Epoch 51/1000, LR 0.000284
Train loss: 0.2665;  Loss pred: 0.2665; Loss self: 0.0000; time: 0.28s
Val loss: 0.4746 score: 0.8409 time: 0.11s
Test loss: 0.4588 score: 0.8372 time: 0.11s
Epoch 52/1000, LR 0.000284
Train loss: 0.2663;  Loss pred: 0.2663; Loss self: 0.0000; time: 0.33s
Val loss: 0.4698 score: 0.8409 time: 0.30s
Test loss: 0.4512 score: 0.8372 time: 0.27s
Epoch 53/1000, LR 0.000284
Train loss: 0.2696;  Loss pred: 0.2696; Loss self: 0.0000; time: 0.42s
Val loss: 0.4646 score: 0.8409 time: 0.28s
Test loss: 0.4430 score: 0.8372 time: 0.12s
Epoch 54/1000, LR 0.000284
Train loss: 0.2528;  Loss pred: 0.2528; Loss self: 0.0000; time: 0.31s
Val loss: 0.4595 score: 0.8409 time: 0.12s
Test loss: 0.4350 score: 0.8372 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 0.2495;  Loss pred: 0.2495; Loss self: 0.0000; time: 0.36s
Val loss: 0.4565 score: 0.8409 time: 0.10s
Test loss: 0.4290 score: 0.8372 time: 0.13s
Epoch 56/1000, LR 0.000284
Train loss: 0.2432;  Loss pred: 0.2432; Loss self: 0.0000; time: 0.76s
Val loss: 0.4551 score: 0.8409 time: 0.10s
Test loss: 0.4255 score: 0.8372 time: 0.13s
Epoch 57/1000, LR 0.000283
Train loss: 0.2541;  Loss pred: 0.2541; Loss self: 0.0000; time: 0.32s
Val loss: 0.4541 score: 0.8409 time: 0.09s
Test loss: 0.4219 score: 0.8372 time: 0.14s
Epoch 58/1000, LR 0.000283
Train loss: 0.2572;  Loss pred: 0.2572; Loss self: 0.0000; time: 0.28s
Val loss: 0.4531 score: 0.8409 time: 0.13s
Test loss: 0.4187 score: 0.8372 time: 0.12s
Epoch 59/1000, LR 0.000283
Train loss: 0.2574;  Loss pred: 0.2574; Loss self: 0.0000; time: 0.43s
Val loss: 0.4528 score: 0.8409 time: 0.11s
Test loss: 0.4173 score: 0.8372 time: 0.12s
Epoch 60/1000, LR 0.000283
Train loss: 0.2340;  Loss pred: 0.2340; Loss self: 0.0000; time: 0.23s
Val loss: 0.4520 score: 0.8409 time: 0.38s
Test loss: 0.4158 score: 0.8372 time: 0.10s
Epoch 61/1000, LR 0.000283
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.30s
Val loss: 0.4534 score: 0.8409 time: 0.13s
Test loss: 0.4159 score: 0.8372 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.2418;  Loss pred: 0.2418; Loss self: 0.0000; time: 0.31s
Val loss: 0.4534 score: 0.8409 time: 0.19s
Test loss: 0.4144 score: 0.8372 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.2368;  Loss pred: 0.2368; Loss self: 0.0000; time: 0.58s
Val loss: 0.4531 score: 0.8409 time: 0.15s
Test loss: 0.4135 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.2421;  Loss pred: 0.2421; Loss self: 0.0000; time: 0.65s
Val loss: 0.4547 score: 0.8409 time: 0.12s
Test loss: 0.4148 score: 0.8140 time: 0.24s
     INFO: Early stopping counter 4 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.2393;  Loss pred: 0.2393; Loss self: 0.0000; time: 0.58s
Val loss: 0.4536 score: 0.8409 time: 0.16s
Test loss: 0.4124 score: 0.8140 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.2182;  Loss pred: 0.2182; Loss self: 0.0000; time: 0.41s
Val loss: 0.4496 score: 0.8409 time: 0.13s
Test loss: 0.4070 score: 0.8140 time: 0.15s
Epoch 67/1000, LR 0.000283
Train loss: 0.2209;  Loss pred: 0.2209; Loss self: 0.0000; time: 0.34s
Val loss: 0.4444 score: 0.8409 time: 0.12s
Test loss: 0.3993 score: 0.8372 time: 0.20s
Epoch 68/1000, LR 0.000283
Train loss: 0.2231;  Loss pred: 0.2231; Loss self: 0.0000; time: 0.61s
Val loss: 0.4426 score: 0.8409 time: 0.10s
Test loss: 0.3954 score: 0.8372 time: 0.10s
Epoch 69/1000, LR 0.000283
Train loss: 0.2166;  Loss pred: 0.2166; Loss self: 0.0000; time: 0.29s
Val loss: 0.4397 score: 0.8409 time: 0.10s
Test loss: 0.3914 score: 0.8372 time: 0.11s
Epoch 70/1000, LR 0.000283
Train loss: 0.2078;  Loss pred: 0.2078; Loss self: 0.0000; time: 0.26s
Val loss: 0.4369 score: 0.8409 time: 0.08s
Test loss: 0.3885 score: 0.8837 time: 0.17s
Epoch 71/1000, LR 0.000282
Train loss: 0.2124;  Loss pred: 0.2124; Loss self: 0.0000; time: 0.31s
Val loss: 0.4329 score: 0.8182 time: 0.55s
Test loss: 0.3849 score: 0.8837 time: 0.13s
Epoch 72/1000, LR 0.000282
Train loss: 0.2065;  Loss pred: 0.2065; Loss self: 0.0000; time: 0.36s
Val loss: 0.4302 score: 0.8182 time: 0.14s
Test loss: 0.3824 score: 0.8837 time: 0.18s
Epoch 73/1000, LR 0.000282
Train loss: 0.1983;  Loss pred: 0.1983; Loss self: 0.0000; time: 0.34s
Val loss: 0.4276 score: 0.8182 time: 0.19s
Test loss: 0.3800 score: 0.8837 time: 0.13s
Epoch 74/1000, LR 0.000282
Train loss: 0.2015;  Loss pred: 0.2015; Loss self: 0.0000; time: 0.51s
Val loss: 0.4252 score: 0.8182 time: 0.27s
Test loss: 0.3774 score: 0.8837 time: 0.11s
Epoch 75/1000, LR 0.000282
Train loss: 0.2002;  Loss pred: 0.2002; Loss self: 0.0000; time: 0.30s
Val loss: 0.4236 score: 0.8409 time: 0.11s
Test loss: 0.3752 score: 0.8837 time: 0.09s
Epoch 76/1000, LR 0.000282
Train loss: 0.1983;  Loss pred: 0.1983; Loss self: 0.0000; time: 0.30s
Val loss: 0.4220 score: 0.8409 time: 0.12s
Test loss: 0.3724 score: 0.8837 time: 0.09s
Epoch 77/1000, LR 0.000282
Train loss: 0.1928;  Loss pred: 0.1928; Loss self: 0.0000; time: 0.36s
Val loss: 0.4198 score: 0.8409 time: 0.16s
Test loss: 0.3700 score: 0.8837 time: 0.60s
Epoch 78/1000, LR 0.000282
Train loss: 0.1923;  Loss pred: 0.1923; Loss self: 0.0000; time: 0.34s
Val loss: 0.4201 score: 0.8409 time: 0.13s
Test loss: 0.3694 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.1923;  Loss pred: 0.1923; Loss self: 0.0000; time: 0.34s
Val loss: 0.4205 score: 0.8409 time: 0.17s
Test loss: 0.3696 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.1823;  Loss pred: 0.1823; Loss self: 0.0000; time: 0.34s
Val loss: 0.4208 score: 0.8409 time: 0.14s
Test loss: 0.3690 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.1864;  Loss pred: 0.1864; Loss self: 0.0000; time: 0.50s
Val loss: 0.4224 score: 0.8409 time: 0.44s
Test loss: 0.3694 score: 0.8837 time: 0.30s
     INFO: Early stopping counter 4 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.1858;  Loss pred: 0.1858; Loss self: 0.0000; time: 0.51s
Val loss: 0.4217 score: 0.8409 time: 0.11s
Test loss: 0.3669 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.1816;  Loss pred: 0.1816; Loss self: 0.0000; time: 0.49s
Val loss: 0.4201 score: 0.8409 time: 0.10s
Test loss: 0.3634 score: 0.8837 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.1843;  Loss pred: 0.1843; Loss self: 0.0000; time: 0.33s
Val loss: 0.4178 score: 0.8409 time: 0.14s
Test loss: 0.3591 score: 0.8837 time: 0.07s
Epoch 85/1000, LR 0.000281
Train loss: 0.1752;  Loss pred: 0.1752; Loss self: 0.0000; time: 0.26s
Val loss: 0.4155 score: 0.8409 time: 0.32s
Test loss: 0.3552 score: 0.8837 time: 0.11s
Epoch 86/1000, LR 0.000281
Train loss: 0.1801;  Loss pred: 0.1801; Loss self: 0.0000; time: 0.37s
Val loss: 0.4161 score: 0.8409 time: 0.09s
Test loss: 0.3541 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.1866;  Loss pred: 0.1866; Loss self: 0.0000; time: 0.38s
Val loss: 0.4186 score: 0.8409 time: 0.09s
Test loss: 0.3559 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.1704;  Loss pred: 0.1704; Loss self: 0.0000; time: 0.31s
Val loss: 0.4213 score: 0.8409 time: 0.14s
Test loss: 0.3592 score: 0.8837 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.1719;  Loss pred: 0.1719; Loss self: 0.0000; time: 0.64s
Val loss: 0.4231 score: 0.8182 time: 0.20s
Test loss: 0.3622 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.1692;  Loss pred: 0.1692; Loss self: 0.0000; time: 0.52s
Val loss: 0.4210 score: 0.8409 time: 0.20s
Test loss: 0.3598 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.1617;  Loss pred: 0.1617; Loss self: 0.0000; time: 0.26s
Val loss: 0.4208 score: 0.8409 time: 0.11s
Test loss: 0.3588 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.1698;  Loss pred: 0.1698; Loss self: 0.0000; time: 0.30s
Val loss: 0.4237 score: 0.8409 time: 0.12s
Test loss: 0.3611 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.25s
Val loss: 0.4253 score: 0.8409 time: 0.08s
Test loss: 0.3632 score: 0.8837 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.1512;  Loss pred: 0.1512; Loss self: 0.0000; time: 0.52s
Val loss: 0.4284 score: 0.8409 time: 0.12s
Test loss: 0.3643 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1592;  Loss pred: 0.1592; Loss self: 0.0000; time: 0.57s
Val loss: 0.4307 score: 0.8409 time: 0.16s
Test loss: 0.3642 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 10 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.1536;  Loss pred: 0.1536; Loss self: 0.0000; time: 0.36s
Val loss: 0.4316 score: 0.8409 time: 0.16s
Test loss: 0.3618 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.1458;  Loss pred: 0.1458; Loss self: 0.0000; time: 0.62s
Val loss: 0.4310 score: 0.8409 time: 0.12s
Test loss: 0.3589 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 12 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.37s
Val loss: 0.4321 score: 0.8409 time: 0.12s
Test loss: 0.3604 score: 0.8837 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 99/1000, LR 0.000279
Train loss: 0.1415;  Loss pred: 0.1415; Loss self: 0.0000; time: 0.38s
Val loss: 0.4329 score: 0.8409 time: 0.14s
Test loss: 0.3633 score: 0.8837 time: 0.24s
     INFO: Early stopping counter 14 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.1415;  Loss pred: 0.1415; Loss self: 0.0000; time: 0.43s
Val loss: 0.4347 score: 0.8182 time: 0.26s
Test loss: 0.3680 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1364;  Loss pred: 0.1364; Loss self: 0.0000; time: 0.35s
Val loss: 0.4362 score: 0.8182 time: 0.15s
Test loss: 0.3722 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 16 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 0.44s
Val loss: 0.4391 score: 0.8182 time: 0.15s
Test loss: 0.3762 score: 0.8837 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1308;  Loss pred: 0.1308; Loss self: 0.0000; time: 0.30s
Val loss: 0.4379 score: 0.8182 time: 0.32s
Test loss: 0.3760 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1292;  Loss pred: 0.1292; Loss self: 0.0000; time: 0.26s
Val loss: 0.4378 score: 0.8182 time: 0.07s
Test loss: 0.3779 score: 0.8837 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1425;  Loss pred: 0.1425; Loss self: 0.0000; time: 0.32s
Val loss: 0.4351 score: 0.8182 time: 0.12s
Test loss: 0.3765 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.1752,   Val_Loss: 0.4155,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.4155,   Test_Precision: 1.0000,   Test_Recall: 0.7727,   Test_accuracy: 0.8718,   Test_Score: 0.8837,   Test_loss: 0.3552


[0.12573948699991888, 0.10512629400000151, 0.08509011099977215, 0.1315615120001894, 0.4032009870006732, 0.23139045400057512, 0.19763101799981087, 0.10254254400024365, 0.11870687899954646, 0.08311424499970599, 0.12155232500026614, 0.11163750499963498, 0.11703549899993959, 0.12614515499990375, 0.12146880999989662, 0.45555025999965437, 0.1317049009994662, 0.15253168399976857, 0.11699377300010383, 0.11372926500007452, 0.13993537399983325, 0.16628425300041272, 0.3247410249996392, 0.17735963299946889, 0.22670055300022796, 0.12141111500022816, 0.10281331599981058, 0.3484907830006705, 0.15818188299999747, 0.14706619499975204, 0.16107373199974973, 0.18963777600038156, 0.1690489840002556, 0.15687270099988382, 0.13791378799942322, 0.11964505899959477, 0.13092204100030358, 0.348320903000058, 0.10700176200043643, 0.11201094999978523, 0.09014317499986646, 0.13619935800034, 0.37129204199936794, 0.12391749700054788, 0.12508754999998928, 0.1284354829995209, 0.07484220799960895, 0.10306252699956531, 0.1321344810003211, 0.37597801299943967, 0.15935097100009443, 0.14593084799980716, 0.22935112100003607, 0.14752635300010297, 0.12732040800074174, 0.12409375399965938, 0.12332497899933514, 0.10251831500045228, 0.0834320479998496, 0.13186303499969654, 0.2200311859996873, 0.16351609000048484, 0.13120912100021087, 0.10511967900038144, 0.09813915000086126, 0.1276796350002769, 0.30697054599932017, 0.10758703400006198, 0.1089868139997634, 0.15644118599993817, 0.1299997539999822, 0.1360674289999224, 0.23291198599963536, 0.14684972599934554, 0.11457124000025942, 0.27774077499998384, 0.12818045299991354, 0.17058108099990932, 0.13864501399984874, 0.13151023799946415, 0.14439856999979384, 0.12743365000005724, 0.12787188500078628, 0.10300221999932546, 0.12656508899999608, 0.11616524200053391, 0.11099370299962175, 0.24610895800014987, 0.12497704999987036, 0.15143953900042106, 0.20576277999953163, 0.10544775700054743, 0.11500970899942331, 0.1703232489999209, 0.1378175370000463, 0.1845114100005958, 0.13199293199977546, 0.11155925999992178, 0.09246572299980471, 0.09319958500054781, 0.606692854999892, 0.1476868480003759, 0.14826732999972592, 0.1300321589997111, 0.3081910529999732, 0.14504124300037802, 0.15435919199990167, 0.08012811500066164, 0.11151347100076237, 0.12747667100029503, 0.17509529099970678, 0.15873391500008438, 0.13013739399957558, 0.14335564700013492, 0.1182222779998483, 0.1207770500004699, 0.1516571340007431, 0.22797603199978766, 0.13818795299994235, 0.16316075200029445, 0.12459157099965523, 0.16392178100068122, 0.24138627799948154, 0.17655230699983804, 0.1475359190008021, 0.09897706100036885, 0.11169091500050854, 0.09176475100048265, 0.18514779299948714]
[0.00285771561363452, 0.002389233954545489, 0.0019338661590857307, 0.0029900343636406683, 0.009163658795469846, 0.005258873954558526, 0.0044916140454502465, 0.002330512363641901, 0.002697883613626056, 0.0018889601136296815, 0.0027625528409151393, 0.002537216022718977, 0.0026598977045440815, 0.0028669353409069035, 0.002760654772724923, 0.010353414999992145, 0.0029932932045333223, 0.003466629181812922, 0.002658949386365996, 0.0025847560227289664, 0.003180349409087119, 0.0037791875681911983, 0.0073804778409008905, 0.004030900749987929, 0.005272105883726231, 0.0028235143023308874, 0.0023910073488328043, 0.008104436813969081, 0.0036786484418604064, 0.0034201440697616756, 0.003745900744180226, 0.0044101808372181755, 0.0039313717209361766, 0.0036482023488345074, 0.0032072973953354237, 0.0027824432325487154, 0.003044698627914037, 0.00810048611628042, 0.0024884130697775913, 0.0026049058139484935, 0.0020963529069736387, 0.003167426930240465, 0.008634698651148092, 0.002881802255826695, 0.002909012790697425, 0.002986871697663277, 0.0017405164651071849, 0.002396802953478263, 0.0030728949069842118, 0.0087436747209172, 0.003705836534885917, 0.003393740651158306, 0.0053337470000008385, 0.003430845418607046, 0.0029609397209474824, 0.0028859012558060324, 0.0028680227674263985, 0.0023841468604756345, 0.001940280186043014, 0.003066582209295268, 0.005117004325574123, 0.003802699767453136, 0.003051374906981648, 0.0024446436976832894, 0.0022823058139735176, 0.002969293837215742, 0.007138849906960934, 0.0025020240465130693, 0.0025345770697619394, 0.003638167116277632, 0.003023250093022842, 0.003164358813951684, 0.005416557813945008, 0.003415109906961524, 0.002664447441866498, 0.006459087790697299, 0.0029809407674398496, 0.003967001883718822, 0.0032243026511592733, 0.003058377627894515, 0.003358106279064973, 0.0029635732558152845, 0.002973764767460146, 0.002395400465100592, 0.0029433741627906064, 0.00270151725582637, 0.0025812489069679478, 0.005723464139538369, 0.002906443023252799, 0.0035218497441958387, 0.004785180930221666, 0.002452273418617382, 0.0026746443953354257, 0.003961005790695835, 0.0032050590000010765, 0.004290963023269669, 0.00306960306976222, 0.0025944013953470182, 0.0021503656511582493, 0.0021674322093150652, 0.014109136162788187, 0.0034345778604738583, 0.003448077441854091, 0.0030240036976677, 0.007167233790697051, 0.003373052162799489, 0.003589748651160504, 0.0018634445348991078, 0.0025933365349014503, 0.0029645737441929077, 0.004071983511621088, 0.0036914863953507997, 0.0030264510232459437, 0.003333852255817091, 0.0027493553023220534, 0.0028087686046620905, 0.0035269100930405374, 0.005301768186041574, 0.003213673325580055, 0.0037944360930301036, 0.0028974783953408193, 0.0038121344418763077, 0.005613634372080966, 0.004105867604647396, 0.0034310678837395836, 0.0023017921162876475, 0.00259746313954671, 0.0021340639767554103, 0.00430576262789505]
[349.929851392096, 418.54419409096045, 517.0988671071051, 334.4443168146065, 109.12671699369263, 190.15477622033032, 222.6371166091049, 429.09019304119715, 370.66091174183856, 529.3918028149765, 361.984026220014, 394.1327782284624, 375.9543076756798, 348.80451809689856, 362.2329057149522, 96.58648861276774, 334.0802025292767, 288.46465761216376, 376.088392328034, 386.8837101863902, 314.4308600629633, 264.60713631068114, 135.49258212770894, 248.08350838283343, 189.67752584157464, 354.1685619139499, 418.23376263906533, 123.38920309383697, 271.83896906828886, 292.385343892745, 266.9584883031506, 226.74807154410775, 254.36414335347314, 274.1076026990308, 311.78898516064135, 359.39637089522967, 328.4397315491002, 123.44938138838265, 401.8625412899707, 383.8910392250262, 477.01892017963314, 315.713676123882, 115.81180078207332, 347.00507225230575, 343.75923103461287, 334.79844506957943, 574.5421086484338, 417.22244982583595, 325.426033193376, 114.36838994110032, 269.84460609263886, 294.660111891194, 187.48545815912203, 291.4733478158305, 337.7306173865661, 346.5122023798073, 348.67226695600584, 419.4372488448557, 515.3894819899125, 326.09593734968223, 195.42684281155087, 262.97106296923135, 327.7211193262311, 409.0575657089285, 438.153377114257, 336.7804113781085, 140.07858591128553, 399.67641453871875, 394.543141706054, 274.86367944063653, 330.76985668762023, 316.01978751303164, 184.61909469986367, 292.8163447863133, 375.3123384184603, 154.8206236552861, 335.4645657246117, 252.07953747240504, 310.14458262485306, 326.97074124506725, 297.78688251595145, 337.43049814535397, 336.27407619537, 417.46673033145885, 339.74613647213044, 370.1623588904706, 387.40936501727975, 174.71936149505706, 344.06316999836855, 283.94169900292974, 208.97851399605838, 407.7848711355403, 373.88147812995174, 252.4611305413741, 312.00673684935725, 233.04791828245826, 325.7750195296302, 385.4453677805872, 465.0371900524783, 461.375444963057, 70.87606133091457, 291.15659642144, 290.01668810033533, 330.68742633193943, 139.5238426989759, 296.46739858598653, 278.5710357957004, 536.6406036089197, 385.6036370682608, 337.31662164209234, 245.58056218697516, 270.89358943850874, 330.4200174789134, 299.9533042459048, 363.72163290623763, 356.0279043065939, 283.53430442506783, 188.61631910515948, 311.17039558446845, 263.54377184975465, 345.1276812306909, 262.32023430627135, 178.137715019958, 243.55388343942425, 291.4544491349678, 434.44409811117504, 384.9910263498532, 468.589513197435, 232.24689478269454]
Elapsed: 0.15830801119380536~0.07884661146923494
Time per graph: 0.0036653535020692626~0.0018214893126364123
Speed: 314.16157194132364~96.76464715808406
Total Time: 0.1864
best val loss: 0.4154704362154007 test_score: 0.8837

Testing...
Test loss: 0.5318 score: 0.7907 time: 0.16s
test Score 0.7907
Epoch Time List: [1.0991551069992056, 0.45861375000004045, 0.5094064380000418, 0.8094554159997642, 0.82464784900003, 0.7140949290005665, 0.9736223429999882, 0.5261049140008254, 0.6817055850006, 0.5548772789998111, 0.5465628109996032, 0.5299621449994447, 1.173014386999057, 0.7663024109997423, 0.5394408260008277, 0.9711991019994457, 0.7058600190002835, 0.5860561529989354, 0.5183480380001129, 0.6396399600007499, 0.7342172420003408, 0.6822621069995876, 0.7714736440002525, 0.6893293530010851, 0.7133278649998829, 0.5952190570005769, 0.5604243259995201, 0.8785209330008001, 0.6195646320011292, 0.6231448030002866, 0.5891079950006315, 0.9344381430000794, 0.6584436740004094, 0.6975364800000534, 1.092981900000268, 0.5896785710001495, 0.6274951390005299, 0.7905592509996495, 0.5649235209994004, 0.5679959279996183, 0.4998194899999362, 0.6052234289991247, 0.9021041659998446, 0.5979105699998399, 0.6744487649993971, 0.594457216999217, 0.8048161900005653, 0.5542549210003926, 0.5212413649996961, 0.7512942499997735, 0.6006252749994019, 0.6065416159999586, 0.7018283770003109, 0.9571862430011606, 0.6412613630009218, 0.7338618950007003, 0.5112059810016945, 0.6786549160005961, 0.463107577998926, 0.5449185429997669, 0.7250005260011676, 0.847462380998877, 0.5958663030005482, 0.662384234999081, 0.877650633000485, 0.5488299800008463, 0.6406905369985907, 0.6560936260002563, 0.5637207540003146, 0.9106415959986407, 0.8676369300010265, 0.5766257630002656, 1.2368079999996553, 0.732760471999427, 0.49896211699979176, 0.9031777339996552, 0.8228405700010626, 0.5934736949993749, 0.5876214909994815, 0.9889683110004626, 0.5534509930002969, 0.5315135060009197, 0.6560255759995925, 0.7086664170001313, 0.5522003859996403, 0.6074850379991403, 0.835336102999463, 1.0178307259993744, 0.8547253970000384, 0.6922717970001031, 0.6530235640011597, 0.8088627829993129, 0.5052298279997558, 0.49880226499954006, 0.9921614530003353, 0.6759266070002923, 0.6564802510001755, 0.8815925420003623, 0.49756939700091607, 0.5127735009991738, 1.1156336479998572, 0.6054267570007141, 0.6569153570007984, 0.6100126949995683, 1.2471700280002551, 0.7650860900002954, 0.7411232179983926, 0.5416152830002829, 0.6800975649994143, 0.5890827670000363, 0.6443177059991285, 0.6036362869990626, 0.9701432169995314, 0.857490074999987, 0.48624129200106836, 0.5354059180008335, 0.4709830579995469, 0.8692944919994261, 0.8614337770013663, 0.6807813069999611, 0.8587729250002667, 0.6453928750006526, 0.7614961500003119, 0.859557455999493, 0.6334621019996121, 0.6855672410001716, 0.7176854439994713, 0.41468889200041303, 0.6239003169994248]
Total Epoch List: [24, 105]
Total Time List: [0.17772020600023097, 0.18637990299976082]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a50565600>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7222;  Loss pred: 0.7222; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7138 score: 0.5000 time: 0.15s
Test loss: 0.7123 score: 0.4651 time: 0.12s
Epoch 2/1000, LR 0.000015
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7097 score: 0.5000 time: 0.12s
Test loss: 0.7082 score: 0.4651 time: 0.11s
Epoch 3/1000, LR 0.000045
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.5000 time: 0.14s
Test loss: 0.7022 score: 0.4651 time: 0.17s
Epoch 4/1000, LR 0.000075
Train loss: 0.7169;  Loss pred: 0.7169; Loss self: 0.0000; time: 0.35s
Val loss: 0.6974 score: 0.4773 time: 0.12s
Test loss: 0.6964 score: 0.4884 time: 0.13s
Epoch 5/1000, LR 0.000105
Train loss: 0.7096;  Loss pred: 0.7096; Loss self: 0.0000; time: 0.31s
Val loss: 0.6916 score: 0.5909 time: 0.13s
Test loss: 0.6907 score: 0.5814 time: 0.21s
Epoch 6/1000, LR 0.000135
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.51s
Val loss: 0.6874 score: 0.4545 time: 0.15s
Test loss: 0.6861 score: 0.5116 time: 0.13s
Epoch 7/1000, LR 0.000165
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.5116 time: 0.13s
Epoch 8/1000, LR 0.000195
Train loss: 0.6585;  Loss pred: 0.6585; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6830 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.5116 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5919;  Loss pred: 0.5919; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7007 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5897;  Loss pred: 0.5897; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7038 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7059 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7064 score: 0.5000 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7061 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 8 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7046 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7030 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6829 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6755 score: 0.5116 time: 0.51s
     INFO: Early stopping counter 11 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6676 score: 0.5116 time: 0.24s
     INFO: Early stopping counter 12 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5312;  Loss pred: 0.5312; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6829 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6582 score: 0.5116 time: 0.17s
Epoch 21/1000, LR 0.000285
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6742 score: 0.5000 time: 0.12s
Test loss: 0.6481 score: 0.5581 time: 0.11s
Epoch 22/1000, LR 0.000285
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6651 score: 0.5000 time: 0.12s
Test loss: 0.6375 score: 0.5581 time: 0.12s
Epoch 23/1000, LR 0.000285
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.39s
Val loss: 0.6560 score: 0.5227 time: 0.14s
Test loss: 0.6270 score: 0.5814 time: 0.15s
Epoch 24/1000, LR 0.000285
Train loss: 0.4921;  Loss pred: 0.4921; Loss self: 0.0000; time: 0.35s
Val loss: 0.6467 score: 0.5227 time: 0.28s
Test loss: 0.6163 score: 0.5814 time: 0.13s
Epoch 25/1000, LR 0.000285
Train loss: 0.4774;  Loss pred: 0.4774; Loss self: 0.0000; time: 0.33s
Val loss: 0.6377 score: 0.5227 time: 0.12s
Test loss: 0.6062 score: 0.6512 time: 0.14s
Epoch 26/1000, LR 0.000285
Train loss: 0.4834;  Loss pred: 0.4834; Loss self: 0.0000; time: 0.59s
Val loss: 0.6281 score: 0.5455 time: 0.12s
Test loss: 0.5959 score: 0.6744 time: 0.14s
Epoch 27/1000, LR 0.000285
Train loss: 0.4606;  Loss pred: 0.4606; Loss self: 0.0000; time: 0.41s
Val loss: 0.6208 score: 0.5682 time: 0.12s
Test loss: 0.5874 score: 0.6744 time: 0.11s
Epoch 28/1000, LR 0.000285
Train loss: 0.4539;  Loss pred: 0.4539; Loss self: 0.0000; time: 0.41s
Val loss: 0.6139 score: 0.6136 time: 0.14s
Test loss: 0.5799 score: 0.6977 time: 0.19s
Epoch 29/1000, LR 0.000285
Train loss: 0.4469;  Loss pred: 0.4469; Loss self: 0.0000; time: 0.58s
Val loss: 0.6066 score: 0.6364 time: 0.13s
Test loss: 0.5727 score: 0.6977 time: 0.23s
Epoch 30/1000, LR 0.000285
Train loss: 0.4415;  Loss pred: 0.4415; Loss self: 0.0000; time: 0.32s
Val loss: 0.5996 score: 0.6364 time: 0.08s
Test loss: 0.5661 score: 0.6977 time: 0.12s
Epoch 31/1000, LR 0.000285
Train loss: 0.4446;  Loss pred: 0.4446; Loss self: 0.0000; time: 0.30s
Val loss: 0.5932 score: 0.6364 time: 0.11s
Test loss: 0.5605 score: 0.6977 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.4232;  Loss pred: 0.4232; Loss self: 0.0000; time: 0.46s
Val loss: 0.5865 score: 0.6364 time: 0.08s
Test loss: 0.5549 score: 0.6977 time: 0.12s
Epoch 33/1000, LR 0.000285
Train loss: 0.4163;  Loss pred: 0.4163; Loss self: 0.0000; time: 0.44s
Val loss: 0.5793 score: 0.6364 time: 0.10s
Test loss: 0.5495 score: 0.7209 time: 0.10s
Epoch 34/1000, LR 0.000285
Train loss: 0.3995;  Loss pred: 0.3995; Loss self: 0.0000; time: 0.29s
Val loss: 0.5718 score: 0.6364 time: 0.13s
Test loss: 0.5443 score: 0.7209 time: 0.15s
Epoch 35/1000, LR 0.000285
Train loss: 0.3953;  Loss pred: 0.3953; Loss self: 0.0000; time: 0.29s
Val loss: 0.5636 score: 0.6364 time: 0.13s
Test loss: 0.5389 score: 0.7442 time: 0.12s
Epoch 36/1000, LR 0.000285
Train loss: 0.4051;  Loss pred: 0.4051; Loss self: 0.0000; time: 0.46s
Val loss: 0.5576 score: 0.6364 time: 0.17s
Test loss: 0.5354 score: 0.7442 time: 0.27s
Epoch 37/1000, LR 0.000285
Train loss: 0.3867;  Loss pred: 0.3867; Loss self: 0.0000; time: 0.65s
Val loss: 0.5514 score: 0.6364 time: 0.16s
Test loss: 0.5317 score: 0.7442 time: 0.10s
Epoch 38/1000, LR 0.000284
Train loss: 0.3760;  Loss pred: 0.3760; Loss self: 0.0000; time: 0.34s
Val loss: 0.5444 score: 0.6591 time: 0.14s
Test loss: 0.5276 score: 0.7442 time: 0.14s
Epoch 39/1000, LR 0.000284
Train loss: 0.3862;  Loss pred: 0.3862; Loss self: 0.0000; time: 0.32s
Val loss: 0.5387 score: 0.6591 time: 0.25s
Test loss: 0.5247 score: 0.7442 time: 0.12s
Epoch 40/1000, LR 0.000284
Train loss: 0.3692;  Loss pred: 0.3692; Loss self: 0.0000; time: 0.32s
Val loss: 0.5320 score: 0.6591 time: 0.16s
Test loss: 0.5212 score: 0.7442 time: 0.14s
Epoch 41/1000, LR 0.000284
Train loss: 0.3692;  Loss pred: 0.3692; Loss self: 0.0000; time: 0.33s
Val loss: 0.5229 score: 0.6591 time: 0.16s
Test loss: 0.5167 score: 0.7442 time: 0.23s
Epoch 42/1000, LR 0.000284
Train loss: 0.3663;  Loss pred: 0.3663; Loss self: 0.0000; time: 0.60s
Val loss: 0.5136 score: 0.6591 time: 0.28s
Test loss: 0.5127 score: 0.7442 time: 0.15s
Epoch 43/1000, LR 0.000284
Train loss: 0.3524;  Loss pred: 0.3524; Loss self: 0.0000; time: 0.33s
Val loss: 0.5053 score: 0.6591 time: 0.14s
Test loss: 0.5099 score: 0.7674 time: 0.12s
Epoch 44/1000, LR 0.000284
Train loss: 0.3471;  Loss pred: 0.3471; Loss self: 0.0000; time: 0.37s
Val loss: 0.4987 score: 0.6591 time: 0.13s
Test loss: 0.5078 score: 0.7674 time: 0.12s
Epoch 45/1000, LR 0.000284
Train loss: 0.3471;  Loss pred: 0.3471; Loss self: 0.0000; time: 0.25s
Val loss: 0.4918 score: 0.6591 time: 0.13s
Test loss: 0.5058 score: 0.7674 time: 0.48s
Epoch 46/1000, LR 0.000284
Train loss: 0.3428;  Loss pred: 0.3428; Loss self: 0.0000; time: 0.40s
Val loss: 0.4852 score: 0.6818 time: 0.14s
Test loss: 0.5041 score: 0.7674 time: 0.14s
Epoch 47/1000, LR 0.000284
Train loss: 0.3283;  Loss pred: 0.3283; Loss self: 0.0000; time: 0.31s
Val loss: 0.4767 score: 0.7045 time: 0.11s
Test loss: 0.5012 score: 0.7907 time: 0.10s
Epoch 48/1000, LR 0.000284
Train loss: 0.3188;  Loss pred: 0.3188; Loss self: 0.0000; time: 0.34s
Val loss: 0.4708 score: 0.7045 time: 0.12s
Test loss: 0.5001 score: 0.7907 time: 0.20s
Epoch 49/1000, LR 0.000284
Train loss: 0.3201;  Loss pred: 0.3201; Loss self: 0.0000; time: 0.35s
Val loss: 0.4662 score: 0.7045 time: 0.30s
Test loss: 0.5005 score: 0.7907 time: 0.12s
Epoch 50/1000, LR 0.000284
Train loss: 0.3331;  Loss pred: 0.3331; Loss self: 0.0000; time: 0.30s
Val loss: 0.4603 score: 0.7045 time: 0.12s
Test loss: 0.4996 score: 0.7907 time: 0.11s
Epoch 51/1000, LR 0.000284
Train loss: 0.3088;  Loss pred: 0.3088; Loss self: 0.0000; time: 0.25s
Val loss: 0.4555 score: 0.7273 time: 0.12s
Test loss: 0.4990 score: 0.8140 time: 0.09s
Epoch 52/1000, LR 0.000284
Train loss: 0.3218;  Loss pred: 0.3218; Loss self: 0.0000; time: 0.28s
Val loss: 0.4550 score: 0.7045 time: 0.11s
Test loss: 0.5017 score: 0.8140 time: 0.13s
Epoch 53/1000, LR 0.000284
Train loss: 0.3049;  Loss pred: 0.3049; Loss self: 0.0000; time: 0.58s
Val loss: 0.4553 score: 0.7045 time: 0.41s
Test loss: 0.5051 score: 0.8140 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.3060;  Loss pred: 0.3060; Loss self: 0.0000; time: 0.51s
Val loss: 0.4538 score: 0.7045 time: 0.13s
Test loss: 0.5070 score: 0.8140 time: 0.13s
Epoch 55/1000, LR 0.000284
Train loss: 0.3137;  Loss pred: 0.3137; Loss self: 0.0000; time: 0.40s
Val loss: 0.4503 score: 0.7045 time: 0.12s
Test loss: 0.5077 score: 0.8140 time: 0.36s
Epoch 56/1000, LR 0.000284
Train loss: 0.2997;  Loss pred: 0.2997; Loss self: 0.0000; time: 0.34s
Val loss: 0.4469 score: 0.7727 time: 0.14s
Test loss: 0.5086 score: 0.8140 time: 0.12s
Epoch 57/1000, LR 0.000283
Train loss: 0.2911;  Loss pred: 0.2911; Loss self: 0.0000; time: 0.35s
Val loss: 0.4418 score: 0.7727 time: 0.14s
Test loss: 0.5072 score: 0.8140 time: 0.35s
Epoch 58/1000, LR 0.000283
Train loss: 0.2833;  Loss pred: 0.2833; Loss self: 0.0000; time: 0.33s
Val loss: 0.4381 score: 0.7955 time: 0.12s
Test loss: 0.5067 score: 0.8140 time: 0.13s
Epoch 59/1000, LR 0.000283
Train loss: 0.2802;  Loss pred: 0.2802; Loss self: 0.0000; time: 0.30s
Val loss: 0.4338 score: 0.7955 time: 0.49s
Test loss: 0.5068 score: 0.8140 time: 0.12s
Epoch 60/1000, LR 0.000283
Train loss: 0.2873;  Loss pred: 0.2873; Loss self: 0.0000; time: 0.31s
Val loss: 0.4281 score: 0.7955 time: 0.27s
Test loss: 0.5053 score: 0.8140 time: 0.11s
Epoch 61/1000, LR 0.000283
Train loss: 0.2790;  Loss pred: 0.2790; Loss self: 0.0000; time: 0.35s
Val loss: 0.4209 score: 0.7955 time: 0.12s
Test loss: 0.5021 score: 0.8140 time: 0.22s
Epoch 62/1000, LR 0.000283
Train loss: 0.2766;  Loss pred: 0.2766; Loss self: 0.0000; time: 0.32s
Val loss: 0.4140 score: 0.7955 time: 0.14s
Test loss: 0.4987 score: 0.8140 time: 0.12s
Epoch 63/1000, LR 0.000283
Train loss: 0.2948;  Loss pred: 0.2948; Loss self: 0.0000; time: 0.50s
Val loss: 0.4081 score: 0.8182 time: 0.11s
Test loss: 0.4964 score: 0.8372 time: 0.16s
Epoch 64/1000, LR 0.000283
Train loss: 0.2569;  Loss pred: 0.2569; Loss self: 0.0000; time: 0.37s
Val loss: 0.4015 score: 0.8182 time: 0.14s
Test loss: 0.4919 score: 0.8372 time: 0.12s
Epoch 65/1000, LR 0.000283
Train loss: 0.2549;  Loss pred: 0.2549; Loss self: 0.0000; time: 0.25s
Val loss: 0.3956 score: 0.8182 time: 0.24s
Test loss: 0.4874 score: 0.8372 time: 0.11s
Epoch 66/1000, LR 0.000283
Train loss: 0.2613;  Loss pred: 0.2613; Loss self: 0.0000; time: 0.34s
Val loss: 0.3924 score: 0.8182 time: 0.11s
Test loss: 0.4855 score: 0.8605 time: 0.26s
Epoch 67/1000, LR 0.000283
Train loss: 0.2556;  Loss pred: 0.2556; Loss self: 0.0000; time: 0.30s
Val loss: 0.3872 score: 0.8182 time: 0.11s
Test loss: 0.4816 score: 0.8605 time: 0.12s
Epoch 68/1000, LR 0.000283
Train loss: 0.2464;  Loss pred: 0.2464; Loss self: 0.0000; time: 0.26s
Val loss: 0.3813 score: 0.8182 time: 0.08s
Test loss: 0.4765 score: 0.8605 time: 0.19s
Epoch 69/1000, LR 0.000283
Train loss: 0.2442;  Loss pred: 0.2442; Loss self: 0.0000; time: 0.32s
Val loss: 0.3792 score: 0.8182 time: 0.13s
Test loss: 0.4747 score: 0.8605 time: 0.12s
Epoch 70/1000, LR 0.000283
Train loss: 0.2556;  Loss pred: 0.2556; Loss self: 0.0000; time: 0.34s
Val loss: 0.3786 score: 0.8182 time: 0.27s
Test loss: 0.4738 score: 0.8605 time: 0.18s
Epoch 71/1000, LR 0.000282
Train loss: 0.2513;  Loss pred: 0.2513; Loss self: 0.0000; time: 0.32s
Val loss: 0.3763 score: 0.8182 time: 0.13s
Test loss: 0.4724 score: 0.8605 time: 0.13s
Epoch 72/1000, LR 0.000282
Train loss: 0.2681;  Loss pred: 0.2681; Loss self: 0.0000; time: 0.32s
Val loss: 0.3767 score: 0.8182 time: 0.12s
Test loss: 0.4720 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.2486;  Loss pred: 0.2486; Loss self: 0.0000; time: 0.32s
Val loss: 0.3791 score: 0.8182 time: 0.12s
Test loss: 0.4727 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.2399;  Loss pred: 0.2399; Loss self: 0.0000; time: 0.35s
Val loss: 0.3816 score: 0.8182 time: 0.39s
Test loss: 0.4750 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.2393;  Loss pred: 0.2393; Loss self: 0.0000; time: 0.41s
Val loss: 0.3838 score: 0.8182 time: 0.21s
Test loss: 0.4778 score: 0.8372 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.2353;  Loss pred: 0.2353; Loss self: 0.0000; time: 0.46s
Val loss: 0.3819 score: 0.8182 time: 0.16s
Test loss: 0.4797 score: 0.8140 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.2404;  Loss pred: 0.2404; Loss self: 0.0000; time: 0.34s
Val loss: 0.3786 score: 0.8182 time: 0.13s
Test loss: 0.4798 score: 0.8605 time: 0.30s
     INFO: Early stopping counter 6 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.2371;  Loss pred: 0.2371; Loss self: 0.0000; time: 0.34s
Val loss: 0.3765 score: 0.8182 time: 0.19s
Test loss: 0.4788 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.2234;  Loss pred: 0.2234; Loss self: 0.0000; time: 0.35s
Val loss: 0.3733 score: 0.8182 time: 0.14s
Test loss: 0.4768 score: 0.8837 time: 0.19s
Epoch 80/1000, LR 0.000282
Train loss: 0.2135;  Loss pred: 0.2135; Loss self: 0.0000; time: 0.37s
Val loss: 0.3716 score: 0.8182 time: 0.10s
Test loss: 0.4750 score: 0.9070 time: 0.10s
Epoch 81/1000, LR 0.000281
Train loss: 0.2168;  Loss pred: 0.2168; Loss self: 0.0000; time: 0.30s
Val loss: 0.3702 score: 0.8182 time: 0.13s
Test loss: 0.4749 score: 0.9070 time: 0.11s
Epoch 82/1000, LR 0.000281
Train loss: 0.2144;  Loss pred: 0.2144; Loss self: 0.0000; time: 0.28s
Val loss: 0.3664 score: 0.8182 time: 0.10s
Test loss: 0.4733 score: 0.9070 time: 0.13s
Epoch 83/1000, LR 0.000281
Train loss: 0.2168;  Loss pred: 0.2168; Loss self: 0.0000; time: 0.51s
Val loss: 0.3671 score: 0.8182 time: 0.10s
Test loss: 0.4739 score: 0.9070 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2268;  Loss pred: 0.2268; Loss self: 0.0000; time: 0.28s
Val loss: 0.3676 score: 0.8182 time: 0.10s
Test loss: 0.4750 score: 0.9070 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.2200;  Loss pred: 0.2200; Loss self: 0.0000; time: 0.28s
Val loss: 0.3655 score: 0.8182 time: 0.11s
Test loss: 0.4732 score: 0.9070 time: 0.09s
Epoch 86/1000, LR 0.000281
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.24s
Val loss: 0.3632 score: 0.8182 time: 0.18s
Test loss: 0.4707 score: 0.9070 time: 0.12s
Epoch 87/1000, LR 0.000281
Train loss: 0.2128;  Loss pred: 0.2128; Loss self: 0.0000; time: 0.30s
Val loss: 0.3608 score: 0.8409 time: 0.12s
Test loss: 0.4686 score: 0.9070 time: 0.10s
Epoch 88/1000, LR 0.000281
Train loss: 0.2024;  Loss pred: 0.2024; Loss self: 0.0000; time: 0.64s
Val loss: 0.3579 score: 0.8409 time: 0.13s
Test loss: 0.4663 score: 0.9070 time: 0.11s
Epoch 89/1000, LR 0.000281
Train loss: 0.2081;  Loss pred: 0.2081; Loss self: 0.0000; time: 0.41s
Val loss: 0.3544 score: 0.8409 time: 0.13s
Test loss: 0.4630 score: 0.9070 time: 0.21s
Epoch 90/1000, LR 0.000281
Train loss: 0.1994;  Loss pred: 0.1994; Loss self: 0.0000; time: 0.30s
Val loss: 0.3529 score: 0.8636 time: 0.12s
Test loss: 0.4615 score: 0.9070 time: 0.13s
Epoch 91/1000, LR 0.000280
Train loss: 0.2160;  Loss pred: 0.2160; Loss self: 0.0000; time: 0.41s
Val loss: 0.3506 score: 0.8636 time: 0.29s
Test loss: 0.4598 score: 0.9070 time: 0.17s
Epoch 92/1000, LR 0.000280
Train loss: 0.2040;  Loss pred: 0.2040; Loss self: 0.0000; time: 0.40s
Val loss: 0.3505 score: 0.8636 time: 0.13s
Test loss: 0.4595 score: 0.9070 time: 0.13s
Epoch 93/1000, LR 0.000280
Train loss: 0.1976;  Loss pred: 0.1976; Loss self: 0.0000; time: 0.35s
Val loss: 0.3486 score: 0.8636 time: 0.15s
Test loss: 0.4581 score: 0.9070 time: 0.13s
Epoch 94/1000, LR 0.000280
Train loss: 0.1939;  Loss pred: 0.1939; Loss self: 0.0000; time: 0.39s
Val loss: 0.3467 score: 0.8636 time: 0.12s
Test loss: 0.4568 score: 0.9070 time: 0.11s
Epoch 95/1000, LR 0.000280
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 0.47s
Val loss: 0.3480 score: 0.8636 time: 0.19s
Test loss: 0.4587 score: 0.9070 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 0.36s
Val loss: 0.3464 score: 0.8636 time: 0.17s
Test loss: 0.4587 score: 0.9070 time: 0.12s
Epoch 97/1000, LR 0.000280
Train loss: 0.2010;  Loss pred: 0.2010; Loss self: 0.0000; time: 0.39s
Val loss: 0.3456 score: 0.8636 time: 0.12s
Test loss: 0.4602 score: 0.9070 time: 0.15s
Epoch 98/1000, LR 0.000280
Train loss: 0.2012;  Loss pred: 0.2012; Loss self: 0.0000; time: 0.43s
Val loss: 0.3448 score: 0.8636 time: 0.17s
Test loss: 0.4610 score: 0.9070 time: 0.10s
Epoch 99/1000, LR 0.000279
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.55s
Val loss: 0.3446 score: 0.8636 time: 0.11s
Test loss: 0.4620 score: 0.9070 time: 0.10s
Epoch 100/1000, LR 0.000279
Train loss: 0.1943;  Loss pred: 0.1943; Loss self: 0.0000; time: 0.28s
Val loss: 0.3437 score: 0.8636 time: 0.08s
Test loss: 0.4622 score: 0.9070 time: 0.16s
Epoch 101/1000, LR 0.000279
Train loss: 0.1908;  Loss pred: 0.1908; Loss self: 0.0000; time: 0.30s
Val loss: 0.3448 score: 0.8636 time: 0.14s
Test loss: 0.4625 score: 0.9070 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1737;  Loss pred: 0.1737; Loss self: 0.0000; time: 0.36s
Val loss: 0.3416 score: 0.8636 time: 0.09s
Test loss: 0.4616 score: 0.9070 time: 0.11s
Epoch 103/1000, LR 0.000279
Train loss: 0.1723;  Loss pred: 0.1723; Loss self: 0.0000; time: 0.28s
Val loss: 0.3364 score: 0.8636 time: 0.10s
Test loss: 0.4584 score: 0.9070 time: 0.17s
Epoch 104/1000, LR 0.000279
Train loss: 0.1867;  Loss pred: 0.1867; Loss self: 0.0000; time: 0.43s
Val loss: 0.3321 score: 0.8864 time: 0.12s
Test loss: 0.4569 score: 0.8837 time: 0.22s
Epoch 105/1000, LR 0.000279
Train loss: 0.1723;  Loss pred: 0.1723; Loss self: 0.0000; time: 0.33s
Val loss: 0.3282 score: 0.8864 time: 0.14s
Test loss: 0.4550 score: 0.8837 time: 0.33s
Epoch 106/1000, LR 0.000279
Train loss: 0.1701;  Loss pred: 0.1701; Loss self: 0.0000; time: 0.33s
Val loss: 0.3248 score: 0.8864 time: 0.18s
Test loss: 0.4532 score: 0.8837 time: 0.21s
Epoch 107/1000, LR 0.000278
Train loss: 0.1627;  Loss pred: 0.1627; Loss self: 0.0000; time: 0.48s
Val loss: 0.3222 score: 0.8864 time: 0.14s
Test loss: 0.4516 score: 0.8605 time: 0.12s
Epoch 108/1000, LR 0.000278
Train loss: 0.1827;  Loss pred: 0.1827; Loss self: 0.0000; time: 0.34s
Val loss: 0.3210 score: 0.8864 time: 0.19s
Test loss: 0.4506 score: 0.8605 time: 0.15s
Epoch 109/1000, LR 0.000278
Train loss: 0.1718;  Loss pred: 0.1718; Loss self: 0.0000; time: 0.53s
Val loss: 0.3194 score: 0.8864 time: 0.15s
Test loss: 0.4488 score: 0.8605 time: 0.15s
Epoch 110/1000, LR 0.000278
Train loss: 0.1612;  Loss pred: 0.1612; Loss self: 0.0000; time: 0.41s
Val loss: 0.3198 score: 0.8864 time: 0.17s
Test loss: 0.4492 score: 0.8605 time: 0.36s
     INFO: Early stopping counter 1 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1712;  Loss pred: 0.1712; Loss self: 0.0000; time: 0.29s
Val loss: 0.3211 score: 0.8864 time: 0.12s
Test loss: 0.4493 score: 0.8837 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1685;  Loss pred: 0.1685; Loss self: 0.0000; time: 0.41s
Val loss: 0.3227 score: 0.8864 time: 0.13s
Test loss: 0.4489 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1581;  Loss pred: 0.1581; Loss self: 0.0000; time: 0.36s
Val loss: 0.3244 score: 0.8864 time: 0.13s
Test loss: 0.4482 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1583;  Loss pred: 0.1583; Loss self: 0.0000; time: 0.40s
Val loss: 0.3272 score: 0.8636 time: 0.35s
Test loss: 0.4471 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1615;  Loss pred: 0.1615; Loss self: 0.0000; time: 0.40s
Val loss: 0.3284 score: 0.8636 time: 0.17s
Test loss: 0.4476 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1502;  Loss pred: 0.1502; Loss self: 0.0000; time: 0.27s
Val loss: 0.3310 score: 0.8636 time: 0.12s
Test loss: 0.4483 score: 0.9070 time: 0.13s
     INFO: Early stopping counter 7 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1500;  Loss pred: 0.1500; Loss self: 0.0000; time: 0.27s
Val loss: 0.3325 score: 0.8636 time: 0.10s
Test loss: 0.4486 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 118/1000, LR 0.000277
Train loss: 0.1674;  Loss pred: 0.1674; Loss self: 0.0000; time: 0.67s
Val loss: 0.3350 score: 0.8409 time: 0.19s
Test loss: 0.4508 score: 0.9070 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 119/1000, LR 0.000277
Train loss: 0.1570;  Loss pred: 0.1570; Loss self: 0.0000; time: 0.33s
Val loss: 0.3379 score: 0.8409 time: 0.12s
Test loss: 0.4529 score: 0.9070 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 120/1000, LR 0.000277
Train loss: 0.1466;  Loss pred: 0.1466; Loss self: 0.0000; time: 0.25s
Val loss: 0.3426 score: 0.8182 time: 0.16s
Test loss: 0.4578 score: 0.9070 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 121/1000, LR 0.000276
Train loss: 0.1606;  Loss pred: 0.1606; Loss self: 0.0000; time: 0.49s
Val loss: 0.3385 score: 0.8182 time: 0.13s
Test loss: 0.4548 score: 0.9070 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 122/1000, LR 0.000276
Train loss: 0.1522;  Loss pred: 0.1522; Loss self: 0.0000; time: 0.41s
Val loss: 0.3319 score: 0.8409 time: 0.14s
Test loss: 0.4503 score: 0.9070 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 123/1000, LR 0.000276
Train loss: 0.1501;  Loss pred: 0.1501; Loss self: 0.0000; time: 0.36s
Val loss: 0.3267 score: 0.8409 time: 0.14s
Test loss: 0.4464 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 124/1000, LR 0.000276
Train loss: 0.1427;  Loss pred: 0.1427; Loss self: 0.0000; time: 0.38s
Val loss: 0.3266 score: 0.8409 time: 0.13s
Test loss: 0.4456 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 15 of 20
Epoch 125/1000, LR 0.000276
Train loss: 0.1389;  Loss pred: 0.1389; Loss self: 0.0000; time: 0.59s
Val loss: 0.3353 score: 0.8182 time: 0.14s
Test loss: 0.4486 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 126/1000, LR 0.000276
Train loss: 0.1355;  Loss pred: 0.1355; Loss self: 0.0000; time: 0.54s
Val loss: 0.3371 score: 0.8182 time: 0.13s
Test loss: 0.4501 score: 0.8837 time: 0.10s
     INFO: Early stopping counter 17 of 20
Epoch 127/1000, LR 0.000275
Train loss: 0.1464;  Loss pred: 0.1464; Loss self: 0.0000; time: 0.36s
Val loss: 0.3291 score: 0.8409 time: 0.35s
Test loss: 0.4424 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 18 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1279;  Loss pred: 0.1279; Loss self: 0.0000; time: 0.30s
Val loss: 0.3177 score: 0.8636 time: 0.14s
Test loss: 0.4346 score: 0.8837 time: 0.10s
Epoch 129/1000, LR 0.000275
Train loss: 0.1245;  Loss pred: 0.1245; Loss self: 0.0000; time: 0.34s
Val loss: 0.3151 score: 0.8636 time: 0.15s
Test loss: 0.4312 score: 0.8837 time: 0.15s
Epoch 130/1000, LR 0.000275
Train loss: 0.1247;  Loss pred: 0.1247; Loss self: 0.0000; time: 0.35s
Val loss: 0.3173 score: 0.8409 time: 0.29s
Test loss: 0.4274 score: 0.8837 time: 0.36s
     INFO: Early stopping counter 1 of 20
Epoch 131/1000, LR 0.000275
Train loss: 0.1205;  Loss pred: 0.1205; Loss self: 0.0000; time: 0.43s
Val loss: 0.3338 score: 0.8182 time: 0.12s
Test loss: 0.4297 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 132/1000, LR 0.000275
Train loss: 0.1268;  Loss pred: 0.1268; Loss self: 0.0000; time: 0.37s
Val loss: 0.3370 score: 0.8182 time: 0.11s
Test loss: 0.4251 score: 0.8837 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 133/1000, LR 0.000274
Train loss: 0.1237;  Loss pred: 0.1237; Loss self: 0.0000; time: 0.30s
Val loss: 0.3201 score: 0.8182 time: 0.23s
Test loss: 0.4160 score: 0.8837 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 134/1000, LR 0.000274
Train loss: 0.1064;  Loss pred: 0.1064; Loss self: 0.0000; time: 0.25s
Val loss: 0.3136 score: 0.8636 time: 0.08s
Test loss: 0.4126 score: 0.8837 time: 0.09s
Epoch 135/1000, LR 0.000274
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.48s
Val loss: 0.3195 score: 0.8409 time: 0.16s
Test loss: 0.4149 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 136/1000, LR 0.000274
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.35s
Val loss: 0.3118 score: 0.8636 time: 0.10s
Test loss: 0.4155 score: 0.8837 time: 0.10s
Epoch 137/1000, LR 0.000274
Train loss: 0.0977;  Loss pred: 0.0977; Loss self: 0.0000; time: 0.49s
Val loss: 0.3226 score: 0.8409 time: 0.12s
Test loss: 0.4144 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 138/1000, LR 0.000274
Train loss: 0.0973;  Loss pred: 0.0973; Loss self: 0.0000; time: 0.63s
Val loss: 0.3330 score: 0.8182 time: 0.12s
Test loss: 0.4018 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 139/1000, LR 0.000273
Train loss: 0.0876;  Loss pred: 0.0876; Loss self: 0.0000; time: 0.32s
Val loss: 0.3141 score: 0.8636 time: 0.24s
Test loss: 0.4023 score: 0.8837 time: 0.26s
     INFO: Early stopping counter 3 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 0.32s
Val loss: 0.3153 score: 0.8636 time: 0.16s
Test loss: 0.4095 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.37s
Val loss: 0.3360 score: 0.8182 time: 0.36s
Test loss: 0.4177 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0882;  Loss pred: 0.0882; Loss self: 0.0000; time: 0.32s
Val loss: 0.3024 score: 0.8636 time: 0.13s
Test loss: 0.4067 score: 0.8605 time: 0.13s
Epoch 143/1000, LR 0.000273
Train loss: 0.0923;  Loss pred: 0.0923; Loss self: 0.0000; time: 0.25s
Val loss: 0.3127 score: 0.8636 time: 0.31s
Test loss: 0.4060 score: 0.8605 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 144/1000, LR 0.000272
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 0.32s
Val loss: 0.3096 score: 0.8636 time: 0.13s
Test loss: 0.3983 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 145/1000, LR 0.000272
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 0.39s
Val loss: 0.3075 score: 0.8636 time: 0.32s
Test loss: 0.3944 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.47s
Val loss: 0.3104 score: 0.8636 time: 0.13s
Test loss: 0.3908 score: 0.8605 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 147/1000, LR 0.000272
Train loss: 0.0684;  Loss pred: 0.0684; Loss self: 0.0000; time: 0.38s
Val loss: 0.3420 score: 0.8182 time: 0.15s
Test loss: 0.3916 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 148/1000, LR 0.000272
Train loss: 0.0716;  Loss pred: 0.0716; Loss self: 0.0000; time: 0.40s
Val loss: 0.3154 score: 0.8636 time: 0.16s
Test loss: 0.3774 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 149/1000, LR 0.000272
Train loss: 0.0735;  Loss pred: 0.0735; Loss self: 0.0000; time: 0.74s
Val loss: 0.3120 score: 0.8636 time: 0.12s
Test loss: 0.3767 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 7 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0717;  Loss pred: 0.0717; Loss self: 0.0000; time: 0.33s
Val loss: 0.3479 score: 0.8182 time: 0.12s
Test loss: 0.3853 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 151/1000, LR 0.000271
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 0.26s
Val loss: 0.3325 score: 0.8636 time: 0.13s
Test loss: 0.3816 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 152/1000, LR 0.000271
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.31s
Val loss: 0.3236 score: 0.8636 time: 0.20s
Test loss: 0.3846 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 153/1000, LR 0.000271
Train loss: 0.0668;  Loss pred: 0.0668; Loss self: 0.0000; time: 0.57s
Val loss: 0.3139 score: 0.8636 time: 0.12s
Test loss: 0.3861 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 11 of 20
Epoch 154/1000, LR 0.000271
Train loss: 0.0603;  Loss pred: 0.0603; Loss self: 0.0000; time: 0.34s
Val loss: 0.3294 score: 0.8636 time: 0.10s
Test loss: 0.3923 score: 0.8605 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 155/1000, LR 0.000270
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.49s
Val loss: 0.3322 score: 0.8636 time: 0.17s
Test loss: 0.3950 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 13 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.32s
Val loss: 0.3423 score: 0.8636 time: 0.13s
Test loss: 0.3955 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 157/1000, LR 0.000270
Train loss: 0.0587;  Loss pred: 0.0587; Loss self: 0.0000; time: 0.57s
Val loss: 0.3226 score: 0.8636 time: 0.13s
Test loss: 0.3880 score: 0.8605 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 158/1000, LR 0.000270
Train loss: 0.0659;  Loss pred: 0.0659; Loss self: 0.0000; time: 0.34s
Val loss: 0.3243 score: 0.8636 time: 0.15s
Test loss: 0.3833 score: 0.8605 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 159/1000, LR 0.000270
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.35s
Val loss: 0.3140 score: 0.8636 time: 0.13s
Test loss: 0.3787 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 17 of 20
Epoch 160/1000, LR 0.000269
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.34s
Val loss: 0.3125 score: 0.8636 time: 0.14s
Test loss: 0.3799 score: 0.8605 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 161/1000, LR 0.000269
Train loss: 0.0543;  Loss pred: 0.0543; Loss self: 0.0000; time: 0.26s
Val loss: 0.3377 score: 0.8636 time: 0.14s
Test loss: 0.3851 score: 0.8605 time: 0.38s
     INFO: Early stopping counter 19 of 20
Epoch 162/1000, LR 0.000269
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.36s
Val loss: 0.3165 score: 0.8636 time: 0.14s
Test loss: 0.3831 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 141,   Train_Loss: 0.0882,   Val_Loss: 0.3024,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.3024,   Test_Precision: 0.8947,   Test_Recall: 0.8095,   Test_accuracy: 0.8500,   Test_Score: 0.8605,   Test_loss: 0.4067


[0.12573948699991888, 0.10512629400000151, 0.08509011099977215, 0.1315615120001894, 0.4032009870006732, 0.23139045400057512, 0.19763101799981087, 0.10254254400024365, 0.11870687899954646, 0.08311424499970599, 0.12155232500026614, 0.11163750499963498, 0.11703549899993959, 0.12614515499990375, 0.12146880999989662, 0.45555025999965437, 0.1317049009994662, 0.15253168399976857, 0.11699377300010383, 0.11372926500007452, 0.13993537399983325, 0.16628425300041272, 0.3247410249996392, 0.17735963299946889, 0.22670055300022796, 0.12141111500022816, 0.10281331599981058, 0.3484907830006705, 0.15818188299999747, 0.14706619499975204, 0.16107373199974973, 0.18963777600038156, 0.1690489840002556, 0.15687270099988382, 0.13791378799942322, 0.11964505899959477, 0.13092204100030358, 0.348320903000058, 0.10700176200043643, 0.11201094999978523, 0.09014317499986646, 0.13619935800034, 0.37129204199936794, 0.12391749700054788, 0.12508754999998928, 0.1284354829995209, 0.07484220799960895, 0.10306252699956531, 0.1321344810003211, 0.37597801299943967, 0.15935097100009443, 0.14593084799980716, 0.22935112100003607, 0.14752635300010297, 0.12732040800074174, 0.12409375399965938, 0.12332497899933514, 0.10251831500045228, 0.0834320479998496, 0.13186303499969654, 0.2200311859996873, 0.16351609000048484, 0.13120912100021087, 0.10511967900038144, 0.09813915000086126, 0.1276796350002769, 0.30697054599932017, 0.10758703400006198, 0.1089868139997634, 0.15644118599993817, 0.1299997539999822, 0.1360674289999224, 0.23291198599963536, 0.14684972599934554, 0.11457124000025942, 0.27774077499998384, 0.12818045299991354, 0.17058108099990932, 0.13864501399984874, 0.13151023799946415, 0.14439856999979384, 0.12743365000005724, 0.12787188500078628, 0.10300221999932546, 0.12656508899999608, 0.11616524200053391, 0.11099370299962175, 0.24610895800014987, 0.12497704999987036, 0.15143953900042106, 0.20576277999953163, 0.10544775700054743, 0.11500970899942331, 0.1703232489999209, 0.1378175370000463, 0.1845114100005958, 0.13199293199977546, 0.11155925999992178, 0.09246572299980471, 0.09319958500054781, 0.606692854999892, 0.1476868480003759, 0.14826732999972592, 0.1300321589997111, 0.3081910529999732, 0.14504124300037802, 0.15435919199990167, 0.08012811500066164, 0.11151347100076237, 0.12747667100029503, 0.17509529099970678, 0.15873391500008438, 0.13013739399957558, 0.14335564700013492, 0.1182222779998483, 0.1207770500004699, 0.1516571340007431, 0.22797603199978766, 0.13818795299994235, 0.16316075200029445, 0.12459157099965523, 0.16392178100068122, 0.24138627799948154, 0.17655230699983804, 0.1475359190008021, 0.09897706100036885, 0.11169091500050854, 0.09176475100048265, 0.18514779299948714, 0.1307856579996951, 0.11259050599983311, 0.18066454699965107, 0.13117088399940258, 0.219214636000288, 0.13996877899990068, 0.1340641090000645, 0.11276864499996009, 0.21059925699955784, 0.14661998199972004, 0.09008815400011372, 0.11551259799944091, 0.08581611799945676, 0.1263034739995419, 0.14660982099940156, 0.0952261130005354, 0.08718031799980963, 0.5155649869993795, 0.24969048499951896, 0.17639540500022122, 0.11693386399929295, 0.12045047500032524, 0.15965970199977164, 0.13205830900005822, 0.14175203399918246, 0.14468558599946846, 0.11087727300036931, 0.19639057899985346, 0.23385677700025553, 0.12996482499966078, 0.16350801899989165, 0.13031582500025252, 0.10895516999971733, 0.15786650600057328, 0.1248853010001767, 0.27873356999953103, 0.10945905900007347, 0.14031998500013287, 0.12454347299990332, 0.14355811000041285, 0.2310643999999229, 0.15593827499924373, 0.12263700800031074, 0.12546088500039332, 0.48459378900042793, 0.14753255600044213, 0.10137649300031626, 0.20847664099983376, 0.12464471600014804, 0.1170619220001754, 0.09594508999998652, 0.13536971600024117, 0.15627528699951654, 0.13434886000050028, 0.36093608400005905, 0.1306610580004417, 0.35248391599998286, 0.13115609400028916, 0.12509629800024413, 0.11946491100025014, 0.22952239200003532, 0.13022505699973408, 0.1684722359996158, 0.12308445400049095, 0.1159312529998715, 0.26459266600068077, 0.12847943100041448, 0.1942894449994128, 0.1234400919993277, 0.1857775689995833, 0.13412787600009324, 0.1450744849998955, 0.1361630490000607, 0.13194822799960093, 0.14875777700035542, 0.1618588430001182, 0.30346854100025666, 0.1511092020000433, 0.19629552900005365, 0.10514531500029989, 0.11190615100076684, 0.13853652899979352, 0.0799617530001342, 0.10126826300074754, 0.09824947700053599, 0.12009812500036787, 0.1083230320000439, 0.11236444400037726, 0.21111146600014763, 0.13111332100015716, 0.1731017329993847, 0.1311157030004324, 0.13930782599982194, 0.11924542199994903, 0.12604801399993448, 0.12022746900038328, 0.15684789300030388, 0.10777417300050729, 0.10866801599968312, 0.16932095399988611, 0.13373450600010983, 0.11721299100008764, 0.17950963800012687, 0.2249394909995317, 0.33345657999961986, 0.21373797500018554, 0.1231208669996704, 0.15381066600002669, 0.15848809099952632, 0.3626286930002607, 0.21500775100048486, 0.1387679540002864, 0.1630960350003079, 0.1423855580005693, 0.11647810100021161, 0.13374134300011065, 0.1523554180002975, 0.17053806200055988, 0.11537322599997424, 0.11955918900002871, 0.10310535699954926, 0.1506409830008124, 0.13646619500013912, 0.12008651900032419, 0.1883655159999762, 0.10863378500016552, 0.12423139699967578, 0.10598385899993445, 0.15197819100012566, 0.3680369439998685, 0.12127310300002137, 0.10982217699984176, 0.13076527600060217, 0.09650948200032872, 0.1399954649996289, 0.10307455100064544, 0.11205183400034002, 0.1984793150004407, 0.26515762100007123, 0.1411960879995604, 0.13115487199956988, 0.1348694409998643, 0.17845448900061456, 0.11794715299947711, 0.12880229399979726, 0.1554940349997196, 0.1494571299999734, 0.14833495399943786, 0.11410664100003487, 0.12047151400020084, 0.12454612999954406, 0.10962247300085437, 0.10699973900045734, 0.08812892900004954, 0.1436468629999581, 0.13407518499934667, 0.16035506099979102, 0.15778962499916815, 0.1303074250008649, 0.1824664980003945, 0.3889558760001819, 0.12037071000031574]
[0.00285771561363452, 0.002389233954545489, 0.0019338661590857307, 0.0029900343636406683, 0.009163658795469846, 0.005258873954558526, 0.0044916140454502465, 0.002330512363641901, 0.002697883613626056, 0.0018889601136296815, 0.0027625528409151393, 0.002537216022718977, 0.0026598977045440815, 0.0028669353409069035, 0.002760654772724923, 0.010353414999992145, 0.0029932932045333223, 0.003466629181812922, 0.002658949386365996, 0.0025847560227289664, 0.003180349409087119, 0.0037791875681911983, 0.0073804778409008905, 0.004030900749987929, 0.005272105883726231, 0.0028235143023308874, 0.0023910073488328043, 0.008104436813969081, 0.0036786484418604064, 0.0034201440697616756, 0.003745900744180226, 0.0044101808372181755, 0.0039313717209361766, 0.0036482023488345074, 0.0032072973953354237, 0.0027824432325487154, 0.003044698627914037, 0.00810048611628042, 0.0024884130697775913, 0.0026049058139484935, 0.0020963529069736387, 0.003167426930240465, 0.008634698651148092, 0.002881802255826695, 0.002909012790697425, 0.002986871697663277, 0.0017405164651071849, 0.002396802953478263, 0.0030728949069842118, 0.0087436747209172, 0.003705836534885917, 0.003393740651158306, 0.0053337470000008385, 0.003430845418607046, 0.0029609397209474824, 0.0028859012558060324, 0.0028680227674263985, 0.0023841468604756345, 0.001940280186043014, 0.003066582209295268, 0.005117004325574123, 0.003802699767453136, 0.003051374906981648, 0.0024446436976832894, 0.0022823058139735176, 0.002969293837215742, 0.007138849906960934, 0.0025020240465130693, 0.0025345770697619394, 0.003638167116277632, 0.003023250093022842, 0.003164358813951684, 0.005416557813945008, 0.003415109906961524, 0.002664447441866498, 0.006459087790697299, 0.0029809407674398496, 0.003967001883718822, 0.0032243026511592733, 0.003058377627894515, 0.003358106279064973, 0.0029635732558152845, 0.002973764767460146, 0.002395400465100592, 0.0029433741627906064, 0.00270151725582637, 0.0025812489069679478, 0.005723464139538369, 0.002906443023252799, 0.0035218497441958387, 0.004785180930221666, 0.002452273418617382, 0.0026746443953354257, 0.003961005790695835, 0.0032050590000010765, 0.004290963023269669, 0.00306960306976222, 0.0025944013953470182, 0.0021503656511582493, 0.0021674322093150652, 0.014109136162788187, 0.0034345778604738583, 0.003448077441854091, 0.0030240036976677, 0.007167233790697051, 0.003373052162799489, 0.003589748651160504, 0.0018634445348991078, 0.0025933365349014503, 0.0029645737441929077, 0.004071983511621088, 0.0036914863953507997, 0.0030264510232459437, 0.003333852255817091, 0.0027493553023220534, 0.0028087686046620905, 0.0035269100930405374, 0.005301768186041574, 0.003213673325580055, 0.0037944360930301036, 0.0028974783953408193, 0.0038121344418763077, 0.005613634372080966, 0.004105867604647396, 0.0034310678837395836, 0.0023017921162876475, 0.00259746313954671, 0.0021340639767554103, 0.00430576262789505, 0.0030415269302254675, 0.002618383860461235, 0.0042015010930151414, 0.0030504856744047112, 0.005098014790704372, 0.0032550878837186202, 0.003117769976745686, 0.0026225266279060484, 0.004897657139524601, 0.0034097670232493033, 0.002095073348839854, 0.002686339488359091, 0.001995723674405971, 0.0029372900930126023, 0.0034095307209163154, 0.002214560767454312, 0.0020274492558095265, 0.01198988341859022, 0.005806755465105092, 0.004102218720935377, 0.0027193921860300687, 0.0028011738372168663, 0.0037130163255760847, 0.003071123465117633, 0.0032965589302135457, 0.0033647810697550805, 0.002578541232566728, 0.004567222767438452, 0.0054385296976803615, 0.0030224377906897856, 0.0038025120697649222, 0.0030306005814012215, 0.002533841162784124, 0.0036713140930365877, 0.0029043093255855046, 0.006482176046500722, 0.0025455595116296156, 0.0032632554651193693, 0.002896359837207054, 0.00333856069768402, 0.0053735906976726255, 0.0036264715116103192, 0.0028520234418676917, 0.002917695000009147, 0.011269623000009952, 0.0034309896744288866, 0.002357592860472471, 0.00484829397674032, 0.002898714325584838, 0.0027223702790738465, 0.0022312811627903844, 0.0031481329302381665, 0.003634308999988757, 0.0031243920930348903, 0.008393862418606025, 0.0030386292558242257, 0.008197300372092625, 0.003050141720936957, 0.0029092162325638172, 0.0027782537441918636, 0.005337730046512449, 0.0030284896976682345, 0.003917958976735251, 0.002862429162802115, 0.0026960756511598023, 0.00615331781396932, 0.0029878937441956857, 0.004518359186032856, 0.0028706998139378537, 0.004320408581385659, 0.0031192529302347264, 0.0033738252325557095, 0.003166582534885133, 0.0030685634418511844, 0.0034594831860547772, 0.0037641591395376327, 0.007057407930238527, 0.0035141674883731, 0.004565012302326829, 0.0024452398837279043, 0.0026024686279248104, 0.0032217797441812445, 0.0018595756511659116, 0.002355075883738315, 0.002284871558152, 0.0027929796511713457, 0.002519140279070788, 0.002613126604659936, 0.004909568976747619, 0.003049147000003655, 0.004025621697660109, 0.003049202395358893, 0.0032397168837167895, 0.002773149348836024, 0.002931349162789174, 0.002795987651171704, 0.003647625418611718, 0.0025063761162908674, 0.0025271631627833283, 0.0039376966046485145, 0.0031101047907002286, 0.002725883511629945, 0.004174642744188997, 0.005231150953477481, 0.007754804186037671, 0.004970650581399664, 0.002863275976736521, 0.00357699223255876, 0.0036857695581285193, 0.008433225418610713, 0.005000180255825229, 0.003227161720936893, 0.003792931046518788, 0.0033112920465248673, 0.002708793046516549, 0.0031102637907002477, 0.003543149255820872, 0.003966001441873485, 0.0026830982790691685, 0.002780446255814621, 0.0023977989999895174, 0.003503278674437498, 0.0031736324418637006, 0.0027927097441935857, 0.004380593395348284, 0.002526367093027105, 0.0028891022558064135, 0.0024647409069752197, 0.0035343765348866433, 0.008558998697671362, 0.0028203047209307292, 0.00255400411627539, 0.0030410529302465623, 0.0022444065581471795, 0.0032557084883634625, 0.002397082581410359, 0.0026058566046590704, 0.004615798023266063, 0.006166456302327238, 0.003283629953478149, 0.0030501133023155785, 0.003136498627903821, 0.004150104395363129, 0.0027429570464994678, 0.002995402186041797, 0.003616140348830688, 0.003475747209301707, 0.0034496500930101827, 0.0026536428139542993, 0.0028016631162837407, 0.0028964216278963735, 0.0025493598372291716, 0.0024883660232664496, 0.002049509976745338, 0.003340624720929258, 0.0031180275581243412, 0.003729187465111419, 0.0036695261627713526, 0.0030304052325782534, 0.0042434069302417325, 0.009045485488376323, 0.0027993188372166453]
[349.929851392096, 418.54419409096045, 517.0988671071051, 334.4443168146065, 109.12671699369263, 190.15477622033032, 222.6371166091049, 429.09019304119715, 370.66091174183856, 529.3918028149765, 361.984026220014, 394.1327782284624, 375.9543076756798, 348.80451809689856, 362.2329057149522, 96.58648861276774, 334.0802025292767, 288.46465761216376, 376.088392328034, 386.8837101863902, 314.4308600629633, 264.60713631068114, 135.49258212770894, 248.08350838283343, 189.67752584157464, 354.1685619139499, 418.23376263906533, 123.38920309383697, 271.83896906828886, 292.385343892745, 266.9584883031506, 226.74807154410775, 254.36414335347314, 274.1076026990308, 311.78898516064135, 359.39637089522967, 328.4397315491002, 123.44938138838265, 401.8625412899707, 383.8910392250262, 477.01892017963314, 315.713676123882, 115.81180078207332, 347.00507225230575, 343.75923103461287, 334.79844506957943, 574.5421086484338, 417.22244982583595, 325.426033193376, 114.36838994110032, 269.84460609263886, 294.660111891194, 187.48545815912203, 291.4733478158305, 337.7306173865661, 346.5122023798073, 348.67226695600584, 419.4372488448557, 515.3894819899125, 326.09593734968223, 195.42684281155087, 262.97106296923135, 327.7211193262311, 409.0575657089285, 438.153377114257, 336.7804113781085, 140.07858591128553, 399.67641453871875, 394.543141706054, 274.86367944063653, 330.76985668762023, 316.01978751303164, 184.61909469986367, 292.8163447863133, 375.3123384184603, 154.8206236552861, 335.4645657246117, 252.07953747240504, 310.14458262485306, 326.97074124506725, 297.78688251595145, 337.43049814535397, 336.27407619537, 417.46673033145885, 339.74613647213044, 370.1623588904706, 387.40936501727975, 174.71936149505706, 344.06316999836855, 283.94169900292974, 208.97851399605838, 407.7848711355403, 373.88147812995174, 252.4611305413741, 312.00673684935725, 233.04791828245826, 325.7750195296302, 385.4453677805872, 465.0371900524783, 461.375444963057, 70.87606133091457, 291.15659642144, 290.01668810033533, 330.68742633193943, 139.5238426989759, 296.46739858598653, 278.5710357957004, 536.6406036089197, 385.6036370682608, 337.31662164209234, 245.58056218697516, 270.89358943850874, 330.4200174789134, 299.9533042459048, 363.72163290623763, 356.0279043065939, 283.53430442506783, 188.61631910515948, 311.17039558446845, 263.54377184975465, 345.1276812306909, 262.32023430627135, 178.137715019958, 243.55388343942425, 291.4544491349678, 434.44409811117504, 384.9910263498532, 468.589513197435, 232.24689478269454, 328.78222778907644, 381.91497247613165, 238.01017252202254, 327.8166517517397, 196.1547859420459, 307.21136747238836, 320.7420712427911, 381.31166690896407, 204.1792578598237, 293.2751690017402, 477.31025768322127, 372.253769240028, 501.0713721666156, 340.4498596780953, 293.2954948507544, 451.5568119404205, 493.2305936311668, 83.40364664843261, 172.21321028745987, 243.77052225337772, 367.7292319721856, 356.9931957502359, 269.32281259087847, 325.61374082096603, 303.3466172361802, 297.19615608536134, 387.81617581681303, 218.95143962089998, 183.87322596151668, 330.8587535135929, 262.9840436145734, 329.9675998668364, 394.65772941395585, 272.38203396890185, 344.31594155295477, 154.26918257485937, 392.84094338844204, 306.4424500897665, 345.2609676304224, 299.53027383737736, 186.09530503190604, 275.7501325457688, 350.62825407393376, 342.73630382780414, 88.73411293342438, 291.4610928307318, 424.1614473669537, 206.25811982472553, 344.98052849628164, 367.32696051185263, 448.17301229282333, 317.64859431280325, 275.15546972012936, 320.0622617850265, 119.1346665134012, 329.095758583668, 121.99138187060453, 327.85361845180597, 343.7351919072464, 359.93832532056183, 187.34555537393223, 330.19759016183656, 255.23493378516125, 349.3536234870773, 370.909473393233, 162.51395267278258, 334.6839230620603, 221.3192795940611, 348.3471156213509, 231.45959025923332, 320.5895842261016, 296.39946679825175, 315.7978637800688, 325.8853919594134, 289.0605174874135, 265.6635819395335, 141.6950826542631, 284.56241864071046, 219.0574600402042, 408.95783135822415, 384.2505493706538, 310.3874502302861, 537.7570949442271, 424.61476800172414, 437.66136281585926, 358.04056058217634, 396.96082362227986, 382.68333352724665, 203.68386812287085, 327.96057389125593, 248.40883597712363, 327.9546157782351, 308.6689472855241, 360.60084554037115, 341.1398453292755, 357.65537075277626, 274.1509571946669, 398.98241668528135, 395.7006079886964, 253.95557362633878, 321.53257439755066, 366.8535341783732, 239.54145570707246, 191.16252023567316, 128.95232116891808, 201.18090853982628, 349.25030214508735, 279.5644874198291, 271.3137607300003, 118.57859245564191, 199.99279002691875, 309.8698133137515, 263.6483468155362, 301.99692022015375, 369.16810654323666, 321.5161373096457, 282.2347939074673, 252.14312567864664, 372.70345547943333, 359.6544971544569, 417.04913547981783, 285.44688930878857, 315.09635041818353, 358.075164122993, 228.27957533376457, 395.8252950491828, 346.1282818876473, 405.722158126235, 282.93533247783193, 116.83609675884973, 354.57161510901904, 391.54204710458396, 328.83347410823336, 445.5520753893751, 307.1528067006598, 417.17377939129454, 383.75097010790125, 216.64726120152383, 162.16769421078962, 304.5410153299281, 327.85667314090335, 318.8268571532321, 240.95779400568577, 364.57005452425483, 333.8449857117272, 276.5379392211254, 287.7079199902183, 289.8844732183822, 376.8404680318901, 356.93085088918457, 345.25360202005004, 392.25533618152247, 401.8701391394628, 487.92150872474383, 299.3452074203148, 320.7155746248609, 268.15492901752594, 272.5147486739175, 329.9888705475885, 235.65969901996493, 110.55238563867304, 357.2297612923211]
Elapsed: 0.15694286046393457~0.07325993146522916
Time per graph: 0.0036426400405164443~0.0016978151807501398
Speed: 311.2368645775889~90.52112906508974
Total Time: 0.1214
best val loss: 0.3024309920993718 test_score: 0.8605

Testing...
Test loss: 0.4569 score: 0.8837 time: 0.13s
test Score 0.8837
Epoch Time List: [1.0991551069992056, 0.45861375000004045, 0.5094064380000418, 0.8094554159997642, 0.82464784900003, 0.7140949290005665, 0.9736223429999882, 0.5261049140008254, 0.6817055850006, 0.5548772789998111, 0.5465628109996032, 0.5299621449994447, 1.173014386999057, 0.7663024109997423, 0.5394408260008277, 0.9711991019994457, 0.7058600190002835, 0.5860561529989354, 0.5183480380001129, 0.6396399600007499, 0.7342172420003408, 0.6822621069995876, 0.7714736440002525, 0.6893293530010851, 0.7133278649998829, 0.5952190570005769, 0.5604243259995201, 0.8785209330008001, 0.6195646320011292, 0.6231448030002866, 0.5891079950006315, 0.9344381430000794, 0.6584436740004094, 0.6975364800000534, 1.092981900000268, 0.5896785710001495, 0.6274951390005299, 0.7905592509996495, 0.5649235209994004, 0.5679959279996183, 0.4998194899999362, 0.6052234289991247, 0.9021041659998446, 0.5979105699998399, 0.6744487649993971, 0.594457216999217, 0.8048161900005653, 0.5542549210003926, 0.5212413649996961, 0.7512942499997735, 0.6006252749994019, 0.6065416159999586, 0.7018283770003109, 0.9571862430011606, 0.6412613630009218, 0.7338618950007003, 0.5112059810016945, 0.6786549160005961, 0.463107577998926, 0.5449185429997669, 0.7250005260011676, 0.847462380998877, 0.5958663030005482, 0.662384234999081, 0.877650633000485, 0.5488299800008463, 0.6406905369985907, 0.6560936260002563, 0.5637207540003146, 0.9106415959986407, 0.8676369300010265, 0.5766257630002656, 1.2368079999996553, 0.732760471999427, 0.49896211699979176, 0.9031777339996552, 0.8228405700010626, 0.5934736949993749, 0.5876214909994815, 0.9889683110004626, 0.5534509930002969, 0.5315135060009197, 0.6560255759995925, 0.7086664170001313, 0.5522003859996403, 0.6074850379991403, 0.835336102999463, 1.0178307259993744, 0.8547253970000384, 0.6922717970001031, 0.6530235640011597, 0.8088627829993129, 0.5052298279997558, 0.49880226499954006, 0.9921614530003353, 0.6759266070002923, 0.6564802510001755, 0.8815925420003623, 0.49756939700091607, 0.5127735009991738, 1.1156336479998572, 0.6054267570007141, 0.6569153570007984, 0.6100126949995683, 1.2471700280002551, 0.7650860900002954, 0.7411232179983926, 0.5416152830002829, 0.6800975649994143, 0.5890827670000363, 0.6443177059991285, 0.6036362869990626, 0.9701432169995314, 0.857490074999987, 0.48624129200106836, 0.5354059180008335, 0.4709830579995469, 0.8692944919994261, 0.8614337770013663, 0.6807813069999611, 0.8587729250002667, 0.6453928750006526, 0.7614961500003119, 0.859557455999493, 0.6334621019996121, 0.6855672410001716, 0.7176854439994713, 0.41468889200041303, 0.6239003169994248, 0.5979407599979822, 0.8277541420011403, 0.6355081659994539, 0.6029294410000148, 0.6521183679988098, 0.7992721060008989, 0.6555995929993514, 0.6010939779998807, 0.7040704609989916, 0.8337119040006655, 0.5888836510002875, 0.5147867960004078, 0.5085937720004949, 0.7787823399994522, 0.6864606339995589, 0.4496721099994829, 0.4827363200001855, 0.8648025120010061, 0.7050564219998705, 0.8635539099996095, 0.5479713119984808, 0.7130936590001511, 0.6777498029996423, 0.757504137000069, 0.5937225010011389, 0.8562066499998764, 0.6427984259989898, 0.7394418119993134, 0.9359689810016789, 0.5274804450000374, 0.5734830409992355, 0.6650554679990819, 0.6401641439997547, 0.570906007999838, 0.5435668299996905, 0.897279964999143, 0.9138036029999057, 0.6127496999997675, 0.6905399950001083, 0.6136449409996203, 0.7150083259994062, 1.0318600529999458, 0.5904761289993985, 0.6247724380000363, 0.8533977360002609, 0.6846592850006346, 0.519955461999416, 0.6662220039997919, 0.764045353000256, 0.5290330420002647, 0.4612064069997359, 0.5207737709997673, 1.1381456369999796, 0.7637702239990176, 0.8728872900010174, 0.6050921460000609, 0.8349927440012834, 0.5728667569992467, 0.9016261750002741, 0.6924300279997624, 0.6915541260004829, 0.5889692490000016, 0.7772651400000541, 0.6294243210004424, 0.5928495950001889, 0.7083582090008349, 0.532041842999206, 0.5302281310005128, 0.572432179000316, 0.788057206999838, 0.5822441689997504, 0.5828326899991225, 0.5738758990000861, 0.863535284000136, 0.7633723119988645, 0.7684675569998944, 0.7703842350001651, 0.670460409000043, 0.6814639190006346, 0.5704430460000367, 0.5301412029994026, 0.5092110740006319, 0.686809223999262, 0.4801396350003415, 0.486165420999896, 0.5341691070007073, 0.5229030180007612, 0.873509516998638, 0.7412484779997612, 0.5517074410008718, 0.8693716529987796, 0.6550700470006632, 0.6338242430001628, 0.6281473160006499, 0.7828975879992868, 0.6391180759992494, 0.6661979579994295, 0.7116481089997251, 0.7677846160004265, 0.5221212530004777, 0.5685463930003607, 0.562057998000455, 0.5626299159994232, 0.7671581240001615, 0.8002740709989666, 0.7160165780005627, 0.7350804590005282, 0.6736454310002955, 0.8315077539991762, 0.9381583060003322, 0.6251536290001241, 0.6667407499999172, 0.6512008930012598, 0.8824220669994247, 0.6814425309994476, 0.5192781499990815, 0.5213943750004546, 1.0249729039996964, 0.5652742789998229, 0.5310257210003329, 0.7145076980004887, 0.692506866000258, 0.6294283529996392, 0.62098590199912, 0.9088260339995031, 0.773048481000842, 0.8270118370001001, 0.5349150650008596, 0.6434517899997445, 1.009489358000792, 0.6643313230006243, 0.5884334749989648, 0.6583616079997228, 0.41828616700058774, 0.7719678389994442, 0.5394454070001302, 0.7210975419993702, 0.9483605279992844, 0.8181114910003089, 0.6165382109993516, 0.8539653530006035, 0.5813338699990709, 0.7307588740004576, 0.5609846979996291, 0.8390842220005652, 0.7545398559996102, 0.6736173240005883, 0.7044623219999266, 0.9651339150004787, 0.5638835010004186, 0.5036552480005412, 0.6138901850008551, 0.7905393510009162, 0.5189635059996363, 0.7993627169998945, 0.576552442999855, 0.8545925979988169, 0.6441750879994288, 0.6048022590002802, 0.6579560910004147, 0.7828633080007421, 0.6149652049998622]
Total Epoch List: [24, 105, 162]
Total Time List: [0.17772020600023097, 0.18637990299976082, 0.12142610400042031]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049c970>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7087 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7061 score: 0.5000 time: 0.10s
Epoch 2/1000, LR 0.000015
Train loss: 0.7053;  Loss pred: 0.7053; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7075 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7050 score: 0.5000 time: 0.12s
Epoch 3/1000, LR 0.000045
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7058 score: 0.4884 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7037 score: 0.5000 time: 0.13s
Epoch 4/1000, LR 0.000075
Train loss: 0.7245;  Loss pred: 0.7245; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7046 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7028 score: 0.5000 time: 0.11s
Epoch 5/1000, LR 0.000105
Train loss: 0.7187;  Loss pred: 0.7187; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7034 score: 0.4884 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000135
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7029 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.13s
Epoch 7/1000, LR 0.000165
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7022 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7014 score: 0.5000 time: 0.36s
Epoch 8/1000, LR 0.000195
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7020 score: 0.4884 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.5000 time: 0.15s
Epoch 9/1000, LR 0.000225
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4884 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7025 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7042 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7050 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7063 score: 0.5000 time: 0.30s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7055 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7074 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.6164;  Loss pred: 0.6164; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7043 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7070 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7065 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.4884 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7034 score: 0.5000 time: 0.12s
Epoch 16/1000, LR 0.000285
Train loss: 0.5896;  Loss pred: 0.5896; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5000 time: 0.15s
Epoch 17/1000, LR 0.000285
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.13s
Epoch 18/1000, LR 0.000285
Train loss: 0.5637;  Loss pred: 0.5637; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.12s
Epoch 19/1000, LR 0.000285
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6729 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.5000 time: 0.11s
Epoch 20/1000, LR 0.000285
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.24s
Val loss: 0.6642 score: 0.5116 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6764 score: 0.5000 time: 0.11s
Epoch 21/1000, LR 0.000285
Train loss: 0.5375;  Loss pred: 0.5375; Loss self: 0.0000; time: 0.27s
Val loss: 0.6542 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6683 score: 0.5000 time: 0.10s
Epoch 22/1000, LR 0.000285
Train loss: 0.5394;  Loss pred: 0.5394; Loss self: 0.0000; time: 0.26s
Val loss: 0.6453 score: 0.5116 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6611 score: 0.5000 time: 0.11s
Epoch 23/1000, LR 0.000285
Train loss: 0.5181;  Loss pred: 0.5181; Loss self: 0.0000; time: 0.32s
Val loss: 0.6343 score: 0.5581 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6515 score: 0.5000 time: 0.10s
Epoch 24/1000, LR 0.000285
Train loss: 0.4961;  Loss pred: 0.4961; Loss self: 0.0000; time: 0.34s
Val loss: 0.6240 score: 0.5814 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6420 score: 0.5000 time: 0.34s
Epoch 25/1000, LR 0.000285
Train loss: 0.4854;  Loss pred: 0.4854; Loss self: 0.0000; time: 0.33s
Val loss: 0.6127 score: 0.5814 time: 0.14s
Test loss: 0.6315 score: 0.5455 time: 0.13s
Epoch 26/1000, LR 0.000285
Train loss: 0.4842;  Loss pred: 0.4842; Loss self: 0.0000; time: 0.35s
Val loss: 0.6024 score: 0.6512 time: 0.16s
Test loss: 0.6216 score: 0.5909 time: 0.18s
Epoch 27/1000, LR 0.000285
Train loss: 0.4719;  Loss pred: 0.4719; Loss self: 0.0000; time: 0.40s
Val loss: 0.5925 score: 0.7209 time: 0.13s
Test loss: 0.6120 score: 0.5909 time: 0.14s
Epoch 28/1000, LR 0.000285
Train loss: 0.4661;  Loss pred: 0.4661; Loss self: 0.0000; time: 0.34s
Val loss: 0.5829 score: 0.7674 time: 0.29s
Test loss: 0.6025 score: 0.6364 time: 0.13s
Epoch 29/1000, LR 0.000285
Train loss: 0.4693;  Loss pred: 0.4693; Loss self: 0.0000; time: 0.55s
Val loss: 0.5733 score: 0.7907 time: 0.15s
Test loss: 0.5925 score: 0.7273 time: 0.13s
Epoch 30/1000, LR 0.000285
Train loss: 0.4773;  Loss pred: 0.4773; Loss self: 0.0000; time: 0.27s
Val loss: 0.5652 score: 0.7907 time: 0.18s
Test loss: 0.5842 score: 0.7727 time: 0.14s
Epoch 31/1000, LR 0.000285
Train loss: 0.4812;  Loss pred: 0.4812; Loss self: 0.0000; time: 0.49s
Val loss: 0.5583 score: 0.7907 time: 0.14s
Test loss: 0.5772 score: 0.7955 time: 0.16s
Epoch 32/1000, LR 0.000285
Train loss: 0.4318;  Loss pred: 0.4318; Loss self: 0.0000; time: 0.55s
Val loss: 0.5523 score: 0.8140 time: 0.12s
Test loss: 0.5710 score: 0.8409 time: 0.14s
Epoch 33/1000, LR 0.000285
Train loss: 0.4033;  Loss pred: 0.4033; Loss self: 0.0000; time: 0.38s
Val loss: 0.5457 score: 0.8372 time: 0.11s
Test loss: 0.5639 score: 0.8409 time: 0.16s
Epoch 34/1000, LR 0.000285
Train loss: 0.4180;  Loss pred: 0.4180; Loss self: 0.0000; time: 0.34s
Val loss: 0.5394 score: 0.8605 time: 0.11s
Test loss: 0.5570 score: 0.8636 time: 0.13s
Epoch 35/1000, LR 0.000285
Train loss: 0.4382;  Loss pred: 0.4382; Loss self: 0.0000; time: 0.33s
Val loss: 0.5331 score: 0.8837 time: 0.12s
Test loss: 0.5504 score: 0.9091 time: 0.11s
Epoch 36/1000, LR 0.000285
Train loss: 0.4067;  Loss pred: 0.4067; Loss self: 0.0000; time: 0.46s
Val loss: 0.5262 score: 0.8837 time: 0.12s
Test loss: 0.5440 score: 0.8864 time: 0.11s
Epoch 37/1000, LR 0.000285
Train loss: 0.3853;  Loss pred: 0.3853; Loss self: 0.0000; time: 0.30s
Val loss: 0.5198 score: 0.8837 time: 0.08s
Test loss: 0.5380 score: 0.8636 time: 0.21s
Epoch 38/1000, LR 0.000284
Train loss: 0.3950;  Loss pred: 0.3950; Loss self: 0.0000; time: 0.37s
Val loss: 0.5138 score: 0.8837 time: 0.25s
Test loss: 0.5325 score: 0.8636 time: 0.13s
Epoch 39/1000, LR 0.000284
Train loss: 0.3845;  Loss pred: 0.3845; Loss self: 0.0000; time: 0.23s
Val loss: 0.5078 score: 0.8837 time: 0.11s
Test loss: 0.5271 score: 0.8636 time: 0.16s
Epoch 40/1000, LR 0.000284
Train loss: 0.3881;  Loss pred: 0.3881; Loss self: 0.0000; time: 0.29s
Val loss: 0.5006 score: 0.9070 time: 0.32s
Test loss: 0.5211 score: 0.8636 time: 0.13s
Epoch 41/1000, LR 0.000284
Train loss: 0.3726;  Loss pred: 0.3726; Loss self: 0.0000; time: 0.27s
Val loss: 0.4941 score: 0.9070 time: 0.12s
Test loss: 0.5156 score: 0.8636 time: 0.26s
Epoch 42/1000, LR 0.000284
Train loss: 0.3696;  Loss pred: 0.3696; Loss self: 0.0000; time: 0.34s
Val loss: 0.4867 score: 0.8837 time: 0.17s
Test loss: 0.5090 score: 0.8636 time: 0.17s
Epoch 43/1000, LR 0.000284
Train loss: 0.3784;  Loss pred: 0.3784; Loss self: 0.0000; time: 0.29s
Val loss: 0.4801 score: 0.8605 time: 0.17s
Test loss: 0.5033 score: 0.8636 time: 0.13s
Epoch 44/1000, LR 0.000284
Train loss: 0.3477;  Loss pred: 0.3477; Loss self: 0.0000; time: 0.51s
Val loss: 0.4732 score: 0.8837 time: 0.12s
Test loss: 0.4971 score: 0.8636 time: 0.13s
Epoch 45/1000, LR 0.000284
Train loss: 0.3573;  Loss pred: 0.3573; Loss self: 0.0000; time: 0.31s
Val loss: 0.4674 score: 0.8837 time: 0.10s
Test loss: 0.4920 score: 0.8636 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.3503;  Loss pred: 0.3503; Loss self: 0.0000; time: 0.33s
Val loss: 0.4603 score: 0.8837 time: 0.14s
Test loss: 0.4859 score: 0.8636 time: 0.18s
Epoch 47/1000, LR 0.000284
Train loss: 0.3366;  Loss pred: 0.3366; Loss self: 0.0000; time: 0.32s
Val loss: 0.4520 score: 0.8837 time: 0.15s
Test loss: 0.4791 score: 0.8636 time: 0.25s
Epoch 48/1000, LR 0.000284
Train loss: 0.3553;  Loss pred: 0.3553; Loss self: 0.0000; time: 0.30s
Val loss: 0.4451 score: 0.8837 time: 0.29s
Test loss: 0.4738 score: 0.8636 time: 0.11s
Epoch 49/1000, LR 0.000284
Train loss: 0.3398;  Loss pred: 0.3398; Loss self: 0.0000; time: 0.31s
Val loss: 0.4391 score: 0.8605 time: 0.13s
Test loss: 0.4698 score: 0.8636 time: 0.14s
Epoch 50/1000, LR 0.000284
Train loss: 0.3280;  Loss pred: 0.3280; Loss self: 0.0000; time: 0.32s
Val loss: 0.4339 score: 0.8605 time: 0.12s
Test loss: 0.4670 score: 0.8636 time: 0.13s
Epoch 51/1000, LR 0.000284
Train loss: 0.3122;  Loss pred: 0.3122; Loss self: 0.0000; time: 0.51s
Val loss: 0.4297 score: 0.8372 time: 0.33s
Test loss: 0.4649 score: 0.8636 time: 0.14s
Epoch 52/1000, LR 0.000284
Train loss: 0.3188;  Loss pred: 0.3188; Loss self: 0.0000; time: 0.31s
Val loss: 0.4255 score: 0.8372 time: 0.14s
Test loss: 0.4629 score: 0.8636 time: 0.09s
Epoch 53/1000, LR 0.000284
Train loss: 0.3165;  Loss pred: 0.3165; Loss self: 0.0000; time: 0.34s
Val loss: 0.4221 score: 0.8372 time: 0.09s
Test loss: 0.4614 score: 0.8636 time: 0.12s
Epoch 54/1000, LR 0.000284
Train loss: 0.3008;  Loss pred: 0.3008; Loss self: 0.0000; time: 0.28s
Val loss: 0.4178 score: 0.8372 time: 0.34s
Test loss: 0.4594 score: 0.8864 time: 0.09s
Epoch 55/1000, LR 0.000284
Train loss: 0.2976;  Loss pred: 0.2976; Loss self: 0.0000; time: 0.19s
Val loss: 0.4145 score: 0.8372 time: 0.15s
Test loss: 0.4583 score: 0.8864 time: 0.12s
Epoch 56/1000, LR 0.000284
Train loss: 0.2939;  Loss pred: 0.2939; Loss self: 0.0000; time: 0.32s
Val loss: 0.4101 score: 0.8372 time: 0.10s
Test loss: 0.4565 score: 0.8864 time: 0.12s
Epoch 57/1000, LR 0.000283
Train loss: 0.2832;  Loss pred: 0.2832; Loss self: 0.0000; time: 0.39s
Val loss: 0.4060 score: 0.8372 time: 0.18s
Test loss: 0.4546 score: 0.8864 time: 0.16s
Epoch 58/1000, LR 0.000283
Train loss: 0.2916;  Loss pred: 0.2916; Loss self: 0.0000; time: 0.53s
Val loss: 0.4029 score: 0.8372 time: 0.13s
Test loss: 0.4524 score: 0.9091 time: 0.26s
Epoch 59/1000, LR 0.000283
Train loss: 0.2841;  Loss pred: 0.2841; Loss self: 0.0000; time: 0.26s
Val loss: 0.3977 score: 0.8372 time: 0.11s
Test loss: 0.4488 score: 0.8864 time: 0.10s
Epoch 60/1000, LR 0.000283
Train loss: 0.2768;  Loss pred: 0.2768; Loss self: 0.0000; time: 0.29s
Val loss: 0.3929 score: 0.8372 time: 0.10s
Test loss: 0.4454 score: 0.8864 time: 0.15s
Epoch 61/1000, LR 0.000283
Train loss: 0.2714;  Loss pred: 0.2714; Loss self: 0.0000; time: 0.35s
Val loss: 0.3904 score: 0.8605 time: 0.14s
Test loss: 0.4437 score: 0.9091 time: 0.12s
Epoch 62/1000, LR 0.000283
Train loss: 0.2606;  Loss pred: 0.2606; Loss self: 0.0000; time: 0.59s
Val loss: 0.3859 score: 0.8837 time: 0.11s
Test loss: 0.4408 score: 0.8864 time: 0.13s
Epoch 63/1000, LR 0.000283
Train loss: 0.2536;  Loss pred: 0.2536; Loss self: 0.0000; time: 0.32s
Val loss: 0.3839 score: 0.8837 time: 0.13s
Test loss: 0.4392 score: 0.9091 time: 0.21s
Epoch 64/1000, LR 0.000283
Train loss: 0.2615;  Loss pred: 0.2615; Loss self: 0.0000; time: 0.35s
Val loss: 0.3842 score: 0.8837 time: 0.12s
Test loss: 0.4393 score: 0.9091 time: 0.27s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.2532;  Loss pred: 0.2532; Loss self: 0.0000; time: 0.46s
Val loss: 0.3835 score: 0.8605 time: 0.37s
Test loss: 0.4389 score: 0.9091 time: 0.13s
Epoch 66/1000, LR 0.000283
Train loss: 0.2580;  Loss pred: 0.2580; Loss self: 0.0000; time: 0.30s
Val loss: 0.3840 score: 0.8605 time: 0.14s
Test loss: 0.4396 score: 0.8864 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.2651;  Loss pred: 0.2651; Loss self: 0.0000; time: 0.41s
Val loss: 0.3859 score: 0.8605 time: 0.12s
Test loss: 0.4407 score: 0.8864 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.2434;  Loss pred: 0.2434; Loss self: 0.0000; time: 0.33s
Val loss: 0.3889 score: 0.8605 time: 0.35s
Test loss: 0.4424 score: 0.8864 time: 0.37s
     INFO: Early stopping counter 3 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.2539;  Loss pred: 0.2539; Loss self: 0.0000; time: 0.36s
Val loss: 0.3907 score: 0.8605 time: 0.13s
Test loss: 0.4428 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.2496;  Loss pred: 0.2496; Loss self: 0.0000; time: 0.31s
Val loss: 0.3932 score: 0.8605 time: 0.15s
Test loss: 0.4450 score: 0.8864 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.2449;  Loss pred: 0.2449; Loss self: 0.0000; time: 0.47s
Val loss: 0.3929 score: 0.8605 time: 0.10s
Test loss: 0.4458 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.2588;  Loss pred: 0.2588; Loss self: 0.0000; time: 0.32s
Val loss: 0.3900 score: 0.8605 time: 0.09s
Test loss: 0.4448 score: 0.8864 time: 0.40s
     INFO: Early stopping counter 7 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.2334;  Loss pred: 0.2334; Loss self: 0.0000; time: 0.27s
Val loss: 0.3866 score: 0.8605 time: 0.13s
Test loss: 0.4424 score: 0.8864 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.2337;  Loss pred: 0.2337; Loss self: 0.0000; time: 0.31s
Val loss: 0.3844 score: 0.8605 time: 0.32s
Test loss: 0.4415 score: 0.8864 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.2230;  Loss pred: 0.2230; Loss self: 0.0000; time: 0.27s
Val loss: 0.3815 score: 0.8605 time: 0.13s
Test loss: 0.4403 score: 0.9091 time: 0.13s
Epoch 76/1000, LR 0.000282
Train loss: 0.2305;  Loss pred: 0.2305; Loss self: 0.0000; time: 0.28s
Val loss: 0.3789 score: 0.8605 time: 0.11s
Test loss: 0.4389 score: 0.9091 time: 0.29s
Epoch 77/1000, LR 0.000282
Train loss: 0.2151;  Loss pred: 0.2151; Loss self: 0.0000; time: 0.31s
Val loss: 0.3761 score: 0.8837 time: 0.12s
Test loss: 0.4373 score: 0.9091 time: 0.12s
Epoch 78/1000, LR 0.000282
Train loss: 0.2180;  Loss pred: 0.2180; Loss self: 0.0000; time: 0.29s
Val loss: 0.3708 score: 0.9070 time: 0.11s
Test loss: 0.4330 score: 0.9091 time: 0.15s
Epoch 79/1000, LR 0.000282
Train loss: 0.2224;  Loss pred: 0.2224; Loss self: 0.0000; time: 0.33s
Val loss: 0.3673 score: 0.9070 time: 0.15s
Test loss: 0.4306 score: 0.9091 time: 0.14s
Epoch 80/1000, LR 0.000282
Train loss: 0.2145;  Loss pred: 0.2145; Loss self: 0.0000; time: 0.40s
Val loss: 0.3657 score: 0.9070 time: 0.37s
Test loss: 0.4294 score: 0.8864 time: 0.13s
Epoch 81/1000, LR 0.000281
Train loss: 0.2140;  Loss pred: 0.2140; Loss self: 0.0000; time: 0.34s
Val loss: 0.3660 score: 0.9070 time: 0.13s
Test loss: 0.4297 score: 0.8864 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.2100;  Loss pred: 0.2100; Loss self: 0.0000; time: 0.26s
Val loss: 0.3653 score: 0.9070 time: 0.14s
Test loss: 0.4294 score: 0.8636 time: 0.13s
Epoch 83/1000, LR 0.000281
Train loss: 0.2071;  Loss pred: 0.2071; Loss self: 0.0000; time: 0.29s
Val loss: 0.3640 score: 0.9070 time: 0.14s
Test loss: 0.4294 score: 0.8636 time: 0.11s
Epoch 84/1000, LR 0.000281
Train loss: 0.1999;  Loss pred: 0.1999; Loss self: 0.0000; time: 0.57s
Val loss: 0.3634 score: 0.9070 time: 0.13s
Test loss: 0.4299 score: 0.8636 time: 0.12s
Epoch 85/1000, LR 0.000281
Train loss: 0.2157;  Loss pred: 0.2157; Loss self: 0.0000; time: 0.31s
Val loss: 0.3631 score: 0.9070 time: 0.18s
Test loss: 0.4301 score: 0.8636 time: 0.15s
Epoch 86/1000, LR 0.000281
Train loss: 0.2012;  Loss pred: 0.2012; Loss self: 0.0000; time: 0.35s
Val loss: 0.3630 score: 0.9070 time: 0.14s
Test loss: 0.4309 score: 0.8864 time: 0.18s
Epoch 87/1000, LR 0.000281
Train loss: 0.1897;  Loss pred: 0.1897; Loss self: 0.0000; time: 0.37s
Val loss: 0.3618 score: 0.9070 time: 0.38s
Test loss: 0.4309 score: 0.8864 time: 0.16s
Epoch 88/1000, LR 0.000281
Train loss: 0.1987;  Loss pred: 0.1987; Loss self: 0.0000; time: 0.31s
Val loss: 0.3624 score: 0.9070 time: 0.12s
Test loss: 0.4316 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.2003;  Loss pred: 0.2003; Loss self: 0.0000; time: 0.31s
Val loss: 0.3629 score: 0.9070 time: 0.11s
Test loss: 0.4311 score: 0.8864 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.22s
Val loss: 0.3602 score: 0.9070 time: 0.19s
Test loss: 0.4297 score: 0.8864 time: 0.11s
Epoch 91/1000, LR 0.000280
Train loss: 0.1818;  Loss pred: 0.1818; Loss self: 0.0000; time: 0.35s
Val loss: 0.3589 score: 0.9070 time: 0.12s
Test loss: 0.4287 score: 0.8864 time: 0.34s
Epoch 92/1000, LR 0.000280
Train loss: 0.1870;  Loss pred: 0.1870; Loss self: 0.0000; time: 0.27s
Val loss: 0.3596 score: 0.9070 time: 0.14s
Test loss: 0.4292 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1848;  Loss pred: 0.1848; Loss self: 0.0000; time: 0.26s
Val loss: 0.3595 score: 0.9070 time: 0.10s
Test loss: 0.4286 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.1711;  Loss pred: 0.1711; Loss self: 0.0000; time: 0.25s
Val loss: 0.3609 score: 0.9070 time: 0.13s
Test loss: 0.4285 score: 0.8864 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1806;  Loss pred: 0.1806; Loss self: 0.0000; time: 0.28s
Val loss: 0.3585 score: 0.9070 time: 0.28s
Test loss: 0.4259 score: 0.8864 time: 0.14s
Epoch 96/1000, LR 0.000280
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.37s
Val loss: 0.3584 score: 0.9070 time: 0.13s
Test loss: 0.4256 score: 0.8864 time: 0.13s
Epoch 97/1000, LR 0.000280
Train loss: 0.1795;  Loss pred: 0.1795; Loss self: 0.0000; time: 0.33s
Val loss: 0.3592 score: 0.9070 time: 0.12s
Test loss: 0.4256 score: 0.8864 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.1775;  Loss pred: 0.1775; Loss self: 0.0000; time: 0.46s
Val loss: 0.3562 score: 0.9070 time: 0.32s
Test loss: 0.4232 score: 0.8864 time: 0.12s
Epoch 99/1000, LR 0.000279
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.35s
Val loss: 0.3548 score: 0.9070 time: 0.12s
Test loss: 0.4212 score: 0.8864 time: 0.10s
Epoch 100/1000, LR 0.000279
Train loss: 0.1732;  Loss pred: 0.1732; Loss self: 0.0000; time: 0.35s
Val loss: 0.3539 score: 0.9070 time: 0.14s
Test loss: 0.4203 score: 0.8864 time: 0.13s
Epoch 101/1000, LR 0.000279
Train loss: 0.1802;  Loss pred: 0.1802; Loss self: 0.0000; time: 0.32s
Val loss: 0.3539 score: 0.9070 time: 0.13s
Test loss: 0.4200 score: 0.8864 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1533;  Loss pred: 0.1533; Loss self: 0.0000; time: 0.30s
Val loss: 0.3528 score: 0.9070 time: 0.35s
Test loss: 0.4181 score: 0.8864 time: 0.12s
Epoch 103/1000, LR 0.000279
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.33s
Val loss: 0.3539 score: 0.9070 time: 0.14s
Test loss: 0.4171 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1756;  Loss pred: 0.1756; Loss self: 0.0000; time: 0.34s
Val loss: 0.3535 score: 0.9070 time: 0.12s
Test loss: 0.4154 score: 0.8864 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1528;  Loss pred: 0.1528; Loss self: 0.0000; time: 0.32s
Val loss: 0.3577 score: 0.9070 time: 0.16s
Test loss: 0.4162 score: 0.8864 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1576;  Loss pred: 0.1576; Loss self: 0.0000; time: 0.31s
Val loss: 0.3592 score: 0.9070 time: 0.08s
Test loss: 0.4162 score: 0.8864 time: 0.29s
     INFO: Early stopping counter 4 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1541;  Loss pred: 0.1541; Loss self: 0.0000; time: 0.36s
Val loss: 0.3620 score: 0.8837 time: 0.11s
Test loss: 0.4139 score: 0.8864 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.27s
Val loss: 0.3604 score: 0.9070 time: 0.16s
Test loss: 0.4120 score: 0.8636 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1508;  Loss pred: 0.1508; Loss self: 0.0000; time: 0.31s
Val loss: 0.3551 score: 0.9070 time: 0.13s
Test loss: 0.4092 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1500;  Loss pred: 0.1500; Loss self: 0.0000; time: 0.31s
Val loss: 0.3520 score: 0.9070 time: 0.44s
Test loss: 0.4052 score: 0.8864 time: 0.29s
Epoch 111/1000, LR 0.000278
Train loss: 0.1335;  Loss pred: 0.1335; Loss self: 0.0000; time: 0.21s
Val loss: 0.3547 score: 0.9070 time: 0.11s
Test loss: 0.4041 score: 0.8864 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 0.27s
Val loss: 0.3608 score: 0.8837 time: 0.09s
Test loss: 0.4036 score: 0.8636 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1418;  Loss pred: 0.1418; Loss self: 0.0000; time: 0.33s
Val loss: 0.3625 score: 0.8837 time: 0.09s
Test loss: 0.4030 score: 0.8636 time: 0.37s
     INFO: Early stopping counter 3 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1426;  Loss pred: 0.1426; Loss self: 0.0000; time: 0.32s
Val loss: 0.3687 score: 0.8837 time: 0.18s
Test loss: 0.4041 score: 0.8409 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1312;  Loss pred: 0.1312; Loss self: 0.0000; time: 0.41s
Val loss: 0.3696 score: 0.8837 time: 0.11s
Test loss: 0.4039 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1366;  Loss pred: 0.1366; Loss self: 0.0000; time: 0.33s
Val loss: 0.3609 score: 0.8837 time: 0.11s
Test loss: 0.4031 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1358;  Loss pred: 0.1358; Loss self: 0.0000; time: 0.39s
Val loss: 0.3549 score: 0.8837 time: 0.12s
Test loss: 0.4046 score: 0.8864 time: 0.33s
     INFO: Early stopping counter 7 of 20
Epoch 118/1000, LR 0.000277
Train loss: 0.1340;  Loss pred: 0.1340; Loss self: 0.0000; time: 0.55s
Val loss: 0.3548 score: 0.9070 time: 0.14s
Test loss: 0.4010 score: 0.8864 time: 0.21s
     INFO: Early stopping counter 8 of 20
Epoch 119/1000, LR 0.000277
Train loss: 0.1291;  Loss pred: 0.1291; Loss self: 0.0000; time: 0.30s
Val loss: 0.3638 score: 0.9070 time: 0.23s
Test loss: 0.3976 score: 0.8636 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 120/1000, LR 0.000277
Train loss: 0.1208;  Loss pred: 0.1208; Loss self: 0.0000; time: 0.30s
Val loss: 0.3676 score: 0.8837 time: 0.27s
Test loss: 0.3964 score: 0.8409 time: 0.43s
     INFO: Early stopping counter 10 of 20
Epoch 121/1000, LR 0.000276
Train loss: 0.1281;  Loss pred: 0.1281; Loss self: 0.0000; time: 0.30s
Val loss: 0.3499 score: 0.8837 time: 0.18s
Test loss: 0.3916 score: 0.8636 time: 0.12s
Epoch 122/1000, LR 0.000276
Train loss: 0.1160;  Loss pred: 0.1160; Loss self: 0.0000; time: 0.31s
Val loss: 0.3479 score: 0.8837 time: 0.13s
Test loss: 0.3890 score: 0.8864 time: 0.18s
Epoch 123/1000, LR 0.000276
Train loss: 0.1164;  Loss pred: 0.1164; Loss self: 0.0000; time: 0.59s
Val loss: 0.3511 score: 0.8837 time: 0.13s
Test loss: 0.3870 score: 0.8636 time: 0.32s
     INFO: Early stopping counter 1 of 20
Epoch 124/1000, LR 0.000276
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.29s
Val loss: 0.3517 score: 0.8837 time: 0.11s
Test loss: 0.3838 score: 0.8636 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 125/1000, LR 0.000276
Train loss: 0.1099;  Loss pred: 0.1099; Loss self: 0.0000; time: 0.26s
Val loss: 0.3559 score: 0.8605 time: 0.10s
Test loss: 0.3810 score: 0.8864 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 126/1000, LR 0.000276
Train loss: 0.1037;  Loss pred: 0.1037; Loss self: 0.0000; time: 0.30s
Val loss: 0.3556 score: 0.8605 time: 0.16s
Test loss: 0.3825 score: 0.8864 time: 0.26s
     INFO: Early stopping counter 4 of 20
Epoch 127/1000, LR 0.000275
Train loss: 0.1035;  Loss pred: 0.1035; Loss self: 0.0000; time: 0.31s
Val loss: 0.3498 score: 0.8837 time: 0.17s
Test loss: 0.3806 score: 0.8864 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1094;  Loss pred: 0.1094; Loss self: 0.0000; time: 0.26s
Val loss: 0.3539 score: 0.8605 time: 0.26s
Test loss: 0.3788 score: 0.8636 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 129/1000, LR 0.000275
Train loss: 0.1013;  Loss pred: 0.1013; Loss self: 0.0000; time: 0.26s
Val loss: 0.3524 score: 0.8837 time: 0.13s
Test loss: 0.3804 score: 0.8636 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 130/1000, LR 0.000275
Train loss: 0.1021;  Loss pred: 0.1021; Loss self: 0.0000; time: 0.47s
Val loss: 0.3487 score: 0.8837 time: 0.17s
Test loss: 0.3791 score: 0.8864 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 131/1000, LR 0.000275
Train loss: 0.0987;  Loss pred: 0.0987; Loss self: 0.0000; time: 0.33s
Val loss: 0.3481 score: 0.8837 time: 0.19s
Test loss: 0.3782 score: 0.8864 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 132/1000, LR 0.000275
Train loss: 0.1021;  Loss pred: 0.1021; Loss self: 0.0000; time: 0.70s
Val loss: 0.3583 score: 0.8605 time: 0.11s
Test loss: 0.3784 score: 0.8636 time: 0.12s
     INFO: Early stopping counter 10 of 20
Epoch 133/1000, LR 0.000274
Train loss: 0.0942;  Loss pred: 0.0942; Loss self: 0.0000; time: 0.48s
Val loss: 0.3652 score: 0.8605 time: 0.17s
Test loss: 0.3800 score: 0.8636 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 134/1000, LR 0.000274
Train loss: 0.1111;  Loss pred: 0.1111; Loss self: 0.0000; time: 0.34s
Val loss: 0.3665 score: 0.8837 time: 0.17s
Test loss: 0.3787 score: 0.8636 time: 0.12s
     INFO: Early stopping counter 12 of 20
Epoch 135/1000, LR 0.000274
Train loss: 0.0938;  Loss pred: 0.0938; Loss self: 0.0000; time: 0.31s
Val loss: 0.3713 score: 0.8372 time: 0.11s
Test loss: 0.3799 score: 0.8636 time: 0.48s
     INFO: Early stopping counter 13 of 20
Epoch 136/1000, LR 0.000274
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.29s
Val loss: 0.3876 score: 0.8372 time: 0.31s
Test loss: 0.3880 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 14 of 20
Epoch 137/1000, LR 0.000274
Train loss: 0.0991;  Loss pred: 0.0991; Loss self: 0.0000; time: 0.40s
Val loss: 0.3945 score: 0.8605 time: 0.12s
Test loss: 0.3907 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 15 of 20
Epoch 138/1000, LR 0.000274
Train loss: 0.0871;  Loss pred: 0.0871; Loss self: 0.0000; time: 0.30s
Val loss: 0.3979 score: 0.8372 time: 0.14s
Test loss: 0.3903 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 139/1000, LR 0.000273
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.53s
Val loss: 0.3975 score: 0.8372 time: 0.13s
Test loss: 0.3908 score: 0.8409 time: 0.31s
     INFO: Early stopping counter 17 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0861;  Loss pred: 0.0861; Loss self: 0.0000; time: 0.28s
Val loss: 0.3938 score: 0.8372 time: 0.08s
Test loss: 0.3882 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.24s
Val loss: 0.3934 score: 0.8372 time: 0.10s
Test loss: 0.3849 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.28s
Val loss: 0.3954 score: 0.8372 time: 0.12s
Test loss: 0.3848 score: 0.8636 time: 0.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 121,   Train_Loss: 0.1160,   Val_Loss: 0.3479,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8837,   Val_Loss: 0.3479,   Test_Precision: 0.9474,   Test_Recall: 0.8182,   Test_accuracy: 0.8780,   Test_Score: 0.8864,   Test_loss: 0.3890


[0.10336486600044736, 0.12080822999996599, 0.13049509400025272, 0.11760731100002886, 0.0862169960000756, 0.13462990900006844, 0.36057238200010033, 0.1518735370000286, 0.1336085089997141, 0.13444490600068093, 0.3098335839995343, 0.13538932499977818, 0.14317906999986008, 0.14266892300020118, 0.12763659800020832, 0.15775754600053915, 0.1314477980004085, 0.12377250499957881, 0.11268039799961116, 0.12006400700011, 0.10782446200028062, 0.1129598369998348, 0.10491331600042031, 0.3431949599998916, 0.1320551799999521, 0.18522281399964413, 0.14616930299962405, 0.13938250499995775, 0.1397868269996252, 0.14425410900003044, 0.169081580999773, 0.1457468799999333, 0.1698775860004389, 0.1326585699998759, 0.11393630899965501, 0.11697478800033423, 0.21235319199968217, 0.1388538459996198, 0.1648935570001413, 0.13707980700019107, 0.26577058900056727, 0.17834133500036842, 0.1317943739995826, 0.13245196200023202, 0.18222994699954143, 0.1847605660004774, 0.2554221679993134, 0.11824811699989368, 0.14901385000030132, 0.13240944299923285, 0.14230561400017905, 0.09150011700057803, 0.12066802499975893, 0.09749128600014956, 0.12153079200015782, 0.12577299300028244, 0.16531993199987483, 0.2652613590007604, 0.10402420799982792, 0.15283779699984734, 0.12899835999996867, 0.1338757080002324, 0.21811689700007264, 0.27948235900021245, 0.13939477100029762, 0.14057348599999386, 0.1489718709999579, 0.37150966500030336, 0.1258845319998727, 0.0827234819998921, 0.12696339500053, 0.4040936299998066, 0.20988042299995868, 0.15946574700046767, 0.1352813799994692, 0.30127861100027076, 0.12195919900023, 0.15696796599968366, 0.14678954899954988, 0.1330646729993532, 0.14219486999991204, 0.13944446799996513, 0.11927763600033359, 0.12811844199950428, 0.15248457199959375, 0.18135324200011382, 0.16176260599968373, 0.12747470600061206, 0.10779220899985376, 0.1134258009997211, 0.34577967499990336, 0.12578004100032558, 0.1274494549998053, 0.11875636499917164, 0.14562853500046913, 0.13650902500012307, 0.22185264200015808, 0.12990211899978021, 0.11094472799959476, 0.1362492589996691, 0.1736387979999563, 0.12208700700011832, 0.12433680600042862, 0.1517151300004116, 0.15806852900004742, 0.2975248840002678, 0.23133697799949005, 0.11822237900014443, 0.1276771259999805, 0.2933157030001894, 0.10557830099969578, 0.12293804100045236, 0.3745481559999462, 0.14401415300017106, 0.1268489909998607, 0.12520784299977095, 0.3403485869994256, 0.21600190199933422, 0.17441197500011185, 0.43574766000074305, 0.12433711199992104, 0.1862980450005125, 0.3268519080002079, 0.1049327059999996, 0.08535169100014173, 0.26801242900000943, 0.1984486729998025, 0.119710350000787, 0.12049912700058485, 0.12924216100054764, 0.13583690499945078, 0.12194080200060853, 0.12725568499990914, 0.12313725700005307, 0.4873229940003512, 0.12794211900018126, 0.12309404799998447, 0.10415099199963151, 0.3179776649994892, 0.11236671000006027, 0.09434970400070597, 0.12593194100008986]
[0.0023492015000101674, 0.002745641590908318, 0.0029657975909148345, 0.0026728934318188376, 0.0019594771818199, 0.0030597706590924645, 0.008194826863638644, 0.0034516712954551954, 0.003036557022720775, 0.003055566045470021, 0.00704167236362578, 0.0030770301136313224, 0.0032540697727240927, 0.003242475522731845, 0.0029008317727320073, 0.003585398772739526, 0.0029874499545547387, 0.002813011477263155, 0.002560918136354799, 0.002728727431820682, 0.002450555954551832, 0.0025672690227235184, 0.0023843935454640978, 0.00779988545454299, 0.003001254090908002, 0.004209609409082821, 0.0033220296136278193, 0.003167784204544494, 0.0031769733409005726, 0.003278502477273419, 0.003842763204540295, 0.003312429090907575, 0.003860854227282702, 0.00301496749999718, 0.0025894615681739774, 0.0026585179090985052, 0.004826208909083685, 0.0031557692272640866, 0.0037475808409123024, 0.0031154501590952514, 0.006040240659103802, 0.004053212159099282, 0.0029953266818086954, 0.0030102718636416366, 0.004141589704535032, 0.004199103772738123, 0.005805049272711668, 0.002687457204543038, 0.003386678409097757, 0.0030093055227098375, 0.0032342185000040695, 0.0020795481136495005, 0.0027424551136308846, 0.0022157110454579447, 0.002762063454549041, 0.0028584771136427826, 0.003757271181815337, 0.006028667250017283, 0.0023641865454506346, 0.003473586295451076, 0.002931780909090197, 0.0030426297272780093, 0.004957202204547106, 0.006351871795459374, 0.0031680629772794914, 0.003194851954545315, 0.0033857243409081343, 0.008443401477279622, 0.002861012090906198, 0.001880079136361184, 0.0028855317045575, 0.00918394613635924, 0.004770009613635425, 0.0036242215227379015, 0.003074576818169755, 0.006847241159097062, 0.0027717999772779544, 0.003567453772720083, 0.0033361261136261337, 0.003024197113621664, 0.003231701590907092, 0.003169192454544662, 0.002710855363643945, 0.0029117827727160065, 0.0034655584545362217, 0.004121664590911678, 0.0036764228636291755, 0.0028971524091048195, 0.0024498229318148583, 0.002577859113630025, 0.007858628977270531, 0.002858637295461945, 0.002896578522722848, 0.002699008295435719, 0.003309739431828844, 0.003102477840911888, 0.005042105500003593, 0.002952320886358641, 0.002521471090899881, 0.0030965740681742977, 0.0039463363181808245, 0.0027747047045481436, 0.0028258365000097415, 0.0034480711363729906, 0.003592466568182896, 0.006761929181824267, 0.005257658590897501, 0.0026868722500032823, 0.0029017528636359202, 0.006666265977277031, 0.002399506840902177, 0.0027940463863739174, 0.008512458090907867, 0.0032730489318220698, 0.0028829316136331977, 0.0028456327954493395, 0.0077351951590778545, 0.004909134136348505, 0.003963908522729815, 0.009903355909107797, 0.00282584345454366, 0.004234046477284375, 0.00742845245455018, 0.002384834227272718, 0.00193981115909413, 0.006091191568182033, 0.004510197113631875, 0.002720689772745159, 0.002738616522740565, 0.0029373218409215374, 0.003087202386351154, 0.0027713818636501937, 0.002892174659088844, 0.0027985740227284787, 0.011075522590917071, 0.0029077754318223015, 0.002797591999999647, 0.002367067999991625, 0.007226765113624755, 0.0025537888636377334, 0.0021443114545614994, 0.0028620895681838606]
[425.67655435077495, 364.21359703732423, 337.1774267614596, 374.12640103632, 510.34021180651456, 326.8218802701384, 122.02820348006509, 289.714724955617, 329.3203429138944, 327.27160372872106, 142.01171942698804, 324.9886946409703, 307.30748565445356, 308.40633737690695, 344.7287117440108, 278.9090038193775, 334.7336408013716, 355.49090648322687, 390.4849537374889, 366.4711939853856, 408.070665818722, 389.5189756697714, 419.3938546354185, 128.2070109654696, 333.1940481245489, 237.5517305340396, 301.0207964124531, 315.67806877924414, 314.7649957039083, 305.0173080337747, 260.22940961297894, 301.8932549363673, 259.0100379686719, 331.67853384851924, 386.1806687114397, 376.14943144734985, 207.20197132739995, 316.8799516012, 266.8388068065164, 320.9809012930597, 165.55631744453893, 246.71790193736692, 333.85340105746354, 332.1959096379631, 241.4531789339253, 238.14605547314846, 172.26382637280832, 372.09894851889743, 295.2745667594726, 332.3025835872969, 309.19370475394345, 480.87370204916834, 364.63677929665215, 451.32238793047105, 362.0481630691822, 349.8366298709389, 266.150605481941, 165.87414075592469, 422.978466705296, 287.8869027407137, 341.08960765090876, 328.66306111280187, 201.7266915363524, 157.4339080198137, 315.6502907839065, 313.0035489053883, 295.35777260938363, 118.43568053596687, 349.5266598762467, 531.8925042354647, 346.55658034204527, 108.88565602981927, 209.64318334735137, 275.9213237176944, 325.2480126989586, 146.04422084235, 360.776393750479, 280.3119714253587, 299.748860187144, 330.6662768427941, 309.4345105419571, 315.53779530365455, 368.8872572883398, 343.432212516058, 288.55378234669683, 242.62042142027096, 272.00353090309403, 345.1665148361961, 408.19276651116496, 387.91879459690296, 127.24865913536502, 349.8170270105581, 345.2349011619329, 370.506456645982, 302.1385884288286, 322.32301124383775, 198.32984454595157, 338.71656858864975, 396.5938787119359, 322.9375361234578, 253.3995887256204, 360.39871138750544, 353.8775155592168, 290.0172184532988, 278.3602800528801, 147.88678986581976, 190.19873251779487, 372.17995756916923, 344.61928599494576, 150.00901605316233, 416.75230216222917, 357.90386475930666, 117.47488085352187, 305.5255270636334, 346.86913670482664, 351.41568567777733, 129.27922042489413, 203.70190999584634, 252.27625568698355, 100.97587213646763, 353.8766446499734, 236.1806856313439, 134.61754061405696, 419.3163569040159, 515.5140980150813, 164.171490718434, 221.71979955766088, 367.5538497691364, 365.1478736421588, 340.4461799413394, 323.9178631181114, 360.83082346613094, 345.76058429162845, 357.3248346759993, 90.28919329008369, 343.9055124601904, 357.45026437026064, 422.46357096777024, 138.37449872484183, 391.5750492292281, 466.3501646986701, 349.39507523328496]
Elapsed: 0.1649186019014297~0.07606401476415733
Time per graph: 0.003748150043214312~0.0017287276082763026
Speed: 305.0128743418616~90.70607675018319
Total Time: 0.1267
best val loss: 0.34789828956127167 test_score: 0.8864

Testing...
Test loss: 0.5211 score: 0.8636 time: 0.39s
test Score 0.8636
Epoch Time List: [0.5304423199995654, 0.4908159059996251, 0.8340306530008093, 0.49096506600017165, 0.4700082299996211, 0.6369020920001276, 0.8069120899990594, 0.6644631100007246, 0.6397377839994078, 0.5609344029999193, 0.8935040619999199, 0.5506251599999814, 0.6168153230000826, 0.584143647000019, 0.6127859920006813, 0.867127951000839, 0.5980707970002186, 0.6477464550007426, 0.5774093119998724, 0.8696597420002945, 0.6196800400002758, 0.44453778000024613, 0.5021751379999841, 0.8322389679997286, 0.596626364000258, 0.6838741290002872, 0.6704849999996441, 0.7626779130005161, 0.8399589459995696, 0.586118905001058, 0.7882433740005581, 0.8141040300006352, 0.647093553000559, 0.5816822510005295, 0.5570730489989728, 0.6870726370007105, 0.5894031880006878, 0.7510749170005511, 0.49844251300055475, 0.7471167769999738, 0.6568645179995656, 0.6806770530010908, 0.5896278210002492, 0.7518709269988904, 0.591101441000319, 0.643510447999688, 0.7199022629993124, 0.7013564010012487, 0.5796683030002896, 0.5655444499998339, 0.9807784429995081, 0.5299312150000333, 0.5453748810004981, 0.7050028059993565, 0.45797188300093694, 0.5375204219990337, 0.7256463789999543, 0.912989708999703, 0.4743762529997184, 0.5304723670005842, 0.6154073830002744, 0.8312156820011296, 0.6626647580005738, 0.7491348449993893, 0.9666651570005342, 0.5810500909992697, 0.6753339220003909, 1.041000943000654, 0.6093936450006368, 0.5373976020000555, 0.6915380210002695, 0.8009620309994716, 0.6064013769992016, 0.7910630259993923, 0.5271632549993228, 0.6909539309990578, 0.5504932769999868, 0.5599964419989192, 0.6205845120002778, 0.8969174500007284, 0.6131789409992052, 0.5347818970003573, 0.5421769579997999, 0.8250670060006087, 0.6449141120001514, 0.6632055059999402, 0.906739158000164, 0.5529876340006012, 0.5209528659997886, 0.5202051540009052, 0.8071566099988559, 0.529338975000428, 0.48226187599993864, 0.4966255179997461, 0.7017100229995776, 0.6263568630001828, 0.6646705469993321, 0.9086972100012645, 0.581911004000176, 0.6156505529997958, 0.6215615970004364, 0.7653518590004751, 0.5893096149993653, 0.6098649259993181, 0.627659273000063, 0.6795481979988836, 0.6932821009995678, 0.5418393049994847, 0.5550858129990957, 1.0345903489987904, 0.4285439129998849, 0.47664217699912115, 0.780425739999373, 0.6395673509996413, 0.6339497910003047, 0.5610617039992576, 0.8505138980008269, 0.8996547970000393, 0.6999611720002576, 0.9955837889992836, 0.6012260229990716, 0.6188406739993297, 1.0451105230004032, 0.49761767699965276, 0.43961603899879265, 0.719650811000065, 0.6752207439994891, 0.6333552919995782, 0.5035851480006386, 0.7667075449999174, 0.6502564389993495, 0.9318712949989276, 0.766181573000722, 0.6294566500000656, 0.8987375269998665, 0.7255996530002449, 0.6383179340000424, 0.5427978800016717, 0.977040940999359, 0.4669238159995075, 0.4345744049996938, 0.5190234580004471]
Total Epoch List: [142]
Total Time List: [0.12670906799939985]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049f910>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7997;  Loss pred: 0.7997; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7291 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7241 score: 0.5116 time: 0.38s
Epoch 2/1000, LR 0.000015
Train loss: 0.7906;  Loss pred: 0.7906; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7193 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7147 score: 0.5116 time: 0.14s
Epoch 3/1000, LR 0.000045
Train loss: 0.7735;  Loss pred: 0.7735; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7073 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7032 score: 0.5116 time: 0.14s
Epoch 4/1000, LR 0.000075
Train loss: 0.7778;  Loss pred: 0.7778; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.12s
Epoch 5/1000, LR 0.000105
Train loss: 0.7550;  Loss pred: 0.7550; Loss self: 0.0000; time: 0.56s
Val loss: 0.6845 score: 0.5455 time: 0.12s
Test loss: 0.6816 score: 0.5349 time: 0.15s
Epoch 6/1000, LR 0.000135
Train loss: 0.7417;  Loss pred: 0.7417; Loss self: 0.0000; time: 0.34s
Val loss: 0.6791 score: 0.7500 time: 0.14s
Test loss: 0.6767 score: 0.7674 time: 0.13s
Epoch 7/1000, LR 0.000165
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6796 score: 0.4884 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6528;  Loss pred: 0.6528; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7084 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7075 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6152;  Loss pred: 0.6152; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7269 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7278 score: 0.4884 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7409 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7431 score: 0.4884 time: 0.29s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5918;  Loss pred: 0.5918; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7534 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7554 score: 0.4884 time: 0.13s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5507;  Loss pred: 0.5507; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7638 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7659 score: 0.4884 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5810;  Loss pred: 0.5810; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7729 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7755 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7789 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7826 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7858 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7902 score: 0.4884 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5282;  Loss pred: 0.5282; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7888 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7941 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5251;  Loss pred: 0.5251; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7917 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7980 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5245;  Loss pred: 0.5245; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7883 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7959 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 13 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5081;  Loss pred: 0.5081; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7852 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7943 score: 0.4884 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4961;  Loss pred: 0.4961; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7816 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7922 score: 0.4884 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.4856;  Loss pred: 0.4856; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7774 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7900 score: 0.4884 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.4695;  Loss pred: 0.4695; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7712 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7863 score: 0.4884 time: 0.18s
     INFO: Early stopping counter 17 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.4844;  Loss pred: 0.4844; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7647 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7830 score: 0.4884 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.4610;  Loss pred: 0.4610; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7569 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7784 score: 0.4884 time: 0.14s
     INFO: Early stopping counter 19 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.4458;  Loss pred: 0.4458; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7490 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7733 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.7417,   Val_Loss: 0.6791,   Val_Precision: 0.8235,   Val_Recall: 0.6364,   Val_accuracy: 0.7179,   Val_Score: 0.7500,   Val_Loss: 0.6791,   Test_Precision: 0.7727,   Test_Recall: 0.7727,   Test_accuracy: 0.7727,   Test_Score: 0.7674,   Test_loss: 0.6767


[0.10336486600044736, 0.12080822999996599, 0.13049509400025272, 0.11760731100002886, 0.0862169960000756, 0.13462990900006844, 0.36057238200010033, 0.1518735370000286, 0.1336085089997141, 0.13444490600068093, 0.3098335839995343, 0.13538932499977818, 0.14317906999986008, 0.14266892300020118, 0.12763659800020832, 0.15775754600053915, 0.1314477980004085, 0.12377250499957881, 0.11268039799961116, 0.12006400700011, 0.10782446200028062, 0.1129598369998348, 0.10491331600042031, 0.3431949599998916, 0.1320551799999521, 0.18522281399964413, 0.14616930299962405, 0.13938250499995775, 0.1397868269996252, 0.14425410900003044, 0.169081580999773, 0.1457468799999333, 0.1698775860004389, 0.1326585699998759, 0.11393630899965501, 0.11697478800033423, 0.21235319199968217, 0.1388538459996198, 0.1648935570001413, 0.13707980700019107, 0.26577058900056727, 0.17834133500036842, 0.1317943739995826, 0.13245196200023202, 0.18222994699954143, 0.1847605660004774, 0.2554221679993134, 0.11824811699989368, 0.14901385000030132, 0.13240944299923285, 0.14230561400017905, 0.09150011700057803, 0.12066802499975893, 0.09749128600014956, 0.12153079200015782, 0.12577299300028244, 0.16531993199987483, 0.2652613590007604, 0.10402420799982792, 0.15283779699984734, 0.12899835999996867, 0.1338757080002324, 0.21811689700007264, 0.27948235900021245, 0.13939477100029762, 0.14057348599999386, 0.1489718709999579, 0.37150966500030336, 0.1258845319998727, 0.0827234819998921, 0.12696339500053, 0.4040936299998066, 0.20988042299995868, 0.15946574700046767, 0.1352813799994692, 0.30127861100027076, 0.12195919900023, 0.15696796599968366, 0.14678954899954988, 0.1330646729993532, 0.14219486999991204, 0.13944446799996513, 0.11927763600033359, 0.12811844199950428, 0.15248457199959375, 0.18135324200011382, 0.16176260599968373, 0.12747470600061206, 0.10779220899985376, 0.1134258009997211, 0.34577967499990336, 0.12578004100032558, 0.1274494549998053, 0.11875636499917164, 0.14562853500046913, 0.13650902500012307, 0.22185264200015808, 0.12990211899978021, 0.11094472799959476, 0.1362492589996691, 0.1736387979999563, 0.12208700700011832, 0.12433680600042862, 0.1517151300004116, 0.15806852900004742, 0.2975248840002678, 0.23133697799949005, 0.11822237900014443, 0.1276771259999805, 0.2933157030001894, 0.10557830099969578, 0.12293804100045236, 0.3745481559999462, 0.14401415300017106, 0.1268489909998607, 0.12520784299977095, 0.3403485869994256, 0.21600190199933422, 0.17441197500011185, 0.43574766000074305, 0.12433711199992104, 0.1862980450005125, 0.3268519080002079, 0.1049327059999996, 0.08535169100014173, 0.26801242900000943, 0.1984486729998025, 0.119710350000787, 0.12049912700058485, 0.12924216100054764, 0.13583690499945078, 0.12194080200060853, 0.12725568499990914, 0.12313725700005307, 0.4873229940003512, 0.12794211900018126, 0.12309404799998447, 0.10415099199963151, 0.3179776649994892, 0.11236671000006027, 0.09434970400070597, 0.12593194100008986, 0.3901498939994781, 0.14637111600040953, 0.14213125300011598, 0.1300795070001186, 0.15631519699945784, 0.13411293099943578, 0.14406271200004994, 0.2086604129999614, 0.11761158299941599, 0.22552752100000362, 0.29750332399999024, 0.1389503769996736, 0.18143467499976396, 0.11298426400026074, 0.10546277899993584, 0.09052866000001814, 0.11096278400054871, 0.10736000399992918, 0.1157153180001842, 0.13101346799976454, 0.16422283399970183, 0.1584496980003678, 0.1889165229995342, 0.17124403199977678, 0.1438983999996708, 0.11716347899982793]
[0.0023492015000101674, 0.002745641590908318, 0.0029657975909148345, 0.0026728934318188376, 0.0019594771818199, 0.0030597706590924645, 0.008194826863638644, 0.0034516712954551954, 0.003036557022720775, 0.003055566045470021, 0.00704167236362578, 0.0030770301136313224, 0.0032540697727240927, 0.003242475522731845, 0.0029008317727320073, 0.003585398772739526, 0.0029874499545547387, 0.002813011477263155, 0.002560918136354799, 0.002728727431820682, 0.002450555954551832, 0.0025672690227235184, 0.0023843935454640978, 0.00779988545454299, 0.003001254090908002, 0.004209609409082821, 0.0033220296136278193, 0.003167784204544494, 0.0031769733409005726, 0.003278502477273419, 0.003842763204540295, 0.003312429090907575, 0.003860854227282702, 0.00301496749999718, 0.0025894615681739774, 0.0026585179090985052, 0.004826208909083685, 0.0031557692272640866, 0.0037475808409123024, 0.0031154501590952514, 0.006040240659103802, 0.004053212159099282, 0.0029953266818086954, 0.0030102718636416366, 0.004141589704535032, 0.004199103772738123, 0.005805049272711668, 0.002687457204543038, 0.003386678409097757, 0.0030093055227098375, 0.0032342185000040695, 0.0020795481136495005, 0.0027424551136308846, 0.0022157110454579447, 0.002762063454549041, 0.0028584771136427826, 0.003757271181815337, 0.006028667250017283, 0.0023641865454506346, 0.003473586295451076, 0.002931780909090197, 0.0030426297272780093, 0.004957202204547106, 0.006351871795459374, 0.0031680629772794914, 0.003194851954545315, 0.0033857243409081343, 0.008443401477279622, 0.002861012090906198, 0.001880079136361184, 0.0028855317045575, 0.00918394613635924, 0.004770009613635425, 0.0036242215227379015, 0.003074576818169755, 0.006847241159097062, 0.0027717999772779544, 0.003567453772720083, 0.0033361261136261337, 0.003024197113621664, 0.003231701590907092, 0.003169192454544662, 0.002710855363643945, 0.0029117827727160065, 0.0034655584545362217, 0.004121664590911678, 0.0036764228636291755, 0.0028971524091048195, 0.0024498229318148583, 0.002577859113630025, 0.007858628977270531, 0.002858637295461945, 0.002896578522722848, 0.002699008295435719, 0.003309739431828844, 0.003102477840911888, 0.005042105500003593, 0.002952320886358641, 0.002521471090899881, 0.0030965740681742977, 0.0039463363181808245, 0.0027747047045481436, 0.0028258365000097415, 0.0034480711363729906, 0.003592466568182896, 0.006761929181824267, 0.005257658590897501, 0.0026868722500032823, 0.0029017528636359202, 0.006666265977277031, 0.002399506840902177, 0.0027940463863739174, 0.008512458090907867, 0.0032730489318220698, 0.0028829316136331977, 0.0028456327954493395, 0.0077351951590778545, 0.004909134136348505, 0.003963908522729815, 0.009903355909107797, 0.00282584345454366, 0.004234046477284375, 0.00742845245455018, 0.002384834227272718, 0.00193981115909413, 0.006091191568182033, 0.004510197113631875, 0.002720689772745159, 0.002738616522740565, 0.0029373218409215374, 0.003087202386351154, 0.0027713818636501937, 0.002892174659088844, 0.0027985740227284787, 0.011075522590917071, 0.0029077754318223015, 0.002797591999999647, 0.002367067999991625, 0.007226765113624755, 0.0025537888636377334, 0.0021443114545614994, 0.0028620895681838606, 0.009073253348825071, 0.003403979441869989, 0.003305377976746883, 0.003025104813956247, 0.003635237139522275, 0.0031189053720799016, 0.0033502956279081383, 0.004852567744185149, 0.002735153093009674, 0.005244826069767526, 0.0069186819534881455, 0.003231404116271479, 0.0042194110465061385, 0.0026275410232618774, 0.0024526227674403684, 0.0021053176744190265, 0.002580529860477877, 0.0024967442790681206, 0.0026910539069810277, 0.0030468248372038267, 0.0038191356744116706, 0.003684876697682972, 0.0043934075116170744, 0.003982419348832018, 0.0033464744185969955, 0.0027247320697634404]
[425.67655435077495, 364.21359703732423, 337.1774267614596, 374.12640103632, 510.34021180651456, 326.8218802701384, 122.02820348006509, 289.714724955617, 329.3203429138944, 327.27160372872106, 142.01171942698804, 324.9886946409703, 307.30748565445356, 308.40633737690695, 344.7287117440108, 278.9090038193775, 334.7336408013716, 355.49090648322687, 390.4849537374889, 366.4711939853856, 408.070665818722, 389.5189756697714, 419.3938546354185, 128.2070109654696, 333.1940481245489, 237.5517305340396, 301.0207964124531, 315.67806877924414, 314.7649957039083, 305.0173080337747, 260.22940961297894, 301.8932549363673, 259.0100379686719, 331.67853384851924, 386.1806687114397, 376.14943144734985, 207.20197132739995, 316.8799516012, 266.8388068065164, 320.9809012930597, 165.55631744453893, 246.71790193736692, 333.85340105746354, 332.1959096379631, 241.4531789339253, 238.14605547314846, 172.26382637280832, 372.09894851889743, 295.2745667594726, 332.3025835872969, 309.19370475394345, 480.87370204916834, 364.63677929665215, 451.32238793047105, 362.0481630691822, 349.8366298709389, 266.150605481941, 165.87414075592469, 422.978466705296, 287.8869027407137, 341.08960765090876, 328.66306111280187, 201.7266915363524, 157.4339080198137, 315.6502907839065, 313.0035489053883, 295.35777260938363, 118.43568053596687, 349.5266598762467, 531.8925042354647, 346.55658034204527, 108.88565602981927, 209.64318334735137, 275.9213237176944, 325.2480126989586, 146.04422084235, 360.776393750479, 280.3119714253587, 299.748860187144, 330.6662768427941, 309.4345105419571, 315.53779530365455, 368.8872572883398, 343.432212516058, 288.55378234669683, 242.62042142027096, 272.00353090309403, 345.1665148361961, 408.19276651116496, 387.91879459690296, 127.24865913536502, 349.8170270105581, 345.2349011619329, 370.506456645982, 302.1385884288286, 322.32301124383775, 198.32984454595157, 338.71656858864975, 396.5938787119359, 322.9375361234578, 253.3995887256204, 360.39871138750544, 353.8775155592168, 290.0172184532988, 278.3602800528801, 147.88678986581976, 190.19873251779487, 372.17995756916923, 344.61928599494576, 150.00901605316233, 416.75230216222917, 357.90386475930666, 117.47488085352187, 305.5255270636334, 346.86913670482664, 351.41568567777733, 129.27922042489413, 203.70190999584634, 252.27625568698355, 100.97587213646763, 353.8766446499734, 236.1806856313439, 134.61754061405696, 419.3163569040159, 515.5140980150813, 164.171490718434, 221.71979955766088, 367.5538497691364, 365.1478736421588, 340.4461799413394, 323.9178631181114, 360.83082346613094, 345.76058429162845, 357.3248346759993, 90.28919329008369, 343.9055124601904, 357.45026437026064, 422.46357096777024, 138.37449872484183, 391.5750492292281, 466.3501646986701, 349.39507523328496, 110.21405019286644, 293.7738071210695, 302.53726110445893, 330.5670584987748, 275.08521772293926, 320.62530942807376, 298.4811225821231, 206.07646357916465, 365.61024776117097, 190.66409194467806, 144.53620020730057, 309.4629962760087, 236.99990092883812, 380.5839722945911, 407.726786718053, 474.98770002771926, 387.5173139111881, 400.52159461570403, 371.60162321752034, 328.21053176057654, 261.83934933237157, 271.37950114553195, 227.61375933277165, 251.1036413815347, 298.8219465963372, 367.00856245539745]
Elapsed: 0.16398377509524056~0.07428162331831276
Time per graph: 0.003739899904466221~0.0016920762498487372
Speed: 304.3177271826256~89.539593930699
Total Time: 0.1179
best val loss: 0.6791330521756952 test_score: 0.7674

Testing...
Test loss: 0.6767 score: 0.7674 time: 0.09s
test Score 0.7674
Epoch Time List: [0.5304423199995654, 0.4908159059996251, 0.8340306530008093, 0.49096506600017165, 0.4700082299996211, 0.6369020920001276, 0.8069120899990594, 0.6644631100007246, 0.6397377839994078, 0.5609344029999193, 0.8935040619999199, 0.5506251599999814, 0.6168153230000826, 0.584143647000019, 0.6127859920006813, 0.867127951000839, 0.5980707970002186, 0.6477464550007426, 0.5774093119998724, 0.8696597420002945, 0.6196800400002758, 0.44453778000024613, 0.5021751379999841, 0.8322389679997286, 0.596626364000258, 0.6838741290002872, 0.6704849999996441, 0.7626779130005161, 0.8399589459995696, 0.586118905001058, 0.7882433740005581, 0.8141040300006352, 0.647093553000559, 0.5816822510005295, 0.5570730489989728, 0.6870726370007105, 0.5894031880006878, 0.7510749170005511, 0.49844251300055475, 0.7471167769999738, 0.6568645179995656, 0.6806770530010908, 0.5896278210002492, 0.7518709269988904, 0.591101441000319, 0.643510447999688, 0.7199022629993124, 0.7013564010012487, 0.5796683030002896, 0.5655444499998339, 0.9807784429995081, 0.5299312150000333, 0.5453748810004981, 0.7050028059993565, 0.45797188300093694, 0.5375204219990337, 0.7256463789999543, 0.912989708999703, 0.4743762529997184, 0.5304723670005842, 0.6154073830002744, 0.8312156820011296, 0.6626647580005738, 0.7491348449993893, 0.9666651570005342, 0.5810500909992697, 0.6753339220003909, 1.041000943000654, 0.6093936450006368, 0.5373976020000555, 0.6915380210002695, 0.8009620309994716, 0.6064013769992016, 0.7910630259993923, 0.5271632549993228, 0.6909539309990578, 0.5504932769999868, 0.5599964419989192, 0.6205845120002778, 0.8969174500007284, 0.6131789409992052, 0.5347818970003573, 0.5421769579997999, 0.8250670060006087, 0.6449141120001514, 0.6632055059999402, 0.906739158000164, 0.5529876340006012, 0.5209528659997886, 0.5202051540009052, 0.8071566099988559, 0.529338975000428, 0.48226187599993864, 0.4966255179997461, 0.7017100229995776, 0.6263568630001828, 0.6646705469993321, 0.9086972100012645, 0.581911004000176, 0.6156505529997958, 0.6215615970004364, 0.7653518590004751, 0.5893096149993653, 0.6098649259993181, 0.627659273000063, 0.6795481979988836, 0.6932821009995678, 0.5418393049994847, 0.5550858129990957, 1.0345903489987904, 0.4285439129998849, 0.47664217699912115, 0.780425739999373, 0.6395673509996413, 0.6339497910003047, 0.5610617039992576, 0.8505138980008269, 0.8996547970000393, 0.6999611720002576, 0.9955837889992836, 0.6012260229990716, 0.6188406739993297, 1.0451105230004032, 0.49761767699965276, 0.43961603899879265, 0.719650811000065, 0.6752207439994891, 0.6333552919995782, 0.5035851480006386, 0.7667075449999174, 0.6502564389993495, 0.9318712949989276, 0.766181573000722, 0.6294566500000656, 0.8987375269998665, 0.7255996530002449, 0.6383179340000424, 0.5427978800016717, 0.977040940999359, 0.4669238159995075, 0.4345744049996938, 0.5190234580004471, 0.9778660929996477, 0.8135988289986926, 0.6639286489998995, 0.7045890449999206, 0.8331600269993942, 0.6082587669998247, 0.7569831890004934, 1.1006101309994847, 0.6310421429989219, 0.6884784800004127, 0.7435781200001657, 0.5998042089995579, 0.6324974760000259, 0.6867782279996391, 0.4676261250006064, 0.8787418790006996, 0.6074084690008021, 0.5580773560004673, 0.5785792389997368, 0.7070165490004001, 0.6052366220001204, 0.7196880339997733, 0.9586037639992355, 0.6325265460009177, 0.5954384870001377, 0.9026793129987709]
Total Epoch List: [142, 26]
Total Time List: [0.12670906799939985, 0.1179172229994947]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049da50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7931;  Loss pred: 0.7931; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7144 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7215 score: 0.4884 time: 0.15s
Epoch 2/1000, LR 0.000015
Train loss: 0.8779;  Loss pred: 0.8779; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7122 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7191 score: 0.4884 time: 0.13s
Epoch 3/1000, LR 0.000045
Train loss: 0.8382;  Loss pred: 0.8382; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7080 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7147 score: 0.4884 time: 0.13s
Epoch 4/1000, LR 0.000075
Train loss: 0.7795;  Loss pred: 0.7795; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7027 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7086 score: 0.4884 time: 0.24s
Epoch 5/1000, LR 0.000105
Train loss: 0.7968;  Loss pred: 0.7968; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.4884 time: 0.19s
Epoch 6/1000, LR 0.000135
Train loss: 0.7280;  Loss pred: 0.7280; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.21s
Epoch 7/1000, LR 0.000165
Train loss: 0.7257;  Loss pred: 0.7257; Loss self: 0.0000; time: 0.62s
Val loss: 0.6863 score: 0.5227 time: 0.17s
Test loss: 0.6879 score: 0.5116 time: 0.13s
Epoch 8/1000, LR 0.000195
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.33s
Val loss: 0.6831 score: 0.7045 time: 0.14s
Test loss: 0.6829 score: 0.7674 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6813 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5116 time: 0.12s
Epoch 10/1000, LR 0.000255
Train loss: 0.6578;  Loss pred: 0.6578; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6780 score: 0.5116 time: 0.29s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6783 score: 0.5116 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6812 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.6065;  Loss pred: 0.6065; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.5116 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5836;  Loss pred: 0.5836; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7130 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5116 time: 0.47s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7213 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7064 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7295 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7136 score: 0.5116 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5524;  Loss pred: 0.5524; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7380 score: 0.5000 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7207 score: 0.5116 time: 0.18s
     INFO: Early stopping counter 9 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7459 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7273 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7537 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7334 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7597 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7375 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7640 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7395 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 13 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7666 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7397 score: 0.5116 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.5369;  Loss pred: 0.5369; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7686 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7392 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 15 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7695 score: 0.5000 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7376 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 16 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.5195;  Loss pred: 0.5195; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7679 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7336 score: 0.5116 time: 0.16s
     INFO: Early stopping counter 17 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7641 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7274 score: 0.5116 time: 0.17s
     INFO: Early stopping counter 18 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7622 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7228 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 19 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.5025;  Loss pred: 0.5025; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7608 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7191 score: 0.5116 time: 0.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 0.6721,   Val_Loss: 0.6813,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.5000,   Val_Loss: 0.6813,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5116,   Test_loss: 0.6795


[0.10336486600044736, 0.12080822999996599, 0.13049509400025272, 0.11760731100002886, 0.0862169960000756, 0.13462990900006844, 0.36057238200010033, 0.1518735370000286, 0.1336085089997141, 0.13444490600068093, 0.3098335839995343, 0.13538932499977818, 0.14317906999986008, 0.14266892300020118, 0.12763659800020832, 0.15775754600053915, 0.1314477980004085, 0.12377250499957881, 0.11268039799961116, 0.12006400700011, 0.10782446200028062, 0.1129598369998348, 0.10491331600042031, 0.3431949599998916, 0.1320551799999521, 0.18522281399964413, 0.14616930299962405, 0.13938250499995775, 0.1397868269996252, 0.14425410900003044, 0.169081580999773, 0.1457468799999333, 0.1698775860004389, 0.1326585699998759, 0.11393630899965501, 0.11697478800033423, 0.21235319199968217, 0.1388538459996198, 0.1648935570001413, 0.13707980700019107, 0.26577058900056727, 0.17834133500036842, 0.1317943739995826, 0.13245196200023202, 0.18222994699954143, 0.1847605660004774, 0.2554221679993134, 0.11824811699989368, 0.14901385000030132, 0.13240944299923285, 0.14230561400017905, 0.09150011700057803, 0.12066802499975893, 0.09749128600014956, 0.12153079200015782, 0.12577299300028244, 0.16531993199987483, 0.2652613590007604, 0.10402420799982792, 0.15283779699984734, 0.12899835999996867, 0.1338757080002324, 0.21811689700007264, 0.27948235900021245, 0.13939477100029762, 0.14057348599999386, 0.1489718709999579, 0.37150966500030336, 0.1258845319998727, 0.0827234819998921, 0.12696339500053, 0.4040936299998066, 0.20988042299995868, 0.15946574700046767, 0.1352813799994692, 0.30127861100027076, 0.12195919900023, 0.15696796599968366, 0.14678954899954988, 0.1330646729993532, 0.14219486999991204, 0.13944446799996513, 0.11927763600033359, 0.12811844199950428, 0.15248457199959375, 0.18135324200011382, 0.16176260599968373, 0.12747470600061206, 0.10779220899985376, 0.1134258009997211, 0.34577967499990336, 0.12578004100032558, 0.1274494549998053, 0.11875636499917164, 0.14562853500046913, 0.13650902500012307, 0.22185264200015808, 0.12990211899978021, 0.11094472799959476, 0.1362492589996691, 0.1736387979999563, 0.12208700700011832, 0.12433680600042862, 0.1517151300004116, 0.15806852900004742, 0.2975248840002678, 0.23133697799949005, 0.11822237900014443, 0.1276771259999805, 0.2933157030001894, 0.10557830099969578, 0.12293804100045236, 0.3745481559999462, 0.14401415300017106, 0.1268489909998607, 0.12520784299977095, 0.3403485869994256, 0.21600190199933422, 0.17441197500011185, 0.43574766000074305, 0.12433711199992104, 0.1862980450005125, 0.3268519080002079, 0.1049327059999996, 0.08535169100014173, 0.26801242900000943, 0.1984486729998025, 0.119710350000787, 0.12049912700058485, 0.12924216100054764, 0.13583690499945078, 0.12194080200060853, 0.12725568499990914, 0.12313725700005307, 0.4873229940003512, 0.12794211900018126, 0.12309404799998447, 0.10415099199963151, 0.3179776649994892, 0.11236671000006027, 0.09434970400070597, 0.12593194100008986, 0.3901498939994781, 0.14637111600040953, 0.14213125300011598, 0.1300795070001186, 0.15631519699945784, 0.13411293099943578, 0.14406271200004994, 0.2086604129999614, 0.11761158299941599, 0.22552752100000362, 0.29750332399999024, 0.1389503769996736, 0.18143467499976396, 0.11298426400026074, 0.10546277899993584, 0.09052866000001814, 0.11096278400054871, 0.10736000399992918, 0.1157153180001842, 0.13101346799976454, 0.16422283399970183, 0.1584496980003678, 0.1889165229995342, 0.17124403199977678, 0.1438983999996708, 0.11716347899982793, 0.15359390300000086, 0.1381268220002312, 0.1375065430001996, 0.24853410800005804, 0.19166393999967113, 0.21841357099947345, 0.13253199199971277, 0.1775986090005972, 0.1254032039996673, 0.29930787299963413, 0.17516141000032803, 0.12341130300046643, 0.19955290500001865, 0.11239011500038032, 0.4753575950007871, 0.14290126400010195, 0.17264112900011241, 0.180993805000071, 0.10492620000059105, 0.1261530239999047, 0.11879561799923977, 0.1354128489992945, 0.17257786099980876, 0.12530783400052314, 0.13110170400068455, 0.16518528100004914, 0.17305142399982287, 0.2133553670000765, 0.19936037999923428]
[0.0023492015000101674, 0.002745641590908318, 0.0029657975909148345, 0.0026728934318188376, 0.0019594771818199, 0.0030597706590924645, 0.008194826863638644, 0.0034516712954551954, 0.003036557022720775, 0.003055566045470021, 0.00704167236362578, 0.0030770301136313224, 0.0032540697727240927, 0.003242475522731845, 0.0029008317727320073, 0.003585398772739526, 0.0029874499545547387, 0.002813011477263155, 0.002560918136354799, 0.002728727431820682, 0.002450555954551832, 0.0025672690227235184, 0.0023843935454640978, 0.00779988545454299, 0.003001254090908002, 0.004209609409082821, 0.0033220296136278193, 0.003167784204544494, 0.0031769733409005726, 0.003278502477273419, 0.003842763204540295, 0.003312429090907575, 0.003860854227282702, 0.00301496749999718, 0.0025894615681739774, 0.0026585179090985052, 0.004826208909083685, 0.0031557692272640866, 0.0037475808409123024, 0.0031154501590952514, 0.006040240659103802, 0.004053212159099282, 0.0029953266818086954, 0.0030102718636416366, 0.004141589704535032, 0.004199103772738123, 0.005805049272711668, 0.002687457204543038, 0.003386678409097757, 0.0030093055227098375, 0.0032342185000040695, 0.0020795481136495005, 0.0027424551136308846, 0.0022157110454579447, 0.002762063454549041, 0.0028584771136427826, 0.003757271181815337, 0.006028667250017283, 0.0023641865454506346, 0.003473586295451076, 0.002931780909090197, 0.0030426297272780093, 0.004957202204547106, 0.006351871795459374, 0.0031680629772794914, 0.003194851954545315, 0.0033857243409081343, 0.008443401477279622, 0.002861012090906198, 0.001880079136361184, 0.0028855317045575, 0.00918394613635924, 0.004770009613635425, 0.0036242215227379015, 0.003074576818169755, 0.006847241159097062, 0.0027717999772779544, 0.003567453772720083, 0.0033361261136261337, 0.003024197113621664, 0.003231701590907092, 0.003169192454544662, 0.002710855363643945, 0.0029117827727160065, 0.0034655584545362217, 0.004121664590911678, 0.0036764228636291755, 0.0028971524091048195, 0.0024498229318148583, 0.002577859113630025, 0.007858628977270531, 0.002858637295461945, 0.002896578522722848, 0.002699008295435719, 0.003309739431828844, 0.003102477840911888, 0.005042105500003593, 0.002952320886358641, 0.002521471090899881, 0.0030965740681742977, 0.0039463363181808245, 0.0027747047045481436, 0.0028258365000097415, 0.0034480711363729906, 0.003592466568182896, 0.006761929181824267, 0.005257658590897501, 0.0026868722500032823, 0.0029017528636359202, 0.006666265977277031, 0.002399506840902177, 0.0027940463863739174, 0.008512458090907867, 0.0032730489318220698, 0.0028829316136331977, 0.0028456327954493395, 0.0077351951590778545, 0.004909134136348505, 0.003963908522729815, 0.009903355909107797, 0.00282584345454366, 0.004234046477284375, 0.00742845245455018, 0.002384834227272718, 0.00193981115909413, 0.006091191568182033, 0.004510197113631875, 0.002720689772745159, 0.002738616522740565, 0.0029373218409215374, 0.003087202386351154, 0.0027713818636501937, 0.002892174659088844, 0.0027985740227284787, 0.011075522590917071, 0.0029077754318223015, 0.002797591999999647, 0.002367067999991625, 0.007226765113624755, 0.0025537888636377334, 0.0021443114545614994, 0.0028620895681838606, 0.009073253348825071, 0.003403979441869989, 0.003305377976746883, 0.003025104813956247, 0.003635237139522275, 0.0031189053720799016, 0.0033502956279081383, 0.004852567744185149, 0.002735153093009674, 0.005244826069767526, 0.0069186819534881455, 0.003231404116271479, 0.0042194110465061385, 0.0026275410232618774, 0.0024526227674403684, 0.0021053176744190265, 0.002580529860477877, 0.0024967442790681206, 0.0026910539069810277, 0.0030468248372038267, 0.0038191356744116706, 0.003684876697682972, 0.0043934075116170744, 0.003982419348832018, 0.0033464744185969955, 0.0027247320697634404, 0.0035719512325581597, 0.0032122516744239812, 0.0031978265813999906, 0.005779862976745536, 0.00445730093022491, 0.005079385372080778, 0.00308213934883053, 0.004130200209316214, 0.002916353581387612, 0.006960648209293817, 0.004073521162798326, 0.002870030302336429, 0.004640765232558574, 0.0026137236046600075, 0.01105482779071598, 0.0033232852093046964, 0.0040149099767468, 0.004209158255815605, 0.002440144186060257, 0.002933791255811737, 0.0027626887906799947, 0.0031491360232394072, 0.00401343862790253, 0.0029141356744307708, 0.003048876837225222, 0.0038415181627918407, 0.004024451720926113, 0.004961752720932012, 0.004636287906958937]
[425.67655435077495, 364.21359703732423, 337.1774267614596, 374.12640103632, 510.34021180651456, 326.8218802701384, 122.02820348006509, 289.714724955617, 329.3203429138944, 327.27160372872106, 142.01171942698804, 324.9886946409703, 307.30748565445356, 308.40633737690695, 344.7287117440108, 278.9090038193775, 334.7336408013716, 355.49090648322687, 390.4849537374889, 366.4711939853856, 408.070665818722, 389.5189756697714, 419.3938546354185, 128.2070109654696, 333.1940481245489, 237.5517305340396, 301.0207964124531, 315.67806877924414, 314.7649957039083, 305.0173080337747, 260.22940961297894, 301.8932549363673, 259.0100379686719, 331.67853384851924, 386.1806687114397, 376.14943144734985, 207.20197132739995, 316.8799516012, 266.8388068065164, 320.9809012930597, 165.55631744453893, 246.71790193736692, 333.85340105746354, 332.1959096379631, 241.4531789339253, 238.14605547314846, 172.26382637280832, 372.09894851889743, 295.2745667594726, 332.3025835872969, 309.19370475394345, 480.87370204916834, 364.63677929665215, 451.32238793047105, 362.0481630691822, 349.8366298709389, 266.150605481941, 165.87414075592469, 422.978466705296, 287.8869027407137, 341.08960765090876, 328.66306111280187, 201.7266915363524, 157.4339080198137, 315.6502907839065, 313.0035489053883, 295.35777260938363, 118.43568053596687, 349.5266598762467, 531.8925042354647, 346.55658034204527, 108.88565602981927, 209.64318334735137, 275.9213237176944, 325.2480126989586, 146.04422084235, 360.776393750479, 280.3119714253587, 299.748860187144, 330.6662768427941, 309.4345105419571, 315.53779530365455, 368.8872572883398, 343.432212516058, 288.55378234669683, 242.62042142027096, 272.00353090309403, 345.1665148361961, 408.19276651116496, 387.91879459690296, 127.24865913536502, 349.8170270105581, 345.2349011619329, 370.506456645982, 302.1385884288286, 322.32301124383775, 198.32984454595157, 338.71656858864975, 396.5938787119359, 322.9375361234578, 253.3995887256204, 360.39871138750544, 353.8775155592168, 290.0172184532988, 278.3602800528801, 147.88678986581976, 190.19873251779487, 372.17995756916923, 344.61928599494576, 150.00901605316233, 416.75230216222917, 357.90386475930666, 117.47488085352187, 305.5255270636334, 346.86913670482664, 351.41568567777733, 129.27922042489413, 203.70190999584634, 252.27625568698355, 100.97587213646763, 353.8766446499734, 236.1806856313439, 134.61754061405696, 419.3163569040159, 515.5140980150813, 164.171490718434, 221.71979955766088, 367.5538497691364, 365.1478736421588, 340.4461799413394, 323.9178631181114, 360.83082346613094, 345.76058429162845, 357.3248346759993, 90.28919329008369, 343.9055124601904, 357.45026437026064, 422.46357096777024, 138.37449872484183, 391.5750492292281, 466.3501646986701, 349.39507523328496, 110.21405019286644, 293.7738071210695, 302.53726110445893, 330.5670584987748, 275.08521772293926, 320.62530942807376, 298.4811225821231, 206.07646357916465, 365.61024776117097, 190.66409194467806, 144.53620020730057, 309.4629962760087, 236.99990092883812, 380.5839722945911, 407.726786718053, 474.98770002771926, 387.5173139111881, 400.52159461570403, 371.60162321752034, 328.21053176057654, 261.83934933237157, 271.37950114553195, 227.61375933277165, 251.1036413815347, 298.8219465963372, 367.00856245539745, 279.95902936329287, 311.30811074425526, 312.7123921654956, 173.01448218121416, 224.3510177244284, 196.87421346223795, 324.4499637498333, 242.11901344258504, 342.8939503022114, 143.6647809129049, 245.4878617380362, 348.4283769359236, 215.48170396214468, 382.5959249161236, 90.45821598775296, 300.9070654544338, 249.07158710698656, 237.5771922137508, 409.8118487065936, 340.855879919553, 361.9662132678596, 317.54741383680675, 249.16289813122467, 343.15492198054017, 327.98963467153317, 260.31375035156555, 248.48105266122522, 201.54168420794673, 215.68979754234596]
Elapsed: 0.1655816845126962~0.07395542650521901
Time per graph: 0.0037879063325303645~0.001691195737202185
Speed: 299.6103966716848~88.04809920093484
Total Time: 0.2004
best val loss: 0.6813190091740001 test_score: 0.5116

Testing...
Test loss: 0.6829 score: 0.7674 time: 0.09s
test Score 0.7674
Epoch Time List: [0.5304423199995654, 0.4908159059996251, 0.8340306530008093, 0.49096506600017165, 0.4700082299996211, 0.6369020920001276, 0.8069120899990594, 0.6644631100007246, 0.6397377839994078, 0.5609344029999193, 0.8935040619999199, 0.5506251599999814, 0.6168153230000826, 0.584143647000019, 0.6127859920006813, 0.867127951000839, 0.5980707970002186, 0.6477464550007426, 0.5774093119998724, 0.8696597420002945, 0.6196800400002758, 0.44453778000024613, 0.5021751379999841, 0.8322389679997286, 0.596626364000258, 0.6838741290002872, 0.6704849999996441, 0.7626779130005161, 0.8399589459995696, 0.586118905001058, 0.7882433740005581, 0.8141040300006352, 0.647093553000559, 0.5816822510005295, 0.5570730489989728, 0.6870726370007105, 0.5894031880006878, 0.7510749170005511, 0.49844251300055475, 0.7471167769999738, 0.6568645179995656, 0.6806770530010908, 0.5896278210002492, 0.7518709269988904, 0.591101441000319, 0.643510447999688, 0.7199022629993124, 0.7013564010012487, 0.5796683030002896, 0.5655444499998339, 0.9807784429995081, 0.5299312150000333, 0.5453748810004981, 0.7050028059993565, 0.45797188300093694, 0.5375204219990337, 0.7256463789999543, 0.912989708999703, 0.4743762529997184, 0.5304723670005842, 0.6154073830002744, 0.8312156820011296, 0.6626647580005738, 0.7491348449993893, 0.9666651570005342, 0.5810500909992697, 0.6753339220003909, 1.041000943000654, 0.6093936450006368, 0.5373976020000555, 0.6915380210002695, 0.8009620309994716, 0.6064013769992016, 0.7910630259993923, 0.5271632549993228, 0.6909539309990578, 0.5504932769999868, 0.5599964419989192, 0.6205845120002778, 0.8969174500007284, 0.6131789409992052, 0.5347818970003573, 0.5421769579997999, 0.8250670060006087, 0.6449141120001514, 0.6632055059999402, 0.906739158000164, 0.5529876340006012, 0.5209528659997886, 0.5202051540009052, 0.8071566099988559, 0.529338975000428, 0.48226187599993864, 0.4966255179997461, 0.7017100229995776, 0.6263568630001828, 0.6646705469993321, 0.9086972100012645, 0.581911004000176, 0.6156505529997958, 0.6215615970004364, 0.7653518590004751, 0.5893096149993653, 0.6098649259993181, 0.627659273000063, 0.6795481979988836, 0.6932821009995678, 0.5418393049994847, 0.5550858129990957, 1.0345903489987904, 0.4285439129998849, 0.47664217699912115, 0.780425739999373, 0.6395673509996413, 0.6339497910003047, 0.5610617039992576, 0.8505138980008269, 0.8996547970000393, 0.6999611720002576, 0.9955837889992836, 0.6012260229990716, 0.6188406739993297, 1.0451105230004032, 0.49761767699965276, 0.43961603899879265, 0.719650811000065, 0.6752207439994891, 0.6333552919995782, 0.5035851480006386, 0.7667075449999174, 0.6502564389993495, 0.9318712949989276, 0.766181573000722, 0.6294566500000656, 0.8987375269998665, 0.7255996530002449, 0.6383179340000424, 0.5427978800016717, 0.977040940999359, 0.4669238159995075, 0.4345744049996938, 0.5190234580004471, 0.9778660929996477, 0.8135988289986926, 0.6639286489998995, 0.7045890449999206, 0.8331600269993942, 0.6082587669998247, 0.7569831890004934, 1.1006101309994847, 0.6310421429989219, 0.6884784800004127, 0.7435781200001657, 0.5998042089995579, 0.6324974760000259, 0.6867782279996391, 0.4676261250006064, 0.8787418790006996, 0.6074084690008021, 0.5580773560004673, 0.5785792389997368, 0.7070165490004001, 0.6052366220001204, 0.7196880339997733, 0.9586037639992355, 0.6325265460009177, 0.5954384870001377, 0.9026793129987709, 0.618183743001282, 0.5346605800014004, 0.747848336000061, 0.7178793649991349, 0.7156382480006869, 0.6691885990003357, 0.9212497649987199, 0.6439317300009861, 0.5995938589994694, 0.8032303920008417, 0.6145272750000004, 0.5607453300008274, 0.6247990710007798, 0.5026025160004792, 1.056423530999382, 0.5970576439995057, 0.6497118289989885, 0.9726316729993414, 0.48048660200038285, 0.4576721980001821, 0.5647308550005619, 0.9409707579998212, 0.6952405519987224, 0.5512004929987597, 0.8634989769998356, 0.8407755979997091, 0.6994679439994798, 0.7073800079997454, 0.9933228909994796]
Total Epoch List: [142, 26, 29]
Total Time List: [0.12670906799939985, 0.1179172229994947, 0.20040372199946432]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049ec80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7661;  Loss pred: 0.7661; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7568 score: 0.4884 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7524 score: 0.5000 time: 0.12s
Epoch 2/1000, LR 0.000015
Train loss: 0.8330;  Loss pred: 0.8330; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7624 score: 0.4884 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7576 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.7629;  Loss pred: 0.7629; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7657 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7608 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.7922;  Loss pred: 0.7922; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7668 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7621 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.7932;  Loss pred: 0.7932; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7662 score: 0.4884 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7619 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.7739;  Loss pred: 0.7739; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7618 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7580 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.7437;  Loss pred: 0.7437; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7556 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7526 score: 0.5000 time: 0.18s
Epoch 8/1000, LR 0.000195
Train loss: 0.7524;  Loss pred: 0.7524; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7463 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7444 score: 0.5000 time: 0.17s
Epoch 9/1000, LR 0.000225
Train loss: 0.7122;  Loss pred: 0.7122; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7363 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7353 score: 0.5000 time: 0.15s
Epoch 10/1000, LR 0.000255
Train loss: 0.7212;  Loss pred: 0.7212; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7237 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7239 score: 0.5000 time: 0.14s
Epoch 11/1000, LR 0.000285
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7133 score: 0.4884 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7151 score: 0.5000 time: 0.41s
Epoch 12/1000, LR 0.000285
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7029 score: 0.4884 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7058 score: 0.5000 time: 0.15s
Epoch 13/1000, LR 0.000285
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.13s
Epoch 14/1000, LR 0.000285
Train loss: 0.6274;  Loss pred: 0.6274; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.4884 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5000 time: 0.26s
Epoch 15/1000, LR 0.000285
Train loss: 0.6199;  Loss pred: 0.6199; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6725 score: 0.4884 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6778 score: 0.5000 time: 0.29s
Epoch 16/1000, LR 0.000285
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6648 score: 0.4884 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6710 score: 0.5000 time: 0.13s
Epoch 17/1000, LR 0.000285
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.30s
Val loss: 0.6554 score: 0.5116 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6627 score: 0.5000 time: 0.12s
Epoch 18/1000, LR 0.000285
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 0.43s
Val loss: 0.6476 score: 0.5116 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6559 score: 0.5000 time: 0.13s
Epoch 19/1000, LR 0.000285
Train loss: 0.5710;  Loss pred: 0.5710; Loss self: 0.0000; time: 0.32s
Val loss: 0.6406 score: 0.6047 time: 0.12s
Test loss: 0.6498 score: 0.5227 time: 0.13s
Epoch 20/1000, LR 0.000285
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.56s
Val loss: 0.6348 score: 0.6512 time: 0.09s
Test loss: 0.6453 score: 0.5682 time: 0.11s
Epoch 21/1000, LR 0.000285
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.22s
Val loss: 0.6278 score: 0.7209 time: 0.24s
Test loss: 0.6395 score: 0.6364 time: 0.11s
Epoch 22/1000, LR 0.000285
Train loss: 0.4995;  Loss pred: 0.4995; Loss self: 0.0000; time: 0.26s
Val loss: 0.6210 score: 0.7907 time: 0.09s
Test loss: 0.6341 score: 0.7273 time: 0.08s
Epoch 23/1000, LR 0.000285
Train loss: 0.4932;  Loss pred: 0.4932; Loss self: 0.0000; time: 0.52s
Val loss: 0.6148 score: 0.7907 time: 0.17s
Test loss: 0.6298 score: 0.7727 time: 0.14s
Epoch 24/1000, LR 0.000285
Train loss: 0.5177;  Loss pred: 0.5177; Loss self: 0.0000; time: 0.31s
Val loss: 0.6079 score: 0.7907 time: 0.18s
Test loss: 0.6250 score: 0.8182 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.4981;  Loss pred: 0.4981; Loss self: 0.0000; time: 0.38s
Val loss: 0.6013 score: 0.8372 time: 0.12s
Test loss: 0.6201 score: 0.8409 time: 0.23s
Epoch 26/1000, LR 0.000285
Train loss: 0.4989;  Loss pred: 0.4989; Loss self: 0.0000; time: 0.38s
Val loss: 0.5952 score: 0.8372 time: 0.13s
Test loss: 0.6155 score: 0.8409 time: 0.11s
Epoch 27/1000, LR 0.000285
Train loss: 0.4687;  Loss pred: 0.4687; Loss self: 0.0000; time: 0.63s
Val loss: 0.5888 score: 0.8372 time: 0.24s
Test loss: 0.6110 score: 0.8409 time: 0.32s
Epoch 28/1000, LR 0.000285
Train loss: 0.5004;  Loss pred: 0.5004; Loss self: 0.0000; time: 0.40s
Val loss: 0.5826 score: 0.8837 time: 0.15s
Test loss: 0.6066 score: 0.8409 time: 0.21s
Epoch 29/1000, LR 0.000285
Train loss: 0.4784;  Loss pred: 0.4784; Loss self: 0.0000; time: 0.30s
Val loss: 0.5772 score: 0.8837 time: 0.18s
Test loss: 0.6028 score: 0.8182 time: 0.37s
Epoch 30/1000, LR 0.000285
Train loss: 0.4414;  Loss pred: 0.4414; Loss self: 0.0000; time: 0.42s
Val loss: 0.5717 score: 0.8837 time: 0.12s
Test loss: 0.5987 score: 0.8636 time: 0.13s
Epoch 31/1000, LR 0.000285
Train loss: 0.4460;  Loss pred: 0.4460; Loss self: 0.0000; time: 0.27s
Val loss: 0.5661 score: 0.8605 time: 0.13s
Test loss: 0.5943 score: 0.8636 time: 0.19s
Epoch 32/1000, LR 0.000285
Train loss: 0.4358;  Loss pred: 0.4358; Loss self: 0.0000; time: 0.31s
Val loss: 0.5604 score: 0.8605 time: 0.12s
Test loss: 0.5893 score: 0.8409 time: 0.07s
Epoch 33/1000, LR 0.000285
Train loss: 0.4513;  Loss pred: 0.4513; Loss self: 0.0000; time: 0.28s
Val loss: 0.5555 score: 0.8605 time: 0.10s
Test loss: 0.5848 score: 0.8409 time: 0.48s
Epoch 34/1000, LR 0.000285
Train loss: 0.4552;  Loss pred: 0.4552; Loss self: 0.0000; time: 0.24s
Val loss: 0.5506 score: 0.8605 time: 0.08s
Test loss: 0.5791 score: 0.8182 time: 0.09s
Epoch 35/1000, LR 0.000285
Train loss: 0.4029;  Loss pred: 0.4029; Loss self: 0.0000; time: 0.28s
Val loss: 0.5460 score: 0.8605 time: 0.10s
Test loss: 0.5737 score: 0.8182 time: 0.13s
Epoch 36/1000, LR 0.000285
Train loss: 0.4083;  Loss pred: 0.4083; Loss self: 0.0000; time: 0.32s
Val loss: 0.5415 score: 0.8605 time: 0.14s
Test loss: 0.5692 score: 0.7955 time: 0.12s
Epoch 37/1000, LR 0.000285
Train loss: 0.4083;  Loss pred: 0.4083; Loss self: 0.0000; time: 0.29s
Val loss: 0.5365 score: 0.8605 time: 0.37s
Test loss: 0.5640 score: 0.7955 time: 0.27s
Epoch 38/1000, LR 0.000284
Train loss: 0.4706;  Loss pred: 0.4706; Loss self: 0.0000; time: 0.31s
Val loss: 0.5302 score: 0.8605 time: 0.08s
Test loss: 0.5579 score: 0.7955 time: 0.11s
Epoch 39/1000, LR 0.000284
Train loss: 0.3936;  Loss pred: 0.3936; Loss self: 0.0000; time: 0.25s
Val loss: 0.5244 score: 0.8605 time: 0.11s
Test loss: 0.5526 score: 0.7955 time: 0.12s
Epoch 40/1000, LR 0.000284
Train loss: 0.3700;  Loss pred: 0.3700; Loss self: 0.0000; time: 0.27s
Val loss: 0.5187 score: 0.8605 time: 0.10s
Test loss: 0.5476 score: 0.7955 time: 0.11s
Epoch 41/1000, LR 0.000284
Train loss: 0.3682;  Loss pred: 0.3682; Loss self: 0.0000; time: 0.22s
Val loss: 0.5124 score: 0.8605 time: 0.10s
Test loss: 0.5427 score: 0.8182 time: 0.13s
Epoch 42/1000, LR 0.000284
Train loss: 0.3589;  Loss pred: 0.3589; Loss self: 0.0000; time: 0.57s
Val loss: 0.5069 score: 0.8837 time: 0.14s
Test loss: 0.5384 score: 0.8182 time: 0.13s
Epoch 43/1000, LR 0.000284
Train loss: 0.3596;  Loss pred: 0.3596; Loss self: 0.0000; time: 0.34s
Val loss: 0.5006 score: 0.8837 time: 0.14s
Test loss: 0.5339 score: 0.8409 time: 0.12s
Epoch 44/1000, LR 0.000284
Train loss: 0.3684;  Loss pred: 0.3684; Loss self: 0.0000; time: 0.31s
Val loss: 0.4942 score: 0.9070 time: 0.11s
Test loss: 0.5300 score: 0.8636 time: 0.10s
Epoch 45/1000, LR 0.000284
Train loss: 0.3600;  Loss pred: 0.3600; Loss self: 0.0000; time: 0.31s
Val loss: 0.4881 score: 0.9070 time: 0.14s
Test loss: 0.5266 score: 0.8636 time: 0.18s
Epoch 46/1000, LR 0.000284
Train loss: 0.3382;  Loss pred: 0.3382; Loss self: 0.0000; time: 0.32s
Val loss: 0.4814 score: 0.9070 time: 0.13s
Test loss: 0.5220 score: 0.8636 time: 0.37s
Epoch 47/1000, LR 0.000284
Train loss: 0.3457;  Loss pred: 0.3457; Loss self: 0.0000; time: 0.38s
Val loss: 0.4735 score: 0.9070 time: 0.12s
Test loss: 0.5167 score: 0.8636 time: 0.18s
Epoch 48/1000, LR 0.000284
Train loss: 0.3245;  Loss pred: 0.3245; Loss self: 0.0000; time: 0.36s
Val loss: 0.4674 score: 0.9070 time: 0.18s
Test loss: 0.5128 score: 0.8636 time: 0.16s
Epoch 49/1000, LR 0.000284
Train loss: 0.3437;  Loss pred: 0.3437; Loss self: 0.0000; time: 0.32s
Val loss: 0.4606 score: 0.8605 time: 0.12s
Test loss: 0.5078 score: 0.8409 time: 0.12s
Epoch 50/1000, LR 0.000284
Train loss: 0.3278;  Loss pred: 0.3278; Loss self: 0.0000; time: 0.33s
Val loss: 0.4547 score: 0.8605 time: 0.14s
Test loss: 0.5024 score: 0.8409 time: 0.17s
Epoch 51/1000, LR 0.000284
Train loss: 0.3159;  Loss pred: 0.3159; Loss self: 0.0000; time: 0.27s
Val loss: 0.4485 score: 0.8605 time: 0.30s
Test loss: 0.4970 score: 0.8409 time: 0.11s
Epoch 52/1000, LR 0.000284
Train loss: 0.3348;  Loss pred: 0.3348; Loss self: 0.0000; time: 0.24s
Val loss: 0.4421 score: 0.8605 time: 0.11s
Test loss: 0.4919 score: 0.8409 time: 0.10s
Epoch 53/1000, LR 0.000284
Train loss: 0.3077;  Loss pred: 0.3077; Loss self: 0.0000; time: 0.23s
Val loss: 0.4370 score: 0.8605 time: 0.09s
Test loss: 0.4882 score: 0.8409 time: 0.11s
Epoch 54/1000, LR 0.000284
Train loss: 0.3064;  Loss pred: 0.3064; Loss self: 0.0000; time: 0.22s
Val loss: 0.4309 score: 0.8605 time: 0.11s
Test loss: 0.4835 score: 0.8636 time: 0.13s
Epoch 55/1000, LR 0.000284
Train loss: 0.2939;  Loss pred: 0.2939; Loss self: 0.0000; time: 0.36s
Val loss: 0.4268 score: 0.8605 time: 0.11s
Test loss: 0.4805 score: 0.8636 time: 0.10s
Epoch 56/1000, LR 0.000284
Train loss: 0.2842;  Loss pred: 0.2842; Loss self: 0.0000; time: 0.33s
Val loss: 0.4221 score: 0.8605 time: 0.15s
Test loss: 0.4772 score: 0.8409 time: 0.15s
Epoch 57/1000, LR 0.000283
Train loss: 0.2860;  Loss pred: 0.2860; Loss self: 0.0000; time: 0.48s
Val loss: 0.4181 score: 0.8837 time: 0.12s
Test loss: 0.4745 score: 0.8182 time: 0.13s
Epoch 58/1000, LR 0.000283
Train loss: 0.2863;  Loss pred: 0.2863; Loss self: 0.0000; time: 0.28s
Val loss: 0.4162 score: 0.8837 time: 0.11s
Test loss: 0.4734 score: 0.8409 time: 0.12s
Epoch 59/1000, LR 0.000283
Train loss: 0.2857;  Loss pred: 0.2857; Loss self: 0.0000; time: 0.28s
Val loss: 0.4127 score: 0.8605 time: 0.12s
Test loss: 0.4710 score: 0.8182 time: 0.11s
Epoch 60/1000, LR 0.000283
Train loss: 0.2797;  Loss pred: 0.2797; Loss self: 0.0000; time: 0.28s
Val loss: 0.4079 score: 0.8605 time: 0.07s
Test loss: 0.4675 score: 0.8182 time: 0.07s
Epoch 61/1000, LR 0.000283
Train loss: 0.2692;  Loss pred: 0.2692; Loss self: 0.0000; time: 0.33s
Val loss: 0.4057 score: 0.8605 time: 0.24s
Test loss: 0.4654 score: 0.8182 time: 0.12s
Epoch 62/1000, LR 0.000283
Train loss: 0.2787;  Loss pred: 0.2787; Loss self: 0.0000; time: 0.35s
Val loss: 0.4047 score: 0.8605 time: 0.34s
Test loss: 0.4652 score: 0.8182 time: 0.12s
Epoch 63/1000, LR 0.000283
Train loss: 0.2749;  Loss pred: 0.2749; Loss self: 0.0000; time: 0.33s
Val loss: 0.4036 score: 0.8372 time: 0.12s
Test loss: 0.4652 score: 0.8182 time: 0.24s
Epoch 64/1000, LR 0.000283
Train loss: 0.2610;  Loss pred: 0.2610; Loss self: 0.0000; time: 0.34s
Val loss: 0.4044 score: 0.8372 time: 0.13s
Test loss: 0.4667 score: 0.8182 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.2697;  Loss pred: 0.2697; Loss self: 0.0000; time: 0.31s
Val loss: 0.4044 score: 0.8372 time: 0.12s
Test loss: 0.4675 score: 0.8182 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.2645;  Loss pred: 0.2645; Loss self: 0.0000; time: 0.31s
Val loss: 0.4038 score: 0.8372 time: 0.12s
Test loss: 0.4680 score: 0.8182 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.2438;  Loss pred: 0.2438; Loss self: 0.0000; time: 0.57s
Val loss: 0.4046 score: 0.8140 time: 0.19s
Test loss: 0.4682 score: 0.8182 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.2367;  Loss pred: 0.2367; Loss self: 0.0000; time: 0.39s
Val loss: 0.4039 score: 0.8140 time: 0.12s
Test loss: 0.4682 score: 0.8182 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.2494;  Loss pred: 0.2494; Loss self: 0.0000; time: 0.33s
Val loss: 0.4023 score: 0.8140 time: 0.11s
Test loss: 0.4668 score: 0.8182 time: 0.24s
Epoch 70/1000, LR 0.000283
Train loss: 0.2579;  Loss pred: 0.2579; Loss self: 0.0000; time: 0.27s
Val loss: 0.4019 score: 0.8605 time: 0.14s
Test loss: 0.4668 score: 0.8182 time: 0.14s
Epoch 71/1000, LR 0.000282
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.55s
Val loss: 0.3994 score: 0.8605 time: 0.11s
Test loss: 0.4648 score: 0.8182 time: 0.10s
Epoch 72/1000, LR 0.000282
Train loss: 0.2327;  Loss pred: 0.2327; Loss self: 0.0000; time: 0.35s
Val loss: 0.3960 score: 0.8605 time: 0.08s
Test loss: 0.4612 score: 0.8409 time: 0.09s
Epoch 73/1000, LR 0.000282
Train loss: 0.2341;  Loss pred: 0.2341; Loss self: 0.0000; time: 0.25s
Val loss: 0.3922 score: 0.8605 time: 0.08s
Test loss: 0.4575 score: 0.8182 time: 0.09s
Epoch 74/1000, LR 0.000282
Train loss: 0.2190;  Loss pred: 0.2190; Loss self: 0.0000; time: 0.31s
Val loss: 0.3902 score: 0.8837 time: 0.20s
Test loss: 0.4557 score: 0.7955 time: 0.12s
Epoch 75/1000, LR 0.000282
Train loss: 0.2355;  Loss pred: 0.2355; Loss self: 0.0000; time: 0.32s
Val loss: 0.3912 score: 0.8605 time: 0.10s
Test loss: 0.4566 score: 0.7955 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.2166;  Loss pred: 0.2166; Loss self: 0.0000; time: 0.66s
Val loss: 0.3904 score: 0.8605 time: 0.10s
Test loss: 0.4558 score: 0.7955 time: 0.25s
     INFO: Early stopping counter 2 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.2246;  Loss pred: 0.2246; Loss self: 0.0000; time: 0.35s
Val loss: 0.3942 score: 0.8605 time: 0.10s
Test loss: 0.4586 score: 0.8182 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.2218;  Loss pred: 0.2218; Loss self: 0.0000; time: 0.24s
Val loss: 0.3958 score: 0.8605 time: 0.17s
Test loss: 0.4599 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.2133;  Loss pred: 0.2133; Loss self: 0.0000; time: 0.28s
Val loss: 0.4016 score: 0.8605 time: 0.31s
Test loss: 0.4646 score: 0.8409 time: 0.40s
     INFO: Early stopping counter 5 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.1932;  Loss pred: 0.1932; Loss self: 0.0000; time: 0.31s
Val loss: 0.4077 score: 0.8372 time: 0.14s
Test loss: 0.4697 score: 0.8182 time: 0.13s
     INFO: Early stopping counter 6 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.2183;  Loss pred: 0.2183; Loss self: 0.0000; time: 0.29s
Val loss: 0.4074 score: 0.8372 time: 0.14s
Test loss: 0.4694 score: 0.8182 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.2090;  Loss pred: 0.2090; Loss self: 0.0000; time: 0.30s
Val loss: 0.4078 score: 0.8372 time: 0.22s
Test loss: 0.4701 score: 0.7955 time: 0.26s
     INFO: Early stopping counter 8 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.1897;  Loss pred: 0.1897; Loss self: 0.0000; time: 0.42s
Val loss: 0.4114 score: 0.8140 time: 0.10s
Test loss: 0.4726 score: 0.7955 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.31s
Val loss: 0.4132 score: 0.8140 time: 0.19s
Test loss: 0.4734 score: 0.7955 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.1852;  Loss pred: 0.1852; Loss self: 0.0000; time: 0.28s
Val loss: 0.4128 score: 0.8140 time: 0.18s
Test loss: 0.4720 score: 0.7955 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.1963;  Loss pred: 0.1963; Loss self: 0.0000; time: 0.43s
Val loss: 0.4118 score: 0.8140 time: 0.16s
Test loss: 0.4698 score: 0.7955 time: 0.14s
     INFO: Early stopping counter 12 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.1922;  Loss pred: 0.1922; Loss self: 0.0000; time: 0.40s
Val loss: 0.4103 score: 0.8140 time: 0.13s
Test loss: 0.4670 score: 0.8182 time: 0.39s
     INFO: Early stopping counter 13 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.1966;  Loss pred: 0.1966; Loss self: 0.0000; time: 0.31s
Val loss: 0.4109 score: 0.8140 time: 0.12s
Test loss: 0.4674 score: 0.8182 time: 0.24s
     INFO: Early stopping counter 14 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.1883;  Loss pred: 0.1883; Loss self: 0.0000; time: 0.31s
Val loss: 0.4116 score: 0.8140 time: 0.15s
Test loss: 0.4672 score: 0.8182 time: 0.11s
     INFO: Early stopping counter 15 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.1937;  Loss pred: 0.1937; Loss self: 0.0000; time: 0.26s
Val loss: 0.4130 score: 0.8140 time: 0.09s
Test loss: 0.4687 score: 0.8182 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.1752;  Loss pred: 0.1752; Loss self: 0.0000; time: 0.25s
Val loss: 0.4167 score: 0.8140 time: 0.26s
Test loss: 0.4727 score: 0.7955 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.1715;  Loss pred: 0.1715; Loss self: 0.0000; time: 0.46s
Val loss: 0.4216 score: 0.8140 time: 0.10s
Test loss: 0.4778 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1767;  Loss pred: 0.1767; Loss self: 0.0000; time: 0.31s
Val loss: 0.4201 score: 0.8140 time: 0.16s
Test loss: 0.4771 score: 0.7955 time: 0.10s
     INFO: Early stopping counter 19 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.1688;  Loss pred: 0.1688; Loss self: 0.0000; time: 0.33s
Val loss: 0.4184 score: 0.8140 time: 0.12s
Test loss: 0.4759 score: 0.7955 time: 0.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 073,   Train_Loss: 0.2190,   Val_Loss: 0.3902,   Val_Precision: 1.0000,   Val_Recall: 0.7727,   Val_accuracy: 0.8718,   Val_Score: 0.8837,   Val_Loss: 0.3902,   Test_Precision: 0.8824,   Test_Recall: 0.6818,   Test_accuracy: 0.7692,   Test_Score: 0.7955,   Test_loss: 0.4557


[0.12722452400066686, 0.09806141199987906, 0.11957316499956505, 0.1296438279996437, 0.12003866499981086, 0.1385010499998316, 0.18457351800043398, 0.17395222399954946, 0.15977032899991173, 0.14125581499956752, 0.41985147099967435, 0.15905578499950934, 0.14040368899986788, 0.2616285979993336, 0.29791033399942535, 0.13328123500014044, 0.1263923460001024, 0.13544637399991188, 0.13274455900045723, 0.11860781999985193, 0.11689429299985932, 0.08329211899945221, 0.14218129199980467, 0.17718170800071675, 0.23434658600035618, 0.1150742950003405, 0.32553596700017806, 0.21158868899965455, 0.3788514399993801, 0.13674791699941125, 0.19074360800004797, 0.08061353599987342, 0.4836917859993264, 0.09067176200005633, 0.13947237800039147, 0.12716068999998242, 0.2789795549997507, 0.1119985600007567, 0.12622770899997704, 0.10998614100026316, 0.13968187800037413, 0.13858125200022187, 0.12565805800022645, 0.10782500799996342, 0.18502085900036036, 0.377239529999315, 0.18720210100036638, 0.16309600000022328, 0.1212192060002053, 0.17919968699970923, 0.11380097500023112, 0.10982206899916491, 0.11652157400021679, 0.1313218670002243, 0.11009885299972666, 0.1603592840001511, 0.1299671959995976, 0.12137515999984316, 0.11554082299971924, 0.07729332400049316, 0.12118748400007462, 0.12019701600002008, 0.24344660899987502, 0.1137395360001392, 0.13257000600060564, 0.14103941600023973, 0.1572256979998201, 0.12118026300049678, 0.24994416399931652, 0.1439516830005232, 0.10271060500053864, 0.09940705000008165, 0.09553171500010649, 0.12448372000017116, 0.10501942800055986, 0.2557832700003928, 0.11359275499944488, 0.12108320999959687, 0.4002952889995868, 0.13559747999988758, 0.14517505599997094, 0.26673317400036467, 0.1138039919997027, 0.11670398699970974, 0.11536138000064966, 0.1407842469998286, 0.39261755500047, 0.2511974529998042, 0.11796693599990249, 0.10744874099964363, 0.1165048039993053, 0.07840109399967332, 0.10580043599929922, 0.12262095999994926]
[0.0028914664545606106, 0.0022286684545427056, 0.0027175719318082965, 0.0029464506363555383, 0.0027281514772684286, 0.003147751136359809, 0.004194852681828045, 0.0039534596363533965, 0.0036311438409070847, 0.003210359431808353, 0.009542078886356236, 0.003614904204534303, 0.003190992931815179, 0.005946104499984854, 0.006770689409077849, 0.003029118977275919, 0.0028725533181841456, 0.003078326681816179, 0.003016921795464937, 0.0026956322727239076, 0.00265668847726953, 0.0018930027045330048, 0.0032313929999955608, 0.00402685700001629, 0.005326058772735368, 0.0026153248863713748, 0.007398544704549501, 0.0048088338409012395, 0.00861025999998591, 0.0031079072045320736, 0.00433508200000109, 0.0018321258181789415, 0.010992995136348327, 0.0020607218636376438, 0.00316982677273617, 0.0028900156818177825, 0.006340444431812516, 0.002545421818199016, 0.0028688115681812965, 0.002499685022733254, 0.003174588136372139, 0.0031495739090959514, 0.002855864954550601, 0.002450568363635532, 0.0042050195227354625, 0.008573625681802614, 0.004254593204553781, 0.0037067272727323475, 0.0027549819545501205, 0.004072720159084301, 0.0025863857954597984, 0.0024959561136173843, 0.002648217590914018, 0.002984587886368734, 0.0025022466590846966, 0.0036445291818216156, 0.002953799909081764, 0.002758526363632799, 0.0026259277954481645, 0.0017566664545566628, 0.0027542610000016957, 0.00273175036363682, 0.005532877477269887, 0.002584989454548618, 0.0030129546818319464, 0.0032054412727327212, 0.00357331131817773, 0.0027540968863749267, 0.0056805491818026485, 0.0032716291591028, 0.0023343319318304234, 0.002259251136365492, 0.0021711753409115113, 0.0028291754545493445, 0.002386805181830906, 0.005813256136372564, 0.0025816535227146564, 0.0027518911363544744, 0.009097620204536062, 0.0030817609090883543, 0.0032994330909084306, 0.006062117590917379, 0.002586454363629607, 0.002652363340902494, 0.0026218495454693107, 0.003199641977268832, 0.008923126250010682, 0.005709033022722823, 0.002681066727270511, 0.0024420168409009916, 0.002647836454529666, 0.0017818430454471209, 0.0024045553636204368, 0.002786839999998847]
[345.84527115046916, 448.69841360283766, 367.9755403326495, 339.3913977927351, 366.54856166610426, 317.6871222279795, 238.38739423006797, 252.94301497469766, 275.395314483106, 311.4916012493695, 104.7989659181976, 276.6325034964037, 313.38207929879536, 168.17733358075816, 147.695447181382, 330.12899377735835, 348.1223459525338, 324.85181183239814, 331.4636798021111, 370.97048069895334, 376.40845306325565, 528.2612632329522, 309.4640608559138, 248.33263262041703, 187.75609558030055, 382.3616733856141, 135.1617162473968, 207.95062443092993, 116.1405114365462, 321.75992852738983, 230.67614407287994, 545.8140429427273, 90.96701923332077, 485.2668463636194, 315.47465262172915, 346.0188836660613, 157.71765067171077, 392.86219393983924, 348.5763969621599, 400.0504027129629, 315.001492175543, 317.5032651597747, 350.15661311525844, 408.06859944786584, 237.81102432301594, 116.63676921684188, 235.04009711896282, 269.7797616124231, 362.97878407094566, 245.5361431522556, 386.6399211422454, 400.6480701099756, 377.61247543667866, 335.05463336067896, 399.64085729493695, 274.3838641731435, 338.5469668833682, 362.51239545271744, 380.8177824742249, 569.2600307850554, 363.0737972905924, 366.0656600659092, 180.73778140003827, 386.848773499007, 331.900113211121, 311.9695277235494, 279.8524704278962, 363.09543246179965, 176.0393173257701, 305.658114464977, 428.3880909840737, 442.62454222274835, 460.5800283178309, 353.4598741099599, 418.97009760675365, 172.0206329363622, 387.3486473694121, 363.3864678690505, 109.91885542786245, 324.4898061530087, 303.08236974269744, 164.9588588479806, 386.629671128891, 377.02225203419516, 381.4101391622761, 312.5349670695299, 112.06834600135831, 175.1610116844389, 372.9858678370384, 409.49758545933946, 377.6668299468781, 561.21665853519, 415.87730319269605, 358.82935511203146]
Elapsed: 0.1614796562552851~0.08182144871159532
Time per graph: 0.003669992187620116~0.0018595783798089846
Speed: 319.43199211643423~103.65436389797632
Total Time: 0.1233
best val loss: 0.39017765951711075 test_score: 0.7955

Testing...
Test loss: 0.5300 score: 0.8636 time: 0.11s
test Score 0.8636
Epoch Time List: [0.59359449799922, 0.43451698900116753, 0.7289134720003858, 0.586121847000868, 0.6124484910005776, 0.5687859440004104, 0.6336568769993391, 0.8032918189992415, 0.5855596699993839, 0.5508156630003214, 1.1097964669997964, 0.5365317869991486, 0.5800194749999719, 0.725338166000256, 0.6795032690006337, 0.5286194540003635, 0.5132996520014785, 0.7238685470001656, 0.5722424280002087, 0.766421543999968, 0.5687526629999411, 0.42337635199965007, 0.8332226820002688, 0.6636775309998484, 0.7225688010003068, 0.614486215999932, 1.1935887380004715, 0.7493309719993704, 0.8532342839989724, 0.665324196000256, 0.5823150640007952, 0.503838743000415, 0.8555980739984079, 0.40441299300073297, 0.5115125009997428, 0.5850804789988615, 0.9281550149989926, 0.5016845809996084, 0.4780968810000559, 0.46767072700004064, 0.4614939829998548, 0.8414917319996675, 0.5955094389992155, 0.5216268229996786, 0.6277916300005018, 0.8190111890007756, 0.6824635980001403, 0.6893319470000279, 0.5630799470000056, 0.6410911259999921, 0.6794703809991915, 0.4541493970009469, 0.4322822399999495, 0.44832047699947, 0.5744308920002368, 0.6304670720010108, 0.7292201669997667, 0.5130218619997322, 0.516447842001071, 0.41927594000026147, 0.6840180830004101, 0.8140898219999144, 0.6957565830007297, 0.5807775600005698, 0.555309877999207, 0.5617562490006094, 0.9115622089993849, 0.633095525000499, 0.6862871839994114, 0.5527116160001242, 0.7590132610012006, 0.529773553999803, 0.41899082100007945, 0.6238945410004817, 0.5191491510004198, 1.0090812899998127, 0.5536562389997925, 0.5298637099995176, 0.9849563089992444, 0.5772017939998477, 0.5741130789992894, 0.7825496049999856, 0.6265723829992567, 0.6057111229993097, 0.5734202349985935, 0.7329839739995805, 0.9149408049997874, 0.6765166989989666, 0.5824226120003004, 0.45818025399967155, 0.6153089659992474, 0.6344653579999431, 0.5701357669986464, 0.5644534549992386]
Total Epoch List: [94]
Total Time List: [0.12325592399974994]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049fd60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7168;  Loss pred: 0.7168; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.13s
Epoch 2/1000, LR 0.000015
Train loss: 0.7352;  Loss pred: 0.7352; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.26s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.7401;  Loss pred: 0.7401; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5116 time: 0.14s
Epoch 4/1000, LR 0.000075
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.34s
Val loss: 0.6915 score: 0.5227 time: 0.15s
Test loss: 0.6840 score: 0.5349 time: 0.12s
Epoch 5/1000, LR 0.000105
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 0.34s
Val loss: 0.6847 score: 0.5455 time: 0.12s
Test loss: 0.6776 score: 0.5581 time: 0.47s
Epoch 6/1000, LR 0.000135
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.33s
Val loss: 0.6775 score: 0.5455 time: 0.11s
Test loss: 0.6706 score: 0.6279 time: 0.11s
Epoch 7/1000, LR 0.000165
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.33s
Val loss: 0.6713 score: 0.6591 time: 0.11s
Test loss: 0.6653 score: 0.7209 time: 0.13s
Epoch 8/1000, LR 0.000195
Train loss: 0.6267;  Loss pred: 0.6267; Loss self: 0.0000; time: 0.29s
Val loss: 0.6673 score: 0.7045 time: 0.13s
Test loss: 0.6624 score: 0.6512 time: 0.08s
Epoch 9/1000, LR 0.000225
Train loss: 0.6256;  Loss pred: 0.6256; Loss self: 0.0000; time: 0.24s
Val loss: 0.6660 score: 0.5682 time: 0.28s
Test loss: 0.6622 score: 0.5349 time: 0.31s
Epoch 10/1000, LR 0.000255
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6679 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6646 score: 0.4884 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6724 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6698 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5360;  Loss pred: 0.5360; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6783 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6767 score: 0.4884 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.4884 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5254;  Loss pred: 0.5254; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5093;  Loss pred: 0.5093; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4884 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7015 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7039 score: 0.4884 time: 0.33s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5015;  Loss pred: 0.5015; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7046 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.4846;  Loss pred: 0.4846; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7038 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.4661;  Loss pred: 0.4661; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7006 score: 0.4884 time: 0.13s
     INFO: Early stopping counter 11 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4767;  Loss pred: 0.4767; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.41s
     INFO: Early stopping counter 12 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.4494;  Loss pred: 0.4494; Loss self: 0.0000; time: 0.49s
Val loss: 0.6802 score: 0.5227 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.14s
     INFO: Early stopping counter 13 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.4410;  Loss pred: 0.4410; Loss self: 0.0000; time: 0.33s
Val loss: 0.6748 score: 0.5455 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4884 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.4564;  Loss pred: 0.4564; Loss self: 0.0000; time: 0.32s
Val loss: 0.6677 score: 0.5455 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.4884 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.4314;  Loss pred: 0.4314; Loss self: 0.0000; time: 0.54s
Val loss: 0.6604 score: 0.5455 time: 0.35s
Test loss: 0.6750 score: 0.5349 time: 0.12s
Epoch 26/1000, LR 0.000285
Train loss: 0.4112;  Loss pred: 0.4112; Loss self: 0.0000; time: 0.26s
Val loss: 0.6556 score: 0.5455 time: 0.10s
Test loss: 0.6712 score: 0.5349 time: 0.12s
Epoch 27/1000, LR 0.000285
Train loss: 0.3993;  Loss pred: 0.3993; Loss self: 0.0000; time: 0.36s
Val loss: 0.6456 score: 0.5682 time: 0.11s
Test loss: 0.6618 score: 0.5581 time: 0.10s
Epoch 28/1000, LR 0.000285
Train loss: 0.4235;  Loss pred: 0.4235; Loss self: 0.0000; time: 0.36s
Val loss: 0.6396 score: 0.5909 time: 0.12s
Test loss: 0.6565 score: 0.5581 time: 0.13s
Epoch 29/1000, LR 0.000285
Train loss: 0.4007;  Loss pred: 0.4007; Loss self: 0.0000; time: 0.35s
Val loss: 0.6343 score: 0.5909 time: 0.36s
Test loss: 0.6518 score: 0.5814 time: 0.14s
Epoch 30/1000, LR 0.000285
Train loss: 0.3856;  Loss pred: 0.3856; Loss self: 0.0000; time: 0.34s
Val loss: 0.6279 score: 0.6364 time: 0.15s
Test loss: 0.6460 score: 0.5814 time: 0.11s
Epoch 31/1000, LR 0.000285
Train loss: 0.3925;  Loss pred: 0.3925; Loss self: 0.0000; time: 0.34s
Val loss: 0.6222 score: 0.6364 time: 0.18s
Test loss: 0.6408 score: 0.5814 time: 0.13s
Epoch 32/1000, LR 0.000285
Train loss: 0.3726;  Loss pred: 0.3726; Loss self: 0.0000; time: 0.33s
Val loss: 0.6148 score: 0.6364 time: 0.13s
Test loss: 0.6337 score: 0.6279 time: 0.24s
Epoch 33/1000, LR 0.000285
Train loss: 0.3550;  Loss pred: 0.3550; Loss self: 0.0000; time: 0.51s
Val loss: 0.6085 score: 0.6818 time: 0.10s
Test loss: 0.6276 score: 0.6279 time: 0.11s
Epoch 34/1000, LR 0.000285
Train loss: 0.3749;  Loss pred: 0.3749; Loss self: 0.0000; time: 0.27s
Val loss: 0.5999 score: 0.6818 time: 0.10s
Test loss: 0.6189 score: 0.6279 time: 0.14s
Epoch 35/1000, LR 0.000285
Train loss: 0.3549;  Loss pred: 0.3549; Loss self: 0.0000; time: 0.24s
Val loss: 0.5952 score: 0.6818 time: 0.08s
Test loss: 0.6140 score: 0.6279 time: 0.08s
Epoch 36/1000, LR 0.000285
Train loss: 0.3542;  Loss pred: 0.3542; Loss self: 0.0000; time: 0.30s
Val loss: 0.5891 score: 0.6818 time: 0.12s
Test loss: 0.6075 score: 0.6279 time: 0.12s
Epoch 37/1000, LR 0.000285
Train loss: 0.3565;  Loss pred: 0.3565; Loss self: 0.0000; time: 0.33s
Val loss: 0.5810 score: 0.6818 time: 0.12s
Test loss: 0.5991 score: 0.6977 time: 0.13s
Epoch 38/1000, LR 0.000284
Train loss: 0.3344;  Loss pred: 0.3344; Loss self: 0.0000; time: 0.30s
Val loss: 0.5741 score: 0.6818 time: 0.34s
Test loss: 0.5921 score: 0.6977 time: 0.13s
Epoch 39/1000, LR 0.000284
Train loss: 0.3408;  Loss pred: 0.3408; Loss self: 0.0000; time: 0.34s
Val loss: 0.5672 score: 0.7045 time: 0.12s
Test loss: 0.5849 score: 0.6977 time: 0.17s
Epoch 40/1000, LR 0.000284
Train loss: 0.3261;  Loss pred: 0.3261; Loss self: 0.0000; time: 0.35s
Val loss: 0.5611 score: 0.7273 time: 0.12s
Test loss: 0.5786 score: 0.6977 time: 0.12s
Epoch 41/1000, LR 0.000284
Train loss: 0.3254;  Loss pred: 0.3254; Loss self: 0.0000; time: 0.31s
Val loss: 0.5550 score: 0.7273 time: 0.12s
Test loss: 0.5726 score: 0.6977 time: 0.11s
Epoch 42/1000, LR 0.000284
Train loss: 0.3113;  Loss pred: 0.3113; Loss self: 0.0000; time: 0.34s
Val loss: 0.5459 score: 0.7273 time: 0.13s
Test loss: 0.5643 score: 0.6977 time: 0.15s
Epoch 43/1000, LR 0.000284
Train loss: 0.3258;  Loss pred: 0.3258; Loss self: 0.0000; time: 0.53s
Val loss: 0.5375 score: 0.7273 time: 0.18s
Test loss: 0.5566 score: 0.6977 time: 0.11s
Epoch 44/1000, LR 0.000284
Train loss: 0.3113;  Loss pred: 0.3113; Loss self: 0.0000; time: 0.34s
Val loss: 0.5286 score: 0.7273 time: 0.07s
Test loss: 0.5484 score: 0.6977 time: 0.09s
Epoch 45/1000, LR 0.000284
Train loss: 0.3029;  Loss pred: 0.3029; Loss self: 0.0000; time: 0.30s
Val loss: 0.5214 score: 0.7273 time: 0.08s
Test loss: 0.5415 score: 0.6977 time: 0.10s
Epoch 46/1000, LR 0.000284
Train loss: 0.3146;  Loss pred: 0.3146; Loss self: 0.0000; time: 0.23s
Val loss: 0.5125 score: 0.7500 time: 0.07s
Test loss: 0.5337 score: 0.6977 time: 0.09s
Epoch 47/1000, LR 0.000284
Train loss: 0.3033;  Loss pred: 0.3033; Loss self: 0.0000; time: 0.26s
Val loss: 0.5059 score: 0.7727 time: 0.07s
Test loss: 0.5281 score: 0.6977 time: 0.11s
Epoch 48/1000, LR 0.000284
Train loss: 0.3180;  Loss pred: 0.3180; Loss self: 0.0000; time: 0.52s
Val loss: 0.4995 score: 0.7727 time: 0.14s
Test loss: 0.5232 score: 0.7209 time: 0.16s
Epoch 49/1000, LR 0.000284
Train loss: 0.2859;  Loss pred: 0.2859; Loss self: 0.0000; time: 0.32s
Val loss: 0.4929 score: 0.7727 time: 0.11s
Test loss: 0.5179 score: 0.7209 time: 0.12s
Epoch 50/1000, LR 0.000284
Train loss: 0.2883;  Loss pred: 0.2883; Loss self: 0.0000; time: 0.32s
Val loss: 0.4855 score: 0.7727 time: 0.12s
Test loss: 0.5125 score: 0.7209 time: 0.12s
Epoch 51/1000, LR 0.000284
Train loss: 0.2857;  Loss pred: 0.2857; Loss self: 0.0000; time: 0.32s
Val loss: 0.4772 score: 0.7727 time: 0.13s
Test loss: 0.5060 score: 0.7209 time: 0.13s
Epoch 52/1000, LR 0.000284
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.28s
Val loss: 0.4691 score: 0.7727 time: 0.19s
Test loss: 0.4992 score: 0.7209 time: 0.16s
Epoch 53/1000, LR 0.000284
Train loss: 0.2782;  Loss pred: 0.2782; Loss self: 0.0000; time: 0.41s
Val loss: 0.4623 score: 0.7955 time: 0.13s
Test loss: 0.4940 score: 0.7674 time: 0.10s
Epoch 54/1000, LR 0.000284
Train loss: 0.2738;  Loss pred: 0.2738; Loss self: 0.0000; time: 0.25s
Val loss: 0.4571 score: 0.7955 time: 0.08s
Test loss: 0.4905 score: 0.7674 time: 0.11s
Epoch 55/1000, LR 0.000284
Train loss: 0.2911;  Loss pred: 0.2911; Loss self: 0.0000; time: 0.33s
Val loss: 0.4510 score: 0.8182 time: 0.11s
Test loss: 0.4851 score: 0.7674 time: 0.09s
Epoch 56/1000, LR 0.000284
Train loss: 0.2815;  Loss pred: 0.2815; Loss self: 0.0000; time: 0.41s
Val loss: 0.4460 score: 0.8182 time: 0.12s
Test loss: 0.4803 score: 0.7674 time: 0.10s
Epoch 57/1000, LR 0.000283
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 0.37s
Val loss: 0.4420 score: 0.8182 time: 0.13s
Test loss: 0.4763 score: 0.7674 time: 0.10s
Epoch 58/1000, LR 0.000283
Train loss: 0.2642;  Loss pred: 0.2642; Loss self: 0.0000; time: 0.39s
Val loss: 0.4391 score: 0.8182 time: 0.19s
Test loss: 0.4724 score: 0.7674 time: 0.10s
Epoch 59/1000, LR 0.000283
Train loss: 0.2617;  Loss pred: 0.2617; Loss self: 0.0000; time: 0.55s
Val loss: 0.4356 score: 0.8182 time: 0.11s
Test loss: 0.4683 score: 0.7674 time: 0.11s
Epoch 60/1000, LR 0.000283
Train loss: 0.2727;  Loss pred: 0.2727; Loss self: 0.0000; time: 0.31s
Val loss: 0.4325 score: 0.8182 time: 0.13s
Test loss: 0.4648 score: 0.7674 time: 0.10s
Epoch 61/1000, LR 0.000283
Train loss: 0.2577;  Loss pred: 0.2577; Loss self: 0.0000; time: 0.33s
Val loss: 0.4310 score: 0.8409 time: 0.11s
Test loss: 0.4634 score: 0.7674 time: 0.12s
Epoch 62/1000, LR 0.000283
Train loss: 0.2616;  Loss pred: 0.2616; Loss self: 0.0000; time: 0.33s
Val loss: 0.4272 score: 0.8409 time: 0.13s
Test loss: 0.4590 score: 0.7907 time: 0.12s
Epoch 63/1000, LR 0.000283
Train loss: 0.2409;  Loss pred: 0.2409; Loss self: 0.0000; time: 0.38s
Val loss: 0.4224 score: 0.8409 time: 0.11s
Test loss: 0.4528 score: 0.8140 time: 0.12s
Epoch 64/1000, LR 0.000283
Train loss: 0.2580;  Loss pred: 0.2580; Loss self: 0.0000; time: 0.57s
Val loss: 0.4185 score: 0.8409 time: 0.23s
Test loss: 0.4481 score: 0.8140 time: 0.10s
Epoch 65/1000, LR 0.000283
Train loss: 0.2416;  Loss pred: 0.2416; Loss self: 0.0000; time: 0.24s
Val loss: 0.4145 score: 0.8409 time: 0.09s
Test loss: 0.4431 score: 0.8140 time: 0.14s
Epoch 66/1000, LR 0.000283
Train loss: 0.2521;  Loss pred: 0.2521; Loss self: 0.0000; time: 0.26s
Val loss: 0.4112 score: 0.8409 time: 0.10s
Test loss: 0.4392 score: 0.8140 time: 0.08s
Epoch 67/1000, LR 0.000283
Train loss: 0.2356;  Loss pred: 0.2356; Loss self: 0.0000; time: 0.27s
Val loss: 0.4081 score: 0.8409 time: 0.11s
Test loss: 0.4360 score: 0.8140 time: 0.09s
Epoch 68/1000, LR 0.000283
Train loss: 0.2493;  Loss pred: 0.2493; Loss self: 0.0000; time: 0.24s
Val loss: 0.4044 score: 0.8636 time: 0.21s
Test loss: 0.4323 score: 0.8140 time: 0.13s
Epoch 69/1000, LR 0.000283
Train loss: 0.2328;  Loss pred: 0.2328; Loss self: 0.0000; time: 0.36s
Val loss: 0.4018 score: 0.8636 time: 0.14s
Test loss: 0.4286 score: 0.8140 time: 0.30s
Epoch 70/1000, LR 0.000283
Train loss: 0.2362;  Loss pred: 0.2362; Loss self: 0.0000; time: 0.32s
Val loss: 0.4005 score: 0.8636 time: 0.12s
Test loss: 0.4261 score: 0.8140 time: 0.11s
Epoch 71/1000, LR 0.000282
Train loss: 0.2364;  Loss pred: 0.2364; Loss self: 0.0000; time: 0.44s
Val loss: 0.4009 score: 0.8636 time: 0.13s
Test loss: 0.4258 score: 0.8140 time: 0.31s
     INFO: Early stopping counter 1 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.2371;  Loss pred: 0.2371; Loss self: 0.0000; time: 0.34s
Val loss: 0.4001 score: 0.8636 time: 0.12s
Test loss: 0.4249 score: 0.8140 time: 0.15s
Epoch 73/1000, LR 0.000282
Train loss: 0.2416;  Loss pred: 0.2416; Loss self: 0.0000; time: 0.30s
Val loss: 0.4010 score: 0.8636 time: 0.12s
Test loss: 0.4259 score: 0.8140 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.2226;  Loss pred: 0.2226; Loss self: 0.0000; time: 0.31s
Val loss: 0.3989 score: 0.8636 time: 0.11s
Test loss: 0.4221 score: 0.8140 time: 0.28s
Epoch 75/1000, LR 0.000282
Train loss: 0.2232;  Loss pred: 0.2232; Loss self: 0.0000; time: 0.40s
Val loss: 0.3967 score: 0.8636 time: 0.11s
Test loss: 0.4183 score: 0.8140 time: 0.11s
Epoch 76/1000, LR 0.000282
Train loss: 0.2158;  Loss pred: 0.2158; Loss self: 0.0000; time: 0.30s
Val loss: 0.3968 score: 0.8636 time: 0.11s
Test loss: 0.4176 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.2120;  Loss pred: 0.2120; Loss self: 0.0000; time: 0.31s
Val loss: 0.3942 score: 0.8636 time: 0.12s
Test loss: 0.4137 score: 0.8140 time: 0.25s
Epoch 78/1000, LR 0.000282
Train loss: 0.2373;  Loss pred: 0.2373; Loss self: 0.0000; time: 0.58s
Val loss: 0.3927 score: 0.8636 time: 0.13s
Test loss: 0.4112 score: 0.8140 time: 0.10s
Epoch 79/1000, LR 0.000282
Train loss: 0.2181;  Loss pred: 0.2181; Loss self: 0.0000; time: 0.50s
Val loss: 0.3915 score: 0.8636 time: 0.36s
Test loss: 0.4092 score: 0.8140 time: 0.10s
Epoch 80/1000, LR 0.000282
Train loss: 0.2160;  Loss pred: 0.2160; Loss self: 0.0000; time: 0.37s
Val loss: 0.3897 score: 0.8636 time: 0.12s
Test loss: 0.4074 score: 0.8140 time: 0.11s
Epoch 81/1000, LR 0.000281
Train loss: 0.2142;  Loss pred: 0.2142; Loss self: 0.0000; time: 0.34s
Val loss: 0.3881 score: 0.8636 time: 0.12s
Test loss: 0.4060 score: 0.8140 time: 0.22s
Epoch 82/1000, LR 0.000281
Train loss: 0.2036;  Loss pred: 0.2036; Loss self: 0.0000; time: 0.35s
Val loss: 0.3881 score: 0.8636 time: 0.12s
Test loss: 0.4062 score: 0.8140 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.2187;  Loss pred: 0.2187; Loss self: 0.0000; time: 0.35s
Val loss: 0.3876 score: 0.8636 time: 0.14s
Test loss: 0.4056 score: 0.8140 time: 0.11s
Epoch 84/1000, LR 0.000281
Train loss: 0.1960;  Loss pred: 0.1960; Loss self: 0.0000; time: 0.32s
Val loss: 0.3877 score: 0.8636 time: 0.12s
Test loss: 0.4053 score: 0.8140 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.1976;  Loss pred: 0.1976; Loss self: 0.0000; time: 0.32s
Val loss: 0.3874 score: 0.8636 time: 0.12s
Test loss: 0.4042 score: 0.8140 time: 0.09s
Epoch 86/1000, LR 0.000281
Train loss: 0.2108;  Loss pred: 0.2108; Loss self: 0.0000; time: 0.25s
Val loss: 0.3843 score: 0.8636 time: 0.12s
Test loss: 0.4002 score: 0.8140 time: 0.10s
Epoch 87/1000, LR 0.000281
Train loss: 0.1870;  Loss pred: 0.1870; Loss self: 0.0000; time: 0.38s
Val loss: 0.3826 score: 0.8636 time: 0.10s
Test loss: 0.3980 score: 0.8140 time: 0.22s
Epoch 88/1000, LR 0.000281
Train loss: 0.1975;  Loss pred: 0.1975; Loss self: 0.0000; time: 0.47s
Val loss: 0.3800 score: 0.8636 time: 0.11s
Test loss: 0.3941 score: 0.8372 time: 0.18s
Epoch 89/1000, LR 0.000281
Train loss: 0.1954;  Loss pred: 0.1954; Loss self: 0.0000; time: 0.34s
Val loss: 0.3775 score: 0.8636 time: 0.14s
Test loss: 0.3907 score: 0.8372 time: 0.17s
Epoch 90/1000, LR 0.000281
Train loss: 0.1907;  Loss pred: 0.1907; Loss self: 0.0000; time: 0.29s
Val loss: 0.3749 score: 0.8636 time: 0.18s
Test loss: 0.3873 score: 0.8372 time: 0.13s
Epoch 91/1000, LR 0.000280
Train loss: 0.1801;  Loss pred: 0.1801; Loss self: 0.0000; time: 0.33s
Val loss: 0.3735 score: 0.8636 time: 0.18s
Test loss: 0.3848 score: 0.8605 time: 0.44s
Epoch 92/1000, LR 0.000280
Train loss: 0.1829;  Loss pred: 0.1829; Loss self: 0.0000; time: 0.27s
Val loss: 0.3730 score: 0.8636 time: 0.24s
Test loss: 0.3837 score: 0.8605 time: 0.11s
Epoch 93/1000, LR 0.000280
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 0.29s
Val loss: 0.3725 score: 0.8636 time: 0.12s
Test loss: 0.3828 score: 0.8605 time: 0.12s
Epoch 94/1000, LR 0.000280
Train loss: 0.1899;  Loss pred: 0.1899; Loss self: 0.0000; time: 0.28s
Val loss: 0.3718 score: 0.8636 time: 0.10s
Test loss: 0.3817 score: 0.8605 time: 0.13s
Epoch 95/1000, LR 0.000280
Train loss: 0.1821;  Loss pred: 0.1821; Loss self: 0.0000; time: 0.31s
Val loss: 0.3709 score: 0.8636 time: 0.11s
Test loss: 0.3803 score: 0.8605 time: 0.47s
Epoch 96/1000, LR 0.000280
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.38s
Val loss: 0.3705 score: 0.8636 time: 0.14s
Test loss: 0.3799 score: 0.8605 time: 0.29s
Epoch 97/1000, LR 0.000280
Train loss: 0.1917;  Loss pred: 0.1917; Loss self: 0.0000; time: 0.41s
Val loss: 0.3699 score: 0.8636 time: 0.11s
Test loss: 0.3790 score: 0.8605 time: 0.12s
Epoch 98/1000, LR 0.000280
Train loss: 0.1788;  Loss pred: 0.1788; Loss self: 0.0000; time: 0.28s
Val loss: 0.3696 score: 0.8636 time: 0.42s
Test loss: 0.3786 score: 0.8605 time: 0.09s
Epoch 99/1000, LR 0.000279
Train loss: 0.1827;  Loss pred: 0.1827; Loss self: 0.0000; time: 0.68s
Val loss: 0.3673 score: 0.8636 time: 0.13s
Test loss: 0.3751 score: 0.8605 time: 0.15s
Epoch 100/1000, LR 0.000279
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 0.37s
Val loss: 0.3679 score: 0.8636 time: 0.17s
Test loss: 0.3751 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.34s
Val loss: 0.3668 score: 0.8636 time: 0.22s
Test loss: 0.3733 score: 0.8605 time: 0.11s
Epoch 102/1000, LR 0.000279
Train loss: 0.1699;  Loss pred: 0.1699; Loss self: 0.0000; time: 0.42s
Val loss: 0.3651 score: 0.8636 time: 0.11s
Test loss: 0.3710 score: 0.8605 time: 0.10s
Epoch 103/1000, LR 0.000279
Train loss: 0.1667;  Loss pred: 0.1667; Loss self: 0.0000; time: 0.34s
Val loss: 0.3624 score: 0.8636 time: 0.07s
Test loss: 0.3677 score: 0.8605 time: 0.10s
Epoch 104/1000, LR 0.000279
Train loss: 0.1686;  Loss pred: 0.1686; Loss self: 0.0000; time: 0.31s
Val loss: 0.3606 score: 0.8636 time: 0.12s
Test loss: 0.3650 score: 0.8605 time: 0.15s
Epoch 105/1000, LR 0.000279
Train loss: 0.1584;  Loss pred: 0.1584; Loss self: 0.0000; time: 0.62s
Val loss: 0.3588 score: 0.8636 time: 0.10s
Test loss: 0.3622 score: 0.8605 time: 0.34s
Epoch 106/1000, LR 0.000279
Train loss: 0.1662;  Loss pred: 0.1662; Loss self: 0.0000; time: 0.28s
Val loss: 0.3578 score: 0.8409 time: 0.18s
Test loss: 0.3584 score: 0.8605 time: 0.13s
Epoch 107/1000, LR 0.000278
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.33s
Val loss: 0.3595 score: 0.8636 time: 0.40s
Test loss: 0.3585 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1727;  Loss pred: 0.1727; Loss self: 0.0000; time: 0.37s
Val loss: 0.3594 score: 0.8409 time: 0.12s
Test loss: 0.3579 score: 0.8605 time: 0.23s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1563;  Loss pred: 0.1563; Loss self: 0.0000; time: 0.42s
Val loss: 0.3599 score: 0.8409 time: 0.14s
Test loss: 0.3584 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1647;  Loss pred: 0.1647; Loss self: 0.0000; time: 0.28s
Val loss: 0.3591 score: 0.8409 time: 0.10s
Test loss: 0.3576 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1475;  Loss pred: 0.1475; Loss self: 0.0000; time: 0.52s
Val loss: 0.3589 score: 0.8409 time: 0.23s
Test loss: 0.3576 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1474;  Loss pred: 0.1474; Loss self: 0.0000; time: 0.36s
Val loss: 0.3584 score: 0.8409 time: 0.13s
Test loss: 0.3573 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1467;  Loss pred: 0.1467; Loss self: 0.0000; time: 0.33s
Val loss: 0.3586 score: 0.8409 time: 0.15s
Test loss: 0.3573 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1502;  Loss pred: 0.1502; Loss self: 0.0000; time: 0.37s
Val loss: 0.3571 score: 0.8409 time: 0.13s
Test loss: 0.3549 score: 0.8605 time: 0.16s
Epoch 115/1000, LR 0.000277
Train loss: 0.1480;  Loss pred: 0.1480; Loss self: 0.0000; time: 0.76s
Val loss: 0.3548 score: 0.8409 time: 0.12s
Test loss: 0.3521 score: 0.8605 time: 0.16s
Epoch 116/1000, LR 0.000277
Train loss: 0.1438;  Loss pred: 0.1438; Loss self: 0.0000; time: 0.37s
Val loss: 0.3540 score: 0.8409 time: 0.13s
Test loss: 0.3520 score: 0.8605 time: 0.15s
Epoch 117/1000, LR 0.000277
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.34s
Val loss: 0.3536 score: 0.8636 time: 0.13s
Test loss: 0.3522 score: 0.8605 time: 0.16s
Epoch 118/1000, LR 0.000277
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.75s
Val loss: 0.3519 score: 0.8636 time: 0.10s
Test loss: 0.3507 score: 0.8605 time: 0.07s
Epoch 119/1000, LR 0.000277
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.32s
Val loss: 0.3507 score: 0.8636 time: 0.09s
Test loss: 0.3489 score: 0.8605 time: 0.12s
Epoch 120/1000, LR 0.000277
Train loss: 0.1318;  Loss pred: 0.1318; Loss self: 0.0000; time: 0.27s
Val loss: 0.3491 score: 0.8636 time: 0.09s
Test loss: 0.3470 score: 0.8605 time: 0.09s
Epoch 121/1000, LR 0.000276
Train loss: 0.1526;  Loss pred: 0.1526; Loss self: 0.0000; time: 0.47s
Val loss: 0.3472 score: 0.8636 time: 0.11s
Test loss: 0.3455 score: 0.8605 time: 0.14s
Epoch 122/1000, LR 0.000276
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 0.60s
Val loss: 0.3478 score: 0.8636 time: 0.22s
Test loss: 0.3476 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 123/1000, LR 0.000276
Train loss: 0.1359;  Loss pred: 0.1359; Loss self: 0.0000; time: 0.32s
Val loss: 0.3481 score: 0.8636 time: 0.13s
Test loss: 0.3491 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 124/1000, LR 0.000276
Train loss: 0.1328;  Loss pred: 0.1328; Loss self: 0.0000; time: 0.35s
Val loss: 0.3487 score: 0.8636 time: 0.12s
Test loss: 0.3503 score: 0.8605 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 125/1000, LR 0.000276
Train loss: 0.1315;  Loss pred: 0.1315; Loss self: 0.0000; time: 0.39s
Val loss: 0.3484 score: 0.8636 time: 0.13s
Test loss: 0.3504 score: 0.8605 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 126/1000, LR 0.000276
Train loss: 0.1340;  Loss pred: 0.1340; Loss self: 0.0000; time: 0.38s
Val loss: 0.3504 score: 0.8636 time: 0.35s
Test loss: 0.3525 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 127/1000, LR 0.000275
Train loss: 0.1248;  Loss pred: 0.1248; Loss self: 0.0000; time: 0.35s
Val loss: 0.3505 score: 0.8636 time: 0.13s
Test loss: 0.3526 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1218;  Loss pred: 0.1218; Loss self: 0.0000; time: 0.30s
Val loss: 0.3485 score: 0.8636 time: 0.11s
Test loss: 0.3503 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 7 of 20
Epoch 129/1000, LR 0.000275
Train loss: 0.1242;  Loss pred: 0.1242; Loss self: 0.0000; time: 0.27s
Val loss: 0.3464 score: 0.8636 time: 0.12s
Test loss: 0.3469 score: 0.8605 time: 0.12s
Epoch 130/1000, LR 0.000275
Train loss: 0.1236;  Loss pred: 0.1236; Loss self: 0.0000; time: 0.25s
Val loss: 0.3455 score: 0.8409 time: 0.31s
Test loss: 0.3450 score: 0.8605 time: 0.17s
Epoch 131/1000, LR 0.000275
Train loss: 0.1297;  Loss pred: 0.1297; Loss self: 0.0000; time: 0.40s
Val loss: 0.3436 score: 0.8409 time: 0.13s
Test loss: 0.3433 score: 0.8605 time: 0.14s
Epoch 132/1000, LR 0.000275
Train loss: 0.1137;  Loss pred: 0.1137; Loss self: 0.0000; time: 0.39s
Val loss: 0.3431 score: 0.8409 time: 0.14s
Test loss: 0.3438 score: 0.8605 time: 0.12s
Epoch 133/1000, LR 0.000274
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.35s
Val loss: 0.3426 score: 0.8409 time: 0.12s
Test loss: 0.3426 score: 0.8605 time: 0.34s
Epoch 134/1000, LR 0.000274
Train loss: 0.1147;  Loss pred: 0.1147; Loss self: 0.0000; time: 0.35s
Val loss: 0.3416 score: 0.8409 time: 0.13s
Test loss: 0.3409 score: 0.8605 time: 0.13s
Epoch 135/1000, LR 0.000274
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.43s
Val loss: 0.3380 score: 0.8182 time: 0.13s
Test loss: 0.3357 score: 0.8837 time: 0.12s
Epoch 136/1000, LR 0.000274
Train loss: 0.1140;  Loss pred: 0.1140; Loss self: 0.0000; time: 0.30s
Val loss: 0.3337 score: 0.8182 time: 0.13s
Test loss: 0.3296 score: 0.8837 time: 0.10s
Epoch 137/1000, LR 0.000274
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 0.41s
Val loss: 0.3323 score: 0.8409 time: 0.14s
Test loss: 0.3271 score: 0.8837 time: 0.10s
Epoch 138/1000, LR 0.000274
Train loss: 0.1140;  Loss pred: 0.1140; Loss self: 0.0000; time: 0.26s
Val loss: 0.3301 score: 0.8409 time: 0.08s
Test loss: 0.3223 score: 0.8837 time: 0.09s
Epoch 139/1000, LR 0.000273
Train loss: 0.1081;  Loss pred: 0.1081; Loss self: 0.0000; time: 0.36s
Val loss: 0.3285 score: 0.8409 time: 0.14s
Test loss: 0.3183 score: 0.8837 time: 0.12s
Epoch 140/1000, LR 0.000273
Train loss: 0.1094;  Loss pred: 0.1094; Loss self: 0.0000; time: 0.33s
Val loss: 0.3276 score: 0.8409 time: 0.10s
Test loss: 0.3158 score: 0.9070 time: 0.13s
Epoch 141/1000, LR 0.000273
Train loss: 0.1010;  Loss pred: 0.1010; Loss self: 0.0000; time: 0.35s
Val loss: 0.3269 score: 0.8409 time: 0.12s
Test loss: 0.3126 score: 0.9070 time: 0.34s
Epoch 142/1000, LR 0.000273
Train loss: 0.1033;  Loss pred: 0.1033; Loss self: 0.0000; time: 0.37s
Val loss: 0.3247 score: 0.8636 time: 0.13s
Test loss: 0.3074 score: 0.9070 time: 0.19s
Epoch 143/1000, LR 0.000273
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 0.35s
Val loss: 0.3227 score: 0.8409 time: 0.12s
Test loss: 0.3029 score: 0.9070 time: 0.10s
Epoch 144/1000, LR 0.000272
Train loss: 0.0987;  Loss pred: 0.0987; Loss self: 0.0000; time: 0.34s
Val loss: 0.3218 score: 0.8182 time: 0.12s
Test loss: 0.3043 score: 0.9070 time: 0.16s
Epoch 145/1000, LR 0.000272
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.49s
Val loss: 0.3226 score: 0.8409 time: 0.15s
Test loss: 0.3091 score: 0.9070 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0938;  Loss pred: 0.0938; Loss self: 0.0000; time: 0.30s
Val loss: 0.3220 score: 0.8409 time: 0.13s
Test loss: 0.3100 score: 0.9070 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 147/1000, LR 0.000272
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.30s
Val loss: 0.3226 score: 0.8409 time: 0.11s
Test loss: 0.3097 score: 0.9070 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 148/1000, LR 0.000272
Train loss: 0.0846;  Loss pred: 0.0846; Loss self: 0.0000; time: 0.29s
Val loss: 0.3215 score: 0.8409 time: 0.11s
Test loss: 0.3044 score: 0.9070 time: 0.11s
Epoch 149/1000, LR 0.000272
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.58s
Val loss: 0.3216 score: 0.8409 time: 0.10s
Test loss: 0.3025 score: 0.9070 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0857;  Loss pred: 0.0857; Loss self: 0.0000; time: 0.32s
Val loss: 0.3207 score: 0.8409 time: 0.14s
Test loss: 0.3009 score: 0.9070 time: 0.17s
Epoch 151/1000, LR 0.000271
Train loss: 0.0799;  Loss pred: 0.0799; Loss self: 0.0000; time: 0.30s
Val loss: 0.3210 score: 0.8182 time: 0.20s
Test loss: 0.2998 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 152/1000, LR 0.000271
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.34s
Val loss: 0.3213 score: 0.8182 time: 0.17s
Test loss: 0.3005 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 153/1000, LR 0.000271
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.72s
Val loss: 0.3260 score: 0.7955 time: 0.14s
Test loss: 0.3005 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 3 of 20
Epoch 154/1000, LR 0.000271
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.33s
Val loss: 0.3304 score: 0.7955 time: 0.14s
Test loss: 0.3040 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 155/1000, LR 0.000270
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 0.30s
Val loss: 0.3337 score: 0.7955 time: 0.11s
Test loss: 0.3075 score: 0.8837 time: 0.27s
     INFO: Early stopping counter 5 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 0.27s
Val loss: 0.3251 score: 0.8182 time: 0.11s
Test loss: 0.3044 score: 0.8837 time: 0.29s
     INFO: Early stopping counter 6 of 20
Epoch 157/1000, LR 0.000270
Train loss: 0.0627;  Loss pred: 0.0627; Loss self: 0.0000; time: 0.31s
Val loss: 0.3241 score: 0.8182 time: 0.11s
Test loss: 0.3061 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 7 of 20
Epoch 158/1000, LR 0.000270
Train loss: 0.0638;  Loss pred: 0.0638; Loss self: 0.0000; time: 0.46s
Val loss: 0.3338 score: 0.7955 time: 0.13s
Test loss: 0.3122 score: 0.8837 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 159/1000, LR 0.000270
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.41s
Val loss: 0.3287 score: 0.8182 time: 0.19s
Test loss: 0.3078 score: 0.8837 time: 0.22s
     INFO: Early stopping counter 9 of 20
Epoch 160/1000, LR 0.000269
Train loss: 0.0592;  Loss pred: 0.0592; Loss self: 0.0000; time: 0.70s
Val loss: 0.3314 score: 0.8182 time: 0.13s
Test loss: 0.3046 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 10 of 20
Epoch 161/1000, LR 0.000269
Train loss: 0.0550;  Loss pred: 0.0550; Loss self: 0.0000; time: 0.34s
Val loss: 0.3399 score: 0.8182 time: 0.40s
Test loss: 0.3083 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 11 of 20
Epoch 162/1000, LR 0.000269
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.34s
Val loss: 0.3459 score: 0.8182 time: 0.35s
Test loss: 0.3169 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 12 of 20
Epoch 163/1000, LR 0.000269
Train loss: 0.0636;  Loss pred: 0.0636; Loss self: 0.0000; time: 0.36s
Val loss: 0.3349 score: 0.8182 time: 0.12s
Test loss: 0.3123 score: 0.8605 time: 0.21s
     INFO: Early stopping counter 13 of 20
Epoch 164/1000, LR 0.000269
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.26s
Val loss: 0.3217 score: 0.8182 time: 0.26s
Test loss: 0.3043 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 14 of 20
Epoch 165/1000, LR 0.000268
Train loss: 0.0518;  Loss pred: 0.0518; Loss self: 0.0000; time: 0.28s
Val loss: 0.3196 score: 0.8182 time: 0.10s
Test loss: 0.3057 score: 0.8837 time: 0.10s
Epoch 166/1000, LR 0.000268
Train loss: 0.0593;  Loss pred: 0.0593; Loss self: 0.0000; time: 0.24s
Val loss: 0.3230 score: 0.8182 time: 0.08s
Test loss: 0.3097 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 167/1000, LR 0.000268
Train loss: 0.0517;  Loss pred: 0.0517; Loss self: 0.0000; time: 0.56s
Val loss: 0.3318 score: 0.8182 time: 0.12s
Test loss: 0.3173 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 168/1000, LR 0.000268
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.42s
Val loss: 0.3421 score: 0.8409 time: 0.15s
Test loss: 0.3283 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 169/1000, LR 0.000267
Train loss: 0.0416;  Loss pred: 0.0416; Loss self: 0.0000; time: 0.35s
Val loss: 0.3345 score: 0.8182 time: 0.16s
Test loss: 0.3168 score: 0.8837 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 170/1000, LR 0.000267
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.63s
Val loss: 0.3266 score: 0.8182 time: 0.13s
Test loss: 0.3096 score: 0.8837 time: 0.28s
     INFO: Early stopping counter 5 of 20
Epoch 171/1000, LR 0.000267
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.35s
Val loss: 0.3298 score: 0.8182 time: 0.09s
Test loss: 0.3107 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 6 of 20
Epoch 172/1000, LR 0.000267
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.28s
Val loss: 0.3327 score: 0.8182 time: 0.08s
Test loss: 0.3124 score: 0.8605 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 173/1000, LR 0.000267
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 0.35s
Val loss: 0.3342 score: 0.8409 time: 0.31s
Test loss: 0.3134 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 174/1000, LR 0.000266
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 0.58s
Val loss: 0.3431 score: 0.8182 time: 0.14s
Test loss: 0.3203 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 175/1000, LR 0.000266
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.38s
Val loss: 0.3523 score: 0.8409 time: 0.14s
Test loss: 0.3327 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 10 of 20
Epoch 176/1000, LR 0.000266
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.32s
Val loss: 0.3521 score: 0.8409 time: 0.17s
Test loss: 0.3323 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 11 of 20
Epoch 177/1000, LR 0.000266
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.70s
Val loss: 0.3379 score: 0.8409 time: 0.18s
Test loss: 0.3189 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 12 of 20
Epoch 178/1000, LR 0.000265
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.32s
Val loss: 0.3355 score: 0.8636 time: 0.13s
Test loss: 0.3171 score: 0.8605 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 179/1000, LR 0.000265
Train loss: 0.0459;  Loss pred: 0.0459; Loss self: 0.0000; time: 0.33s
Val loss: 0.3376 score: 0.8636 time: 0.12s
Test loss: 0.3195 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.0444;  Loss pred: 0.0444; Loss self: 0.0000; time: 0.35s
Val loss: 0.3469 score: 0.8182 time: 0.15s
Test loss: 0.3294 score: 0.8605 time: 0.34s
     INFO: Early stopping counter 15 of 20
Epoch 181/1000, LR 0.000265
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.43s
Val loss: 0.3541 score: 0.8182 time: 0.10s
Test loss: 0.3385 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 16 of 20
Epoch 182/1000, LR 0.000265
Train loss: 0.0371;  Loss pred: 0.0371; Loss self: 0.0000; time: 0.30s
Val loss: 0.3564 score: 0.8182 time: 0.13s
Test loss: 0.3396 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 17 of 20
Epoch 183/1000, LR 0.000264
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.42s
Val loss: 0.3515 score: 0.8182 time: 0.11s
Test loss: 0.3333 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 18 of 20
Epoch 184/1000, LR 0.000264
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.32s
Val loss: 0.3412 score: 0.8409 time: 0.40s
Test loss: 0.3254 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 19 of 20
Epoch 185/1000, LR 0.000264
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.35s
Val loss: 0.3501 score: 0.8182 time: 0.11s
Test loss: 0.3289 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 164,   Train_Loss: 0.0518,   Val_Loss: 0.3196,   Val_Precision: 0.8500,   Val_Recall: 0.7727,   Val_accuracy: 0.8095,   Val_Score: 0.8182,   Val_Loss: 0.3196,   Test_Precision: 0.9474,   Test_Recall: 0.8182,   Test_accuracy: 0.8780,   Test_Score: 0.8837,   Test_loss: 0.3057


[0.12722452400066686, 0.09806141199987906, 0.11957316499956505, 0.1296438279996437, 0.12003866499981086, 0.1385010499998316, 0.18457351800043398, 0.17395222399954946, 0.15977032899991173, 0.14125581499956752, 0.41985147099967435, 0.15905578499950934, 0.14040368899986788, 0.2616285979993336, 0.29791033399942535, 0.13328123500014044, 0.1263923460001024, 0.13544637399991188, 0.13274455900045723, 0.11860781999985193, 0.11689429299985932, 0.08329211899945221, 0.14218129199980467, 0.17718170800071675, 0.23434658600035618, 0.1150742950003405, 0.32553596700017806, 0.21158868899965455, 0.3788514399993801, 0.13674791699941125, 0.19074360800004797, 0.08061353599987342, 0.4836917859993264, 0.09067176200005633, 0.13947237800039147, 0.12716068999998242, 0.2789795549997507, 0.1119985600007567, 0.12622770899997704, 0.10998614100026316, 0.13968187800037413, 0.13858125200022187, 0.12565805800022645, 0.10782500799996342, 0.18502085900036036, 0.377239529999315, 0.18720210100036638, 0.16309600000022328, 0.1212192060002053, 0.17919968699970923, 0.11380097500023112, 0.10982206899916491, 0.11652157400021679, 0.1313218670002243, 0.11009885299972666, 0.1603592840001511, 0.1299671959995976, 0.12137515999984316, 0.11554082299971924, 0.07729332400049316, 0.12118748400007462, 0.12019701600002008, 0.24344660899987502, 0.1137395360001392, 0.13257000600060564, 0.14103941600023973, 0.1572256979998201, 0.12118026300049678, 0.24994416399931652, 0.1439516830005232, 0.10271060500053864, 0.09940705000008165, 0.09553171500010649, 0.12448372000017116, 0.10501942800055986, 0.2557832700003928, 0.11359275499944488, 0.12108320999959687, 0.4002952889995868, 0.13559747999988758, 0.14517505599997094, 0.26673317400036467, 0.1138039919997027, 0.11670398699970974, 0.11536138000064966, 0.1407842469998286, 0.39261755500047, 0.2511974529998042, 0.11796693599990249, 0.10744874099964363, 0.1165048039993053, 0.07840109399967332, 0.10580043599929922, 0.12262095999994926, 0.13116166000054363, 0.26787199399950623, 0.14221267999982956, 0.12886745299965696, 0.47663879900028405, 0.11103692699998646, 0.1332987249998041, 0.080026480000015, 0.3138955039994471, 0.07140698799958045, 0.11805659200035734, 0.14437734700004512, 0.13848218400016776, 0.17166948900012358, 0.09560071600026276, 0.11911722900003952, 0.33420605799983605, 0.1197489759997552, 0.10932270099965535, 0.1304332069994416, 0.4215004730003784, 0.1428243760001351, 0.17306282099980308, 0.1548828869999852, 0.12712859499970364, 0.12645298999996157, 0.10986586099988926, 0.13192408200029604, 0.1530201189998479, 0.1106806059997325, 0.1333639510003195, 0.24333297599969228, 0.11322574899986648, 0.1483287740002197, 0.08456404100070358, 0.1263594510000985, 0.13032591200044408, 0.14001874699988548, 0.1760986959998263, 0.12870894399929966, 0.11852363199977844, 0.15312589800032583, 0.11890671900073357, 0.0933672649998698, 0.10846099099944695, 0.09069164599986834, 0.1142167280004287, 0.16270968999924662, 0.1286667149997811, 0.12181958899964229, 0.13148656899920752, 0.16645728900039103, 0.10321846299939352, 0.1162057729998196, 0.10021840900026291, 0.10045219299991004, 0.10994682599994121, 0.10239130500031024, 0.117178034999597, 0.10922464900068007, 0.12427361800018843, 0.13084372899993468, 0.12707404399952793, 0.10037076200023876, 0.14849770500040904, 0.08991443000013533, 0.09551527999974496, 0.13569134600038524, 0.3017969949996768, 0.11659813899950677, 0.3145283740004743, 0.15496151499974076, 0.12939907500003756, 0.2889697279997563, 0.1143008250001003, 0.11873709399969812, 0.2618662699997003, 0.10271054199984064, 0.10326764000001276, 0.12037657099972421, 0.22044325099977868, 0.16262285300035728, 0.112856974999886, 0.1262819829998989, 0.09512594299940247, 0.11049359800017555, 0.22307542799990188, 0.1833726069999102, 0.17238469399944734, 0.1328251300001284, 0.4417958509993696, 0.11160788999950455, 0.12918044900015957, 0.13864614100020844, 0.4741559670001152, 0.29590391400051885, 0.12657125500027178, 0.09807566800009226, 0.1578748160000032, 0.12723334499969496, 0.11961628999961249, 0.10050620699985302, 0.10753434100024606, 0.15292522900017502, 0.34709534700050426, 0.13277652700071485, 0.11671684799966897, 0.2339431729997159, 0.12378105099924142, 0.10773859300024924, 0.07911960799992812, 0.1190262909994999, 0.10158490699996037, 0.16214509500059648, 0.16053949899924191, 0.15111312700082635, 0.16928378699958557, 0.07406917800017254, 0.12842739899951994, 0.09505650700066326, 0.14662045699969894, 0.142769648000467, 0.13403790299980756, 0.17129033999935928, 0.19140701800006354, 0.11731240100016294, 0.12625527599993802, 0.13495577499998035, 0.1271844119992238, 0.17425407499922585, 0.14886183100043127, 0.12839207800061558, 0.34837973499998043, 0.13380856799994945, 0.12566891999995278, 0.10574558600001183, 0.10487565800031007, 0.09554877300070075, 0.1306848229996831, 0.13677338200068334, 0.3416135400002531, 0.1989281550004307, 0.10824969500026782, 0.1695181949999096, 0.13541192000047886, 0.12357916800010571, 0.10987278500033426, 0.11548625500017806, 0.1123197959996105, 0.1794513700006064, 0.11997267799961264, 0.13567836499987607, 0.22412913700009085, 0.1612325369997052, 0.2718414439996195, 0.3008028109998122, 0.13154316800046217, 0.1621764000001349, 0.2267729919994963, 0.1464990829999806, 0.13570969800002786, 0.12722929299980024, 0.21396690300025512, 0.11987971999951696, 0.1062655879995873, 0.11214011399988522, 0.19111610599975393, 0.15043345800040697, 0.10863701200014475, 0.28166323300047225, 0.13601341899993713, 0.08960999899954913, 0.0974264160004168, 0.1333935209995616, 0.19241431499995088, 0.14794033199996193, 0.1400226619998648, 0.1574717209996379, 0.13236478399994667, 0.3486907339993195, 0.13487492100011877, 0.10963280000032682, 0.12318115400012175, 0.14218248399993172, 0.16437899500033382]
[0.0028914664545606106, 0.0022286684545427056, 0.0027175719318082965, 0.0029464506363555383, 0.0027281514772684286, 0.003147751136359809, 0.004194852681828045, 0.0039534596363533965, 0.0036311438409070847, 0.003210359431808353, 0.009542078886356236, 0.003614904204534303, 0.003190992931815179, 0.005946104499984854, 0.006770689409077849, 0.003029118977275919, 0.0028725533181841456, 0.003078326681816179, 0.003016921795464937, 0.0026956322727239076, 0.00265668847726953, 0.0018930027045330048, 0.0032313929999955608, 0.00402685700001629, 0.005326058772735368, 0.0026153248863713748, 0.007398544704549501, 0.0048088338409012395, 0.00861025999998591, 0.0031079072045320736, 0.00433508200000109, 0.0018321258181789415, 0.010992995136348327, 0.0020607218636376438, 0.00316982677273617, 0.0028900156818177825, 0.006340444431812516, 0.002545421818199016, 0.0028688115681812965, 0.002499685022733254, 0.003174588136372139, 0.0031495739090959514, 0.002855864954550601, 0.002450568363635532, 0.0042050195227354625, 0.008573625681802614, 0.004254593204553781, 0.0037067272727323475, 0.0027549819545501205, 0.004072720159084301, 0.0025863857954597984, 0.0024959561136173843, 0.002648217590914018, 0.002984587886368734, 0.0025022466590846966, 0.0036445291818216156, 0.002953799909081764, 0.002758526363632799, 0.0026259277954481645, 0.0017566664545566628, 0.0027542610000016957, 0.00273175036363682, 0.005532877477269887, 0.002584989454548618, 0.0030129546818319464, 0.0032054412727327212, 0.00357331131817773, 0.0027540968863749267, 0.0056805491818026485, 0.0032716291591028, 0.0023343319318304234, 0.002259251136365492, 0.0021711753409115113, 0.0028291754545493445, 0.002386805181830906, 0.005813256136372564, 0.0025816535227146564, 0.0027518911363544744, 0.009097620204536062, 0.0030817609090883543, 0.0032994330909084306, 0.006062117590917379, 0.002586454363629607, 0.002652363340902494, 0.0026218495454693107, 0.003199641977268832, 0.008923126250010682, 0.005709033022722823, 0.002681066727270511, 0.0024420168409009916, 0.002647836454529666, 0.0017818430454471209, 0.0024045553636204368, 0.002786839999998847, 0.0030502711628033405, 0.006229581255802471, 0.003307271627903013, 0.002996917511619929, 0.011084623232564745, 0.002582254116278755, 0.003099970348832654, 0.001861080930232907, 0.007299895441847607, 0.0016606276278972197, 0.0027455021395431938, 0.003357612720931282, 0.0032205159069806457, 0.003992313697677293, 0.00222327246512239, 0.0027701681162799886, 0.007772233906972932, 0.0027848599069710514, 0.002542388395340822, 0.003033330395335851, 0.009802336581404149, 0.0033214971162822114, 0.004024716767437281, 0.0036019276046508186, 0.0029564789534814798, 0.002940767209301432, 0.0025550200232532386, 0.0030680019069836287, 0.0035586074186011144, 0.002573967581389128, 0.0031014872325655697, 0.005658906418597495, 0.002633156953485267, 0.003449506372098133, 0.001966605604667525, 0.002938591883723221, 0.0030308351628010252, 0.003256249930229895, 0.004095318511623868, 0.0029932312557976667, 0.0027563635348785685, 0.003561067395356415, 0.002765272534900781, 0.002171331744183019, 0.002522348627894115, 0.0021091080465085663, 0.0026562029767541557, 0.003783946279052247, 0.002992249186041421, 0.0028330136976661, 0.0030578271860280818, 0.00387109974419514, 0.0024004293720789193, 0.002702459837205107, 0.002330660674424719, 0.0023360975116258147, 0.002556902930231191, 0.0023811931395420986, 0.0027250705813859767, 0.0025401081162948853, 0.002890084139539266, 0.003042877418603132, 0.002955210325570417, 0.002334203767447413, 0.0034534350000095127, 0.0020910332558171005, 0.002221285581389418, 0.0031556126976833776, 0.007018534767434345, 0.0027115846278955064, 0.0073146133488482395, 0.0036037561627846686, 0.003009280813954362, 0.006720226232552472, 0.0026581587209325653, 0.00276132776743484, 0.006089913255806984, 0.0023886172558102475, 0.0024015730232561107, 0.00279945513952847, 0.005126587232552993, 0.0037819268139617974, 0.0026245808139508373, 0.00293679030232323, 0.0022122312325442432, 0.0025696185581436173, 0.0051878006511605085, 0.0042644792325560515, 0.004008946372080171, 0.0030889565116308934, 0.010274322116264408, 0.0025955323255698734, 0.003004196488375804, 0.0032243288604699638, 0.011026882953491052, 0.00688148637210509, 0.002943517558145855, 0.002280829488374239, 0.0036715073488372833, 0.002958914999992906, 0.0027817741860375, 0.0023373536511593724, 0.002500798627912699, 0.0035564006744226748, 0.008071984813965215, 0.00308782620931895, 0.002714345302317883, 0.005440538906970137, 0.0028786290930056142, 0.002505548674424401, 0.0018399908837192587, 0.002768053279058137, 0.0023624396976734967, 0.0037708161628045695, 0.0037334767209126025, 0.003514258767461078, 0.003936832255804316, 0.0017225390232598265, 0.0029866836976632544, 0.00221061644187589, 0.0034097780697604405, 0.0033202243721038834, 0.0031171605348792454, 0.0039834962790548665, 0.004451326000001477, 0.0027281953720968127, 0.002936169209300884, 0.00313850639534838, 0.0029577770232377633, 0.0040524203488192054, 0.0034619030465216573, 0.0029858622790840833, 0.008101854302325127, 0.003111827162789522, 0.0029225330232547156, 0.0024591996744188797, 0.0024389687907048854, 0.0022220644883883893, 0.0030391819302251883, 0.003180776325597287, 0.007944500930238445, 0.0046262361628007145, 0.002517434767448089, 0.00394228360464906, 0.0031491144186157876, 0.002873934139537342, 0.0025551810465194013, 0.0026857268604692573, 0.0026120882790607093, 0.004173287674432707, 0.002790062279060759, 0.0031553108139506063, 0.00521230551163002, 0.003749593883714074, 0.00632189404650278, 0.0069954142092979585, 0.0030591434418712135, 0.0037715441860496487, 0.005273790511616193, 0.0034069554186042, 0.003156039488372741, 0.002958820767437215, 0.004975974488378026, 0.002787900465105046, 0.0024712927441764487, 0.0026079096279043074, 0.00444456060464544, 0.0034984525116373713, 0.00252644213953825, 0.006550307744197029, 0.0031631027674403982, 0.0020839534651057937, 0.0022657306046608558, 0.0031021749069665484, 0.004474751511626764, 0.0034404728372084167, 0.0032563409767410417, 0.0036621330465032072, 0.003078250790696434, 0.008109086837193476, 0.003136626069770204, 0.0025496000000076006, 0.0028646780000028313, 0.0033065693953472494, 0.0038227673255891584]
[345.84527115046916, 448.69841360283766, 367.9755403326495, 339.3913977927351, 366.54856166610426, 317.6871222279795, 238.38739423006797, 252.94301497469766, 275.395314483106, 311.4916012493695, 104.7989659181976, 276.6325034964037, 313.38207929879536, 168.17733358075816, 147.695447181382, 330.12899377735835, 348.1223459525338, 324.85181183239814, 331.4636798021111, 370.97048069895334, 376.40845306325565, 528.2612632329522, 309.4640608559138, 248.33263262041703, 187.75609558030055, 382.3616733856141, 135.1617162473968, 207.95062443092993, 116.1405114365462, 321.75992852738983, 230.67614407287994, 545.8140429427273, 90.96701923332077, 485.2668463636194, 315.47465262172915, 346.0188836660613, 157.71765067171077, 392.86219393983924, 348.5763969621599, 400.0504027129629, 315.001492175543, 317.5032651597747, 350.15661311525844, 408.06859944786584, 237.81102432301594, 116.63676921684188, 235.04009711896282, 269.7797616124231, 362.97878407094566, 245.5361431522556, 386.6399211422454, 400.6480701099756, 377.61247543667866, 335.05463336067896, 399.64085729493695, 274.3838641731435, 338.5469668833682, 362.51239545271744, 380.8177824742249, 569.2600307850554, 363.0737972905924, 366.0656600659092, 180.73778140003827, 386.848773499007, 331.900113211121, 311.9695277235494, 279.8524704278962, 363.09543246179965, 176.0393173257701, 305.658114464977, 428.3880909840737, 442.62454222274835, 460.5800283178309, 353.4598741099599, 418.97009760675365, 172.0206329363622, 387.3486473694121, 363.3864678690505, 109.91885542786245, 324.4898061530087, 303.08236974269744, 164.9588588479806, 386.629671128891, 377.02225203419516, 381.4101391622761, 312.5349670695299, 112.06834600135831, 175.1610116844389, 372.9858678370384, 409.49758545933946, 377.6668299468781, 561.21665853519, 415.87730319269605, 358.82935511203146, 327.83970559553586, 160.52443317414983, 302.3640367374522, 333.6761843202912, 90.21506451046251, 387.25855588569414, 322.5837306400582, 537.3221463694509, 136.9882634574968, 602.1819601220632, 364.23209641584305, 297.8306562177414, 310.5092565549653, 250.48131878559414, 449.78742627703554, 360.9889212582819, 128.6631375186535, 359.0844902096524, 393.3309331621395, 329.67064897962746, 102.01649287345259, 301.06905560686175, 248.46468901630195, 277.62912244787964, 338.24018899996685, 340.04731718888627, 391.38636523354, 325.9450386010911, 281.0087998954094, 388.505281585682, 322.42596051984833, 176.71258826857226, 379.7722724717918, 289.89655102209844, 508.4903641211072, 340.299041026749, 329.94206094528215, 307.1017340273383, 244.18125163199625, 334.087116744847, 362.7968471307087, 280.8146796951911, 361.62800858826756, 460.54685226197813, 396.4559018294353, 474.13407845814623, 376.4772529627937, 264.27436497604475, 334.1967656520426, 352.98099717054754, 327.02959950425935, 258.3245243162586, 416.59213623683445, 370.03325127458817, 429.062888035742, 428.0643230958483, 391.0981477539242, 419.95753447882817, 366.9629721999339, 393.68403005563573, 346.01068748102915, 328.6363078204647, 338.38539049060046, 428.411612536106, 289.56676468421887, 478.2324705827005, 450.18974974595494, 316.8956699705663, 142.47988121964582, 368.7880472962085, 136.71262612362966, 277.4882524868961, 332.30531207419773, 148.80451422245955, 376.20025927163925, 362.1446217987222, 164.20595138140246, 418.65225480035645, 416.39375122734174, 357.2123681783436, 195.06153989737328, 264.415481629175, 381.01322492511815, 340.5077983296669, 452.03231257607723, 389.1628182832066, 192.75991257996785, 234.4952209793312, 249.44209954126123, 323.733919928845, 97.33002223251155, 385.2774208005445, 332.8677081773178, 310.1420615806058, 90.68745938610017, 145.3174424719661, 339.7295855200904, 438.4369831665031, 272.36769669457055, 337.9617190768905, 359.48280957501106, 427.83427296249357, 399.8722603405512, 281.18316566294493, 123.88526775594468, 323.8524231001199, 368.41296468288755, 183.80532096165174, 347.38758196732005, 399.11417814692015, 543.4809535461685, 361.2647225996538, 423.29122770193385, 265.19457773201106, 267.8468555592232, 284.55502743825065, 254.01133068995716, 580.5383718434115, 334.8195193158178, 452.3625089621689, 293.2742188908079, 301.18446464096735, 320.80478012299056, 251.0357560161352, 224.65215982825524, 366.5426641463103, 340.57982654143586, 318.6228970194589, 338.0917466541609, 246.7661086272504, 288.8584650008465, 334.9116290476569, 123.42853409657256, 321.35460862279194, 342.16893087022754, 406.6363583251143, 410.00934649556973, 450.03194336869865, 329.03591261017567, 314.388657873395, 125.87323090287387, 216.15844172438486, 397.2297566278927, 253.66008645870096, 317.5495923833584, 347.95508576302456, 391.3617007147783, 372.3386822088369, 382.83545315688707, 239.6192350041947, 358.4149384423913, 316.9259889005832, 191.8536812105004, 266.6955491749077, 158.18044286161233, 142.95079177310893, 326.8888886715038, 265.14338707706077, 189.61693639468103, 293.51719559914034, 316.8528162223983, 337.9724824853622, 200.96566056269333, 358.69286314793914, 404.6465164260607, 383.44887004523656, 224.99411954351646, 285.84066717314755, 395.8135372863781, 152.66458295580208, 316.14527681287, 479.8571641566066, 441.3587378582832, 322.3544867680742, 223.47609636014337, 290.6577227365629, 307.09314753665745, 273.0649016028654, 324.859820721026, 123.31844757332702, 318.8139031418757, 392.2183871968226, 349.0793729693221, 302.42825128882015, 261.5906004286781]
Elapsed: 0.15648467315767575~0.07577615036610992
Time per graph: 0.003610422984525736~0.0017451656607008377
Speed: 319.22038509536367~98.46776807676311
Total Time: 0.1655
best val loss: 0.3196326263926246 test_score: 0.8837

Testing...
Test loss: 0.4323 score: 0.8140 time: 0.13s
test Score 0.8140
Epoch Time List: [0.59359449799922, 0.43451698900116753, 0.7289134720003858, 0.586121847000868, 0.6124484910005776, 0.5687859440004104, 0.6336568769993391, 0.8032918189992415, 0.5855596699993839, 0.5508156630003214, 1.1097964669997964, 0.5365317869991486, 0.5800194749999719, 0.725338166000256, 0.6795032690006337, 0.5286194540003635, 0.5132996520014785, 0.7238685470001656, 0.5722424280002087, 0.766421543999968, 0.5687526629999411, 0.42337635199965007, 0.8332226820002688, 0.6636775309998484, 0.7225688010003068, 0.614486215999932, 1.1935887380004715, 0.7493309719993704, 0.8532342839989724, 0.665324196000256, 0.5823150640007952, 0.503838743000415, 0.8555980739984079, 0.40441299300073297, 0.5115125009997428, 0.5850804789988615, 0.9281550149989926, 0.5016845809996084, 0.4780968810000559, 0.46767072700004064, 0.4614939829998548, 0.8414917319996675, 0.5955094389992155, 0.5216268229996786, 0.6277916300005018, 0.8190111890007756, 0.6824635980001403, 0.6893319470000279, 0.5630799470000056, 0.6410911259999921, 0.6794703809991915, 0.4541493970009469, 0.4322822399999495, 0.44832047699947, 0.5744308920002368, 0.6304670720010108, 0.7292201669997667, 0.5130218619997322, 0.516447842001071, 0.41927594000026147, 0.6840180830004101, 0.8140898219999144, 0.6957565830007297, 0.5807775600005698, 0.555309877999207, 0.5617562490006094, 0.9115622089993849, 0.633095525000499, 0.6862871839994114, 0.5527116160001242, 0.7590132610012006, 0.529773553999803, 0.41899082100007945, 0.6238945410004817, 0.5191491510004198, 1.0090812899998127, 0.5536562389997925, 0.5298637099995176, 0.9849563089992444, 0.5772017939998477, 0.5741130789992894, 0.7825496049999856, 0.6265723829992567, 0.6057111229993097, 0.5734202349985935, 0.7329839739995805, 0.9149408049997874, 0.6765166989989666, 0.5824226120003004, 0.45818025399967155, 0.6153089659992474, 0.6344653579999431, 0.5701357669986464, 0.5644534549992386, 1.04702530799932, 0.7024793439995847, 0.6478155550003066, 0.6098368769989975, 0.9299069459993916, 0.55319681400033, 0.5675407010003255, 0.49734208599966223, 0.83337524900071, 0.4428576570007863, 0.5988300989993149, 0.6962245899994741, 0.9348991530005151, 0.6340714910011229, 0.598191093999958, 0.5188203899997461, 0.7204123649999019, 0.722581056999843, 0.5672802670005694, 0.6073020619996896, 0.8742685000015626, 0.7950559679993603, 0.6301737030007644, 0.5837498459995913, 1.0161821970004894, 0.4819849259993134, 0.5731603949998316, 0.6111984549988847, 0.8623928650013113, 0.6024237559995527, 0.6542608200006725, 0.6965329930008011, 0.7225832820013238, 0.5143987499996001, 0.4014278099994044, 0.537624696000421, 0.5732581110005412, 0.7802905220005414, 0.6360405830000673, 0.5900695510008518, 0.5480412719989545, 0.6146223790001386, 0.8262722379995466, 0.4990311150004345, 0.48383635800018965, 0.38661512499948003, 0.43748049499936315, 0.8204065320005611, 0.552159782999297, 0.5527473880001708, 0.5698630560000311, 0.6323785229997156, 0.6363838240004043, 0.45085346800078696, 0.5357114940006795, 0.6255690940006389, 0.6077149079992523, 0.6746472950007956, 0.7749320339999031, 0.5448100290004732, 0.5524554480007282, 0.5799493600006826, 0.6095130050007356, 0.8925097680003091, 0.4742715259990291, 0.45081087699963973, 0.46953845800089766, 0.5759815770006753, 0.7967056020006567, 0.5512155930009612, 0.8739582449998125, 0.6097083639997436, 0.5386413510013881, 0.7031427199999598, 0.6187735559988141, 0.5228026010008762, 0.682264748000307, 0.8092476679985339, 0.9529116160010744, 0.5983805939995364, 0.6789199370005008, 0.6288859240003148, 0.6034799290000592, 0.5519580600002882, 0.5263259850007671, 0.47614070100007666, 0.698175565999918, 0.7610044189987093, 0.6410754139997152, 0.6018753480002488, 0.9455972820005627, 0.6118431200002306, 0.5323339380001926, 0.511601807000261, 0.8865088820011806, 0.8148482560000048, 0.6422699539998575, 0.7956010500001867, 0.9614807819998532, 0.6571175199997015, 0.6702555199999551, 0.6264230679998946, 0.5186966160008524, 0.5707962850010517, 1.0593866349991004, 0.5890119139994567, 0.8399965790003989, 0.7171207410001443, 0.6788269330008916, 0.4794288299999607, 0.825223732998893, 0.6105395010008579, 0.5727594260006299, 0.6563287340004536, 1.039297911000176, 0.648767483001393, 0.6336052349997772, 0.927472621000561, 0.5320941930003755, 0.4512231189992235, 0.7196470409999165, 0.9646081360006065, 0.584749584999372, 0.6408412690007026, 0.7077414529994712, 0.8421748089995162, 0.5994126699997651, 0.5358435989992358, 0.5146779629985758, 0.7346620779999284, 0.6772238599996854, 0.6495673959998385, 0.8124878359985814, 0.6129418900000019, 0.6771638209993398, 0.5304326569994373, 0.6504851190002228, 0.4280201359988496, 0.6244933170009972, 0.5574647949979408, 0.8027419189993452, 0.6868057159990713, 0.5741702800005442, 0.6240434240007744, 0.7649822260000292, 0.5512772930005667, 0.5161816919990088, 0.5114177340001334, 0.7870159789990794, 0.62658686299892, 0.608030258999861, 0.6395208760004607, 1.0683147959998678, 0.6240858870005468, 0.675865096999587, 0.6790944759995909, 0.547890491000544, 0.7360742150003716, 0.8120814430012615, 0.9732323680000263, 0.8716774680005983, 0.8115757200012013, 0.6927116220003882, 0.6309988750008415, 0.4790806850005538, 0.4295937110000523, 0.8718929619990377, 0.7148941309997099, 0.6103069789987785, 1.0378598239985877, 0.5710038799998074, 0.4473059199999625, 0.7590205939986845, 0.8511234469997362, 0.7073455280005874, 0.6346987120004997, 1.0102326239994, 0.605586583999866, 0.5754281720001018, 0.8402939869993133, 0.6551903109993873, 0.5319715809991976, 0.6447814289995222, 0.8494291140004862, 0.6204614090001996]
Total Epoch List: [94, 185]
Total Time List: [0.12325592399974994, 0.1655029920002562]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x777a5049d870>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7050;  Loss pred: 0.7050; Loss self: 0.0000; time: 0.34s
Val loss: 0.6870 score: 0.5909 time: 0.11s
Test loss: 0.6878 score: 0.5116 time: 0.13s
Epoch 2/1000, LR 0.000015
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.61s
Val loss: 0.6855 score: 0.5682 time: 0.12s
Test loss: 0.6862 score: 0.5349 time: 0.12s
Epoch 3/1000, LR 0.000045
Train loss: 0.6809;  Loss pred: 0.6809; Loss self: 0.0000; time: 0.41s
Val loss: 0.6833 score: 0.5682 time: 0.14s
Test loss: 0.6836 score: 0.5581 time: 0.12s
Epoch 4/1000, LR 0.000075
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.37s
Val loss: 0.6805 score: 0.5682 time: 0.15s
Test loss: 0.6801 score: 0.6512 time: 0.39s
Epoch 5/1000, LR 0.000105
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.34s
Val loss: 0.6774 score: 0.7955 time: 0.13s
Test loss: 0.6763 score: 0.7674 time: 0.12s
Epoch 6/1000, LR 0.000135
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.47s
Val loss: 0.6743 score: 0.6364 time: 0.20s
Test loss: 0.6722 score: 0.7209 time: 0.12s
Epoch 7/1000, LR 0.000165
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.31s
Val loss: 0.6712 score: 0.5227 time: 0.13s
Test loss: 0.6682 score: 0.5581 time: 0.36s
Epoch 8/1000, LR 0.000195
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6688 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6643 score: 0.5116 time: 0.16s
Epoch 9/1000, LR 0.000225
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6668 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6608 score: 0.5116 time: 0.12s
Epoch 10/1000, LR 0.000255
Train loss: 0.6275;  Loss pred: 0.6275; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6655 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6579 score: 0.5116 time: 0.11s
Epoch 11/1000, LR 0.000285
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6660 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6563 score: 0.5116 time: 0.36s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6666 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6549 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5962;  Loss pred: 0.5962; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6680 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6544 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6684 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6533 score: 0.5116 time: 0.40s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6685 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6519 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6679 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6500 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6684 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6489 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6659 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6451 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6621 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6401 score: 0.5116 time: 0.13s
Epoch 20/1000, LR 0.000285
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6571 score: 0.5000 time: 0.20s
Test loss: 0.6342 score: 0.5581 time: 0.12s
Epoch 21/1000, LR 0.000285
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6515 score: 0.5000 time: 0.14s
Test loss: 0.6275 score: 0.5581 time: 0.14s
Epoch 22/1000, LR 0.000285
Train loss: 0.5318;  Loss pred: 0.5318; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6433 score: 0.5000 time: 0.12s
Test loss: 0.6191 score: 0.5581 time: 0.12s
Epoch 23/1000, LR 0.000285
Train loss: 0.5113;  Loss pred: 0.5113; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6352 score: 0.5000 time: 0.42s
Test loss: 0.6113 score: 0.5814 time: 0.11s
Epoch 24/1000, LR 0.000285
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6282 score: 0.5000 time: 0.15s
Test loss: 0.6047 score: 0.5814 time: 0.12s
Epoch 25/1000, LR 0.000285
Train loss: 0.5040;  Loss pred: 0.5040; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6213 score: 0.5000 time: 0.13s
Test loss: 0.5986 score: 0.6279 time: 0.10s
Epoch 26/1000, LR 0.000285
Train loss: 0.5007;  Loss pred: 0.5007; Loss self: 0.0000; time: 0.41s
Val loss: 0.6122 score: 0.5455 time: 0.16s
Test loss: 0.5912 score: 0.6744 time: 0.11s
Epoch 27/1000, LR 0.000285
Train loss: 0.4737;  Loss pred: 0.4737; Loss self: 0.0000; time: 0.35s
Val loss: 0.6047 score: 0.5682 time: 0.31s
Test loss: 0.5854 score: 0.6744 time: 0.13s
Epoch 28/1000, LR 0.000285
Train loss: 0.4812;  Loss pred: 0.4812; Loss self: 0.0000; time: 0.24s
Val loss: 0.5977 score: 0.5682 time: 0.10s
Test loss: 0.5801 score: 0.6744 time: 0.10s
Epoch 29/1000, LR 0.000285
Train loss: 0.4713;  Loss pred: 0.4713; Loss self: 0.0000; time: 0.34s
Val loss: 0.5897 score: 0.5909 time: 0.11s
Test loss: 0.5746 score: 0.6744 time: 0.11s
Epoch 30/1000, LR 0.000285
Train loss: 0.4678;  Loss pred: 0.4678; Loss self: 0.0000; time: 0.32s
Val loss: 0.5811 score: 0.5909 time: 0.14s
Test loss: 0.5689 score: 0.6744 time: 0.18s
Epoch 31/1000, LR 0.000285
Train loss: 0.4368;  Loss pred: 0.4368; Loss self: 0.0000; time: 0.31s
Val loss: 0.5733 score: 0.6136 time: 0.31s
Test loss: 0.5634 score: 0.6977 time: 0.13s
Epoch 32/1000, LR 0.000285
Train loss: 0.4281;  Loss pred: 0.4281; Loss self: 0.0000; time: 0.33s
Val loss: 0.5636 score: 0.6136 time: 0.13s
Test loss: 0.5567 score: 0.7209 time: 0.13s
Epoch 33/1000, LR 0.000285
Train loss: 0.4217;  Loss pred: 0.4217; Loss self: 0.0000; time: 0.35s
Val loss: 0.5545 score: 0.6364 time: 0.14s
Test loss: 0.5507 score: 0.8140 time: 0.08s
Epoch 34/1000, LR 0.000285
Train loss: 0.4188;  Loss pred: 0.4188; Loss self: 0.0000; time: 0.31s
Val loss: 0.5461 score: 0.6818 time: 0.14s
Test loss: 0.5451 score: 0.8140 time: 0.29s
Epoch 35/1000, LR 0.000285
Train loss: 0.4178;  Loss pred: 0.4178; Loss self: 0.0000; time: 0.28s
Val loss: 0.5396 score: 0.6818 time: 0.10s
Test loss: 0.5408 score: 0.8140 time: 0.09s
Epoch 36/1000, LR 0.000285
Train loss: 0.4320;  Loss pred: 0.4320; Loss self: 0.0000; time: 0.41s
Val loss: 0.5327 score: 0.7045 time: 0.14s
Test loss: 0.5360 score: 0.8372 time: 0.11s
Epoch 37/1000, LR 0.000285
Train loss: 0.4060;  Loss pred: 0.4060; Loss self: 0.0000; time: 0.48s
Val loss: 0.5261 score: 0.7273 time: 0.12s
Test loss: 0.5313 score: 0.8372 time: 0.17s
Epoch 38/1000, LR 0.000284
Train loss: 0.3914;  Loss pred: 0.3914; Loss self: 0.0000; time: 0.61s
Val loss: 0.5198 score: 0.7727 time: 0.14s
Test loss: 0.5269 score: 0.8605 time: 0.14s
Epoch 39/1000, LR 0.000284
Train loss: 0.3861;  Loss pred: 0.3861; Loss self: 0.0000; time: 0.33s
Val loss: 0.5126 score: 0.7955 time: 0.14s
Test loss: 0.5215 score: 0.8605 time: 0.12s
Epoch 40/1000, LR 0.000284
Train loss: 0.3695;  Loss pred: 0.3695; Loss self: 0.0000; time: 0.33s
Val loss: 0.5050 score: 0.7955 time: 0.15s
Test loss: 0.5166 score: 0.8605 time: 0.21s
Epoch 41/1000, LR 0.000284
Train loss: 0.3718;  Loss pred: 0.3718; Loss self: 0.0000; time: 0.35s
Val loss: 0.4987 score: 0.7727 time: 0.34s
Test loss: 0.5122 score: 0.8605 time: 0.17s
Epoch 42/1000, LR 0.000284
Train loss: 0.3732;  Loss pred: 0.3732; Loss self: 0.0000; time: 0.33s
Val loss: 0.4910 score: 0.7727 time: 0.14s
Test loss: 0.5067 score: 0.8605 time: 0.16s
Epoch 43/1000, LR 0.000284
Train loss: 0.3573;  Loss pred: 0.3573; Loss self: 0.0000; time: 0.32s
Val loss: 0.4838 score: 0.7727 time: 0.13s
Test loss: 0.5019 score: 0.8605 time: 0.13s
Epoch 44/1000, LR 0.000284
Train loss: 0.3712;  Loss pred: 0.3712; Loss self: 0.0000; time: 0.33s
Val loss: 0.4756 score: 0.7727 time: 0.15s
Test loss: 0.4967 score: 0.8605 time: 0.19s
Epoch 45/1000, LR 0.000284
Train loss: 0.3657;  Loss pred: 0.3657; Loss self: 0.0000; time: 0.36s
Val loss: 0.4684 score: 0.7727 time: 0.36s
Test loss: 0.4917 score: 0.8605 time: 0.11s
Epoch 46/1000, LR 0.000284
Train loss: 0.3532;  Loss pred: 0.3532; Loss self: 0.0000; time: 0.28s
Val loss: 0.4623 score: 0.7727 time: 0.12s
Test loss: 0.4877 score: 0.8605 time: 0.11s
Epoch 47/1000, LR 0.000284
Train loss: 0.3402;  Loss pred: 0.3402; Loss self: 0.0000; time: 0.23s
Val loss: 0.4566 score: 0.7727 time: 0.12s
Test loss: 0.4841 score: 0.8605 time: 0.12s
Epoch 48/1000, LR 0.000284
Train loss: 0.3530;  Loss pred: 0.3530; Loss self: 0.0000; time: 0.34s
Val loss: 0.4514 score: 0.7727 time: 0.11s
Test loss: 0.4810 score: 0.8605 time: 0.15s
Epoch 49/1000, LR 0.000284
Train loss: 0.3315;  Loss pred: 0.3315; Loss self: 0.0000; time: 0.34s
Val loss: 0.4476 score: 0.7727 time: 0.44s
Test loss: 0.4786 score: 0.8605 time: 0.12s
Epoch 50/1000, LR 0.000284
Train loss: 0.3334;  Loss pred: 0.3334; Loss self: 0.0000; time: 0.31s
Val loss: 0.4434 score: 0.7727 time: 0.20s
Test loss: 0.4765 score: 0.8605 time: 0.12s
Epoch 51/1000, LR 0.000284
Train loss: 0.3380;  Loss pred: 0.3380; Loss self: 0.0000; time: 0.34s
Val loss: 0.4379 score: 0.7727 time: 0.13s
Test loss: 0.4729 score: 0.8605 time: 0.17s
Epoch 52/1000, LR 0.000284
Train loss: 0.3290;  Loss pred: 0.3290; Loss self: 0.0000; time: 0.26s
Val loss: 0.4338 score: 0.7727 time: 0.11s
Test loss: 0.4699 score: 0.8605 time: 0.10s
Epoch 53/1000, LR 0.000284
Train loss: 0.3169;  Loss pred: 0.3169; Loss self: 0.0000; time: 0.29s
Val loss: 0.4312 score: 0.7727 time: 0.12s
Test loss: 0.4682 score: 0.8605 time: 0.12s
Epoch 54/1000, LR 0.000284
Train loss: 0.3083;  Loss pred: 0.3083; Loss self: 0.0000; time: 0.75s
Val loss: 0.4267 score: 0.7727 time: 0.11s
Test loss: 0.4653 score: 0.8605 time: 0.13s
Epoch 55/1000, LR 0.000284
Train loss: 0.3173;  Loss pred: 0.3173; Loss self: 0.0000; time: 0.38s
Val loss: 0.4229 score: 0.7727 time: 0.25s
Test loss: 0.4625 score: 0.8605 time: 0.15s
Epoch 56/1000, LR 0.000284
Train loss: 0.3087;  Loss pred: 0.3087; Loss self: 0.0000; time: 0.41s
Val loss: 0.4177 score: 0.7727 time: 0.30s
Test loss: 0.4587 score: 0.8605 time: 0.16s
Epoch 57/1000, LR 0.000283
Train loss: 0.2991;  Loss pred: 0.2991; Loss self: 0.0000; time: 0.64s
Val loss: 0.4146 score: 0.7727 time: 0.13s
Test loss: 0.4571 score: 0.8605 time: 0.13s
Epoch 58/1000, LR 0.000283
Train loss: 0.3059;  Loss pred: 0.3059; Loss self: 0.0000; time: 0.38s
Val loss: 0.4111 score: 0.7727 time: 0.13s
Test loss: 0.4552 score: 0.8605 time: 0.14s
Epoch 59/1000, LR 0.000283
Train loss: 0.3026;  Loss pred: 0.3026; Loss self: 0.0000; time: 0.32s
Val loss: 0.4081 score: 0.7727 time: 0.32s
Test loss: 0.4541 score: 0.8605 time: 0.12s
Epoch 60/1000, LR 0.000283
Train loss: 0.2851;  Loss pred: 0.2851; Loss self: 0.0000; time: 0.40s
Val loss: 0.4051 score: 0.7727 time: 0.28s
Test loss: 0.4531 score: 0.8837 time: 0.11s
Epoch 61/1000, LR 0.000283
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 0.35s
Val loss: 0.4028 score: 0.7727 time: 0.12s
Test loss: 0.4526 score: 0.8837 time: 0.16s
Epoch 62/1000, LR 0.000283
Train loss: 0.2888;  Loss pred: 0.2888; Loss self: 0.0000; time: 0.35s
Val loss: 0.4022 score: 0.7727 time: 0.09s
Test loss: 0.4530 score: 0.8605 time: 0.25s
Epoch 63/1000, LR 0.000283
Train loss: 0.2879;  Loss pred: 0.2879; Loss self: 0.0000; time: 0.29s
Val loss: 0.4019 score: 0.7727 time: 0.12s
Test loss: 0.4539 score: 0.8605 time: 0.14s
Epoch 64/1000, LR 0.000283
Train loss: 0.2771;  Loss pred: 0.2771; Loss self: 0.0000; time: 0.37s
Val loss: 0.4023 score: 0.7727 time: 0.29s
Test loss: 0.4554 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 0.43s
Val loss: 0.3990 score: 0.7727 time: 0.13s
Test loss: 0.4536 score: 0.8605 time: 0.11s
Epoch 66/1000, LR 0.000283
Train loss: 0.2757;  Loss pred: 0.2757; Loss self: 0.0000; time: 0.38s
Val loss: 0.3971 score: 0.7727 time: 0.12s
Test loss: 0.4518 score: 0.8605 time: 0.19s
Epoch 67/1000, LR 0.000283
Train loss: 0.2621;  Loss pred: 0.2621; Loss self: 0.0000; time: 0.31s
Val loss: 0.3936 score: 0.7727 time: 0.33s
Test loss: 0.4496 score: 0.8605 time: 0.11s
Epoch 68/1000, LR 0.000283
Train loss: 0.2624;  Loss pred: 0.2624; Loss self: 0.0000; time: 0.27s
Val loss: 0.3917 score: 0.7727 time: 0.14s
Test loss: 0.4491 score: 0.8605 time: 0.10s
Epoch 69/1000, LR 0.000283
Train loss: 0.2649;  Loss pred: 0.2649; Loss self: 0.0000; time: 0.34s
Val loss: 0.3894 score: 0.7727 time: 0.16s
Test loss: 0.4480 score: 0.8605 time: 0.25s
Epoch 70/1000, LR 0.000283
Train loss: 0.2688;  Loss pred: 0.2688; Loss self: 0.0000; time: 0.28s
Val loss: 0.3838 score: 0.7727 time: 0.13s
Test loss: 0.4433 score: 0.8605 time: 0.10s
Epoch 71/1000, LR 0.000282
Train loss: 0.2585;  Loss pred: 0.2585; Loss self: 0.0000; time: 0.44s
Val loss: 0.3787 score: 0.7727 time: 0.22s
Test loss: 0.4388 score: 0.8837 time: 0.11s
Epoch 72/1000, LR 0.000282
Train loss: 0.2611;  Loss pred: 0.2611; Loss self: 0.0000; time: 0.53s
Val loss: 0.3742 score: 0.7955 time: 0.13s
Test loss: 0.4350 score: 0.8837 time: 0.33s
Epoch 73/1000, LR 0.000282
Train loss: 0.2502;  Loss pred: 0.2502; Loss self: 0.0000; time: 0.33s
Val loss: 0.3712 score: 0.8182 time: 0.14s
Test loss: 0.4326 score: 0.8837 time: 0.23s
Epoch 74/1000, LR 0.000282
Train loss: 0.2456;  Loss pred: 0.2456; Loss self: 0.0000; time: 0.30s
Val loss: 0.3689 score: 0.8182 time: 0.12s
Test loss: 0.4315 score: 0.8837 time: 0.35s
Epoch 75/1000, LR 0.000282
Train loss: 0.2537;  Loss pred: 0.2537; Loss self: 0.0000; time: 0.41s
Val loss: 0.3657 score: 0.8182 time: 0.12s
Test loss: 0.4298 score: 0.8837 time: 0.12s
Epoch 76/1000, LR 0.000282
Train loss: 0.2450;  Loss pred: 0.2450; Loss self: 0.0000; time: 0.33s
Val loss: 0.3638 score: 0.8182 time: 0.11s
Test loss: 0.4297 score: 0.8837 time: 0.12s
Epoch 77/1000, LR 0.000282
Train loss: 0.2447;  Loss pred: 0.2447; Loss self: 0.0000; time: 0.32s
Val loss: 0.3632 score: 0.8182 time: 0.14s
Test loss: 0.4292 score: 0.8837 time: 0.19s
Epoch 78/1000, LR 0.000282
Train loss: 0.2441;  Loss pred: 0.2441; Loss self: 0.0000; time: 0.37s
Val loss: 0.3607 score: 0.8182 time: 0.15s
Test loss: 0.4263 score: 0.8837 time: 0.10s
Epoch 79/1000, LR 0.000282
Train loss: 0.2304;  Loss pred: 0.2304; Loss self: 0.0000; time: 0.48s
Val loss: 0.3589 score: 0.8182 time: 0.29s
Test loss: 0.4245 score: 0.9070 time: 0.13s
Epoch 80/1000, LR 0.000282
Train loss: 0.2523;  Loss pred: 0.2523; Loss self: 0.0000; time: 0.28s
Val loss: 0.3581 score: 0.8182 time: 0.13s
Test loss: 0.4222 score: 0.9070 time: 0.15s
Epoch 81/1000, LR 0.000281
Train loss: 0.2237;  Loss pred: 0.2237; Loss self: 0.0000; time: 0.38s
Val loss: 0.3539 score: 0.8182 time: 0.26s
Test loss: 0.4174 score: 0.9070 time: 0.08s
Epoch 82/1000, LR 0.000281
Train loss: 0.2289;  Loss pred: 0.2289; Loss self: 0.0000; time: 0.35s
Val loss: 0.3507 score: 0.8409 time: 0.12s
Test loss: 0.4139 score: 0.9070 time: 0.11s
Epoch 83/1000, LR 0.000281
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.58s
Val loss: 0.3469 score: 0.8636 time: 0.13s
Test loss: 0.4111 score: 0.9070 time: 0.14s
Epoch 84/1000, LR 0.000281
Train loss: 0.2161;  Loss pred: 0.2161; Loss self: 0.0000; time: 0.34s
Val loss: 0.3473 score: 0.8409 time: 0.26s
Test loss: 0.4111 score: 0.9070 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.2241;  Loss pred: 0.2241; Loss self: 0.0000; time: 0.36s
Val loss: 0.3479 score: 0.8409 time: 0.15s
Test loss: 0.4122 score: 0.9070 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.31s
Val loss: 0.3498 score: 0.8182 time: 0.16s
Test loss: 0.4141 score: 0.9070 time: 0.41s
     INFO: Early stopping counter 3 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.1990;  Loss pred: 0.1990; Loss self: 0.0000; time: 0.33s
Val loss: 0.3494 score: 0.8182 time: 0.26s
Test loss: 0.4126 score: 0.9070 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.2194;  Loss pred: 0.2194; Loss self: 0.0000; time: 0.30s
Val loss: 0.3462 score: 0.8182 time: 0.12s
Test loss: 0.4083 score: 0.9070 time: 0.09s
Epoch 89/1000, LR 0.000281
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.23s
Val loss: 0.3463 score: 0.8182 time: 0.08s
Test loss: 0.4071 score: 0.9070 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.33s
Val loss: 0.3442 score: 0.8182 time: 0.13s
Test loss: 0.4053 score: 0.8837 time: 0.63s
Epoch 91/1000, LR 0.000280
Train loss: 0.1915;  Loss pred: 0.1915; Loss self: 0.0000; time: 0.32s
Val loss: 0.3449 score: 0.8182 time: 0.16s
Test loss: 0.4044 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.1967;  Loss pred: 0.1967; Loss self: 0.0000; time: 0.34s
Val loss: 0.3510 score: 0.7955 time: 0.15s
Test loss: 0.4077 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.58s
Val loss: 0.3522 score: 0.7955 time: 0.25s
Test loss: 0.4070 score: 0.8837 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.1929;  Loss pred: 0.1929; Loss self: 0.0000; time: 0.36s
Val loss: 0.3477 score: 0.8182 time: 0.13s
Test loss: 0.4035 score: 0.8837 time: 0.24s
     INFO: Early stopping counter 4 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1796;  Loss pred: 0.1796; Loss self: 0.0000; time: 0.29s
Val loss: 0.3407 score: 0.8182 time: 0.16s
Test loss: 0.3974 score: 0.8837 time: 0.12s
Epoch 96/1000, LR 0.000280
Train loss: 0.1783;  Loss pred: 0.1783; Loss self: 0.0000; time: 0.42s
Val loss: 0.3401 score: 0.8409 time: 0.14s
Test loss: 0.3961 score: 0.8837 time: 0.38s
Epoch 97/1000, LR 0.000280
Train loss: 0.1771;  Loss pred: 0.1771; Loss self: 0.0000; time: 0.36s
Val loss: 0.3410 score: 0.8409 time: 0.17s
Test loss: 0.3924 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.1717;  Loss pred: 0.1717; Loss self: 0.0000; time: 0.30s
Val loss: 0.3682 score: 0.7955 time: 0.11s
Test loss: 0.4049 score: 0.8837 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 99/1000, LR 0.000279
Train loss: 0.1587;  Loss pred: 0.1587; Loss self: 0.0000; time: 0.27s
Val loss: 0.3626 score: 0.7955 time: 0.16s
Test loss: 0.4048 score: 0.8605 time: 0.28s
     INFO: Early stopping counter 3 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.55s
Val loss: 0.3513 score: 0.8182 time: 0.13s
Test loss: 0.4021 score: 0.8837 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1515;  Loss pred: 0.1515; Loss self: 0.0000; time: 0.31s
Val loss: 0.3575 score: 0.8182 time: 0.19s
Test loss: 0.3978 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1403;  Loss pred: 0.1403; Loss self: 0.0000; time: 0.39s
Val loss: 0.3560 score: 0.8182 time: 0.13s
Test loss: 0.3938 score: 0.8837 time: 0.29s
     INFO: Early stopping counter 6 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1440;  Loss pred: 0.1440; Loss self: 0.0000; time: 0.45s
Val loss: 0.3458 score: 0.8409 time: 0.32s
Test loss: 0.3904 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 7 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1370;  Loss pred: 0.1370; Loss self: 0.0000; time: 0.24s
Val loss: 0.3496 score: 0.8182 time: 0.12s
Test loss: 0.3978 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1361;  Loss pred: 0.1361; Loss self: 0.0000; time: 0.27s
Val loss: 0.3621 score: 0.7955 time: 0.16s
Test loss: 0.4047 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1288;  Loss pred: 0.1288; Loss self: 0.0000; time: 0.24s
Val loss: 0.3539 score: 0.8182 time: 0.24s
Test loss: 0.3952 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 10 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1306;  Loss pred: 0.1306; Loss self: 0.0000; time: 0.58s
Val loss: 0.3449 score: 0.8409 time: 0.13s
Test loss: 0.3851 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 11 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1214;  Loss pred: 0.1214; Loss self: 0.0000; time: 0.32s
Val loss: 0.3415 score: 0.8409 time: 0.13s
Test loss: 0.3814 score: 0.8837 time: 0.13s
     INFO: Early stopping counter 12 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1230;  Loss pred: 0.1230; Loss self: 0.0000; time: 0.35s
Val loss: 0.3397 score: 0.8409 time: 0.15s
Test loss: 0.3795 score: 0.8837 time: 0.13s
Epoch 110/1000, LR 0.000278
Train loss: 0.1203;  Loss pred: 0.1203; Loss self: 0.0000; time: 0.29s
Val loss: 0.3213 score: 0.8409 time: 0.14s
Test loss: 0.3674 score: 0.8605 time: 0.16s
Epoch 111/1000, LR 0.000278
Train loss: 0.1172;  Loss pred: 0.1172; Loss self: 0.0000; time: 0.41s
Val loss: 0.3259 score: 0.8409 time: 0.15s
Test loss: 0.3682 score: 0.8372 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1345;  Loss pred: 0.1345; Loss self: 0.0000; time: 0.32s
Val loss: 0.3142 score: 0.8409 time: 0.14s
Test loss: 0.3639 score: 0.8605 time: 0.12s
Epoch 113/1000, LR 0.000278
Train loss: 0.1100;  Loss pred: 0.1100; Loss self: 0.0000; time: 0.25s
Val loss: 0.3324 score: 0.8409 time: 0.14s
Test loss: 0.3794 score: 0.8605 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1026;  Loss pred: 0.1026; Loss self: 0.0000; time: 0.33s
Val loss: 0.3240 score: 0.8409 time: 0.12s
Test loss: 0.3776 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1051;  Loss pred: 0.1051; Loss self: 0.0000; time: 0.45s
Val loss: 0.3252 score: 0.8409 time: 0.11s
Test loss: 0.3787 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.27s
Val loss: 0.3337 score: 0.8409 time: 0.12s
Test loss: 0.3837 score: 0.8372 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1105;  Loss pred: 0.1105; Loss self: 0.0000; time: 0.26s
Val loss: 0.3306 score: 0.8409 time: 0.19s
Test loss: 0.3800 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 118/1000, LR 0.000277
Train loss: 0.1132;  Loss pred: 0.1132; Loss self: 0.0000; time: 0.38s
Val loss: 0.3375 score: 0.8409 time: 0.14s
Test loss: 0.3823 score: 0.8372 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 119/1000, LR 0.000277
Train loss: 0.0972;  Loss pred: 0.0972; Loss self: 0.0000; time: 0.32s
Val loss: 0.3246 score: 0.8409 time: 0.13s
Test loss: 0.3807 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 7 of 20
Epoch 120/1000, LR 0.000277
Train loss: 0.0962;  Loss pred: 0.0962; Loss self: 0.0000; time: 0.41s
Val loss: 0.3389 score: 0.8409 time: 0.20s
Test loss: 0.3843 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 8 of 20
Epoch 121/1000, LR 0.000276
Train loss: 0.0885;  Loss pred: 0.0885; Loss self: 0.0000; time: 0.32s
Val loss: 0.3310 score: 0.8409 time: 0.15s
Test loss: 0.3848 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 9 of 20
Epoch 122/1000, LR 0.000276
Train loss: 0.1021;  Loss pred: 0.1021; Loss self: 0.0000; time: 0.31s
Val loss: 0.3284 score: 0.8409 time: 0.17s
Test loss: 0.3876 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 123/1000, LR 0.000276
Train loss: 0.0938;  Loss pred: 0.0938; Loss self: 0.0000; time: 0.26s
Val loss: 0.3167 score: 0.8409 time: 0.12s
Test loss: 0.3815 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 124/1000, LR 0.000276
Train loss: 0.0990;  Loss pred: 0.0990; Loss self: 0.0000; time: 0.48s
Val loss: 0.3306 score: 0.8409 time: 0.12s
Test loss: 0.3831 score: 0.8605 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 125/1000, LR 0.000276
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 0.32s
Val loss: 0.3405 score: 0.8409 time: 0.08s
Test loss: 0.3825 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 13 of 20
Epoch 126/1000, LR 0.000276
Train loss: 0.0952;  Loss pred: 0.0952; Loss self: 0.0000; time: 0.32s
Val loss: 0.3301 score: 0.8636 time: 0.15s
Test loss: 0.3780 score: 0.8605 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 127/1000, LR 0.000275
Train loss: 0.0889;  Loss pred: 0.0889; Loss self: 0.0000; time: 0.38s
Val loss: 0.3183 score: 0.8636 time: 0.13s
Test loss: 0.3731 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 15 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.0778;  Loss pred: 0.0778; Loss self: 0.0000; time: 0.66s
Val loss: 0.3441 score: 0.8636 time: 0.15s
Test loss: 0.3790 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 16 of 20
Epoch 129/1000, LR 0.000275
Train loss: 0.0933;  Loss pred: 0.0933; Loss self: 0.0000; time: 0.40s
Val loss: 0.3116 score: 0.8636 time: 0.14s
Test loss: 0.3756 score: 0.8605 time: 0.13s
Epoch 130/1000, LR 0.000275
Train loss: 0.0850;  Loss pred: 0.0850; Loss self: 0.0000; time: 0.37s
Val loss: 0.3310 score: 0.8636 time: 0.10s
Test loss: 0.3780 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 131/1000, LR 0.000275
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.32s
Val loss: 0.3392 score: 0.8409 time: 0.14s
Test loss: 0.3813 score: 0.8837 time: 0.33s
     INFO: Early stopping counter 2 of 20
Epoch 132/1000, LR 0.000275
Train loss: 0.0779;  Loss pred: 0.0779; Loss self: 0.0000; time: 0.29s
Val loss: 0.3218 score: 0.8409 time: 0.19s
Test loss: 0.3800 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 133/1000, LR 0.000274
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.29s
Val loss: 0.3114 score: 0.8636 time: 0.08s
Test loss: 0.3759 score: 0.8605 time: 0.09s
Epoch 134/1000, LR 0.000274
Train loss: 0.0756;  Loss pred: 0.0756; Loss self: 0.0000; time: 0.24s
Val loss: 0.3177 score: 0.8636 time: 0.11s
Test loss: 0.3799 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 135/1000, LR 0.000274
Train loss: 0.0862;  Loss pred: 0.0862; Loss self: 0.0000; time: 0.26s
Val loss: 0.3464 score: 0.8409 time: 0.11s
Test loss: 0.3833 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 136/1000, LR 0.000274
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 0.34s
Val loss: 0.3424 score: 0.8409 time: 0.09s
Test loss: 0.3861 score: 0.8605 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 137/1000, LR 0.000274
Train loss: 0.0767;  Loss pred: 0.0767; Loss self: 0.0000; time: 0.61s
Val loss: 0.3108 score: 0.8636 time: 0.15s
Test loss: 0.3829 score: 0.8605 time: 0.12s
Epoch 138/1000, LR 0.000274
Train loss: 0.0725;  Loss pred: 0.0725; Loss self: 0.0000; time: 0.29s
Val loss: 0.3154 score: 0.8636 time: 0.17s
Test loss: 0.3862 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 139/1000, LR 0.000273
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.32s
Val loss: 0.3158 score: 0.8636 time: 0.15s
Test loss: 0.3881 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0723;  Loss pred: 0.0723; Loss self: 0.0000; time: 0.38s
Val loss: 0.3227 score: 0.8636 time: 0.15s
Test loss: 0.3891 score: 0.8605 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.50s
Val loss: 0.3340 score: 0.8636 time: 0.17s
Test loss: 0.3914 score: 0.8605 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.28s
Val loss: 0.3238 score: 0.8636 time: 0.13s
Test loss: 0.3910 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 143/1000, LR 0.000273
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.39s
Val loss: 0.3312 score: 0.8636 time: 0.10s
Test loss: 0.3925 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 144/1000, LR 0.000272
Train loss: 0.0782;  Loss pred: 0.0782; Loss self: 0.0000; time: 0.30s
Val loss: 0.3432 score: 0.8409 time: 0.17s
Test loss: 0.3919 score: 0.8605 time: 0.45s
     INFO: Early stopping counter 7 of 20
Epoch 145/1000, LR 0.000272
Train loss: 0.0699;  Loss pred: 0.0699; Loss self: 0.0000; time: 0.43s
Val loss: 0.3296 score: 0.8636 time: 0.11s
Test loss: 0.3906 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 8 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0659;  Loss pred: 0.0659; Loss self: 0.0000; time: 0.51s
Val loss: 0.3169 score: 0.8864 time: 0.11s
Test loss: 0.3900 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 147/1000, LR 0.000272
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.30s
Val loss: 0.3413 score: 0.8636 time: 0.18s
Test loss: 0.3863 score: 0.8605 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 148/1000, LR 0.000272
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.39s
Val loss: 0.3369 score: 0.8636 time: 0.17s
Test loss: 0.3846 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 149/1000, LR 0.000272
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.34s
Val loss: 0.3315 score: 0.8636 time: 0.18s
Test loss: 0.3837 score: 0.8605 time: 0.32s
     INFO: Early stopping counter 12 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0618;  Loss pred: 0.0618; Loss self: 0.0000; time: 0.50s
Val loss: 0.3255 score: 0.8636 time: 0.12s
Test loss: 0.3825 score: 0.8605 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 151/1000, LR 0.000271
Train loss: 0.0620;  Loss pred: 0.0620; Loss self: 0.0000; time: 0.39s
Val loss: 0.3207 score: 0.8636 time: 0.13s
Test loss: 0.3831 score: 0.8605 time: 0.36s
     INFO: Early stopping counter 14 of 20
Epoch 152/1000, LR 0.000271
Train loss: 0.0713;  Loss pred: 0.0713; Loss self: 0.0000; time: 0.33s
Val loss: 0.3519 score: 0.8409 time: 0.13s
Test loss: 0.3849 score: 0.8837 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 153/1000, LR 0.000271
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.36s
Val loss: 0.3401 score: 0.8636 time: 0.17s
Test loss: 0.3865 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 154/1000, LR 0.000271
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.25s
Val loss: 0.3302 score: 0.8636 time: 0.12s
Test loss: 0.3875 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 155/1000, LR 0.000270
Train loss: 0.0620;  Loss pred: 0.0620; Loss self: 0.0000; time: 0.41s
Val loss: 0.3358 score: 0.8636 time: 0.35s
Test loss: 0.3861 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.33s
Val loss: 0.3226 score: 0.8864 time: 0.18s
Test loss: 0.3843 score: 0.8605 time: 0.14s
     INFO: Early stopping counter 19 of 20
Epoch 157/1000, LR 0.000270
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 0.29s
Val loss: 0.3156 score: 0.8864 time: 0.13s
Test loss: 0.3852 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 136,   Train_Loss: 0.0767,   Val_Loss: 0.3108,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.3108,   Test_Precision: 0.8947,   Test_Recall: 0.8095,   Test_accuracy: 0.8500,   Test_Score: 0.8605,   Test_loss: 0.3829


[0.12722452400066686, 0.09806141199987906, 0.11957316499956505, 0.1296438279996437, 0.12003866499981086, 0.1385010499998316, 0.18457351800043398, 0.17395222399954946, 0.15977032899991173, 0.14125581499956752, 0.41985147099967435, 0.15905578499950934, 0.14040368899986788, 0.2616285979993336, 0.29791033399942535, 0.13328123500014044, 0.1263923460001024, 0.13544637399991188, 0.13274455900045723, 0.11860781999985193, 0.11689429299985932, 0.08329211899945221, 0.14218129199980467, 0.17718170800071675, 0.23434658600035618, 0.1150742950003405, 0.32553596700017806, 0.21158868899965455, 0.3788514399993801, 0.13674791699941125, 0.19074360800004797, 0.08061353599987342, 0.4836917859993264, 0.09067176200005633, 0.13947237800039147, 0.12716068999998242, 0.2789795549997507, 0.1119985600007567, 0.12622770899997704, 0.10998614100026316, 0.13968187800037413, 0.13858125200022187, 0.12565805800022645, 0.10782500799996342, 0.18502085900036036, 0.377239529999315, 0.18720210100036638, 0.16309600000022328, 0.1212192060002053, 0.17919968699970923, 0.11380097500023112, 0.10982206899916491, 0.11652157400021679, 0.1313218670002243, 0.11009885299972666, 0.1603592840001511, 0.1299671959995976, 0.12137515999984316, 0.11554082299971924, 0.07729332400049316, 0.12118748400007462, 0.12019701600002008, 0.24344660899987502, 0.1137395360001392, 0.13257000600060564, 0.14103941600023973, 0.1572256979998201, 0.12118026300049678, 0.24994416399931652, 0.1439516830005232, 0.10271060500053864, 0.09940705000008165, 0.09553171500010649, 0.12448372000017116, 0.10501942800055986, 0.2557832700003928, 0.11359275499944488, 0.12108320999959687, 0.4002952889995868, 0.13559747999988758, 0.14517505599997094, 0.26673317400036467, 0.1138039919997027, 0.11670398699970974, 0.11536138000064966, 0.1407842469998286, 0.39261755500047, 0.2511974529998042, 0.11796693599990249, 0.10744874099964363, 0.1165048039993053, 0.07840109399967332, 0.10580043599929922, 0.12262095999994926, 0.13116166000054363, 0.26787199399950623, 0.14221267999982956, 0.12886745299965696, 0.47663879900028405, 0.11103692699998646, 0.1332987249998041, 0.080026480000015, 0.3138955039994471, 0.07140698799958045, 0.11805659200035734, 0.14437734700004512, 0.13848218400016776, 0.17166948900012358, 0.09560071600026276, 0.11911722900003952, 0.33420605799983605, 0.1197489759997552, 0.10932270099965535, 0.1304332069994416, 0.4215004730003784, 0.1428243760001351, 0.17306282099980308, 0.1548828869999852, 0.12712859499970364, 0.12645298999996157, 0.10986586099988926, 0.13192408200029604, 0.1530201189998479, 0.1106806059997325, 0.1333639510003195, 0.24333297599969228, 0.11322574899986648, 0.1483287740002197, 0.08456404100070358, 0.1263594510000985, 0.13032591200044408, 0.14001874699988548, 0.1760986959998263, 0.12870894399929966, 0.11852363199977844, 0.15312589800032583, 0.11890671900073357, 0.0933672649998698, 0.10846099099944695, 0.09069164599986834, 0.1142167280004287, 0.16270968999924662, 0.1286667149997811, 0.12181958899964229, 0.13148656899920752, 0.16645728900039103, 0.10321846299939352, 0.1162057729998196, 0.10021840900026291, 0.10045219299991004, 0.10994682599994121, 0.10239130500031024, 0.117178034999597, 0.10922464900068007, 0.12427361800018843, 0.13084372899993468, 0.12707404399952793, 0.10037076200023876, 0.14849770500040904, 0.08991443000013533, 0.09551527999974496, 0.13569134600038524, 0.3017969949996768, 0.11659813899950677, 0.3145283740004743, 0.15496151499974076, 0.12939907500003756, 0.2889697279997563, 0.1143008250001003, 0.11873709399969812, 0.2618662699997003, 0.10271054199984064, 0.10326764000001276, 0.12037657099972421, 0.22044325099977868, 0.16262285300035728, 0.112856974999886, 0.1262819829998989, 0.09512594299940247, 0.11049359800017555, 0.22307542799990188, 0.1833726069999102, 0.17238469399944734, 0.1328251300001284, 0.4417958509993696, 0.11160788999950455, 0.12918044900015957, 0.13864614100020844, 0.4741559670001152, 0.29590391400051885, 0.12657125500027178, 0.09807566800009226, 0.1578748160000032, 0.12723334499969496, 0.11961628999961249, 0.10050620699985302, 0.10753434100024606, 0.15292522900017502, 0.34709534700050426, 0.13277652700071485, 0.11671684799966897, 0.2339431729997159, 0.12378105099924142, 0.10773859300024924, 0.07911960799992812, 0.1190262909994999, 0.10158490699996037, 0.16214509500059648, 0.16053949899924191, 0.15111312700082635, 0.16928378699958557, 0.07406917800017254, 0.12842739899951994, 0.09505650700066326, 0.14662045699969894, 0.142769648000467, 0.13403790299980756, 0.17129033999935928, 0.19140701800006354, 0.11731240100016294, 0.12625527599993802, 0.13495577499998035, 0.1271844119992238, 0.17425407499922585, 0.14886183100043127, 0.12839207800061558, 0.34837973499998043, 0.13380856799994945, 0.12566891999995278, 0.10574558600001183, 0.10487565800031007, 0.09554877300070075, 0.1306848229996831, 0.13677338200068334, 0.3416135400002531, 0.1989281550004307, 0.10824969500026782, 0.1695181949999096, 0.13541192000047886, 0.12357916800010571, 0.10987278500033426, 0.11548625500017806, 0.1123197959996105, 0.1794513700006064, 0.11997267799961264, 0.13567836499987607, 0.22412913700009085, 0.1612325369997052, 0.2718414439996195, 0.3008028109998122, 0.13154316800046217, 0.1621764000001349, 0.2267729919994963, 0.1464990829999806, 0.13570969800002786, 0.12722929299980024, 0.21396690300025512, 0.11987971999951696, 0.1062655879995873, 0.11214011399988522, 0.19111610599975393, 0.15043345800040697, 0.10863701200014475, 0.28166323300047225, 0.13601341899993713, 0.08960999899954913, 0.0974264160004168, 0.1333935209995616, 0.19241431499995088, 0.14794033199996193, 0.1400226619998648, 0.1574717209996379, 0.13236478399994667, 0.3486907339993195, 0.13487492100011877, 0.10963280000032682, 0.12318115400012175, 0.14218248399993172, 0.16437899500033382, 0.13505700699988665, 0.12075613799970597, 0.13044297000033112, 0.39898661600000196, 0.12302211200039892, 0.12847293899994838, 0.3659651890002351, 0.16127295899968885, 0.12233828900025401, 0.1176643270000568, 0.3698634660004245, 0.11250137499973789, 0.13807470199935779, 0.40975521499967726, 0.09970698699999048, 0.1373792410004171, 0.1238498669999899, 0.10910946100011643, 0.13737474099980318, 0.13000343499970768, 0.14039726900045935, 0.1251420890002919, 0.11531779699998879, 0.1297896830001264, 0.10864411399961682, 0.11042692200044257, 0.13100569200014434, 0.10448483899926941, 0.11664707199997792, 0.18213127800026996, 0.13116815200010024, 0.13352296900029614, 0.08512156000051618, 0.29044849799993244, 0.0967472150005051, 0.11966266300078132, 0.17322614699969563, 0.14209582500006945, 0.12024946000019554, 0.2188577119995898, 0.17114358700018784, 0.1681860640001105, 0.13310143400030938, 0.19768397100051516, 0.11339068699999189, 0.11391995000030875, 0.12164417100029823, 0.1524189749998186, 0.12723511700005474, 0.12481399799980863, 0.17803910900056508, 0.10803421800028445, 0.1243544839999231, 0.13773911699991004, 0.1598909790000107, 0.16333507999934227, 0.1379294589996789, 0.14730742900064797, 0.12959498799955327, 0.11177642300026491, 0.17047977000038372, 0.25758722299997316, 0.1449861470000542, 0.1179615080000076, 0.11636252799962676, 0.19776394199925562, 0.11695300599967595, 0.10922606399981305, 0.2525271189997511, 0.10609597700022277, 0.11366763699970761, 0.33281874600015726, 0.23609319900060655, 0.35076931200001127, 0.12313968400030717, 0.12960442199982936, 0.19621887700031948, 0.11026187100014795, 0.13461794900013047, 0.15432132800015097, 0.08855340699938097, 0.11373580700001185, 0.14546787000017503, 0.14480577099948277, 0.17490867899959994, 0.41309628499948303, 0.11044284600029641, 0.09952691499984212, 0.13950770099927468, 0.6377105530000335, 0.12053956199997629, 0.11521201699997619, 0.193638395000562, 0.24242500899981678, 0.12141595700086327, 0.38757464900027117, 0.1153959829998712, 0.16820075899977383, 0.28351949700027035, 0.20837090399982117, 0.13711990899992088, 0.29504281699973944, 0.13989703900006134, 0.11264182899958541, 0.12462116400001833, 0.1414747199996782, 0.13898243499988894, 0.13424430000031862, 0.1350619069999084, 0.1621178269997472, 0.1357328839994807, 0.12191082999925129, 0.15315189699958864, 0.14087668699994538, 0.09908844900019176, 0.17342211599952861, 0.12687012700007472, 0.12702613999954337, 0.2558780110002772, 0.14730243000030896, 0.16251039399958245, 0.0975574490003055, 0.0909046099995976, 0.15257141900019633, 0.10317237100025523, 0.20857458899990888, 0.12975447100052406, 0.13141044799976953, 0.13973593799983064, 0.10832550899976923, 0.3365854630001195, 0.09950880300038989, 0.09215742299966223, 0.10559520600054384, 0.12309907999951974, 0.1740216039997904, 0.12347295299969119, 0.1451012309998987, 0.12029606899977807, 0.19583195299946965, 0.12440194300052099, 0.09926862099928258, 0.093941227999494, 0.4572520359997725, 0.1384796570000617, 0.11851640500026406, 0.1818621450001956, 0.17018394800015813, 0.323544505999962, 0.18097194999973, 0.3654529679997722, 0.08950907599955826, 0.10902898899985303, 0.1197801479993359, 0.12066470600075263, 0.14042673699987063, 0.10349821599993447]
[0.0028914664545606106, 0.0022286684545427056, 0.0027175719318082965, 0.0029464506363555383, 0.0027281514772684286, 0.003147751136359809, 0.004194852681828045, 0.0039534596363533965, 0.0036311438409070847, 0.003210359431808353, 0.009542078886356236, 0.003614904204534303, 0.003190992931815179, 0.005946104499984854, 0.006770689409077849, 0.003029118977275919, 0.0028725533181841456, 0.003078326681816179, 0.003016921795464937, 0.0026956322727239076, 0.00265668847726953, 0.0018930027045330048, 0.0032313929999955608, 0.00402685700001629, 0.005326058772735368, 0.0026153248863713748, 0.007398544704549501, 0.0048088338409012395, 0.00861025999998591, 0.0031079072045320736, 0.00433508200000109, 0.0018321258181789415, 0.010992995136348327, 0.0020607218636376438, 0.00316982677273617, 0.0028900156818177825, 0.006340444431812516, 0.002545421818199016, 0.0028688115681812965, 0.002499685022733254, 0.003174588136372139, 0.0031495739090959514, 0.002855864954550601, 0.002450568363635532, 0.0042050195227354625, 0.008573625681802614, 0.004254593204553781, 0.0037067272727323475, 0.0027549819545501205, 0.004072720159084301, 0.0025863857954597984, 0.0024959561136173843, 0.002648217590914018, 0.002984587886368734, 0.0025022466590846966, 0.0036445291818216156, 0.002953799909081764, 0.002758526363632799, 0.0026259277954481645, 0.0017566664545566628, 0.0027542610000016957, 0.00273175036363682, 0.005532877477269887, 0.002584989454548618, 0.0030129546818319464, 0.0032054412727327212, 0.00357331131817773, 0.0027540968863749267, 0.0056805491818026485, 0.0032716291591028, 0.0023343319318304234, 0.002259251136365492, 0.0021711753409115113, 0.0028291754545493445, 0.002386805181830906, 0.005813256136372564, 0.0025816535227146564, 0.0027518911363544744, 0.009097620204536062, 0.0030817609090883543, 0.0032994330909084306, 0.006062117590917379, 0.002586454363629607, 0.002652363340902494, 0.0026218495454693107, 0.003199641977268832, 0.008923126250010682, 0.005709033022722823, 0.002681066727270511, 0.0024420168409009916, 0.002647836454529666, 0.0017818430454471209, 0.0024045553636204368, 0.002786839999998847, 0.0030502711628033405, 0.006229581255802471, 0.003307271627903013, 0.002996917511619929, 0.011084623232564745, 0.002582254116278755, 0.003099970348832654, 0.001861080930232907, 0.007299895441847607, 0.0016606276278972197, 0.0027455021395431938, 0.003357612720931282, 0.0032205159069806457, 0.003992313697677293, 0.00222327246512239, 0.0027701681162799886, 0.007772233906972932, 0.0027848599069710514, 0.002542388395340822, 0.003033330395335851, 0.009802336581404149, 0.0033214971162822114, 0.004024716767437281, 0.0036019276046508186, 0.0029564789534814798, 0.002940767209301432, 0.0025550200232532386, 0.0030680019069836287, 0.0035586074186011144, 0.002573967581389128, 0.0031014872325655697, 0.005658906418597495, 0.002633156953485267, 0.003449506372098133, 0.001966605604667525, 0.002938591883723221, 0.0030308351628010252, 0.003256249930229895, 0.004095318511623868, 0.0029932312557976667, 0.0027563635348785685, 0.003561067395356415, 0.002765272534900781, 0.002171331744183019, 0.002522348627894115, 0.0021091080465085663, 0.0026562029767541557, 0.003783946279052247, 0.002992249186041421, 0.0028330136976661, 0.0030578271860280818, 0.00387109974419514, 0.0024004293720789193, 0.002702459837205107, 0.002330660674424719, 0.0023360975116258147, 0.002556902930231191, 0.0023811931395420986, 0.0027250705813859767, 0.0025401081162948853, 0.002890084139539266, 0.003042877418603132, 0.002955210325570417, 0.002334203767447413, 0.0034534350000095127, 0.0020910332558171005, 0.002221285581389418, 0.0031556126976833776, 0.007018534767434345, 0.0027115846278955064, 0.0073146133488482395, 0.0036037561627846686, 0.003009280813954362, 0.006720226232552472, 0.0026581587209325653, 0.00276132776743484, 0.006089913255806984, 0.0023886172558102475, 0.0024015730232561107, 0.00279945513952847, 0.005126587232552993, 0.0037819268139617974, 0.0026245808139508373, 0.00293679030232323, 0.0022122312325442432, 0.0025696185581436173, 0.0051878006511605085, 0.0042644792325560515, 0.004008946372080171, 0.0030889565116308934, 0.010274322116264408, 0.0025955323255698734, 0.003004196488375804, 0.0032243288604699638, 0.011026882953491052, 0.00688148637210509, 0.002943517558145855, 0.002280829488374239, 0.0036715073488372833, 0.002958914999992906, 0.0027817741860375, 0.0023373536511593724, 0.002500798627912699, 0.0035564006744226748, 0.008071984813965215, 0.00308782620931895, 0.002714345302317883, 0.005440538906970137, 0.0028786290930056142, 0.002505548674424401, 0.0018399908837192587, 0.002768053279058137, 0.0023624396976734967, 0.0037708161628045695, 0.0037334767209126025, 0.003514258767461078, 0.003936832255804316, 0.0017225390232598265, 0.0029866836976632544, 0.00221061644187589, 0.0034097780697604405, 0.0033202243721038834, 0.0031171605348792454, 0.0039834962790548665, 0.004451326000001477, 0.0027281953720968127, 0.002936169209300884, 0.00313850639534838, 0.0029577770232377633, 0.0040524203488192054, 0.0034619030465216573, 0.0029858622790840833, 0.008101854302325127, 0.003111827162789522, 0.0029225330232547156, 0.0024591996744188797, 0.0024389687907048854, 0.0022220644883883893, 0.0030391819302251883, 0.003180776325597287, 0.007944500930238445, 0.0046262361628007145, 0.002517434767448089, 0.00394228360464906, 0.0031491144186157876, 0.002873934139537342, 0.0025551810465194013, 0.0026857268604692573, 0.0026120882790607093, 0.004173287674432707, 0.002790062279060759, 0.0031553108139506063, 0.00521230551163002, 0.003749593883714074, 0.00632189404650278, 0.0069954142092979585, 0.0030591434418712135, 0.0037715441860496487, 0.005273790511616193, 0.0034069554186042, 0.003156039488372741, 0.002958820767437215, 0.004975974488378026, 0.002787900465105046, 0.0024712927441764487, 0.0026079096279043074, 0.00444456060464544, 0.0034984525116373713, 0.00252644213953825, 0.006550307744197029, 0.0031631027674403982, 0.0020839534651057937, 0.0022657306046608558, 0.0031021749069665484, 0.004474751511626764, 0.0034404728372084167, 0.0032563409767410417, 0.0036621330465032072, 0.003078250790696434, 0.008109086837193476, 0.003136626069770204, 0.0025496000000076006, 0.0028646780000028313, 0.0033065693953472494, 0.0038227673255891584, 0.0031408606279043406, 0.0028082822790629295, 0.0030335574418681654, 0.009278758511627953, 0.0028609793488464865, 0.00298774276744066, 0.008510818348842677, 0.003750533930225322, 0.002845076488378, 0.0027363796976757394, 0.008601475953498245, 0.002616311046505532, 0.0032110395813804136, 0.009529191046504123, 0.0023187671395346623, 0.0031948660697771417, 0.002880229465116044, 0.002537429325584103, 0.003194761418600074, 0.0030233356976676205, 0.003265052767452543, 0.002910281139541672, 0.0026818092325578786, 0.0030183647209331716, 0.00252660730231667, 0.0025680679534986644, 0.0030466440000033565, 0.0024298799767271955, 0.0027127226046506493, 0.004235611116285348, 0.003050422139537215, 0.0031051853255882824, 0.0019795711628027017, 0.006754616232556568, 0.002249935232569886, 0.002782852627925147, 0.004028515046504549, 0.003304554069769057, 0.0027964990697719894, 0.0050897142325486006, 0.0039800834186090196, 0.003911303813956058, 0.0030953821860537066, 0.004597301651174771, 0.002636992720930044, 0.0026493011627978778, 0.0028289342093092613, 0.0035446273255771766, 0.0029589562093035988, 0.0029026511162746193, 0.0041404443953619785, 0.00251242367442522, 0.0028919647441842584, 0.0032032352790676752, 0.003718394860465365, 0.0037984902325428433, 0.003207661837201835, 0.0034257541628057667, 0.003013836930222169, 0.0025994516976805796, 0.003964645813962412, 0.005990400534883097, 0.0033717708604663766, 0.002743290883721107, 0.0027061053023169014, 0.004599161441843154, 0.002719837348829673, 0.0025401410232514664, 0.00587272369766863, 0.0024673483023307623, 0.0026434334185978514, 0.007739970837212959, 0.005490539511642013, 0.008157425860465379, 0.0028637135814024925, 0.0030140563255774268, 0.004563229697681848, 0.0025642295581429757, 0.00313064997674722, 0.0035888680930267667, 0.002059381558125139, 0.002645018767442136, 0.0033829737209343028, 0.0033675760697554134, 0.004067643697665115, 0.009606890348825187, 0.002568438279076661, 0.0023145794186009796, 0.0032443651395180156, 0.014830477976744966, 0.0028032456279064254, 0.0026793492325575857, 0.004503218488385163, 0.005637790906972483, 0.00282362690699682, 0.009013363930238865, 0.0026836275116249113, 0.003911645558134275, 0.006593476674424892, 0.004845834976740027, 0.0031888350930214158, 0.006861460860459057, 0.0032534195116293335, 0.0026195774185950095, 0.002898166604651589, 0.0032901097674343767, 0.003232149651160208, 0.0031219604651236887, 0.0031409745813932182, 0.003770182023249935, 0.0031565786976623417, 0.002835135581377937, 0.0035616720232462475, 0.0032762020232545435, 0.0023043825348881804, 0.004033072465105317, 0.0029504680697691795, 0.0029540962790591484, 0.0059506514186110976, 0.0034256379069839294, 0.0037793114883623827, 0.002268777883728035, 0.0021140606976650603, 0.003548172534888287, 0.0023993574651222146, 0.0048505718372071835, 0.00301754583722149, 0.0030560569302271984, 0.0032496729767402476, 0.0025191978837155633, 0.007827568906979523, 0.0023141582093113926, 0.002143195883713075, 0.0024557024651289264, 0.0028627693023144125, 0.004047014046506753, 0.0028714640232486324, 0.0033744472325557833, 0.002797582999994839, 0.0045542314651039455, 0.002893068441872581, 0.0023085725813786644, 0.0021846797209184653, 0.010633768279064476, 0.0032204571395363186, 0.00275619546512242, 0.004229352209306874, 0.003957766232561817, 0.007524290837208418, 0.004208649999993721, 0.008498906232552842, 0.002081606418594378, 0.0025355578837175123, 0.002785584837193858, 0.0028061559535058752, 0.003265738069764433, 0.0024069352558124296]
[345.84527115046916, 448.69841360283766, 367.9755403326495, 339.3913977927351, 366.54856166610426, 317.6871222279795, 238.38739423006797, 252.94301497469766, 275.395314483106, 311.4916012493695, 104.7989659181976, 276.6325034964037, 313.38207929879536, 168.17733358075816, 147.695447181382, 330.12899377735835, 348.1223459525338, 324.85181183239814, 331.4636798021111, 370.97048069895334, 376.40845306325565, 528.2612632329522, 309.4640608559138, 248.33263262041703, 187.75609558030055, 382.3616733856141, 135.1617162473968, 207.95062443092993, 116.1405114365462, 321.75992852738983, 230.67614407287994, 545.8140429427273, 90.96701923332077, 485.2668463636194, 315.47465262172915, 346.0188836660613, 157.71765067171077, 392.86219393983924, 348.5763969621599, 400.0504027129629, 315.001492175543, 317.5032651597747, 350.15661311525844, 408.06859944786584, 237.81102432301594, 116.63676921684188, 235.04009711896282, 269.7797616124231, 362.97878407094566, 245.5361431522556, 386.6399211422454, 400.6480701099756, 377.61247543667866, 335.05463336067896, 399.64085729493695, 274.3838641731435, 338.5469668833682, 362.51239545271744, 380.8177824742249, 569.2600307850554, 363.0737972905924, 366.0656600659092, 180.73778140003827, 386.848773499007, 331.900113211121, 311.9695277235494, 279.8524704278962, 363.09543246179965, 176.0393173257701, 305.658114464977, 428.3880909840737, 442.62454222274835, 460.5800283178309, 353.4598741099599, 418.97009760675365, 172.0206329363622, 387.3486473694121, 363.3864678690505, 109.91885542786245, 324.4898061530087, 303.08236974269744, 164.9588588479806, 386.629671128891, 377.02225203419516, 381.4101391622761, 312.5349670695299, 112.06834600135831, 175.1610116844389, 372.9858678370384, 409.49758545933946, 377.6668299468781, 561.21665853519, 415.87730319269605, 358.82935511203146, 327.83970559553586, 160.52443317414983, 302.3640367374522, 333.6761843202912, 90.21506451046251, 387.25855588569414, 322.5837306400582, 537.3221463694509, 136.9882634574968, 602.1819601220632, 364.23209641584305, 297.8306562177414, 310.5092565549653, 250.48131878559414, 449.78742627703554, 360.9889212582819, 128.6631375186535, 359.0844902096524, 393.3309331621395, 329.67064897962746, 102.01649287345259, 301.06905560686175, 248.46468901630195, 277.62912244787964, 338.24018899996685, 340.04731718888627, 391.38636523354, 325.9450386010911, 281.0087998954094, 388.505281585682, 322.42596051984833, 176.71258826857226, 379.7722724717918, 289.89655102209844, 508.4903641211072, 340.299041026749, 329.94206094528215, 307.1017340273383, 244.18125163199625, 334.087116744847, 362.7968471307087, 280.8146796951911, 361.62800858826756, 460.54685226197813, 396.4559018294353, 474.13407845814623, 376.4772529627937, 264.27436497604475, 334.1967656520426, 352.98099717054754, 327.02959950425935, 258.3245243162586, 416.59213623683445, 370.03325127458817, 429.062888035742, 428.0643230958483, 391.0981477539242, 419.95753447882817, 366.9629721999339, 393.68403005563573, 346.01068748102915, 328.6363078204647, 338.38539049060046, 428.411612536106, 289.56676468421887, 478.2324705827005, 450.18974974595494, 316.8956699705663, 142.47988121964582, 368.7880472962085, 136.71262612362966, 277.4882524868961, 332.30531207419773, 148.80451422245955, 376.20025927163925, 362.1446217987222, 164.20595138140246, 418.65225480035645, 416.39375122734174, 357.2123681783436, 195.06153989737328, 264.415481629175, 381.01322492511815, 340.5077983296669, 452.03231257607723, 389.1628182832066, 192.75991257996785, 234.4952209793312, 249.44209954126123, 323.733919928845, 97.33002223251155, 385.2774208005445, 332.8677081773178, 310.1420615806058, 90.68745938610017, 145.3174424719661, 339.7295855200904, 438.4369831665031, 272.36769669457055, 337.9617190768905, 359.48280957501106, 427.83427296249357, 399.8722603405512, 281.18316566294493, 123.88526775594468, 323.8524231001199, 368.41296468288755, 183.80532096165174, 347.38758196732005, 399.11417814692015, 543.4809535461685, 361.2647225996538, 423.29122770193385, 265.19457773201106, 267.8468555592232, 284.55502743825065, 254.01133068995716, 580.5383718434115, 334.8195193158178, 452.3625089621689, 293.2742188908079, 301.18446464096735, 320.80478012299056, 251.0357560161352, 224.65215982825524, 366.5426641463103, 340.57982654143586, 318.6228970194589, 338.0917466541609, 246.7661086272504, 288.8584650008465, 334.9116290476569, 123.42853409657256, 321.35460862279194, 342.16893087022754, 406.6363583251143, 410.00934649556973, 450.03194336869865, 329.03591261017567, 314.388657873395, 125.87323090287387, 216.15844172438486, 397.2297566278927, 253.66008645870096, 317.5495923833584, 347.95508576302456, 391.3617007147783, 372.3386822088369, 382.83545315688707, 239.6192350041947, 358.4149384423913, 316.9259889005832, 191.8536812105004, 266.6955491749077, 158.18044286161233, 142.95079177310893, 326.8888886715038, 265.14338707706077, 189.61693639468103, 293.51719559914034, 316.8528162223983, 337.9724824853622, 200.96566056269333, 358.69286314793914, 404.6465164260607, 383.44887004523656, 224.99411954351646, 285.84066717314755, 395.8135372863781, 152.66458295580208, 316.14527681287, 479.8571641566066, 441.3587378582832, 322.3544867680742, 223.47609636014337, 290.6577227365629, 307.09314753665745, 273.0649016028654, 324.859820721026, 123.31844757332702, 318.8139031418757, 392.2183871968226, 349.0793729693221, 302.42825128882015, 261.5906004286781, 318.3840731790842, 356.0895596056964, 329.64597478799243, 107.77303868258024, 349.53065998298393, 334.7008353254632, 117.49751422387985, 266.6287037003082, 351.4843991312542, 365.44635996583094, 116.2591170871433, 382.2175506752711, 311.4256223431864, 104.94070221908919, 431.2636585839677, 313.00216602499245, 347.1945593611619, 394.0996464087942, 313.01241907390823, 330.7604910600761, 306.27376377142576, 343.609415053793, 372.882600246033, 331.30522400581043, 395.78766319684524, 389.39779558310664, 328.2300130894513, 411.54296079549556, 368.63334212116473, 236.0934402488312, 327.8234795895206, 322.04197017147385, 505.15991482932463, 148.0469009001727, 444.4572383791669, 359.3434988131531, 248.23042447555883, 302.61269111867983, 357.58996339717515, 196.4746848860441, 251.25101532406708, 255.66922120236876, 323.06188376527973, 217.51890040638622, 379.2198560363522, 377.4580308355425, 353.49001638471094, 282.1171051704762, 337.9570122923021, 344.5126403215281, 241.519968513567, 398.0220414969522, 345.78568152015, 312.18437388434893, 268.9332460713567, 263.26249082666845, 311.75356092783704, 291.90652699403813, 331.80295522036874, 384.6965115344413, 252.2293407593359, 166.9337457782387, 296.5800587830224, 364.5256891764831, 369.5347698198678, 217.4309409758926, 367.6690447795685, 393.6789299674261, 170.27874142912302, 405.2934071186291, 378.2958908533535, 129.19945320621957, 182.13146410833093, 122.58769090951326, 349.19693313402314, 331.7788030415862, 219.1430338271174, 389.9806851630724, 319.42248652115717, 278.6393854772811, 485.5826721641618, 378.06915108093904, 295.59792138255864, 296.9494910541486, 245.84257479926626, 104.091955220691, 389.341650973795, 432.04393505081725, 308.2267121599514, 67.42871009066984, 356.72935330566787, 373.22495621276113, 222.06339811830807, 177.37443911998577, 354.15443786926846, 110.94636894058031, 372.6299554122954, 255.64688444751798, 151.6650546257106, 206.36278470067444, 313.59414043969946, 145.7415585888945, 307.36890721454904, 381.740960546355, 345.0457259410101, 303.94122709765963, 309.3916148471162, 320.31155140216714, 318.37252231326164, 265.2391831039473, 316.79869117173195, 352.7168176958854, 280.76700871759687, 305.2314823389953, 433.9557277752605, 247.94992122064107, 338.9292737129102, 338.51300212818063, 168.04882854882524, 291.9164334214297, 264.59846008440843, 440.7659326953634, 473.023315321306, 281.83522367282086, 416.7782477335296, 206.16125965382477, 331.39513165466434, 327.2190351263045, 307.72327158996217, 396.95174661114777, 127.75358631571304, 432.1225731137729, 466.59290809550527, 407.2154563511012, 349.31211508784435, 247.09575714548518, 348.2544067777139, 296.3448325261281, 357.4514143107979, 219.5760157695841, 345.6537652295344, 433.1681005250467, 457.7329987663309, 94.04004053471594, 310.5149227801802, 362.81897008185655, 236.44282871486945, 252.66777804426096, 132.90289033683993, 237.6058831220206, 117.66219942158686, 480.3982112407485, 394.3905230567438, 358.9910408212086, 356.35938150573867, 306.20949342460057, 415.4660984690521]
Elapsed: 0.15849171031648673~0.07867644353923026
Time per graph: 0.003667452871365048~0.0018197091617547987
Speed: 314.9926096603843~96.41409508723058
Total Time: 0.1056
best val loss: 0.3108100267973813 test_score: 0.8605

Testing...
Test loss: 0.3900 score: 0.8605 time: 0.18s
test Score 0.8605
Epoch Time List: [0.59359449799922, 0.43451698900116753, 0.7289134720003858, 0.586121847000868, 0.6124484910005776, 0.5687859440004104, 0.6336568769993391, 0.8032918189992415, 0.5855596699993839, 0.5508156630003214, 1.1097964669997964, 0.5365317869991486, 0.5800194749999719, 0.725338166000256, 0.6795032690006337, 0.5286194540003635, 0.5132996520014785, 0.7238685470001656, 0.5722424280002087, 0.766421543999968, 0.5687526629999411, 0.42337635199965007, 0.8332226820002688, 0.6636775309998484, 0.7225688010003068, 0.614486215999932, 1.1935887380004715, 0.7493309719993704, 0.8532342839989724, 0.665324196000256, 0.5823150640007952, 0.503838743000415, 0.8555980739984079, 0.40441299300073297, 0.5115125009997428, 0.5850804789988615, 0.9281550149989926, 0.5016845809996084, 0.4780968810000559, 0.46767072700004064, 0.4614939829998548, 0.8414917319996675, 0.5955094389992155, 0.5216268229996786, 0.6277916300005018, 0.8190111890007756, 0.6824635980001403, 0.6893319470000279, 0.5630799470000056, 0.6410911259999921, 0.6794703809991915, 0.4541493970009469, 0.4322822399999495, 0.44832047699947, 0.5744308920002368, 0.6304670720010108, 0.7292201669997667, 0.5130218619997322, 0.516447842001071, 0.41927594000026147, 0.6840180830004101, 0.8140898219999144, 0.6957565830007297, 0.5807775600005698, 0.555309877999207, 0.5617562490006094, 0.9115622089993849, 0.633095525000499, 0.6862871839994114, 0.5527116160001242, 0.7590132610012006, 0.529773553999803, 0.41899082100007945, 0.6238945410004817, 0.5191491510004198, 1.0090812899998127, 0.5536562389997925, 0.5298637099995176, 0.9849563089992444, 0.5772017939998477, 0.5741130789992894, 0.7825496049999856, 0.6265723829992567, 0.6057111229993097, 0.5734202349985935, 0.7329839739995805, 0.9149408049997874, 0.6765166989989666, 0.5824226120003004, 0.45818025399967155, 0.6153089659992474, 0.6344653579999431, 0.5701357669986464, 0.5644534549992386, 1.04702530799932, 0.7024793439995847, 0.6478155550003066, 0.6098368769989975, 0.9299069459993916, 0.55319681400033, 0.5675407010003255, 0.49734208599966223, 0.83337524900071, 0.4428576570007863, 0.5988300989993149, 0.6962245899994741, 0.9348991530005151, 0.6340714910011229, 0.598191093999958, 0.5188203899997461, 0.7204123649999019, 0.722581056999843, 0.5672802670005694, 0.6073020619996896, 0.8742685000015626, 0.7950559679993603, 0.6301737030007644, 0.5837498459995913, 1.0161821970004894, 0.4819849259993134, 0.5731603949998316, 0.6111984549988847, 0.8623928650013113, 0.6024237559995527, 0.6542608200006725, 0.6965329930008011, 0.7225832820013238, 0.5143987499996001, 0.4014278099994044, 0.537624696000421, 0.5732581110005412, 0.7802905220005414, 0.6360405830000673, 0.5900695510008518, 0.5480412719989545, 0.6146223790001386, 0.8262722379995466, 0.4990311150004345, 0.48383635800018965, 0.38661512499948003, 0.43748049499936315, 0.8204065320005611, 0.552159782999297, 0.5527473880001708, 0.5698630560000311, 0.6323785229997156, 0.6363838240004043, 0.45085346800078696, 0.5357114940006795, 0.6255690940006389, 0.6077149079992523, 0.6746472950007956, 0.7749320339999031, 0.5448100290004732, 0.5524554480007282, 0.5799493600006826, 0.6095130050007356, 0.8925097680003091, 0.4742715259990291, 0.45081087699963973, 0.46953845800089766, 0.5759815770006753, 0.7967056020006567, 0.5512155930009612, 0.8739582449998125, 0.6097083639997436, 0.5386413510013881, 0.7031427199999598, 0.6187735559988141, 0.5228026010008762, 0.682264748000307, 0.8092476679985339, 0.9529116160010744, 0.5983805939995364, 0.6789199370005008, 0.6288859240003148, 0.6034799290000592, 0.5519580600002882, 0.5263259850007671, 0.47614070100007666, 0.698175565999918, 0.7610044189987093, 0.6410754139997152, 0.6018753480002488, 0.9455972820005627, 0.6118431200002306, 0.5323339380001926, 0.511601807000261, 0.8865088820011806, 0.8148482560000048, 0.6422699539998575, 0.7956010500001867, 0.9614807819998532, 0.6571175199997015, 0.6702555199999551, 0.6264230679998946, 0.5186966160008524, 0.5707962850010517, 1.0593866349991004, 0.5890119139994567, 0.8399965790003989, 0.7171207410001443, 0.6788269330008916, 0.4794288299999607, 0.825223732998893, 0.6105395010008579, 0.5727594260006299, 0.6563287340004536, 1.039297911000176, 0.648767483001393, 0.6336052349997772, 0.927472621000561, 0.5320941930003755, 0.4512231189992235, 0.7196470409999165, 0.9646081360006065, 0.584749584999372, 0.6408412690007026, 0.7077414529994712, 0.8421748089995162, 0.5994126699997651, 0.5358435989992358, 0.5146779629985758, 0.7346620779999284, 0.6772238599996854, 0.6495673959998385, 0.8124878359985814, 0.6129418900000019, 0.6771638209993398, 0.5304326569994373, 0.6504851190002228, 0.4280201359988496, 0.6244933170009972, 0.5574647949979408, 0.8027419189993452, 0.6868057159990713, 0.5741702800005442, 0.6240434240007744, 0.7649822260000292, 0.5512772930005667, 0.5161816919990088, 0.5114177340001334, 0.7870159789990794, 0.62658686299892, 0.608030258999861, 0.6395208760004607, 1.0683147959998678, 0.6240858870005468, 0.675865096999587, 0.6790944759995909, 0.547890491000544, 0.7360742150003716, 0.8120814430012615, 0.9732323680000263, 0.8716774680005983, 0.8115757200012013, 0.6927116220003882, 0.6309988750008415, 0.4790806850005538, 0.4295937110000523, 0.8718929619990377, 0.7148941309997099, 0.6103069789987785, 1.0378598239985877, 0.5710038799998074, 0.4473059199999625, 0.7590205939986845, 0.8511234469997362, 0.7073455280005874, 0.6346987120004997, 1.0102326239994, 0.605586583999866, 0.5754281720001018, 0.8402939869993133, 0.6551903109993873, 0.5319715809991976, 0.6447814289995222, 0.8494291140004862, 0.6204614090001996, 0.5773919039993416, 0.8380461449996801, 0.6803151160002017, 0.9132877699994424, 0.5874007109996455, 0.7949062019997655, 0.7987490009991234, 0.6853167879989996, 0.5971625869997297, 0.5582325570003377, 0.8219060420005917, 0.7889081030007219, 0.686496027999965, 0.868684571999438, 0.5433317899996837, 0.5048139609998543, 0.5400082599999223, 0.5260324169994419, 0.735778731001119, 0.6556024429992249, 0.6060515790004501, 0.5322462389995053, 0.8690384029996494, 0.674816300999737, 0.542726243999823, 0.6709090249996734, 0.7863601209992339, 0.439963770999384, 0.567644716000359, 0.6368857159995969, 0.7422524300000077, 0.5863156230007007, 0.5694489120005528, 0.7375526430014361, 0.46340372400027263, 0.6673755099991467, 0.7681479200000467, 0.8915231910004877, 0.5855330109998249, 0.6902770840006269, 0.8471920000001774, 0.637584646000505, 0.581595562000075, 0.6696684259995891, 0.8235882769986347, 0.5164402500013239, 0.4652435419993708, 0.6031040890002259, 0.8998220309995304, 0.6280716110004505, 0.6403287759994782, 0.4704962180003349, 0.5240352640003039, 1.0007456910007022, 0.7811877579988504, 0.8667196689993943, 0.9010756460002085, 0.6512623610005903, 0.7574222600005669, 0.7839762809999229, 0.6402341529992555, 0.7008970399992904, 0.5493546740008242, 0.7676954670005216, 0.6711632890001056, 0.6952337930006252, 0.7505476590004037, 0.510490577001292, 0.7463994309991904, 0.5049132630010718, 0.770088751000003, 0.9886411039988161, 0.6945977780005705, 0.7635734730001786, 0.6558451450000575, 0.5719287390002137, 0.6498246250012016, 0.6199009959991599, 0.9009300119996624, 0.5550701579995803, 0.7237147429996185, 0.5761266019990217, 0.8583019630004856, 0.7341464800001631, 0.6769266569999672, 0.878300864999801, 0.6892871099998956, 0.5117142670005705, 0.4408458329990026, 1.0873379090007802, 0.5871302979994653, 0.596748053999363, 1.0123077190010008, 0.7271574899996267, 0.5620425589995648, 0.9395514699999694, 0.641105816000163, 0.5698502900004314, 0.7053927059996568, 0.880236652998974, 0.6309901069998887, 0.8061260669992407, 0.9034119030011425, 0.47337623000021267, 0.5545179079999798, 0.6223695779999616, 0.8413722990007955, 0.5759305760002462, 0.629571092000333, 0.5930519380008263, 0.6933902750006382, 0.5781071479996172, 0.5342450980006106, 0.5832283010004176, 0.6569972999996025, 0.5582983820004301, 0.570094274999974, 0.6470023990004847, 0.7000754870014134, 0.7451011690000087, 0.6251784220003174, 0.5724878039991381, 0.4657579300001089, 0.7467130499990162, 0.49978203199862037, 0.6713214809997226, 0.6377261180005007, 0.9272250469994106, 0.6803214020010273, 0.5693483579998428, 0.794537838000906, 0.5749625719990945, 0.45601928399992175, 0.4541221039989978, 0.4849337799996647, 0.5988687890003348, 0.8800711110006887, 0.5963071679998393, 0.5791792780000833, 0.7187287409997225, 0.7897317010001643, 0.5104191460004586, 0.5839785279986245, 0.9263156730003175, 0.6701372050001737, 0.7339172570000301, 0.6626672219999818, 0.7227509219992498, 0.8434865099998206, 0.8011273599995548, 0.888645371000166, 0.5443505080002069, 0.6385494450005353, 0.4826051770005506, 0.8786397679996298, 0.6434859329992832, 0.5162293789990144]
Total Epoch List: [94, 185, 157]
Total Time List: [0.12325592399974994, 0.1655029920002562, 0.1055698330001178]
T-times Epoch Time: 0.676095617086133 ~ 0.011448756358378387
T-times Total Epoch: 102.66666666666667 ~ 32.76967884877501
T-times Total Time: 0.1472094416665439 ~ 0.012436276002534701
T-times Inference Elapsed: 0.16033875176437248 ~ 0.0037608502424337953
T-times Time Per Graph: 0.003699333081470619 ~ 6.344464645736415e-05
T-times Speed: 308.61329030321934 ~ 6.548051903035963
T-times cross validation test micro f1 score:0.6614648675624286 ~ 0.053693964397931344
T-times cross validation test precision:0.7599211933577258 ~ 0.139337094948835
T-times cross validation test recall:0.6142376142376142 ~ 0.11014228351152475
T-times cross validation test f1_score:0.6614648675624286 ~ 0.12269938373129041
