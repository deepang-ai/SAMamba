Namespace(seed=15, model='FAGNN', dataset='mining/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/averVolume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=10, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 382], edge_attr=[382, 2], x=[103, 14887], y=[1, 1], num_nodes=115)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fccfd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.40s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.30s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.29s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.6929,   Val_Loss: 0.6935,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6935,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6932


[0.40636897599324584, 0.3002284551039338, 0.2978265050332993]
[0.009235658545301041, 0.006823373979634859, 0.006768784205302258]
[108.27598217224957, 146.55506249322028, 147.73701888984144]
Elapsed: 0.33480797871015966~0.050610766905523547
Time per graph: 0.00760927224341272~0.001150244702398262
Speed: 134.18935451843709~18.32987370378377
Total Time: 0.2981
best val loss: 0.693485677242279 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.30s
test Score 0.5000
Epoch Time List: [1.2259161819238216, 0.9466558269923553, 1.0952765841502696]
Total Epoch List: [3]
Total Time List: [0.29812087200116366]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcd720>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.28s
Epoch 3/1000, LR 0.000030
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.27s
Epoch 4/1000, LR 0.000060
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.27s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.28s
Epoch 7/1000, LR 0.000150
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.43s
Val loss: 0.6928 score: 0.5455 time: 0.25s
Test loss: 0.6930 score: 0.5814 time: 0.27s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.53s
Val loss: 0.6928 score: 0.5455 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.27s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.27s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.28s
Epoch 15/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.35s
Epoch 16/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.26s
Epoch 18/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.26s
Epoch 20/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.26s
Epoch 23/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.41s
Val loss: 0.6908 score: 0.5227 time: 3.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 2.79s
Epoch 24/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 5.78s
Val loss: 0.6905 score: 0.5455 time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.28s
Epoch 25/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.43s
Val loss: 0.6901 score: 0.5682 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.27s
Epoch 26/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.43s
Val loss: 0.6897 score: 0.5909 time: 0.32s
Test loss: 0.6916 score: 0.5116 time: 0.27s
Epoch 27/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.47s
Val loss: 0.6892 score: 0.6364 time: 0.26s
Test loss: 0.6914 score: 0.5349 time: 0.27s
Epoch 28/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 3.02s
Val loss: 0.6887 score: 0.6364 time: 4.02s
Test loss: 0.6910 score: 0.6047 time: 3.33s
Epoch 29/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 7.63s
Val loss: 0.6880 score: 0.6591 time: 0.29s
Test loss: 0.6906 score: 0.5814 time: 0.30s
Epoch 30/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.43s
Val loss: 0.6873 score: 0.6818 time: 0.29s
Test loss: 0.6902 score: 0.5116 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.42s
Val loss: 0.6864 score: 0.6818 time: 0.28s
Test loss: 0.6897 score: 0.5349 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.54s
Val loss: 0.6855 score: 0.7045 time: 0.86s
Test loss: 0.6891 score: 0.6047 time: 3.17s
Epoch 33/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 9.21s
Val loss: 0.6845 score: 0.7500 time: 1.69s
Test loss: 0.6885 score: 0.6512 time: 0.30s
Epoch 34/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.39s
Val loss: 0.6835 score: 0.7045 time: 0.27s
Test loss: 0.6878 score: 0.6512 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.41s
Val loss: 0.6825 score: 0.6591 time: 0.26s
Test loss: 0.6871 score: 0.6047 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.42s
Val loss: 0.6814 score: 0.6136 time: 0.25s
Test loss: 0.6864 score: 0.5814 time: 0.27s
Epoch 37/1000, LR 0.000270
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.42s
Val loss: 0.6802 score: 0.6136 time: 0.26s
Test loss: 0.6856 score: 0.6047 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 0.51s
Val loss: 0.6790 score: 0.5682 time: 2.47s
Test loss: 0.6847 score: 0.5581 time: 0.28s
Epoch 39/1000, LR 0.000269
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 10.43s
Val loss: 0.6776 score: 0.5682 time: 0.97s
Test loss: 0.6838 score: 0.5581 time: 1.17s
Epoch 40/1000, LR 0.000269
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 2.64s
Val loss: 0.6762 score: 0.5682 time: 0.27s
Test loss: 0.6828 score: 0.5581 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.44s
Val loss: 0.6746 score: 0.5682 time: 0.27s
Test loss: 0.6818 score: 0.5581 time: 0.26s
Epoch 42/1000, LR 0.000269
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.40s
Val loss: 0.6730 score: 0.5455 time: 0.26s
Test loss: 0.6807 score: 0.5581 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.48s
Val loss: 0.6712 score: 0.5455 time: 0.26s
Test loss: 0.6795 score: 0.5581 time: 0.29s
Epoch 44/1000, LR 0.000269
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 7.79s
Val loss: 0.6692 score: 0.5455 time: 2.88s
Test loss: 0.6782 score: 0.5581 time: 2.59s
Epoch 45/1000, LR 0.000269
Train loss: 0.6556;  Loss pred: 0.6556; Loss self: 0.0000; time: 2.55s
Val loss: 0.6671 score: 0.5682 time: 0.29s
Test loss: 0.6768 score: 0.5581 time: 0.27s
Epoch 46/1000, LR 0.000269
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.46s
Val loss: 0.6649 score: 0.5682 time: 0.27s
Test loss: 0.6753 score: 0.5581 time: 0.28s
Epoch 47/1000, LR 0.000269
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.44s
Val loss: 0.6624 score: 0.5682 time: 0.28s
Test loss: 0.6736 score: 0.5581 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6446;  Loss pred: 0.6446; Loss self: 0.0000; time: 0.44s
Val loss: 0.6597 score: 0.5682 time: 2.78s
Test loss: 0.6718 score: 0.5581 time: 3.60s
Epoch 49/1000, LR 0.000269
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 6.88s
Val loss: 0.6569 score: 0.5682 time: 0.29s
Test loss: 0.6699 score: 0.5581 time: 0.35s
Epoch 50/1000, LR 0.000269
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.39s
Val loss: 0.6538 score: 0.5682 time: 0.25s
Test loss: 0.6677 score: 0.5814 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.41s
Val loss: 0.6504 score: 0.5682 time: 0.24s
Test loss: 0.6654 score: 0.5814 time: 0.26s
Epoch 52/1000, LR 0.000269
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 0.39s
Val loss: 0.6468 score: 0.5682 time: 2.61s
Test loss: 0.6629 score: 0.5814 time: 4.74s
Epoch 53/1000, LR 0.000269
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 5.53s
Val loss: 0.6429 score: 0.5909 time: 0.26s
Test loss: 0.6602 score: 0.5814 time: 0.27s
Epoch 54/1000, LR 0.000269
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.56s
Val loss: 0.6388 score: 0.6136 time: 0.25s
Test loss: 0.6572 score: 0.6047 time: 0.35s
Epoch 55/1000, LR 0.000269
Train loss: 0.6009;  Loss pred: 0.6009; Loss self: 0.0000; time: 0.45s
Val loss: 0.6343 score: 0.6136 time: 0.27s
Test loss: 0.6540 score: 0.6047 time: 0.27s
Epoch 56/1000, LR 0.000269
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.37s
Val loss: 0.6296 score: 0.6136 time: 2.03s
Test loss: 0.6505 score: 0.6279 time: 3.64s
Epoch 57/1000, LR 0.000269
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 9.12s
Val loss: 0.6246 score: 0.6136 time: 0.44s
Test loss: 0.6469 score: 0.6279 time: 0.28s
Epoch 58/1000, LR 0.000269
Train loss: 0.5770;  Loss pred: 0.5770; Loss self: 0.0000; time: 0.40s
Val loss: 0.6192 score: 0.6136 time: 0.30s
Test loss: 0.6429 score: 0.6512 time: 0.28s
Epoch 59/1000, LR 0.000268
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.42s
Val loss: 0.6136 score: 0.6136 time: 0.27s
Test loss: 0.6388 score: 0.6512 time: 0.28s
Epoch 60/1000, LR 0.000268
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.53s
Val loss: 0.6076 score: 0.6591 time: 0.27s
Test loss: 0.6344 score: 0.6744 time: 3.31s
Epoch 61/1000, LR 0.000268
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 9.88s
Val loss: 0.6014 score: 0.7045 time: 0.27s
Test loss: 0.6299 score: 0.7209 time: 0.28s
Epoch 62/1000, LR 0.000268
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.53s
Val loss: 0.5949 score: 0.7045 time: 0.27s
Test loss: 0.6251 score: 0.6977 time: 0.26s
Epoch 63/1000, LR 0.000268
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.39s
Val loss: 0.5881 score: 0.7045 time: 0.28s
Test loss: 0.6202 score: 0.6977 time: 0.26s
Epoch 64/1000, LR 0.000268
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.46s
Val loss: 0.5812 score: 0.7273 time: 2.24s
Test loss: 0.6151 score: 0.6977 time: 2.76s
Epoch 65/1000, LR 0.000268
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 3.24s
Val loss: 0.5741 score: 0.7273 time: 0.37s
Test loss: 0.6097 score: 0.6977 time: 0.26s
Epoch 66/1000, LR 0.000268
Train loss: 0.4995;  Loss pred: 0.4995; Loss self: 0.0000; time: 0.51s
Val loss: 0.5668 score: 0.7727 time: 0.29s
Test loss: 0.6042 score: 0.6512 time: 0.27s
Epoch 67/1000, LR 0.000268
Train loss: 0.4842;  Loss pred: 0.4842; Loss self: 0.0000; time: 0.45s
Val loss: 0.5595 score: 0.7727 time: 0.32s
Test loss: 0.5985 score: 0.6279 time: 0.27s
Epoch 68/1000, LR 0.000268
Train loss: 0.4637;  Loss pred: 0.4637; Loss self: 0.0000; time: 2.72s
Val loss: 0.5522 score: 0.7727 time: 2.26s
Test loss: 0.5927 score: 0.6279 time: 3.30s
Epoch 69/1000, LR 0.000268
Train loss: 0.4553;  Loss pred: 0.4553; Loss self: 0.0000; time: 4.04s
Val loss: 0.5448 score: 0.7727 time: 0.29s
Test loss: 0.5868 score: 0.6279 time: 0.26s
Epoch 70/1000, LR 0.000268
Train loss: 0.4440;  Loss pred: 0.4440; Loss self: 0.0000; time: 0.42s
Val loss: 0.5373 score: 0.7727 time: 0.34s
Test loss: 0.5810 score: 0.6744 time: 0.26s
Epoch 71/1000, LR 0.000268
Train loss: 0.4378;  Loss pred: 0.4378; Loss self: 0.0000; time: 0.39s
Val loss: 0.5300 score: 0.7955 time: 0.25s
Test loss: 0.5751 score: 0.6744 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.4238;  Loss pred: 0.4238; Loss self: 0.0000; time: 0.40s
Val loss: 0.5227 score: 0.8182 time: 0.27s
Test loss: 0.5693 score: 0.6977 time: 0.27s
Epoch 73/1000, LR 0.000267
Train loss: 0.4094;  Loss pred: 0.4094; Loss self: 0.0000; time: 0.42s
Val loss: 0.5156 score: 0.8182 time: 2.92s
Test loss: 0.5635 score: 0.7209 time: 2.83s
Epoch 74/1000, LR 0.000267
Train loss: 0.3924;  Loss pred: 0.3924; Loss self: 0.0000; time: 1.00s
Val loss: 0.5086 score: 0.8182 time: 0.26s
Test loss: 0.5577 score: 0.7209 time: 0.27s
Epoch 75/1000, LR 0.000267
Train loss: 0.3930;  Loss pred: 0.3930; Loss self: 0.0000; time: 0.44s
Val loss: 0.5018 score: 0.8182 time: 0.40s
Test loss: 0.5518 score: 0.7209 time: 0.27s
Epoch 76/1000, LR 0.000267
Train loss: 0.3729;  Loss pred: 0.3729; Loss self: 0.0000; time: 0.45s
Val loss: 0.4951 score: 0.8182 time: 0.28s
Test loss: 0.5458 score: 0.7209 time: 0.31s
Epoch 77/1000, LR 0.000267
Train loss: 0.3560;  Loss pred: 0.3560; Loss self: 0.0000; time: 0.40s
Val loss: 0.4887 score: 0.8182 time: 2.97s
Test loss: 0.5399 score: 0.7209 time: 2.50s
Epoch 78/1000, LR 0.000267
Train loss: 0.3530;  Loss pred: 0.3530; Loss self: 0.0000; time: 9.14s
Val loss: 0.4826 score: 0.8182 time: 0.33s
Test loss: 0.5341 score: 0.7674 time: 0.30s
Epoch 79/1000, LR 0.000267
Train loss: 0.3400;  Loss pred: 0.3400; Loss self: 0.0000; time: 0.39s
Val loss: 0.4767 score: 0.8182 time: 0.29s
Test loss: 0.5284 score: 0.7674 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.41s
Val loss: 0.4713 score: 0.8182 time: 0.38s
Test loss: 0.5230 score: 0.7674 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.3148;  Loss pred: 0.3148; Loss self: 0.0000; time: 0.46s
Val loss: 0.4663 score: 0.8182 time: 0.27s
Test loss: 0.5180 score: 0.7674 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.3045;  Loss pred: 0.3045; Loss self: 0.0000; time: 0.43s
Val loss: 0.4616 score: 0.7955 time: 4.07s
Test loss: 0.5132 score: 0.7907 time: 2.64s
Epoch 83/1000, LR 0.000266
Train loss: 0.2944;  Loss pred: 0.2944; Loss self: 0.0000; time: 7.32s
Val loss: 0.4572 score: 0.7955 time: 0.28s
Test loss: 0.5087 score: 0.7907 time: 0.27s
Epoch 84/1000, LR 0.000266
Train loss: 0.2869;  Loss pred: 0.2869; Loss self: 0.0000; time: 0.38s
Val loss: 0.4527 score: 0.7955 time: 0.36s
Test loss: 0.5038 score: 0.7907 time: 0.27s
Epoch 85/1000, LR 0.000266
Train loss: 0.2829;  Loss pred: 0.2829; Loss self: 0.0000; time: 0.37s
Val loss: 0.4480 score: 0.8182 time: 0.35s
Test loss: 0.4985 score: 0.7907 time: 0.26s
Epoch 86/1000, LR 0.000266
Train loss: 0.2645;  Loss pred: 0.2645; Loss self: 0.0000; time: 0.40s
Val loss: 0.4435 score: 0.8182 time: 0.32s
Test loss: 0.4932 score: 0.8372 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.2669;  Loss pred: 0.2669; Loss self: 0.0000; time: 4.82s
Val loss: 0.4393 score: 0.8182 time: 1.11s
Test loss: 0.4880 score: 0.8140 time: 0.35s
Epoch 88/1000, LR 0.000266
Train loss: 0.2498;  Loss pred: 0.2498; Loss self: 0.0000; time: 0.44s
Val loss: 0.4354 score: 0.8182 time: 0.27s
Test loss: 0.4834 score: 0.8140 time: 0.26s
Epoch 89/1000, LR 0.000266
Train loss: 0.2563;  Loss pred: 0.2563; Loss self: 0.0000; time: 0.40s
Val loss: 0.4316 score: 0.8182 time: 0.28s
Test loss: 0.4791 score: 0.8140 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.2420;  Loss pred: 0.2420; Loss self: 0.0000; time: 0.42s
Val loss: 0.4278 score: 0.8182 time: 0.40s
Test loss: 0.4757 score: 0.8140 time: 0.25s
Epoch 91/1000, LR 0.000266
Train loss: 0.2329;  Loss pred: 0.2329; Loss self: 0.0000; time: 0.40s
Val loss: 0.4245 score: 0.8182 time: 0.32s
Test loss: 0.4732 score: 0.8140 time: 3.15s
Epoch 92/1000, LR 0.000266
Train loss: 0.2168;  Loss pred: 0.2168; Loss self: 0.0000; time: 5.39s
Val loss: 0.4216 score: 0.8182 time: 0.31s
Test loss: 0.4711 score: 0.8140 time: 0.26s
Epoch 93/1000, LR 0.000265
Train loss: 0.2103;  Loss pred: 0.2103; Loss self: 0.0000; time: 0.42s
Val loss: 0.4191 score: 0.8182 time: 0.32s
Test loss: 0.4691 score: 0.8140 time: 0.24s
Epoch 94/1000, LR 0.000265
Train loss: 0.2029;  Loss pred: 0.2029; Loss self: 0.0000; time: 0.43s
Val loss: 0.4171 score: 0.8182 time: 0.26s
Test loss: 0.4672 score: 0.8140 time: 0.27s
Epoch 95/1000, LR 0.000265
Train loss: 0.1992;  Loss pred: 0.1992; Loss self: 0.0000; time: 0.42s
Val loss: 0.4155 score: 0.8182 time: 0.35s
Test loss: 0.4658 score: 0.8140 time: 0.27s
Epoch 96/1000, LR 0.000265
Train loss: 0.1974;  Loss pred: 0.1974; Loss self: 0.0000; time: 0.42s
Val loss: 0.4143 score: 0.8182 time: 0.59s
Test loss: 0.4653 score: 0.8140 time: 4.29s
Epoch 97/1000, LR 0.000265
Train loss: 0.1763;  Loss pred: 0.1763; Loss self: 0.0000; time: 3.67s
Val loss: 0.4136 score: 0.8182 time: 0.28s
Test loss: 0.4647 score: 0.8140 time: 0.25s
Epoch 98/1000, LR 0.000265
Train loss: 0.1721;  Loss pred: 0.1721; Loss self: 0.0000; time: 0.39s
Val loss: 0.4135 score: 0.8182 time: 0.36s
Test loss: 0.4642 score: 0.8140 time: 0.25s
Epoch 99/1000, LR 0.000265
Train loss: 0.1595;  Loss pred: 0.1595; Loss self: 0.0000; time: 0.45s
Val loss: 0.4139 score: 0.8182 time: 0.27s
Test loss: 0.4639 score: 0.8140 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 100/1000, LR 0.000265
Train loss: 0.1425;  Loss pred: 0.1425; Loss self: 0.0000; time: 0.40s
Val loss: 0.4149 score: 0.8182 time: 0.36s
Test loss: 0.4643 score: 0.8140 time: 0.26s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.1721,   Val_Loss: 0.4135,   Val_Precision: 0.8889,   Val_Recall: 0.7273,   Val_accuracy: 0.8000,   Val_Score: 0.8182,   Val_Loss: 0.4135,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4642


[0.40636897599324584, 0.3002284551039338, 0.2978265050332993, 0.2545609090011567, 0.2837188630364835, 0.27263582800514996, 0.254939550999552, 0.2749983440153301, 0.2832459290511906, 0.2575019010109827, 0.25182334205601364, 0.2721152190351859, 0.2753718039020896, 0.25613876106217504, 0.2596663129515946, 0.2707451180322096, 0.28005320904776454, 0.35770337702706456, 0.25649273104500026, 0.26821374299470335, 0.25421305594500154, 0.2656306040007621, 0.25807498290669173, 0.2581513290060684, 0.26099681307096034, 2.7968804870033637, 0.288131756009534, 0.275230975006707, 0.27446003805380315, 0.2746391830733046, 3.338902318966575, 0.3024427240015939, 0.2565098729683086, 0.2615188549971208, 3.1736195660196245, 0.30317858303897083, 0.24559014500118792, 0.2592047690413892, 0.27508937602397054, 0.2680619159946218, 0.28599837596993893, 1.1743416889803484, 0.25063800590578467, 0.26314399403054267, 0.26289390004239976, 0.2954602830577642, 2.5907203539973125, 0.27518242597579956, 0.28677149198483676, 0.27090878097806126, 3.6040550579782575, 0.352381553966552, 0.26153387199155986, 0.26929621398448944, 4.745579993003048, 0.2744280679617077, 0.35612681007478386, 0.27421071694698185, 3.6486057810252532, 0.2910055450629443, 0.28495489398483187, 0.2850041020428762, 3.313398226047866, 0.2868257029913366, 0.2692249519750476, 0.2606490550097078, 2.761696593952365, 0.2678535240702331, 0.2736266009742394, 0.2737494909670204, 3.304999663028866, 0.26257871894631535, 0.2690953010460362, 0.264408916933462, 0.26974624989088625, 2.832577483030036, 0.2775886619929224, 0.277745881001465, 0.3180003680754453, 2.503262465004809, 0.3066908939508721, 0.24967700894922018, 0.2549796380335465, 0.24501780199352652, 2.6426375559531152, 0.27124184102285653, 0.2723184850765392, 0.26396348199341446, 0.2537716639926657, 0.35085650300607085, 0.2688884820090607, 0.2494402170414105, 0.25469479302410036, 3.1545641629491, 0.2623940100893378, 0.24368973006494343, 0.2706529110437259, 0.2754707190906629, 4.295712860999629, 0.25470353302080184, 0.25492486194707453, 0.25803392904344946, 0.26525914296507835]
[0.009235658545301041, 0.006823373979634859, 0.006768784205302258, 0.005920021139561783, 0.00659811309387171, 0.006340368093143023, 0.005928826767431442, 0.006395310325937909, 0.006587114629097456, 0.005988416302580993, 0.005856356792000317, 0.00632826090779502, 0.006403995439583479, 0.0059567153735389546, 0.006038751463990572, 0.006296398093772315, 0.006512865326692199, 0.00831868318667592, 0.005964947233604657, 0.006237528906853566, 0.005911931533604687, 0.0061774559069944675, 0.006001743788527715, 0.006003519279210893, 0.006069693327231636, 0.06504373225589218, 0.006700738511849628, 0.006400720348993186, 0.006382791582646585, 0.006386957745890804, 0.07764889113875756, 0.0070335517209673, 0.005965345882983922, 0.006081833837142344, 0.07380510618650289, 0.0070506647218365305, 0.005711398720957859, 0.006028017884683471, 0.006397427349394664, 0.006233998046386554, 0.006651125022556719, 0.02731027183675229, 0.0058287908350182485, 0.0061196277681521555, 0.006113811628893018, 0.006871169373436376, 0.060249310558077036, 0.00639959130176278, 0.0066691044647636455, 0.006300204208792123, 0.08381523390647111, 0.008194919859687256, 0.006082183069571159, 0.00626270265080208, 0.11036232541867554, 0.006382048092132738, 0.008282018838948461, 0.006376993417371671, 0.08485129723314543, 0.006767570815417309, 0.006626857999647253, 0.006628002373090144, 0.07705577269878758, 0.0066703651858450366, 0.006261045394768548, 0.006061605930458321, 0.06422550218493872, 0.006229151722563561, 0.0063634093249823115, 0.006366267231791172, 0.07686045727974107, 0.006106481835960822, 0.006258030256884563, 0.006149044579847954, 0.006273168602113633, 0.06587389495418688, 0.006455550278905172, 0.006459206534917791, 0.007395357397103379, 0.05821540616290254, 0.007132346370950515, 0.005806442068586516, 0.005929759024035966, 0.005698088418454105, 0.06145668734774687, 0.006307949791229222, 0.006332988025035796, 0.006138685627753825, 0.005901666604480598, 0.008159453558280717, 0.00625322051183862, 0.005800935280032803, 0.005923134721490706, 0.07336195727788605, 0.006102186281147392, 0.005667203024766127, 0.006294253745202928, 0.0064062957928061135, 0.09990029909301462, 0.0059233379772279495, 0.005928485161559873, 0.00600078904752208, 0.006168817278257636]
[108.27598217224957, 146.55506249322028, 147.73701888984144, 168.918315733248, 151.5584813071474, 157.71954960808654, 168.66743442281955, 156.36457795397823, 151.8115375710437, 166.98905845423647, 170.75462365373346, 158.02129756821827, 156.15251594636376, 167.8777543144366, 165.5971447016918, 158.82096162075374, 153.54225058233888, 120.21133364012533, 167.64607645920339, 160.31989830159134, 169.14945552325588, 161.87893771410702, 166.61824217013262, 166.56896621666894, 164.75296956330678, 15.374271514215147, 149.23728156703828, 156.23241533389236, 156.67126006726923, 156.56906461348876, 12.878483972334042, 142.17568017861586, 167.63487308464178, 164.42409095311086, 13.54919803885974, 141.83059887997678, 175.08845886219095, 165.89200946813543, 156.31283411051504, 160.4107015368148, 150.35050410398028, 36.616259478394085, 171.5621692911321, 163.40863168250408, 163.56408419162605, 145.53563529753086, 16.59770030125166, 156.2599786215329, 149.9451695926374, 158.72501380264313, 11.931005300491003, 122.02681870255203, 164.4146498981504, 159.67547171857623, 9.06106315000481, 156.68951182500763, 120.74350704169207, 156.81371056082384, 11.785323649823594, 147.76350736099937, 150.90107560071905, 150.87502141822173, 12.97761303243325, 149.91682945966244, 159.71773672740906, 164.97278303348722, 15.57013905660836, 160.53550219008915, 157.14846380761145, 157.07791765421172, 13.010591341662245, 163.7604150578228, 159.79468921548974, 162.62689057049033, 159.40907433335485, 15.18052030619211, 154.90546224505508, 154.81777747686272, 135.21996927311193, 17.177583493993463, 140.20631472314935, 172.2225053118344, 168.64091710077133, 175.4974522264961, 16.271622229515796, 158.53011407770438, 157.90334610562408, 162.90132133153477, 169.44366176848945, 122.55722676246356, 159.917597357521, 172.38599496913287, 168.8295213633643, 13.631043078800683, 163.87569207604892, 176.45388662271685, 158.87506930621205, 156.09644517553187, 10.009980040889822, 168.82372808110264, 168.67715322692737, 166.64475156195206, 162.10562817682404]
Elapsed: 0.7168420409677478~1.0743796588365249
Time per graph: 0.016665590969281507~0.0249873875800193
Speed: 135.40152827478977~52.93846305122128
Total Time: 0.2658
best val loss: 0.4134824573993683 test_score: 0.8140

Testing...
Test loss: 0.5693 score: 0.6977 time: 2.93s
test Score 0.6977
Epoch Time List: [1.2259161819238216, 0.9466558269923553, 1.0952765841502696, 0.9307083790190518, 1.0105749950744212, 0.963841357966885, 1.1787077150074765, 0.938034156919457, 0.9406237729126588, 0.9529275810346007, 0.9629078931175172, 0.9485100810416043, 1.0507864539977163, 0.9102751731406897, 0.9510700849350542, 0.9445665259845555, 0.9990228560054675, 1.0351890180027112, 0.9325778449419886, 1.0138044549385086, 0.994988527148962, 0.960284736007452, 0.8979176010470837, 1.0372082219691947, 0.9831718681380153, 6.278061967925169, 6.758528442936949, 0.9468545350246131, 1.0240557010984048, 1.003283228026703, 10.375304469023831, 8.217113528051414, 0.9638867459725589, 0.9548561649862677, 4.563093572971411, 11.205831540981308, 0.9014534429879859, 0.9201623059343547, 0.9411807861179113, 0.9440275031374767, 3.2612401369260624, 12.567819097894244, 3.152018722030334, 0.9679665720323101, 0.9137300899019465, 1.0360538300592452, 13.254474085988477, 3.1167041130829602, 1.0091113760136068, 0.9898806818528101, 6.816670585074462, 7.509232199168764, 0.8958589870017022, 0.910440869978629, 7.738425516989082, 6.053479096968658, 1.1658521982608363, 0.9910285169025883, 6.035840519005433, 9.838966213050298, 0.9771912220166996, 0.9667037220206112, 4.10711242002435, 10.434591786935925, 1.0657130208564922, 0.9229093889007345, 5.450373966130428, 3.8717764139873907, 1.0699876920552924, 1.038716790964827, 8.27632680197712, 4.588070287019946, 1.0243177359225228, 0.9020925419172272, 0.9315553001360968, 6.167513453867286, 1.5362426169449463, 1.1103437008569017, 1.03722470800858, 5.861072517000139, 9.766637007938698, 0.9218138930154964, 1.0349829881452024, 0.9712983010103926, 7.141945926123299, 7.864326884970069, 1.0049059389857575, 0.9812136460095644, 0.967886856989935, 6.276592102018185, 0.9712267380673438, 0.9238113510655239, 1.077893126057461, 3.867370677064173, 5.962600427097641, 0.979072007117793, 0.9581098410999402, 1.0451382481260225, 5.299412045045756, 4.206460719928145, 0.9992093190085143, 0.9818493608618155, 1.0220185408834368]
Total Epoch List: [3, 100]
Total Time List: [0.29812087200116366, 0.26575117697939277]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcc1c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7017;  Loss pred: 0.7017; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5116 time: 0.26s
Epoch 2/1000, LR 0.000000
Train loss: 0.7017;  Loss pred: 0.7017; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5116 time: 3.46s
Epoch 3/1000, LR 0.000030
Train loss: 0.7016;  Loss pred: 0.7016; Loss self: 0.0000; time: 8.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5116 time: 0.27s
Epoch 4/1000, LR 0.000060
Train loss: 0.7015;  Loss pred: 0.7015; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.5000 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.7013;  Loss pred: 0.7013; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5116 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.7011;  Loss pred: 0.7011; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5116 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.7008;  Loss pred: 0.7008; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5000 time: 3.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5116 time: 2.99s
Epoch 8/1000, LR 0.000180
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 8.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5116 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.7001;  Loss pred: 0.7001; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5116 time: 0.26s
Epoch 11/1000, LR 0.000270
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5116 time: 2.32s
Epoch 12/1000, LR 0.000270
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 10.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 3.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5116 time: 3.13s
Epoch 13/1000, LR 0.000270
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.26s
Epoch 16/1000, LR 0.000270
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 1.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 2.98s
Epoch 17/1000, LR 0.000270
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 5.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.27s
Epoch 18/1000, LR 0.000270
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.23s
Epoch 20/1000, LR 0.000270
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.26s
Epoch 21/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 5.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 3.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.53s
Epoch 22/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.28s
Epoch 25/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5116 time: 0.35s
Epoch 26/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 7.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 4.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5116 time: 1.43s
Epoch 27/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 3.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5116 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5116 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5116 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.54s
Val loss: 0.6900 score: 0.5455 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5116 time: 2.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 6.50s
Val loss: 0.6896 score: 0.5682 time: 0.39s
Test loss: 0.6893 score: 0.5814 time: 0.31s
Epoch 32/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.41s
Val loss: 0.6892 score: 0.6364 time: 0.28s
Test loss: 0.6890 score: 0.7442 time: 0.26s
Epoch 33/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.44s
Val loss: 0.6888 score: 0.6364 time: 0.28s
Test loss: 0.6887 score: 0.6977 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.36s
Val loss: 0.6884 score: 0.7727 time: 0.29s
Test loss: 0.6883 score: 0.6279 time: 0.30s
Epoch 35/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.40s
Val loss: 0.6879 score: 0.7045 time: 4.09s
Test loss: 0.6879 score: 0.6279 time: 3.71s
Epoch 36/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 7.66s
Val loss: 0.6874 score: 0.5682 time: 0.54s
Test loss: 0.6875 score: 0.5814 time: 0.27s
Epoch 37/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.40s
Val loss: 0.6869 score: 0.5455 time: 0.29s
Test loss: 0.6870 score: 0.5581 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.41s
Val loss: 0.6864 score: 0.5455 time: 0.29s
Test loss: 0.6865 score: 0.5581 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.45s
Val loss: 0.6858 score: 0.5227 time: 0.30s
Test loss: 0.6860 score: 0.5581 time: 3.44s
Epoch 40/1000, LR 0.000269
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 9.65s
Val loss: 0.6853 score: 0.5455 time: 1.06s
Test loss: 0.6855 score: 0.5349 time: 0.27s
Epoch 41/1000, LR 0.000269
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.42s
Val loss: 0.6847 score: 0.5455 time: 0.34s
Test loss: 0.6850 score: 0.5349 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.50s
Val loss: 0.6841 score: 0.5455 time: 0.31s
Test loss: 0.6844 score: 0.5349 time: 0.30s
Epoch 43/1000, LR 0.000269
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.46s
Val loss: 0.6833 score: 0.5455 time: 0.30s
Test loss: 0.6838 score: 0.5349 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 10.28s
Val loss: 0.6826 score: 0.5455 time: 4.60s
Test loss: 0.6831 score: 0.5349 time: 2.41s
Epoch 45/1000, LR 0.000269
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 5.14s
Val loss: 0.6817 score: 0.5455 time: 0.30s
Test loss: 0.6823 score: 0.5349 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.49s
Val loss: 0.6808 score: 0.5682 time: 0.27s
Test loss: 0.6814 score: 0.5349 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6773;  Loss pred: 0.6773; Loss self: 0.0000; time: 0.54s
Val loss: 0.6798 score: 0.5682 time: 0.27s
Test loss: 0.6804 score: 0.5349 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.40s
Val loss: 0.6786 score: 0.5682 time: 0.41s
Test loss: 0.6792 score: 0.5349 time: 3.76s
Epoch 49/1000, LR 0.000269
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 9.44s
Val loss: 0.6774 score: 0.5682 time: 0.31s
Test loss: 0.6780 score: 0.5349 time: 0.26s
Epoch 50/1000, LR 0.000269
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.42s
Val loss: 0.6761 score: 0.5682 time: 0.27s
Test loss: 0.6766 score: 0.5349 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.48s
Val loss: 0.6747 score: 0.5682 time: 0.28s
Test loss: 0.6752 score: 0.5349 time: 0.26s
Epoch 52/1000, LR 0.000269
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.53s
Val loss: 0.6732 score: 0.5682 time: 0.28s
Test loss: 0.6736 score: 0.5349 time: 0.29s
Epoch 53/1000, LR 0.000269
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 8.26s
Val loss: 0.6716 score: 0.5682 time: 4.03s
Test loss: 0.6720 score: 0.5349 time: 1.12s
Epoch 54/1000, LR 0.000269
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 2.72s
Val loss: 0.6698 score: 0.5682 time: 0.34s
Test loss: 0.6701 score: 0.5349 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6624;  Loss pred: 0.6624; Loss self: 0.0000; time: 0.43s
Val loss: 0.6679 score: 0.5682 time: 0.29s
Test loss: 0.6681 score: 0.5349 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.6584;  Loss pred: 0.6584; Loss self: 0.0000; time: 0.37s
Val loss: 0.6658 score: 0.5682 time: 0.31s
Test loss: 0.6659 score: 0.5349 time: 0.34s
Epoch 57/1000, LR 0.000269
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 6.80s
Val loss: 0.6635 score: 0.5682 time: 0.40s
Test loss: 0.6634 score: 0.5349 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.39s
Val loss: 0.6609 score: 0.5682 time: 0.29s
Test loss: 0.6607 score: 0.5349 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.39s
Val loss: 0.6582 score: 0.5682 time: 0.27s
Test loss: 0.6578 score: 0.5349 time: 0.28s
Epoch 60/1000, LR 0.000268
Train loss: 0.6447;  Loss pred: 0.6447; Loss self: 0.0000; time: 0.48s
Val loss: 0.6552 score: 0.5909 time: 3.86s
Test loss: 0.6546 score: 0.5349 time: 2.92s
Epoch 61/1000, LR 0.000268
Train loss: 0.6417;  Loss pred: 0.6417; Loss self: 0.0000; time: 0.42s
Val loss: 0.6519 score: 0.5909 time: 0.29s
Test loss: 0.6511 score: 0.5349 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 0.41s
Val loss: 0.6484 score: 0.5909 time: 0.37s
Test loss: 0.6474 score: 0.5349 time: 0.23s
Epoch 63/1000, LR 0.000268
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.39s
Val loss: 0.6446 score: 0.5909 time: 0.29s
Test loss: 0.6433 score: 0.5349 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.45s
Val loss: 0.6405 score: 0.5909 time: 0.90s
Test loss: 0.6390 score: 0.5349 time: 3.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 2.16s
Val loss: 0.6361 score: 0.5909 time: 0.31s
Test loss: 0.6343 score: 0.5581 time: 0.26s
Epoch 66/1000, LR 0.000268
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 0.39s
Val loss: 0.6313 score: 0.5909 time: 0.28s
Test loss: 0.6292 score: 0.5581 time: 0.29s
Epoch 67/1000, LR 0.000268
Train loss: 0.6072;  Loss pred: 0.6072; Loss self: 0.0000; time: 0.39s
Val loss: 0.6262 score: 0.6364 time: 0.36s
Test loss: 0.6237 score: 0.5814 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.6000;  Loss pred: 0.6000; Loss self: 0.0000; time: 0.42s
Val loss: 0.6208 score: 0.7045 time: 1.88s
Test loss: 0.6178 score: 0.6047 time: 3.95s
Epoch 69/1000, LR 0.000268
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 2.77s
Val loss: 0.6149 score: 0.7045 time: 0.31s
Test loss: 0.6115 score: 0.6047 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.41s
Val loss: 0.6087 score: 0.7045 time: 0.27s
Test loss: 0.6049 score: 0.6047 time: 0.28s
Epoch 71/1000, LR 0.000268
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.46s
Val loss: 0.6020 score: 0.7045 time: 0.28s
Test loss: 0.5978 score: 0.6744 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 6.69s
Val loss: 0.5950 score: 0.7045 time: 2.92s
Test loss: 0.5904 score: 0.6977 time: 2.29s
Epoch 73/1000, LR 0.000267
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 2.27s
Val loss: 0.5875 score: 0.7500 time: 0.31s
Test loss: 0.5827 score: 0.7209 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.5421;  Loss pred: 0.5421; Loss self: 0.0000; time: 0.51s
Val loss: 0.5796 score: 0.7727 time: 0.28s
Test loss: 0.5745 score: 0.7209 time: 0.26s
Epoch 75/1000, LR 0.000267
Train loss: 0.5405;  Loss pred: 0.5405; Loss self: 0.0000; time: 0.35s
Val loss: 0.5714 score: 0.8182 time: 0.28s
Test loss: 0.5660 score: 0.7209 time: 0.26s
Epoch 76/1000, LR 0.000267
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.63s
Val loss: 0.5629 score: 0.8409 time: 3.31s
Test loss: 0.5573 score: 0.7674 time: 3.72s
Epoch 77/1000, LR 0.000267
Train loss: 0.5049;  Loss pred: 0.5049; Loss self: 0.0000; time: 7.09s
Val loss: 0.5541 score: 0.8409 time: 0.30s
Test loss: 0.5484 score: 0.7674 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.4907;  Loss pred: 0.4907; Loss self: 0.0000; time: 0.57s
Val loss: 0.5450 score: 0.8409 time: 0.30s
Test loss: 0.5393 score: 0.8140 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.4846;  Loss pred: 0.4846; Loss self: 0.0000; time: 0.45s
Val loss: 0.5357 score: 0.8409 time: 0.29s
Test loss: 0.5298 score: 0.8140 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.4667;  Loss pred: 0.4667; Loss self: 0.0000; time: 0.42s
Val loss: 0.5261 score: 0.8409 time: 3.13s
Test loss: 0.5202 score: 0.8140 time: 4.08s
Epoch 81/1000, LR 0.000267
Train loss: 0.4488;  Loss pred: 0.4488; Loss self: 0.0000; time: 5.04s
Val loss: 0.5162 score: 0.8864 time: 0.31s
Test loss: 0.5109 score: 0.8140 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.4406;  Loss pred: 0.4406; Loss self: 0.0000; time: 0.41s
Val loss: 0.5061 score: 0.8864 time: 0.28s
Test loss: 0.5018 score: 0.7907 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.4245;  Loss pred: 0.4245; Loss self: 0.0000; time: 0.50s
Val loss: 0.4958 score: 0.8864 time: 0.34s
Test loss: 0.4930 score: 0.7907 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.4123;  Loss pred: 0.4123; Loss self: 0.0000; time: 0.43s
Val loss: 0.4854 score: 0.8864 time: 0.27s
Test loss: 0.4856 score: 0.7907 time: 3.07s
Epoch 85/1000, LR 0.000266
Train loss: 0.3998;  Loss pred: 0.3998; Loss self: 0.0000; time: 2.65s
Val loss: 0.4749 score: 0.8864 time: 0.30s
Test loss: 0.4786 score: 0.7907 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3994;  Loss pred: 0.3994; Loss self: 0.0000; time: 0.40s
Val loss: 0.4645 score: 0.9091 time: 0.31s
Test loss: 0.4713 score: 0.7907 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.3849;  Loss pred: 0.3849; Loss self: 0.0000; time: 0.42s
Val loss: 0.4544 score: 0.9091 time: 0.26s
Test loss: 0.4630 score: 0.7907 time: 0.33s
Epoch 88/1000, LR 0.000266
Train loss: 0.3553;  Loss pred: 0.3553; Loss self: 0.0000; time: 0.42s
Val loss: 0.4444 score: 0.9091 time: 0.27s
Test loss: 0.4553 score: 0.7907 time: 0.25s
Epoch 89/1000, LR 0.000266
Train loss: 0.3517;  Loss pred: 0.3517; Loss self: 0.0000; time: 0.34s
Val loss: 0.4349 score: 0.9091 time: 0.29s
Test loss: 0.4473 score: 0.8140 time: 0.23s
Epoch 90/1000, LR 0.000266
Train loss: 0.3436;  Loss pred: 0.3436; Loss self: 0.0000; time: 0.46s
Val loss: 0.4257 score: 0.9091 time: 0.29s
Test loss: 0.4392 score: 0.8140 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.3327;  Loss pred: 0.3327; Loss self: 0.0000; time: 0.41s
Val loss: 0.4169 score: 0.9318 time: 0.26s
Test loss: 0.4319 score: 0.8372 time: 0.26s
Epoch 92/1000, LR 0.000266
Train loss: 0.3123;  Loss pred: 0.3123; Loss self: 0.0000; time: 0.42s
Val loss: 0.4082 score: 0.9318 time: 0.27s
Test loss: 0.4253 score: 0.8605 time: 0.25s
Epoch 93/1000, LR 0.000265
Train loss: 0.3205;  Loss pred: 0.3205; Loss self: 0.0000; time: 0.38s
Val loss: 0.3993 score: 0.9318 time: 0.37s
Test loss: 0.4198 score: 0.8605 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.3171;  Loss pred: 0.3171; Loss self: 0.0000; time: 0.47s
Val loss: 0.3918 score: 0.9545 time: 0.30s
Test loss: 0.4132 score: 0.8605 time: 0.24s
Epoch 95/1000, LR 0.000265
Train loss: 0.3070;  Loss pred: 0.3070; Loss self: 0.0000; time: 0.40s
Val loss: 0.3846 score: 0.9545 time: 0.27s
Test loss: 0.4070 score: 0.8605 time: 0.25s
Epoch 96/1000, LR 0.000265
Train loss: 0.2714;  Loss pred: 0.2714; Loss self: 0.0000; time: 0.44s
Val loss: 0.3775 score: 0.9318 time: 0.30s
Test loss: 0.4015 score: 0.8605 time: 0.26s
Epoch 97/1000, LR 0.000265
Train loss: 0.2705;  Loss pred: 0.2705; Loss self: 0.0000; time: 0.36s
Val loss: 0.3711 score: 0.9091 time: 0.28s
Test loss: 0.3961 score: 0.8605 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.2482;  Loss pred: 0.2482; Loss self: 0.0000; time: 0.41s
Val loss: 0.3631 score: 0.9318 time: 0.38s
Test loss: 0.3928 score: 0.8605 time: 0.23s
Epoch 99/1000, LR 0.000265
Train loss: 0.2361;  Loss pred: 0.2361; Loss self: 0.0000; time: 0.47s
Val loss: 0.3551 score: 0.9318 time: 0.28s
Test loss: 0.3902 score: 0.8605 time: 0.23s
Epoch 100/1000, LR 0.000265
Train loss: 0.2234;  Loss pred: 0.2234; Loss self: 0.0000; time: 0.41s
Val loss: 0.3478 score: 0.9318 time: 0.32s
Test loss: 0.3873 score: 0.8605 time: 0.26s
Epoch 101/1000, LR 0.000265
Train loss: 0.2026;  Loss pred: 0.2026; Loss self: 0.0000; time: 0.42s
Val loss: 0.3399 score: 0.9318 time: 0.27s
Test loss: 0.3857 score: 0.8605 time: 0.25s
Epoch 102/1000, LR 0.000264
Train loss: 0.2219;  Loss pred: 0.2219; Loss self: 0.0000; time: 0.38s
Val loss: 0.3307 score: 0.9318 time: 0.29s
Test loss: 0.3861 score: 0.8605 time: 0.23s
Epoch 103/1000, LR 0.000264
Train loss: 0.1857;  Loss pred: 0.1857; Loss self: 0.0000; time: 0.40s
Val loss: 0.3217 score: 0.9318 time: 0.28s
Test loss: 0.3870 score: 0.8605 time: 0.25s
Epoch 104/1000, LR 0.000264
Train loss: 0.1890;  Loss pred: 0.1890; Loss self: 0.0000; time: 0.50s
Val loss: 0.3145 score: 0.9318 time: 0.29s
Test loss: 0.3868 score: 0.8605 time: 0.25s
Epoch 105/1000, LR 0.000264
Train loss: 0.1923;  Loss pred: 0.1923; Loss self: 0.0000; time: 0.42s
Val loss: 0.3098 score: 0.9318 time: 0.29s
Test loss: 0.3848 score: 0.8605 time: 0.26s
Epoch 106/1000, LR 0.000264
Train loss: 0.1766;  Loss pred: 0.1766; Loss self: 0.0000; time: 0.38s
Val loss: 0.3056 score: 0.9318 time: 0.28s
Test loss: 0.3835 score: 0.8605 time: 0.25s
Epoch 107/1000, LR 0.000264
Train loss: 0.1638;  Loss pred: 0.1638; Loss self: 0.0000; time: 0.38s
Val loss: 0.2965 score: 0.9318 time: 0.30s
Test loss: 0.3878 score: 0.8605 time: 0.24s
Epoch 108/1000, LR 0.000264
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.40s
Val loss: 0.2880 score: 0.9318 time: 0.26s
Test loss: 0.3925 score: 0.8605 time: 0.26s
Epoch 109/1000, LR 0.000264
Train loss: 0.1551;  Loss pred: 0.1551; Loss self: 0.0000; time: 0.57s
Val loss: 0.2804 score: 0.9318 time: 0.27s
Test loss: 0.3979 score: 0.8605 time: 0.24s
Epoch 110/1000, LR 0.000263
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.41s
Val loss: 0.2725 score: 0.9091 time: 0.30s
Test loss: 0.4048 score: 0.8605 time: 0.29s
Epoch 111/1000, LR 0.000263
Train loss: 0.1244;  Loss pred: 0.1244; Loss self: 0.0000; time: 0.36s
Val loss: 0.2674 score: 0.9091 time: 0.29s
Test loss: 0.4087 score: 0.8605 time: 0.23s
Epoch 112/1000, LR 0.000263
Train loss: 0.1067;  Loss pred: 0.1067; Loss self: 0.0000; time: 0.39s
Val loss: 0.2630 score: 0.9318 time: 0.36s
Test loss: 0.4123 score: 0.8605 time: 0.24s
Epoch 113/1000, LR 0.000263
Train loss: 0.1129;  Loss pred: 0.1129; Loss self: 0.0000; time: 0.42s
Val loss: 0.2626 score: 0.9318 time: 0.28s
Test loss: 0.4114 score: 0.8605 time: 0.26s
Epoch 114/1000, LR 0.000263
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.45s
Val loss: 0.2595 score: 0.9318 time: 0.28s
Test loss: 0.4156 score: 0.8605 time: 0.24s
Epoch 115/1000, LR 0.000263
Train loss: 0.1200;  Loss pred: 0.1200; Loss self: 0.0000; time: 0.38s
Val loss: 0.2550 score: 0.9318 time: 0.29s
Test loss: 0.4218 score: 0.8605 time: 0.23s
Epoch 116/1000, LR 0.000263
Train loss: 0.1095;  Loss pred: 0.1095; Loss self: 0.0000; time: 0.44s
Val loss: 0.2514 score: 0.9318 time: 0.28s
Test loss: 0.4278 score: 0.8605 time: 0.24s
Epoch 117/1000, LR 0.000262
Train loss: 0.1043;  Loss pred: 0.1043; Loss self: 0.0000; time: 0.42s
Val loss: 0.2472 score: 0.9318 time: 0.28s
Test loss: 0.4350 score: 0.8605 time: 0.26s
Epoch 118/1000, LR 0.000262
Train loss: 0.0859;  Loss pred: 0.0859; Loss self: 0.0000; time: 0.35s
Val loss: 0.2446 score: 0.9318 time: 0.30s
Test loss: 0.4406 score: 0.8605 time: 0.32s
Epoch 119/1000, LR 0.000262
Train loss: 0.0842;  Loss pred: 0.0842; Loss self: 0.0000; time: 0.38s
Val loss: 0.2418 score: 0.9318 time: 0.29s
Test loss: 0.4474 score: 0.8605 time: 0.23s
Epoch 120/1000, LR 0.000262
Train loss: 0.0824;  Loss pred: 0.0824; Loss self: 0.0000; time: 0.45s
Val loss: 0.2370 score: 0.9318 time: 0.29s
Test loss: 0.4566 score: 0.8605 time: 0.23s
Epoch 121/1000, LR 0.000262
Train loss: 0.0981;  Loss pred: 0.0981; Loss self: 0.0000; time: 0.42s
Val loss: 0.2331 score: 0.9318 time: 0.28s
Test loss: 0.4648 score: 0.8605 time: 0.25s
Epoch 122/1000, LR 0.000262
Train loss: 0.0748;  Loss pred: 0.0748; Loss self: 0.0000; time: 0.48s
Val loss: 0.2298 score: 0.9318 time: 0.28s
Test loss: 0.4728 score: 0.8605 time: 0.26s
Epoch 123/1000, LR 0.000262
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.36s
Val loss: 0.2267 score: 0.9318 time: 0.32s
Test loss: 0.4812 score: 0.8605 time: 0.24s
Epoch 124/1000, LR 0.000261
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.48s
Val loss: 0.2278 score: 0.9318 time: 0.36s
Test loss: 0.4835 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 125/1000, LR 0.000261
Train loss: 0.0739;  Loss pred: 0.0739; Loss self: 0.0000; time: 0.40s
Val loss: 0.2321 score: 0.9318 time: 0.29s
Test loss: 0.4838 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 122,   Train_Loss: 0.0485,   Val_Loss: 0.2267,   Val_Precision: 1.0000,   Val_Recall: 0.8636,   Val_accuracy: 0.9268,   Val_Score: 0.9318,   Val_Loss: 0.2267,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.4812


[0.40636897599324584, 0.3002284551039338, 0.2978265050332993, 0.2545609090011567, 0.2837188630364835, 0.27263582800514996, 0.254939550999552, 0.2749983440153301, 0.2832459290511906, 0.2575019010109827, 0.25182334205601364, 0.2721152190351859, 0.2753718039020896, 0.25613876106217504, 0.2596663129515946, 0.2707451180322096, 0.28005320904776454, 0.35770337702706456, 0.25649273104500026, 0.26821374299470335, 0.25421305594500154, 0.2656306040007621, 0.25807498290669173, 0.2581513290060684, 0.26099681307096034, 2.7968804870033637, 0.288131756009534, 0.275230975006707, 0.27446003805380315, 0.2746391830733046, 3.338902318966575, 0.3024427240015939, 0.2565098729683086, 0.2615188549971208, 3.1736195660196245, 0.30317858303897083, 0.24559014500118792, 0.2592047690413892, 0.27508937602397054, 0.2680619159946218, 0.28599837596993893, 1.1743416889803484, 0.25063800590578467, 0.26314399403054267, 0.26289390004239976, 0.2954602830577642, 2.5907203539973125, 0.27518242597579956, 0.28677149198483676, 0.27090878097806126, 3.6040550579782575, 0.352381553966552, 0.26153387199155986, 0.26929621398448944, 4.745579993003048, 0.2744280679617077, 0.35612681007478386, 0.27421071694698185, 3.6486057810252532, 0.2910055450629443, 0.28495489398483187, 0.2850041020428762, 3.313398226047866, 0.2868257029913366, 0.2692249519750476, 0.2606490550097078, 2.761696593952365, 0.2678535240702331, 0.2736266009742394, 0.2737494909670204, 3.304999663028866, 0.26257871894631535, 0.2690953010460362, 0.264408916933462, 0.26974624989088625, 2.832577483030036, 0.2775886619929224, 0.277745881001465, 0.3180003680754453, 2.503262465004809, 0.3066908939508721, 0.24967700894922018, 0.2549796380335465, 0.24501780199352652, 2.6426375559531152, 0.27124184102285653, 0.2723184850765392, 0.26396348199341446, 0.2537716639926657, 0.35085650300607085, 0.2688884820090607, 0.2494402170414105, 0.25469479302410036, 3.1545641629491, 0.2623940100893378, 0.24368973006494343, 0.2706529110437259, 0.2754707190906629, 4.295712860999629, 0.25470353302080184, 0.25492486194707453, 0.25803392904344946, 0.26525914296507835, 0.26481673202943057, 3.4631612410303205, 0.27395518601406366, 0.2561106589855626, 0.23526518791913986, 0.24040945107117295, 3.0041970920283347, 0.25863359100185335, 0.22835141397081316, 0.26576596091035753, 2.3256782370153815, 3.136794095975347, 0.24893425800837576, 0.2481388170272112, 0.26195126795209944, 2.9889870809856802, 0.27179097093176097, 0.23684458702336997, 0.23127601307351142, 0.26082086202222854, 0.5325053270207718, 0.25071858591400087, 0.252462582080625, 0.2816919609904289, 0.35760252899490297, 1.4388682410353795, 0.25876218802295625, 0.23023003502748907, 0.2626829460496083, 2.094258843921125, 0.31741617002990097, 0.2614656729856506, 0.25847742904443294, 0.30270677898079157, 3.711446372093633, 0.2771116729127243, 0.24249100498855114, 0.24660336191300303, 3.4497966170310974, 0.2779298269888386, 0.2349572969833389, 0.3083835960133001, 0.2579336540075019, 2.418929420062341, 0.2503753730561584, 0.253926892997697, 0.26108462701085955, 3.763454247964546, 0.267902833991684, 0.25212958408519626, 0.2606769719859585, 0.2948816259158775, 1.1300009989645332, 0.2511851800372824, 0.268516412936151, 0.3489673410076648, 0.2581872440641746, 0.22857194894459099, 0.28037612000480294, 2.92561181797646, 0.2544754600385204, 0.23755813401658088, 0.23905667196959257, 3.0827885309699923, 0.2677674819715321, 0.29004700796213, 0.2596278080018237, 3.957012293045409, 0.23945629398804158, 0.28313630691263825, 0.2670128360623494, 2.298311103018932, 0.2574774220120162, 0.2668278379132971, 0.26763755606953055, 3.727729024947621, 0.24663932097610086, 0.23763980704825372, 0.30803145898971707, 4.088207479100674, 0.24113658792339265, 0.2553302739979699, 0.236139110988006, 3.07230864407029, 0.23313686798792332, 0.2511285630753264, 0.33303323993459344, 0.2513798449654132, 0.23754923802334815, 0.24501684692222625, 0.26877951598726213, 0.25372725701890886, 0.2312296339077875, 0.24339786695782095, 0.25628510990645736, 0.26622063608374447, 0.23590805591084063, 0.23808999999891967, 0.23196089605335146, 0.26209481293335557, 0.2572839839849621, 0.23687951895408332, 0.2579290120629594, 0.2533084279857576, 0.25951594405341893, 0.2528618989745155, 0.24892746203113347, 0.2616468999767676, 0.24802251800429076, 0.29027085402049124, 0.232814859948121, 0.24369360704440624, 0.2656848169863224, 0.24716138700023293, 0.23236791405361146, 0.24505312996916473, 0.26257795898709446, 0.3249488009605557, 0.23420859896577895, 0.23205996700562537, 0.2557405539555475, 0.26334774296265095, 0.24885223899036646, 0.2506196149624884, 0.2541426420211792]
[0.009235658545301041, 0.006823373979634859, 0.006768784205302258, 0.005920021139561783, 0.00659811309387171, 0.006340368093143023, 0.005928826767431442, 0.006395310325937909, 0.006587114629097456, 0.005988416302580993, 0.005856356792000317, 0.00632826090779502, 0.006403995439583479, 0.0059567153735389546, 0.006038751463990572, 0.006296398093772315, 0.006512865326692199, 0.00831868318667592, 0.005964947233604657, 0.006237528906853566, 0.005911931533604687, 0.0061774559069944675, 0.006001743788527715, 0.006003519279210893, 0.006069693327231636, 0.06504373225589218, 0.006700738511849628, 0.006400720348993186, 0.006382791582646585, 0.006386957745890804, 0.07764889113875756, 0.0070335517209673, 0.005965345882983922, 0.006081833837142344, 0.07380510618650289, 0.0070506647218365305, 0.005711398720957859, 0.006028017884683471, 0.006397427349394664, 0.006233998046386554, 0.006651125022556719, 0.02731027183675229, 0.0058287908350182485, 0.0061196277681521555, 0.006113811628893018, 0.006871169373436376, 0.060249310558077036, 0.00639959130176278, 0.0066691044647636455, 0.006300204208792123, 0.08381523390647111, 0.008194919859687256, 0.006082183069571159, 0.00626270265080208, 0.11036232541867554, 0.006382048092132738, 0.008282018838948461, 0.006376993417371671, 0.08485129723314543, 0.006767570815417309, 0.006626857999647253, 0.006628002373090144, 0.07705577269878758, 0.0066703651858450366, 0.006261045394768548, 0.006061605930458321, 0.06422550218493872, 0.006229151722563561, 0.0063634093249823115, 0.006366267231791172, 0.07686045727974107, 0.006106481835960822, 0.006258030256884563, 0.006149044579847954, 0.006273168602113633, 0.06587389495418688, 0.006455550278905172, 0.006459206534917791, 0.007395357397103379, 0.05821540616290254, 0.007132346370950515, 0.005806442068586516, 0.005929759024035966, 0.005698088418454105, 0.06145668734774687, 0.006307949791229222, 0.006332988025035796, 0.006138685627753825, 0.005901666604480598, 0.008159453558280717, 0.00625322051183862, 0.005800935280032803, 0.005923134721490706, 0.07336195727788605, 0.006102186281147392, 0.005667203024766127, 0.006294253745202928, 0.0064062957928061135, 0.09990029909301462, 0.0059233379772279495, 0.005928485161559873, 0.00600078904752208, 0.006168817278257636, 0.006158528651847223, 0.08053863351233304, 0.006371050837536364, 0.005956061836873549, 0.005471283439979996, 0.005590917466771464, 0.06986504865182173, 0.006014734674461706, 0.005310497999321236, 0.006180603742101338, 0.054085540395706545, 0.07294869990640342, 0.00578916879089246, 0.005770670163423516, 0.0060918899523744055, 0.06951132746478327, 0.006320720254226999, 0.005508013651706278, 0.005378511931942126, 0.006065601442377408, 0.012383844814436553, 0.005830664788697695, 0.005871222839084303, 0.006550975836986719, 0.008316337883602394, 0.03346205211710185, 0.006017725302859447, 0.0053541868611043965, 0.006108905722083914, 0.04870369404467732, 0.007381771396044208, 0.006080597046177921, 0.006011103001033324, 0.007039692534437013, 0.0863127063277589, 0.00644445750959824, 0.005639325697408166, 0.0057349619049535585, 0.08022782830304878, 0.006463484348577642, 0.005464123185659044, 0.007171711535193026, 0.005998457069941904, 0.05625417255958932, 0.005822683094329265, 0.005905276581341791, 0.006071735511880455, 0.08752219181312898, 0.0062302984649228845, 0.005863478699655727, 0.006062255162464151, 0.006857712230601803, 0.02627909299917519, 0.005841515814820521, 0.0062445677427011865, 0.008115519558317786, 0.0060043545131203396, 0.0053156267196416505, 0.006520374883832627, 0.06803748413898744, 0.005918033954384195, 0.005524607767827462, 0.0055594574876649434, 0.07169275653418587, 0.006227150743524003, 0.006745279254933256, 0.0060378560000424126, 0.09202354169873045, 0.005568751022977711, 0.006584565277038099, 0.006209600838659288, 0.05344909541904493, 0.00598784702353526, 0.006205298556123189, 0.006224129210919315, 0.08669137267320048, 0.005735798162234904, 0.005526507140657063, 0.007163522302086444, 0.09507459253722499, 0.00560782762612541, 0.005937913348789998, 0.005491607232279209, 0.0714490382341928, 0.005421787627626124, 0.00584019914128666, 0.007744959068246359, 0.0058460429061724, 0.005524400884263911, 0.005698066207493634, 0.006250686418308422, 0.0059006338841606715, 0.005377433346692733, 0.005660415510646999, 0.005960118835033892, 0.006191177583342895, 0.005486233858391643, 0.005536976744160923, 0.005394439443101196, 0.006095228207752455, 0.005983348464766561, 0.005508826022187985, 0.005998349117743241, 0.005890893674087386, 0.0060352545128702076, 0.005880509278477105, 0.0057890107449100805, 0.006084811627366688, 0.005767965534983506, 0.006750484977220726, 0.005414299068560954, 0.005667293187079214, 0.006178716674100521, 0.005747939232563557, 0.005403904977990964, 0.005698909999282901, 0.0061064641624905686, 0.007556948859547806, 0.005446711603855324, 0.005396743418735473, 0.005947454743152268, 0.006124366115410488, 0.005787261371868987, 0.005828363138662521, 0.005910294000492539]
[108.27598217224957, 146.55506249322028, 147.73701888984144, 168.918315733248, 151.5584813071474, 157.71954960808654, 168.66743442281955, 156.36457795397823, 151.8115375710437, 166.98905845423647, 170.75462365373346, 158.02129756821827, 156.15251594636376, 167.8777543144366, 165.5971447016918, 158.82096162075374, 153.54225058233888, 120.21133364012533, 167.64607645920339, 160.31989830159134, 169.14945552325588, 161.87893771410702, 166.61824217013262, 166.56896621666894, 164.75296956330678, 15.374271514215147, 149.23728156703828, 156.23241533389236, 156.67126006726923, 156.56906461348876, 12.878483972334042, 142.17568017861586, 167.63487308464178, 164.42409095311086, 13.54919803885974, 141.83059887997678, 175.08845886219095, 165.89200946813543, 156.31283411051504, 160.4107015368148, 150.35050410398028, 36.616259478394085, 171.5621692911321, 163.40863168250408, 163.56408419162605, 145.53563529753086, 16.59770030125166, 156.2599786215329, 149.9451695926374, 158.72501380264313, 11.931005300491003, 122.02681870255203, 164.4146498981504, 159.67547171857623, 9.06106315000481, 156.68951182500763, 120.74350704169207, 156.81371056082384, 11.785323649823594, 147.76350736099937, 150.90107560071905, 150.87502141822173, 12.97761303243325, 149.91682945966244, 159.71773672740906, 164.97278303348722, 15.57013905660836, 160.53550219008915, 157.14846380761145, 157.07791765421172, 13.010591341662245, 163.7604150578228, 159.79468921548974, 162.62689057049033, 159.40907433335485, 15.18052030619211, 154.90546224505508, 154.81777747686272, 135.21996927311193, 17.177583493993463, 140.20631472314935, 172.2225053118344, 168.64091710077133, 175.4974522264961, 16.271622229515796, 158.53011407770438, 157.90334610562408, 162.90132133153477, 169.44366176848945, 122.55722676246356, 159.917597357521, 172.38599496913287, 168.8295213633643, 13.631043078800683, 163.87569207604892, 176.45388662271685, 158.87506930621205, 156.09644517553187, 10.009980040889822, 168.82372808110264, 168.67715322692737, 166.64475156195206, 162.10562817682404, 162.3764467995216, 12.41640137645082, 156.95997811040732, 167.89617492032608, 182.77247212103055, 178.86152066155626, 14.313308575559475, 166.25837283329474, 188.30625680073987, 161.79649136671733, 18.48923007302304, 13.7082634958957, 172.7363696102977, 173.2900983213947, 164.15266983118022, 14.3861444813672, 158.20981783385324, 181.55365313777298, 185.92503143130708, 164.86411273472177, 80.75036589882352, 171.5070298567712, 170.322269722599, 152.64901365595242, 120.24523462084602, 29.884598723965233, 166.1757474248333, 186.7697235717567, 163.69543834748734, 20.53232346365084, 135.46883889358676, 164.45753474629103, 166.35881964226823, 142.05165852175577, 11.585779690450877, 155.17209920472297, 177.32616515829187, 174.3690745593711, 12.464502918147648, 154.71531237173346, 183.01197941960143, 139.43672930691642, 166.7095368592319, 17.77645914071019, 171.74213052637262, 169.34007852563292, 164.69755608479952, 11.425673640979277, 160.50595418984915, 170.54722140607672, 164.95511541509012, 145.8212252677515, 38.05306370472476, 171.18844349661745, 160.13918676257873, 123.22069989654287, 166.54579569125417, 188.1245717094699, 153.36541499776584, 14.69778038760506, 168.97503591698396, 181.0083253011186, 179.87366612996894, 13.94841052768227, 160.5870872870649, 148.2518309777369, 165.62170412692447, 10.86678453730711, 179.57347991925164, 151.87031458055876, 161.040947072519, 18.709390536171373, 167.00493450642534, 161.15260062922067, 160.66504503885423, 11.535173214636814, 174.343652219861, 180.94611561129855, 139.59613132058522, 10.518057172934666, 178.32217155556995, 168.409328540265, 182.0960527042217, 13.995989655203477, 184.441012573899, 171.227037949889, 129.1162407945977, 171.05587763377082, 181.0151038908979, 175.4981363124355, 159.98242962100517, 169.47331755056751, 185.96232357115633, 176.6654759034991, 167.7818895358172, 161.52016099335583, 182.27440277093154, 180.60397328823183, 185.37607300029535, 164.0627661369776, 167.1304965586715, 181.52687995087965, 166.71253712825404, 169.75353067375121, 165.69309510767036, 170.05329855698713, 172.7410854918931, 164.34362495339366, 173.37135493179738, 148.13750469402788, 184.6961143699412, 176.4510793759332, 161.84590631768643, 173.97539527466512, 185.05136638649313, 175.4721517142455, 163.76088901701542, 132.32853875100037, 183.5970164625889, 185.29693231817808, 168.1391524923114, 163.2822044201018, 172.79330165747317, 171.57475884892744, 169.19632084574204]
Elapsed: 0.6913952081149706~1.042715548104325
Time per graph: 0.016076629909575993~0.02424998654592857
Speed: 139.93565509277175~54.79722343670672
Total Time: 0.2549
best val loss: 0.22669076919555664 test_score: 0.8605

Testing...
Test loss: 0.4132 score: 0.8605 time: 0.25s
test Score 0.8605
Epoch Time List: [1.2259161819238216, 0.9466558269923553, 1.0952765841502696, 0.9307083790190518, 1.0105749950744212, 0.963841357966885, 1.1787077150074765, 0.938034156919457, 0.9406237729126588, 0.9529275810346007, 0.9629078931175172, 0.9485100810416043, 1.0507864539977163, 0.9102751731406897, 0.9510700849350542, 0.9445665259845555, 0.9990228560054675, 1.0351890180027112, 0.9325778449419886, 1.0138044549385086, 0.994988527148962, 0.960284736007452, 0.8979176010470837, 1.0372082219691947, 0.9831718681380153, 6.278061967925169, 6.758528442936949, 0.9468545350246131, 1.0240557010984048, 1.003283228026703, 10.375304469023831, 8.217113528051414, 0.9638867459725589, 0.9548561649862677, 4.563093572971411, 11.205831540981308, 0.9014534429879859, 0.9201623059343547, 0.9411807861179113, 0.9440275031374767, 3.2612401369260624, 12.567819097894244, 3.152018722030334, 0.9679665720323101, 0.9137300899019465, 1.0360538300592452, 13.254474085988477, 3.1167041130829602, 1.0091113760136068, 0.9898806818528101, 6.816670585074462, 7.509232199168764, 0.8958589870017022, 0.910440869978629, 7.738425516989082, 6.053479096968658, 1.1658521982608363, 0.9910285169025883, 6.035840519005433, 9.838966213050298, 0.9771912220166996, 0.9667037220206112, 4.10711242002435, 10.434591786935925, 1.0657130208564922, 0.9229093889007345, 5.450373966130428, 3.8717764139873907, 1.0699876920552924, 1.038716790964827, 8.27632680197712, 4.588070287019946, 1.0243177359225228, 0.9020925419172272, 0.9315553001360968, 6.167513453867286, 1.5362426169449463, 1.1103437008569017, 1.03722470800858, 5.861072517000139, 9.766637007938698, 0.9218138930154964, 1.0349829881452024, 0.9712983010103926, 7.141945926123299, 7.864326884970069, 1.0049059389857575, 0.9812136460095644, 0.967886856989935, 6.276592102018185, 0.9712267380673438, 0.9238113510655239, 1.077893126057461, 3.867370677064173, 5.962600427097641, 0.979072007117793, 0.9581098410999402, 1.0451382481260225, 5.299412045045756, 4.206460719928145, 0.9992093190085143, 0.9818493608618155, 1.0220185408834368, 0.9303354009753093, 4.244724275893532, 9.262153745861724, 0.9974822730291635, 0.919919642037712, 0.9847212990280241, 7.069795334828086, 9.00063249701634, 1.1060435370309278, 1.0394769150298089, 3.0710113199893385, 16.985351920011453, 1.0093363571213558, 0.9065567490179092, 1.0869881580583751, 5.089871093980037, 6.5408763360464945, 0.8916336359689012, 0.9311474839923903, 1.0161714128917083, 9.361666667973623, 0.9886139639420435, 1.0236561329802498, 0.9768725880421698, 1.0014920650282875, 12.811916960054077, 3.7235141278943047, 0.8735902381595224, 0.9295740069355816, 2.9353203428909183, 7.194623988936655, 0.9485196779714897, 0.9668509770417586, 0.9525074949488044, 8.19655770494137, 8.467149484087713, 0.9252263378584757, 0.9359849989414215, 4.198059754096903, 10.98176274402067, 0.9899598360061646, 1.1083407909609377, 1.0064988780068234, 17.294218354858458, 5.679963078000583, 1.0080696739023551, 1.065604413044639, 4.566202198038809, 10.010488700936548, 0.9401082469848916, 1.0068247690796852, 1.103464181884192, 13.4151942350436, 3.299383518868126, 0.986036361893639, 1.0266152920667082, 7.449667768902145, 0.9063431291142479, 0.9382033151341602, 7.259874618961476, 0.9609703809255734, 1.007674427004531, 0.9165310269454494, 4.420727586024441, 2.7334552140673622, 0.9536661170423031, 1.008024011971429, 6.255849713925272, 3.321916454937309, 0.9632351160980761, 1.0097690600669011, 11.895223479834385, 2.838150994037278, 1.0555825700284913, 0.8962904630461708, 7.66036272700876, 7.629316882928833, 1.0979140668641776, 1.0396196889923885, 7.635593579965644, 5.584159915102646, 0.9414263478247449, 1.0729243190726265, 3.776173137128353, 3.1753107520053163, 0.9646309659583494, 1.0051191500388086, 0.9404461920494214, 0.8643837459385395, 0.9951339169638231, 0.9405340128578246, 0.9393115469720215, 0.9699158680159599, 1.0036540840519592, 0.924355752998963, 0.9973352251108736, 0.8738064570352435, 1.0215380830923095, 0.979569357004948, 0.9914409338962287, 0.9369584289379418, 0.9022810228634626, 0.9419900909997523, 1.0425788559950888, 0.964493929874152, 0.9133346860762686, 0.9273560680449009, 0.9176097349263728, 1.07624807395041, 0.9984308348502964, 0.8833330499473959, 0.9833251100499183, 0.960792392026633, 0.969733603997156, 0.8937567989341915, 0.9598999859299511, 0.951548216980882, 0.9795642290264368, 0.9085890488931909, 0.9624158391961828, 0.9475493530044332, 1.0229287040419877, 0.9245761460624635, 1.082159765996039, 0.9425019660266116]
Total Epoch List: [3, 100, 125]
Total Time List: [0.29812087200116366, 0.26575117697939277, 0.25491444708313793]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcead0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.30s
Epoch 2/1000, LR 0.000000
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.28s
Epoch 3/1000, LR 0.000030
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.28s
Epoch 4/1000, LR 0.000060
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.30s
Epoch 5/1000, LR 0.000090
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.41s
Epoch 6/1000, LR 0.000120
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.28s
Epoch 7/1000, LR 0.000150
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.29s
Epoch 8/1000, LR 0.000180
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.30s
Epoch 9/1000, LR 0.000210
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.29s
Epoch 10/1000, LR 0.000240
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.29s
     INFO: Early stopping counter 1 of 2
Epoch 11/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.47s
Val loss: 0.6929 score: 0.6279 time: 0.27s
Test loss: 0.6927 score: 0.8182 time: 0.29s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 0.6931,   Val_Loss: 0.6929,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.6929,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6928


[0.3056238789577037, 0.2838673379737884, 0.2822310660267249, 0.30379543802700937, 0.41317588998936117, 0.28435167705174536, 0.29632433189544827, 0.3093688919907436, 0.29447981994599104, 0.2964312139665708, 0.295494441059418]
[0.006945997249038721, 0.00645153040849519, 0.006414342409698293, 0.006904441773341122, 0.009390361136121845, 0.006462538114812394, 0.006734643906714733, 0.007031111181607808, 0.006692723180590706, 0.006737073044694791, 0.00671578275135041]
[143.96780824213445, 155.00198196124575, 155.90062645985816, 144.83430128430078, 106.49217697850897, 154.73796552285862, 148.4859502375406, 142.22503017955825, 149.41601094455234, 148.43241172625625, 148.90297036468607]
Elapsed: 0.30592218062586407~0.03497360582444661
Time per graph: 0.006952776832406002~0.000794854677828332
Speed: 145.30883944559093~13.013822965515441
Total Time: 0.2965
best val loss: 0.6928503513336182 test_score: 0.5000

Testing...
Test loss: 0.6927 score: 0.8182 time: 0.30s
test Score 0.8182
Epoch Time List: [0.9350183268543333, 0.8981378941098228, 0.9070376269519329, 0.9181678760796785, 1.020753141026944, 0.8785162250278518, 0.9238926719408482, 0.9424957711016759, 0.9129333830205724, 0.9253993699094281, 1.0298739928985015]
Total Epoch List: [11]
Total Time List: [0.2965386339928955]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcc490>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6992 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7017 score: 0.4884 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7017 score: 0.4884 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7015 score: 0.4884 time: 0.27s
Epoch 4/1000, LR 0.000060
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.4884 time: 0.32s
Epoch 5/1000, LR 0.000090
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.4884 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7007 score: 0.4884 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.4884 time: 0.26s
Epoch 8/1000, LR 0.000180
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.4884 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.4884 time: 0.34s
Epoch 10/1000, LR 0.000240
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6989 score: 0.4884 time: 0.26s
Epoch 11/1000, LR 0.000270
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4884 time: 0.27s
Epoch 12/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.4884 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4884 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.34s
Epoch 15/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4884 time: 0.26s
Epoch 16/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4884 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4884 time: 0.26s
Epoch 18/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.29s
Epoch 19/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.35s
Epoch 20/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.26s
Epoch 22/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 23/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.26s
Epoch 24/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.32s
Epoch 25/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.41s
Val loss: 0.6886 score: 0.5227 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.27s
Epoch 27/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.41s
Val loss: 0.6880 score: 0.5682 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.39s
Val loss: 0.6873 score: 0.6364 time: 0.27s
Test loss: 0.6903 score: 0.5814 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.42s
Val loss: 0.6865 score: 0.6591 time: 0.26s
Test loss: 0.6898 score: 0.5814 time: 0.34s
Epoch 30/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.44s
Val loss: 0.6858 score: 0.6818 time: 0.26s
Test loss: 0.6892 score: 0.4651 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.43s
Val loss: 0.6850 score: 0.7045 time: 0.26s
Test loss: 0.6886 score: 0.6047 time: 0.27s
Epoch 32/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.43s
Val loss: 0.6841 score: 0.6591 time: 0.28s
Test loss: 0.6879 score: 0.6279 time: 0.26s
Epoch 33/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.40s
Val loss: 0.6832 score: 0.6364 time: 0.29s
Test loss: 0.6873 score: 0.6977 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.44s
Val loss: 0.6823 score: 0.6591 time: 0.29s
Test loss: 0.6866 score: 0.6279 time: 0.35s
Epoch 35/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.44s
Val loss: 0.6812 score: 0.6136 time: 0.26s
Test loss: 0.6859 score: 0.5814 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.46s
Val loss: 0.6802 score: 0.6136 time: 0.24s
Test loss: 0.6851 score: 0.5814 time: 0.28s
Epoch 37/1000, LR 0.000270
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 0.39s
Val loss: 0.6790 score: 0.6136 time: 0.26s
Test loss: 0.6843 score: 0.5581 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.41s
Val loss: 0.6777 score: 0.5909 time: 0.27s
Test loss: 0.6835 score: 0.5581 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.42s
Val loss: 0.6763 score: 0.5682 time: 0.27s
Test loss: 0.6825 score: 0.5581 time: 0.33s
Epoch 40/1000, LR 0.000269
Train loss: 0.6675;  Loss pred: 0.6675; Loss self: 0.0000; time: 0.41s
Val loss: 0.6747 score: 0.5682 time: 0.24s
Test loss: 0.6814 score: 0.5581 time: 0.27s
Epoch 41/1000, LR 0.000269
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.41s
Val loss: 0.6729 score: 0.5682 time: 0.25s
Test loss: 0.6803 score: 0.5581 time: 0.26s
Epoch 42/1000, LR 0.000269
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 0.38s
Val loss: 0.6710 score: 0.5682 time: 0.27s
Test loss: 0.6791 score: 0.5581 time: 0.31s
Epoch 43/1000, LR 0.000269
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.43s
Val loss: 0.6690 score: 0.5682 time: 0.27s
Test loss: 0.6777 score: 0.5581 time: 0.26s
Epoch 44/1000, LR 0.000269
Train loss: 0.6543;  Loss pred: 0.6543; Loss self: 0.0000; time: 0.41s
Val loss: 0.6668 score: 0.5909 time: 0.25s
Test loss: 0.6762 score: 0.5581 time: 0.33s
Epoch 45/1000, LR 0.000269
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.43s
Val loss: 0.6645 score: 0.5909 time: 0.26s
Test loss: 0.6747 score: 0.5581 time: 0.27s
Epoch 46/1000, LR 0.000269
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.45s
Val loss: 0.6620 score: 0.5909 time: 0.25s
Test loss: 0.6730 score: 0.5581 time: 0.26s
Epoch 47/1000, LR 0.000269
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.39s
Val loss: 0.6593 score: 0.5909 time: 0.27s
Test loss: 0.6711 score: 0.5581 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 0.41s
Val loss: 0.6565 score: 0.5909 time: 0.26s
Test loss: 0.6692 score: 0.5581 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6331;  Loss pred: 0.6331; Loss self: 0.0000; time: 0.44s
Val loss: 0.6534 score: 0.6136 time: 0.28s
Test loss: 0.6670 score: 0.5581 time: 0.35s
Epoch 50/1000, LR 0.000269
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 0.40s
Val loss: 0.6501 score: 0.6136 time: 0.26s
Test loss: 0.6647 score: 0.5814 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.42s
Val loss: 0.6465 score: 0.6136 time: 0.27s
Test loss: 0.6622 score: 0.6047 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6154;  Loss pred: 0.6154; Loss self: 0.0000; time: 0.41s
Val loss: 0.6426 score: 0.6364 time: 0.26s
Test loss: 0.6594 score: 0.6047 time: 0.26s
Epoch 53/1000, LR 0.000269
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.43s
Val loss: 0.6384 score: 0.6364 time: 0.26s
Test loss: 0.6565 score: 0.6279 time: 0.26s
Epoch 54/1000, LR 0.000269
Train loss: 0.6053;  Loss pred: 0.6053; Loss self: 0.0000; time: 0.41s
Val loss: 0.6340 score: 0.6364 time: 0.26s
Test loss: 0.6533 score: 0.6512 time: 0.35s
Epoch 55/1000, LR 0.000269
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.37s
Val loss: 0.6294 score: 0.6364 time: 0.27s
Test loss: 0.6498 score: 0.6512 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.40s
Val loss: 0.6244 score: 0.6591 time: 0.27s
Test loss: 0.6461 score: 0.6279 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 0.43s
Val loss: 0.6191 score: 0.6818 time: 0.24s
Test loss: 0.6422 score: 0.6279 time: 0.27s
Epoch 58/1000, LR 0.000269
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.42s
Val loss: 0.6136 score: 0.6818 time: 0.25s
Test loss: 0.6380 score: 0.6512 time: 0.28s
Epoch 59/1000, LR 0.000268
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.41s
Val loss: 0.6077 score: 0.7273 time: 0.28s
Test loss: 0.6336 score: 0.6744 time: 0.32s
Epoch 60/1000, LR 0.000268
Train loss: 0.5462;  Loss pred: 0.5462; Loss self: 0.0000; time: 0.40s
Val loss: 0.6014 score: 0.6818 time: 0.27s
Test loss: 0.6289 score: 0.7209 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5368;  Loss pred: 0.5368; Loss self: 0.0000; time: 0.42s
Val loss: 0.5948 score: 0.7273 time: 0.24s
Test loss: 0.6240 score: 0.7674 time: 0.27s
Epoch 62/1000, LR 0.000268
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.37s
Val loss: 0.5880 score: 0.7955 time: 0.26s
Test loss: 0.6188 score: 0.7442 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.5131;  Loss pred: 0.5131; Loss self: 0.0000; time: 0.41s
Val loss: 0.5809 score: 0.7955 time: 0.26s
Test loss: 0.6134 score: 0.7209 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.4993;  Loss pred: 0.4993; Loss self: 0.0000; time: 0.43s
Val loss: 0.5737 score: 0.7955 time: 0.25s
Test loss: 0.6078 score: 0.7209 time: 0.35s
Epoch 65/1000, LR 0.000268
Train loss: 0.4884;  Loss pred: 0.4884; Loss self: 0.0000; time: 0.42s
Val loss: 0.5662 score: 0.7955 time: 0.26s
Test loss: 0.6020 score: 0.7209 time: 0.27s
Epoch 66/1000, LR 0.000268
Train loss: 0.4728;  Loss pred: 0.4728; Loss self: 0.0000; time: 0.39s
Val loss: 0.5585 score: 0.7955 time: 0.27s
Test loss: 0.5960 score: 0.7209 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.4621;  Loss pred: 0.4621; Loss self: 0.0000; time: 0.43s
Val loss: 0.5508 score: 0.7955 time: 0.25s
Test loss: 0.5899 score: 0.7209 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.4472;  Loss pred: 0.4472; Loss self: 0.0000; time: 0.47s
Val loss: 0.5430 score: 0.7955 time: 0.26s
Test loss: 0.5837 score: 0.7209 time: 0.27s
Epoch 69/1000, LR 0.000268
Train loss: 0.4412;  Loss pred: 0.4412; Loss self: 0.0000; time: 0.36s
Val loss: 0.5351 score: 0.7955 time: 0.27s
Test loss: 0.5775 score: 0.7209 time: 0.32s
Epoch 70/1000, LR 0.000268
Train loss: 0.4204;  Loss pred: 0.4204; Loss self: 0.0000; time: 0.39s
Val loss: 0.5273 score: 0.7955 time: 0.27s
Test loss: 0.5714 score: 0.7209 time: 0.26s
Epoch 71/1000, LR 0.000268
Train loss: 0.4041;  Loss pred: 0.4041; Loss self: 0.0000; time: 0.41s
Val loss: 0.5196 score: 0.7955 time: 0.24s
Test loss: 0.5654 score: 0.7209 time: 0.25s
Epoch 72/1000, LR 0.000267
Train loss: 0.3898;  Loss pred: 0.3898; Loss self: 0.0000; time: 0.41s
Val loss: 0.5120 score: 0.7955 time: 0.26s
Test loss: 0.5592 score: 0.7442 time: 0.28s
Epoch 73/1000, LR 0.000267
Train loss: 0.3779;  Loss pred: 0.3779; Loss self: 0.0000; time: 0.40s
Val loss: 0.5044 score: 0.7727 time: 0.27s
Test loss: 0.5531 score: 0.7442 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.3638;  Loss pred: 0.3638; Loss self: 0.0000; time: 0.43s
Val loss: 0.4968 score: 0.7727 time: 0.27s
Test loss: 0.5470 score: 0.7674 time: 0.34s
Epoch 75/1000, LR 0.000267
Train loss: 0.3523;  Loss pred: 0.3523; Loss self: 0.0000; time: 0.43s
Val loss: 0.4894 score: 0.7727 time: 0.28s
Test loss: 0.5410 score: 0.7674 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3347;  Loss pred: 0.3347; Loss self: 0.0000; time: 0.42s
Val loss: 0.4823 score: 0.7727 time: 0.24s
Test loss: 0.5351 score: 0.7674 time: 0.27s
Epoch 77/1000, LR 0.000267
Train loss: 0.3237;  Loss pred: 0.3237; Loss self: 0.0000; time: 0.43s
Val loss: 0.4755 score: 0.7727 time: 0.26s
Test loss: 0.5292 score: 0.7907 time: 0.26s
Epoch 78/1000, LR 0.000267
Train loss: 0.3105;  Loss pred: 0.3105; Loss self: 0.0000; time: 0.40s
Val loss: 0.4692 score: 0.7727 time: 0.27s
Test loss: 0.5233 score: 0.7907 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.3060;  Loss pred: 0.3060; Loss self: 0.0000; time: 0.42s
Val loss: 0.4631 score: 0.7727 time: 0.26s
Test loss: 0.5176 score: 0.7907 time: 0.34s
Epoch 80/1000, LR 0.000267
Train loss: 0.2932;  Loss pred: 0.2932; Loss self: 0.0000; time: 0.41s
Val loss: 0.4575 score: 0.7727 time: 0.26s
Test loss: 0.5122 score: 0.7907 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.2842;  Loss pred: 0.2842; Loss self: 0.0000; time: 0.42s
Val loss: 0.4521 score: 0.7955 time: 0.26s
Test loss: 0.5069 score: 0.7907 time: 0.29s
Epoch 82/1000, LR 0.000267
Train loss: 0.2615;  Loss pred: 0.2615; Loss self: 0.0000; time: 0.39s
Val loss: 0.4472 score: 0.7955 time: 0.28s
Test loss: 0.5020 score: 0.7907 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.41s
Val loss: 0.4428 score: 0.7955 time: 0.29s
Test loss: 0.4975 score: 0.8140 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.2527;  Loss pred: 0.2527; Loss self: 0.0000; time: 0.42s
Val loss: 0.4388 score: 0.7955 time: 0.24s
Test loss: 0.4931 score: 0.8140 time: 0.33s
Epoch 85/1000, LR 0.000266
Train loss: 0.2347;  Loss pred: 0.2347; Loss self: 0.0000; time: 0.50s
Val loss: 0.4352 score: 0.7727 time: 0.26s
Test loss: 0.4892 score: 0.8140 time: 0.27s
Epoch 86/1000, LR 0.000266
Train loss: 0.2332;  Loss pred: 0.2332; Loss self: 0.0000; time: 0.37s
Val loss: 0.4320 score: 0.7727 time: 0.26s
Test loss: 0.4855 score: 0.8372 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.2216;  Loss pred: 0.2216; Loss self: 0.0000; time: 0.41s
Val loss: 0.4292 score: 0.7727 time: 0.29s
Test loss: 0.4822 score: 0.8372 time: 0.25s
Epoch 88/1000, LR 0.000266
Train loss: 0.2052;  Loss pred: 0.2052; Loss self: 0.0000; time: 0.42s
Val loss: 0.4268 score: 0.7727 time: 0.25s
Test loss: 0.4794 score: 0.8372 time: 0.27s
Epoch 89/1000, LR 0.000266
Train loss: 0.2011;  Loss pred: 0.2011; Loss self: 0.0000; time: 0.40s
Val loss: 0.4248 score: 0.7727 time: 0.26s
Test loss: 0.4770 score: 0.8372 time: 0.39s
Epoch 90/1000, LR 0.000266
Train loss: 0.2013;  Loss pred: 0.2013; Loss self: 0.0000; time: 0.38s
Val loss: 0.4231 score: 0.7727 time: 0.27s
Test loss: 0.4750 score: 0.8372 time: 0.25s
Epoch 91/1000, LR 0.000266
Train loss: 0.1919;  Loss pred: 0.1919; Loss self: 0.0000; time: 0.40s
Val loss: 0.4220 score: 0.7727 time: 0.38s
Test loss: 0.4735 score: 0.8372 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 0.1741;  Loss pred: 0.1741; Loss self: 0.0000; time: 0.43s
Val loss: 0.4213 score: 0.7727 time: 0.27s
Test loss: 0.4725 score: 0.8372 time: 0.26s
Epoch 93/1000, LR 0.000265
Train loss: 0.1764;  Loss pred: 0.1764; Loss self: 0.0000; time: 0.38s
Val loss: 0.4213 score: 0.7727 time: 0.26s
Test loss: 0.4720 score: 0.8372 time: 0.25s
Epoch 94/1000, LR 0.000265
Train loss: 0.1802;  Loss pred: 0.1802; Loss self: 0.0000; time: 0.42s
Val loss: 0.4214 score: 0.7727 time: 0.27s
Test loss: 0.4721 score: 0.8372 time: 0.34s
     INFO: Early stopping counter 1 of 2
Epoch 95/1000, LR 0.000265
Train loss: 0.1601;  Loss pred: 0.1601; Loss self: 0.0000; time: 0.41s
Val loss: 0.4221 score: 0.7727 time: 0.31s
Test loss: 0.4727 score: 0.8372 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.1764,   Val_Loss: 0.4213,   Val_Precision: 0.8333,   Val_Recall: 0.6818,   Val_accuracy: 0.7500,   Val_Score: 0.7727,   Val_Loss: 0.4213,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.4720


[0.3056238789577037, 0.2838673379737884, 0.2822310660267249, 0.30379543802700937, 0.41317588998936117, 0.28435167705174536, 0.29632433189544827, 0.3093688919907436, 0.29447981994599104, 0.2964312139665708, 0.295494441059418, 0.2439169459976256, 0.2543146830284968, 0.2723409329773858, 0.3265857039950788, 0.24600696098059416, 0.2587404849473387, 0.26509431994054466, 0.25148736499249935, 0.3445048510329798, 0.2613286840496585, 0.27255050209350884, 0.2468020779779181, 0.26159683999139816, 0.34351012797560543, 0.2675867669750005, 0.24702442297711968, 0.2629165049875155, 0.2937401799717918, 0.35846180201042444, 0.2584102679975331, 0.2671553159598261, 0.2608540509827435, 0.2686891430057585, 0.32808945199940354, 0.25311378308106214, 0.2700384119525552, 0.26380912598688155, 0.24603012902662158, 0.3470591250807047, 0.2607282039243728, 0.2799398850183934, 0.26065535692032427, 0.2559288430493325, 0.35100598295684904, 0.25784095795825124, 0.2800107350340113, 0.2537385190371424, 0.2634434000356123, 0.337132660089992, 0.2784375329501927, 0.2612634270917624, 0.3109754410106689, 0.2611132540041581, 0.3377502840012312, 0.2736996409948915, 0.25951659004203975, 0.2614755609538406, 0.2571384279290214, 0.3528224390465766, 0.2599565179552883, 0.24496058200020343, 0.26033025002107024, 0.26844094798434526, 0.3520630559651181, 0.2509417369728908, 0.24838670704048127, 0.27425070002209395, 0.28361001098528504, 0.32839369401335716, 0.2525412510149181, 0.27489256591070443, 0.2471888659056276, 0.25498443411197513, 0.3528276700526476, 0.2713535811053589, 0.24865941202733666, 0.26746611890848726, 0.2721804219763726, 0.3250887560425326, 0.26067866501398385, 0.25410076801199466, 0.28718440397642553, 0.24712032999377698, 0.3397462060675025, 0.25062943098600954, 0.2694087950512767, 0.26636351901106536, 0.25445725000463426, 0.34228361304849386, 0.2676022859523073, 0.2934794760076329, 0.2538324400084093, 0.244169743033126, 0.3297032730188221, 0.27286389691289514, 0.24964067398104817, 0.2517943630227819, 0.27687897998839617, 0.39721588301472366, 0.2547724249307066, 0.25450286199338734, 0.263604830019176, 0.2583588380366564, 0.34488216298632324, 0.25549232098273933]
[0.006945997249038721, 0.00645153040849519, 0.006414342409698293, 0.006904441773341122, 0.009390361136121845, 0.006462538114812394, 0.006734643906714733, 0.007031111181607808, 0.006692723180590706, 0.006737073044694791, 0.00671578275135041, 0.005672487116223851, 0.0059142949541510885, 0.006333510069241531, 0.007595016371978577, 0.0057210921158277715, 0.006017220580170668, 0.0061649841846638295, 0.00584854337191859, 0.008011740721697204, 0.006077411256968802, 0.006338383769616485, 0.005739583208788793, 0.006083647441660422, 0.007988607627339661, 0.006222948069186058, 0.005744754022723714, 0.006114337325291058, 0.006831166976088181, 0.008336320976986614, 0.0060095411162217, 0.006212914324647119, 0.006066373278668454, 0.006248584721064152, 0.007629987255800082, 0.0058863670483967935, 0.006279963068664074, 0.006135095953183292, 0.005721630907595851, 0.008071142443737318, 0.006063446602892391, 0.006510229884148683, 0.006061752486519169, 0.005951833559286802, 0.008162929836205791, 0.005996301347866308, 0.006511877558930496, 0.005900895791561451, 0.006126590698502611, 0.007840294420697488, 0.0064752914639579695, 0.006075893653296801, 0.007231987000248114, 0.006072401255910653, 0.007854657767470493, 0.006365107930113756, 0.006035269535861389, 0.006080826998926526, 0.0059799634402098005, 0.008205173001083177, 0.006045500417564844, 0.005696757720934964, 0.006054191860955122, 0.006242812743821983, 0.008187512929421352, 0.005835854348206763, 0.005776435047453053, 0.006377923256327766, 0.006595581650820582, 0.007637062651473422, 0.005873052349184142, 0.006392850370016382, 0.005748578276875061, 0.005929870560743608, 0.008205294652387153, 0.006310548397799044, 0.00578277702389155, 0.006220142300197378, 0.006329777255264479, 0.007560203628896108, 0.006062294535208927, 0.005909320186325457, 0.006678707069219198, 0.00574698441845993, 0.00790107455970936, 0.0058285914182792916, 0.006265320815145969, 0.006194500442117799, 0.005917610465224052, 0.007960084024383578, 0.006223308975635053, 0.006825104093200765, 0.005903080000195566, 0.005678366117049442, 0.007667517977181909, 0.006345672021230119, 0.0058055970693267015, 0.005855682860994928, 0.006439046046241771, 0.009237578674761016, 0.005924940114667596, 0.005918671209148543, 0.006130344884166884, 0.006008345070619916, 0.008020515418286587, 0.005941681883319519]
[143.96780824213445, 155.00198196124575, 155.90062645985816, 144.83430128430078, 106.49217697850897, 154.73796552285862, 148.4859502375406, 142.22503017955825, 149.41601094455234, 148.43241172625625, 148.90297036468607, 176.2895145481962, 169.08186144793578, 157.89033080668253, 131.66528563249037, 174.7918019417019, 166.1896861975495, 162.20641773706822, 170.9827450030441, 124.81682005657322, 164.54374366278526, 157.76892601447938, 174.22867891674437, 164.3750742608891, 125.17825967289579, 160.695539940573, 174.07185687053632, 163.55002133487906, 146.38787245289132, 119.95699335001814, 166.40205643999602, 160.95506033825728, 164.84313675789767, 160.0362393469632, 131.06181786081368, 169.88407141079645, 159.23660522620372, 162.99663568931373, 174.77534223195568, 123.8981974325004, 164.92270246479603, 153.60440687890792, 164.96879445736468, 168.01545104359877, 122.50503435232388, 166.7694703762403, 153.5655409596244, 169.4657955881962, 163.22291617169859, 127.54623058033553, 154.43320282431858, 164.58484250417987, 138.2745848361857, 164.67949956808874, 127.31299435367241, 157.10652686169425, 165.69268266446596, 164.45131561488827, 167.22510262787094, 121.8743346262155, 165.41227870807174, 175.53844642630824, 165.1748115961158, 160.1842055873966, 122.1372117052247, 171.3545164654905, 173.11715474770554, 156.790848652477, 151.6166508037371, 130.94039497071162, 170.26921276104673, 156.42474672802913, 173.95605519067615, 168.63774508335973, 121.87252772319049, 158.46483331762008, 172.92729701811064, 160.7680261540428, 157.98344233492568, 132.27156953522606, 164.954044082178, 169.22420320260588, 149.72957933861127, 174.00429985296165, 126.56506307374788, 171.56803904007714, 159.6087462245462, 161.433518222192, 168.98712848314156, 125.62681460858563, 160.68622077340387, 146.51791186543363, 169.40309126199722, 176.10699616523036, 130.4203006730395, 157.58772225453723, 172.24757213748111, 170.774275816927, 155.30250798309825, 108.2534758520875, 168.77807718670968, 168.95684261938575, 163.12295945742707, 166.43518110999977, 124.68026652252591, 168.30251427754266]
Elapsed: 0.28151702647150123~0.03627456110261044
Time per graph: 0.006530128183594268~0.0008318362318398345
Speed: 155.30659040975226~17.161994583502103
Total Time: 0.2561
best val loss: 0.4212567210197449 test_score: 0.8372

Testing...
Test loss: 0.6188 score: 0.7442 time: 0.28s
test Score 0.7442
Epoch Time List: [0.9350183268543333, 0.8981378941098228, 0.9070376269519329, 0.9181678760796785, 1.020753141026944, 0.8785162250278518, 0.9238926719408482, 0.9424957711016759, 0.9129333830205724, 0.9253993699094281, 1.0298739928985015, 0.9251572380308062, 0.936839897884056, 0.9497729048598558, 0.977636608062312, 0.9299643639242277, 0.9278708699857816, 0.9245470749447122, 0.887074853060767, 1.0266611080151051, 0.9250256229424849, 0.9806968420743942, 0.8700348319252953, 0.932383761042729, 1.0059582451358438, 0.9531366070732474, 0.8706989840138704, 0.9347475208342075, 0.9656197390286252, 1.071874859975651, 0.914219562895596, 0.938054192927666, 0.9126541831064969, 0.9386926898732781, 0.9868791239568964, 0.9632263489766046, 0.9202684849733487, 0.9228078630985692, 0.9099867390468717, 1.0258247532183304, 0.961548883933574, 0.9646895489422604, 0.9718752920161933, 0.9388954639434814, 1.071505482075736, 0.9580093611730263, 0.9798052549595013, 0.8965207898290828, 0.9417390119051561, 1.0167380730854347, 0.9274875529808924, 0.9246014070231467, 0.961208677967079, 0.9634917408693582, 1.0001129308948293, 0.9567160991718993, 0.9571182900108397, 0.9238025491358712, 0.9246933860704303, 1.06713071197737, 0.9087700730888173, 0.9369114519795403, 0.9274511939147487, 0.9532601220998913, 1.0205006450414658, 0.8810566919855773, 0.9165030198637396, 0.9399656669702381, 0.9445228989934549, 1.0093519958900288, 0.9140707058832049, 0.9385717030381784, 0.8769739670678973, 0.9264765760162845, 1.0295451919082552, 0.945335305063054, 0.9135595959378406, 0.9449682231061161, 0.995217714109458, 0.9560541940154508, 0.9209999319864437, 0.9035032199462876, 0.9466070929775015, 0.9163020480191335, 1.0314956940710545, 0.9620502510806546, 0.9266888010315597, 0.9485620750347152, 0.9249660830246285, 1.0170771139673889, 0.9287647529272363, 0.9658152539050207, 0.9146734789246693, 0.9476227428531274, 0.9940357968444005, 1.0267834711121395, 0.8768507540225983, 0.9465763219632208, 0.9371477279346436, 1.047184825874865, 0.8972007799893618, 1.0290096431272104, 0.9588931389153004, 0.889156676013954, 1.0317400739295408, 0.9627639480168]
Total Epoch List: [11, 95]
Total Time List: [0.2965386339928955, 0.2560925220604986]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcce20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7021 score: 0.4884 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7020 score: 0.4884 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7019 score: 0.4884 time: 0.26s
Epoch 4/1000, LR 0.000060
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6992 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.4884 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.4884 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.4884 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.4884 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7007 score: 0.4884 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.4884 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6999 score: 0.4884 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.4884 time: 0.26s
Epoch 13/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.4884 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4884 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.4884 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4884 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.26s
Epoch 18/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4884 time: 0.33s
Epoch 20/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.4884 time: 0.32s
Epoch 21/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.34s
Epoch 25/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.23s
Epoch 31/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4884 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4884 time: 0.27s
Epoch 36/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.4884 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.4884 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6825 score: 0.4884 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.4884 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6791 score: 0.4884 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6789 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.4884 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.41s
Val loss: 0.6771 score: 0.5227 time: 0.28s
Test loss: 0.6752 score: 0.5349 time: 0.24s
Epoch 43/1000, LR 0.000269
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.44s
Val loss: 0.6751 score: 0.5227 time: 0.28s
Test loss: 0.6730 score: 0.5349 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.36s
Val loss: 0.6729 score: 0.5909 time: 0.29s
Test loss: 0.6705 score: 0.5349 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.41s
Val loss: 0.6705 score: 0.6364 time: 0.33s
Test loss: 0.6678 score: 0.6279 time: 0.39s
Epoch 46/1000, LR 0.000269
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.42s
Val loss: 0.6678 score: 0.6818 time: 0.30s
Test loss: 0.6648 score: 0.6279 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.40s
Val loss: 0.6649 score: 0.7955 time: 0.31s
Test loss: 0.6615 score: 0.6744 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.42s
Val loss: 0.6617 score: 0.8182 time: 0.27s
Test loss: 0.6579 score: 0.7442 time: 0.26s
Epoch 49/1000, LR 0.000269
Train loss: 0.6442;  Loss pred: 0.6442; Loss self: 0.0000; time: 0.36s
Val loss: 0.6583 score: 0.8182 time: 0.28s
Test loss: 0.6540 score: 0.7907 time: 0.30s
Epoch 50/1000, LR 0.000269
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.39s
Val loss: 0.6545 score: 0.8864 time: 0.30s
Test loss: 0.6497 score: 0.7674 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.39s
Val loss: 0.6504 score: 0.8864 time: 0.38s
Test loss: 0.6451 score: 0.7674 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.40s
Val loss: 0.6461 score: 0.8864 time: 0.26s
Test loss: 0.6401 score: 0.7907 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.6179;  Loss pred: 0.6179; Loss self: 0.0000; time: 0.39s
Val loss: 0.6415 score: 0.8864 time: 0.27s
Test loss: 0.6347 score: 0.8140 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6122;  Loss pred: 0.6122; Loss self: 0.0000; time: 0.43s
Val loss: 0.6364 score: 0.9091 time: 0.26s
Test loss: 0.6291 score: 0.8140 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 0.38s
Val loss: 0.6310 score: 0.8864 time: 0.28s
Test loss: 0.6231 score: 0.8605 time: 0.40s
Epoch 56/1000, LR 0.000269
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.49s
Val loss: 0.6252 score: 0.8864 time: 0.43s
Test loss: 0.6169 score: 0.8605 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.37s
Val loss: 0.6190 score: 0.9091 time: 0.24s
Test loss: 0.6105 score: 0.8605 time: 0.20s
Epoch 58/1000, LR 0.000269
Train loss: 0.5802;  Loss pred: 0.5802; Loss self: 0.0000; time: 0.49s
Val loss: 0.6123 score: 0.9091 time: 0.25s
Test loss: 0.6038 score: 0.8605 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5693;  Loss pred: 0.5693; Loss self: 0.0000; time: 0.47s
Val loss: 0.6052 score: 0.9091 time: 0.28s
Test loss: 0.5968 score: 0.8837 time: 0.45s
Epoch 60/1000, LR 0.000268
Train loss: 0.5589;  Loss pred: 0.5589; Loss self: 0.0000; time: 0.50s
Val loss: 0.5976 score: 0.9091 time: 0.28s
Test loss: 0.5894 score: 0.8837 time: 0.26s
Epoch 61/1000, LR 0.000268
Train loss: 0.5480;  Loss pred: 0.5480; Loss self: 0.0000; time: 0.35s
Val loss: 0.5895 score: 0.8864 time: 0.28s
Test loss: 0.5816 score: 0.8605 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.5469;  Loss pred: 0.5469; Loss self: 0.0000; time: 0.54s
Val loss: 0.5811 score: 0.9091 time: 0.30s
Test loss: 0.5735 score: 0.8605 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.41s
Val loss: 0.5724 score: 0.8864 time: 0.40s
Test loss: 0.5649 score: 0.8605 time: 0.22s
Epoch 64/1000, LR 0.000268
Train loss: 0.5120;  Loss pred: 0.5120; Loss self: 0.0000; time: 0.51s
Val loss: 0.5633 score: 0.8636 time: 0.28s
Test loss: 0.5559 score: 0.8605 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5025;  Loss pred: 0.5025; Loss self: 0.0000; time: 0.39s
Val loss: 0.5541 score: 0.8636 time: 0.27s
Test loss: 0.5465 score: 0.8605 time: 0.35s
Epoch 66/1000, LR 0.000268
Train loss: 0.4939;  Loss pred: 0.4939; Loss self: 0.0000; time: 0.42s
Val loss: 0.5446 score: 0.8636 time: 0.27s
Test loss: 0.5368 score: 0.8605 time: 0.26s
Epoch 67/1000, LR 0.000268
Train loss: 0.4872;  Loss pred: 0.4872; Loss self: 0.0000; time: 0.49s
Val loss: 0.5353 score: 0.8636 time: 0.28s
Test loss: 0.5267 score: 0.8605 time: 0.24s
Epoch 68/1000, LR 0.000268
Train loss: 0.4655;  Loss pred: 0.4655; Loss self: 0.0000; time: 0.34s
Val loss: 0.5253 score: 0.8636 time: 0.28s
Test loss: 0.5167 score: 0.8605 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.4558;  Loss pred: 0.4558; Loss self: 0.0000; time: 0.39s
Val loss: 0.5149 score: 0.8636 time: 0.28s
Test loss: 0.5067 score: 0.8605 time: 0.28s
Epoch 70/1000, LR 0.000268
Train loss: 0.4165;  Loss pred: 0.4165; Loss self: 0.0000; time: 0.41s
Val loss: 0.5039 score: 0.8636 time: 0.27s
Test loss: 0.4969 score: 0.8605 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.4125;  Loss pred: 0.4125; Loss self: 0.0000; time: 0.35s
Val loss: 0.4926 score: 0.8636 time: 0.28s
Test loss: 0.4871 score: 0.8605 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.3991;  Loss pred: 0.3991; Loss self: 0.0000; time: 0.47s
Val loss: 0.4814 score: 0.8636 time: 0.29s
Test loss: 0.4772 score: 0.8605 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.3916;  Loss pred: 0.3916; Loss self: 0.0000; time: 0.40s
Val loss: 0.4699 score: 0.8636 time: 0.28s
Test loss: 0.4676 score: 0.8605 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.3661;  Loss pred: 0.3661; Loss self: 0.0000; time: 0.41s
Val loss: 0.4579 score: 0.8636 time: 0.28s
Test loss: 0.4585 score: 0.8605 time: 0.27s
Epoch 75/1000, LR 0.000267
Train loss: 0.3512;  Loss pred: 0.3512; Loss self: 0.0000; time: 0.36s
Val loss: 0.4469 score: 0.8636 time: 0.28s
Test loss: 0.4492 score: 0.8605 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.3342;  Loss pred: 0.3342; Loss self: 0.0000; time: 0.40s
Val loss: 0.4360 score: 0.8636 time: 0.29s
Test loss: 0.4403 score: 0.8605 time: 0.31s
Epoch 77/1000, LR 0.000267
Train loss: 0.3141;  Loss pred: 0.3141; Loss self: 0.0000; time: 0.41s
Val loss: 0.4256 score: 0.8636 time: 0.28s
Test loss: 0.4318 score: 0.8605 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.3126;  Loss pred: 0.3126; Loss self: 0.0000; time: 0.38s
Val loss: 0.4147 score: 0.8636 time: 0.28s
Test loss: 0.4243 score: 0.8605 time: 0.26s
Epoch 79/1000, LR 0.000267
Train loss: 0.2888;  Loss pred: 0.2888; Loss self: 0.0000; time: 0.37s
Val loss: 0.4051 score: 0.8636 time: 0.29s
Test loss: 0.4172 score: 0.8605 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.2718;  Loss pred: 0.2718; Loss self: 0.0000; time: 0.40s
Val loss: 0.3953 score: 0.8864 time: 0.28s
Test loss: 0.4109 score: 0.8605 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.2440;  Loss pred: 0.2440; Loss self: 0.0000; time: 0.39s
Val loss: 0.3865 score: 0.8864 time: 0.27s
Test loss: 0.4053 score: 0.8605 time: 0.25s
Epoch 82/1000, LR 0.000267
Train loss: 0.2472;  Loss pred: 0.2472; Loss self: 0.0000; time: 0.37s
Val loss: 0.3768 score: 0.9091 time: 0.36s
Test loss: 0.4008 score: 0.8605 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.2333;  Loss pred: 0.2333; Loss self: 0.0000; time: 0.40s
Val loss: 0.3702 score: 0.8864 time: 0.29s
Test loss: 0.3968 score: 0.8605 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2161;  Loss pred: 0.2161; Loss self: 0.0000; time: 0.41s
Val loss: 0.3616 score: 0.9091 time: 0.27s
Test loss: 0.3942 score: 0.8605 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.2244;  Loss pred: 0.2244; Loss self: 0.0000; time: 0.38s
Val loss: 0.3498 score: 0.9318 time: 0.28s
Test loss: 0.3926 score: 0.8372 time: 0.25s
Epoch 86/1000, LR 0.000266
Train loss: 0.1892;  Loss pred: 0.1892; Loss self: 0.0000; time: 0.40s
Val loss: 0.3363 score: 0.9318 time: 0.29s
Test loss: 0.3922 score: 0.8372 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.1780;  Loss pred: 0.1780; Loss self: 0.0000; time: 0.49s
Val loss: 0.3263 score: 0.9318 time: 0.37s
Test loss: 0.3924 score: 0.8372 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.1565;  Loss pred: 0.1565; Loss self: 0.0000; time: 0.39s
Val loss: 0.3140 score: 0.9091 time: 0.27s
Test loss: 0.3941 score: 0.8372 time: 0.26s
Epoch 89/1000, LR 0.000266
Train loss: 0.1710;  Loss pred: 0.1710; Loss self: 0.0000; time: 0.43s
Val loss: 0.3043 score: 0.9091 time: 0.28s
Test loss: 0.3962 score: 0.8372 time: 0.26s
Epoch 90/1000, LR 0.000266
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.35s
Val loss: 0.2984 score: 0.9091 time: 0.39s
Test loss: 0.3980 score: 0.8372 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.1277;  Loss pred: 0.1277; Loss self: 0.0000; time: 0.44s
Val loss: 0.2928 score: 0.9091 time: 0.29s
Test loss: 0.4007 score: 0.8372 time: 0.22s
Epoch 92/1000, LR 0.000266
Train loss: 0.1513;  Loss pred: 0.1513; Loss self: 0.0000; time: 0.39s
Val loss: 0.2892 score: 0.9091 time: 0.30s
Test loss: 0.4037 score: 0.8372 time: 0.24s
Epoch 93/1000, LR 0.000265
Train loss: 0.1336;  Loss pred: 0.1336; Loss self: 0.0000; time: 0.49s
Val loss: 0.2912 score: 0.9318 time: 0.26s
Test loss: 0.4069 score: 0.8372 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 94/1000, LR 0.000265
Train loss: 0.1004;  Loss pred: 0.1004; Loss self: 0.0000; time: 0.36s
Val loss: 0.2969 score: 0.9318 time: 0.29s
Test loss: 0.4109 score: 0.8372 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 091,   Train_Loss: 0.1513,   Val_Loss: 0.2892,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2892,   Test_Precision: 0.8500,   Test_Recall: 0.8095,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.4037


[0.3056238789577037, 0.2838673379737884, 0.2822310660267249, 0.30379543802700937, 0.41317588998936117, 0.28435167705174536, 0.29632433189544827, 0.3093688919907436, 0.29447981994599104, 0.2964312139665708, 0.295494441059418, 0.2439169459976256, 0.2543146830284968, 0.2723409329773858, 0.3265857039950788, 0.24600696098059416, 0.2587404849473387, 0.26509431994054466, 0.25148736499249935, 0.3445048510329798, 0.2613286840496585, 0.27255050209350884, 0.2468020779779181, 0.26159683999139816, 0.34351012797560543, 0.2675867669750005, 0.24702442297711968, 0.2629165049875155, 0.2937401799717918, 0.35846180201042444, 0.2584102679975331, 0.2671553159598261, 0.2608540509827435, 0.2686891430057585, 0.32808945199940354, 0.25311378308106214, 0.2700384119525552, 0.26380912598688155, 0.24603012902662158, 0.3470591250807047, 0.2607282039243728, 0.2799398850183934, 0.26065535692032427, 0.2559288430493325, 0.35100598295684904, 0.25784095795825124, 0.2800107350340113, 0.2537385190371424, 0.2634434000356123, 0.337132660089992, 0.2784375329501927, 0.2612634270917624, 0.3109754410106689, 0.2611132540041581, 0.3377502840012312, 0.2736996409948915, 0.25951659004203975, 0.2614755609538406, 0.2571384279290214, 0.3528224390465766, 0.2599565179552883, 0.24496058200020343, 0.26033025002107024, 0.26844094798434526, 0.3520630559651181, 0.2509417369728908, 0.24838670704048127, 0.27425070002209395, 0.28361001098528504, 0.32839369401335716, 0.2525412510149181, 0.27489256591070443, 0.2471888659056276, 0.25498443411197513, 0.3528276700526476, 0.2713535811053589, 0.24865941202733666, 0.26746611890848726, 0.2721804219763726, 0.3250887560425326, 0.26067866501398385, 0.25410076801199466, 0.28718440397642553, 0.24712032999377698, 0.3397462060675025, 0.25062943098600954, 0.2694087950512767, 0.26636351901106536, 0.25445725000463426, 0.34228361304849386, 0.2676022859523073, 0.2934794760076329, 0.2538324400084093, 0.244169743033126, 0.3297032730188221, 0.27286389691289514, 0.24964067398104817, 0.2517943630227819, 0.27687897998839617, 0.39721588301472366, 0.2547724249307066, 0.25450286199338734, 0.263604830019176, 0.2583588380366564, 0.34488216298632324, 0.25549232098273933, 0.23744919791352004, 0.26721115701366216, 0.2633578049717471, 0.2586799029959366, 0.23516272997949272, 0.25033721805084497, 0.25539751001633704, 0.2579213110730052, 0.24421480402816087, 0.2572688760701567, 0.2561725500272587, 0.26124645909294486, 0.2548202029429376, 0.23736962606199086, 0.23781115794554353, 0.25147647003177553, 0.2619929569773376, 0.2351533561013639, 0.33032534399535507, 0.32114776596426964, 0.24954665102995932, 0.25256914901547134, 0.2406438400503248, 0.3430075420765206, 0.24357527599204332, 0.254086147993803, 0.264062513015233, 0.2349643810885027, 0.26228435593657196, 0.23700496996752918, 0.26006142992991954, 0.25724274502135813, 0.2377124719787389, 0.24351735995151103, 0.27585444296710193, 0.2604667330160737, 0.23173078300897032, 0.24889617902226746, 0.26426745508797467, 0.24548378598410636, 0.24452256597578526, 0.24041327298618853, 0.2560524110449478, 0.23661214590538293, 0.3918163889320567, 0.24967999407090247, 0.27366625401191413, 0.26053392805624753, 0.3027934549609199, 0.23740015702787787, 0.23880383395589888, 0.240356000023894, 0.23235853400547057, 0.23168089508544654, 0.4066599829820916, 0.2236653290456161, 0.2068991439882666, 0.2582023619906977, 0.45852192596066743, 0.2682562550762668, 0.24243461003061384, 0.25879429397173226, 0.22712634201161563, 0.24405416392255574, 0.35036096000112593, 0.26182982709724456, 0.24909373803529888, 0.22766813496127725, 0.281302549992688, 0.2523434970062226, 0.24282865005079657, 0.23140910698566586, 0.24249552807305008, 0.2716166509781033, 0.23825114301871508, 0.3101275049848482, 0.25012217299081385, 0.26126763701904565, 0.23973608296364546, 0.24415268399752676, 0.2557501670671627, 0.23564938397612423, 0.23184728901833296, 0.2563847879646346, 0.24925692903343588, 0.23567884310614318, 0.2280231440672651, 0.2662333300104365, 0.2611445130314678, 0.24552263994701207, 0.22823305404745042, 0.246691485051997, 0.2589580190833658, 0.25577718298882246]
[0.006945997249038721, 0.00645153040849519, 0.006414342409698293, 0.006904441773341122, 0.009390361136121845, 0.006462538114812394, 0.006734643906714733, 0.007031111181607808, 0.006692723180590706, 0.006737073044694791, 0.00671578275135041, 0.005672487116223851, 0.0059142949541510885, 0.006333510069241531, 0.007595016371978577, 0.0057210921158277715, 0.006017220580170668, 0.0061649841846638295, 0.00584854337191859, 0.008011740721697204, 0.006077411256968802, 0.006338383769616485, 0.005739583208788793, 0.006083647441660422, 0.007988607627339661, 0.006222948069186058, 0.005744754022723714, 0.006114337325291058, 0.006831166976088181, 0.008336320976986614, 0.0060095411162217, 0.006212914324647119, 0.006066373278668454, 0.006248584721064152, 0.007629987255800082, 0.0058863670483967935, 0.006279963068664074, 0.006135095953183292, 0.005721630907595851, 0.008071142443737318, 0.006063446602892391, 0.006510229884148683, 0.006061752486519169, 0.005951833559286802, 0.008162929836205791, 0.005996301347866308, 0.006511877558930496, 0.005900895791561451, 0.006126590698502611, 0.007840294420697488, 0.0064752914639579695, 0.006075893653296801, 0.007231987000248114, 0.006072401255910653, 0.007854657767470493, 0.006365107930113756, 0.006035269535861389, 0.006080826998926526, 0.0059799634402098005, 0.008205173001083177, 0.006045500417564844, 0.005696757720934964, 0.006054191860955122, 0.006242812743821983, 0.008187512929421352, 0.005835854348206763, 0.005776435047453053, 0.006377923256327766, 0.006595581650820582, 0.007637062651473422, 0.005873052349184142, 0.006392850370016382, 0.005748578276875061, 0.005929870560743608, 0.008205294652387153, 0.006310548397799044, 0.00578277702389155, 0.006220142300197378, 0.006329777255264479, 0.007560203628896108, 0.006062294535208927, 0.005909320186325457, 0.006678707069219198, 0.00574698441845993, 0.00790107455970936, 0.0058285914182792916, 0.006265320815145969, 0.006194500442117799, 0.005917610465224052, 0.007960084024383578, 0.006223308975635053, 0.006825104093200765, 0.005903080000195566, 0.005678366117049442, 0.007667517977181909, 0.006345672021230119, 0.0058055970693267015, 0.005855682860994928, 0.006439046046241771, 0.009237578674761016, 0.005924940114667596, 0.005918671209148543, 0.006130344884166884, 0.006008345070619916, 0.008020515418286587, 0.005941681883319519, 0.005522074370081861, 0.006214212953806097, 0.006124600115622026, 0.00601581169757992, 0.005468900697197505, 0.005821795768624302, 0.005939476977124117, 0.005998170024953609, 0.005679414047166532, 0.005982997117910621, 0.005957501163424621, 0.0060754990486731365, 0.005926051231231107, 0.005520223861906764, 0.005530492045245198, 0.005848290000738966, 0.006092859464589246, 0.005468682700031718, 0.007681984744078025, 0.00746855269684348, 0.005803410489068821, 0.005873701139894682, 0.005596368373263367, 0.007976919583174898, 0.005664541302140542, 0.005908980185902396, 0.00614098867477286, 0.00546428793229076, 0.006099636184571441, 0.005511743487616958, 0.006047940230928361, 0.005982389419101352, 0.0055281970227613695, 0.005663194417477001, 0.006415219603886091, 0.006057365884094737, 0.005389087976952798, 0.005788283233075987, 0.006145754769487783, 0.005708925255444334, 0.005686571301762448, 0.005591006348516012, 0.005954707233603437, 0.005502608044311231, 0.009112009044931552, 0.005806511490020988, 0.006364331488649166, 0.0060589285594476175, 0.007041708254905114, 0.005520933884369253, 0.005553577533858114, 0.005589674419160326, 0.005403686837336525, 0.005387927792684803, 0.00945720890656027, 0.005201519280130607, 0.00481160799972713, 0.006004706092806923, 0.010663300603736452, 0.006238517559913182, 0.005638014186758461, 0.006018471952830983, 0.005282007953758503, 0.0056756782307571104, 0.008147929302351765, 0.006089065746447548, 0.005792877628727881, 0.005294607789797145, 0.006541919767271814, 0.005868453418749363, 0.00564717790815806, 0.0053816071392015314, 0.00563943088541977, 0.006316666301816355, 0.005540724256249188, 0.007212267557787168, 0.005816794720716601, 0.006075991558582457, 0.005575257743340592, 0.00567796939529132, 0.005947678303887504, 0.005480218232002889, 0.005391797419030999, 0.005962436929410107, 0.005796672768219439, 0.005480903328049841, 0.005302863815517793, 0.0061914727909403834, 0.006073128210034135, 0.005709828835977025, 0.005307745442963963, 0.005737011280279, 0.006022279513566647, 0.005948306581135406]
[143.96780824213445, 155.00198196124575, 155.90062645985816, 144.83430128430078, 106.49217697850897, 154.73796552285862, 148.4859502375406, 142.22503017955825, 149.41601094455234, 148.43241172625625, 148.90297036468607, 176.2895145481962, 169.08186144793578, 157.89033080668253, 131.66528563249037, 174.7918019417019, 166.1896861975495, 162.20641773706822, 170.9827450030441, 124.81682005657322, 164.54374366278526, 157.76892601447938, 174.22867891674437, 164.3750742608891, 125.17825967289579, 160.695539940573, 174.07185687053632, 163.55002133487906, 146.38787245289132, 119.95699335001814, 166.40205643999602, 160.95506033825728, 164.84313675789767, 160.0362393469632, 131.06181786081368, 169.88407141079645, 159.23660522620372, 162.99663568931373, 174.77534223195568, 123.8981974325004, 164.92270246479603, 153.60440687890792, 164.96879445736468, 168.01545104359877, 122.50503435232388, 166.7694703762403, 153.5655409596244, 169.4657955881962, 163.22291617169859, 127.54623058033553, 154.43320282431858, 164.58484250417987, 138.2745848361857, 164.67949956808874, 127.31299435367241, 157.10652686169425, 165.69268266446596, 164.45131561488827, 167.22510262787094, 121.8743346262155, 165.41227870807174, 175.53844642630824, 165.1748115961158, 160.1842055873966, 122.1372117052247, 171.3545164654905, 173.11715474770554, 156.790848652477, 151.6166508037371, 130.94039497071162, 170.26921276104673, 156.42474672802913, 173.95605519067615, 168.63774508335973, 121.87252772319049, 158.46483331762008, 172.92729701811064, 160.7680261540428, 157.98344233492568, 132.27156953522606, 164.954044082178, 169.22420320260588, 149.72957933861127, 174.00429985296165, 126.56506307374788, 171.56803904007714, 159.6087462245462, 161.433518222192, 168.98712848314156, 125.62681460858563, 160.68622077340387, 146.51791186543363, 169.40309126199722, 176.10699616523036, 130.4203006730395, 157.58772225453723, 172.24757213748111, 170.774275816927, 155.30250798309825, 108.2534758520875, 168.77807718670968, 168.95684261938575, 163.12295945742707, 166.43518110999977, 124.68026652252591, 168.30251427754266, 181.0913676602975, 160.92142439172727, 163.2759659605039, 166.2286072554908, 182.852104173777, 171.76830650593251, 168.36499305435441, 166.71751481531805, 176.07450199882882, 167.14031116719298, 167.85561136595032, 164.5955323157191, 168.74643181109573, 181.15207372307285, 180.81573787991306, 170.9901526555017, 164.1265494160574, 182.85939317602023, 130.17469225917057, 133.89475050803907, 172.31247072451248, 170.2504053547965, 178.68730814388434, 125.36167496400778, 176.53680089191255, 169.23394029748027, 162.84022866024705, 183.00646166366568, 163.94420416899993, 181.43079449300666, 165.34554936342346, 167.15728949490813, 180.89080325514405, 176.57878686169283, 155.87930916569695, 165.08826099241787, 185.56015494210567, 172.76279679710555, 162.71394442302892, 175.16431819567913, 175.85289042099384, 178.85867725144402, 167.9343686884937, 181.73200634085347, 109.74528175608411, 172.22044625565454, 157.12569368574023, 165.0456826134237, 142.0109955994584, 181.1287765700615, 180.06411072923154, 178.90129639253993, 185.0588366984086, 185.60011167144842, 105.73944277643277, 192.25152232347213, 207.8307293646346, 166.5360443199554, 93.7795938763648, 160.29449150318916, 177.3674146384054, 166.1551317074125, 189.32194134399833, 176.19039687290456, 122.7305690675748, 164.22880646073085, 172.6257767022088, 188.87140269899265, 152.8603277898395, 170.40264762178379, 177.07959909592614, 185.81809748163258, 177.32285762831285, 158.31135479049294, 180.4818203815386, 138.65264869718936, 171.9159860392675, 164.58219047185483, 179.3639049592743, 176.11930082421532, 168.1328324947203, 182.47448507803745, 185.46690876596725, 167.71665878215586, 172.51275688400978, 182.45167632902033, 188.57734891733327, 161.51245975969417, 164.6597874136399, 175.13659843866182, 188.4039109911755, 174.30678643381862, 166.05008082857285, 168.1150738214171]
Elapsed: 0.27081680150469767~0.03862373084897867
Time per graph: 0.006289172064625937~0.0008899514738862336
Speed: 161.63260323715542~18.891917999982557
Total Time: 0.2564
best val loss: 0.2892407178878784 test_score: 0.8372

Testing...
Test loss: 0.3926 score: 0.8372 time: 0.25s
test Score 0.8372
Epoch Time List: [0.9350183268543333, 0.8981378941098228, 0.9070376269519329, 0.9181678760796785, 1.020753141026944, 0.8785162250278518, 0.9238926719408482, 0.9424957711016759, 0.9129333830205724, 0.9253993699094281, 1.0298739928985015, 0.9251572380308062, 0.936839897884056, 0.9497729048598558, 0.977636608062312, 0.9299643639242277, 0.9278708699857816, 0.9245470749447122, 0.887074853060767, 1.0266611080151051, 0.9250256229424849, 0.9806968420743942, 0.8700348319252953, 0.932383761042729, 1.0059582451358438, 0.9531366070732474, 0.8706989840138704, 0.9347475208342075, 0.9656197390286252, 1.071874859975651, 0.914219562895596, 0.938054192927666, 0.9126541831064969, 0.9386926898732781, 0.9868791239568964, 0.9632263489766046, 0.9202684849733487, 0.9228078630985692, 0.9099867390468717, 1.0258247532183304, 0.961548883933574, 0.9646895489422604, 0.9718752920161933, 0.9388954639434814, 1.071505482075736, 0.9580093611730263, 0.9798052549595013, 0.8965207898290828, 0.9417390119051561, 1.0167380730854347, 0.9274875529808924, 0.9246014070231467, 0.961208677967079, 0.9634917408693582, 1.0001129308948293, 0.9567160991718993, 0.9571182900108397, 0.9238025491358712, 0.9246933860704303, 1.06713071197737, 0.9087700730888173, 0.9369114519795403, 0.9274511939147487, 0.9532601220998913, 1.0205006450414658, 0.8810566919855773, 0.9165030198637396, 0.9399656669702381, 0.9445228989934549, 1.0093519958900288, 0.9140707058832049, 0.9385717030381784, 0.8769739670678973, 0.9264765760162845, 1.0295451919082552, 0.945335305063054, 0.9135595959378406, 0.9449682231061161, 0.995217714109458, 0.9560541940154508, 0.9209999319864437, 0.9035032199462876, 0.9466070929775015, 0.9163020480191335, 1.0314956940710545, 0.9620502510806546, 0.9266888010315597, 0.9485620750347152, 0.9249660830246285, 1.0170771139673889, 0.9287647529272363, 0.9658152539050207, 0.9146734789246693, 0.9476227428531274, 0.9940357968444005, 1.0267834711121395, 0.8768507540225983, 0.9465763219632208, 0.9371477279346436, 1.047184825874865, 0.8972007799893618, 1.0290096431272104, 0.9588931389153004, 0.889156676013954, 1.0317400739295408, 0.9627639480168, 0.9684838020475581, 0.9931800699559972, 0.9323967221425846, 1.036545523093082, 0.8903118090238422, 0.9348771069198847, 0.9345206769648939, 1.0170599819393829, 0.9752900409512222, 0.9515814499463886, 0.9463810300221667, 1.0686685040127486, 0.9291746070375666, 1.0241779739735648, 0.9920748369768262, 0.9081868469947949, 0.9453747951192781, 0.8898299919674173, 1.05998208001256, 1.0179212229559198, 0.9963516960851848, 0.9646850401768461, 0.8730646540643647, 1.0472976970486343, 0.9390346580184996, 0.9305461881449446, 0.9263939769007266, 0.9170036530122161, 1.0399204900022596, 0.9896773230284452, 0.9421374439261854, 0.9400206711143255, 0.9190604529576376, 0.9390156451845542, 1.032920798053965, 0.962009762879461, 0.8656583150150254, 0.9021098210942, 0.9474542620591819, 0.9614420110592619, 0.9336060929344967, 0.9189012569840997, 0.9664806020446122, 0.8895241582067683, 1.1285497959470376, 0.9638519189320505, 0.9880370970349759, 0.9450055439956486, 0.9336474719457328, 0.9197871120413765, 0.999492947012186, 0.8950933249434456, 0.8877181279240176, 0.9177418999606743, 1.0610143130179495, 1.1408133240183815, 0.8114075870253146, 0.9932292388984933, 1.2012921510031447, 1.0455230820225552, 0.8717454409925267, 1.0914321929449216, 1.0343645471148193, 1.0362297201063484, 1.0061425009043887, 0.9507172028534114, 1.0113223700318485, 0.8441968290135264, 0.9413796960143372, 0.9288422239478678, 0.8687516159843653, 0.9864379219943658, 0.9182876369450241, 0.9536795989843085, 0.8766824610065669, 1.0041741500608623, 0.9322841251268983, 0.9173284939024597, 0.8871883281972259, 0.9127706089057028, 0.9198775049299002, 0.9592238100012764, 0.9200091100065038, 0.9300057899672538, 0.9075260268291458, 0.9239887479925528, 1.0757988670375198, 0.9259004100458696, 0.9653475331142545, 0.9794291870202869, 0.9547985790995881, 0.9326778911054134, 1.0112304139183834, 0.8974650261225179]
Total Epoch List: [11, 95, 94]
Total Time List: [0.2965386339928955, 0.2560925220604986, 0.25641881907358766]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcdab0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.36s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.29s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.6929,   Val_Loss: 0.6935,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6935,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6932


[0.36508442903868854, 0.2925925279268995, 0.3022386860102415]
[0.008297373387242922, 0.006649830180156807, 0.006869061045687307]
[120.52006741524781, 150.3797800707776, 145.58030469504172]
Elapsed: 0.3199718809919432~0.03214154720290274
Time per graph: 0.007272088204362345~0.0007304897091568806
Speed: 138.82671739368905~13.092206741555465
Total Time: 0.3025
best val loss: 0.6934980154037476 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.29s
test Score 0.5000
Epoch Time List: [0.9876890230225399, 0.9427361860871315, 0.9241607239237055]
Total Epoch List: [3]
Total Time List: [0.30250520596746355]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcdcf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.27s
Epoch 2/1000, LR 0.000000
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.35s
Epoch 4/1000, LR 0.000060
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.26s
Epoch 5/1000, LR 0.000090
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.27s
Epoch 7/1000, LR 0.000150
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.26s
Epoch 9/1000, LR 0.000210
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.26s
Epoch 11/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.33s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.26s
Epoch 15/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.39s
Val loss: 0.6920 score: 0.5455 time: 0.27s
Test loss: 0.6925 score: 0.5814 time: 0.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.40s
Val loss: 0.6919 score: 0.6136 time: 0.27s
Test loss: 0.6924 score: 0.5814 time: 0.26s
Epoch 17/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.44s
Val loss: 0.6917 score: 0.7045 time: 0.26s
Test loss: 0.6923 score: 0.6047 time: 0.26s
Epoch 18/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.38s
Val loss: 0.6914 score: 0.6818 time: 0.37s
Test loss: 0.6923 score: 0.4884 time: 0.30s
Epoch 19/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.39s
Val loss: 0.6912 score: 0.6591 time: 0.27s
Test loss: 0.6921 score: 0.5814 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.47s
Val loss: 0.6909 score: 0.6591 time: 0.27s
Test loss: 0.6920 score: 0.6047 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.49s
Val loss: 0.6906 score: 0.6364 time: 0.26s
Test loss: 0.6918 score: 0.6279 time: 0.32s
Epoch 22/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.40s
Val loss: 0.6902 score: 0.6364 time: 0.26s
Test loss: 0.6917 score: 0.6279 time: 0.26s
Epoch 23/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.40s
Val loss: 0.6898 score: 0.6591 time: 0.30s
Test loss: 0.6914 score: 0.6047 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.49s
Val loss: 0.6894 score: 0.6591 time: 0.27s
Test loss: 0.6912 score: 0.6047 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.44s
Val loss: 0.6889 score: 0.6818 time: 0.25s
Test loss: 0.6909 score: 0.5814 time: 0.28s
Epoch 26/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.42s
Val loss: 0.6884 score: 0.6818 time: 0.26s
Test loss: 0.6906 score: 0.5116 time: 0.27s
Epoch 27/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.38s
Val loss: 0.6877 score: 0.7045 time: 0.28s
Test loss: 0.6902 score: 0.4884 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.43s
Val loss: 0.6871 score: 0.7273 time: 0.27s
Test loss: 0.6898 score: 0.5349 time: 0.26s
Epoch 29/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.43s
Val loss: 0.6863 score: 0.7500 time: 0.33s
Test loss: 0.6893 score: 0.5581 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.43s
Val loss: 0.6855 score: 0.7045 time: 0.24s
Test loss: 0.6888 score: 0.6047 time: 0.28s
Epoch 31/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.40s
Val loss: 0.6846 score: 0.7500 time: 0.26s
Test loss: 0.6882 score: 0.6512 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.40s
Val loss: 0.6836 score: 0.7500 time: 0.27s
Test loss: 0.6875 score: 0.6279 time: 0.26s
Epoch 33/1000, LR 0.000270
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.43s
Val loss: 0.6825 score: 0.7045 time: 0.26s
Test loss: 0.6868 score: 0.6279 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.44s
Val loss: 0.6813 score: 0.7045 time: 0.26s
Test loss: 0.6860 score: 0.6977 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.46s
Val loss: 0.6800 score: 0.6818 time: 0.26s
Test loss: 0.6851 score: 0.6977 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.38s
Val loss: 0.6786 score: 0.6591 time: 0.28s
Test loss: 0.6842 score: 0.6977 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.43s
Val loss: 0.6770 score: 0.6364 time: 0.26s
Test loss: 0.6831 score: 0.6744 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.42s
Val loss: 0.6754 score: 0.6364 time: 0.24s
Test loss: 0.6820 score: 0.6512 time: 0.27s
Epoch 39/1000, LR 0.000269
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.36s
Val loss: 0.6735 score: 0.6364 time: 0.29s
Test loss: 0.6807 score: 0.6279 time: 0.27s
Epoch 40/1000, LR 0.000269
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.40s
Val loss: 0.6716 score: 0.6136 time: 0.27s
Test loss: 0.6793 score: 0.6512 time: 0.26s
Epoch 41/1000, LR 0.000269
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.49s
Val loss: 0.6695 score: 0.6136 time: 0.27s
Test loss: 0.6779 score: 0.6279 time: 0.27s
Epoch 42/1000, LR 0.000269
Train loss: 0.6550;  Loss pred: 0.6550; Loss self: 0.0000; time: 0.41s
Val loss: 0.6673 score: 0.6136 time: 0.25s
Test loss: 0.6763 score: 0.6512 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6524;  Loss pred: 0.6524; Loss self: 0.0000; time: 0.43s
Val loss: 0.6649 score: 0.6136 time: 0.26s
Test loss: 0.6748 score: 0.6512 time: 0.27s
Epoch 44/1000, LR 0.000269
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 0.37s
Val loss: 0.6623 score: 0.6364 time: 0.27s
Test loss: 0.6730 score: 0.6512 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 0.41s
Val loss: 0.6594 score: 0.6591 time: 0.26s
Test loss: 0.6711 score: 0.6512 time: 0.26s
Epoch 46/1000, LR 0.000269
Train loss: 0.6423;  Loss pred: 0.6423; Loss self: 0.0000; time: 0.43s
Val loss: 0.6563 score: 0.6591 time: 0.25s
Test loss: 0.6690 score: 0.6512 time: 0.34s
Epoch 47/1000, LR 0.000269
Train loss: 0.6330;  Loss pred: 0.6330; Loss self: 0.0000; time: 0.42s
Val loss: 0.6530 score: 0.6591 time: 0.26s
Test loss: 0.6667 score: 0.6744 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.38s
Val loss: 0.6493 score: 0.6591 time: 0.27s
Test loss: 0.6641 score: 0.6512 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.41s
Val loss: 0.6453 score: 0.7045 time: 0.26s
Test loss: 0.6614 score: 0.6744 time: 0.26s
Epoch 50/1000, LR 0.000269
Train loss: 0.6161;  Loss pred: 0.6161; Loss self: 0.0000; time: 0.41s
Val loss: 0.6410 score: 0.7045 time: 0.24s
Test loss: 0.6583 score: 0.6977 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.36s
Val loss: 0.6364 score: 0.7045 time: 0.27s
Test loss: 0.6551 score: 0.7442 time: 0.32s
Epoch 52/1000, LR 0.000269
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.44s
Val loss: 0.6313 score: 0.7500 time: 0.26s
Test loss: 0.6515 score: 0.7442 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.42s
Val loss: 0.6260 score: 0.7727 time: 0.26s
Test loss: 0.6477 score: 0.7907 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.43s
Val loss: 0.6202 score: 0.8182 time: 0.26s
Test loss: 0.6436 score: 0.7209 time: 0.26s
Epoch 55/1000, LR 0.000269
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.37s
Val loss: 0.6141 score: 0.8182 time: 0.26s
Test loss: 0.6392 score: 0.6744 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.41s
Val loss: 0.6077 score: 0.8182 time: 0.26s
Test loss: 0.6345 score: 0.6977 time: 0.27s
Epoch 57/1000, LR 0.000269
Train loss: 0.5509;  Loss pred: 0.5509; Loss self: 0.0000; time: 0.47s
Val loss: 0.6009 score: 0.8182 time: 0.27s
Test loss: 0.6296 score: 0.6977 time: 0.26s
Epoch 58/1000, LR 0.000269
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.42s
Val loss: 0.5938 score: 0.8182 time: 0.27s
Test loss: 0.6244 score: 0.6977 time: 0.26s
Epoch 59/1000, LR 0.000268
Train loss: 0.5181;  Loss pred: 0.5181; Loss self: 0.0000; time: 0.37s
Val loss: 0.5863 score: 0.8182 time: 0.26s
Test loss: 0.6188 score: 0.6977 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5088;  Loss pred: 0.5088; Loss self: 0.0000; time: 0.39s
Val loss: 0.5786 score: 0.8182 time: 0.24s
Test loss: 0.6129 score: 0.6977 time: 0.26s
Epoch 61/1000, LR 0.000268
Train loss: 0.5019;  Loss pred: 0.5019; Loss self: 0.0000; time: 0.36s
Val loss: 0.5706 score: 0.8182 time: 0.25s
Test loss: 0.6069 score: 0.6977 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.4875;  Loss pred: 0.4875; Loss self: 0.0000; time: 0.41s
Val loss: 0.5623 score: 0.8182 time: 0.35s
Test loss: 0.6008 score: 0.7209 time: 0.26s
Epoch 63/1000, LR 0.000268
Train loss: 0.4744;  Loss pred: 0.4744; Loss self: 0.0000; time: 0.42s
Val loss: 0.5539 score: 0.8182 time: 0.24s
Test loss: 0.5943 score: 0.7209 time: 0.26s
Epoch 64/1000, LR 0.000268
Train loss: 0.4610;  Loss pred: 0.4610; Loss self: 0.0000; time: 0.40s
Val loss: 0.5452 score: 0.8182 time: 0.25s
Test loss: 0.5878 score: 0.7442 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.4480;  Loss pred: 0.4480; Loss self: 0.0000; time: 0.40s
Val loss: 0.5365 score: 0.8182 time: 0.28s
Test loss: 0.5811 score: 0.7209 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.4306;  Loss pred: 0.4306; Loss self: 0.0000; time: 0.40s
Val loss: 0.5278 score: 0.8182 time: 0.24s
Test loss: 0.5744 score: 0.7209 time: 0.27s
Epoch 67/1000, LR 0.000268
Train loss: 0.4168;  Loss pred: 0.4168; Loss self: 0.0000; time: 0.41s
Val loss: 0.5192 score: 0.8182 time: 0.34s
Test loss: 0.5679 score: 0.7209 time: 0.27s
Epoch 68/1000, LR 0.000268
Train loss: 0.4067;  Loss pred: 0.4067; Loss self: 0.0000; time: 0.44s
Val loss: 0.5108 score: 0.8182 time: 0.26s
Test loss: 0.5616 score: 0.7209 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.3833;  Loss pred: 0.3833; Loss self: 0.0000; time: 0.40s
Val loss: 0.5025 score: 0.8182 time: 0.27s
Test loss: 0.5551 score: 0.7442 time: 0.28s
Epoch 70/1000, LR 0.000268
Train loss: 0.3742;  Loss pred: 0.3742; Loss self: 0.0000; time: 0.40s
Val loss: 0.4946 score: 0.8182 time: 0.25s
Test loss: 0.5488 score: 0.7674 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.3502;  Loss pred: 0.3502; Loss self: 0.0000; time: 0.41s
Val loss: 0.4867 score: 0.8182 time: 0.31s
Test loss: 0.5424 score: 0.7907 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.3399;  Loss pred: 0.3399; Loss self: 0.0000; time: 0.39s
Val loss: 0.4794 score: 0.8182 time: 0.34s
Test loss: 0.5363 score: 0.7907 time: 0.26s
Epoch 73/1000, LR 0.000267
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.40s
Val loss: 0.4720 score: 0.7955 time: 0.27s
Test loss: 0.5300 score: 0.8140 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.3082;  Loss pred: 0.3082; Loss self: 0.0000; time: 0.41s
Val loss: 0.4648 score: 0.7955 time: 0.25s
Test loss: 0.5236 score: 0.8140 time: 0.26s
Epoch 75/1000, LR 0.000267
Train loss: 0.3019;  Loss pred: 0.3019; Loss self: 0.0000; time: 0.41s
Val loss: 0.4579 score: 0.7955 time: 0.25s
Test loss: 0.5173 score: 0.8140 time: 0.26s
Epoch 76/1000, LR 0.000267
Train loss: 0.2823;  Loss pred: 0.2823; Loss self: 0.0000; time: 0.40s
Val loss: 0.4514 score: 0.8182 time: 0.27s
Test loss: 0.5113 score: 0.8140 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.2726;  Loss pred: 0.2726; Loss self: 0.0000; time: 0.42s
Val loss: 0.4456 score: 0.7955 time: 0.35s
Test loss: 0.5057 score: 0.8140 time: 0.26s
Epoch 78/1000, LR 0.000267
Train loss: 0.2583;  Loss pred: 0.2583; Loss self: 0.0000; time: 0.41s
Val loss: 0.4404 score: 0.7955 time: 0.24s
Test loss: 0.5009 score: 0.8140 time: 0.26s
Epoch 79/1000, LR 0.000267
Train loss: 0.2431;  Loss pred: 0.2431; Loss self: 0.0000; time: 0.42s
Val loss: 0.4358 score: 0.7955 time: 0.27s
Test loss: 0.4967 score: 0.8140 time: 0.26s
Epoch 80/1000, LR 0.000267
Train loss: 0.2228;  Loss pred: 0.2228; Loss self: 0.0000; time: 0.38s
Val loss: 0.4320 score: 0.7955 time: 0.28s
Test loss: 0.4934 score: 0.8140 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.2118;  Loss pred: 0.2118; Loss self: 0.0000; time: 0.42s
Val loss: 0.4289 score: 0.7955 time: 0.29s
Test loss: 0.4909 score: 0.8140 time: 0.35s
Epoch 82/1000, LR 0.000267
Train loss: 0.2041;  Loss pred: 0.2041; Loss self: 0.0000; time: 0.45s
Val loss: 0.4264 score: 0.7955 time: 0.32s
Test loss: 0.4893 score: 0.8140 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.1967;  Loss pred: 0.1967; Loss self: 0.0000; time: 0.46s
Val loss: 0.4247 score: 0.7955 time: 0.24s
Test loss: 0.4886 score: 0.8140 time: 0.26s
Epoch 84/1000, LR 0.000266
Train loss: 0.1803;  Loss pred: 0.1803; Loss self: 0.0000; time: 0.37s
Val loss: 0.4240 score: 0.8182 time: 0.27s
Test loss: 0.4892 score: 0.8140 time: 0.26s
Epoch 85/1000, LR 0.000266
Train loss: 0.1656;  Loss pred: 0.1656; Loss self: 0.0000; time: 0.44s
Val loss: 0.4242 score: 0.8182 time: 0.27s
Test loss: 0.4910 score: 0.8140 time: 0.26s
     INFO: Early stopping counter 1 of 2
Epoch 86/1000, LR 0.000266
Train loss: 0.1567;  Loss pred: 0.1567; Loss self: 0.0000; time: 0.43s
Val loss: 0.4251 score: 0.8182 time: 0.31s
Test loss: 0.4931 score: 0.8140 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 083,   Train_Loss: 0.1803,   Val_Loss: 0.4240,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4240,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4892


[0.36508442903868854, 0.2925925279268995, 0.3022386860102415, 0.27548548800405115, 0.2619688770500943, 0.3591921749757603, 0.2606349040288478, 0.25767510489095, 0.2694889970589429, 0.2572508290177211, 0.26878434303216636, 0.254717368981801, 0.26838228898122907, 0.24700007203500718, 0.3361826069885865, 0.26118398702237755, 0.2644460799638182, 0.2587827609386295, 0.25995374692138284, 0.2681648519355804, 0.30831863288767636, 0.24548811500426382, 0.24946720292791724, 0.3199422990437597, 0.2635758720571175, 0.2570958170108497, 0.24200212606228888, 0.2841548250289634, 0.27107982302550226, 0.24709771806374192, 0.26122553704772145, 0.25588878302369267, 0.27970135293435305, 0.2565303159644827, 0.2661976789822802, 0.2574459459865466, 0.2691000560298562, 0.266081849928014, 0.2459800059441477, 0.257404841016978, 0.27437521098181605, 0.27849126991350204, 0.2623132959706709, 0.2757224109955132, 0.25845045503228903, 0.27065484202466905, 0.24336556601338089, 0.26679509901441634, 0.3421194429975003, 0.2677278839983046, 0.24293711001519114, 0.26038938807323575, 0.2647635330213234, 0.32287059305235744, 0.2581490589072928, 0.25599896104540676, 0.26818814501166344, 0.24420143093448132, 0.27184281300287694, 0.2593666319735348, 0.26379124401137233, 0.24412011203821748, 0.26828115701209754, 0.24421318201348186, 0.2601393039803952, 0.2633289749501273, 0.24059036595281214, 0.24159502401016653, 0.27206365193706006, 0.2708210020791739, 0.2584312130929902, 0.2864642620552331, 0.25901431404054165, 0.26231817808002234, 0.26038044900633395, 0.2580159400822595, 0.2632403071038425, 0.2609126219758764, 0.2466688989661634, 0.26888264005538076, 0.2647885759361088, 0.26621732907369733, 0.2424905279185623, 0.3550117339473218, 0.2540857680141926, 0.2679031790466979, 0.26261299406178296, 0.2626605830155313, 0.2570108079817146]
[0.008297373387242922, 0.006649830180156807, 0.006869061045687307, 0.006406639255908167, 0.0060922994662812625, 0.008353306394785123, 0.006061276837880182, 0.005992444299789536, 0.006267185978114952, 0.00598257741901677, 0.00625079867516666, 0.005923659743762813, 0.006241448580958816, 0.005744187721744353, 0.007818200162525267, 0.0060740462098227335, 0.006149908836367865, 0.0060182037427588255, 0.00604543597491588, 0.006236391905478613, 0.007170200764829683, 0.0057090259303317166, 0.005801562858788773, 0.007440518582413016, 0.00612967144318878, 0.005978972488624412, 0.00562795642005323, 0.006608251744859614, 0.006304181930825634, 0.0057464585596219055, 0.006075012489481894, 0.005950901930783551, 0.006504682626380304, 0.005965821301499598, 0.00619064369726233, 0.005987115022942944, 0.0062581408379036325, 0.006187949998325906, 0.005720465254515063, 0.005986159093418094, 0.0063808188600422335, 0.006476541160779117, 0.006100309208620253, 0.006412149092918912, 0.006010475698425327, 0.00629429865173649, 0.0056596643258925786, 0.006204537186381775, 0.007956266116220937, 0.006226229860425689, 0.005649700232911422, 0.006055567164493855, 0.006157291465612172, 0.00750861844307808, 0.0060034664862161115, 0.005953464210358297, 0.006236933604922406, 0.0056791030449879374, 0.006321925883787835, 0.0060317821389194145, 0.006134680093287728, 0.005677211907865523, 0.006239096674699943, 0.005679376325894927, 0.006049751255358028, 0.0061239296500029605, 0.005595124789600282, 0.005618488930468989, 0.006327061672954885, 0.006298162839050556, 0.006010028211464889, 0.00666195958267984, 0.006023588698617247, 0.006100422746047031, 0.0060553592792170685, 0.006000370699587429, 0.0061218676070661044, 0.006067735394787823, 0.005736486022468916, 0.006253084652450716, 0.006157873858979275, 0.006191100676132496, 0.005639314602757263, 0.008256086835984227, 0.00590897134916727, 0.006230306489458091, 0.006107278931669371, 0.006108385651523984, 0.005976995534458479]
[120.52006741524781, 150.3797800707776, 145.58030469504172, 156.088076767832, 164.14163577063943, 119.71307560612036, 164.98174010968475, 166.8768118604159, 159.56124542849145, 167.152036649172, 159.97955652816444, 168.81455776607154, 160.21921626507725, 174.08901805464103, 127.90667662785977, 164.63490158880174, 162.604036353586, 166.16253665443145, 165.41404195648843, 160.34912737307434, 139.46610880201115, 175.161229289056, 172.36734727179635, 134.3992342635468, 163.1408810844484, 167.25281842366715, 177.6843893881008, 151.32595406612253, 158.62486377658107, 174.02022299901458, 164.60871508188202, 168.04175428048615, 153.73540223844486, 167.6215141993333, 161.53409062166298, 167.02535297350173, 159.7918656517456, 161.60440861198634, 174.81095601633407, 167.05202524595794, 156.7196972573801, 154.4034037884045, 163.92611682485133, 155.95395327041345, 166.3761822149931, 158.87393581556813, 176.68892401004564, 161.17237595011625, 125.6870981177009, 160.61083872859615, 177.0005414047741, 165.13729809874604, 162.40907314277638, 133.18029243074182, 166.57043098283106, 167.96943168989287, 160.33520049191563, 176.0841442879866, 158.17964626324303, 165.78848124298278, 163.00768496374437, 176.1427997102847, 160.27961292138386, 176.0756714501438, 165.2960523152649, 163.29384188786634, 178.7270235435518, 177.98379820186415, 158.05124901413782, 158.77645998602813, 166.38857003905133, 150.10598422119816, 166.01399100000904, 163.92306592981328, 165.14296739289358, 166.6563700920607, 163.3488445332859, 164.80613193169214, 174.32274672737924, 159.9210718518066, 162.393712976407, 161.52216743221945, 177.32651402549243, 121.1227570477446, 169.23419338307107, 160.50574746074483, 163.7390417546655, 163.70937544693328, 167.30813905327784]
Elapsed: 0.2686265888336125~0.025262747207070054
Time per graph: 0.006241429354998282~0.0005758769874233279
Speed: 161.37786775432895~12.647335625496687
Total Time: 0.2574
best val loss: 0.4239825904369354 test_score: 0.8140

Testing...
Test loss: 0.6436 score: 0.7209 time: 0.26s
test Score 0.7209
Epoch Time List: [0.9876890230225399, 0.9427361860871315, 0.9241607239237055, 1.0485212790081277, 0.9345165340928361, 1.0338250518543646, 0.9248575259698555, 0.9505719820735976, 0.9846465800656006, 0.9635142821352929, 0.9468143949052319, 0.9795520129846409, 0.9314190612640232, 0.899150945013389, 0.9983168421313167, 0.910841649863869, 0.9326360550476238, 0.9128264971077442, 0.9248333519790322, 0.9564010619651526, 1.051969944150187, 0.898959552985616, 0.9792096510063857, 1.0709754910785705, 0.9177988269366324, 0.9520828210515901, 0.9918222889536992, 0.9669483619509265, 0.9467859879368916, 0.9084730789763853, 0.9549811020260677, 1.0149300061166286, 0.95285953884013, 0.9099836382083595, 0.9346934258937836, 0.94292862189468, 0.9627565030241385, 0.9845615150406957, 0.9016779160592705, 0.9414955178508535, 0.930300027015619, 0.9284626670414582, 0.9291213650722057, 1.0360266879433766, 0.9205585699528456, 0.9635913230013102, 0.8865819118218496, 0.933074138010852, 1.0107256579212844, 0.9447386170504615, 0.882229067152366, 0.9216562618967146, 0.914163765963167, 0.9469911659834906, 0.9596214781049639, 0.9324727579951286, 0.9546660430496559, 0.8703738040057942, 0.9410316660068929, 0.9958716729888692, 0.9496491459431127, 0.8715645869961008, 0.8980492189293727, 0.8574596439721063, 1.0187560060294345, 0.9171640380518511, 0.8829554211115465, 0.9231211440637708, 0.9115048691164702, 1.0114098698832095, 0.9449019669555128, 0.9557733669644222, 0.8995827049948275, 0.9748504399321973, 0.9878833689726889, 0.9294287587981671, 0.9214652669616044, 0.918198129045777, 0.9161929730325937, 1.0356971708824858, 0.9069172539748251, 0.9519460431765765, 0.8978090630844235, 1.0557590160751715, 1.018333266954869, 0.9643144168658182, 0.9049255999270827, 0.9706956209847704, 0.997415081015788]
Total Epoch List: [3, 86]
Total Time List: [0.30250520596746355, 0.2574160760268569]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcdc30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.31s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.27s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.22s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.23s
Epoch 20/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.28s
Epoch 23/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.43s
Epoch 27/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.27s
Epoch 31/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4884 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4884 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4884 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.4884 time: 0.23s
Epoch 36/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.4884 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.4884 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6839 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.4884 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6829 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6830 score: 0.4884 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6817 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.4884 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6804 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.4884 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6790 score: 0.5000 time: 0.27s
Test loss: 0.6789 score: 0.5116 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6774 score: 0.5000 time: 0.28s
Test loss: 0.6771 score: 0.5116 time: 0.29s
Epoch 44/1000, LR 0.000269
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6758 score: 0.5000 time: 0.28s
Test loss: 0.6753 score: 0.5116 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.42s
Val loss: 0.6739 score: 0.5227 time: 0.26s
Test loss: 0.6732 score: 0.5116 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.35s
Val loss: 0.6718 score: 0.5227 time: 0.29s
Test loss: 0.6710 score: 0.5116 time: 0.22s
Epoch 47/1000, LR 0.000269
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.42s
Val loss: 0.6696 score: 0.5455 time: 0.27s
Test loss: 0.6685 score: 0.5349 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.57s
Val loss: 0.6671 score: 0.5682 time: 0.27s
Test loss: 0.6658 score: 0.5349 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.35s
Val loss: 0.6644 score: 0.5682 time: 0.27s
Test loss: 0.6628 score: 0.5349 time: 0.26s
Epoch 50/1000, LR 0.000269
Train loss: 0.6532;  Loss pred: 0.6532; Loss self: 0.0000; time: 0.40s
Val loss: 0.6614 score: 0.5909 time: 0.30s
Test loss: 0.6596 score: 0.5349 time: 0.24s
Epoch 51/1000, LR 0.000269
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 0.41s
Val loss: 0.6581 score: 0.5909 time: 0.27s
Test loss: 0.6560 score: 0.5581 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6426;  Loss pred: 0.6426; Loss self: 0.0000; time: 0.40s
Val loss: 0.6547 score: 0.6364 time: 0.28s
Test loss: 0.6521 score: 0.5349 time: 0.31s
Epoch 53/1000, LR 0.000269
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.44s
Val loss: 0.6509 score: 0.6591 time: 0.28s
Test loss: 0.6479 score: 0.5814 time: 0.35s
Epoch 54/1000, LR 0.000269
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.39s
Val loss: 0.6468 score: 0.6591 time: 0.29s
Test loss: 0.6433 score: 0.5814 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.41s
Val loss: 0.6423 score: 0.7045 time: 0.27s
Test loss: 0.6383 score: 0.6047 time: 0.27s
Epoch 56/1000, LR 0.000269
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.40s
Val loss: 0.6375 score: 0.7045 time: 0.28s
Test loss: 0.6330 score: 0.6744 time: 0.26s
Epoch 57/1000, LR 0.000269
Train loss: 0.6151;  Loss pred: 0.6151; Loss self: 0.0000; time: 0.40s
Val loss: 0.6324 score: 0.7045 time: 0.31s
Test loss: 0.6273 score: 0.7209 time: 0.31s
Epoch 58/1000, LR 0.000269
Train loss: 0.6050;  Loss pred: 0.6050; Loss self: 0.0000; time: 0.38s
Val loss: 0.6269 score: 0.7500 time: 0.30s
Test loss: 0.6215 score: 0.6977 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.6012;  Loss pred: 0.6012; Loss self: 0.0000; time: 0.51s
Val loss: 0.6211 score: 0.7955 time: 0.30s
Test loss: 0.6154 score: 0.7209 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.42s
Val loss: 0.6148 score: 0.8182 time: 0.32s
Test loss: 0.6089 score: 0.7442 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 0.43s
Val loss: 0.6081 score: 0.8409 time: 0.29s
Test loss: 0.6021 score: 0.7442 time: 0.26s
Epoch 62/1000, LR 0.000268
Train loss: 0.5723;  Loss pred: 0.5723; Loss self: 0.0000; time: 0.51s
Val loss: 0.6010 score: 0.8182 time: 0.27s
Test loss: 0.5954 score: 0.7442 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 0.36s
Val loss: 0.5936 score: 0.8409 time: 0.39s
Test loss: 0.5882 score: 0.7442 time: 0.26s
Epoch 64/1000, LR 0.000268
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.39s
Val loss: 0.5858 score: 0.8409 time: 0.30s
Test loss: 0.5807 score: 0.7442 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5326;  Loss pred: 0.5326; Loss self: 0.0000; time: 0.42s
Val loss: 0.5776 score: 0.8409 time: 0.30s
Test loss: 0.5731 score: 0.7674 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5304;  Loss pred: 0.5304; Loss self: 0.0000; time: 0.42s
Val loss: 0.5692 score: 0.8409 time: 0.27s
Test loss: 0.5654 score: 0.7674 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.5199;  Loss pred: 0.5199; Loss self: 0.0000; time: 0.42s
Val loss: 0.5604 score: 0.8409 time: 0.29s
Test loss: 0.5575 score: 0.7674 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.4996;  Loss pred: 0.4996; Loss self: 0.0000; time: 0.38s
Val loss: 0.5514 score: 0.8409 time: 0.38s
Test loss: 0.5499 score: 0.7674 time: 0.26s
Epoch 69/1000, LR 0.000268
Train loss: 0.4990;  Loss pred: 0.4990; Loss self: 0.0000; time: 0.38s
Val loss: 0.5422 score: 0.8409 time: 0.29s
Test loss: 0.5417 score: 0.7674 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.4907;  Loss pred: 0.4907; Loss self: 0.0000; time: 0.43s
Val loss: 0.5329 score: 0.8409 time: 0.29s
Test loss: 0.5329 score: 0.7907 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.4710;  Loss pred: 0.4710; Loss self: 0.0000; time: 0.43s
Val loss: 0.5233 score: 0.8636 time: 0.27s
Test loss: 0.5238 score: 0.8140 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.4442;  Loss pred: 0.4442; Loss self: 0.0000; time: 0.39s
Val loss: 0.5137 score: 0.8864 time: 0.29s
Test loss: 0.5148 score: 0.8140 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.4421;  Loss pred: 0.4421; Loss self: 0.0000; time: 0.38s
Val loss: 0.5041 score: 0.9091 time: 0.29s
Test loss: 0.5060 score: 0.7907 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.4274;  Loss pred: 0.4274; Loss self: 0.0000; time: 0.47s
Val loss: 0.4944 score: 0.9091 time: 0.29s
Test loss: 0.4975 score: 0.7907 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.4187;  Loss pred: 0.4187; Loss self: 0.0000; time: 0.41s
Val loss: 0.4847 score: 0.9318 time: 0.27s
Test loss: 0.4897 score: 0.7907 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 0.37s
Val loss: 0.4749 score: 0.9318 time: 0.29s
Test loss: 0.4823 score: 0.7907 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.3845;  Loss pred: 0.3845; Loss self: 0.0000; time: 0.40s
Val loss: 0.4650 score: 0.9318 time: 0.30s
Test loss: 0.4753 score: 0.7907 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.3931;  Loss pred: 0.3931; Loss self: 0.0000; time: 0.41s
Val loss: 0.4555 score: 0.9318 time: 0.29s
Test loss: 0.4683 score: 0.8140 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.3804;  Loss pred: 0.3804; Loss self: 0.0000; time: 0.51s
Val loss: 0.4460 score: 0.9318 time: 0.28s
Test loss: 0.4619 score: 0.8140 time: 0.27s
Epoch 80/1000, LR 0.000267
Train loss: 0.3717;  Loss pred: 0.3717; Loss self: 0.0000; time: 0.43s
Val loss: 0.4373 score: 0.9318 time: 0.28s
Test loss: 0.4548 score: 0.8140 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.3411;  Loss pred: 0.3411; Loss self: 0.0000; time: 0.35s
Val loss: 0.4291 score: 0.9318 time: 0.29s
Test loss: 0.4472 score: 0.8140 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.3253;  Loss pred: 0.3253; Loss self: 0.0000; time: 0.42s
Val loss: 0.4212 score: 0.9318 time: 0.29s
Test loss: 0.4399 score: 0.8372 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.3416;  Loss pred: 0.3416; Loss self: 0.0000; time: 0.40s
Val loss: 0.4125 score: 0.9318 time: 0.27s
Test loss: 0.4344 score: 0.8605 time: 0.26s
Epoch 84/1000, LR 0.000266
Train loss: 0.3309;  Loss pred: 0.3309; Loss self: 0.0000; time: 0.57s
Val loss: 0.4054 score: 0.9318 time: 0.27s
Test loss: 0.4275 score: 0.8605 time: 0.26s
Epoch 85/1000, LR 0.000266
Train loss: 0.3020;  Loss pred: 0.3020; Loss self: 0.0000; time: 0.36s
Val loss: 0.3980 score: 0.9091 time: 0.32s
Test loss: 0.4214 score: 0.8605 time: 0.27s
Epoch 86/1000, LR 0.000266
Train loss: 0.2660;  Loss pred: 0.2660; Loss self: 0.0000; time: 0.39s
Val loss: 0.3888 score: 0.9091 time: 0.29s
Test loss: 0.4178 score: 0.8605 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.2792;  Loss pred: 0.2792; Loss self: 0.0000; time: 0.39s
Val loss: 0.3786 score: 0.9091 time: 0.30s
Test loss: 0.4163 score: 0.8605 time: 0.26s
Epoch 88/1000, LR 0.000266
Train loss: 0.2602;  Loss pred: 0.2602; Loss self: 0.0000; time: 0.41s
Val loss: 0.3703 score: 0.9091 time: 0.28s
Test loss: 0.4125 score: 0.8605 time: 0.33s
Epoch 89/1000, LR 0.000266
Train loss: 0.2427;  Loss pred: 0.2427; Loss self: 0.0000; time: 0.45s
Val loss: 0.3641 score: 0.9091 time: 0.28s
Test loss: 0.4064 score: 0.8605 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.2380;  Loss pred: 0.2380; Loss self: 0.0000; time: 0.37s
Val loss: 0.3586 score: 0.9318 time: 0.30s
Test loss: 0.4002 score: 0.8605 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.2428;  Loss pred: 0.2428; Loss self: 0.0000; time: 0.44s
Val loss: 0.3539 score: 0.9091 time: 0.30s
Test loss: 0.3942 score: 0.8605 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.2151;  Loss pred: 0.2151; Loss self: 0.0000; time: 0.41s
Val loss: 0.3472 score: 0.9318 time: 0.28s
Test loss: 0.3910 score: 0.8605 time: 0.26s
Epoch 93/1000, LR 0.000265
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.35s
Val loss: 0.3388 score: 0.9318 time: 0.29s
Test loss: 0.3902 score: 0.8605 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.2016;  Loss pred: 0.2016; Loss self: 0.0000; time: 0.43s
Val loss: 0.3308 score: 0.9318 time: 0.37s
Test loss: 0.3898 score: 0.8605 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.1647;  Loss pred: 0.1647; Loss self: 0.0000; time: 0.40s
Val loss: 0.3213 score: 0.9318 time: 0.29s
Test loss: 0.3919 score: 0.8605 time: 0.25s
Epoch 96/1000, LR 0.000265
Train loss: 0.1791;  Loss pred: 0.1791; Loss self: 0.0000; time: 0.42s
Val loss: 0.3100 score: 0.9091 time: 0.27s
Test loss: 0.3974 score: 0.8605 time: 0.25s
Epoch 97/1000, LR 0.000265
Train loss: 0.2060;  Loss pred: 0.2060; Loss self: 0.0000; time: 0.38s
Val loss: 0.3016 score: 0.9091 time: 0.28s
Test loss: 0.4010 score: 0.8605 time: 0.24s
Epoch 98/1000, LR 0.000265
Train loss: 0.1490;  Loss pred: 0.1490; Loss self: 0.0000; time: 0.40s
Val loss: 0.2940 score: 0.9091 time: 0.31s
Test loss: 0.4045 score: 0.8605 time: 0.23s
Epoch 99/1000, LR 0.000265
Train loss: 0.1451;  Loss pred: 0.1451; Loss self: 0.0000; time: 0.41s
Val loss: 0.2863 score: 0.9091 time: 0.38s
Test loss: 0.4092 score: 0.8605 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.1351;  Loss pred: 0.1351; Loss self: 0.0000; time: 0.41s
Val loss: 0.2784 score: 0.9091 time: 0.27s
Test loss: 0.4160 score: 0.8605 time: 0.26s
Epoch 101/1000, LR 0.000265
Train loss: 0.1365;  Loss pred: 0.1365; Loss self: 0.0000; time: 0.40s
Val loss: 0.2742 score: 0.9091 time: 0.37s
Test loss: 0.4161 score: 0.8605 time: 0.25s
Epoch 102/1000, LR 0.000264
Train loss: 0.1597;  Loss pred: 0.1597; Loss self: 0.0000; time: 0.41s
Val loss: 0.2800 score: 0.9318 time: 0.29s
Test loss: 0.4045 score: 0.8605 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 103/1000, LR 0.000264
Train loss: 0.1276;  Loss pred: 0.1276; Loss self: 0.0000; time: 0.38s
Val loss: 0.2865 score: 0.9318 time: 0.29s
Test loss: 0.3989 score: 0.8372 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 100,   Train_Loss: 0.1365,   Val_Loss: 0.2742,   Val_Precision: 0.9091,   Val_Recall: 0.9091,   Val_accuracy: 0.9091,   Val_Score: 0.9091,   Val_Loss: 0.2742,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.4161


[0.36508442903868854, 0.2925925279268995, 0.3022386860102415, 0.27548548800405115, 0.2619688770500943, 0.3591921749757603, 0.2606349040288478, 0.25767510489095, 0.2694889970589429, 0.2572508290177211, 0.26878434303216636, 0.254717368981801, 0.26838228898122907, 0.24700007203500718, 0.3361826069885865, 0.26118398702237755, 0.2644460799638182, 0.2587827609386295, 0.25995374692138284, 0.2681648519355804, 0.30831863288767636, 0.24548811500426382, 0.24946720292791724, 0.3199422990437597, 0.2635758720571175, 0.2570958170108497, 0.24200212606228888, 0.2841548250289634, 0.27107982302550226, 0.24709771806374192, 0.26122553704772145, 0.25588878302369267, 0.27970135293435305, 0.2565303159644827, 0.2661976789822802, 0.2574459459865466, 0.2691000560298562, 0.266081849928014, 0.2459800059441477, 0.257404841016978, 0.27437521098181605, 0.27849126991350204, 0.2623132959706709, 0.2757224109955132, 0.25845045503228903, 0.27065484202466905, 0.24336556601338089, 0.26679509901441634, 0.3421194429975003, 0.2677278839983046, 0.24293711001519114, 0.26038938807323575, 0.2647635330213234, 0.32287059305235744, 0.2581490589072928, 0.25599896104540676, 0.26818814501166344, 0.24420143093448132, 0.27184281300287694, 0.2593666319735348, 0.26379124401137233, 0.24412011203821748, 0.26828115701209754, 0.24421318201348186, 0.2601393039803952, 0.2633289749501273, 0.24059036595281214, 0.24159502401016653, 0.27206365193706006, 0.2708210020791739, 0.2584312130929902, 0.2864642620552331, 0.25901431404054165, 0.26231817808002234, 0.26038044900633395, 0.2580159400822595, 0.2632403071038425, 0.2609126219758764, 0.2466688989661634, 0.26888264005538076, 0.2647885759361088, 0.26621732907369733, 0.2424905279185623, 0.3550117339473218, 0.2540857680141926, 0.2679031790466979, 0.26261299406178296, 0.2626605830155313, 0.2570108079817146, 0.3171239800285548, 0.25108166597783566, 0.2425996910315007, 0.2622282700613141, 0.2529773829737678, 0.2328026369214058, 0.2561069799121469, 0.27201383793726563, 0.24837981001473963, 0.235340929008089, 0.2445822930894792, 0.22259291599038988, 0.24785879999399185, 0.24767463398166, 0.23301991494372487, 0.25034902803599834, 0.2499590020161122, 0.2564033039379865, 0.23645257798489183, 0.23707245802506804, 0.24565439694561064, 0.28484737291000783, 0.241851294063963, 0.24626064393669367, 0.2590254129609093, 0.4320003860630095, 0.2410804679384455, 0.248130923951976, 0.2593224860029295, 0.2789312290260568, 0.23420209100004286, 0.22923415200784802, 0.26006100699305534, 0.26076672005001456, 0.23884615395218134, 0.24785194406285882, 0.2456893780035898, 0.2609991949284449, 0.24086451099719852, 0.24723677802830935, 0.2577936650486663, 0.25892874493729323, 0.290262992028147, 0.2248354870826006, 0.25194665999151766, 0.2240619290387258, 0.25782058492768556, 0.2540178910130635, 0.26808123697992414, 0.2413604500470683, 0.2541260889265686, 0.3131756139919162, 0.34985690598841757, 0.23276911000721157, 0.27010332501959056, 0.26462410495150834, 0.3137900660512969, 0.24102507799398154, 0.23919358500279486, 0.24752065201755613, 0.2600541429128498, 0.25837478996254504, 0.262988336966373, 0.23996871896088123, 0.2362685379339382, 0.2524989160010591, 0.26370503392536193, 0.26348391093779355, 0.2548269029939547, 0.2518986059585586, 0.26690413895994425, 0.24377924401778728, 0.2288688599364832, 0.22972126002423465, 0.25754427001811564, 0.25220812403131276, 0.23704381100833416, 0.23976077500265092, 0.27132537798024714, 0.2608268950134516, 0.23632001702208072, 0.2432562840403989, 0.25990509893745184, 0.260854245047085, 0.2703157670330256, 0.23390880494844168, 0.2669825019547716, 0.33062126801814884, 0.25741734507028013, 0.24619044899009168, 0.24243313900660723, 0.2677943770540878, 0.23510041099507362, 0.23349409003276378, 0.24971538200043142, 0.2553478400222957, 0.24619316903408617, 0.23501097597181797, 0.22665102500468493, 0.2625901900464669, 0.25871016399469227, 0.24404830194544047, 0.2587588479509577]
[0.008297373387242922, 0.006649830180156807, 0.006869061045687307, 0.006406639255908167, 0.0060922994662812625, 0.008353306394785123, 0.006061276837880182, 0.005992444299789536, 0.006267185978114952, 0.00598257741901677, 0.00625079867516666, 0.005923659743762813, 0.006241448580958816, 0.005744187721744353, 0.007818200162525267, 0.0060740462098227335, 0.006149908836367865, 0.0060182037427588255, 0.00604543597491588, 0.006236391905478613, 0.007170200764829683, 0.0057090259303317166, 0.005801562858788773, 0.007440518582413016, 0.00612967144318878, 0.005978972488624412, 0.00562795642005323, 0.006608251744859614, 0.006304181930825634, 0.0057464585596219055, 0.006075012489481894, 0.005950901930783551, 0.006504682626380304, 0.005965821301499598, 0.00619064369726233, 0.005987115022942944, 0.0062581408379036325, 0.006187949998325906, 0.005720465254515063, 0.005986159093418094, 0.0063808188600422335, 0.006476541160779117, 0.006100309208620253, 0.006412149092918912, 0.006010475698425327, 0.00629429865173649, 0.0056596643258925786, 0.006204537186381775, 0.007956266116220937, 0.006226229860425689, 0.005649700232911422, 0.006055567164493855, 0.006157291465612172, 0.00750861844307808, 0.0060034664862161115, 0.005953464210358297, 0.006236933604922406, 0.0056791030449879374, 0.006321925883787835, 0.0060317821389194145, 0.006134680093287728, 0.005677211907865523, 0.006239096674699943, 0.005679376325894927, 0.006049751255358028, 0.0061239296500029605, 0.005595124789600282, 0.005618488930468989, 0.006327061672954885, 0.006298162839050556, 0.006010028211464889, 0.00666195958267984, 0.006023588698617247, 0.006100422746047031, 0.0060553592792170685, 0.006000370699587429, 0.0061218676070661044, 0.006067735394787823, 0.005736486022468916, 0.006253084652450716, 0.006157873858979275, 0.006191100676132496, 0.005639314602757263, 0.008256086835984227, 0.00590897134916727, 0.006230306489458091, 0.006107278931669371, 0.006108385651523984, 0.005976995534458479, 0.0073749762797338326, 0.005839108511112457, 0.0056418532798023415, 0.006098331861891026, 0.005883194952878321, 0.0054140148121257165, 0.005955976277026672, 0.0063259032078433865, 0.005776274651505573, 0.0054730448606532325, 0.005687960304406493, 0.005176579441636974, 0.00576415813939516, 0.0057598752088758145, 0.0054190677893889505, 0.005822070419441822, 0.00581300004688633, 0.005962867533441546, 0.005498897162439345, 0.005513312977327164, 0.005712892952223503, 0.006624357509535066, 0.00562444869916193, 0.005726991719457992, 0.006023846813044403, 0.0100465206061165, 0.005606522510196407, 0.005770486603534326, 0.006030755488440221, 0.006486772768047833, 0.00544656025581495, 0.005331026790880187, 0.006047930395187333, 0.006064342326744524, 0.005554561719818171, 0.005763998699136252, 0.005713706465199763, 0.006069748719266161, 0.005601500255748803, 0.005749692512286264, 0.005995201512759681, 0.0060215987194719356, 0.006750302140189465, 0.0052287322577348975, 0.005859224650965527, 0.0052107425357843205, 0.005995827556457804, 0.00590739281425729, 0.006234447371626143, 0.005613033722024844, 0.0059099090448039215, 0.007283153813765492, 0.00813620711600971, 0.005413235116446781, 0.006281472674874199, 0.006154048952360659, 0.007297443396541788, 0.005605234371953059, 0.0055626415116929034, 0.005756294232966422, 0.0060477707654151116, 0.006008716045640582, 0.0061160078364272796, 0.005580667882811191, 0.005494617161254377, 0.005872067813978119, 0.006132675207566557, 0.006127532812506827, 0.00592620704637104, 0.005858107115315317, 0.006207072999068471, 0.005669284744599704, 0.005322531626429842, 0.005342354884284527, 0.005989401628328271, 0.00586530521003053, 0.0055126467676356785, 0.005575831976805835, 0.006309892511168538, 0.0060657417444988735, 0.005495814349350714, 0.005657122884660439, 0.006044304626452369, 0.006066377791792674, 0.006286413186814549, 0.00543973964996376, 0.006208895394297013, 0.007688866698096485, 0.005986449885355352, 0.0057253592788393416, 0.0056379799768978426, 0.006227776210560182, 0.0054674514184900845, 0.005430095117041018, 0.005807334465126312, 0.005938321860983621, 0.0057254225356764225, 0.005465371534228325, 0.005270954069876393, 0.006106748605731788, 0.00601651544173703, 0.005675541905707918, 0.006017647626766458]
[120.52006741524781, 150.3797800707776, 145.58030469504172, 156.088076767832, 164.14163577063943, 119.71307560612036, 164.98174010968475, 166.8768118604159, 159.56124542849145, 167.152036649172, 159.97955652816444, 168.81455776607154, 160.21921626507725, 174.08901805464103, 127.90667662785977, 164.63490158880174, 162.604036353586, 166.16253665443145, 165.41404195648843, 160.34912737307434, 139.46610880201115, 175.161229289056, 172.36734727179635, 134.3992342635468, 163.1408810844484, 167.25281842366715, 177.6843893881008, 151.32595406612253, 158.62486377658107, 174.02022299901458, 164.60871508188202, 168.04175428048615, 153.73540223844486, 167.6215141993333, 161.53409062166298, 167.02535297350173, 159.7918656517456, 161.60440861198634, 174.81095601633407, 167.05202524595794, 156.7196972573801, 154.4034037884045, 163.92611682485133, 155.95395327041345, 166.3761822149931, 158.87393581556813, 176.68892401004564, 161.17237595011625, 125.6870981177009, 160.61083872859615, 177.0005414047741, 165.13729809874604, 162.40907314277638, 133.18029243074182, 166.57043098283106, 167.96943168989287, 160.33520049191563, 176.0841442879866, 158.17964626324303, 165.78848124298278, 163.00768496374437, 176.1427997102847, 160.27961292138386, 176.0756714501438, 165.2960523152649, 163.29384188786634, 178.7270235435518, 177.98379820186415, 158.05124901413782, 158.77645998602813, 166.38857003905133, 150.10598422119816, 166.01399100000904, 163.92306592981328, 165.14296739289358, 166.6563700920607, 163.3488445332859, 164.80613193169214, 174.32274672737924, 159.9210718518066, 162.393712976407, 161.52216743221945, 177.32651402549243, 121.1227570477446, 169.23419338307107, 160.50574746074483, 163.7390417546655, 163.70937544693328, 167.30813905327784, 135.593656449847, 171.25901977962758, 177.24672202660227, 163.97926886352016, 169.97566934455494, 184.70581162066821, 167.8985868122392, 158.08019300075853, 173.12196187543685, 182.71364943291283, 175.80994706051212, 193.17775594375368, 173.48587179895299, 173.61487249915183, 184.53358379426348, 171.76020349404718, 172.02821123933055, 167.7045472487357, 181.85464656996672, 181.37914609824975, 175.04266373673116, 150.9580360903839, 177.79520331459415, 174.61174190324147, 166.00687750467682, 99.53694808441266, 178.363682332735, 173.29561070075388, 165.81670437755344, 154.15986280970742, 183.6021182235821, 187.58112446756127, 165.34581826466692, 164.8983428243937, 180.03220603924356, 173.49067066060448, 175.017741301668, 164.75146604106885, 178.5236015965014, 173.9223441711264, 166.80006466366214, 166.06885423406212, 148.14151711021523, 191.25094778389035, 170.67104601207132, 191.91122822372955, 166.78264853080213, 169.27941503848098, 160.3991405158286, 178.15677751518305, 169.20734184212438, 137.30315541461647, 122.90739231948609, 184.73241573449238, 159.19833640286072, 162.49464502819814, 137.03429347240893, 178.404672069326, 179.77070747736636, 173.72287786697532, 165.3501825364509, 166.42490548800615, 163.50534968970197, 179.19002187534988, 181.9963012257815, 170.29776080234592, 163.06097521130582, 163.1978204929255, 168.74199503582278, 170.70360447756585, 161.10653123462146, 176.38909404798432, 187.8805181794219, 187.18337168907954, 166.96158682534613, 170.49411142149157, 181.40106597631512, 179.3454329613531, 158.481305066607, 164.8602993866195, 181.95665581719328, 176.76830084627437, 165.44500348701615, 164.84301412169225, 159.07322192843642, 183.83232734431732, 161.05924427693188, 130.05817882726043, 167.04391069009017, 174.66152800156192, 177.3684908597751, 160.57095923009268, 182.90057349538634, 184.1588367138811, 172.19604037017515, 168.39774323622808, 174.65959826873396, 182.97017755101137, 189.71897435324274, 163.75326127907098, 166.2091637067734, 176.1946289206843, 166.1778924295943]
Elapsed: 0.2615427478103811~0.02719059894415124
Time per graph: 0.006079747010050883~0.0006266734206416663
Speed: 165.91736331337975~14.192306958422607
Total Time: 0.2594
best val loss: 0.2742123007774353 test_score: 0.8605

Testing...
Test loss: 0.4897 score: 0.7907 time: 0.25s
test Score 0.7907
Epoch Time List: [0.9876890230225399, 0.9427361860871315, 0.9241607239237055, 1.0485212790081277, 0.9345165340928361, 1.0338250518543646, 0.9248575259698555, 0.9505719820735976, 0.9846465800656006, 0.9635142821352929, 0.9468143949052319, 0.9795520129846409, 0.9314190612640232, 0.899150945013389, 0.9983168421313167, 0.910841649863869, 0.9326360550476238, 0.9128264971077442, 0.9248333519790322, 0.9564010619651526, 1.051969944150187, 0.898959552985616, 0.9792096510063857, 1.0709754910785705, 0.9177988269366324, 0.9520828210515901, 0.9918222889536992, 0.9669483619509265, 0.9467859879368916, 0.9084730789763853, 0.9549811020260677, 1.0149300061166286, 0.95285953884013, 0.9099836382083595, 0.9346934258937836, 0.94292862189468, 0.9627565030241385, 0.9845615150406957, 0.9016779160592705, 0.9414955178508535, 0.930300027015619, 0.9284626670414582, 0.9291213650722057, 1.0360266879433766, 0.9205585699528456, 0.9635913230013102, 0.8865819118218496, 0.933074138010852, 1.0107256579212844, 0.9447386170504615, 0.882229067152366, 0.9216562618967146, 0.914163765963167, 0.9469911659834906, 0.9596214781049639, 0.9324727579951286, 0.9546660430496559, 0.8703738040057942, 0.9410316660068929, 0.9958716729888692, 0.9496491459431127, 0.8715645869961008, 0.8980492189293727, 0.8574596439721063, 1.0187560060294345, 0.9171640380518511, 0.8829554211115465, 0.9231211440637708, 0.9115048691164702, 1.0114098698832095, 0.9449019669555128, 0.9557733669644222, 0.8995827049948275, 0.9748504399321973, 0.9878833689726889, 0.9294287587981671, 0.9214652669616044, 0.918198129045777, 0.9161929730325937, 1.0356971708824858, 0.9069172539748251, 0.9519460431765765, 0.8978090630844235, 1.0557590160751715, 1.018333266954869, 0.9643144168658182, 0.9049255999270827, 0.9706956209847704, 0.997415081015788, 1.0455154480878264, 0.9540585640352219, 0.9704817689489573, 0.9641448629554361, 0.9012495109345764, 1.008835089043714, 0.9771194498753175, 0.9518083170987666, 0.965033304062672, 0.9176803251029924, 0.9980243941536173, 1.0061516410205513, 0.919965943088755, 0.9139116769656539, 0.9515008990420029, 0.934722872916609, 1.0037533969152719, 0.9994832469383255, 0.886115014902316, 0.9059736849740148, 0.9124274649657309, 1.0631208039121702, 0.8674915060400963, 0.9239105670712888, 0.9192128719296306, 1.0721013410948217, 0.8828523049596697, 0.9240922630997375, 0.9421208609128371, 0.9342480088816956, 0.9233356401091442, 1.0074329460039735, 0.9502814169973135, 0.9798667309805751, 0.8893547499319538, 0.9280334970680997, 1.0226039621047676, 0.9632700200891122, 0.8820048060733825, 0.9359742989763618, 0.932749745901674, 0.9984179908642545, 1.0010958618950099, 0.8742144810967147, 0.9332311118487269, 0.8554720680695027, 0.9514889511046931, 1.09459999401588, 0.8792476159287617, 0.9298392540076748, 0.9361431748839095, 0.9885419278871268, 1.0606280089123175, 0.9112387469504029, 0.9412334690568969, 0.9482355760410428, 1.0188804150093347, 0.9170907490188256, 1.0468644731445238, 0.9813513910630718, 0.977377948933281, 1.0378000020282343, 1.0091257049934939, 0.9166272059082985, 0.9419306190684438, 0.935356350033544, 0.9639591281302273, 1.0212016160367057, 0.9289543339982629, 0.9683573689544573, 0.9628143679583445, 0.9200341460527852, 0.8921949909999967, 0.9804774340009317, 0.9271576770115644, 0.9146567359566689, 0.9302807019557804, 0.9368717570323497, 1.05091209593229, 0.9646893709432334, 0.8736378209432587, 0.9550526628736407, 0.9196375091560185, 1.097416453063488, 0.943423549993895, 0.912098387023434, 0.9489670098992065, 1.014325019903481, 0.9837648799875751, 0.9155843999469653, 0.9840375249041244, 0.9560157340019941, 0.8680302411084995, 1.0350753730162978, 0.9373037479817867, 0.9338743619155139, 0.9074241991620511, 0.94008611922618, 1.0073548220098019, 0.9392702508484945, 1.0194511469453573, 0.9322664770297706, 0.9290407368680462]
Total Epoch List: [3, 86, 103]
Total Time List: [0.30250520596746355, 0.2574160760268569, 0.2593718629796058]
========================training times:3========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcdd50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.42s
Epoch 2/1000, LR 0.000000
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.29s
Epoch 3/1000, LR 0.000030
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.29s
Epoch 4/1000, LR 0.000060
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.28s
Epoch 5/1000, LR 0.000090
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.29s
     INFO: Early stopping counter 1 of 2
Epoch 6/1000, LR 0.000120
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.29s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.27s
     INFO: Early stopping counter 1 of 2
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.29s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.6933,   Val_Loss: 0.6929,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.6929,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6930


[0.422225168091245, 0.29660237196367234, 0.2979703500168398, 0.28937650204170495, 0.29411626898217946, 0.2951351690571755, 0.27493204397615045, 0.28958423202857375]
[0.009596026547528296, 0.006740962999174371, 0.006772053409473632, 0.006576738682766022, 0.006684460658685897, 0.0067076174785721705, 0.00624845554491251, 0.006581459818831222]
[104.20979923795394, 148.34675700229764, 147.6656989445874, 152.05104661075322, 149.6007009481885, 149.08423194890756, 160.03954782301358, 151.94197450522253]
Elapsed: 0.30749276326969266~0.043894130131837815
Time per graph: 0.006988471892493015~0.000997593866632678
Speed: 145.36746962761555~15.98049829859778
Total Time: 0.2898
best val loss: 0.6929181218147278 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.29s
test Score 0.5000
Epoch Time List: [1.0662560678320006, 0.9339699920965359, 0.9944040660047904, 0.8767642710590735, 0.9259646128630266, 0.9234538851305842, 0.9401062949327752, 0.907263200962916]
Total Epoch List: [8]
Total Time List: [0.2898203480290249]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcf5b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.26s
Epoch 2/1000, LR 0.000000
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.29s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.37s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.51s
Val loss: 0.6930 score: 0.5682 time: 0.39s
Test loss: 0.6931 score: 0.6047 time: 0.35s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.27s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.31s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.27s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.26s
Epoch 15/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.27s
Epoch 19/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.26s
Epoch 20/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.26s
Epoch 21/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.26s
Epoch 22/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.29s
Epoch 23/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.26s
Epoch 24/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.42s
Val loss: 0.6895 score: 0.5227 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.42s
Val loss: 0.6890 score: 0.5682 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.42s
Val loss: 0.6884 score: 0.5909 time: 0.25s
Test loss: 0.6909 score: 0.5116 time: 0.27s
Epoch 27/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.50s
Val loss: 0.6878 score: 0.6591 time: 0.26s
Test loss: 0.6905 score: 0.5814 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.42s
Val loss: 0.6871 score: 0.6364 time: 0.26s
Test loss: 0.6900 score: 0.6047 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.41s
Val loss: 0.6863 score: 0.6818 time: 0.26s
Test loss: 0.6895 score: 0.6047 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.41s
Val loss: 0.6854 score: 0.6818 time: 0.26s
Test loss: 0.6889 score: 0.5116 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.47s
Val loss: 0.6845 score: 0.7273 time: 0.24s
Test loss: 0.6883 score: 0.5349 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.40s
Val loss: 0.6834 score: 0.7273 time: 0.27s
Test loss: 0.6876 score: 0.5814 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.48s
Val loss: 0.6823 score: 0.7500 time: 0.27s
Test loss: 0.6868 score: 0.6512 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.42s
Val loss: 0.6811 score: 0.7955 time: 0.25s
Test loss: 0.6859 score: 0.6512 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.45s
Val loss: 0.6798 score: 0.7500 time: 0.25s
Test loss: 0.6850 score: 0.6512 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.39s
Val loss: 0.6785 score: 0.7273 time: 0.27s
Test loss: 0.6841 score: 0.6744 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.40s
Val loss: 0.6770 score: 0.7273 time: 0.26s
Test loss: 0.6830 score: 0.6744 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.43s
Val loss: 0.6754 score: 0.7273 time: 0.24s
Test loss: 0.6819 score: 0.6744 time: 0.35s
Epoch 39/1000, LR 0.000269
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 0.42s
Val loss: 0.6737 score: 0.7045 time: 0.25s
Test loss: 0.6807 score: 0.6744 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.38s
Val loss: 0.6718 score: 0.7273 time: 0.26s
Test loss: 0.6795 score: 0.6977 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.40s
Val loss: 0.6698 score: 0.7273 time: 0.25s
Test loss: 0.6781 score: 0.6977 time: 0.26s
Epoch 42/1000, LR 0.000269
Train loss: 0.6564;  Loss pred: 0.6564; Loss self: 0.0000; time: 0.37s
Val loss: 0.6677 score: 0.7500 time: 0.25s
Test loss: 0.6766 score: 0.6977 time: 0.24s
Epoch 43/1000, LR 0.000269
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 0.40s
Val loss: 0.6654 score: 0.7500 time: 0.34s
Test loss: 0.6749 score: 0.6744 time: 0.35s
Epoch 44/1000, LR 0.000269
Train loss: 0.6482;  Loss pred: 0.6482; Loss self: 0.0000; time: 0.42s
Val loss: 0.6629 score: 0.7727 time: 0.26s
Test loss: 0.6732 score: 0.6977 time: 0.27s
Epoch 45/1000, LR 0.000269
Train loss: 0.6442;  Loss pred: 0.6442; Loss self: 0.0000; time: 0.39s
Val loss: 0.6602 score: 0.8182 time: 0.26s
Test loss: 0.6713 score: 0.6744 time: 0.34s
Epoch 46/1000, LR 0.000269
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 0.40s
Val loss: 0.6573 score: 0.8182 time: 0.27s
Test loss: 0.6694 score: 0.6977 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.42s
Val loss: 0.6541 score: 0.8409 time: 0.26s
Test loss: 0.6672 score: 0.6744 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 0.43s
Val loss: 0.6507 score: 0.8636 time: 0.25s
Test loss: 0.6648 score: 0.6512 time: 0.27s
Epoch 49/1000, LR 0.000269
Train loss: 0.6214;  Loss pred: 0.6214; Loss self: 0.0000; time: 0.61s
Val loss: 0.6470 score: 0.8636 time: 0.26s
Test loss: 0.6623 score: 0.6744 time: 0.27s
Epoch 50/1000, LR 0.000269
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.51s
Val loss: 0.6430 score: 0.8409 time: 0.26s
Test loss: 0.6594 score: 0.6744 time: 0.28s
Epoch 51/1000, LR 0.000269
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.39s
Val loss: 0.6387 score: 0.8409 time: 0.28s
Test loss: 0.6564 score: 0.6744 time: 0.26s
Epoch 52/1000, LR 0.000269
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.42s
Val loss: 0.6340 score: 0.8409 time: 0.26s
Test loss: 0.6530 score: 0.6977 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 0.42s
Val loss: 0.6289 score: 0.8409 time: 0.25s
Test loss: 0.6493 score: 0.6977 time: 0.26s
Epoch 54/1000, LR 0.000269
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.39s
Val loss: 0.6234 score: 0.8409 time: 0.33s
Test loss: 0.6453 score: 0.6977 time: 0.26s
Epoch 55/1000, LR 0.000269
Train loss: 0.5761;  Loss pred: 0.5761; Loss self: 0.0000; time: 0.38s
Val loss: 0.6176 score: 0.8182 time: 0.27s
Test loss: 0.6410 score: 0.6977 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 0.41s
Val loss: 0.6114 score: 0.8182 time: 0.27s
Test loss: 0.6365 score: 0.7674 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.43s
Val loss: 0.6049 score: 0.8182 time: 0.26s
Test loss: 0.6317 score: 0.7674 time: 0.27s
Epoch 58/1000, LR 0.000269
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.39s
Val loss: 0.5982 score: 0.7955 time: 0.29s
Test loss: 0.6267 score: 0.7674 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.40s
Val loss: 0.5911 score: 0.7955 time: 0.35s
Test loss: 0.6214 score: 0.7674 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.41s
Val loss: 0.5838 score: 0.7955 time: 0.26s
Test loss: 0.6159 score: 0.7674 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.4988;  Loss pred: 0.4988; Loss self: 0.0000; time: 0.43s
Val loss: 0.5761 score: 0.7955 time: 0.24s
Test loss: 0.6102 score: 0.7674 time: 0.27s
Epoch 62/1000, LR 0.000268
Train loss: 0.4844;  Loss pred: 0.4844; Loss self: 0.0000; time: 0.39s
Val loss: 0.5681 score: 0.7955 time: 0.26s
Test loss: 0.6041 score: 0.7907 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.4710;  Loss pred: 0.4710; Loss self: 0.0000; time: 0.40s
Val loss: 0.5598 score: 0.7955 time: 0.27s
Test loss: 0.5978 score: 0.7907 time: 0.26s
Epoch 64/1000, LR 0.000268
Train loss: 0.4611;  Loss pred: 0.4611; Loss self: 0.0000; time: 0.41s
Val loss: 0.5513 score: 0.7955 time: 0.35s
Test loss: 0.5912 score: 0.7907 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.4555;  Loss pred: 0.4555; Loss self: 0.0000; time: 0.44s
Val loss: 0.5429 score: 0.7955 time: 0.25s
Test loss: 0.5846 score: 0.7907 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.4239;  Loss pred: 0.4239; Loss self: 0.0000; time: 0.43s
Val loss: 0.5342 score: 0.7955 time: 0.26s
Test loss: 0.5777 score: 0.7907 time: 0.27s
Epoch 67/1000, LR 0.000268
Train loss: 0.4063;  Loss pred: 0.4063; Loss self: 0.0000; time: 0.41s
Val loss: 0.5253 score: 0.7955 time: 0.26s
Test loss: 0.5705 score: 0.7907 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.3933;  Loss pred: 0.3933; Loss self: 0.0000; time: 0.42s
Val loss: 0.5166 score: 0.7955 time: 0.29s
Test loss: 0.5633 score: 0.7907 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.3846;  Loss pred: 0.3846; Loss self: 0.0000; time: 0.47s
Val loss: 0.5079 score: 0.7955 time: 0.34s
Test loss: 0.5561 score: 0.7907 time: 0.28s
Epoch 70/1000, LR 0.000268
Train loss: 0.3604;  Loss pred: 0.3604; Loss self: 0.0000; time: 0.44s
Val loss: 0.4993 score: 0.7955 time: 0.25s
Test loss: 0.5490 score: 0.7907 time: 0.26s
Epoch 71/1000, LR 0.000268
Train loss: 0.3474;  Loss pred: 0.3474; Loss self: 0.0000; time: 0.38s
Val loss: 0.4910 score: 0.7955 time: 0.27s
Test loss: 0.5419 score: 0.7907 time: 0.39s
Epoch 72/1000, LR 0.000267
Train loss: 0.3303;  Loss pred: 0.3303; Loss self: 0.0000; time: 0.42s
Val loss: 0.4829 score: 0.7955 time: 0.27s
Test loss: 0.5351 score: 0.7907 time: 0.26s
Epoch 73/1000, LR 0.000267
Train loss: 0.3230;  Loss pred: 0.3230; Loss self: 0.0000; time: 0.40s
Val loss: 0.4752 score: 0.7955 time: 0.39s
Test loss: 0.5287 score: 0.7907 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.2980;  Loss pred: 0.2980; Loss self: 0.0000; time: 0.54s
Val loss: 0.4680 score: 0.7955 time: 0.37s
Test loss: 0.5225 score: 0.7907 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.2810;  Loss pred: 0.2810; Loss self: 0.0000; time: 0.49s
Val loss: 0.4614 score: 0.7955 time: 0.27s
Test loss: 0.5170 score: 0.7907 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.2769;  Loss pred: 0.2769; Loss self: 0.0000; time: 0.45s
Val loss: 0.4556 score: 0.7955 time: 0.26s
Test loss: 0.5121 score: 0.7907 time: 0.26s
Epoch 77/1000, LR 0.000267
Train loss: 0.2624;  Loss pred: 0.2624; Loss self: 0.0000; time: 0.37s
Val loss: 0.4507 score: 0.7955 time: 0.26s
Test loss: 0.5081 score: 0.7907 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.2413;  Loss pred: 0.2413; Loss self: 0.0000; time: 0.41s
Val loss: 0.4467 score: 0.7955 time: 0.27s
Test loss: 0.5047 score: 0.7907 time: 0.26s
Epoch 79/1000, LR 0.000267
Train loss: 0.2288;  Loss pred: 0.2288; Loss self: 0.0000; time: 0.51s
Val loss: 0.4441 score: 0.7955 time: 0.39s
Test loss: 0.5027 score: 0.7907 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.2133;  Loss pred: 0.2133; Loss self: 0.0000; time: 0.43s
Val loss: 0.4426 score: 0.8182 time: 0.25s
Test loss: 0.5019 score: 0.7907 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.1976;  Loss pred: 0.1976; Loss self: 0.0000; time: 0.38s
Val loss: 0.4415 score: 0.8182 time: 0.25s
Test loss: 0.5017 score: 0.7907 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.1951;  Loss pred: 0.1951; Loss self: 0.0000; time: 0.40s
Val loss: 0.4420 score: 0.8182 time: 0.27s
Test loss: 0.5037 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 83/1000, LR 0.000266
Train loss: 0.1736;  Loss pred: 0.1736; Loss self: 0.0000; time: 0.49s
Val loss: 0.4422 score: 0.8182 time: 0.26s
Test loss: 0.5046 score: 0.7907 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 080,   Train_Loss: 0.1976,   Val_Loss: 0.4415,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4415,   Test_Precision: 0.9333,   Test_Recall: 0.6364,   Test_accuracy: 0.7568,   Test_Score: 0.7907,   Test_loss: 0.5017


[0.422225168091245, 0.29660237196367234, 0.2979703500168398, 0.28937650204170495, 0.29411626898217946, 0.2951351690571755, 0.27493204397615045, 0.28958423202857375, 0.2667604119051248, 0.26923790702130646, 0.29483585606794804, 0.3711841810727492, 0.3578993799164891, 0.2786534749902785, 0.3127934130607173, 0.24596568895503879, 0.2555394449736923, 0.2586574029410258, 0.2730316269444302, 0.24478137597907335, 0.2473662099801004, 0.26357283000834286, 0.2560108620673418, 0.24445023899897933, 0.25914515799377114, 0.2783089129952714, 0.2615338999312371, 0.26850848097819835, 0.2623047789093107, 0.3018726290902123, 0.2625411259941757, 0.2503938040463254, 0.24323028698563576, 0.27367850800510496, 0.2689930199412629, 0.2528041700134054, 0.25484329904429615, 0.2662892669904977, 0.26232699304819107, 0.24219239596277475, 0.2575413240119815, 0.25957297103013843, 0.267474681022577, 0.2640590489609167, 0.2530000030528754, 0.35217813204508275, 0.2679019389906898, 0.24603485804982483, 0.2620915040606633, 0.24381654197350144, 0.3531398499617353, 0.27268146607093513, 0.3484210280003026, 0.24447109492029995, 0.2625272989971563, 0.2774416559841484, 0.27094940294045955, 0.28492999705486, 0.2685092450119555, 0.25546827795915306, 0.26480797794647515, 0.26360189099796116, 0.2460564860375598, 0.25364594103302807, 0.26978259696625173, 0.25050212803762406, 0.24348995799664408, 0.25889838906005025, 0.27373571798671037, 0.25498310395050794, 0.25929956999607384, 0.2597665439825505, 0.25612620601896197, 0.2730219670338556, 0.26232902705669403, 0.25344362400937825, 0.2870547859929502, 0.2643372219754383, 0.392109917011112, 0.26430174394045025, 0.24857359693851322, 0.24333915195893496, 0.25582319602835923, 0.2660153420874849, 0.2555359200341627, 0.26108013198245317, 0.24661772802937776, 0.2668500490253791, 0.24665618303697556, 0.2422578529221937, 0.2564928460633382]
[0.009596026547528296, 0.006740962999174371, 0.006772053409473632, 0.006576738682766022, 0.006684460658685897, 0.0067076174785721705, 0.00624845554491251, 0.006581459818831222, 0.006203730509421507, 0.006261346674914104, 0.006856647815533676, 0.008632190257505796, 0.008323241393406724, 0.0064803133718669415, 0.007274265420016681, 0.005720132301279972, 0.005942777790085867, 0.006015288440488972, 0.006349572719637911, 0.005692590139048217, 0.005752702557676753, 0.006129600697868439, 0.005953740978310275, 0.005684889279046031, 0.0060266315812504915, 0.006472300302215614, 0.006082183719331095, 0.006244383278562752, 0.00610011113742583, 0.007020293699772378, 0.0061056075812598995, 0.005823111722007567, 0.005656518301991529, 0.006364616465234999, 0.006255651626540998, 0.005879166744497799, 0.005926588349867352, 0.006192773650941807, 0.006100627745306769, 0.005632381301459878, 0.005989333116557709, 0.006036580721631126, 0.006220341419129697, 0.006140908115370156, 0.00588372100122966, 0.008190189117327506, 0.0062302776509462745, 0.005721740884879647, 0.006095151257224728, 0.005670152138918638, 0.008212554650272914, 0.006341429443510119, 0.0081028146046582, 0.005685374300472092, 0.006105286023189682, 0.006452131534515079, 0.0063011489055920826, 0.006626279001275814, 0.0062444010467896625, 0.005941122743236118, 0.006158325068522678, 0.006130276534836306, 0.0057222438613386, 0.005898742814721583, 0.006274013882936086, 0.005825630884595909, 0.005662557162712653, 0.006020892768838378, 0.006365946929923497, 0.005929839626755999, 0.006030222558048229, 0.0060410824181988485, 0.005956423395789813, 0.006349348070554782, 0.006100675047830094, 0.0058940377676599594, 0.00667569269751047, 0.006147377255242751, 0.009118835279328186, 0.006146552184661634, 0.00578078132415147, 0.005659050045556627, 0.005949376651822308, 0.0061864033043601135, 0.00594269581474797, 0.00607163097633612, 0.0057352960006832035, 0.006205815093613468, 0.005736190303185478, 0.005633903556330086, 0.005964949908449726]
[104.20979923795394, 148.34675700229764, 147.6656989445874, 152.05104661075322, 149.6007009481885, 149.08423194890756, 160.03954782301358, 151.94197450522253, 161.19333334697822, 159.71005151439223, 145.84386232212609, 115.84545407007077, 120.1455001403843, 154.31352507446806, 137.47092555191736, 174.82113128331557, 168.2714776359745, 166.24306712692098, 157.49091224787574, 175.6669592529626, 173.8313410043319, 163.14276398913697, 167.96162339662433, 175.9049210836711, 165.9301695346882, 154.50457384643875, 164.41463233372662, 160.1439174038283, 163.9314395216064, 142.4441829310394, 163.78386371723693, 171.72948892954463, 176.7871942795488, 157.11865836099165, 159.85544907220807, 170.09213098707926, 168.7311385516394, 161.47853229673885, 163.91755762663982, 177.54479792424675, 166.96349669305704, 165.65669310387239, 160.76287981953124, 162.8423648771242, 169.96047225743817, 122.09730272092965, 160.50649040466388, 174.7719828841977, 164.06483740902678, 176.36211083935947, 121.764790931013, 157.69315245215094, 123.41390600558888, 175.88991456850323, 163.79249001630788, 154.9875408848367, 158.70121702925155, 150.9142612026239, 160.14346171985775, 168.318353822682, 162.38181467739415, 163.12477819187035, 174.75662069495772, 169.52764875666128, 159.38759758243063, 171.65522838808633, 176.59865874465612, 166.0883258336009, 157.08582101108053, 168.63862481000416, 165.83135868929932, 165.53324897331072, 167.88598350930383, 157.49648450327024, 163.91628666661782, 169.66297798207327, 149.79718889291064, 162.67099910732767, 109.6631279508838, 162.69283493524097, 172.9869967269147, 176.70810329468281, 168.08483619770445, 161.64481214718256, 168.27379882347387, 164.70039169005005, 174.3589171127135, 161.13918718414936, 174.33173363245464, 177.4968261351279, 167.64600128216287]
Elapsed: 0.2721142445736984~0.03292344850112646
Time per graph: 0.006313950544611964~0.000748818016512151
Speed: 160.16021172691086~15.189835819432348
Total Time: 0.2569
best val loss: 0.44153377413749695 test_score: 0.7907

Testing...
Test loss: 0.6648 score: 0.6512 time: 0.26s
test Score 0.6512
Epoch Time List: [1.0662560678320006, 0.9339699920965359, 0.9944040660047904, 0.8767642710590735, 0.9259646128630266, 0.9234538851305842, 0.9401062949327752, 0.907263200962916, 1.0248892289819196, 1.0475094949360937, 0.994640079094097, 1.0989962719613686, 1.2516081450739875, 1.0789345359662548, 0.9968134990194812, 0.8839127371320501, 0.9330750309163705, 1.0152327910764143, 0.9609648940386251, 0.867264827946201, 0.9107700149761513, 0.9132205790374428, 0.8917420849902555, 0.9830988220637664, 0.9229802049230784, 0.9330181078985333, 0.8867743249284104, 0.9395421339431778, 1.009582264116034, 0.9606940459925681, 0.9608422750607133, 0.9375252551399171, 0.9221321360673755, 0.9333365689963102, 1.0221898129675537, 0.9250635310309008, 0.9242681040195748, 0.9245298908790573, 0.9718292838661, 0.9031455109361559, 1.0046136500313878, 0.9195528530981392, 0.9568618499906734, 0.9251358839683235, 0.9140717320842668, 1.0211343851406127, 0.9322857429506257, 0.8855021541239694, 0.9093549700919539, 0.8649806639878079, 1.08706518902909, 0.9523320409934968, 0.9942771049682051, 0.9094507179688662, 0.9369285870343447, 0.9549685369711369, 1.1430653600255027, 1.0548496518749744, 0.9263269460061565, 0.9313946380279958, 0.9219536590389907, 0.9817257709801197, 0.8882657259237021, 0.9380715880542994, 0.955100238090381, 0.9251949781319126, 0.9893329750047997, 0.9210271950578317, 0.9434516319306567, 0.9026138191111386, 0.9210215781349689, 1.0220898960251361, 0.9372260089730844, 0.9546054899692535, 0.9342750309733674, 0.9585907378932461, 1.0895447741495445, 0.9500912689836696, 1.0408879070309922, 0.9460537498816848, 1.0318281310610473, 1.1467713539022952, 1.0113815380027518, 0.9754611679818481, 0.8790301780682057, 0.9390593869611621, 1.1408139480045065, 0.938779967953451, 0.8742974878987297, 0.9061875210609287, 0.9994442081078887]
Total Epoch List: [8, 83]
Total Time List: [0.2898203480290249, 0.2569258409785107]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcebc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.26s
Epoch 15/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.26s
Epoch 17/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.28s
Epoch 19/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.28s
Epoch 22/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.26s
Epoch 24/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.34s
Epoch 27/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.27s
Epoch 29/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.28s
Epoch 30/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.4884 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4884 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.4884 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4884 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.4884 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.4884 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6839 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6838 score: 0.4884 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6829 score: 0.5000 time: 0.27s
Test loss: 0.6827 score: 0.5116 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.5000 time: 0.29s
Test loss: 0.6815 score: 0.5116 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.5000 time: 0.30s
Test loss: 0.6801 score: 0.5116 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.43s
Val loss: 0.6793 score: 0.5455 time: 0.29s
Test loss: 0.6786 score: 0.5349 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.44s
Val loss: 0.6778 score: 0.5455 time: 0.27s
Test loss: 0.6770 score: 0.5349 time: 0.30s
Epoch 43/1000, LR 0.000269
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.49s
Val loss: 0.6763 score: 0.5682 time: 0.28s
Test loss: 0.6753 score: 0.5349 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.35s
Val loss: 0.6746 score: 0.5682 time: 0.31s
Test loss: 0.6733 score: 0.5349 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.45s
Val loss: 0.6728 score: 0.5682 time: 0.29s
Test loss: 0.6713 score: 0.5349 time: 0.23s
Epoch 46/1000, LR 0.000269
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.42s
Val loss: 0.6708 score: 0.5909 time: 0.31s
Test loss: 0.6690 score: 0.5349 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.43s
Val loss: 0.6687 score: 0.5909 time: 0.28s
Test loss: 0.6666 score: 0.5349 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.43s
Val loss: 0.6663 score: 0.6364 time: 0.28s
Test loss: 0.6639 score: 0.5581 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.40s
Val loss: 0.6638 score: 0.6136 time: 0.30s
Test loss: 0.6610 score: 0.5814 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.40s
Val loss: 0.6610 score: 0.6364 time: 0.27s
Test loss: 0.6579 score: 0.5814 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.42s
Val loss: 0.6581 score: 0.6591 time: 0.28s
Test loss: 0.6544 score: 0.6047 time: 0.26s
Epoch 52/1000, LR 0.000269
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 0.34s
Val loss: 0.6548 score: 0.7273 time: 0.28s
Test loss: 0.6507 score: 0.6512 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.49s
Val loss: 0.6513 score: 0.7500 time: 0.29s
Test loss: 0.6467 score: 0.7442 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6325;  Loss pred: 0.6325; Loss self: 0.0000; time: 0.40s
Val loss: 0.6475 score: 0.7727 time: 0.26s
Test loss: 0.6424 score: 0.7674 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6287;  Loss pred: 0.6287; Loss self: 0.0000; time: 0.39s
Val loss: 0.6433 score: 0.7727 time: 0.28s
Test loss: 0.6378 score: 0.7442 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6197;  Loss pred: 0.6197; Loss self: 0.0000; time: 0.37s
Val loss: 0.6388 score: 0.7955 time: 0.29s
Test loss: 0.6328 score: 0.7907 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.6134;  Loss pred: 0.6134; Loss self: 0.0000; time: 0.40s
Val loss: 0.6338 score: 0.8182 time: 0.28s
Test loss: 0.6275 score: 0.7907 time: 0.34s
Epoch 58/1000, LR 0.000269
Train loss: 0.6059;  Loss pred: 0.6059; Loss self: 0.0000; time: 0.40s
Val loss: 0.6283 score: 0.8409 time: 0.27s
Test loss: 0.6219 score: 0.7907 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.6021;  Loss pred: 0.6021; Loss self: 0.0000; time: 0.38s
Val loss: 0.6224 score: 0.8409 time: 0.28s
Test loss: 0.6160 score: 0.8140 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5908;  Loss pred: 0.5908; Loss self: 0.0000; time: 0.40s
Val loss: 0.6161 score: 0.8409 time: 0.29s
Test loss: 0.6096 score: 0.8140 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.40s
Val loss: 0.6094 score: 0.8409 time: 0.27s
Test loss: 0.6027 score: 0.8140 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.5746;  Loss pred: 0.5746; Loss self: 0.0000; time: 0.41s
Val loss: 0.6023 score: 0.8636 time: 0.27s
Test loss: 0.5955 score: 0.8140 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5659;  Loss pred: 0.5659; Loss self: 0.0000; time: 0.37s
Val loss: 0.5948 score: 0.8864 time: 0.37s
Test loss: 0.5878 score: 0.8140 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5559;  Loss pred: 0.5559; Loss self: 0.0000; time: 0.39s
Val loss: 0.5870 score: 0.8864 time: 0.29s
Test loss: 0.5798 score: 0.8140 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.40s
Val loss: 0.5787 score: 0.8864 time: 0.27s
Test loss: 0.5718 score: 0.7907 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.5275;  Loss pred: 0.5275; Loss self: 0.0000; time: 0.41s
Val loss: 0.5699 score: 0.8864 time: 0.28s
Test loss: 0.5637 score: 0.7907 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.37s
Val loss: 0.5608 score: 0.8864 time: 0.30s
Test loss: 0.5554 score: 0.7907 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.5026;  Loss pred: 0.5026; Loss self: 0.0000; time: 0.40s
Val loss: 0.5515 score: 0.8864 time: 0.36s
Test loss: 0.5467 score: 0.7907 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.4965;  Loss pred: 0.4965; Loss self: 0.0000; time: 0.40s
Val loss: 0.5419 score: 0.9091 time: 0.27s
Test loss: 0.5377 score: 0.7907 time: 0.26s
Epoch 70/1000, LR 0.000268
Train loss: 0.4894;  Loss pred: 0.4894; Loss self: 0.0000; time: 0.41s
Val loss: 0.5322 score: 0.9091 time: 0.26s
Test loss: 0.5281 score: 0.7907 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.4609;  Loss pred: 0.4609; Loss self: 0.0000; time: 0.36s
Val loss: 0.5222 score: 0.9091 time: 0.29s
Test loss: 0.5187 score: 0.8140 time: 0.23s
Epoch 72/1000, LR 0.000267
Train loss: 0.4411;  Loss pred: 0.4411; Loss self: 0.0000; time: 0.38s
Val loss: 0.5119 score: 0.9091 time: 0.27s
Test loss: 0.5093 score: 0.8140 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.4375;  Loss pred: 0.4375; Loss self: 0.0000; time: 0.40s
Val loss: 0.5014 score: 0.9318 time: 0.27s
Test loss: 0.4999 score: 0.8140 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.4259;  Loss pred: 0.4259; Loss self: 0.0000; time: 0.45s
Val loss: 0.4908 score: 0.9318 time: 0.28s
Test loss: 0.4911 score: 0.8140 time: 0.26s
Epoch 75/1000, LR 0.000267
Train loss: 0.4263;  Loss pred: 0.4263; Loss self: 0.0000; time: 0.42s
Val loss: 0.4804 score: 0.9318 time: 0.29s
Test loss: 0.4816 score: 0.8140 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.3910;  Loss pred: 0.3910; Loss self: 0.0000; time: 0.40s
Val loss: 0.4697 score: 0.9318 time: 0.30s
Test loss: 0.4729 score: 0.8140 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.3875;  Loss pred: 0.3875; Loss self: 0.0000; time: 0.41s
Val loss: 0.4593 score: 0.9318 time: 0.28s
Test loss: 0.4640 score: 0.8140 time: 0.26s
Epoch 78/1000, LR 0.000267
Train loss: 0.3693;  Loss pred: 0.3693; Loss self: 0.0000; time: 0.35s
Val loss: 0.4491 score: 0.9318 time: 0.28s
Test loss: 0.4550 score: 0.8372 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.3521;  Loss pred: 0.3521; Loss self: 0.0000; time: 0.48s
Val loss: 0.4387 score: 0.9318 time: 0.29s
Test loss: 0.4470 score: 0.8605 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.3419;  Loss pred: 0.3419; Loss self: 0.0000; time: 0.41s
Val loss: 0.4286 score: 0.9318 time: 0.28s
Test loss: 0.4388 score: 0.8605 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.3198;  Loss pred: 0.3198; Loss self: 0.0000; time: 0.42s
Val loss: 0.4181 score: 0.9318 time: 0.27s
Test loss: 0.4318 score: 0.8605 time: 0.25s
Epoch 82/1000, LR 0.000267
Train loss: 0.3059;  Loss pred: 0.3059; Loss self: 0.0000; time: 0.38s
Val loss: 0.4088 score: 0.9318 time: 0.28s
Test loss: 0.4237 score: 0.8605 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2943;  Loss pred: 0.2943; Loss self: 0.0000; time: 0.39s
Val loss: 0.4001 score: 0.9091 time: 0.30s
Test loss: 0.4158 score: 0.8605 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2867;  Loss pred: 0.2867; Loss self: 0.0000; time: 0.49s
Val loss: 0.3906 score: 0.9318 time: 0.29s
Test loss: 0.4100 score: 0.8605 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.2763;  Loss pred: 0.2763; Loss self: 0.0000; time: 0.39s
Val loss: 0.3805 score: 0.9091 time: 0.27s
Test loss: 0.4056 score: 0.8605 time: 0.26s
Epoch 86/1000, LR 0.000266
Train loss: 0.2673;  Loss pred: 0.2673; Loss self: 0.0000; time: 0.40s
Val loss: 0.3707 score: 0.9091 time: 0.28s
Test loss: 0.4017 score: 0.8605 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.2484;  Loss pred: 0.2484; Loss self: 0.0000; time: 0.43s
Val loss: 0.3621 score: 0.9091 time: 0.29s
Test loss: 0.3973 score: 0.8605 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.2491;  Loss pred: 0.2491; Loss self: 0.0000; time: 0.40s
Val loss: 0.3568 score: 0.9318 time: 0.28s
Test loss: 0.3900 score: 0.8605 time: 0.31s
Epoch 89/1000, LR 0.000266
Train loss: 0.2400;  Loss pred: 0.2400; Loss self: 0.0000; time: 0.40s
Val loss: 0.3528 score: 0.9318 time: 0.30s
Test loss: 0.3833 score: 0.8372 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.2383;  Loss pred: 0.2383; Loss self: 0.0000; time: 0.43s
Val loss: 0.3492 score: 0.9318 time: 0.31s
Test loss: 0.3781 score: 0.8372 time: 0.26s
Epoch 91/1000, LR 0.000266
Train loss: 0.2022;  Loss pred: 0.2022; Loss self: 0.0000; time: 0.41s
Val loss: 0.3437 score: 0.9318 time: 0.28s
Test loss: 0.3752 score: 0.8372 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 0.1891;  Loss pred: 0.1891; Loss self: 0.0000; time: 0.37s
Val loss: 0.3366 score: 0.9318 time: 0.29s
Test loss: 0.3740 score: 0.8372 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.2162;  Loss pred: 0.2162; Loss self: 0.0000; time: 0.41s
Val loss: 0.3325 score: 0.9318 time: 0.28s
Test loss: 0.3718 score: 0.8372 time: 0.24s
Epoch 94/1000, LR 0.000265
Train loss: 0.1751;  Loss pred: 0.1751; Loss self: 0.0000; time: 0.41s
Val loss: 0.3258 score: 0.9318 time: 0.34s
Test loss: 0.3720 score: 0.8372 time: 0.24s
Epoch 95/1000, LR 0.000265
Train loss: 0.1889;  Loss pred: 0.1889; Loss self: 0.0000; time: 0.41s
Val loss: 0.3203 score: 0.9318 time: 0.27s
Test loss: 0.3723 score: 0.8372 time: 0.25s
Epoch 96/1000, LR 0.000265
Train loss: 0.1536;  Loss pred: 0.1536; Loss self: 0.0000; time: 0.37s
Val loss: 0.3128 score: 0.9318 time: 0.29s
Test loss: 0.3745 score: 0.8372 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.1766;  Loss pred: 0.1766; Loss self: 0.0000; time: 0.40s
Val loss: 0.3022 score: 0.9318 time: 0.28s
Test loss: 0.3795 score: 0.8372 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.1475;  Loss pred: 0.1475; Loss self: 0.0000; time: 0.42s
Val loss: 0.2924 score: 0.9318 time: 0.28s
Test loss: 0.3849 score: 0.8605 time: 0.25s
Epoch 99/1000, LR 0.000265
Train loss: 0.1262;  Loss pred: 0.1262; Loss self: 0.0000; time: 0.38s
Val loss: 0.2855 score: 0.9318 time: 0.36s
Test loss: 0.3892 score: 0.8605 time: 0.24s
Epoch 100/1000, LR 0.000265
Train loss: 0.1499;  Loss pred: 0.1499; Loss self: 0.0000; time: 0.41s
Val loss: 0.2811 score: 0.9318 time: 0.29s
Test loss: 0.3923 score: 0.8605 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.40s
Val loss: 0.2768 score: 0.9318 time: 0.27s
Test loss: 0.3962 score: 0.8605 time: 0.24s
Epoch 102/1000, LR 0.000264
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.42s
Val loss: 0.2726 score: 0.9318 time: 0.34s
Test loss: 0.4005 score: 0.8605 time: 0.25s
Epoch 103/1000, LR 0.000264
Train loss: 0.1063;  Loss pred: 0.1063; Loss self: 0.0000; time: 0.50s
Val loss: 0.2699 score: 0.9318 time: 0.29s
Test loss: 0.4042 score: 0.8605 time: 0.25s
Epoch 104/1000, LR 0.000264
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.43s
Val loss: 0.2690 score: 0.9318 time: 0.29s
Test loss: 0.4073 score: 0.8372 time: 0.24s
Epoch 105/1000, LR 0.000264
Train loss: 0.1050;  Loss pred: 0.1050; Loss self: 0.0000; time: 0.48s
Val loss: 0.2672 score: 0.9318 time: 0.28s
Test loss: 0.4115 score: 0.8372 time: 0.29s
Epoch 106/1000, LR 0.000264
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.39s
Val loss: 0.2689 score: 0.9318 time: 0.27s
Test loss: 0.4140 score: 0.8372 time: 0.26s
     INFO: Early stopping counter 1 of 2
Epoch 107/1000, LR 0.000264
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 0.59s
Val loss: 0.2578 score: 0.9318 time: 0.28s
Test loss: 0.4249 score: 0.8605 time: 0.25s
Epoch 108/1000, LR 0.000264
Train loss: 0.0964;  Loss pred: 0.0964; Loss self: 0.0000; time: 0.35s
Val loss: 0.2491 score: 0.9318 time: 0.31s
Test loss: 0.4354 score: 0.8605 time: 0.43s
Epoch 109/1000, LR 0.000264
Train loss: 0.0843;  Loss pred: 0.0843; Loss self: 0.0000; time: 0.38s
Val loss: 0.2449 score: 0.9318 time: 0.31s
Test loss: 0.4430 score: 0.8605 time: 0.23s
Epoch 110/1000, LR 0.000263
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.49s
Val loss: 0.2424 score: 0.9318 time: 0.28s
Test loss: 0.4493 score: 0.8605 time: 0.24s
Epoch 111/1000, LR 0.000263
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.41s
Val loss: 0.2407 score: 0.9318 time: 0.28s
Test loss: 0.4553 score: 0.8605 time: 0.24s
Epoch 112/1000, LR 0.000263
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.45s
Val loss: 0.2449 score: 0.9318 time: 0.33s
Test loss: 0.4569 score: 0.8605 time: 0.28s
     INFO: Early stopping counter 1 of 2
Epoch 113/1000, LR 0.000263
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.38s
Val loss: 0.2482 score: 0.9318 time: 0.28s
Test loss: 0.4605 score: 0.8372 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 110,   Train_Loss: 0.0520,   Val_Loss: 0.2407,   Val_Precision: 1.0000,   Val_Recall: 0.8636,   Val_accuracy: 0.9268,   Val_Score: 0.9318,   Val_Loss: 0.2407,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.4553


[0.422225168091245, 0.29660237196367234, 0.2979703500168398, 0.28937650204170495, 0.29411626898217946, 0.2951351690571755, 0.27493204397615045, 0.28958423202857375, 0.2667604119051248, 0.26923790702130646, 0.29483585606794804, 0.3711841810727492, 0.3578993799164891, 0.2786534749902785, 0.3127934130607173, 0.24596568895503879, 0.2555394449736923, 0.2586574029410258, 0.2730316269444302, 0.24478137597907335, 0.2473662099801004, 0.26357283000834286, 0.2560108620673418, 0.24445023899897933, 0.25914515799377114, 0.2783089129952714, 0.2615338999312371, 0.26850848097819835, 0.2623047789093107, 0.3018726290902123, 0.2625411259941757, 0.2503938040463254, 0.24323028698563576, 0.27367850800510496, 0.2689930199412629, 0.2528041700134054, 0.25484329904429615, 0.2662892669904977, 0.26232699304819107, 0.24219239596277475, 0.2575413240119815, 0.25957297103013843, 0.267474681022577, 0.2640590489609167, 0.2530000030528754, 0.35217813204508275, 0.2679019389906898, 0.24603485804982483, 0.2620915040606633, 0.24381654197350144, 0.3531398499617353, 0.27268146607093513, 0.3484210280003026, 0.24447109492029995, 0.2625272989971563, 0.2774416559841484, 0.27094940294045955, 0.28492999705486, 0.2685092450119555, 0.25546827795915306, 0.26480797794647515, 0.26360189099796116, 0.2460564860375598, 0.25364594103302807, 0.26978259696625173, 0.25050212803762406, 0.24348995799664408, 0.25889838906005025, 0.27373571798671037, 0.25498310395050794, 0.25929956999607384, 0.2597665439825505, 0.25612620601896197, 0.2730219670338556, 0.26232902705669403, 0.25344362400937825, 0.2870547859929502, 0.2643372219754383, 0.392109917011112, 0.26430174394045025, 0.24857359693851322, 0.24333915195893496, 0.25582319602835923, 0.2660153420874849, 0.2555359200341627, 0.26108013198245317, 0.24661772802937776, 0.2668500490253791, 0.24665618303697556, 0.2422578529221937, 0.2564928460633382, 0.24159453297033906, 0.2585269700502977, 0.25783363706432283, 0.25683452596422285, 0.2594404189148918, 0.24499436491169035, 0.23685302201192826, 0.24360628402791917, 0.26527823007199913, 0.24365138600114733, 0.2358859470114112, 0.23899246705695987, 0.25729159102775156, 0.2602322680177167, 0.23543884395621717, 0.2602134110638872, 0.24921520298812538, 0.2851340229390189, 0.24457837699446827, 0.2486359110334888, 0.28149952995590866, 0.2584558060625568, 0.2638871589442715, 0.24050607299432158, 0.23701526096556336, 0.3474546350771561, 0.2561020830180496, 0.2710215449333191, 0.28275057300925255, 0.24132669100072235, 0.25440217193681747, 0.25624291598796844, 0.25970668299123645, 0.2617656640941277, 0.24084294389467686, 0.23407468001823872, 0.2614070239942521, 0.2674345379928127, 0.2601599770132452, 0.23418784397654235, 0.25062420195899904, 0.30821111297700554, 0.2537632090970874, 0.23273655702359974, 0.23142672004178166, 0.2513404880883172, 0.2638695890782401, 0.2502812650054693, 0.23260858899448067, 0.25857768999412656, 0.26152708497829735, 0.23318904102779925, 0.22933571599423885, 0.2517659239238128, 0.2503151120617986, 0.22842959791887552, 0.3405140279792249, 0.25131621095351875, 0.2412419019965455, 0.23352627491112798, 0.2560898750089109, 0.25614851294085383, 0.24783176102209836, 0.2566186160547659, 0.25733547599520534, 0.25737578806001693, 0.22912292391993105, 0.24586456699762493, 0.2609868220752105, 0.25367119000293314, 0.23278351000044495, 0.2504471680149436, 0.2555291630560532, 0.2633779509924352, 0.23356916406191885, 0.24177639104891568, 0.2615731239784509, 0.23935412103310227, 0.24020413600374013, 0.24560603802092373, 0.2508217680733651, 0.24919441400561482, 0.2311972320312634, 0.24966657196637243, 0.2612980179255828, 0.25431429489981383, 0.2323434998979792, 0.317240206990391, 0.2516624330310151, 0.2667264350457117, 0.2573052900843322, 0.23620988707989454, 0.2422350220149383, 0.24948663590475917, 0.2536037729587406, 0.23727063601836562, 0.23915470100473613, 0.2578999289544299, 0.2451934750424698, 0.23662888701073825, 0.24744741001632065, 0.25500851101242006, 0.2528774639358744, 0.24709614890161902, 0.298203055979684, 0.2633804939687252, 0.25758446799591184, 0.435185162932612, 0.23657804098911583, 0.24487621302250773, 0.24923326401039958, 0.282242865068838, 0.23856598802376539]
[0.009596026547528296, 0.006740962999174371, 0.006772053409473632, 0.006576738682766022, 0.006684460658685897, 0.0067076174785721705, 0.00624845554491251, 0.006581459818831222, 0.006203730509421507, 0.006261346674914104, 0.006856647815533676, 0.008632190257505796, 0.008323241393406724, 0.0064803133718669415, 0.007274265420016681, 0.005720132301279972, 0.005942777790085867, 0.006015288440488972, 0.006349572719637911, 0.005692590139048217, 0.005752702557676753, 0.006129600697868439, 0.005953740978310275, 0.005684889279046031, 0.0060266315812504915, 0.006472300302215614, 0.006082183719331095, 0.006244383278562752, 0.00610011113742583, 0.007020293699772378, 0.0061056075812598995, 0.005823111722007567, 0.005656518301991529, 0.006364616465234999, 0.006255651626540998, 0.005879166744497799, 0.005926588349867352, 0.006192773650941807, 0.006100627745306769, 0.005632381301459878, 0.005989333116557709, 0.006036580721631126, 0.006220341419129697, 0.006140908115370156, 0.00588372100122966, 0.008190189117327506, 0.0062302776509462745, 0.005721740884879647, 0.006095151257224728, 0.005670152138918638, 0.008212554650272914, 0.006341429443510119, 0.0081028146046582, 0.005685374300472092, 0.006105286023189682, 0.006452131534515079, 0.0063011489055920826, 0.006626279001275814, 0.0062444010467896625, 0.005941122743236118, 0.006158325068522678, 0.006130276534836306, 0.0057222438613386, 0.005898742814721583, 0.006274013882936086, 0.005825630884595909, 0.005662557162712653, 0.006020892768838378, 0.006365946929923497, 0.005929839626755999, 0.006030222558048229, 0.0060410824181988485, 0.005956423395789813, 0.006349348070554782, 0.006100675047830094, 0.0058940377676599594, 0.00667569269751047, 0.006147377255242751, 0.009118835279328186, 0.006146552184661634, 0.00578078132415147, 0.005659050045556627, 0.005949376651822308, 0.0061864033043601135, 0.00594269581474797, 0.00607163097633612, 0.0057352960006832035, 0.006205815093613468, 0.005736190303185478, 0.005633903556330086, 0.005964949908449726, 0.0056184775109381175, 0.0060122551174487835, 0.005996131094519136, 0.005972895952656345, 0.006033498114299809, 0.00569754337003931, 0.00550820981423089, 0.005665262419253934, 0.006169261164465096, 0.005666311302352263, 0.005485719697939795, 0.005557964350161857, 0.005983525372738409, 0.006051913209714342, 0.005475321952470167, 0.006051474675904353, 0.005795702395072684, 0.00663102378927951, 0.005687869232429495, 0.005782230489150902, 0.006546500696649038, 0.0060106001409896935, 0.006136910673122593, 0.005593164488240037, 0.005511982813152636, 0.008080340350631538, 0.005955862395768595, 0.006302826626356258, 0.006575594721145408, 0.005612248627923776, 0.005916329579925988, 0.005959137581115545, 0.006039690302121778, 0.0060875735835843655, 0.005600998695225044, 0.0054435972097264815, 0.006079233116145397, 0.00621940786029797, 0.006050232023563843, 0.005446228929687031, 0.005828469812999977, 0.007167700301790827, 0.0059014699790020326, 0.005412478070316273, 0.005382016745157713, 0.005845127629960866, 0.006136502071586979, 0.005820494535010915, 0.005409502069639085, 0.006013434651026199, 0.006082025232053426, 0.0054230009541348665, 0.005333388744052066, 0.0058550214866002976, 0.00582128167585578, 0.005312316230671524, 0.00791893088323779, 0.0058445630454306684, 0.005610276790617337, 0.005430843602584372, 0.005955578488579323, 0.005956942161415206, 0.00576352932609531, 0.0059678747919713, 0.005984545953376869, 0.0059854834432562076, 0.005328440091161187, 0.005717780627851742, 0.006069460978493268, 0.005899330000068212, 0.005413570000010348, 0.0058243527445335725, 0.005942538675722168, 0.006125068627731051, 0.005431841024695788, 0.005622706768579434, 0.006083095906475602, 0.005566374907746564, 0.005586142697761399, 0.005711768326067994, 0.0058330643737991884, 0.005795218930363135, 0.005376679814680544, 0.005806199348055173, 0.006076698091292624, 0.005914285927902647, 0.005403337206929749, 0.007377679232334675, 0.005852614721651514, 0.006202940349900272, 0.005983843955449587, 0.005493253187904524, 0.005633372604998565, 0.0058020147884827716, 0.005897762161831177, 0.005517921767868968, 0.005561737232668282, 0.005997672766382091, 0.005702173838196972, 0.00550299737234275, 0.005754590930612108, 0.005930430488660932, 0.00588087125432266, 0.005746422067479512, 0.006934954790225209, 0.006125127766714539, 0.005990336465021205, 0.010120585184479348, 0.005501814906723624, 0.005694795651686227, 0.005796122418846502, 0.006563787559740419, 0.005548046233110823]
[104.20979923795394, 148.34675700229764, 147.6656989445874, 152.05104661075322, 149.6007009481885, 149.08423194890756, 160.03954782301358, 151.94197450522253, 161.19333334697822, 159.71005151439223, 145.84386232212609, 115.84545407007077, 120.1455001403843, 154.31352507446806, 137.47092555191736, 174.82113128331557, 168.2714776359745, 166.24306712692098, 157.49091224787574, 175.6669592529626, 173.8313410043319, 163.14276398913697, 167.96162339662433, 175.9049210836711, 165.9301695346882, 154.50457384643875, 164.41463233372662, 160.1439174038283, 163.9314395216064, 142.4441829310394, 163.78386371723693, 171.72948892954463, 176.7871942795488, 157.11865836099165, 159.85544907220807, 170.09213098707926, 168.7311385516394, 161.47853229673885, 163.91755762663982, 177.54479792424675, 166.96349669305704, 165.65669310387239, 160.76287981953124, 162.8423648771242, 169.96047225743817, 122.09730272092965, 160.50649040466388, 174.7719828841977, 164.06483740902678, 176.36211083935947, 121.764790931013, 157.69315245215094, 123.41390600558888, 175.88991456850323, 163.79249001630788, 154.9875408848367, 158.70121702925155, 150.9142612026239, 160.14346171985775, 168.318353822682, 162.38181467739415, 163.12477819187035, 174.75662069495772, 169.52764875666128, 159.38759758243063, 171.65522838808633, 176.59865874465612, 166.0883258336009, 157.08582101108053, 168.63862481000416, 165.83135868929932, 165.53324897331072, 167.88598350930383, 157.49648450327024, 163.91628666661782, 169.66297798207327, 149.79718889291064, 162.67099910732767, 109.6631279508838, 162.69283493524097, 172.9869967269147, 176.70810329468281, 168.08483619770445, 161.64481214718256, 168.27379882347387, 164.70039169005005, 174.3589171127135, 161.13918718414936, 174.33173363245464, 177.4968261351279, 167.64600128216287, 177.9841599531525, 166.32694063460437, 166.77420560635622, 167.4229733660883, 165.7413296657756, 175.5142409724387, 181.54718751207008, 176.51432996314605, 162.0939644701692, 176.4816556380989, 182.29148681722796, 179.92198887905394, 167.12555520464718, 165.23700280348226, 182.63766198238892, 165.24897707690673, 172.54164065604323, 150.80627543769594, 175.81276206184225, 172.94364205582644, 152.75336341320036, 166.3727375874553, 162.94843664250018, 178.78966408060407, 181.42291692452494, 123.75716326377795, 167.90179717893758, 158.6589730738179, 152.07749905636052, 178.18169976014502, 169.02371419485894, 167.80951713029603, 165.5714034954233, 164.26906160059912, 178.5395881010504, 183.7020560987182, 164.49443225728126, 160.78701099240826, 165.28291743280246, 183.61328781995678, 171.57161863814972, 139.51476176398631, 169.4493073010777, 184.75825435382612, 185.80395553390207, 171.0826629129902, 162.95928663173856, 171.80670714230382, 184.8598978476255, 166.29431565026636, 164.4189167006099, 184.39974627655775, 187.49805198715845, 170.79356622150456, 171.78347581900698, 188.24180575439712, 126.27967269126272, 171.09918949061708, 178.24432507009394, 184.133455716554, 167.90980119188146, 167.8713630085721, 173.50479947631018, 167.5638371879584, 167.09705427789976, 167.0708823239151, 187.67218602284734, 174.89303369368952, 164.75927657224153, 169.51077495044984, 184.7209881830453, 171.69289771100262, 168.27824850100362, 163.26347683233, 184.09964420046802, 177.85028477532504, 164.38997763219152, 179.65013434656166, 179.01440297984902, 175.0771289928008, 171.43647590994792, 172.55603489984784, 185.98838585656324, 172.22970484727793, 164.56305463536395, 169.0821194968206, 185.07081118637308, 135.54397914417714, 170.86380149039027, 161.21386690685773, 167.11665736023804, 182.0414908604392, 177.51355539888965, 172.35392125939416, 169.55583703794412, 181.2276509288391, 179.79993627283307, 166.73133712882085, 175.3717140823262, 181.71914909969826, 173.77429813132372, 168.62182297086432, 170.04283153877284, 174.02132809896762, 144.19704673626654, 163.261900500141, 166.93553122419812, 98.80851569073026, 181.75820469313248, 175.5989259603899, 172.52913719496837, 152.35106116681624, 180.24363135836631]
Elapsed: 0.26257093272664966~0.03044813194512154
Time per graph: 0.006099927325706406~0.0006980274923767553
Speed: 165.62015379138518~14.984353952773247
Total Time: 0.2392
best val loss: 0.24069418013095856 test_score: 0.8605

Testing...
Test loss: 0.4999 score: 0.8140 time: 0.24s
test Score 0.8140
Epoch Time List: [1.0662560678320006, 0.9339699920965359, 0.9944040660047904, 0.8767642710590735, 0.9259646128630266, 0.9234538851305842, 0.9401062949327752, 0.907263200962916, 1.0248892289819196, 1.0475094949360937, 0.994640079094097, 1.0989962719613686, 1.2516081450739875, 1.0789345359662548, 0.9968134990194812, 0.8839127371320501, 0.9330750309163705, 1.0152327910764143, 0.9609648940386251, 0.867264827946201, 0.9107700149761513, 0.9132205790374428, 0.8917420849902555, 0.9830988220637664, 0.9229802049230784, 0.9330181078985333, 0.8867743249284104, 0.9395421339431778, 1.009582264116034, 0.9606940459925681, 0.9608422750607133, 0.9375252551399171, 0.9221321360673755, 0.9333365689963102, 1.0221898129675537, 0.9250635310309008, 0.9242681040195748, 0.9245298908790573, 0.9718292838661, 0.9031455109361559, 1.0046136500313878, 0.9195528530981392, 0.9568618499906734, 0.9251358839683235, 0.9140717320842668, 1.0211343851406127, 0.9322857429506257, 0.8855021541239694, 0.9093549700919539, 0.8649806639878079, 1.08706518902909, 0.9523320409934968, 0.9942771049682051, 0.9094507179688662, 0.9369285870343447, 0.9549685369711369, 1.1430653600255027, 1.0548496518749744, 0.9263269460061565, 0.9313946380279958, 0.9219536590389907, 0.9817257709801197, 0.8882657259237021, 0.9380715880542994, 0.955100238090381, 0.9251949781319126, 0.9893329750047997, 0.9210271950578317, 0.9434516319306567, 0.9026138191111386, 0.9210215781349689, 1.0220898960251361, 0.9372260089730844, 0.9546054899692535, 0.9342750309733674, 0.9585907378932461, 1.0895447741495445, 0.9500912689836696, 1.0408879070309922, 0.9460537498816848, 1.0318281310610473, 1.1467713539022952, 1.0113815380027518, 0.9754611679818481, 0.8790301780682057, 0.9390593869611621, 1.1408139480045065, 0.938779967953451, 0.8742974878987297, 0.9061875210609287, 0.9994442081078887, 1.1367093480657786, 0.9911306391004473, 1.0131321781082079, 0.9598164290655404, 0.962573210010305, 1.0644673440838233, 0.9053985100472346, 0.9643894451437518, 0.9969237729674205, 0.8856489469762892, 0.9233789169229567, 1.0797196039929986, 0.9636145558906719, 0.9355562259443104, 0.8943707699654624, 0.9352226250339299, 0.9924848820082843, 0.9640918720979244, 0.9220965160056949, 1.048509250045754, 0.945116254966706, 1.0389767939923331, 0.9519720100797713, 0.9070898471400142, 1.0149170509539545, 1.0288143809884787, 0.957637776969932, 1.0495384060777724, 0.9852586819324642, 0.8983729850733653, 0.9670338911237195, 1.049153353087604, 0.9503974979743361, 0.9470319098327309, 0.8982601760653779, 1.004771159030497, 1.0127368280664086, 0.9794653329299763, 0.9500653279246762, 0.9069360930006951, 0.9714519919361919, 1.011474847793579, 1.017025901004672, 0.8840375980362296, 0.9733434000518173, 0.9761115879518911, 0.965742191998288, 0.9527892550686374, 0.9238240490667522, 0.9189363250043243, 0.9516661320813, 0.8566152268322185, 0.9999557387782261, 0.9094734928803518, 0.918671437073499, 0.8918866469757631, 1.0101983280619606, 0.9165006239200011, 0.8957758849719539, 0.9180627510650083, 0.9198460918851197, 0.9349042921094224, 0.9807457179995254, 0.9360398129792884, 0.9236951520433649, 0.9331986618926749, 0.8937737831147388, 1.0066654570400715, 0.9174226688919589, 0.9221492881188169, 0.8746075051603839, 0.8903074958361685, 0.927750100963749, 0.9980434919707477, 0.9406690270407125, 0.94150321604684, 0.9518977419938892, 0.872820248012431, 1.0045479338150471, 0.9314401510637254, 0.9368878341047093, 0.9031765558756888, 0.9118789160856977, 1.0255382600007579, 0.9160760790109634, 0.9288045520661399, 0.9525420550489798, 0.9973017110023648, 0.9454478907864541, 0.994690784951672, 0.9452321890275925, 0.8972140890546143, 0.9305335651151836, 0.9983979319222271, 0.9324717429699376, 0.8952138440217823, 0.9147833039751276, 0.9499423509696499, 0.9848187291063368, 0.9385095247998834, 0.9057201641844586, 1.0125301241641864, 1.0333028660388663, 0.9657745481235906, 1.0583425249205902, 0.9211201521102339, 1.119891342939809, 1.082502624951303, 0.9277725679567084, 1.0157890090486035, 0.9267559080617502, 1.0597332620527595, 0.893298007780686]
Total Epoch List: [8, 83, 113]
Total Time List: [0.2898203480290249, 0.2569258409785107, 0.2392090520588681]
========================training times:4========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcca90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.37s
Epoch 2/1000, LR 0.000000
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.29s
Epoch 3/1000, LR 0.000030
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.30s
Epoch 4/1000, LR 0.000060
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.27s
Epoch 5/1000, LR 0.000090
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.29s
Epoch 6/1000, LR 0.000120
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.29s
Epoch 7/1000, LR 0.000150
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.27s
Epoch 8/1000, LR 0.000180
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.37s
Epoch 9/1000, LR 0.000210
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.30s
Epoch 10/1000, LR 0.000240
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.29s
Epoch 11/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.28s
Epoch 12/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.29s
Epoch 13/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.30s
Epoch 14/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.38s
Epoch 15/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.28s
Epoch 16/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.37s
Epoch 17/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.30s
Epoch 18/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.30s
Epoch 19/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.27s
Epoch 20/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.29s
Epoch 21/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.28s
Epoch 22/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.29s
Epoch 23/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.29s
Epoch 24/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.36s
Val loss: 0.6909 score: 0.5581 time: 0.24s
Test loss: 0.6902 score: 0.5227 time: 0.28s
Epoch 25/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.47s
Val loss: 0.6906 score: 0.5814 time: 0.33s
Test loss: 0.6898 score: 0.5455 time: 0.30s
Epoch 26/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.37s
Val loss: 0.6903 score: 0.5814 time: 0.26s
Test loss: 0.6894 score: 0.5909 time: 0.35s
Epoch 27/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.35s
Val loss: 0.6900 score: 0.5581 time: 0.26s
Test loss: 0.6889 score: 0.5909 time: 0.29s
Epoch 28/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.36s
Val loss: 0.6896 score: 0.5814 time: 0.25s
Test loss: 0.6884 score: 0.6591 time: 0.29s
Epoch 29/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.46s
Val loss: 0.6892 score: 0.6047 time: 0.25s
Test loss: 0.6878 score: 0.7045 time: 0.29s
Epoch 30/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.32s
Val loss: 0.6887 score: 0.6512 time: 0.33s
Test loss: 0.6872 score: 0.7500 time: 0.29s
Epoch 31/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.36s
Val loss: 0.6882 score: 0.6512 time: 0.27s
Test loss: 0.6865 score: 0.7273 time: 0.29s
Epoch 32/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.35s
Val loss: 0.6876 score: 0.6744 time: 0.24s
Test loss: 0.6857 score: 0.7273 time: 0.29s
Epoch 33/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.37s
Val loss: 0.6869 score: 0.6744 time: 0.25s
Test loss: 0.6849 score: 0.7273 time: 0.30s
Epoch 34/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.35s
Val loss: 0.6862 score: 0.6744 time: 0.27s
Test loss: 0.6839 score: 0.7273 time: 0.29s
Epoch 35/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.37s
Val loss: 0.6853 score: 0.6744 time: 0.24s
Test loss: 0.6829 score: 0.7273 time: 0.29s
Epoch 36/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.46s
Val loss: 0.6843 score: 0.6744 time: 0.24s
Test loss: 0.6817 score: 0.7273 time: 0.29s
Epoch 37/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.44s
Val loss: 0.6833 score: 0.6744 time: 0.26s
Test loss: 0.6804 score: 0.7273 time: 0.27s
Epoch 38/1000, LR 0.000270
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.37s
Val loss: 0.6821 score: 0.6744 time: 0.26s
Test loss: 0.6790 score: 0.7273 time: 0.29s
Epoch 39/1000, LR 0.000269
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.38s
Val loss: 0.6807 score: 0.6977 time: 0.24s
Test loss: 0.6774 score: 0.7273 time: 0.31s
Epoch 40/1000, LR 0.000269
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.47s
Val loss: 0.6792 score: 0.6977 time: 0.25s
Test loss: 0.6757 score: 0.7273 time: 0.30s
Epoch 41/1000, LR 0.000269
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.33s
Val loss: 0.6776 score: 0.6977 time: 0.28s
Test loss: 0.6738 score: 0.7273 time: 0.28s
Epoch 42/1000, LR 0.000269
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.37s
Val loss: 0.6758 score: 0.6977 time: 0.31s
Test loss: 0.6717 score: 0.7273 time: 0.30s
Epoch 43/1000, LR 0.000269
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.41s
Val loss: 0.6737 score: 0.7209 time: 0.24s
Test loss: 0.6695 score: 0.7500 time: 0.37s
Epoch 44/1000, LR 0.000269
Train loss: 0.6668;  Loss pred: 0.6668; Loss self: 0.0000; time: 0.38s
Val loss: 0.6715 score: 0.7442 time: 0.26s
Test loss: 0.6671 score: 0.7273 time: 0.30s
Epoch 45/1000, LR 0.000269
Train loss: 0.6651;  Loss pred: 0.6651; Loss self: 0.0000; time: 0.33s
Val loss: 0.6690 score: 0.7442 time: 0.27s
Test loss: 0.6645 score: 0.7273 time: 0.29s
Epoch 46/1000, LR 0.000269
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.37s
Val loss: 0.6664 score: 0.7442 time: 0.25s
Test loss: 0.6617 score: 0.7273 time: 0.29s
Epoch 47/1000, LR 0.000269
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.47s
Val loss: 0.6634 score: 0.8140 time: 0.26s
Test loss: 0.6587 score: 0.7273 time: 0.30s
Epoch 48/1000, LR 0.000269
Train loss: 0.6564;  Loss pred: 0.6564; Loss self: 0.0000; time: 0.32s
Val loss: 0.6603 score: 0.8605 time: 0.27s
Test loss: 0.6554 score: 0.7955 time: 0.28s
Epoch 49/1000, LR 0.000269
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.37s
Val loss: 0.6570 score: 0.8140 time: 0.27s
Test loss: 0.6520 score: 0.8182 time: 0.37s
Epoch 50/1000, LR 0.000269
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 0.38s
Val loss: 0.6534 score: 0.8372 time: 0.26s
Test loss: 0.6483 score: 0.8636 time: 0.29s
Epoch 51/1000, LR 0.000269
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.36s
Val loss: 0.6495 score: 0.8372 time: 0.25s
Test loss: 0.6443 score: 0.8864 time: 0.28s
Epoch 52/1000, LR 0.000269
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.38s
Val loss: 0.6453 score: 0.8372 time: 0.28s
Test loss: 0.6400 score: 0.8864 time: 0.29s
Epoch 53/1000, LR 0.000269
Train loss: 0.6337;  Loss pred: 0.6337; Loss self: 0.0000; time: 0.38s
Val loss: 0.6408 score: 0.8605 time: 0.28s
Test loss: 0.6354 score: 0.8864 time: 0.31s
Epoch 54/1000, LR 0.000269
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.40s
Val loss: 0.6359 score: 0.8605 time: 0.24s
Test loss: 0.6305 score: 0.8864 time: 0.29s
Epoch 55/1000, LR 0.000269
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.44s
Val loss: 0.6308 score: 0.8605 time: 0.28s
Test loss: 0.6253 score: 0.8864 time: 0.27s
Epoch 56/1000, LR 0.000269
Train loss: 0.6164;  Loss pred: 0.6164; Loss self: 0.0000; time: 0.36s
Val loss: 0.6253 score: 0.8605 time: 0.26s
Test loss: 0.6198 score: 0.8864 time: 0.29s
Epoch 57/1000, LR 0.000269
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.53s
Val loss: 0.6195 score: 0.8605 time: 0.24s
Test loss: 0.6141 score: 0.8864 time: 0.30s
Epoch 58/1000, LR 0.000269
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.33s
Val loss: 0.6133 score: 0.8605 time: 0.25s
Test loss: 0.6081 score: 0.8864 time: 0.28s
Epoch 59/1000, LR 0.000268
Train loss: 0.5945;  Loss pred: 0.5945; Loss self: 0.0000; time: 0.39s
Val loss: 0.6068 score: 0.8605 time: 0.33s
Test loss: 0.6017 score: 0.8864 time: 0.28s
Epoch 60/1000, LR 0.000268
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 0.37s
Val loss: 0.6000 score: 0.8837 time: 0.31s
Test loss: 0.5952 score: 0.8864 time: 0.31s
Epoch 61/1000, LR 0.000268
Train loss: 0.5799;  Loss pred: 0.5799; Loss self: 0.0000; time: 0.39s
Val loss: 0.5928 score: 0.8837 time: 0.24s
Test loss: 0.5882 score: 0.8864 time: 0.30s
Epoch 62/1000, LR 0.000268
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.32s
Val loss: 0.5853 score: 0.8837 time: 0.26s
Test loss: 0.5811 score: 0.8864 time: 0.27s
Epoch 63/1000, LR 0.000268
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 0.36s
Val loss: 0.5775 score: 0.8837 time: 0.26s
Test loss: 0.5739 score: 0.8864 time: 0.32s
Epoch 64/1000, LR 0.000268
Train loss: 0.5557;  Loss pred: 0.5557; Loss self: 0.0000; time: 0.47s
Val loss: 0.5692 score: 0.8837 time: 0.24s
Test loss: 0.5662 score: 0.8864 time: 0.30s
Epoch 65/1000, LR 0.000268
Train loss: 0.5398;  Loss pred: 0.5398; Loss self: 0.0000; time: 0.41s
Val loss: 0.5606 score: 0.8837 time: 0.25s
Test loss: 0.5576 score: 0.8864 time: 0.37s
Epoch 66/1000, LR 0.000268
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.35s
Val loss: 0.5516 score: 0.8837 time: 0.26s
Test loss: 0.5489 score: 0.8864 time: 0.29s
Epoch 67/1000, LR 0.000268
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.39s
Val loss: 0.5423 score: 0.8837 time: 0.37s
Test loss: 0.5398 score: 0.8864 time: 0.29s
Epoch 68/1000, LR 0.000268
Train loss: 0.5029;  Loss pred: 0.5029; Loss self: 0.0000; time: 0.37s
Val loss: 0.5328 score: 0.8837 time: 0.24s
Test loss: 0.5308 score: 0.8864 time: 0.32s
Epoch 69/1000, LR 0.000268
Train loss: 0.4949;  Loss pred: 0.4949; Loss self: 0.0000; time: 0.43s
Val loss: 0.5231 score: 0.8837 time: 0.25s
Test loss: 0.5218 score: 0.8864 time: 0.38s
Epoch 70/1000, LR 0.000268
Train loss: 0.4737;  Loss pred: 0.4737; Loss self: 0.0000; time: 0.37s
Val loss: 0.5133 score: 0.9070 time: 0.25s
Test loss: 0.5130 score: 0.8864 time: 0.28s
Epoch 71/1000, LR 0.000268
Train loss: 0.4641;  Loss pred: 0.4641; Loss self: 0.0000; time: 0.36s
Val loss: 0.5032 score: 0.9070 time: 0.26s
Test loss: 0.5039 score: 0.8864 time: 0.29s
Epoch 72/1000, LR 0.000267
Train loss: 0.4529;  Loss pred: 0.4529; Loss self: 0.0000; time: 0.38s
Val loss: 0.4929 score: 0.9070 time: 0.24s
Test loss: 0.4948 score: 0.8864 time: 0.30s
Epoch 73/1000, LR 0.000267
Train loss: 0.4403;  Loss pred: 0.4403; Loss self: 0.0000; time: 0.39s
Val loss: 0.4825 score: 0.9070 time: 0.25s
Test loss: 0.4856 score: 0.8864 time: 0.29s
Epoch 74/1000, LR 0.000267
Train loss: 0.4316;  Loss pred: 0.4316; Loss self: 0.0000; time: 0.37s
Val loss: 0.4719 score: 0.9070 time: 0.26s
Test loss: 0.4763 score: 0.8864 time: 0.29s
Epoch 75/1000, LR 0.000267
Train loss: 0.4068;  Loss pred: 0.4068; Loss self: 0.0000; time: 0.37s
Val loss: 0.4613 score: 0.9302 time: 0.26s
Test loss: 0.4672 score: 0.8864 time: 0.28s
Epoch 76/1000, LR 0.000267
Train loss: 0.3941;  Loss pred: 0.3941; Loss self: 0.0000; time: 0.41s
Val loss: 0.4510 score: 0.9302 time: 0.25s
Test loss: 0.4588 score: 0.8864 time: 0.37s
Epoch 77/1000, LR 0.000267
Train loss: 0.3807;  Loss pred: 0.3807; Loss self: 0.0000; time: 0.32s
Val loss: 0.4408 score: 0.9302 time: 0.27s
Test loss: 0.4509 score: 0.8864 time: 0.29s
Epoch 78/1000, LR 0.000267
Train loss: 0.3779;  Loss pred: 0.3779; Loss self: 0.0000; time: 0.37s
Val loss: 0.4306 score: 0.9302 time: 0.25s
Test loss: 0.4430 score: 0.8636 time: 0.29s
Epoch 79/1000, LR 0.000267
Train loss: 0.3577;  Loss pred: 0.3577; Loss self: 0.0000; time: 0.50s
Val loss: 0.4202 score: 0.9302 time: 0.24s
Test loss: 0.4340 score: 0.8636 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.3415;  Loss pred: 0.3415; Loss self: 0.0000; time: 0.36s
Val loss: 0.4100 score: 0.9302 time: 0.25s
Test loss: 0.4250 score: 0.8636 time: 0.28s
Epoch 81/1000, LR 0.000267
Train loss: 0.3393;  Loss pred: 0.3393; Loss self: 0.0000; time: 0.36s
Val loss: 0.3998 score: 0.9302 time: 0.34s
Test loss: 0.4139 score: 0.8864 time: 0.29s
Epoch 82/1000, LR 0.000267
Train loss: 0.3160;  Loss pred: 0.3160; Loss self: 0.0000; time: 0.42s
Val loss: 0.3905 score: 0.9302 time: 0.29s
Test loss: 0.4034 score: 0.8864 time: 0.37s
Epoch 83/1000, LR 0.000266
Train loss: 0.2873;  Loss pred: 0.2873; Loss self: 0.0000; time: 0.42s
Val loss: 0.3822 score: 0.9302 time: 0.25s
Test loss: 0.3943 score: 0.8864 time: 0.29s
Epoch 84/1000, LR 0.000266
Train loss: 0.2870;  Loss pred: 0.2870; Loss self: 0.0000; time: 0.38s
Val loss: 0.3741 score: 0.9302 time: 0.24s
Test loss: 0.3874 score: 0.8864 time: 0.32s
Epoch 85/1000, LR 0.000266
Train loss: 0.2887;  Loss pred: 0.2887; Loss self: 0.0000; time: 0.36s
Val loss: 0.3664 score: 0.9302 time: 0.29s
Test loss: 0.3819 score: 0.8864 time: 0.28s
Epoch 86/1000, LR 0.000266
Train loss: 0.2486;  Loss pred: 0.2486; Loss self: 0.0000; time: 0.39s
Val loss: 0.3591 score: 0.9302 time: 0.36s
Test loss: 0.3790 score: 0.8864 time: 0.29s
Epoch 87/1000, LR 0.000266
Train loss: 0.2302;  Loss pred: 0.2302; Loss self: 0.0000; time: 0.47s
Val loss: 0.3526 score: 0.9302 time: 0.26s
Test loss: 0.3774 score: 0.8636 time: 0.30s
Epoch 88/1000, LR 0.000266
Train loss: 0.2257;  Loss pred: 0.2257; Loss self: 0.0000; time: 0.45s
Val loss: 0.3472 score: 0.9070 time: 0.26s
Test loss: 0.3781 score: 0.8636 time: 0.30s
Epoch 89/1000, LR 0.000266
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.55s
Val loss: 0.3423 score: 0.9070 time: 0.24s
Test loss: 0.3779 score: 0.8636 time: 0.30s
Epoch 90/1000, LR 0.000266
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.33s
Val loss: 0.3384 score: 0.9070 time: 0.26s
Test loss: 0.3788 score: 0.8636 time: 0.28s
Epoch 91/1000, LR 0.000266
Train loss: 0.2075;  Loss pred: 0.2075; Loss self: 0.0000; time: 0.36s
Val loss: 0.3349 score: 0.9070 time: 0.30s
Test loss: 0.3782 score: 0.8636 time: 0.29s
Epoch 92/1000, LR 0.000266
Train loss: 0.1856;  Loss pred: 0.1856; Loss self: 0.0000; time: 0.46s
Val loss: 0.3328 score: 0.9070 time: 0.25s
Test loss: 0.3810 score: 0.8636 time: 0.29s
Epoch 93/1000, LR 0.000265
Train loss: 0.1802;  Loss pred: 0.1802; Loss self: 0.0000; time: 0.40s
Val loss: 0.3305 score: 0.9070 time: 0.32s
Test loss: 0.3795 score: 0.8636 time: 0.34s
Epoch 94/1000, LR 0.000265
Train loss: 0.1505;  Loss pred: 0.1505; Loss self: 0.0000; time: 0.38s
Val loss: 0.3294 score: 0.8837 time: 0.25s
Test loss: 0.3800 score: 0.8864 time: 0.34s
Epoch 95/1000, LR 0.000265
Train loss: 0.1810;  Loss pred: 0.1810; Loss self: 0.0000; time: 0.34s
Val loss: 0.3288 score: 0.8837 time: 0.33s
Test loss: 0.3787 score: 0.8636 time: 0.28s
Epoch 96/1000, LR 0.000265
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.47s
Val loss: 0.3297 score: 0.8837 time: 0.26s
Test loss: 0.3815 score: 0.8636 time: 0.28s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.1501;  Loss pred: 0.1501; Loss self: 0.0000; time: 0.38s
Val loss: 0.3312 score: 0.8837 time: 0.24s
Test loss: 0.3828 score: 0.8636 time: 0.29s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.1810,   Val_Loss: 0.3288,   Val_Precision: 0.8696,   Val_Recall: 0.9091,   Val_accuracy: 0.8889,   Val_Score: 0.8837,   Val_Loss: 0.3288,   Test_Precision: 0.8636,   Test_Recall: 0.8636,   Test_accuracy: 0.8636,   Test_Score: 0.8636,   Test_loss: 0.3787


[0.36952373501844704, 0.29176156502217054, 0.30289753898978233, 0.27685029699932784, 0.29375023394823074, 0.29798658192157745, 0.2773585630347952, 0.3693898960482329, 0.30624698207248, 0.29923670704010874, 0.28760483302176, 0.2983795600011945, 0.30848910799250007, 0.38577801606152207, 0.2859298449475318, 0.3704615340102464, 0.30242245795670897, 0.3062286979984492, 0.27652376296464354, 0.2924599249381572, 0.28607970906887203, 0.29042193898931146, 0.2973043540259823, 0.285168417962268, 0.30186239595059305, 0.35106380993966013, 0.29120523494202644, 0.2913693320006132, 0.2960737090324983, 0.29519673506729305, 0.2912233849056065, 0.2961235570255667, 0.2992687710793689, 0.29015659308061004, 0.29325354692991823, 0.29522937908768654, 0.2791310528991744, 0.2940690499963239, 0.314724285970442, 0.3021737849339843, 0.2849383009597659, 0.2995733559364453, 0.37446534202899784, 0.30114319804124534, 0.2919802189571783, 0.2989457229850814, 0.3054814119823277, 0.28103991597890854, 0.3735915549332276, 0.2958805999951437, 0.28198662400245667, 0.29058063100092113, 0.3104076029267162, 0.2912619770504534, 0.2757169050164521, 0.29464187601115555, 0.30556349793914706, 0.2798997019417584, 0.2859817040152848, 0.3113266780273989, 0.301602088031359, 0.279124666005373, 0.3222534799715504, 0.2995140759740025, 0.37784648302476853, 0.2972180239157751, 0.29439367400482297, 0.3266004309989512, 0.3825532579794526, 0.2830586319323629, 0.293835990014486, 0.30552916310261935, 0.29101266199722886, 0.2925049050245434, 0.2835992069449276, 0.3753322159172967, 0.2920607980340719, 0.29411495104432106, 0.30546028702519834, 0.283623980008997, 0.2938370719784871, 0.3771296429913491, 0.29044601402711123, 0.32010544498916715, 0.28566729405429214, 0.28989384102169424, 0.30628324300050735, 0.30827657599002123, 0.3063169269589707, 0.2827313320012763, 0.29227845498826355, 0.29155204200651497, 0.34079158992972225, 0.3467241380130872, 0.2819166959961876, 0.28713195002637804, 0.2918478609062731]
[0.008398266704964706, 0.006630944659594785, 0.006884034977040507, 0.006292052204530178, 0.006676141680641608, 0.0067724223163994875, 0.006303603705336255, 0.008395224910187111, 0.006960158683465455, 0.006800834250911562, 0.006536473477767272, 0.006781353636390783, 0.0070111160907386375, 0.008767682183216411, 0.00649840556698936, 0.008419580318414692, 0.006873237680834295, 0.006959743136328391, 0.006284630976469171, 0.006646816475867209, 0.006501811569747092, 0.006600498613393443, 0.006756917136954144, 0.006481100408233364, 0.006860508998877115, 0.007978722953174094, 0.006618300794136964, 0.006622030272741209, 0.00672894793255678, 0.006709016706074842, 0.006618713293309239, 0.006730080841490152, 0.006801562979076566, 0.006594468024559319, 0.006664853339316323, 0.006709758615629239, 0.006343887565890327, 0.006683387499916452, 0.007152824681146409, 0.006867586021226915, 0.006475870476358316, 0.006808485362191938, 0.008510575955204496, 0.006844163591846485, 0.006635914067208598, 0.006794220976933668, 0.006942759363234721, 0.006387270817702467, 0.008490717157573354, 0.006724559090798721, 0.006408786909146743, 0.006604105250020934, 0.0070547182483344595, 0.006619590387510305, 0.006266293295828456, 0.0066964062729808075, 0.0069446249531624335, 0.006361356862312691, 0.006499584182165563, 0.00707560631880452, 0.006854592909803614, 0.006343742409213023, 0.0073239427266261455, 0.006807138090318238, 0.00858742006874474, 0.006754955088994889, 0.006690765318291431, 0.007422737068157981, 0.008694392226805741, 0.006433150725735521, 0.006678090682147409, 0.006943844615968622, 0.006613924136300656, 0.006647838750557805, 0.006445436521475626, 0.008530277634484017, 0.006637745409865271, 0.0066844307055527515, 0.006942279250572689, 0.006445999545659023, 0.006678115272238342, 0.008571128249803389, 0.006601045773343437, 0.007275123749753799, 0.0064924385012339126, 0.006588496386856687, 0.006960982795466076, 0.007006285817955028, 0.006961748339976607, 0.006425712090938098, 0.006642692158824171, 0.0066261827728753405, 0.007745263407493688, 0.007880094045751983, 0.006407197636276992, 0.006525726136963137, 0.006632905929688026]
[119.07218895642379, 150.8080750686147, 145.2636431010562, 158.93065847101775, 149.78711474917282, 147.6576553087202, 158.63941433270298, 119.11533171512282, 143.67488522576343, 147.04078398410653, 152.98769334891875, 147.46318413977104, 142.6306435463184, 114.05522909056354, 153.88390116489592, 118.77076554669513, 145.4918404449265, 143.6834636583369, 159.11833228461404, 150.4479631159863, 153.80328840241953, 151.50370579138428, 147.9964871155392, 154.29478591777948, 145.7617795069832, 125.3333404191181, 151.0961848222254, 151.01108856544792, 148.61164182318657, 149.0531390530785, 151.08676802950288, 148.58662526534872, 147.02502984626747, 151.64225473165834, 150.04081096592884, 149.03665799104462, 157.63204968776208, 149.62472249476795, 139.80490849102236, 145.61157252477295, 154.41939483668403, 146.87554526489544, 117.50086072476296, 146.10989152733129, 150.6951400925315, 147.1839087063834, 144.03495032472034, 156.5613903873431, 117.77568154040357, 148.70863449892346, 156.0357699789926, 151.4209665263633, 141.74910532197214, 151.06674906755222, 159.58397617068317, 149.33383060028473, 143.99625706851475, 157.1991670400404, 153.8559963180314, 141.3306443212287, 145.88758415832024, 157.63565660353723, 136.53847897588093, 146.90461493976383, 116.4494099502197, 148.0394742563412, 149.45973329331514, 134.72119392316813, 115.01666521518239, 155.44482674710954, 149.7433993631305, 144.0124391177072, 151.196170290415, 150.42482790607585, 155.1485297649101, 117.22947866989192, 150.65356355996448, 149.6013713133866, 144.04491146297622, 155.13497835621743, 149.74284797944557, 116.67075452090438, 151.49114766606124, 137.45470653112142, 154.0253326712215, 151.77969923378694, 143.65787553035383, 142.72897594861135, 143.64207827761842, 155.62477525413198, 150.54137330022093, 150.91645284726485, 129.1111673532602, 126.90203875664176, 156.07447386016122, 153.23965165129775, 150.7634829440487]
Elapsed: 0.3046283999220324~0.027478563657224352
Time per graph: 0.006923372725500736~0.0006245128103914627
Speed: 145.46568671350798~11.430661013879162
Total Time: 0.2923
best val loss: 0.3287575840950012 test_score: 0.8636

Testing...
Test loss: 0.4672 score: 0.8864 time: 0.36s
test Score 0.8864
Epoch Time List: [0.9872064890805632, 0.9171857718611136, 0.9412238380173221, 0.8621066370978951, 0.9233129039639607, 0.9179292449261993, 0.8681507860310376, 0.9766329169506207, 0.9311045568902045, 0.949998828000389, 0.9062795559875667, 0.9235974840121344, 0.9447191280778497, 1.0073519999859855, 0.88636977202259, 1.0117616989882663, 0.9057127689011395, 1.022983851027675, 0.851082180859521, 0.9888799529289827, 0.8906996871810406, 0.9022106871707365, 0.9658788319211453, 0.8790028251241893, 1.097608622862026, 0.9715511448448524, 0.8977847038768232, 0.8970914529636502, 1.0034707499435171, 0.9394364199833944, 0.9161871860269457, 0.883654656005092, 0.9158092540455982, 0.9047825869638473, 0.8982818530639634, 0.99709671898745, 0.9707248149206862, 0.9129735750611871, 0.9275921410880983, 1.0157559911021963, 0.889595712069422, 0.9773796498775482, 1.017859356943518, 0.9356499480782077, 0.8873381270095706, 0.9236242540646344, 1.0236177049810067, 0.8691154149128124, 1.0107488799840212, 0.933516840916127, 0.8936825640266761, 0.9507751279743388, 0.9676882581552491, 0.9255808771122247, 0.9954710860038176, 0.9090603881049901, 1.075700337998569, 0.8561906120739877, 1.0030003801221028, 0.990406752214767, 0.925515174982138, 0.8540504378033802, 0.9349362479988486, 1.0044925319962204, 1.0278184668859467, 0.9069808259373531, 1.0548239500494674, 0.9354174728505313, 1.0633592320373282, 0.9001998530002311, 0.9029836760601029, 0.9230084370356053, 0.9239845089614391, 0.9143341329181567, 0.9092096789972857, 1.0247330599231645, 0.8683197769569233, 0.9046093750512227, 1.04873417399358, 0.8880371160339564, 0.9951755091315135, 1.075993465143256, 0.9567119519924745, 0.9391264340374619, 0.9289557769661769, 1.0378514809999615, 1.035478727077134, 1.0129994129529223, 1.0916588800027966, 0.8711498159682378, 0.9565573589643463, 1.0027554730186239, 1.0611365520162508, 0.9693381859688088, 0.9545569409383461, 1.0181502680061385, 0.9121760150883347]
Total Epoch List: [97]
Total Time List: [0.29230970598291606]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fccbe0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.28s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.27s
Epoch 3/1000, LR 0.000030
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.26s
Epoch 5/1000, LR 0.000090
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.35s
Epoch 6/1000, LR 0.000120
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.26s
Epoch 10/1000, LR 0.000240
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.33s
Epoch 11/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.27s
Epoch 13/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.38s
Epoch 16/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.32s
Epoch 17/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.28s
Epoch 19/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.30s
Epoch 20/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.37s
Val loss: 0.6903 score: 0.5227 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.33s
Epoch 21/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.40s
Val loss: 0.6899 score: 0.5227 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.40s
Val loss: 0.6894 score: 0.5682 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.27s
Epoch 23/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.36s
Val loss: 0.6889 score: 0.5909 time: 0.26s
Test loss: 0.6912 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.41s
Val loss: 0.6883 score: 0.6364 time: 0.30s
Test loss: 0.6908 score: 0.5116 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.42s
Val loss: 0.6876 score: 0.6364 time: 0.24s
Test loss: 0.6904 score: 0.6047 time: 0.35s
Epoch 26/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.40s
Val loss: 0.6869 score: 0.6591 time: 0.25s
Test loss: 0.6900 score: 0.6047 time: 0.27s
Epoch 27/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.38s
Val loss: 0.6862 score: 0.6591 time: 0.24s
Test loss: 0.6895 score: 0.5814 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.52s
Val loss: 0.6853 score: 0.6591 time: 0.24s
Test loss: 0.6890 score: 0.5116 time: 0.35s
Epoch 29/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.37s
Val loss: 0.6844 score: 0.7273 time: 0.25s
Test loss: 0.6884 score: 0.5116 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.40s
Val loss: 0.6834 score: 0.7273 time: 0.26s
Test loss: 0.6878 score: 0.5349 time: 0.32s
Epoch 31/1000, LR 0.000270
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.46s
Val loss: 0.6823 score: 0.7500 time: 0.28s
Test loss: 0.6870 score: 0.5581 time: 0.24s
Epoch 32/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.42s
Val loss: 0.6812 score: 0.7500 time: 0.23s
Test loss: 0.6862 score: 0.6279 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.35s
Val loss: 0.6800 score: 0.7727 time: 0.23s
Test loss: 0.6854 score: 0.6512 time: 0.43s
Epoch 34/1000, LR 0.000270
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 0.38s
Val loss: 0.6787 score: 0.7500 time: 0.29s
Test loss: 0.6845 score: 0.6279 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.43s
Val loss: 0.6773 score: 0.7500 time: 0.38s
Test loss: 0.6836 score: 0.6744 time: 0.35s
Epoch 36/1000, LR 0.000270
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.42s
Val loss: 0.6759 score: 0.7727 time: 0.45s
Test loss: 0.6826 score: 0.6512 time: 0.30s
Epoch 37/1000, LR 0.000270
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.41s
Val loss: 0.6743 score: 0.7727 time: 0.28s
Test loss: 0.6816 score: 0.6512 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6641;  Loss pred: 0.6641; Loss self: 0.0000; time: 0.50s
Val loss: 0.6726 score: 0.7727 time: 0.26s
Test loss: 0.6805 score: 0.6512 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.37s
Val loss: 0.6707 score: 0.7727 time: 0.27s
Test loss: 0.6792 score: 0.6512 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 0.41s
Val loss: 0.6687 score: 0.7727 time: 0.28s
Test loss: 0.6779 score: 0.6512 time: 0.35s
Epoch 41/1000, LR 0.000269
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.42s
Val loss: 0.6665 score: 0.8182 time: 0.26s
Test loss: 0.6764 score: 0.6512 time: 0.26s
Epoch 42/1000, LR 0.000269
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 0.42s
Val loss: 0.6640 score: 0.8182 time: 0.26s
Test loss: 0.6748 score: 0.6512 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6454;  Loss pred: 0.6454; Loss self: 0.0000; time: 0.40s
Val loss: 0.6614 score: 0.8182 time: 0.27s
Test loss: 0.6731 score: 0.6744 time: 0.27s
Epoch 44/1000, LR 0.000269
Train loss: 0.6421;  Loss pred: 0.6421; Loss self: 0.0000; time: 0.42s
Val loss: 0.6585 score: 0.8409 time: 0.26s
Test loss: 0.6712 score: 0.6279 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6368;  Loss pred: 0.6368; Loss self: 0.0000; time: 0.42s
Val loss: 0.6554 score: 0.8182 time: 0.26s
Test loss: 0.6691 score: 0.6047 time: 0.34s
Epoch 46/1000, LR 0.000269
Train loss: 0.6308;  Loss pred: 0.6308; Loss self: 0.0000; time: 0.42s
Val loss: 0.6521 score: 0.8409 time: 0.26s
Test loss: 0.6668 score: 0.6512 time: 0.26s
Epoch 47/1000, LR 0.000269
Train loss: 0.6254;  Loss pred: 0.6254; Loss self: 0.0000; time: 0.46s
Val loss: 0.6484 score: 0.8409 time: 0.26s
Test loss: 0.6643 score: 0.6512 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.41s
Val loss: 0.6444 score: 0.8636 time: 0.26s
Test loss: 0.6615 score: 0.6279 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.46s
Val loss: 0.6401 score: 0.8636 time: 0.24s
Test loss: 0.6586 score: 0.6512 time: 0.27s
Epoch 50/1000, LR 0.000269
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.40s
Val loss: 0.6355 score: 0.8636 time: 0.25s
Test loss: 0.6554 score: 0.6744 time: 0.34s
Epoch 51/1000, LR 0.000269
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.48s
Val loss: 0.6305 score: 0.8636 time: 0.26s
Test loss: 0.6518 score: 0.6744 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.40s
Val loss: 0.6252 score: 0.8636 time: 0.27s
Test loss: 0.6481 score: 0.6744 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.5795;  Loss pred: 0.5795; Loss self: 0.0000; time: 0.42s
Val loss: 0.6197 score: 0.8636 time: 0.24s
Test loss: 0.6441 score: 0.6744 time: 0.27s
Epoch 54/1000, LR 0.000269
Train loss: 0.5712;  Loss pred: 0.5712; Loss self: 0.0000; time: 0.36s
Val loss: 0.6139 score: 0.8409 time: 0.26s
Test loss: 0.6400 score: 0.6744 time: 0.24s
Epoch 55/1000, LR 0.000269
Train loss: 0.5555;  Loss pred: 0.5555; Loss self: 0.0000; time: 0.40s
Val loss: 0.6077 score: 0.8409 time: 0.29s
Test loss: 0.6356 score: 0.6977 time: 0.36s
Epoch 56/1000, LR 0.000269
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.41s
Val loss: 0.6013 score: 0.8409 time: 0.26s
Test loss: 0.6311 score: 0.6977 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 0.43s
Val loss: 0.5946 score: 0.8409 time: 0.26s
Test loss: 0.6264 score: 0.6977 time: 0.27s
Epoch 58/1000, LR 0.000269
Train loss: 0.5225;  Loss pred: 0.5225; Loss self: 0.0000; time: 0.36s
Val loss: 0.5875 score: 0.8182 time: 0.26s
Test loss: 0.6214 score: 0.6977 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.5126;  Loss pred: 0.5126; Loss self: 0.0000; time: 0.41s
Val loss: 0.5803 score: 0.8182 time: 0.26s
Test loss: 0.6163 score: 0.7209 time: 0.27s
Epoch 60/1000, LR 0.000268
Train loss: 0.5006;  Loss pred: 0.5006; Loss self: 0.0000; time: 0.46s
Val loss: 0.5728 score: 0.8182 time: 0.24s
Test loss: 0.6108 score: 0.7674 time: 0.33s
Epoch 61/1000, LR 0.000268
Train loss: 0.4882;  Loss pred: 0.4882; Loss self: 0.0000; time: 0.41s
Val loss: 0.5651 score: 0.8182 time: 0.24s
Test loss: 0.6051 score: 0.7674 time: 0.28s
Epoch 62/1000, LR 0.000268
Train loss: 0.4739;  Loss pred: 0.4739; Loss self: 0.0000; time: 0.37s
Val loss: 0.5572 score: 0.8182 time: 0.26s
Test loss: 0.5994 score: 0.7907 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.4605;  Loss pred: 0.4605; Loss self: 0.0000; time: 0.48s
Val loss: 0.5492 score: 0.8182 time: 0.24s
Test loss: 0.5935 score: 0.7907 time: 0.26s
Epoch 64/1000, LR 0.000268
Train loss: 0.4547;  Loss pred: 0.4547; Loss self: 0.0000; time: 0.37s
Val loss: 0.5410 score: 0.8182 time: 0.26s
Test loss: 0.5874 score: 0.7907 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.4322;  Loss pred: 0.4322; Loss self: 0.0000; time: 0.43s
Val loss: 0.5327 score: 0.8182 time: 0.29s
Test loss: 0.5809 score: 0.7907 time: 0.35s
Epoch 66/1000, LR 0.000268
Train loss: 0.4268;  Loss pred: 0.4268; Loss self: 0.0000; time: 0.40s
Val loss: 0.5244 score: 0.8182 time: 0.24s
Test loss: 0.5746 score: 0.7907 time: 0.26s
Epoch 67/1000, LR 0.000268
Train loss: 0.3975;  Loss pred: 0.3975; Loss self: 0.0000; time: 0.36s
Val loss: 0.5160 score: 0.8182 time: 0.25s
Test loss: 0.5679 score: 0.7907 time: 0.24s
Epoch 68/1000, LR 0.000268
Train loss: 0.3854;  Loss pred: 0.3854; Loss self: 0.0000; time: 0.40s
Val loss: 0.5075 score: 0.8182 time: 0.29s
Test loss: 0.5611 score: 0.7907 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.3745;  Loss pred: 0.3745; Loss self: 0.0000; time: 0.44s
Val loss: 0.4992 score: 0.8182 time: 0.26s
Test loss: 0.5544 score: 0.7907 time: 0.26s
Epoch 70/1000, LR 0.000268
Train loss: 0.3583;  Loss pred: 0.3583; Loss self: 0.0000; time: 0.37s
Val loss: 0.4912 score: 0.8182 time: 0.29s
Test loss: 0.5478 score: 0.7907 time: 0.36s
Epoch 71/1000, LR 0.000268
Train loss: 0.3358;  Loss pred: 0.3358; Loss self: 0.0000; time: 0.40s
Val loss: 0.4835 score: 0.7955 time: 0.28s
Test loss: 0.5412 score: 0.8140 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.3265;  Loss pred: 0.3265; Loss self: 0.0000; time: 0.42s
Val loss: 0.4760 score: 0.7955 time: 0.26s
Test loss: 0.5345 score: 0.8140 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.3291;  Loss pred: 0.3291; Loss self: 0.0000; time: 0.50s
Val loss: 0.4693 score: 0.7955 time: 0.27s
Test loss: 0.5287 score: 0.8140 time: 0.26s
Epoch 74/1000, LR 0.000267
Train loss: 0.2980;  Loss pred: 0.2980; Loss self: 0.0000; time: 0.38s
Val loss: 0.4631 score: 0.7955 time: 0.26s
Test loss: 0.5235 score: 0.8140 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.40s
Val loss: 0.4571 score: 0.7955 time: 0.27s
Test loss: 0.5184 score: 0.8140 time: 0.34s
Epoch 76/1000, LR 0.000267
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 0.41s
Val loss: 0.4516 score: 0.8182 time: 0.26s
Test loss: 0.5134 score: 0.8140 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.2558;  Loss pred: 0.2558; Loss self: 0.0000; time: 0.52s
Val loss: 0.4465 score: 0.8182 time: 0.29s
Test loss: 0.5089 score: 0.8140 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.2426;  Loss pred: 0.2426; Loss self: 0.0000; time: 0.43s
Val loss: 0.4419 score: 0.8182 time: 0.25s
Test loss: 0.5048 score: 0.8140 time: 0.41s
Epoch 79/1000, LR 0.000267
Train loss: 0.2211;  Loss pred: 0.2211; Loss self: 0.0000; time: 0.42s
Val loss: 0.4383 score: 0.8182 time: 0.25s
Test loss: 0.5021 score: 0.8140 time: 0.27s
Epoch 80/1000, LR 0.000267
Train loss: 0.2114;  Loss pred: 0.2114; Loss self: 0.0000; time: 0.47s
Val loss: 0.4352 score: 0.8182 time: 0.31s
Test loss: 0.4998 score: 0.8140 time: 0.34s
Epoch 81/1000, LR 0.000267
Train loss: 0.2039;  Loss pred: 0.2039; Loss self: 0.0000; time: 0.38s
Val loss: 0.4330 score: 0.8182 time: 0.26s
Test loss: 0.4985 score: 0.8140 time: 0.28s
Epoch 82/1000, LR 0.000267
Train loss: 0.1925;  Loss pred: 0.1925; Loss self: 0.0000; time: 0.49s
Val loss: 0.4316 score: 0.8182 time: 0.28s
Test loss: 0.4978 score: 0.8140 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.1794;  Loss pred: 0.1794; Loss self: 0.0000; time: 0.41s
Val loss: 0.4312 score: 0.8182 time: 0.26s
Test loss: 0.4986 score: 0.8140 time: 0.26s
Epoch 84/1000, LR 0.000266
Train loss: 0.1715;  Loss pred: 0.1715; Loss self: 0.0000; time: 0.41s
Val loss: 0.4315 score: 0.8182 time: 0.29s
Test loss: 0.4993 score: 0.8140 time: 0.27s
     INFO: Early stopping counter 1 of 2
Epoch 85/1000, LR 0.000266
Train loss: 0.1572;  Loss pred: 0.1572; Loss self: 0.0000; time: 0.36s
Val loss: 0.4323 score: 0.8182 time: 0.26s
Test loss: 0.5000 score: 0.8140 time: 0.32s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 082,   Train_Loss: 0.1794,   Val_Loss: 0.4312,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4312,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4986


[0.36952373501844704, 0.29176156502217054, 0.30289753898978233, 0.27685029699932784, 0.29375023394823074, 0.29798658192157745, 0.2773585630347952, 0.3693898960482329, 0.30624698207248, 0.29923670704010874, 0.28760483302176, 0.2983795600011945, 0.30848910799250007, 0.38577801606152207, 0.2859298449475318, 0.3704615340102464, 0.30242245795670897, 0.3062286979984492, 0.27652376296464354, 0.2924599249381572, 0.28607970906887203, 0.29042193898931146, 0.2973043540259823, 0.285168417962268, 0.30186239595059305, 0.35106380993966013, 0.29120523494202644, 0.2913693320006132, 0.2960737090324983, 0.29519673506729305, 0.2912233849056065, 0.2961235570255667, 0.2992687710793689, 0.29015659308061004, 0.29325354692991823, 0.29522937908768654, 0.2791310528991744, 0.2940690499963239, 0.314724285970442, 0.3021737849339843, 0.2849383009597659, 0.2995733559364453, 0.37446534202899784, 0.30114319804124534, 0.2919802189571783, 0.2989457229850814, 0.3054814119823277, 0.28103991597890854, 0.3735915549332276, 0.2958805999951437, 0.28198662400245667, 0.29058063100092113, 0.3104076029267162, 0.2912619770504534, 0.2757169050164521, 0.29464187601115555, 0.30556349793914706, 0.2798997019417584, 0.2859817040152848, 0.3113266780273989, 0.301602088031359, 0.279124666005373, 0.3222534799715504, 0.2995140759740025, 0.37784648302476853, 0.2972180239157751, 0.29439367400482297, 0.3266004309989512, 0.3825532579794526, 0.2830586319323629, 0.293835990014486, 0.30552916310261935, 0.29101266199722886, 0.2925049050245434, 0.2835992069449276, 0.3753322159172967, 0.2920607980340719, 0.29411495104432106, 0.30546028702519834, 0.283623980008997, 0.2938370719784871, 0.3771296429913491, 0.29044601402711123, 0.32010544498916715, 0.28566729405429214, 0.28989384102169424, 0.30628324300050735, 0.30827657599002123, 0.3063169269589707, 0.2827313320012763, 0.29227845498826355, 0.29155204200651497, 0.34079158992972225, 0.3467241380130872, 0.2819166959961876, 0.28713195002637804, 0.2918478609062731, 0.28458610503003, 0.2700781859457493, 0.259240108076483, 0.2644670188892633, 0.3549331499962136, 0.24700000998564065, 0.24395788100082427, 0.2565005460055545, 0.25952768803108484, 0.3304636829998344, 0.25307435006834567, 0.2699749160092324, 0.25234922498930246, 0.2541644520824775, 0.38038860901724547, 0.3272944709751755, 0.2520470479503274, 0.28042656800244004, 0.30155518802348524, 0.33090836298651993, 0.2574167399434373, 0.2703969299327582, 0.23975739302113652, 0.24356011906638741, 0.35222392703872174, 0.2772370290476829, 0.23949191195424646, 0.3511020060395822, 0.23357653594575822, 0.3281715679913759, 0.2437028019921854, 0.22764988499693573, 0.43735701695550233, 0.25294741499237716, 0.3500577131053433, 0.306260776007548, 0.26380513596814126, 0.2690988570684567, 0.243063103989698, 0.3571680070599541, 0.2670540849212557, 0.2619574679993093, 0.2763455790700391, 0.256794675020501, 0.3431831559864804, 0.26507724600378424, 0.2494861709419638, 0.252478705951944, 0.27751977799925953, 0.344806976034306, 0.24092489399481565, 0.2515141569310799, 0.27257277606986463, 0.24200042302254587, 0.3644478500355035, 0.258691085036844, 0.2716399859637022, 0.2470307219773531, 0.27412436495069414, 0.3357341749360785, 0.28192092606332153, 0.2472141970647499, 0.264060991932638, 0.2503599270712584, 0.3529456580290571, 0.2646653159754351, 0.2431025499245152, 0.2535372660495341, 0.26354029797948897, 0.35931198799517006, 0.26231147698126733, 0.256732797017321, 0.2676089119631797, 0.2507844999199733, 0.34322812396567315, 0.25695708009880036, 0.252013327088207, 0.4131592500489205, 0.27076816896442324, 0.34376924706157297, 0.28413758997339755, 0.2538291239179671, 0.26152545504737645, 0.27283460705075413, 0.32182722503785044]
[0.008398266704964706, 0.006630944659594785, 0.006884034977040507, 0.006292052204530178, 0.006676141680641608, 0.0067724223163994875, 0.006303603705336255, 0.008395224910187111, 0.006960158683465455, 0.006800834250911562, 0.006536473477767272, 0.006781353636390783, 0.0070111160907386375, 0.008767682183216411, 0.00649840556698936, 0.008419580318414692, 0.006873237680834295, 0.006959743136328391, 0.006284630976469171, 0.006646816475867209, 0.006501811569747092, 0.006600498613393443, 0.006756917136954144, 0.006481100408233364, 0.006860508998877115, 0.007978722953174094, 0.006618300794136964, 0.006622030272741209, 0.00672894793255678, 0.006709016706074842, 0.006618713293309239, 0.006730080841490152, 0.006801562979076566, 0.006594468024559319, 0.006664853339316323, 0.006709758615629239, 0.006343887565890327, 0.006683387499916452, 0.007152824681146409, 0.006867586021226915, 0.006475870476358316, 0.006808485362191938, 0.008510575955204496, 0.006844163591846485, 0.006635914067208598, 0.006794220976933668, 0.006942759363234721, 0.006387270817702467, 0.008490717157573354, 0.006724559090798721, 0.006408786909146743, 0.006604105250020934, 0.0070547182483344595, 0.006619590387510305, 0.006266293295828456, 0.0066964062729808075, 0.0069446249531624335, 0.006361356862312691, 0.006499584182165563, 0.00707560631880452, 0.006854592909803614, 0.006343742409213023, 0.0073239427266261455, 0.006807138090318238, 0.00858742006874474, 0.006754955088994889, 0.006690765318291431, 0.007422737068157981, 0.008694392226805741, 0.006433150725735521, 0.006678090682147409, 0.006943844615968622, 0.006613924136300656, 0.006647838750557805, 0.006445436521475626, 0.008530277634484017, 0.006637745409865271, 0.0066844307055527515, 0.006942279250572689, 0.006445999545659023, 0.006678115272238342, 0.008571128249803389, 0.006601045773343437, 0.007275123749753799, 0.0064924385012339126, 0.006588496386856687, 0.006960982795466076, 0.007006285817955028, 0.006961748339976607, 0.006425712090938098, 0.006642692158824171, 0.0066261827728753405, 0.007745263407493688, 0.007880094045751983, 0.006407197636276992, 0.006525726136963137, 0.006632905929688026, 0.00661828151232628, 0.0062808880452499835, 0.006028839722708907, 0.006150395788122402, 0.008254259302237527, 0.005744186278735829, 0.005673439093042425, 0.00596512897687336, 0.00603552762862988, 0.0076852019302287074, 0.005885450001589434, 0.006278486418819358, 0.0058685866276581965, 0.005910801211220407, 0.00884624672133129, 0.007611499325004082, 0.005861559254658776, 0.006521548093080001, 0.007012911349383377, 0.0076955433252679055, 0.005986435812638077, 0.006288300696110656, 0.005575753326072942, 0.005664188815497382, 0.008191254117179575, 0.0064473727685507645, 0.005569579347773174, 0.008165162931153075, 0.0054320124638548425, 0.0076318969300319985, 0.0056675070230740796, 0.005294183372021761, 0.010171093417569822, 0.005882498023078539, 0.008140877048961472, 0.007122343628082511, 0.006135003162049796, 0.006258112955080389, 0.005652630325341814, 0.008306232722324515, 0.0062105601144478075, 0.006092034139518821, 0.006426641373721839, 0.005971969186523279, 0.007981003627592569, 0.006164587116367076, 0.0058020039753945065, 0.005871597812835907, 0.006453948325564175, 0.008018766884518745, 0.0056029045115073405, 0.005849166440257673, 0.0063389017690666195, 0.005627916814477811, 0.008475531396174501, 0.006016071745042884, 0.006317208975900051, 0.005744900511101235, 0.006374985231411492, 0.00780777151014136, 0.006556300606123756, 0.0057491673735988345, 0.006140953300759023, 0.005822323885378103, 0.008208038558815281, 0.006155007348265932, 0.005653547672663144, 0.005896215489524049, 0.006128844139057883, 0.008356092744073722, 0.006100266906541101, 0.005970530163193512, 0.006223463068911156, 0.005832197672557519, 0.007982049394550538, 0.00597574604880931, 0.0058607750485629535, 0.009608354652300477, 0.006296934161963331, 0.00799463365259472, 0.0066078509296138965, 0.005903002881813188, 0.006081987326683173, 0.006344990861645445, 0.007484354070647684]
[119.07218895642379, 150.8080750686147, 145.2636431010562, 158.93065847101775, 149.78711474917282, 147.6576553087202, 158.63941433270298, 119.11533171512282, 143.67488522576343, 147.04078398410653, 152.98769334891875, 147.46318413977104, 142.6306435463184, 114.05522909056354, 153.88390116489592, 118.77076554669513, 145.4918404449265, 143.6834636583369, 159.11833228461404, 150.4479631159863, 153.80328840241953, 151.50370579138428, 147.9964871155392, 154.29478591777948, 145.7617795069832, 125.3333404191181, 151.0961848222254, 151.01108856544792, 148.61164182318657, 149.0531390530785, 151.08676802950288, 148.58662526534872, 147.02502984626747, 151.64225473165834, 150.04081096592884, 149.03665799104462, 157.63204968776208, 149.62472249476795, 139.80490849102236, 145.61157252477295, 154.41939483668403, 146.87554526489544, 117.50086072476296, 146.10989152733129, 150.6951400925315, 147.1839087063834, 144.03495032472034, 156.5613903873431, 117.77568154040357, 148.70863449892346, 156.0357699789926, 151.4209665263633, 141.74910532197214, 151.06674906755222, 159.58397617068317, 149.33383060028473, 143.99625706851475, 157.1991670400404, 153.8559963180314, 141.3306443212287, 145.88758415832024, 157.63565660353723, 136.53847897588093, 146.90461493976383, 116.4494099502197, 148.0394742563412, 149.45973329331514, 134.72119392316813, 115.01666521518239, 155.44482674710954, 149.7433993631305, 144.0124391177072, 151.196170290415, 150.42482790607585, 155.1485297649101, 117.22947866989192, 150.65356355996448, 149.6013713133866, 144.04491146297622, 155.13497835621743, 149.74284797944557, 116.67075452090438, 151.49114766606124, 137.45470653112142, 154.0253326712215, 151.77969923378694, 143.65787553035383, 142.72897594861135, 143.64207827761842, 155.62477525413198, 150.54137330022093, 150.91645284726485, 129.1111673532602, 126.90203875664176, 156.07447386016122, 153.23965165129775, 150.7634829440487, 151.0966250283462, 159.21315469971879, 165.86939543827768, 162.59116233319367, 121.14957422393124, 174.08906178789144, 176.25993398366464, 167.64096868265082, 165.68559727180127, 130.12019841230645, 169.91054205369826, 159.27405640355684, 170.3987797141951, 169.181801969877, 113.04229143741402, 131.38016011050016, 170.60306934630037, 153.33782496537864, 142.59413105056956, 129.94534079439893, 167.04430337144538, 159.02547418231842, 179.3479627808983, 176.54778690709807, 122.08142803220973, 155.10193623018623, 179.54677320466212, 122.47153038240491, 184.09383385146862, 131.0290232124253, 176.44442184697917, 188.886543916237, 98.31784636571845, 169.99580723644024, 122.83688771931146, 140.40322290223753, 162.99910099245736, 159.79257759932116, 176.90879156148074, 120.39152205696305, 161.01607287781835, 164.14878464206785, 155.60227214310441, 167.4489550710782, 125.29752480536652, 162.2168656429535, 172.35424247223222, 170.31139255040577, 154.94391178172077, 124.70745370221799, 178.4788582325797, 170.96453147877034, 157.75603352617443, 177.68563981391853, 117.98670233837586, 166.221421947632, 158.29775519774125, 174.0674182377287, 156.86310849360024, 128.07751849565778, 152.52503813903436, 173.9382305326806, 162.84116667625526, 171.75272617714563, 121.83178634389071, 162.46934299464857, 176.88008625722668, 169.60031426543426, 163.16290271230793, 119.6731571354586, 163.92725356455065, 167.48931379070714, 160.68224217404378, 171.46195244810397, 125.2811089696732, 167.34312198545547, 170.6258970381737, 104.07609171259882, 158.80744093538505, 125.08390546143941, 151.33513310937101, 169.40530438854137, 164.41994142486195, 157.6046399128572, 133.61206465656448]
Elapsed: 0.29456850277850977~0.037951213206630814
Time per graph: 0.006764617985090112~0.0008576007685589995
Speed: 149.98405868971298~17.224747995959177
Total Time: 0.3224
best val loss: 0.43124711513519287 test_score: 0.8140

Testing...
Test loss: 0.6615 score: 0.6279 time: 0.26s
test Score 0.6279
Epoch Time List: [0.9872064890805632, 0.9171857718611136, 0.9412238380173221, 0.8621066370978951, 0.9233129039639607, 0.9179292449261993, 0.8681507860310376, 0.9766329169506207, 0.9311045568902045, 0.949998828000389, 0.9062795559875667, 0.9235974840121344, 0.9447191280778497, 1.0073519999859855, 0.88636977202259, 1.0117616989882663, 0.9057127689011395, 1.022983851027675, 0.851082180859521, 0.9888799529289827, 0.8906996871810406, 0.9022106871707365, 0.9658788319211453, 0.8790028251241893, 1.097608622862026, 0.9715511448448524, 0.8977847038768232, 0.8970914529636502, 1.0034707499435171, 0.9394364199833944, 0.9161871860269457, 0.883654656005092, 0.9158092540455982, 0.9047825869638473, 0.8982818530639634, 0.99709671898745, 0.9707248149206862, 0.9129735750611871, 0.9275921410880983, 1.0157559911021963, 0.889595712069422, 0.9773796498775482, 1.017859356943518, 0.9356499480782077, 0.8873381270095706, 0.9236242540646344, 1.0236177049810067, 0.8691154149128124, 1.0107488799840212, 0.933516840916127, 0.8936825640266761, 0.9507751279743388, 0.9676882581552491, 0.9255808771122247, 0.9954710860038176, 0.9090603881049901, 1.075700337998569, 0.8561906120739877, 1.0030003801221028, 0.990406752214767, 0.925515174982138, 0.8540504378033802, 0.9349362479988486, 1.0044925319962204, 1.0278184668859467, 0.9069808259373531, 1.0548239500494674, 0.9354174728505313, 1.0633592320373282, 0.9001998530002311, 0.9029836760601029, 0.9230084370356053, 0.9239845089614391, 0.9143341329181567, 0.9092096789972857, 1.0247330599231645, 0.8683197769569233, 0.9046093750512227, 1.04873417399358, 0.8880371160339564, 0.9951755091315135, 1.075993465143256, 0.9567119519924745, 0.9391264340374619, 0.9289557769661769, 1.0378514809999615, 1.035478727077134, 1.0129994129529223, 1.0916588800027966, 0.8711498159682378, 0.9565573589643463, 1.0027554730186239, 1.0611365520162508, 0.9693381859688088, 0.9545569409383461, 1.0181502680061385, 0.9121760150883347, 0.9550224050180987, 0.9165428549749777, 0.9151390560436994, 0.9105445349123329, 1.0173018489731476, 0.8746701141353697, 0.9505884851096198, 0.942069040145725, 0.9206739440560341, 0.9795436530839652, 0.9303048241417855, 0.9248503510607406, 0.8699363610940054, 0.9979156440822408, 1.1513327419525012, 1.0604912790004164, 0.9580656790640205, 1.0301272069336846, 1.002223705057986, 0.9529627431184053, 0.914357498055324, 0.9035087139345706, 0.8579494009027258, 0.9571078560547903, 1.0111478429753333, 0.9201573230093345, 0.8549610059708357, 1.1002935951109976, 0.8496639899676666, 0.978095663129352, 0.977066679042764, 0.8734497808618471, 1.0119287909474224, 0.9233890390023589, 1.1574248949764296, 1.1703419991536066, 0.9520237660035491, 1.027281176066026, 0.8820301960222423, 1.0432953820563853, 0.9496708920923993, 0.9375446230405942, 0.946900884155184, 0.9391532019944862, 1.0214888480259106, 0.9409343319712207, 0.9701804508222267, 0.9151412041392177, 0.9716943989042193, 0.9896062921034172, 0.9760444920975715, 0.9194726390996948, 0.9249236768810079, 0.8513757409527898, 1.051271847099997, 0.9196274301502854, 0.9542346750386059, 0.8599018049426377, 0.9385795691050589, 1.0408422260079533, 0.9294686750508845, 0.8693957749055699, 0.980028664926067, 0.8831559360260144, 1.064206365030259, 0.9011547049740329, 0.8487857939908281, 0.9363734319340438, 0.9538430790416896, 1.0169824010226876, 0.9339258359977975, 0.9281866560922936, 1.0368575940374285, 0.8893089250195771, 1.0080642100656405, 0.9276587850181386, 1.0649786670692265, 1.0899559259414673, 0.9331676970468834, 1.115807453985326, 0.9267746009863913, 1.0201145480386913, 0.9268636310007423, 0.970988218090497, 0.9384288610890508]
Total Epoch List: [97, 85]
Total Time List: [0.29230970598291606, 0.32242769096046686]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcecb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.28s
Epoch 4/1000, LR 0.000060
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.29s
Epoch 7/1000, LR 0.000150
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.35s
Epoch 10/1000, LR 0.000240
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.26s
Epoch 17/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.29s
Epoch 19/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5116 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5116 time: 0.23s
Epoch 23/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.39s
Val loss: 0.6913 score: 0.5455 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.42s
Val loss: 0.6910 score: 0.6136 time: 0.27s
Test loss: 0.6908 score: 0.6977 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.39s
Val loss: 0.6907 score: 0.6136 time: 0.30s
Test loss: 0.6906 score: 0.7442 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.49s
Val loss: 0.6904 score: 0.7727 time: 0.29s
Test loss: 0.6903 score: 0.6047 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.42s
Val loss: 0.6901 score: 0.6136 time: 0.27s
Test loss: 0.6901 score: 0.5581 time: 0.32s
Epoch 28/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.47s
Val loss: 0.6898 score: 0.5455 time: 0.28s
Test loss: 0.6898 score: 0.5581 time: 0.26s
Epoch 29/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.36s
Val loss: 0.6894 score: 0.5227 time: 0.31s
Test loss: 0.6895 score: 0.5581 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.40s
Val loss: 0.6890 score: 0.5455 time: 0.29s
Test loss: 0.6892 score: 0.5349 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.49s
Val loss: 0.6886 score: 0.5455 time: 0.27s
Test loss: 0.6888 score: 0.5349 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.44s
Val loss: 0.6882 score: 0.5455 time: 0.29s
Test loss: 0.6884 score: 0.5349 time: 0.26s
Epoch 33/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.36s
Val loss: 0.6877 score: 0.5455 time: 0.28s
Test loss: 0.6880 score: 0.5349 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.41s
Val loss: 0.6872 score: 0.5227 time: 0.28s
Test loss: 0.6875 score: 0.5116 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5000 time: 0.27s
Test loss: 0.6869 score: 0.5116 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5000 time: 0.28s
Test loss: 0.6863 score: 0.5116 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.5000 time: 0.27s
Test loss: 0.6857 score: 0.5116 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.5000 time: 0.29s
Test loss: 0.6850 score: 0.5116 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6838 score: 0.5000 time: 0.28s
Test loss: 0.6843 score: 0.5116 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6829 score: 0.5000 time: 0.28s
Test loss: 0.6835 score: 0.5116 time: 0.33s
Epoch 41/1000, LR 0.000269
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.5000 time: 0.28s
Test loss: 0.6825 score: 0.5116 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6809 score: 0.5000 time: 0.29s
Test loss: 0.6814 score: 0.5116 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5000 time: 0.27s
Test loss: 0.6802 score: 0.5116 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.5000 time: 0.27s
Test loss: 0.6788 score: 0.5116 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.5000 time: 0.29s
Test loss: 0.6774 score: 0.5116 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.39s
Val loss: 0.6754 score: 0.5227 time: 0.36s
Test loss: 0.6757 score: 0.5116 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.41s
Val loss: 0.6736 score: 0.5227 time: 0.28s
Test loss: 0.6739 score: 0.5116 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.36s
Val loss: 0.6717 score: 0.5227 time: 0.28s
Test loss: 0.6718 score: 0.5349 time: 0.26s
Epoch 49/1000, LR 0.000269
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 0.40s
Val loss: 0.6695 score: 0.5455 time: 0.30s
Test loss: 0.6696 score: 0.5349 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.41s
Val loss: 0.6671 score: 0.5455 time: 0.28s
Test loss: 0.6671 score: 0.5349 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.43s
Val loss: 0.6644 score: 0.5455 time: 0.40s
Test loss: 0.6645 score: 0.5349 time: 0.26s
Epoch 52/1000, LR 0.000269
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.57s
Val loss: 0.6615 score: 0.5682 time: 0.28s
Test loss: 0.6616 score: 0.5349 time: 0.26s
Epoch 53/1000, LR 0.000269
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.42s
Val loss: 0.6585 score: 0.5682 time: 0.28s
Test loss: 0.6584 score: 0.5349 time: 0.28s
Epoch 54/1000, LR 0.000269
Train loss: 0.6462;  Loss pred: 0.6462; Loss self: 0.0000; time: 0.38s
Val loss: 0.6552 score: 0.5682 time: 0.30s
Test loss: 0.6550 score: 0.5349 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.46s
Val loss: 0.6518 score: 0.5682 time: 0.32s
Test loss: 0.6514 score: 0.5349 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.6365;  Loss pred: 0.6365; Loss self: 0.0000; time: 0.44s
Val loss: 0.6481 score: 0.5909 time: 0.29s
Test loss: 0.6474 score: 0.5349 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.62s
Val loss: 0.6441 score: 0.5909 time: 0.29s
Test loss: 0.6430 score: 0.5349 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.6293;  Loss pred: 0.6293; Loss self: 0.0000; time: 0.41s
Val loss: 0.6398 score: 0.5909 time: 0.28s
Test loss: 0.6383 score: 0.5349 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.6192;  Loss pred: 0.6192; Loss self: 0.0000; time: 0.40s
Val loss: 0.6353 score: 0.6364 time: 0.30s
Test loss: 0.6332 score: 0.5581 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.37s
Val loss: 0.6303 score: 0.6591 time: 0.29s
Test loss: 0.6278 score: 0.5814 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.43s
Val loss: 0.6251 score: 0.7045 time: 0.30s
Test loss: 0.6220 score: 0.6047 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.5982;  Loss pred: 0.5982; Loss self: 0.0000; time: 0.52s
Val loss: 0.6195 score: 0.7045 time: 0.31s
Test loss: 0.6159 score: 0.6047 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5882;  Loss pred: 0.5882; Loss self: 0.0000; time: 0.40s
Val loss: 0.6136 score: 0.6818 time: 0.27s
Test loss: 0.6095 score: 0.6512 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.40s
Val loss: 0.6072 score: 0.7273 time: 0.29s
Test loss: 0.6028 score: 0.6977 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.39s
Val loss: 0.6004 score: 0.7273 time: 0.29s
Test loss: 0.5959 score: 0.7209 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.43s
Val loss: 0.5933 score: 0.7727 time: 0.30s
Test loss: 0.5886 score: 0.7209 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.53s
Val loss: 0.5857 score: 0.7727 time: 0.29s
Test loss: 0.5814 score: 0.7209 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 0.42s
Val loss: 0.5779 score: 0.7727 time: 0.28s
Test loss: 0.5739 score: 0.7442 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.5260;  Loss pred: 0.5260; Loss self: 0.0000; time: 0.47s
Val loss: 0.5696 score: 0.7955 time: 0.28s
Test loss: 0.5661 score: 0.7674 time: 0.26s
Epoch 70/1000, LR 0.000268
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.37s
Val loss: 0.5611 score: 0.8182 time: 0.31s
Test loss: 0.5582 score: 0.7674 time: 0.29s
Epoch 71/1000, LR 0.000268
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 0.38s
Val loss: 0.5524 score: 0.8409 time: 0.30s
Test loss: 0.5499 score: 0.7907 time: 0.33s
Epoch 72/1000, LR 0.000267
Train loss: 0.4999;  Loss pred: 0.4999; Loss self: 0.0000; time: 0.41s
Val loss: 0.5435 score: 0.8409 time: 0.30s
Test loss: 0.5411 score: 0.8140 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.4933;  Loss pred: 0.4933; Loss self: 0.0000; time: 0.39s
Val loss: 0.5347 score: 0.8409 time: 0.28s
Test loss: 0.5318 score: 0.8140 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.4731;  Loss pred: 0.4731; Loss self: 0.0000; time: 0.53s
Val loss: 0.5257 score: 0.8636 time: 0.27s
Test loss: 0.5226 score: 0.8140 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.4611;  Loss pred: 0.4611; Loss self: 0.0000; time: 0.36s
Val loss: 0.5164 score: 0.8864 time: 0.28s
Test loss: 0.5137 score: 0.8140 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.4360;  Loss pred: 0.4360; Loss self: 0.0000; time: 0.38s
Val loss: 0.5069 score: 0.9091 time: 0.39s
Test loss: 0.5053 score: 0.7907 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.4274;  Loss pred: 0.4274; Loss self: 0.0000; time: 0.45s
Val loss: 0.4971 score: 0.9091 time: 0.37s
Test loss: 0.4981 score: 0.7907 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.4178;  Loss pred: 0.4178; Loss self: 0.0000; time: 0.46s
Val loss: 0.4872 score: 0.9091 time: 0.28s
Test loss: 0.4918 score: 0.7907 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.4027;  Loss pred: 0.4027; Loss self: 0.0000; time: 0.43s
Val loss: 0.4774 score: 0.9091 time: 0.27s
Test loss: 0.4855 score: 0.7907 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.3898;  Loss pred: 0.3898; Loss self: 0.0000; time: 0.36s
Val loss: 0.4677 score: 0.9091 time: 0.29s
Test loss: 0.4792 score: 0.7907 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.3882;  Loss pred: 0.3882; Loss self: 0.0000; time: 0.40s
Val loss: 0.4581 score: 0.9091 time: 0.28s
Test loss: 0.4728 score: 0.7907 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.3720;  Loss pred: 0.3720; Loss self: 0.0000; time: 0.44s
Val loss: 0.4486 score: 0.9091 time: 0.36s
Test loss: 0.4655 score: 0.7907 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.3621;  Loss pred: 0.3621; Loss self: 0.0000; time: 0.42s
Val loss: 0.4393 score: 0.9091 time: 0.28s
Test loss: 0.4580 score: 0.7907 time: 0.27s
Epoch 84/1000, LR 0.000266
Train loss: 0.3521;  Loss pred: 0.3521; Loss self: 0.0000; time: 0.42s
Val loss: 0.4301 score: 0.9091 time: 0.28s
Test loss: 0.4520 score: 0.8140 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.3487;  Loss pred: 0.3487; Loss self: 0.0000; time: 0.39s
Val loss: 0.4212 score: 0.9091 time: 0.31s
Test loss: 0.4454 score: 0.8140 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3242;  Loss pred: 0.3242; Loss self: 0.0000; time: 0.49s
Val loss: 0.4129 score: 0.9091 time: 0.29s
Test loss: 0.4377 score: 0.8140 time: 0.24s
Epoch 87/1000, LR 0.000266
Train loss: 0.3111;  Loss pred: 0.3111; Loss self: 0.0000; time: 0.40s
Val loss: 0.4052 score: 0.9091 time: 0.27s
Test loss: 0.4307 score: 0.8372 time: 0.35s
Epoch 88/1000, LR 0.000266
Train loss: 0.3283;  Loss pred: 0.3283; Loss self: 0.0000; time: 0.51s
Val loss: 0.3979 score: 0.8864 time: 0.27s
Test loss: 0.4245 score: 0.8372 time: 0.31s
Epoch 89/1000, LR 0.000266
Train loss: 0.2810;  Loss pred: 0.2810; Loss self: 0.0000; time: 0.41s
Val loss: 0.3912 score: 0.9318 time: 0.27s
Test loss: 0.4182 score: 0.8605 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.2825;  Loss pred: 0.2825; Loss self: 0.0000; time: 0.39s
Val loss: 0.3847 score: 0.9091 time: 0.28s
Test loss: 0.4124 score: 0.8605 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.2739;  Loss pred: 0.2739; Loss self: 0.0000; time: 0.49s
Val loss: 0.3771 score: 0.9091 time: 0.29s
Test loss: 0.4083 score: 0.8605 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.2694;  Loss pred: 0.2694; Loss self: 0.0000; time: 0.40s
Val loss: 0.3697 score: 0.9091 time: 0.27s
Test loss: 0.4045 score: 0.8605 time: 0.24s
Epoch 93/1000, LR 0.000265
Train loss: 0.2605;  Loss pred: 0.2605; Loss self: 0.0000; time: 0.50s
Val loss: 0.3633 score: 0.9318 time: 0.27s
Test loss: 0.4002 score: 0.8372 time: 0.26s
Epoch 94/1000, LR 0.000265
Train loss: 0.2442;  Loss pred: 0.2442; Loss self: 0.0000; time: 0.36s
Val loss: 0.3542 score: 0.9318 time: 0.30s
Test loss: 0.3989 score: 0.8605 time: 0.24s
Epoch 95/1000, LR 0.000265
Train loss: 0.2279;  Loss pred: 0.2279; Loss self: 0.0000; time: 0.43s
Val loss: 0.3455 score: 0.9318 time: 0.29s
Test loss: 0.3982 score: 0.8605 time: 0.24s
Epoch 96/1000, LR 0.000265
Train loss: 0.2090;  Loss pred: 0.2090; Loss self: 0.0000; time: 0.40s
Val loss: 0.3377 score: 0.9545 time: 0.33s
Test loss: 0.3968 score: 0.8605 time: 0.30s
Epoch 97/1000, LR 0.000265
Train loss: 0.2136;  Loss pred: 0.2136; Loss self: 0.0000; time: 0.44s
Val loss: 0.3282 score: 0.9318 time: 0.27s
Test loss: 0.3984 score: 0.8605 time: 0.26s
Epoch 98/1000, LR 0.000265
Train loss: 0.2182;  Loss pred: 0.2182; Loss self: 0.0000; time: 0.44s
Val loss: 0.3206 score: 0.9318 time: 0.28s
Test loss: 0.3986 score: 0.8605 time: 0.25s
Epoch 99/1000, LR 0.000265
Train loss: 0.1910;  Loss pred: 0.1910; Loss self: 0.0000; time: 0.39s
Val loss: 0.3133 score: 0.9318 time: 0.29s
Test loss: 0.3991 score: 0.8605 time: 0.24s
Epoch 100/1000, LR 0.000265
Train loss: 0.2004;  Loss pred: 0.2004; Loss self: 0.0000; time: 0.40s
Val loss: 0.3075 score: 0.9318 time: 0.33s
Test loss: 0.3981 score: 0.8605 time: 0.24s
Epoch 101/1000, LR 0.000265
Train loss: 0.1714;  Loss pred: 0.1714; Loss self: 0.0000; time: 0.42s
Val loss: 0.3016 score: 0.9545 time: 0.28s
Test loss: 0.3981 score: 0.8605 time: 0.26s
Epoch 102/1000, LR 0.000264
Train loss: 0.1664;  Loss pred: 0.1664; Loss self: 0.0000; time: 0.36s
Val loss: 0.2964 score: 0.9545 time: 0.28s
Test loss: 0.3979 score: 0.8605 time: 0.32s
Epoch 103/1000, LR 0.000264
Train loss: 0.1386;  Loss pred: 0.1386; Loss self: 0.0000; time: 0.39s
Val loss: 0.2908 score: 0.9545 time: 0.30s
Test loss: 0.3990 score: 0.8605 time: 0.23s
Epoch 104/1000, LR 0.000264
Train loss: 0.1423;  Loss pred: 0.1423; Loss self: 0.0000; time: 0.40s
Val loss: 0.2866 score: 0.9545 time: 0.28s
Test loss: 0.3990 score: 0.8605 time: 0.24s
Epoch 105/1000, LR 0.000264
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 0.40s
Val loss: 0.2806 score: 0.9545 time: 0.28s
Test loss: 0.4020 score: 0.8605 time: 0.25s
Epoch 106/1000, LR 0.000264
Train loss: 0.1256;  Loss pred: 0.1256; Loss self: 0.0000; time: 0.39s
Val loss: 0.2741 score: 0.9545 time: 0.29s
Test loss: 0.4068 score: 0.8605 time: 0.24s
Epoch 107/1000, LR 0.000264
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 0.40s
Val loss: 0.2691 score: 0.9545 time: 0.26s
Test loss: 0.4109 score: 0.8605 time: 0.25s
Epoch 108/1000, LR 0.000264
Train loss: 0.1198;  Loss pred: 0.1198; Loss self: 0.0000; time: 0.39s
Val loss: 0.2658 score: 0.9545 time: 0.36s
Test loss: 0.4132 score: 0.8605 time: 0.26s
Epoch 109/1000, LR 0.000264
Train loss: 0.1022;  Loss pred: 0.1022; Loss self: 0.0000; time: 0.36s
Val loss: 0.2633 score: 0.9545 time: 0.29s
Test loss: 0.4154 score: 0.8605 time: 0.23s
Epoch 110/1000, LR 0.000263
Train loss: 0.0904;  Loss pred: 0.0904; Loss self: 0.0000; time: 0.40s
Val loss: 0.2609 score: 0.9318 time: 0.29s
Test loss: 0.4184 score: 0.8605 time: 0.22s
Epoch 111/1000, LR 0.000263
Train loss: 0.0932;  Loss pred: 0.0932; Loss self: 0.0000; time: 0.41s
Val loss: 0.2589 score: 0.9318 time: 0.27s
Test loss: 0.4215 score: 0.8605 time: 0.25s
Epoch 112/1000, LR 0.000263
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.38s
Val loss: 0.2581 score: 0.9318 time: 0.30s
Test loss: 0.4242 score: 0.8605 time: 0.25s
Epoch 113/1000, LR 0.000263
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.40s
Val loss: 0.2534 score: 0.9318 time: 0.38s
Test loss: 0.4306 score: 0.8605 time: 0.25s
Epoch 114/1000, LR 0.000263
Train loss: 0.0869;  Loss pred: 0.0869; Loss self: 0.0000; time: 0.41s
Val loss: 0.2509 score: 0.9318 time: 0.29s
Test loss: 0.4358 score: 0.8605 time: 0.23s
Epoch 115/1000, LR 0.000263
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.43s
Val loss: 0.2524 score: 0.9318 time: 0.27s
Test loss: 0.4383 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 116/1000, LR 0.000263
Train loss: 0.0868;  Loss pred: 0.0868; Loss self: 0.0000; time: 0.39s
Val loss: 0.2558 score: 0.9318 time: 0.28s
Test loss: 0.4403 score: 0.8372 time: 0.32s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 113,   Train_Loss: 0.0869,   Val_Loss: 0.2509,   Val_Precision: 1.0000,   Val_Recall: 0.8636,   Val_accuracy: 0.9268,   Val_Score: 0.9318,   Val_Loss: 0.2509,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.4358


[0.36952373501844704, 0.29176156502217054, 0.30289753898978233, 0.27685029699932784, 0.29375023394823074, 0.29798658192157745, 0.2773585630347952, 0.3693898960482329, 0.30624698207248, 0.29923670704010874, 0.28760483302176, 0.2983795600011945, 0.30848910799250007, 0.38577801606152207, 0.2859298449475318, 0.3704615340102464, 0.30242245795670897, 0.3062286979984492, 0.27652376296464354, 0.2924599249381572, 0.28607970906887203, 0.29042193898931146, 0.2973043540259823, 0.285168417962268, 0.30186239595059305, 0.35106380993966013, 0.29120523494202644, 0.2913693320006132, 0.2960737090324983, 0.29519673506729305, 0.2912233849056065, 0.2961235570255667, 0.2992687710793689, 0.29015659308061004, 0.29325354692991823, 0.29522937908768654, 0.2791310528991744, 0.2940690499963239, 0.314724285970442, 0.3021737849339843, 0.2849383009597659, 0.2995733559364453, 0.37446534202899784, 0.30114319804124534, 0.2919802189571783, 0.2989457229850814, 0.3054814119823277, 0.28103991597890854, 0.3735915549332276, 0.2958805999951437, 0.28198662400245667, 0.29058063100092113, 0.3104076029267162, 0.2912619770504534, 0.2757169050164521, 0.29464187601115555, 0.30556349793914706, 0.2798997019417584, 0.2859817040152848, 0.3113266780273989, 0.301602088031359, 0.279124666005373, 0.3222534799715504, 0.2995140759740025, 0.37784648302476853, 0.2972180239157751, 0.29439367400482297, 0.3266004309989512, 0.3825532579794526, 0.2830586319323629, 0.293835990014486, 0.30552916310261935, 0.29101266199722886, 0.2925049050245434, 0.2835992069449276, 0.3753322159172967, 0.2920607980340719, 0.29411495104432106, 0.30546028702519834, 0.283623980008997, 0.2938370719784871, 0.3771296429913491, 0.29044601402711123, 0.32010544498916715, 0.28566729405429214, 0.28989384102169424, 0.30628324300050735, 0.30827657599002123, 0.3063169269589707, 0.2827313320012763, 0.29227845498826355, 0.29155204200651497, 0.34079158992972225, 0.3467241380130872, 0.2819166959961876, 0.28713195002637804, 0.2918478609062731, 0.28458610503003, 0.2700781859457493, 0.259240108076483, 0.2644670188892633, 0.3549331499962136, 0.24700000998564065, 0.24395788100082427, 0.2565005460055545, 0.25952768803108484, 0.3304636829998344, 0.25307435006834567, 0.2699749160092324, 0.25234922498930246, 0.2541644520824775, 0.38038860901724547, 0.3272944709751755, 0.2520470479503274, 0.28042656800244004, 0.30155518802348524, 0.33090836298651993, 0.2574167399434373, 0.2703969299327582, 0.23975739302113652, 0.24356011906638741, 0.35222392703872174, 0.2772370290476829, 0.23949191195424646, 0.3511020060395822, 0.23357653594575822, 0.3281715679913759, 0.2437028019921854, 0.22764988499693573, 0.43735701695550233, 0.25294741499237716, 0.3500577131053433, 0.306260776007548, 0.26380513596814126, 0.2690988570684567, 0.243063103989698, 0.3571680070599541, 0.2670540849212557, 0.2619574679993093, 0.2763455790700391, 0.256794675020501, 0.3431831559864804, 0.26507724600378424, 0.2494861709419638, 0.252478705951944, 0.27751977799925953, 0.344806976034306, 0.24092489399481565, 0.2515141569310799, 0.27257277606986463, 0.24200042302254587, 0.3644478500355035, 0.258691085036844, 0.2716399859637022, 0.2470307219773531, 0.27412436495069414, 0.3357341749360785, 0.28192092606332153, 0.2472141970647499, 0.264060991932638, 0.2503599270712584, 0.3529456580290571, 0.2646653159754351, 0.2431025499245152, 0.2535372660495341, 0.26354029797948897, 0.35931198799517006, 0.26231147698126733, 0.256732797017321, 0.2676089119631797, 0.2507844999199733, 0.34322812396567315, 0.25695708009880036, 0.252013327088207, 0.4131592500489205, 0.27076816896442324, 0.34376924706157297, 0.28413758997339755, 0.2538291239179671, 0.26152545504737645, 0.27283460705075413, 0.32182722503785044, 0.23267004603985697, 0.26351954706478864, 0.28297877695877105, 0.24524398206267506, 0.24312506394926459, 0.29121974704321474, 0.2570929659996182, 0.24799542094115168, 0.35260650608688593, 0.2435357489157468, 0.2497525590006262, 0.25726786989253014, 0.25115442590322345, 0.24864072306081653, 0.2413525739684701, 0.2620083090150729, 0.24314014799892902, 0.2946124819573015, 0.24841206893324852, 0.25529680598992854, 0.25376853602938354, 0.2326735999668017, 0.24649765098001808, 0.2572443160461262, 0.23650792997796088, 0.2402055609272793, 0.32808868610300124, 0.26288569602184, 0.23624710994772613, 0.24258380196988583, 0.2613984199706465, 0.2596519269281998, 0.23168884706683457, 0.2449963439721614, 0.2630050670122728, 0.2594753719167784, 0.23329292598646134, 0.24627705395687371, 0.2510896639432758, 0.33846661797724664, 0.24161764606833458, 0.22369262797292322, 0.2457775700604543, 0.2502245729556307, 0.24393482808955014, 0.25103349599521607, 0.27770971099380404, 0.26320764294359833, 0.2560797290643677, 0.2546943810302764, 0.2630818149773404, 0.26535278104711324, 0.2832558579975739, 0.23776190099306405, 0.26786401704885066, 0.24795391003135592, 0.24810118891764432, 0.25467638904228806, 0.2548278180183843, 0.23733493592590094, 0.2517929590540007, 0.25385153596289456, 0.25801520596724004, 0.2598963350756094, 0.2420619090553373, 0.2529356270097196, 0.255023090983741, 0.25489382399246097, 0.26536557893268764, 0.2987099160673097, 0.33228596206754446, 0.23536081600468606, 0.25359410501550883, 0.25272317393682897, 0.2336242439923808, 0.25152748404070735, 0.2344519030302763, 0.2524399330141023, 0.306158383958973, 0.24022732605226338, 0.2332837920403108, 0.24322622700128704, 0.2779643180547282, 0.25105150195304304, 0.23570018098689616, 0.24657489103265107, 0.3511022230377421, 0.3119936949806288, 0.25823406409472227, 0.23094042192678899, 0.24163919896818697, 0.2479902469785884, 0.25958998897112906, 0.24032225098926574, 0.24779102602042258, 0.30118913401383907, 0.263260331004858, 0.2504458980401978, 0.24071147898212075, 0.246761190937832, 0.2669769109925255, 0.31932469899766147, 0.23485490307211876, 0.2488867260981351, 0.2558152989950031, 0.24024299392476678, 0.24996483395807445, 0.2621514940401539, 0.23552547302097082, 0.2285959660075605, 0.2531082920031622, 0.2528246589936316, 0.2558716490166262, 0.23284613096620888, 0.2502782370429486, 0.3282252000644803]
[0.008398266704964706, 0.006630944659594785, 0.006884034977040507, 0.006292052204530178, 0.006676141680641608, 0.0067724223163994875, 0.006303603705336255, 0.008395224910187111, 0.006960158683465455, 0.006800834250911562, 0.006536473477767272, 0.006781353636390783, 0.0070111160907386375, 0.008767682183216411, 0.00649840556698936, 0.008419580318414692, 0.006873237680834295, 0.006959743136328391, 0.006284630976469171, 0.006646816475867209, 0.006501811569747092, 0.006600498613393443, 0.006756917136954144, 0.006481100408233364, 0.006860508998877115, 0.007978722953174094, 0.006618300794136964, 0.006622030272741209, 0.00672894793255678, 0.006709016706074842, 0.006618713293309239, 0.006730080841490152, 0.006801562979076566, 0.006594468024559319, 0.006664853339316323, 0.006709758615629239, 0.006343887565890327, 0.006683387499916452, 0.007152824681146409, 0.006867586021226915, 0.006475870476358316, 0.006808485362191938, 0.008510575955204496, 0.006844163591846485, 0.006635914067208598, 0.006794220976933668, 0.006942759363234721, 0.006387270817702467, 0.008490717157573354, 0.006724559090798721, 0.006408786909146743, 0.006604105250020934, 0.0070547182483344595, 0.006619590387510305, 0.006266293295828456, 0.0066964062729808075, 0.0069446249531624335, 0.006361356862312691, 0.006499584182165563, 0.00707560631880452, 0.006854592909803614, 0.006343742409213023, 0.0073239427266261455, 0.006807138090318238, 0.00858742006874474, 0.006754955088994889, 0.006690765318291431, 0.007422737068157981, 0.008694392226805741, 0.006433150725735521, 0.006678090682147409, 0.006943844615968622, 0.006613924136300656, 0.006647838750557805, 0.006445436521475626, 0.008530277634484017, 0.006637745409865271, 0.0066844307055527515, 0.006942279250572689, 0.006445999545659023, 0.006678115272238342, 0.008571128249803389, 0.006601045773343437, 0.007275123749753799, 0.0064924385012339126, 0.006588496386856687, 0.006960982795466076, 0.007006285817955028, 0.006961748339976607, 0.006425712090938098, 0.006642692158824171, 0.0066261827728753405, 0.007745263407493688, 0.007880094045751983, 0.006407197636276992, 0.006525726136963137, 0.006632905929688026, 0.00661828151232628, 0.0062808880452499835, 0.006028839722708907, 0.006150395788122402, 0.008254259302237527, 0.005744186278735829, 0.005673439093042425, 0.00596512897687336, 0.00603552762862988, 0.0076852019302287074, 0.005885450001589434, 0.006278486418819358, 0.0058685866276581965, 0.005910801211220407, 0.00884624672133129, 0.007611499325004082, 0.005861559254658776, 0.006521548093080001, 0.007012911349383377, 0.0076955433252679055, 0.005986435812638077, 0.006288300696110656, 0.005575753326072942, 0.005664188815497382, 0.008191254117179575, 0.0064473727685507645, 0.005569579347773174, 0.008165162931153075, 0.0054320124638548425, 0.0076318969300319985, 0.0056675070230740796, 0.005294183372021761, 0.010171093417569822, 0.005882498023078539, 0.008140877048961472, 0.007122343628082511, 0.006135003162049796, 0.006258112955080389, 0.005652630325341814, 0.008306232722324515, 0.0062105601144478075, 0.006092034139518821, 0.006426641373721839, 0.005971969186523279, 0.007981003627592569, 0.006164587116367076, 0.0058020039753945065, 0.005871597812835907, 0.006453948325564175, 0.008018766884518745, 0.0056029045115073405, 0.005849166440257673, 0.0063389017690666195, 0.005627916814477811, 0.008475531396174501, 0.006016071745042884, 0.006317208975900051, 0.005744900511101235, 0.006374985231411492, 0.00780777151014136, 0.006556300606123756, 0.0057491673735988345, 0.006140953300759023, 0.005822323885378103, 0.008208038558815281, 0.006155007348265932, 0.005653547672663144, 0.005896215489524049, 0.006128844139057883, 0.008356092744073722, 0.006100266906541101, 0.005970530163193512, 0.006223463068911156, 0.005832197672557519, 0.007982049394550538, 0.00597574604880931, 0.0058607750485629535, 0.009608354652300477, 0.006296934161963331, 0.00799463365259472, 0.0066078509296138965, 0.005903002881813188, 0.006081987326683173, 0.006344990861645445, 0.007484354070647684, 0.005410931303252487, 0.0061283615596462475, 0.006580901789738861, 0.00570334842006221, 0.0056540712546340605, 0.0067725522568189474, 0.005978906186037632, 0.005767335370724457, 0.008200151304346185, 0.005663622067808066, 0.005808199046526191, 0.005982973718430934, 0.005840800602400545, 0.005782342396763175, 0.005612850557406282, 0.006093216488722625, 0.005654422046486722, 0.0068514530687744536, 0.005777024858912756, 0.005937135023021594, 0.005901593861148455, 0.005411013952716318, 0.0057325035111632115, 0.005982425954561074, 0.005500184418092114, 0.005586175835518123, 0.0076299694442558425, 0.006113620837717209, 0.005494118835993631, 0.005641483766741531, 0.006079033022573174, 0.006038416905306973, 0.005388112722484525, 0.005697589394701428, 0.006116396907262157, 0.0060343109748088, 0.005425416883406078, 0.005727373347834273, 0.00583929451030874, 0.00787131669714527, 0.00561901502484499, 0.005202154138905191, 0.0057157574432663795, 0.005819176115247226, 0.005672902978826748, 0.005837988278958513, 0.0064583653719489315, 0.006121107975432519, 0.0059553425363806445, 0.005923125140238987, 0.006118181743659079, 0.006170994908072401, 0.0065873455348273, 0.00552934653472242, 0.006229395745322108, 0.005766370000729207, 0.005769795091108008, 0.005922706721913676, 0.005926228326008936, 0.005519417114555836, 0.0058556502105581556, 0.005903524092160339, 0.006000353627145117, 0.006044100815711846, 0.005629346722217147, 0.0058822238839469675, 0.0059307695577614184, 0.005927763348661883, 0.0061712925333183176, 0.006946742234123481, 0.007727580513198709, 0.005473507348946188, 0.0058975373259420654, 0.005877283114809976, 0.005433121953311182, 0.005849476373039706, 0.005452369837913402, 0.0058706961166070295, 0.0071199624176505345, 0.005586682001215427, 0.005425204466053739, 0.005656423883750861, 0.006464286466389028, 0.005838407022163792, 0.0054813995578347945, 0.0057342997914570015, 0.008165167977621908, 0.007255667325130902, 0.006005443351040053, 0.005370707486669512, 0.005619516255074116, 0.005767215046013684, 0.006036976487700676, 0.005588889557889901, 0.0057625820004749435, 0.007004398465438118, 0.006122333279182744, 0.005824323210237158, 0.005597941371677227, 0.005738632347391442, 0.006208765371919198, 0.00742615579064329, 0.005461741931909739, 0.005788063397631049, 0.005949192999883793, 0.005587046370343413, 0.005813135673443592, 0.0060965463730268355, 0.005477336581883042, 0.005316185255989779, 0.005886239348910749, 0.005879643232410037, 0.005950503465502934, 0.005415026301539741, 0.005820424117277874, 0.007633144187546054]
[119.07218895642379, 150.8080750686147, 145.2636431010562, 158.93065847101775, 149.78711474917282, 147.6576553087202, 158.63941433270298, 119.11533171512282, 143.67488522576343, 147.04078398410653, 152.98769334891875, 147.46318413977104, 142.6306435463184, 114.05522909056354, 153.88390116489592, 118.77076554669513, 145.4918404449265, 143.6834636583369, 159.11833228461404, 150.4479631159863, 153.80328840241953, 151.50370579138428, 147.9964871155392, 154.29478591777948, 145.7617795069832, 125.3333404191181, 151.0961848222254, 151.01108856544792, 148.61164182318657, 149.0531390530785, 151.08676802950288, 148.58662526534872, 147.02502984626747, 151.64225473165834, 150.04081096592884, 149.03665799104462, 157.63204968776208, 149.62472249476795, 139.80490849102236, 145.61157252477295, 154.41939483668403, 146.87554526489544, 117.50086072476296, 146.10989152733129, 150.6951400925315, 147.1839087063834, 144.03495032472034, 156.5613903873431, 117.77568154040357, 148.70863449892346, 156.0357699789926, 151.4209665263633, 141.74910532197214, 151.06674906755222, 159.58397617068317, 149.33383060028473, 143.99625706851475, 157.1991670400404, 153.8559963180314, 141.3306443212287, 145.88758415832024, 157.63565660353723, 136.53847897588093, 146.90461493976383, 116.4494099502197, 148.0394742563412, 149.45973329331514, 134.72119392316813, 115.01666521518239, 155.44482674710954, 149.7433993631305, 144.0124391177072, 151.196170290415, 150.42482790607585, 155.1485297649101, 117.22947866989192, 150.65356355996448, 149.6013713133866, 144.04491146297622, 155.13497835621743, 149.74284797944557, 116.67075452090438, 151.49114766606124, 137.45470653112142, 154.0253326712215, 151.77969923378694, 143.65787553035383, 142.72897594861135, 143.64207827761842, 155.62477525413198, 150.54137330022093, 150.91645284726485, 129.1111673532602, 126.90203875664176, 156.07447386016122, 153.23965165129775, 150.7634829440487, 151.0966250283462, 159.21315469971879, 165.86939543827768, 162.59116233319367, 121.14957422393124, 174.08906178789144, 176.25993398366464, 167.64096868265082, 165.68559727180127, 130.12019841230645, 169.91054205369826, 159.27405640355684, 170.3987797141951, 169.181801969877, 113.04229143741402, 131.38016011050016, 170.60306934630037, 153.33782496537864, 142.59413105056956, 129.94534079439893, 167.04430337144538, 159.02547418231842, 179.3479627808983, 176.54778690709807, 122.08142803220973, 155.10193623018623, 179.54677320466212, 122.47153038240491, 184.09383385146862, 131.0290232124253, 176.44442184697917, 188.886543916237, 98.31784636571845, 169.99580723644024, 122.83688771931146, 140.40322290223753, 162.99910099245736, 159.79257759932116, 176.90879156148074, 120.39152205696305, 161.01607287781835, 164.14878464206785, 155.60227214310441, 167.4489550710782, 125.29752480536652, 162.2168656429535, 172.35424247223222, 170.31139255040577, 154.94391178172077, 124.70745370221799, 178.4788582325797, 170.96453147877034, 157.75603352617443, 177.68563981391853, 117.98670233837586, 166.221421947632, 158.29775519774125, 174.0674182377287, 156.86310849360024, 128.07751849565778, 152.52503813903436, 173.9382305326806, 162.84116667625526, 171.75272617714563, 121.83178634389071, 162.46934299464857, 176.88008625722668, 169.60031426543426, 163.16290271230793, 119.6731571354586, 163.92725356455065, 167.48931379070714, 160.68224217404378, 171.46195244810397, 125.2811089696732, 167.34312198545547, 170.6258970381737, 104.07609171259882, 158.80744093538505, 125.08390546143941, 151.33513310937101, 169.40530438854137, 164.41994142486195, 157.6046399128572, 133.61206465656448, 184.81106928836894, 163.17575101716483, 151.95485846016268, 175.33559697709862, 176.86370669283713, 147.65482230028562, 167.2546731599956, 173.39029824346525, 121.94896934035684, 176.56545370214295, 172.17040807134308, 167.1409648548908, 171.20940570869755, 172.94029501950237, 178.1625913201052, 164.1169326333322, 176.85273433406564, 145.95444060727894, 173.09948016879423, 168.43140607758465, 169.44575033928194, 184.8082464281952, 174.44385303081742, 167.15626864342346, 181.81208555673788, 179.01334104841135, 131.06212381398743, 163.56918862724146, 182.01280857791028, 177.25833155726525, 164.4998466510572, 165.6063196168406, 185.5937415390389, 175.51282318272484, 163.4949489318906, 165.71900324240175, 184.31763337091246, 174.60010711160226, 171.25356466172266, 127.04354791907622, 177.96713402231677, 192.22806039546782, 174.9549399053104, 171.84563247361268, 176.27659132058997, 171.29188210333274, 154.83794155458745, 163.3691161818363, 167.91645382126907, 168.8297944621261, 163.44725310528835, 162.04842410287523, 151.8062161325832, 180.85319733902358, 160.52921356793527, 173.4193261746196, 173.31638025432272, 168.84172169120873, 168.74138912454924, 181.1785518733119, 170.77522803478402, 169.39034793268024, 166.65684426932447, 165.45058239274664, 177.6405059672972, 170.00372983576423, 168.61218266208476, 168.69769273527717, 162.04060893258253, 143.95236879351074, 129.4066102956805, 182.69821089991405, 169.56230113223762, 170.14664437044598, 184.05624033352618, 170.95547297344595, 183.40648740414417, 170.337551141712, 140.45017955726553, 178.99712204532887, 184.32485010604472, 176.79014524931335, 154.69611459818293, 171.27959667830535, 182.4351590226, 174.38920816274845, 122.47145468907405, 137.82329800821714, 166.51559952302523, 186.19520844917977, 177.95126032370752, 173.3939157845697, 165.6458331479892, 178.92641993404365, 173.53332237486276, 142.7674346247306, 163.3364200214673, 171.69376834073077, 178.6370977480218, 174.25754769854893, 161.06261713846806, 134.65917335857225, 183.0917704400476, 172.76935847131222, 168.09002498650375, 178.9854484308735, 172.02419764058575, 164.02729329253285, 182.57048568233358, 188.10480670764696, 169.88775697424714, 170.0783466737836, 168.05300690896755, 184.67130985414678, 171.808785726028, 131.0076130399268]
Elapsed: 0.2801930404790902~0.0381550996838702
Time per graph: 0.006463708358701055~0.0008529389495294334
Speed: 157.11156998148667~18.43079147840088
Total Time: 0.3290
best val loss: 0.25094425678253174 test_score: 0.8605

Testing...
Test loss: 0.3968 score: 0.8605 time: 0.23s
test Score 0.8605
Epoch Time List: [0.9872064890805632, 0.9171857718611136, 0.9412238380173221, 0.8621066370978951, 0.9233129039639607, 0.9179292449261993, 0.8681507860310376, 0.9766329169506207, 0.9311045568902045, 0.949998828000389, 0.9062795559875667, 0.9235974840121344, 0.9447191280778497, 1.0073519999859855, 0.88636977202259, 1.0117616989882663, 0.9057127689011395, 1.022983851027675, 0.851082180859521, 0.9888799529289827, 0.8906996871810406, 0.9022106871707365, 0.9658788319211453, 0.8790028251241893, 1.097608622862026, 0.9715511448448524, 0.8977847038768232, 0.8970914529636502, 1.0034707499435171, 0.9394364199833944, 0.9161871860269457, 0.883654656005092, 0.9158092540455982, 0.9047825869638473, 0.8982818530639634, 0.99709671898745, 0.9707248149206862, 0.9129735750611871, 0.9275921410880983, 1.0157559911021963, 0.889595712069422, 0.9773796498775482, 1.017859356943518, 0.9356499480782077, 0.8873381270095706, 0.9236242540646344, 1.0236177049810067, 0.8691154149128124, 1.0107488799840212, 0.933516840916127, 0.8936825640266761, 0.9507751279743388, 0.9676882581552491, 0.9255808771122247, 0.9954710860038176, 0.9090603881049901, 1.075700337998569, 0.8561906120739877, 1.0030003801221028, 0.990406752214767, 0.925515174982138, 0.8540504378033802, 0.9349362479988486, 1.0044925319962204, 1.0278184668859467, 0.9069808259373531, 1.0548239500494674, 0.9354174728505313, 1.0633592320373282, 0.9001998530002311, 0.9029836760601029, 0.9230084370356053, 0.9239845089614391, 0.9143341329181567, 0.9092096789972857, 1.0247330599231645, 0.8683197769569233, 0.9046093750512227, 1.04873417399358, 0.8880371160339564, 0.9951755091315135, 1.075993465143256, 0.9567119519924745, 0.9391264340374619, 0.9289557769661769, 1.0378514809999615, 1.035478727077134, 1.0129994129529223, 1.0916588800027966, 0.8711498159682378, 0.9565573589643463, 1.0027554730186239, 1.0611365520162508, 0.9693381859688088, 0.9545569409383461, 1.0181502680061385, 0.9121760150883347, 0.9550224050180987, 0.9165428549749777, 0.9151390560436994, 0.9105445349123329, 1.0173018489731476, 0.8746701141353697, 0.9505884851096198, 0.942069040145725, 0.9206739440560341, 0.9795436530839652, 0.9303048241417855, 0.9248503510607406, 0.8699363610940054, 0.9979156440822408, 1.1513327419525012, 1.0604912790004164, 0.9580656790640205, 1.0301272069336846, 1.002223705057986, 0.9529627431184053, 0.914357498055324, 0.9035087139345706, 0.8579494009027258, 0.9571078560547903, 1.0111478429753333, 0.9201573230093345, 0.8549610059708357, 1.1002935951109976, 0.8496639899676666, 0.978095663129352, 0.977066679042764, 0.8734497808618471, 1.0119287909474224, 0.9233890390023589, 1.1574248949764296, 1.1703419991536066, 0.9520237660035491, 1.027281176066026, 0.8820301960222423, 1.0432953820563853, 0.9496708920923993, 0.9375446230405942, 0.946900884155184, 0.9391532019944862, 1.0214888480259106, 0.9409343319712207, 0.9701804508222267, 0.9151412041392177, 0.9716943989042193, 0.9896062921034172, 0.9760444920975715, 0.9194726390996948, 0.9249236768810079, 0.8513757409527898, 1.051271847099997, 0.9196274301502854, 0.9542346750386059, 0.8599018049426377, 0.9385795691050589, 1.0408422260079533, 0.9294686750508845, 0.8693957749055699, 0.980028664926067, 0.8831559360260144, 1.064206365030259, 0.9011547049740329, 0.8487857939908281, 0.9363734319340438, 0.9538430790416896, 1.0169824010226876, 0.9339258359977975, 0.9281866560922936, 1.0368575940374285, 0.8893089250195771, 1.0080642100656405, 0.9276587850181386, 1.0649786670692265, 1.0899559259414673, 0.9331676970468834, 1.115807453985326, 0.9267746009863913, 1.0201145480386913, 0.9268636310007423, 0.970988218090497, 0.9384288610890508, 0.9239218229195103, 0.9350583100458607, 1.054211699985899, 0.8958016629330814, 1.1155080229509622, 0.9799750880338252, 0.9581908809486777, 0.9254480750532821, 1.013246804010123, 0.9628847871208563, 0.9359367699362338, 0.912383352057077, 0.9516614738386124, 0.9101090179756284, 0.9927445400971919, 0.9422744180774316, 0.905924868886359, 0.9736776199424639, 0.9645943200448528, 1.0479773679981008, 0.9372222629608586, 0.9372159850317985, 0.9368535759858787, 0.9419271369697526, 0.9207468188833445, 1.0153544148197398, 1.0058532310649753, 1.012211550027132, 0.9058414699975401, 0.9223586759762838, 1.017675039009191, 0.9912332680542022, 0.8694950910285115, 0.9299479119945318, 0.9443708410253748, 1.0127794928848743, 0.8817049788776785, 0.9286042150342837, 0.9417877070372924, 1.0182481358060613, 0.8816135101951659, 0.8987502600066364, 0.9219861961901188, 0.8967772290343419, 0.9039580209646374, 0.9927923360373825, 0.9653979449067265, 0.9017160061048344, 0.9499916509957984, 0.9394828140502796, 1.0903264702064916, 1.1093992328969762, 0.9776837560348213, 0.9120947130722925, 1.0321023538708687, 0.9770330360624939, 1.149711123900488, 0.935504550114274, 0.9581668480532244, 0.8964032459771261, 0.9715736807556823, 1.0758212100481614, 0.9250867150258273, 0.9486219210084528, 0.9204564311075956, 0.9788351589813828, 1.0708935540169477, 0.9449869130039588, 1.0182154208887368, 0.9833970940671861, 1.0070200769696385, 0.9358233469538391, 0.9225965020013973, 1.0431913509964943, 0.8695805069291964, 1.0163923589279875, 1.0489148129709065, 0.9851885009557009, 1.0011414238251746, 0.8858877688180655, 0.9144860119558871, 1.0376920390408486, 0.9715283120749518, 0.9391182130202651, 0.9272508149733767, 1.0268400841159746, 1.0154022660572082, 1.085585170891136, 0.9403511660639197, 0.8930307681439444, 1.015009721973911, 0.9190315841697156, 1.0229079889832065, 0.89303362602368, 0.9637556859524921, 1.0276762529974803, 0.973374270950444, 0.9657892560353503, 0.9216677059885114, 0.9757673239801079, 0.9680045769782737, 0.9551113621564582, 0.9231858671410009, 0.9270529299974442, 0.9291574930539355, 0.9226750420639291, 0.9117861350532621, 1.012815430876799, 0.8769180119270459, 0.9230478159151971, 0.928326094057411, 0.9294574760133401, 1.0312798349186778, 0.9292049829382449, 0.9418820169521496, 0.9945056210272014]
Total Epoch List: [97, 85, 116]
Total Time List: [0.29230970598291606, 0.32242769096046686, 0.3290065820328891]
========================training times:5========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcc4c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.30s
Epoch 2/1000, LR 0.000000
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.38s
Epoch 3/1000, LR 0.000030
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.28s
Epoch 4/1000, LR 0.000060
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.30s
Epoch 5/1000, LR 0.000090
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.30s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.28s
Epoch 7/1000, LR 0.000150
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.46s
Val loss: 0.6930 score: 0.5349 time: 0.27s
Test loss: 0.6930 score: 0.5227 time: 0.30s
Epoch 8/1000, LR 0.000180
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
Val loss: 0.6930 score: 0.5116 time: 0.24s
Test loss: 0.6929 score: 0.6136 time: 0.31s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.39s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.29s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.29s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.28s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.30s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.29s
Epoch 15/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.37s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.29s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.30s
Epoch 18/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.30s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.28s
Epoch 20/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.30s
Epoch 21/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.29s
Epoch 22/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.30s
Epoch 23/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.31s
Epoch 24/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4884 time: 0.27s
Test loss: 0.6897 score: 0.5227 time: 0.28s
Epoch 25/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4884 time: 0.26s
Test loss: 0.6893 score: 0.5227 time: 0.33s
Epoch 26/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4884 time: 0.32s
Test loss: 0.6888 score: 0.5682 time: 0.29s
Epoch 27/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.42s
Val loss: 0.6903 score: 0.5116 time: 0.24s
Test loss: 0.6882 score: 0.6136 time: 0.30s
Epoch 28/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.33s
Val loss: 0.6898 score: 0.5814 time: 0.27s
Test loss: 0.6876 score: 0.6591 time: 0.29s
Epoch 29/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.43s
Val loss: 0.6893 score: 0.6279 time: 0.26s
Test loss: 0.6870 score: 0.6364 time: 0.30s
Epoch 30/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.38s
Val loss: 0.6888 score: 0.6744 time: 0.25s
Test loss: 0.6863 score: 0.6591 time: 0.30s
Epoch 31/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.36s
Val loss: 0.6881 score: 0.6744 time: 0.34s
Test loss: 0.6856 score: 0.6818 time: 0.30s
Epoch 32/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.33s
Val loss: 0.6875 score: 0.6744 time: 0.27s
Test loss: 0.6848 score: 0.6818 time: 0.29s
Epoch 33/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.39s
Val loss: 0.6868 score: 0.6512 time: 0.25s
Test loss: 0.6840 score: 0.7045 time: 0.29s
Epoch 34/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.37s
Val loss: 0.6861 score: 0.6977 time: 0.24s
Test loss: 0.6831 score: 0.7045 time: 0.29s
Epoch 35/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.36s
Val loss: 0.6853 score: 0.6744 time: 0.28s
Test loss: 0.6821 score: 0.7273 time: 0.28s
Epoch 36/1000, LR 0.000270
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.35s
Val loss: 0.6844 score: 0.6977 time: 0.24s
Test loss: 0.6810 score: 0.7273 time: 0.30s
Epoch 37/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.37s
Val loss: 0.6834 score: 0.7442 time: 0.25s
Test loss: 0.6798 score: 0.7727 time: 0.29s
Epoch 38/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.47s
Val loss: 0.6823 score: 0.7907 time: 0.26s
Test loss: 0.6786 score: 0.7955 time: 0.28s
Epoch 39/1000, LR 0.000269
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.37s
Val loss: 0.6811 score: 0.7907 time: 0.26s
Test loss: 0.6772 score: 0.7955 time: 0.29s
Epoch 40/1000, LR 0.000269
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.37s
Val loss: 0.6798 score: 0.8140 time: 0.24s
Test loss: 0.6757 score: 0.7955 time: 0.31s
Epoch 41/1000, LR 0.000269
Train loss: 0.6746;  Loss pred: 0.6746; Loss self: 0.0000; time: 0.37s
Val loss: 0.6784 score: 0.8140 time: 0.25s
Test loss: 0.6740 score: 0.8182 time: 0.29s
Epoch 42/1000, LR 0.000269
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.36s
Val loss: 0.6768 score: 0.8372 time: 0.26s
Test loss: 0.6722 score: 0.8182 time: 0.29s
Epoch 43/1000, LR 0.000269
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.37s
Val loss: 0.6750 score: 0.8372 time: 0.24s
Test loss: 0.6703 score: 0.8636 time: 0.30s
Epoch 44/1000, LR 0.000269
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.44s
Val loss: 0.6731 score: 0.8372 time: 0.25s
Test loss: 0.6681 score: 0.8636 time: 0.38s
Epoch 45/1000, LR 0.000269
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 0.35s
Val loss: 0.6710 score: 0.8140 time: 0.25s
Test loss: 0.6658 score: 0.8864 time: 0.28s
Epoch 46/1000, LR 0.000269
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.38s
Val loss: 0.6686 score: 0.8140 time: 0.26s
Test loss: 0.6632 score: 0.9091 time: 0.29s
Epoch 47/1000, LR 0.000269
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.36s
Val loss: 0.6660 score: 0.7907 time: 0.24s
Test loss: 0.6604 score: 0.9318 time: 0.29s
Epoch 48/1000, LR 0.000269
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.46s
Val loss: 0.6631 score: 0.8140 time: 0.25s
Test loss: 0.6573 score: 0.9318 time: 0.30s
Epoch 49/1000, LR 0.000269
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.34s
Val loss: 0.6600 score: 0.8372 time: 0.26s
Test loss: 0.6541 score: 0.9318 time: 0.28s
Epoch 50/1000, LR 0.000269
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.37s
Val loss: 0.6566 score: 0.8372 time: 0.33s
Test loss: 0.6505 score: 0.9318 time: 0.37s
Epoch 51/1000, LR 0.000269
Train loss: 0.6428;  Loss pred: 0.6428; Loss self: 0.0000; time: 0.37s
Val loss: 0.6529 score: 0.8372 time: 0.25s
Test loss: 0.6467 score: 0.9091 time: 0.30s
Epoch 52/1000, LR 0.000269
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.49s
Val loss: 0.6489 score: 0.8372 time: 0.25s
Test loss: 0.6426 score: 0.8864 time: 0.30s
Epoch 53/1000, LR 0.000269
Train loss: 0.6361;  Loss pred: 0.6361; Loss self: 0.0000; time: 0.32s
Val loss: 0.6446 score: 0.8372 time: 0.27s
Test loss: 0.6382 score: 0.8864 time: 0.27s
Epoch 54/1000, LR 0.000269
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.42s
Val loss: 0.6400 score: 0.8605 time: 0.26s
Test loss: 0.6335 score: 0.8864 time: 0.30s
Epoch 55/1000, LR 0.000269
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 0.39s
Val loss: 0.6351 score: 0.8605 time: 0.25s
Test loss: 0.6284 score: 0.8864 time: 0.30s
Epoch 56/1000, LR 0.000269
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.53s
Val loss: 0.6296 score: 0.8605 time: 0.25s
Test loss: 0.6230 score: 0.8864 time: 0.31s
Epoch 57/1000, LR 0.000269
Train loss: 0.6093;  Loss pred: 0.6093; Loss self: 0.0000; time: 0.34s
Val loss: 0.6238 score: 0.8605 time: 0.30s
Test loss: 0.6171 score: 0.8864 time: 0.39s
Epoch 58/1000, LR 0.000269
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.34s
Val loss: 0.6175 score: 0.8605 time: 0.26s
Test loss: 0.6109 score: 0.8864 time: 0.29s
Epoch 59/1000, LR 0.000268
Train loss: 0.5923;  Loss pred: 0.5923; Loss self: 0.0000; time: 0.37s
Val loss: 0.6107 score: 0.8605 time: 0.36s
Test loss: 0.6042 score: 0.8864 time: 0.29s
Epoch 60/1000, LR 0.000268
Train loss: 0.5888;  Loss pred: 0.5888; Loss self: 0.0000; time: 0.39s
Val loss: 0.6035 score: 0.8605 time: 0.25s
Test loss: 0.5971 score: 0.8864 time: 0.31s
Epoch 61/1000, LR 0.000268
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.42s
Val loss: 0.5959 score: 0.8605 time: 0.34s
Test loss: 0.5895 score: 0.8864 time: 0.30s
Epoch 62/1000, LR 0.000268
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.36s
Val loss: 0.5878 score: 0.8605 time: 0.32s
Test loss: 0.5815 score: 0.8864 time: 0.35s
Epoch 63/1000, LR 0.000268
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.32s
Val loss: 0.5795 score: 0.8605 time: 0.27s
Test loss: 0.5733 score: 0.8864 time: 0.28s
Epoch 64/1000, LR 0.000268
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.39s
Val loss: 0.5707 score: 0.8605 time: 0.26s
Test loss: 0.5648 score: 0.8864 time: 0.38s
Epoch 65/1000, LR 0.000268
Train loss: 0.5442;  Loss pred: 0.5442; Loss self: 0.0000; time: 0.37s
Val loss: 0.5616 score: 0.8605 time: 0.25s
Test loss: 0.5560 score: 0.8864 time: 0.29s
Epoch 66/1000, LR 0.000268
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.47s
Val loss: 0.5522 score: 0.8605 time: 0.25s
Test loss: 0.5469 score: 0.8864 time: 0.32s
Epoch 67/1000, LR 0.000268
Train loss: 0.5215;  Loss pred: 0.5215; Loss self: 0.0000; time: 0.38s
Val loss: 0.5425 score: 0.8605 time: 0.26s
Test loss: 0.5377 score: 0.8864 time: 0.29s
Epoch 68/1000, LR 0.000268
Train loss: 0.4962;  Loss pred: 0.4962; Loss self: 0.0000; time: 0.36s
Val loss: 0.5326 score: 0.8605 time: 0.28s
Test loss: 0.5285 score: 0.8864 time: 0.30s
Epoch 69/1000, LR 0.000268
Train loss: 0.4925;  Loss pred: 0.4925; Loss self: 0.0000; time: 0.39s
Val loss: 0.5223 score: 0.8605 time: 0.26s
Test loss: 0.5190 score: 0.8864 time: 0.30s
Epoch 70/1000, LR 0.000268
Train loss: 0.4835;  Loss pred: 0.4835; Loss self: 0.0000; time: 0.45s
Val loss: 0.5120 score: 0.8605 time: 0.25s
Test loss: 0.5096 score: 0.8864 time: 0.38s
Epoch 71/1000, LR 0.000268
Train loss: 0.4541;  Loss pred: 0.4541; Loss self: 0.0000; time: 0.41s
Val loss: 0.5015 score: 0.8605 time: 0.25s
Test loss: 0.5004 score: 0.8864 time: 0.33s
Epoch 72/1000, LR 0.000267
Train loss: 0.4602;  Loss pred: 0.4602; Loss self: 0.0000; time: 0.35s
Val loss: 0.4908 score: 0.8605 time: 0.27s
Test loss: 0.4908 score: 0.8864 time: 0.29s
Epoch 73/1000, LR 0.000267
Train loss: 0.4306;  Loss pred: 0.4306; Loss self: 0.0000; time: 0.39s
Val loss: 0.4799 score: 0.8605 time: 0.29s
Test loss: 0.4808 score: 0.8864 time: 0.30s
Epoch 74/1000, LR 0.000267
Train loss: 0.4154;  Loss pred: 0.4154; Loss self: 0.0000; time: 0.39s
Val loss: 0.4689 score: 0.8605 time: 0.31s
Test loss: 0.4705 score: 0.8864 time: 0.40s
Epoch 75/1000, LR 0.000267
Train loss: 0.4073;  Loss pred: 0.4073; Loss self: 0.0000; time: 0.49s
Val loss: 0.4580 score: 0.8837 time: 0.29s
Test loss: 0.4606 score: 0.8864 time: 0.30s
Epoch 76/1000, LR 0.000267
Train loss: 0.4004;  Loss pred: 0.4004; Loss self: 0.0000; time: 0.39s
Val loss: 0.4474 score: 0.8837 time: 0.28s
Test loss: 0.4520 score: 0.8864 time: 0.35s
Epoch 77/1000, LR 0.000267
Train loss: 0.3771;  Loss pred: 0.3771; Loss self: 0.0000; time: 0.40s
Val loss: 0.4368 score: 0.8837 time: 0.29s
Test loss: 0.4431 score: 0.8864 time: 0.39s
Epoch 78/1000, LR 0.000267
Train loss: 0.3802;  Loss pred: 0.3802; Loss self: 0.0000; time: 0.37s
Val loss: 0.4264 score: 0.8837 time: 0.25s
Test loss: 0.4346 score: 0.8864 time: 0.30s
Epoch 79/1000, LR 0.000267
Train loss: 0.3489;  Loss pred: 0.3489; Loss self: 0.0000; time: 0.39s
Val loss: 0.4164 score: 0.9070 time: 0.26s
Test loss: 0.4270 score: 0.8864 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.3300;  Loss pred: 0.3300; Loss self: 0.0000; time: 0.36s
Val loss: 0.4063 score: 0.9070 time: 0.27s
Test loss: 0.4188 score: 0.8864 time: 0.29s
Epoch 81/1000, LR 0.000267
Train loss: 0.3196;  Loss pred: 0.3196; Loss self: 0.0000; time: 0.40s
Val loss: 0.3965 score: 0.9070 time: 0.29s
Test loss: 0.4112 score: 0.8864 time: 0.29s
Epoch 82/1000, LR 0.000267
Train loss: 0.3286;  Loss pred: 0.3286; Loss self: 0.0000; time: 0.38s
Val loss: 0.3869 score: 0.9070 time: 0.26s
Test loss: 0.4029 score: 0.8864 time: 0.29s
Epoch 83/1000, LR 0.000266
Train loss: 0.2878;  Loss pred: 0.2878; Loss self: 0.0000; time: 0.50s
Val loss: 0.3779 score: 0.9070 time: 0.24s
Test loss: 0.3953 score: 0.8864 time: 0.39s
Epoch 84/1000, LR 0.000266
Train loss: 0.2993;  Loss pred: 0.2993; Loss self: 0.0000; time: 0.43s
Val loss: 0.3695 score: 0.9070 time: 0.25s
Test loss: 0.3883 score: 0.8864 time: 0.29s
Epoch 85/1000, LR 0.000266
Train loss: 0.2582;  Loss pred: 0.2582; Loss self: 0.0000; time: 0.36s
Val loss: 0.3617 score: 0.9070 time: 0.27s
Test loss: 0.3833 score: 0.8864 time: 0.29s
Epoch 86/1000, LR 0.000266
Train loss: 0.2505;  Loss pred: 0.2505; Loss self: 0.0000; time: 0.38s
Val loss: 0.3547 score: 0.9070 time: 0.25s
Test loss: 0.3807 score: 0.8864 time: 0.29s
Epoch 87/1000, LR 0.000266
Train loss: 0.2192;  Loss pred: 0.2192; Loss self: 0.0000; time: 0.49s
Val loss: 0.3484 score: 0.8837 time: 0.24s
Test loss: 0.3808 score: 0.8636 time: 0.30s
Epoch 88/1000, LR 0.000266
Train loss: 0.1993;  Loss pred: 0.1993; Loss self: 0.0000; time: 0.34s
Val loss: 0.3427 score: 0.8837 time: 0.26s
Test loss: 0.3800 score: 0.8636 time: 0.28s
Epoch 89/1000, LR 0.000266
Train loss: 0.2328;  Loss pred: 0.2328; Loss self: 0.0000; time: 0.45s
Val loss: 0.3375 score: 0.8837 time: 0.27s
Test loss: 0.3783 score: 0.8636 time: 0.27s
Epoch 90/1000, LR 0.000266
Train loss: 0.2110;  Loss pred: 0.2110; Loss self: 0.0000; time: 0.37s
Val loss: 0.3328 score: 0.8837 time: 0.27s
Test loss: 0.3733 score: 0.8636 time: 0.29s
Epoch 91/1000, LR 0.000266
Train loss: 0.1936;  Loss pred: 0.1936; Loss self: 0.0000; time: 0.43s
Val loss: 0.3291 score: 0.8837 time: 0.24s
Test loss: 0.3675 score: 0.8636 time: 0.30s
Epoch 92/1000, LR 0.000266
Train loss: 0.1927;  Loss pred: 0.1927; Loss self: 0.0000; time: 0.33s
Val loss: 0.3267 score: 0.8837 time: 0.26s
Test loss: 0.3636 score: 0.8636 time: 0.28s
Epoch 93/1000, LR 0.000265
Train loss: 0.1579;  Loss pred: 0.1579; Loss self: 0.0000; time: 0.37s
Val loss: 0.3252 score: 0.8837 time: 0.25s
Test loss: 0.3625 score: 0.8636 time: 0.29s
Epoch 94/1000, LR 0.000265
Train loss: 0.1651;  Loss pred: 0.1651; Loss self: 0.0000; time: 0.51s
Val loss: 0.3244 score: 0.8837 time: 0.35s
Test loss: 0.3622 score: 0.8636 time: 0.30s
Epoch 95/1000, LR 0.000265
Train loss: 0.1488;  Loss pred: 0.1488; Loss self: 0.0000; time: 0.40s
Val loss: 0.3243 score: 0.8605 time: 0.25s
Test loss: 0.3652 score: 0.8636 time: 0.28s
Epoch 96/1000, LR 0.000265
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 0.38s
Val loss: 0.3250 score: 0.8605 time: 0.26s
Test loss: 0.3714 score: 0.8636 time: 0.30s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.1698;  Loss pred: 0.1698; Loss self: 0.0000; time: 0.38s
Val loss: 0.3268 score: 0.8837 time: 0.24s
Test loss: 0.3784 score: 0.8409 time: 0.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.1488,   Val_Loss: 0.3243,   Val_Precision: 0.8636,   Val_Recall: 0.8636,   Val_accuracy: 0.8636,   Val_Score: 0.8605,   Val_Loss: 0.3243,   Test_Precision: 0.8636,   Test_Recall: 0.8636,   Test_accuracy: 0.8636,   Test_Score: 0.8636,   Test_loss: 0.3652


[0.3008384720887989, 0.3823679880006239, 0.28155691106803715, 0.30424436810426414, 0.30634041398297995, 0.28447832097299397, 0.2997548639541492, 0.317103725974448, 0.39863283606246114, 0.29564567806664854, 0.29581137106288224, 0.28361519204918295, 0.3089027020614594, 0.29900632600765675, 0.37476245197467506, 0.2895199320046231, 0.3014188609085977, 0.30226633604615927, 0.2810097150504589, 0.3027656190097332, 0.29860675300005823, 0.30802042596042156, 0.313578751985915, 0.2873057370306924, 0.3321150050032884, 0.2960165160475299, 0.3038980320561677, 0.29250062606297433, 0.3042488479986787, 0.30717705795541406, 0.30715376790612936, 0.29152516496833414, 0.2932433240348473, 0.2946956770028919, 0.28895359102170914, 0.3053249450167641, 0.29505437694024295, 0.28553690400440246, 0.29922581498976797, 0.3113178750500083, 0.2932825960451737, 0.2941406930331141, 0.30295683699660003, 0.3821052919374779, 0.2816089439438656, 0.2940084059955552, 0.2963209699373692, 0.3004275569692254, 0.288389221066609, 0.3778758469270542, 0.3051485070027411, 0.30833206593524665, 0.27748763596173376, 0.29932345694396645, 0.3052681309636682, 0.3117521529784426, 0.39632976707071066, 0.2919371130410582, 0.29500709706917405, 0.3106193230487406, 0.30851817107759416, 0.359289790969342, 0.28554788103792816, 0.38195042207371444, 0.29580746905412525, 0.3198438809486106, 0.29401424899697304, 0.30215807794593275, 0.30736375297419727, 0.3812034389702603, 0.3304374360013753, 0.29590921103954315, 0.30480025010183454, 0.40141311509069055, 0.3006723370635882, 0.34995029494166374, 0.3895039709750563, 0.30328268895391375, 0.30473851796705276, 0.2939894929295406, 0.29239833797328174, 0.29774714494124055, 0.3943545169895515, 0.2956749899312854, 0.29446942696813494, 0.2932730669854209, 0.3029289960395545, 0.28301976900547743, 0.27888675802387297, 0.2925260589690879, 0.30297933996189386, 0.287580223986879, 0.29590417398139834, 0.3037118320353329, 0.28649473492987454, 0.2999019300332293, 0.30556270806118846]
[0.006837238002018156, 0.008690181545468724, 0.006399020706091754, 0.006914644729642367, 0.006962282135976817, 0.006465416385749863, 0.0068126105444124814, 0.007206902863055637, 0.009059837183237753, 0.006719219956060194, 0.006722985705974596, 0.006445799819299613, 0.00702051595594226, 0.0067955983183558355, 0.008517328453969887, 0.006579998454650526, 0.006850428657013585, 0.006869689455594529, 0.006386584432964975, 0.0068810367956757545, 0.0067865171136376875, 0.0070004642263732176, 0.007126789817861704, 0.006529675841606645, 0.007548068295529281, 0.006727648091989316, 0.006906773455821994, 0.006647741501431235, 0.006914746545424516, 0.0069812967717139554, 0.006980767452412031, 0.006625571931098503, 0.0066646210007919844, 0.006697629022792998, 0.006567127068675208, 0.006939203295835548, 0.0067057812940964304, 0.006489475091009147, 0.006800586704312908, 0.007075406251136552, 0.00666551354648122, 0.006685015750752593, 0.006885382659013637, 0.008684211180397224, 0.006400203271451491, 0.0066820092271717094, 0.006734567498576574, 0.00682789902202785, 0.0065543004787865684, 0.008588087430160323, 0.006935193340971388, 0.007007546953073787, 0.006306537180948494, 0.006802805839635601, 0.0069379120673560965, 0.0070852762040555135, 0.009007494706152515, 0.0066349343872967766, 0.006704706751572137, 0.00705953006928956, 0.007011776615399867, 0.008165677067485045, 0.006489724569043822, 0.008680691410766238, 0.006722897023957392, 0.007269179112468423, 0.006682142022658478, 0.0068672290442257445, 0.006985539840322665, 0.00866371452205137, 0.007509941727303984, 0.006725209341807799, 0.00692727841140533, 0.00912302534297024, 0.0068334622059906406, 0.007953415794128721, 0.008852362976705825, 0.006892788385316221, 0.006925875408342108, 0.006681579384762285, 0.006645416772120039, 0.006766980566846376, 0.008962602658853442, 0.006719886134801941, 0.006692486976548521, 0.006665296976941384, 0.006884749909989874, 0.006432267477397214, 0.006338335409633477, 0.006648319522024725, 0.0068858940900430425, 0.006535914181519977, 0.006725094863213599, 0.006902541637166657, 0.006511243975678967, 0.006815952955300666, 0.0069446070013906465]
[146.25788947303408, 115.0723946062352, 156.2739122015995, 144.62058993618334, 143.63106528427102, 154.66907935025742, 146.78660896301682, 138.75585934788253, 110.37725952185663, 148.82679932185889, 148.74343687973604, 155.1397852917899, 142.43967341938543, 147.15407726481772, 117.40770658361832, 151.9757195829177, 145.9762666057668, 145.56698762934928, 156.57821649368682, 145.32693686922724, 147.3509877386846, 142.84766947778226, 140.31562955508014, 153.14695924536846, 132.48422786427352, 148.64035489470845, 144.78540615184943, 150.42702845540904, 144.618460478743, 143.239863982246, 143.25072519848442, 150.93036652523406, 150.04604161004292, 149.30656753260828, 152.27358775649986, 144.10876254340812, 149.12505435874715, 154.0956681358484, 147.04613638199825, 141.33464065605642, 150.0259496926397, 149.58827881406577, 145.23521052107432, 115.15150647848006, 156.24503747569437, 149.65558501978745, 148.4876349091996, 146.45793629546168, 152.57158307535133, 116.44036092228417, 144.19208677171983, 142.70328928175934, 158.56562346463505, 146.99816863413022, 144.13558290903515, 141.13775824682938, 111.01866086215472, 150.71739095334488, 149.14895416798316, 141.65248822300654, 142.61720742838753, 122.46381919533722, 154.08974438915783, 115.19819708828135, 148.74539896066355, 137.567115148498, 149.65261088571592, 145.61914180521504, 143.1528590285457, 115.42393247777775, 133.15682548697922, 148.6942560707249, 144.35683692943, 109.61276138189744, 146.33870355254726, 125.73214149550793, 112.96418850327395, 145.07916739911968, 144.38607988753532, 149.6652127310731, 150.47965150889658, 147.77639600434543, 111.57473315099821, 148.81204531443652, 149.42128441999964, 150.03082435178857, 145.2485584914254, 155.4661716904604, 157.7701297536456, 150.41394997445207, 145.22442357136939, 153.0007849288257, 148.69678723344404, 144.87417136544477, 153.58048381157826, 146.7146276621987, 143.9966292980656]
Elapsed: 0.3098522819513839~0.030489543878655864
Time per graph: 0.007042097317076906~0.0006929441790603605
Speed: 143.18228187902312~12.028518406678813
Total Time: 0.3060
best val loss: 0.3242783546447754 test_score: 0.8636

Testing...
Test loss: 0.4270 score: 0.8864 time: 0.28s
test Score 0.8864
Epoch Time List: [0.9325647850055248, 1.02826225804165, 0.9196861139498651, 0.9314751740312204, 0.9515257851453498, 0.9060573930619285, 1.0266906770411879, 0.9305605189874768, 1.0373830730095506, 0.9067316161235794, 0.926039990154095, 1.025372585048899, 0.9212236140156165, 0.9370648699114099, 0.9933580511715263, 0.9888336119474843, 0.9226645348826423, 0.9630238921381533, 0.8757127522258088, 0.9851632419740781, 1.002686772029847, 0.9287080579670146, 1.07062229514122, 0.8849836111767218, 1.009458996122703, 0.991390666924417, 0.9575355302076787, 0.8906843890435994, 0.989829248865135, 0.9328214880079031, 1.0097140250727534, 0.8893363010138273, 0.9348840320017189, 0.9077081199502572, 0.920971833053045, 0.8925532691646367, 0.9147983019938692, 1.0107704949332401, 0.9301677360199392, 0.9184605869231746, 0.9082575778011233, 0.9114818859379739, 0.9133814490633085, 1.0705526669044048, 0.8824890641262755, 0.9308982848888263, 0.8923204911407083, 0.9995000790804625, 0.8803525590337813, 1.0743921048706397, 0.9201343930326402, 1.038186733960174, 0.8592302310280502, 0.9669520899187773, 0.9378374150255695, 1.079749378026463, 1.0372315250569955, 0.8875996749848127, 1.0160543880192563, 0.9457559660077095, 1.0626029400154948, 1.0315705239772797, 0.8772147349081933, 1.0309126090724021, 0.9130487130023539, 1.0392172060674056, 0.9302638811059296, 0.9363078250316903, 0.9439260609215125, 1.0754015928832814, 0.9885327169904485, 0.9114736530464143, 0.978687408962287, 1.1030115720350295, 1.0838215029798448, 1.012900240952149, 1.0748679880052805, 0.919471194036305, 0.9506512050284073, 0.9188900798326358, 0.9759108148282394, 0.9296601270325482, 1.1232495300937444, 0.9711103270528838, 0.9205958190141246, 0.9196172878146172, 1.027683955966495, 0.8773189899511635, 0.9962631680537015, 0.9278033240698278, 0.9694702250417322, 0.870298410882242, 0.9199930350296199, 1.1599836990935728, 0.9403317060787231, 0.9296959170605987, 0.9240487399511039]
Total Epoch List: [97]
Total Time List: [0.3060347510036081]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fce470>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.27s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.26s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.26s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.28s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.32s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.26s
Epoch 9/1000, LR 0.000210
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.27s
Epoch 10/1000, LR 0.000240
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.26s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.26s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.27s
Epoch 17/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.51s
Val loss: 0.6913 score: 0.5455 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.24s
Epoch 19/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.41s
Val loss: 0.6910 score: 0.5682 time: 0.27s
Test loss: 0.6922 score: 0.5116 time: 0.26s
Epoch 20/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.48s
Val loss: 0.6907 score: 0.6364 time: 0.32s
Test loss: 0.6920 score: 0.6047 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.42s
Val loss: 0.6904 score: 0.6591 time: 0.25s
Test loss: 0.6918 score: 0.5814 time: 0.39s
Epoch 22/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.40s
Val loss: 0.6900 score: 0.6591 time: 0.25s
Test loss: 0.6916 score: 0.5116 time: 0.26s
Epoch 23/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.40s
Val loss: 0.6896 score: 0.6818 time: 0.38s
Test loss: 0.6913 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.42s
Val loss: 0.6891 score: 0.6591 time: 0.28s
Test loss: 0.6911 score: 0.5116 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.55s
Val loss: 0.6886 score: 0.6591 time: 0.27s
Test loss: 0.6907 score: 0.4884 time: 0.26s
Epoch 26/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.41s
Val loss: 0.6880 score: 0.7273 time: 0.26s
Test loss: 0.6904 score: 0.5814 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.43s
Val loss: 0.6874 score: 0.7045 time: 0.24s
Test loss: 0.6900 score: 0.6047 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.38s
Val loss: 0.6867 score: 0.7045 time: 0.26s
Test loss: 0.6896 score: 0.6047 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.48s
Val loss: 0.6860 score: 0.7273 time: 0.26s
Test loss: 0.6891 score: 0.6047 time: 0.35s
Epoch 30/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.38s
Val loss: 0.6852 score: 0.7045 time: 0.26s
Test loss: 0.6886 score: 0.6279 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.41s
Val loss: 0.6843 score: 0.7500 time: 0.27s
Test loss: 0.6881 score: 0.6512 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.42s
Val loss: 0.6834 score: 0.7500 time: 0.24s
Test loss: 0.6875 score: 0.6512 time: 0.26s
Epoch 33/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.41s
Val loss: 0.6823 score: 0.7045 time: 0.25s
Test loss: 0.6868 score: 0.6977 time: 0.35s
Epoch 34/1000, LR 0.000270
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.37s
Val loss: 0.6812 score: 0.7045 time: 0.27s
Test loss: 0.6860 score: 0.6744 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.49s
Val loss: 0.6799 score: 0.6591 time: 0.27s
Test loss: 0.6852 score: 0.6977 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.43s
Val loss: 0.6785 score: 0.6591 time: 0.28s
Test loss: 0.6843 score: 0.6744 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.47s
Val loss: 0.6770 score: 0.6364 time: 0.24s
Test loss: 0.6833 score: 0.6744 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.37s
Val loss: 0.6754 score: 0.6364 time: 0.27s
Test loss: 0.6822 score: 0.6744 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.41s
Val loss: 0.6737 score: 0.6364 time: 0.30s
Test loss: 0.6811 score: 0.6512 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 0.46s
Val loss: 0.6718 score: 0.6364 time: 0.35s
Test loss: 0.6798 score: 0.6512 time: 0.33s
Epoch 41/1000, LR 0.000269
Train loss: 0.6605;  Loss pred: 0.6605; Loss self: 0.0000; time: 0.49s
Val loss: 0.6697 score: 0.6364 time: 0.29s
Test loss: 0.6784 score: 0.6279 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 0.41s
Val loss: 0.6675 score: 0.6364 time: 0.25s
Test loss: 0.6769 score: 0.6512 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.42s
Val loss: 0.6651 score: 0.6364 time: 0.26s
Test loss: 0.6753 score: 0.6512 time: 0.26s
Epoch 44/1000, LR 0.000269
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 0.39s
Val loss: 0.6625 score: 0.6591 time: 0.27s
Test loss: 0.6735 score: 0.6512 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6435;  Loss pred: 0.6435; Loss self: 0.0000; time: 0.41s
Val loss: 0.6597 score: 0.6591 time: 0.25s
Test loss: 0.6716 score: 0.6512 time: 0.41s
Epoch 46/1000, LR 0.000269
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.39s
Val loss: 0.6566 score: 0.6591 time: 0.25s
Test loss: 0.6695 score: 0.6512 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.43s
Val loss: 0.6533 score: 0.6591 time: 0.25s
Test loss: 0.6673 score: 0.6977 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.38s
Val loss: 0.6498 score: 0.6591 time: 0.27s
Test loss: 0.6648 score: 0.6744 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6228;  Loss pred: 0.6228; Loss self: 0.0000; time: 0.41s
Val loss: 0.6459 score: 0.6818 time: 0.28s
Test loss: 0.6621 score: 0.6744 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.42s
Val loss: 0.6418 score: 0.7273 time: 0.24s
Test loss: 0.6592 score: 0.6977 time: 0.27s
Epoch 51/1000, LR 0.000269
Train loss: 0.6083;  Loss pred: 0.6083; Loss self: 0.0000; time: 0.51s
Val loss: 0.6373 score: 0.7500 time: 0.25s
Test loss: 0.6560 score: 0.6977 time: 0.27s
Epoch 52/1000, LR 0.000269
Train loss: 0.6013;  Loss pred: 0.6013; Loss self: 0.0000; time: 0.37s
Val loss: 0.6326 score: 0.7955 time: 0.26s
Test loss: 0.6526 score: 0.7442 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.44s
Val loss: 0.6276 score: 0.7955 time: 0.28s
Test loss: 0.6490 score: 0.7907 time: 0.27s
Epoch 54/1000, LR 0.000269
Train loss: 0.5832;  Loss pred: 0.5832; Loss self: 0.0000; time: 0.42s
Val loss: 0.6223 score: 0.7955 time: 0.26s
Test loss: 0.6452 score: 0.6977 time: 0.26s
Epoch 55/1000, LR 0.000269
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.43s
Val loss: 0.6168 score: 0.8182 time: 0.24s
Test loss: 0.6412 score: 0.6977 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.5648;  Loss pred: 0.5648; Loss self: 0.0000; time: 0.39s
Val loss: 0.6109 score: 0.8182 time: 0.34s
Test loss: 0.6369 score: 0.6744 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5570;  Loss pred: 0.5570; Loss self: 0.0000; time: 0.40s
Val loss: 0.6048 score: 0.8182 time: 0.27s
Test loss: 0.6325 score: 0.6744 time: 0.26s
Epoch 58/1000, LR 0.000269
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.42s
Val loss: 0.5983 score: 0.8182 time: 0.24s
Test loss: 0.6278 score: 0.7209 time: 0.26s
Epoch 59/1000, LR 0.000268
Train loss: 0.5316;  Loss pred: 0.5316; Loss self: 0.0000; time: 0.41s
Val loss: 0.5915 score: 0.8182 time: 0.27s
Test loss: 0.6229 score: 0.7209 time: 0.26s
Epoch 60/1000, LR 0.000268
Train loss: 0.5192;  Loss pred: 0.5192; Loss self: 0.0000; time: 0.39s
Val loss: 0.5845 score: 0.8182 time: 0.27s
Test loss: 0.6177 score: 0.7209 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.5090;  Loss pred: 0.5090; Loss self: 0.0000; time: 0.42s
Val loss: 0.5772 score: 0.8182 time: 0.35s
Test loss: 0.6123 score: 0.7209 time: 0.26s
Epoch 62/1000, LR 0.000268
Train loss: 0.4987;  Loss pred: 0.4987; Loss self: 0.0000; time: 0.40s
Val loss: 0.5697 score: 0.8182 time: 0.25s
Test loss: 0.6067 score: 0.7209 time: 0.26s
Epoch 63/1000, LR 0.000268
Train loss: 0.4838;  Loss pred: 0.4838; Loss self: 0.0000; time: 0.42s
Val loss: 0.5620 score: 0.8182 time: 0.25s
Test loss: 0.6009 score: 0.7209 time: 0.27s
Epoch 64/1000, LR 0.000268
Train loss: 0.4709;  Loss pred: 0.4709; Loss self: 0.0000; time: 0.40s
Val loss: 0.5542 score: 0.8182 time: 0.27s
Test loss: 0.5950 score: 0.7209 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.4650;  Loss pred: 0.4650; Loss self: 0.0000; time: 0.41s
Val loss: 0.5462 score: 0.7955 time: 0.28s
Test loss: 0.5888 score: 0.7442 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.4436;  Loss pred: 0.4436; Loss self: 0.0000; time: 0.43s
Val loss: 0.5381 score: 0.7955 time: 0.34s
Test loss: 0.5824 score: 0.7442 time: 0.26s
Epoch 67/1000, LR 0.000268
Train loss: 0.4321;  Loss pred: 0.4321; Loss self: 0.0000; time: 0.41s
Val loss: 0.5300 score: 0.7955 time: 0.26s
Test loss: 0.5757 score: 0.7442 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.4142;  Loss pred: 0.4142; Loss self: 0.0000; time: 0.39s
Val loss: 0.5218 score: 0.7955 time: 0.27s
Test loss: 0.5689 score: 0.7674 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 0.41s
Val loss: 0.5136 score: 0.7955 time: 0.26s
Test loss: 0.5619 score: 0.8140 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.3924;  Loss pred: 0.3924; Loss self: 0.0000; time: 0.45s
Val loss: 0.5055 score: 0.7955 time: 0.24s
Test loss: 0.5550 score: 0.8140 time: 0.26s
Epoch 71/1000, LR 0.000268
Train loss: 0.3706;  Loss pred: 0.3706; Loss self: 0.0000; time: 0.37s
Val loss: 0.4976 score: 0.7955 time: 0.35s
Test loss: 0.5482 score: 0.8140 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.3577;  Loss pred: 0.3577; Loss self: 0.0000; time: 0.40s
Val loss: 0.4899 score: 0.7955 time: 0.27s
Test loss: 0.5412 score: 0.8140 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.3535;  Loss pred: 0.3535; Loss self: 0.0000; time: 0.43s
Val loss: 0.4824 score: 0.7955 time: 0.26s
Test loss: 0.5345 score: 0.8140 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.3325;  Loss pred: 0.3325; Loss self: 0.0000; time: 0.44s
Val loss: 0.4753 score: 0.7955 time: 0.24s
Test loss: 0.5278 score: 0.8140 time: 0.26s
Epoch 75/1000, LR 0.000267
Train loss: 0.3228;  Loss pred: 0.3228; Loss self: 0.0000; time: 0.37s
Val loss: 0.4686 score: 0.7955 time: 0.26s
Test loss: 0.5213 score: 0.8140 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.3024;  Loss pred: 0.3024; Loss self: 0.0000; time: 0.41s
Val loss: 0.4624 score: 0.7955 time: 0.36s
Test loss: 0.5150 score: 0.8140 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.43s
Val loss: 0.4565 score: 0.7955 time: 0.26s
Test loss: 0.5088 score: 0.8140 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.2837;  Loss pred: 0.2837; Loss self: 0.0000; time: 0.42s
Val loss: 0.4510 score: 0.7955 time: 0.26s
Test loss: 0.5030 score: 0.8140 time: 0.27s
Epoch 79/1000, LR 0.000267
Train loss: 0.2577;  Loss pred: 0.2577; Loss self: 0.0000; time: 0.37s
Val loss: 0.4460 score: 0.7955 time: 0.27s
Test loss: 0.4974 score: 0.8140 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.2469;  Loss pred: 0.2469; Loss self: 0.0000; time: 0.41s
Val loss: 0.4414 score: 0.7727 time: 0.27s
Test loss: 0.4923 score: 0.8140 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.2438;  Loss pred: 0.2438; Loss self: 0.0000; time: 0.42s
Val loss: 0.4372 score: 0.7727 time: 0.31s
Test loss: 0.4876 score: 0.8140 time: 0.26s
Epoch 82/1000, LR 0.000267
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.42s
Val loss: 0.4336 score: 0.7727 time: 0.25s
Test loss: 0.4834 score: 0.8140 time: 0.34s
Epoch 83/1000, LR 0.000266
Train loss: 0.2137;  Loss pred: 0.2137; Loss self: 0.0000; time: 0.36s
Val loss: 0.4305 score: 0.7727 time: 0.26s
Test loss: 0.4798 score: 0.8140 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.2037;  Loss pred: 0.2037; Loss self: 0.0000; time: 0.43s
Val loss: 0.4279 score: 0.7727 time: 0.26s
Test loss: 0.4767 score: 0.8140 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.1926;  Loss pred: 0.1926; Loss self: 0.0000; time: 0.42s
Val loss: 0.4261 score: 0.7727 time: 0.24s
Test loss: 0.4742 score: 0.8140 time: 0.28s
Epoch 86/1000, LR 0.000266
Train loss: 0.1763;  Loss pred: 0.1763; Loss self: 0.0000; time: 0.37s
Val loss: 0.4251 score: 0.7727 time: 0.36s
Test loss: 0.4723 score: 0.8140 time: 0.27s
Epoch 87/1000, LR 0.000266
Train loss: 0.1731;  Loss pred: 0.1731; Loss self: 0.0000; time: 0.36s
Val loss: 0.4247 score: 0.7955 time: 0.29s
Test loss: 0.4711 score: 0.8140 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.1592;  Loss pred: 0.1592; Loss self: 0.0000; time: 0.41s
Val loss: 0.4252 score: 0.7955 time: 0.27s
Test loss: 0.4709 score: 0.8140 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 89/1000, LR 0.000266
Train loss: 0.1541;  Loss pred: 0.1541; Loss self: 0.0000; time: 0.46s
Val loss: 0.4265 score: 0.7955 time: 0.24s
Test loss: 0.4718 score: 0.8140 time: 0.27s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.1731,   Val_Loss: 0.4247,   Val_Precision: 0.8824,   Val_Recall: 0.6818,   Val_accuracy: 0.7692,   Val_Score: 0.7955,   Val_Loss: 0.4247,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4711


[0.3008384720887989, 0.3823679880006239, 0.28155691106803715, 0.30424436810426414, 0.30634041398297995, 0.28447832097299397, 0.2997548639541492, 0.317103725974448, 0.39863283606246114, 0.29564567806664854, 0.29581137106288224, 0.28361519204918295, 0.3089027020614594, 0.29900632600765675, 0.37476245197467506, 0.2895199320046231, 0.3014188609085977, 0.30226633604615927, 0.2810097150504589, 0.3027656190097332, 0.29860675300005823, 0.30802042596042156, 0.313578751985915, 0.2873057370306924, 0.3321150050032884, 0.2960165160475299, 0.3038980320561677, 0.29250062606297433, 0.3042488479986787, 0.30717705795541406, 0.30715376790612936, 0.29152516496833414, 0.2932433240348473, 0.2946956770028919, 0.28895359102170914, 0.3053249450167641, 0.29505437694024295, 0.28553690400440246, 0.29922581498976797, 0.3113178750500083, 0.2932825960451737, 0.2941406930331141, 0.30295683699660003, 0.3821052919374779, 0.2816089439438656, 0.2940084059955552, 0.2963209699373692, 0.3004275569692254, 0.288389221066609, 0.3778758469270542, 0.3051485070027411, 0.30833206593524665, 0.27748763596173376, 0.29932345694396645, 0.3052681309636682, 0.3117521529784426, 0.39632976707071066, 0.2919371130410582, 0.29500709706917405, 0.3106193230487406, 0.30851817107759416, 0.359289790969342, 0.28554788103792816, 0.38195042207371444, 0.29580746905412525, 0.3198438809486106, 0.29401424899697304, 0.30215807794593275, 0.30736375297419727, 0.3812034389702603, 0.3304374360013753, 0.29590921103954315, 0.30480025010183454, 0.40141311509069055, 0.3006723370635882, 0.34995029494166374, 0.3895039709750563, 0.30328268895391375, 0.30473851796705276, 0.2939894929295406, 0.29239833797328174, 0.29774714494124055, 0.3943545169895515, 0.2956749899312854, 0.29446942696813494, 0.2932730669854209, 0.3029289960395545, 0.28301976900547743, 0.27888675802387297, 0.2925260589690879, 0.30297933996189386, 0.287580223986879, 0.29590417398139834, 0.3037118320353329, 0.28649473492987454, 0.2999019300332293, 0.30556270806118846, 0.27530844998545945, 0.267596930032596, 0.2618183009326458, 0.2603730949340388, 0.2860270630335435, 0.32606051210314035, 0.25197975197806954, 0.26794589194469154, 0.2742889750516042, 0.24430776305962354, 0.2664315269794315, 0.26261132408399135, 0.26682702207472175, 0.24439638305921108, 0.25476013496518135, 0.2745634240563959, 0.24579346796963364, 0.2483335790457204, 0.2687746649608016, 0.25907525210641325, 0.39617527392692864, 0.263869907008484, 0.2481356409844011, 0.2575236480915919, 0.2597709440160543, 0.25880056992173195, 0.2538482249947265, 0.2585324289975688, 0.3575592909473926, 0.25973768904805183, 0.26209833996836096, 0.26575631310697645, 0.35520486801397055, 0.24547244200948626, 0.26023679797071964, 0.25540810299571604, 0.2664386979304254, 0.24599357799161226, 0.2583898620214313, 0.3364179120399058, 0.2579181030159816, 0.26427661906927824, 0.2642121189273894, 0.24298696499317884, 0.4185684889089316, 0.2591483350843191, 0.27001950691919774, 0.24560141598340124, 0.25413350702729076, 0.27159911196213216, 0.27398780488874763, 0.24604794604238123, 0.2744811719749123, 0.26449318195227534, 0.2693385439924896, 0.2552678299834952, 0.2600854359334335, 0.26272112503647804, 0.2686714919982478, 0.24323290295433253, 0.260737195960246, 0.26004115503747016, 0.27000570006202906, 0.2391328269150108, 0.2521608219249174, 0.2678733179345727, 0.26325490104500204, 0.24137414002325386, 0.257773493998684, 0.2649128190241754, 0.2625583050539717, 0.24643797799944878, 0.2542866379953921, 0.2656630009878427, 0.24633921799249947, 0.241594337974675, 0.25250280799809843, 0.2752300320426002, 0.24592958798166364, 0.26035551296081394, 0.26048136898316443, 0.34630114398896694, 0.24673405301291496, 0.25380241009406745, 0.28230347600765526, 0.2696878910064697, 0.24757033598143607, 0.2560069609899074, 0.27002591907512397]
[0.006837238002018156, 0.008690181545468724, 0.006399020706091754, 0.006914644729642367, 0.006962282135976817, 0.006465416385749863, 0.0068126105444124814, 0.007206902863055637, 0.009059837183237753, 0.006719219956060194, 0.006722985705974596, 0.006445799819299613, 0.00702051595594226, 0.0067955983183558355, 0.008517328453969887, 0.006579998454650526, 0.006850428657013585, 0.006869689455594529, 0.006386584432964975, 0.0068810367956757545, 0.0067865171136376875, 0.0070004642263732176, 0.007126789817861704, 0.006529675841606645, 0.007548068295529281, 0.006727648091989316, 0.006906773455821994, 0.006647741501431235, 0.006914746545424516, 0.0069812967717139554, 0.006980767452412031, 0.006625571931098503, 0.0066646210007919844, 0.006697629022792998, 0.006567127068675208, 0.006939203295835548, 0.0067057812940964304, 0.006489475091009147, 0.006800586704312908, 0.007075406251136552, 0.00666551354648122, 0.006685015750752593, 0.006885382659013637, 0.008684211180397224, 0.006400203271451491, 0.0066820092271717094, 0.006734567498576574, 0.00682789902202785, 0.0065543004787865684, 0.008588087430160323, 0.006935193340971388, 0.007007546953073787, 0.006306537180948494, 0.006802805839635601, 0.0069379120673560965, 0.0070852762040555135, 0.009007494706152515, 0.0066349343872967766, 0.006704706751572137, 0.00705953006928956, 0.007011776615399867, 0.008165677067485045, 0.006489724569043822, 0.008680691410766238, 0.006722897023957392, 0.007269179112468423, 0.006682142022658478, 0.0068672290442257445, 0.006985539840322665, 0.00866371452205137, 0.007509941727303984, 0.006725209341807799, 0.00692727841140533, 0.00912302534297024, 0.0068334622059906406, 0.007953415794128721, 0.008852362976705825, 0.006892788385316221, 0.006925875408342108, 0.006681579384762285, 0.006645416772120039, 0.006766980566846376, 0.008962602658853442, 0.006719886134801941, 0.006692486976548521, 0.006665296976941384, 0.006884749909989874, 0.006432267477397214, 0.006338335409633477, 0.006648319522024725, 0.0068858940900430425, 0.006535914181519977, 0.006725094863213599, 0.006902541637166657, 0.006511243975678967, 0.006815952955300666, 0.0069446070013906465, 0.006402522092685103, 0.006223184419362698, 0.006088797696108042, 0.0060551882542799725, 0.0066517921635707794, 0.0075828026070497755, 0.005859994232048129, 0.006231299812667245, 0.006378813373293121, 0.005681575885107524, 0.006196082022777477, 0.006107240094976543, 0.006205279583133064, 0.00568363681533049, 0.005924654301515845, 0.006385195908288277, 0.0057161271620845035, 0.0057751995126911725, 0.006250573603739572, 0.006025005862939843, 0.009213378463416945, 0.006136509465313581, 0.005770596301962816, 0.005988922048641672, 0.006041184744559402, 0.006018617905156557, 0.005903447092900616, 0.006012382069710902, 0.008315332347613781, 0.0060404113732105075, 0.006095310231822348, 0.006180379374580848, 0.008260578325906293, 0.005708661442081076, 0.006052018557458596, 0.005939723325481768, 0.006196248789079661, 0.005720780883525866, 0.0060090665586379375, 0.007823672373021064, 0.0059980954189763165, 0.006145967885332052, 0.006144467882032312, 0.005650859651004159, 0.009734150904858874, 0.006026705467077189, 0.006279523416725529, 0.005711660836823285, 0.005910081558774204, 0.006316258417724004, 0.006371809416017387, 0.005722045256799564, 0.006383283069184007, 0.006151004231448264, 0.006263687069592781, 0.0059364611624068645, 0.006048498510079848, 0.00610979360549949, 0.00624817423251739, 0.00565657913847285, 0.006063655720005722, 0.006047468721801632, 0.006279202327023932, 0.005561228532907228, 0.005864205161044591, 0.006229612044990063, 0.006122207001046559, 0.005613352093564043, 0.005994732418574046, 0.006160763233120359, 0.006106007094278412, 0.0057311157674290415, 0.0059136427440788855, 0.006178209325298667, 0.005728819023081383, 0.005618472976155232, 0.0058721583255371725, 0.006400698419595354, 0.00571929274375962, 0.00605477937118172, 0.006057706255422428, 0.008053514976487604, 0.005738001232858487, 0.005902381630094592, 0.006565197116457099, 0.00627181141875511, 0.005757449673986886, 0.005953650255579241, 0.00627967253663079]
[146.25788947303408, 115.0723946062352, 156.2739122015995, 144.62058993618334, 143.63106528427102, 154.66907935025742, 146.78660896301682, 138.75585934788253, 110.37725952185663, 148.82679932185889, 148.74343687973604, 155.1397852917899, 142.43967341938543, 147.15407726481772, 117.40770658361832, 151.9757195829177, 145.9762666057668, 145.56698762934928, 156.57821649368682, 145.32693686922724, 147.3509877386846, 142.84766947778226, 140.31562955508014, 153.14695924536846, 132.48422786427352, 148.64035489470845, 144.78540615184943, 150.42702845540904, 144.618460478743, 143.239863982246, 143.25072519848442, 150.93036652523406, 150.04604161004292, 149.30656753260828, 152.27358775649986, 144.10876254340812, 149.12505435874715, 154.0956681358484, 147.04613638199825, 141.33464065605642, 150.0259496926397, 149.58827881406577, 145.23521052107432, 115.15150647848006, 156.24503747569437, 149.65558501978745, 148.4876349091996, 146.45793629546168, 152.57158307535133, 116.44036092228417, 144.19208677171983, 142.70328928175934, 158.56562346463505, 146.99816863413022, 144.13558290903515, 141.13775824682938, 111.01866086215472, 150.71739095334488, 149.14895416798316, 141.65248822300654, 142.61720742838753, 122.46381919533722, 154.08974438915783, 115.19819708828135, 148.74539896066355, 137.567115148498, 149.65261088571592, 145.61914180521504, 143.1528590285457, 115.42393247777775, 133.15682548697922, 148.6942560707249, 144.35683692943, 109.61276138189744, 146.33870355254726, 125.73214149550793, 112.96418850327395, 145.07916739911968, 144.38607988753532, 149.6652127310731, 150.47965150889658, 147.77639600434543, 111.57473315099821, 148.81204531443652, 149.42128441999964, 150.03082435178857, 145.2485584914254, 155.4661716904604, 157.7701297536456, 150.41394997445207, 145.22442357136939, 153.0007849288257, 148.69678723344404, 144.87417136544477, 153.58048381157826, 146.7146276621987, 143.9966292980656, 156.18844972710087, 160.68943688839093, 164.23603639174937, 165.1476317508663, 150.3354247110429, 131.8773614217907, 170.64863213192783, 160.48016145317843, 156.76896963106176, 176.0075057029842, 161.3923115161953, 163.74008299142216, 161.15309336232954, 175.9436840339089, 168.78621926415963, 156.6122659920198, 174.94362382857304, 173.15419108941092, 159.98531709181432, 165.97494222388355, 108.53781856141534, 162.95909028617444, 173.29231636942944, 166.9749567414735, 165.5304451499492, 166.15110242224088, 169.39255730818402, 166.32342861871447, 120.25977533982345, 165.55163849188224, 164.06055835832734, 161.80236509636916, 121.05689947444293, 175.17241303338756, 165.23412651595152, 168.35801016352735, 161.3879677914824, 174.80131128246848, 166.4151978084709, 127.81721323714581, 166.71958849408702, 162.70830220030874, 162.7480229694431, 176.96422522585564, 102.73109691579185, 165.92813527437514, 159.2477539515972, 175.08042381525243, 169.20240271733368, 158.32157803960456, 156.94129166610205, 174.7626862635681, 156.65919702474244, 162.57507918581751, 159.65037666944187, 168.45052509272418, 165.330287894342, 163.67164990645338, 160.04675330526112, 176.785292934128, 164.91701478049225, 165.35844102755195, 159.25589715373232, 179.81638303168828, 170.5260939100347, 160.52363979940185, 163.3397890383411, 178.1466730274312, 166.81311694607177, 162.31755095277555, 163.77314742019257, 174.4860932112346, 169.10050932671973, 161.8591969529388, 174.55604653786497, 177.98430360775862, 170.29513588745448, 156.23295060091567, 174.84679396610892, 165.15878427537626, 165.07898498790215, 124.16938478658322, 174.27671403650643, 169.42313504455217, 152.31835118754955, 159.44356952596155, 173.68801407299594, 167.96418282429124, 159.2439723833954]
Elapsed: 0.28954937822756266~0.03719335206503839
Time per graph: 0.006648299688743461~0.0008138903455616968
Speed: 152.3831420181181~16.286944600003995
Total Time: 0.2706
best val loss: 0.42474299669265747 test_score: 0.8140

Testing...
Test loss: 0.6412 score: 0.6977 time: 0.24s
test Score 0.6977
Epoch Time List: [0.9325647850055248, 1.02826225804165, 0.9196861139498651, 0.9314751740312204, 0.9515257851453498, 0.9060573930619285, 1.0266906770411879, 0.9305605189874768, 1.0373830730095506, 0.9067316161235794, 0.926039990154095, 1.025372585048899, 0.9212236140156165, 0.9370648699114099, 0.9933580511715263, 0.9888336119474843, 0.9226645348826423, 0.9630238921381533, 0.8757127522258088, 0.9851632419740781, 1.002686772029847, 0.9287080579670146, 1.07062229514122, 0.8849836111767218, 1.009458996122703, 0.991390666924417, 0.9575355302076787, 0.8906843890435994, 0.989829248865135, 0.9328214880079031, 1.0097140250727534, 0.8893363010138273, 0.9348840320017189, 0.9077081199502572, 0.920971833053045, 0.8925532691646367, 0.9147983019938692, 1.0107704949332401, 0.9301677360199392, 0.9184605869231746, 0.9082575778011233, 0.9114818859379739, 0.9133814490633085, 1.0705526669044048, 0.8824890641262755, 0.9308982848888263, 0.8923204911407083, 0.9995000790804625, 0.8803525590337813, 1.0743921048706397, 0.9201343930326402, 1.038186733960174, 0.8592302310280502, 0.9669520899187773, 0.9378374150255695, 1.079749378026463, 1.0372315250569955, 0.8875996749848127, 1.0160543880192563, 0.9457559660077095, 1.0626029400154948, 1.0315705239772797, 0.8772147349081933, 1.0309126090724021, 0.9130487130023539, 1.0392172060674056, 0.9302638811059296, 0.9363078250316903, 0.9439260609215125, 1.0754015928832814, 0.9885327169904485, 0.9114736530464143, 0.978687408962287, 1.1030115720350295, 1.0838215029798448, 1.012900240952149, 1.0748679880052805, 0.919471194036305, 0.9506512050284073, 0.9188900798326358, 0.9759108148282394, 0.9296601270325482, 1.1232495300937444, 0.9711103270528838, 0.9205958190141246, 0.9196172878146172, 1.027683955966495, 0.8773189899511635, 0.9962631680537015, 0.9278033240698278, 0.9694702250417322, 0.870298410882242, 0.9199930350296199, 1.1599836990935728, 0.9403317060787231, 0.9296959170605987, 0.9240487399511039, 1.0749881839146838, 0.9157702560769394, 0.9257234390825033, 0.9369361890712753, 0.9465842420468107, 0.9618215300142765, 0.9265642800601199, 0.9187861719401553, 0.9705855370266363, 0.8879208141006529, 0.9412116081221029, 1.07617719785776, 0.9386053641792387, 0.9160215689335018, 0.9222140460042283, 0.9593352690571919, 0.8720107978442684, 1.030374413006939, 0.9478116859681904, 1.0596928290324286, 1.0637568728998303, 0.9143080001231283, 1.019871161901392, 0.9600928550353274, 1.071581907919608, 0.9238150808960199, 0.9128794240532443, 0.890344938961789, 1.0952084149466828, 0.895997968968004, 0.9378464069450274, 0.9274057899601758, 1.0120289791375399, 0.87762713606935, 1.0227652138564736, 0.9631845079129562, 0.9739103970350698, 0.8822119371034205, 0.9664030810818076, 1.1450551762245595, 1.027005627984181, 0.9136834340170026, 0.9373034070013091, 0.905511713004671, 1.082392704905942, 0.8990892110159621, 0.9475315739400685, 0.891457170015201, 0.94129718397744, 0.9271345708984882, 1.0317898190114647, 0.8721982089336962, 0.9860168199520558, 0.9406517121242359, 0.9417162740137428, 0.9736892351647839, 0.9295245709363371, 0.922848965972662, 0.9500798321096227, 0.9025943240849301, 1.0237721930025145, 0.9095130999339744, 0.9340823710663244, 0.9059591939439997, 0.9390786540461704, 1.0283276481786743, 0.9276418079389259, 0.9004697980126366, 0.9320883211912587, 0.9600274590775371, 0.9756710979854688, 0.9104571641655639, 0.9421529739629477, 0.9476328949676827, 0.8715875080088153, 0.9999120089923963, 0.9333694879896939, 0.9502982039703056, 0.874875235138461, 0.930463369935751, 0.9848591670161113, 1.0188550850143656, 0.8704537468729541, 0.9475270219845697, 0.9408008870668709, 0.9954641199437901, 0.8938691469375044, 0.9277224390534684, 0.9694159800419584]
Total Epoch List: [97, 89]
Total Time List: [0.3060347510036081, 0.27061543404124677]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcfee0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.37s
Epoch 2/1000, LR 0.000000
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.32s
Epoch 8/1000, LR 0.000180
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.27s
Epoch 10/1000, LR 0.000240
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.34s
Epoch 13/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5116 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5116 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5116 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.40s
Val loss: 0.6909 score: 0.5227 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5116 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.40s
Val loss: 0.6906 score: 0.5682 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.43s
Val loss: 0.6903 score: 0.5682 time: 0.27s
Test loss: 0.6899 score: 0.6744 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.36s
Val loss: 0.6900 score: 0.6364 time: 0.28s
Test loss: 0.6896 score: 0.7442 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.40s
Val loss: 0.6896 score: 0.6364 time: 0.29s
Test loss: 0.6893 score: 0.6977 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.41s
Val loss: 0.6893 score: 0.7727 time: 0.27s
Test loss: 0.6890 score: 0.5814 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.35s
Val loss: 0.6889 score: 0.6364 time: 0.36s
Test loss: 0.6887 score: 0.6279 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.39s
Val loss: 0.6885 score: 0.5909 time: 0.32s
Test loss: 0.6884 score: 0.5814 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.40s
Val loss: 0.6881 score: 0.5455 time: 0.26s
Test loss: 0.6880 score: 0.5581 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.42s
Val loss: 0.6877 score: 0.5227 time: 0.27s
Test loss: 0.6877 score: 0.5581 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.36s
Val loss: 0.6873 score: 0.5227 time: 0.29s
Test loss: 0.6873 score: 0.5349 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.45s
Val loss: 0.6868 score: 0.5455 time: 0.28s
Test loss: 0.6869 score: 0.5349 time: 0.31s
Epoch 34/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.39s
Val loss: 0.6863 score: 0.5455 time: 0.28s
Test loss: 0.6864 score: 0.5349 time: 0.27s
Epoch 35/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.43s
Val loss: 0.6858 score: 0.5455 time: 0.29s
Test loss: 0.6859 score: 0.5349 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.35s
Val loss: 0.6852 score: 0.5682 time: 0.27s
Test loss: 0.6853 score: 0.5349 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.40s
Val loss: 0.6845 score: 0.5455 time: 0.28s
Test loss: 0.6847 score: 0.5349 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.42s
Val loss: 0.6839 score: 0.5455 time: 0.27s
Test loss: 0.6841 score: 0.5349 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.36s
Val loss: 0.6831 score: 0.5455 time: 0.37s
Test loss: 0.6833 score: 0.5349 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.38s
Val loss: 0.6823 score: 0.5455 time: 0.29s
Test loss: 0.6825 score: 0.5349 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.40s
Val loss: 0.6815 score: 0.5455 time: 0.32s
Test loss: 0.6817 score: 0.5349 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.42s
Val loss: 0.6806 score: 0.5455 time: 0.27s
Test loss: 0.6808 score: 0.5349 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.38s
Val loss: 0.6795 score: 0.5455 time: 0.28s
Test loss: 0.6797 score: 0.5349 time: 0.26s
Epoch 44/1000, LR 0.000269
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.41s
Val loss: 0.6784 score: 0.5455 time: 0.36s
Test loss: 0.6786 score: 0.5349 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.39s
Val loss: 0.6772 score: 0.5682 time: 0.28s
Test loss: 0.6773 score: 0.5349 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.42s
Val loss: 0.6758 score: 0.5682 time: 0.27s
Test loss: 0.6758 score: 0.5349 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.40s
Val loss: 0.6743 score: 0.5682 time: 0.28s
Test loss: 0.6743 score: 0.5349 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.40s
Val loss: 0.6727 score: 0.5682 time: 0.29s
Test loss: 0.6725 score: 0.5349 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.40s
Val loss: 0.6709 score: 0.5682 time: 0.42s
Test loss: 0.6705 score: 0.5349 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.48s
Val loss: 0.6690 score: 0.5682 time: 0.28s
Test loss: 0.6684 score: 0.5349 time: 0.24s
Epoch 51/1000, LR 0.000269
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.50s
Val loss: 0.6670 score: 0.5455 time: 0.28s
Test loss: 0.6661 score: 0.5349 time: 0.26s
Epoch 52/1000, LR 0.000269
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 0.35s
Val loss: 0.6648 score: 0.5682 time: 0.31s
Test loss: 0.6637 score: 0.5349 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 0.40s
Val loss: 0.6624 score: 0.5682 time: 0.29s
Test loss: 0.6610 score: 0.5349 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.41s
Val loss: 0.6598 score: 0.5682 time: 0.27s
Test loss: 0.6582 score: 0.5349 time: 0.28s
Epoch 55/1000, LR 0.000269
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.48s
Val loss: 0.6571 score: 0.5682 time: 0.28s
Test loss: 0.6552 score: 0.5581 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.36s
Val loss: 0.6541 score: 0.5682 time: 0.30s
Test loss: 0.6519 score: 0.5581 time: 0.23s
Epoch 57/1000, LR 0.000269
Train loss: 0.6377;  Loss pred: 0.6377; Loss self: 0.0000; time: 0.41s
Val loss: 0.6508 score: 0.5909 time: 0.35s
Test loss: 0.6484 score: 0.5581 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.6347;  Loss pred: 0.6347; Loss self: 0.0000; time: 0.52s
Val loss: 0.6473 score: 0.6364 time: 0.26s
Test loss: 0.6446 score: 0.5581 time: 0.26s
Epoch 59/1000, LR 0.000268
Train loss: 0.6290;  Loss pred: 0.6290; Loss self: 0.0000; time: 0.36s
Val loss: 0.6435 score: 0.6364 time: 0.29s
Test loss: 0.6406 score: 0.5581 time: 0.27s
Epoch 60/1000, LR 0.000268
Train loss: 0.6237;  Loss pred: 0.6237; Loss self: 0.0000; time: 0.47s
Val loss: 0.6394 score: 0.6364 time: 0.29s
Test loss: 0.6363 score: 0.5814 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.41s
Val loss: 0.6350 score: 0.6591 time: 0.28s
Test loss: 0.6317 score: 0.5814 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.6132;  Loss pred: 0.6132; Loss self: 0.0000; time: 0.48s
Val loss: 0.6304 score: 0.6818 time: 0.34s
Test loss: 0.6268 score: 0.6047 time: 0.26s
Epoch 63/1000, LR 0.000268
Train loss: 0.6070;  Loss pred: 0.6070; Loss self: 0.0000; time: 0.35s
Val loss: 0.6254 score: 0.6818 time: 0.29s
Test loss: 0.6216 score: 0.6047 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5997;  Loss pred: 0.5997; Loss self: 0.0000; time: 0.44s
Val loss: 0.6202 score: 0.7045 time: 0.28s
Test loss: 0.6162 score: 0.6279 time: 0.33s
Epoch 65/1000, LR 0.000268
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 0.41s
Val loss: 0.6146 score: 0.7273 time: 0.27s
Test loss: 0.6104 score: 0.6512 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 0.40s
Val loss: 0.6088 score: 0.7045 time: 0.28s
Test loss: 0.6044 score: 0.7209 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.5772;  Loss pred: 0.5772; Loss self: 0.0000; time: 0.38s
Val loss: 0.6027 score: 0.7045 time: 0.29s
Test loss: 0.5980 score: 0.7442 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.39s
Val loss: 0.5962 score: 0.7045 time: 0.27s
Test loss: 0.5913 score: 0.7442 time: 0.26s
Epoch 69/1000, LR 0.000268
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.35s
Val loss: 0.5895 score: 0.7045 time: 0.29s
Test loss: 0.5843 score: 0.7442 time: 0.23s
Epoch 70/1000, LR 0.000268
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.41s
Val loss: 0.5824 score: 0.7045 time: 0.38s
Test loss: 0.5772 score: 0.7442 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.5381;  Loss pred: 0.5381; Loss self: 0.0000; time: 0.40s
Val loss: 0.5751 score: 0.7045 time: 0.27s
Test loss: 0.5700 score: 0.7442 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.40s
Val loss: 0.5674 score: 0.7727 time: 0.28s
Test loss: 0.5627 score: 0.7442 time: 0.26s
Epoch 73/1000, LR 0.000267
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.39s
Val loss: 0.5596 score: 0.7727 time: 0.29s
Test loss: 0.5554 score: 0.7442 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.4943;  Loss pred: 0.4943; Loss self: 0.0000; time: 0.41s
Val loss: 0.5516 score: 0.7727 time: 0.28s
Test loss: 0.5480 score: 0.7674 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.4807;  Loss pred: 0.4807; Loss self: 0.0000; time: 0.51s
Val loss: 0.5434 score: 0.7955 time: 0.35s
Test loss: 0.5405 score: 0.7907 time: 0.26s
Epoch 76/1000, LR 0.000267
Train loss: 0.4769;  Loss pred: 0.4769; Loss self: 0.0000; time: 0.38s
Val loss: 0.5351 score: 0.7955 time: 0.28s
Test loss: 0.5329 score: 0.7907 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.4688;  Loss pred: 0.4688; Loss self: 0.0000; time: 0.41s
Val loss: 0.5266 score: 0.7955 time: 0.29s
Test loss: 0.5253 score: 0.7907 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.4638;  Loss pred: 0.4638; Loss self: 0.0000; time: 0.40s
Val loss: 0.5182 score: 0.8409 time: 0.27s
Test loss: 0.5173 score: 0.8140 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.4529;  Loss pred: 0.4529; Loss self: 0.0000; time: 0.39s
Val loss: 0.5097 score: 0.8409 time: 0.28s
Test loss: 0.5089 score: 0.8140 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.4297;  Loss pred: 0.4297; Loss self: 0.0000; time: 0.40s
Val loss: 0.5012 score: 0.8636 time: 0.29s
Test loss: 0.5011 score: 0.8140 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.4248;  Loss pred: 0.4248; Loss self: 0.0000; time: 0.47s
Val loss: 0.4929 score: 0.8636 time: 0.28s
Test loss: 0.4930 score: 0.8140 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.4120;  Loss pred: 0.4120; Loss self: 0.0000; time: 0.50s
Val loss: 0.4847 score: 0.8864 time: 0.28s
Test loss: 0.4850 score: 0.7907 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.3960;  Loss pred: 0.3960; Loss self: 0.0000; time: 0.36s
Val loss: 0.4763 score: 0.8864 time: 0.28s
Test loss: 0.4776 score: 0.7907 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.3842;  Loss pred: 0.3842; Loss self: 0.0000; time: 0.40s
Val loss: 0.4678 score: 0.8864 time: 0.29s
Test loss: 0.4714 score: 0.7907 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.3587;  Loss pred: 0.3587; Loss self: 0.0000; time: 0.45s
Val loss: 0.4592 score: 0.8864 time: 0.27s
Test loss: 0.4654 score: 0.8140 time: 0.26s
Epoch 86/1000, LR 0.000266
Train loss: 0.3558;  Loss pred: 0.3558; Loss self: 0.0000; time: 0.47s
Val loss: 0.4508 score: 0.8864 time: 0.26s
Test loss: 0.4590 score: 0.8140 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.3526;  Loss pred: 0.3526; Loss self: 0.0000; time: 0.38s
Val loss: 0.4424 score: 0.8864 time: 0.29s
Test loss: 0.4534 score: 0.8140 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.3544;  Loss pred: 0.3544; Loss self: 0.0000; time: 0.40s
Val loss: 0.4339 score: 0.8864 time: 0.28s
Test loss: 0.4489 score: 0.8140 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.3548;  Loss pred: 0.3548; Loss self: 0.0000; time: 0.39s
Val loss: 0.4258 score: 0.8864 time: 0.27s
Test loss: 0.4434 score: 0.8140 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.3293;  Loss pred: 0.3293; Loss self: 0.0000; time: 0.38s
Val loss: 0.4182 score: 0.8864 time: 0.29s
Test loss: 0.4375 score: 0.8140 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.3159;  Loss pred: 0.3159; Loss self: 0.0000; time: 0.48s
Val loss: 0.4109 score: 0.8864 time: 0.29s
Test loss: 0.4316 score: 0.8140 time: 0.23s
Epoch 92/1000, LR 0.000266
Train loss: 0.2887;  Loss pred: 0.2887; Loss self: 0.0000; time: 0.42s
Val loss: 0.4038 score: 0.8864 time: 0.27s
Test loss: 0.4260 score: 0.8140 time: 0.27s
Epoch 93/1000, LR 0.000265
Train loss: 0.3064;  Loss pred: 0.3064; Loss self: 0.0000; time: 0.36s
Val loss: 0.3963 score: 0.9091 time: 0.28s
Test loss: 0.4219 score: 0.8372 time: 0.24s
Epoch 94/1000, LR 0.000265
Train loss: 0.2988;  Loss pred: 0.2988; Loss self: 0.0000; time: 0.40s
Val loss: 0.3889 score: 0.9091 time: 0.29s
Test loss: 0.4180 score: 0.8605 time: 0.24s
Epoch 95/1000, LR 0.000265
Train loss: 0.2599;  Loss pred: 0.2599; Loss self: 0.0000; time: 0.41s
Val loss: 0.3820 score: 0.9091 time: 0.27s
Test loss: 0.4139 score: 0.8605 time: 0.33s
Epoch 96/1000, LR 0.000265
Train loss: 0.2867;  Loss pred: 0.2867; Loss self: 0.0000; time: 0.41s
Val loss: 0.3750 score: 0.9091 time: 0.28s
Test loss: 0.4107 score: 0.8605 time: 0.26s
Epoch 97/1000, LR 0.000265
Train loss: 0.2533;  Loss pred: 0.2533; Loss self: 0.0000; time: 0.35s
Val loss: 0.3680 score: 0.8864 time: 0.29s
Test loss: 0.4080 score: 0.8605 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.2499;  Loss pred: 0.2499; Loss self: 0.0000; time: 0.41s
Val loss: 0.3606 score: 0.8864 time: 0.30s
Test loss: 0.4062 score: 0.8605 time: 0.24s
Epoch 99/1000, LR 0.000265
Train loss: 0.2160;  Loss pred: 0.2160; Loss self: 0.0000; time: 0.52s
Val loss: 0.3527 score: 0.9091 time: 0.27s
Test loss: 0.4057 score: 0.8605 time: 0.26s
Epoch 100/1000, LR 0.000265
Train loss: 0.2415;  Loss pred: 0.2415; Loss self: 0.0000; time: 0.35s
Val loss: 0.3453 score: 0.9091 time: 0.28s
Test loss: 0.4052 score: 0.8605 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.2066;  Loss pred: 0.2066; Loss self: 0.0000; time: 0.41s
Val loss: 0.3378 score: 0.9318 time: 0.45s
Test loss: 0.4057 score: 0.8605 time: 0.24s
Epoch 102/1000, LR 0.000264
Train loss: 0.1980;  Loss pred: 0.1980; Loss self: 0.0000; time: 0.40s
Val loss: 0.3303 score: 0.9318 time: 0.29s
Test loss: 0.4069 score: 0.8605 time: 0.23s
Epoch 103/1000, LR 0.000264
Train loss: 0.1978;  Loss pred: 0.1978; Loss self: 0.0000; time: 0.52s
Val loss: 0.3236 score: 0.9318 time: 0.28s
Test loss: 0.4069 score: 0.8605 time: 0.24s
Epoch 104/1000, LR 0.000264
Train loss: 0.2414;  Loss pred: 0.2414; Loss self: 0.0000; time: 0.41s
Val loss: 0.3187 score: 0.9091 time: 0.28s
Test loss: 0.4030 score: 0.8605 time: 0.25s
Epoch 105/1000, LR 0.000264
Train loss: 0.1706;  Loss pred: 0.1706; Loss self: 0.0000; time: 0.34s
Val loss: 0.3142 score: 0.9091 time: 0.33s
Test loss: 0.3997 score: 0.8605 time: 0.23s
Epoch 106/1000, LR 0.000264
Train loss: 0.1846;  Loss pred: 0.1846; Loss self: 0.0000; time: 0.42s
Val loss: 0.3091 score: 0.9091 time: 0.36s
Test loss: 0.3990 score: 0.8605 time: 0.23s
Epoch 107/1000, LR 0.000264
Train loss: 0.1646;  Loss pred: 0.1646; Loss self: 0.0000; time: 0.45s
Val loss: 0.3057 score: 0.8864 time: 0.29s
Test loss: 0.3962 score: 0.8605 time: 0.25s
Epoch 108/1000, LR 0.000264
Train loss: 0.1582;  Loss pred: 0.1582; Loss self: 0.0000; time: 0.42s
Val loss: 0.3018 score: 0.8864 time: 0.27s
Test loss: 0.3954 score: 0.8605 time: 0.26s
Epoch 109/1000, LR 0.000264
Train loss: 0.1509;  Loss pred: 0.1509; Loss self: 0.0000; time: 0.38s
Val loss: 0.2965 score: 0.8864 time: 0.28s
Test loss: 0.3970 score: 0.8605 time: 0.25s
Epoch 110/1000, LR 0.000263
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.41s
Val loss: 0.2928 score: 0.9091 time: 0.29s
Test loss: 0.3973 score: 0.8605 time: 0.24s
Epoch 111/1000, LR 0.000263
Train loss: 0.1229;  Loss pred: 0.1229; Loss self: 0.0000; time: 0.40s
Val loss: 0.2886 score: 0.9091 time: 0.27s
Test loss: 0.3988 score: 0.8605 time: 0.32s
Epoch 112/1000, LR 0.000263
Train loss: 0.1463;  Loss pred: 0.1463; Loss self: 0.0000; time: 0.50s
Val loss: 0.2830 score: 0.9091 time: 0.27s
Test loss: 0.4027 score: 0.8605 time: 0.25s
Epoch 113/1000, LR 0.000263
Train loss: 0.1002;  Loss pred: 0.1002; Loss self: 0.0000; time: 0.41s
Val loss: 0.2767 score: 0.9091 time: 0.27s
Test loss: 0.4087 score: 0.8605 time: 0.25s
Epoch 114/1000, LR 0.000263
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 0.38s
Val loss: 0.2743 score: 0.9091 time: 0.29s
Test loss: 0.4096 score: 0.8605 time: 0.23s
Epoch 115/1000, LR 0.000263
Train loss: 0.1158;  Loss pred: 0.1158; Loss self: 0.0000; time: 0.40s
Val loss: 0.2722 score: 0.9091 time: 0.27s
Test loss: 0.4111 score: 0.8605 time: 0.25s
Epoch 116/1000, LR 0.000263
Train loss: 0.1127;  Loss pred: 0.1127; Loss self: 0.0000; time: 0.42s
Val loss: 0.2735 score: 0.9091 time: 0.27s
Test loss: 0.4097 score: 0.8605 time: 0.26s
     INFO: Early stopping counter 1 of 2
Epoch 117/1000, LR 0.000262
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.46s
Val loss: 0.2734 score: 0.9091 time: 0.30s
Test loss: 0.4108 score: 0.8605 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 114,   Train_Loss: 0.1158,   Val_Loss: 0.2722,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2722,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.4111


[0.3008384720887989, 0.3823679880006239, 0.28155691106803715, 0.30424436810426414, 0.30634041398297995, 0.28447832097299397, 0.2997548639541492, 0.317103725974448, 0.39863283606246114, 0.29564567806664854, 0.29581137106288224, 0.28361519204918295, 0.3089027020614594, 0.29900632600765675, 0.37476245197467506, 0.2895199320046231, 0.3014188609085977, 0.30226633604615927, 0.2810097150504589, 0.3027656190097332, 0.29860675300005823, 0.30802042596042156, 0.313578751985915, 0.2873057370306924, 0.3321150050032884, 0.2960165160475299, 0.3038980320561677, 0.29250062606297433, 0.3042488479986787, 0.30717705795541406, 0.30715376790612936, 0.29152516496833414, 0.2932433240348473, 0.2946956770028919, 0.28895359102170914, 0.3053249450167641, 0.29505437694024295, 0.28553690400440246, 0.29922581498976797, 0.3113178750500083, 0.2932825960451737, 0.2941406930331141, 0.30295683699660003, 0.3821052919374779, 0.2816089439438656, 0.2940084059955552, 0.2963209699373692, 0.3004275569692254, 0.288389221066609, 0.3778758469270542, 0.3051485070027411, 0.30833206593524665, 0.27748763596173376, 0.29932345694396645, 0.3052681309636682, 0.3117521529784426, 0.39632976707071066, 0.2919371130410582, 0.29500709706917405, 0.3106193230487406, 0.30851817107759416, 0.359289790969342, 0.28554788103792816, 0.38195042207371444, 0.29580746905412525, 0.3198438809486106, 0.29401424899697304, 0.30215807794593275, 0.30736375297419727, 0.3812034389702603, 0.3304374360013753, 0.29590921103954315, 0.30480025010183454, 0.40141311509069055, 0.3006723370635882, 0.34995029494166374, 0.3895039709750563, 0.30328268895391375, 0.30473851796705276, 0.2939894929295406, 0.29239833797328174, 0.29774714494124055, 0.3943545169895515, 0.2956749899312854, 0.29446942696813494, 0.2932730669854209, 0.3029289960395545, 0.28301976900547743, 0.27888675802387297, 0.2925260589690879, 0.30297933996189386, 0.287580223986879, 0.29590417398139834, 0.3037118320353329, 0.28649473492987454, 0.2999019300332293, 0.30556270806118846, 0.27530844998545945, 0.267596930032596, 0.2618183009326458, 0.2603730949340388, 0.2860270630335435, 0.32606051210314035, 0.25197975197806954, 0.26794589194469154, 0.2742889750516042, 0.24430776305962354, 0.2664315269794315, 0.26261132408399135, 0.26682702207472175, 0.24439638305921108, 0.25476013496518135, 0.2745634240563959, 0.24579346796963364, 0.2483335790457204, 0.2687746649608016, 0.25907525210641325, 0.39617527392692864, 0.263869907008484, 0.2481356409844011, 0.2575236480915919, 0.2597709440160543, 0.25880056992173195, 0.2538482249947265, 0.2585324289975688, 0.3575592909473926, 0.25973768904805183, 0.26209833996836096, 0.26575631310697645, 0.35520486801397055, 0.24547244200948626, 0.26023679797071964, 0.25540810299571604, 0.2664386979304254, 0.24599357799161226, 0.2583898620214313, 0.3364179120399058, 0.2579181030159816, 0.26427661906927824, 0.2642121189273894, 0.24298696499317884, 0.4185684889089316, 0.2591483350843191, 0.27001950691919774, 0.24560141598340124, 0.25413350702729076, 0.27159911196213216, 0.27398780488874763, 0.24604794604238123, 0.2744811719749123, 0.26449318195227534, 0.2693385439924896, 0.2552678299834952, 0.2600854359334335, 0.26272112503647804, 0.2686714919982478, 0.24323290295433253, 0.260737195960246, 0.26004115503747016, 0.27000570006202906, 0.2391328269150108, 0.2521608219249174, 0.2678733179345727, 0.26325490104500204, 0.24137414002325386, 0.257773493998684, 0.2649128190241754, 0.2625583050539717, 0.24643797799944878, 0.2542866379953921, 0.2656630009878427, 0.24633921799249947, 0.241594337974675, 0.25250280799809843, 0.2752300320426002, 0.24592958798166364, 0.26035551296081394, 0.26048136898316443, 0.34630114398896694, 0.24673405301291496, 0.25380241009406745, 0.28230347600765526, 0.2696878910064697, 0.24757033598143607, 0.2560069609899074, 0.27002591907512397, 0.3713280949741602, 0.2566200130386278, 0.23228680703323334, 0.2438291619764641, 0.2466516300337389, 0.25728869100566953, 0.32746657100506127, 0.24019757600035518, 0.2719374040607363, 0.2406644260045141, 0.2446560829412192, 0.3472679799888283, 0.26492411794606596, 0.24790240393485874, 0.24458374199457467, 0.2508087280439213, 0.24633082607761025, 0.23607209499459714, 0.2536680639022961, 0.2588385579874739, 0.24409017397556454, 0.24136420898139477, 0.2407013470074162, 0.25591930001974106, 0.23716962803155184, 0.2218680219957605, 0.268357518943958, 0.2458537749480456, 0.23176128102932125, 0.24912600801326334, 0.2578330209944397, 0.23706867301370949, 0.31277508300263435, 0.2762013679603115, 0.2522999009815976, 0.2378039340255782, 0.25745779497083277, 0.24826610297895968, 0.25587681494653225, 0.23400159704033285, 0.2449882220244035, 0.26210458390414715, 0.26369315094780177, 0.23576627799775451, 0.2510755310067907, 0.25607230805326253, 0.2529891519807279, 0.2440802810015157, 0.22706938593182713, 0.24671438906807452, 0.2671045269817114, 0.23905783297959715, 0.2451175069436431, 0.2837754039792344, 0.2613766639260575, 0.23446627601515502, 0.24567557498812675, 0.26291130401659757, 0.2784403209807351, 0.23756735306233168, 0.2503019729629159, 0.2604389199987054, 0.23328102903906256, 0.33572311501484364, 0.2548710040282458, 0.24847664299886674, 0.23612204706296325, 0.26476460497360677, 0.2368511320091784, 0.24799089191947132, 0.26602502004243433, 0.26539349707309157, 0.23908829502761364, 0.25057880498934537, 0.2628982700407505, 0.24764604901429266, 0.2507872760761529, 0.24958695005625486, 0.24591526307631284, 0.25506585906259716, 0.24820603500120342, 0.2547929990105331, 0.24140913400333375, 0.23355443589389324, 0.26279132394120097, 0.2553102419478819, 0.23534090700559318, 0.24759603093843907, 0.25106274895370007, 0.23365917499177158, 0.23478107992559671, 0.2705607160460204, 0.2425677189603448, 0.24519156001042575, 0.33044882502872497, 0.26277099805884063, 0.23758991109207273, 0.24475949897896498, 0.25923921598587185, 0.23256584303453565, 0.23964141204487532, 0.23199040000326931, 0.24175161006860435, 0.25163348601199687, 0.2328074510442093, 0.2291913190856576, 0.25276386202313006, 0.2663732538931072, 0.25753986393101513, 0.24230562592856586, 0.32196198706515133, 0.25265198294073343, 0.2520631109364331, 0.23754843208007514, 0.2585600719321519, 0.2631851799087599, 0.24089752696454525]
[0.006837238002018156, 0.008690181545468724, 0.006399020706091754, 0.006914644729642367, 0.006962282135976817, 0.006465416385749863, 0.0068126105444124814, 0.007206902863055637, 0.009059837183237753, 0.006719219956060194, 0.006722985705974596, 0.006445799819299613, 0.00702051595594226, 0.0067955983183558355, 0.008517328453969887, 0.006579998454650526, 0.006850428657013585, 0.006869689455594529, 0.006386584432964975, 0.0068810367956757545, 0.0067865171136376875, 0.0070004642263732176, 0.007126789817861704, 0.006529675841606645, 0.007548068295529281, 0.006727648091989316, 0.006906773455821994, 0.006647741501431235, 0.006914746545424516, 0.0069812967717139554, 0.006980767452412031, 0.006625571931098503, 0.0066646210007919844, 0.006697629022792998, 0.006567127068675208, 0.006939203295835548, 0.0067057812940964304, 0.006489475091009147, 0.006800586704312908, 0.007075406251136552, 0.00666551354648122, 0.006685015750752593, 0.006885382659013637, 0.008684211180397224, 0.006400203271451491, 0.0066820092271717094, 0.006734567498576574, 0.00682789902202785, 0.0065543004787865684, 0.008588087430160323, 0.006935193340971388, 0.007007546953073787, 0.006306537180948494, 0.006802805839635601, 0.0069379120673560965, 0.0070852762040555135, 0.009007494706152515, 0.0066349343872967766, 0.006704706751572137, 0.00705953006928956, 0.007011776615399867, 0.008165677067485045, 0.006489724569043822, 0.008680691410766238, 0.006722897023957392, 0.007269179112468423, 0.006682142022658478, 0.0068672290442257445, 0.006985539840322665, 0.00866371452205137, 0.007509941727303984, 0.006725209341807799, 0.00692727841140533, 0.00912302534297024, 0.0068334622059906406, 0.007953415794128721, 0.008852362976705825, 0.006892788385316221, 0.006925875408342108, 0.006681579384762285, 0.006645416772120039, 0.006766980566846376, 0.008962602658853442, 0.006719886134801941, 0.006692486976548521, 0.006665296976941384, 0.006884749909989874, 0.006432267477397214, 0.006338335409633477, 0.006648319522024725, 0.0068858940900430425, 0.006535914181519977, 0.006725094863213599, 0.006902541637166657, 0.006511243975678967, 0.006815952955300666, 0.0069446070013906465, 0.006402522092685103, 0.006223184419362698, 0.006088797696108042, 0.0060551882542799725, 0.0066517921635707794, 0.0075828026070497755, 0.005859994232048129, 0.006231299812667245, 0.006378813373293121, 0.005681575885107524, 0.006196082022777477, 0.006107240094976543, 0.006205279583133064, 0.00568363681533049, 0.005924654301515845, 0.006385195908288277, 0.0057161271620845035, 0.0057751995126911725, 0.006250573603739572, 0.006025005862939843, 0.009213378463416945, 0.006136509465313581, 0.005770596301962816, 0.005988922048641672, 0.006041184744559402, 0.006018617905156557, 0.005903447092900616, 0.006012382069710902, 0.008315332347613781, 0.0060404113732105075, 0.006095310231822348, 0.006180379374580848, 0.008260578325906293, 0.005708661442081076, 0.006052018557458596, 0.005939723325481768, 0.006196248789079661, 0.005720780883525866, 0.0060090665586379375, 0.007823672373021064, 0.0059980954189763165, 0.006145967885332052, 0.006144467882032312, 0.005650859651004159, 0.009734150904858874, 0.006026705467077189, 0.006279523416725529, 0.005711660836823285, 0.005910081558774204, 0.006316258417724004, 0.006371809416017387, 0.005722045256799564, 0.006383283069184007, 0.006151004231448264, 0.006263687069592781, 0.0059364611624068645, 0.006048498510079848, 0.00610979360549949, 0.00624817423251739, 0.00565657913847285, 0.006063655720005722, 0.006047468721801632, 0.006279202327023932, 0.005561228532907228, 0.005864205161044591, 0.006229612044990063, 0.006122207001046559, 0.005613352093564043, 0.005994732418574046, 0.006160763233120359, 0.006106007094278412, 0.0057311157674290415, 0.0059136427440788855, 0.006178209325298667, 0.005728819023081383, 0.005618472976155232, 0.0058721583255371725, 0.006400698419595354, 0.00571929274375962, 0.00605477937118172, 0.006057706255422428, 0.008053514976487604, 0.005738001232858487, 0.005902381630094592, 0.006565197116457099, 0.00627181141875511, 0.005757449673986886, 0.005953650255579241, 0.00627967253663079, 0.00863553709242233, 0.005967907279968088, 0.0054020187682147286, 0.00567044562735963, 0.005736084419389277, 0.005983457930364408, 0.007615501651280495, 0.005585990139543143, 0.006324125675831077, 0.0055968471163840485, 0.005689676347470214, 0.008075999534623913, 0.00616102599874572, 0.005765172184531598, 0.005687993999873829, 0.005832761117300495, 0.005728623862270006, 0.005490048720804584, 0.0058992573000533986, 0.006019501348545905, 0.005676515673850338, 0.005613121139102204, 0.005597705744358516, 0.005951611628366071, 0.005515572744919811, 0.005159721441761872, 0.006240872533580418, 0.005717529649954549, 0.005389797233240029, 0.0057936280933317055, 0.005996116767312551, 0.005513224953807197, 0.007273839139596148, 0.006423287626983988, 0.005867439557711573, 0.005530324047106469, 0.005987390580717041, 0.005773630301836271, 0.005950623603407727, 0.005441897605589136, 0.00569740051219543, 0.006095455439631329, 0.006132398859251204, 0.005482936697622198, 0.005838965837367226, 0.005955169954727036, 0.005883468650714602, 0.005676285604686412, 0.005280683393763422, 0.005737543931815687, 0.006211733185621195, 0.0055594844878976086, 0.005700407138224258, 0.006599427999517079, 0.006078527068047849, 0.005452704093375698, 0.005713385464840157, 0.006114216372479013, 0.006475356301877561, 0.005524822164240272, 0.005820976115416649, 0.006056719069737334, 0.005425140210210757, 0.007807514302670782, 0.00592723265181967, 0.005778526581368994, 0.005491210396813099, 0.006157316394735041, 0.005508165860678567, 0.005767230044638868, 0.006186628373079869, 0.006171941792397479, 0.005560192907618922, 0.0058274140695196595, 0.006113913256761639, 0.0057592104421928525, 0.005832262234329138, 0.005804347675726857, 0.00571895960642588, 0.005931764164246445, 0.00577223337212101, 0.0059254185816403045, 0.005614165907054273, 0.0054314985091603075, 0.006111426138167464, 0.005937447487160043, 0.005473044348967283, 0.00575804723112649, 0.0058386685803186065, 0.005433934302134223, 0.0054600251145487605, 0.006292109675488846, 0.0056411097432638325, 0.00570212930256804, 0.007684856396016859, 0.006110953443228852, 0.005525346769583087, 0.005692081371603837, 0.006028818976415625, 0.005408507977547341, 0.005573056094066868, 0.0053951255814713795, 0.005622130466711729, 0.005851941535162718, 0.005414126768469984, 0.005330030676410641, 0.005878229349375118, 0.0061947268347234226, 0.0059892991611863984, 0.005635014556478276, 0.007487488071282589, 0.005875627510249615, 0.005861932812475188, 0.005524382141397097, 0.006013024928654695, 0.0061205855792734855, 0.005602268068942913]
[146.25788947303408, 115.0723946062352, 156.2739122015995, 144.62058993618334, 143.63106528427102, 154.66907935025742, 146.78660896301682, 138.75585934788253, 110.37725952185663, 148.82679932185889, 148.74343687973604, 155.1397852917899, 142.43967341938543, 147.15407726481772, 117.40770658361832, 151.9757195829177, 145.9762666057668, 145.56698762934928, 156.57821649368682, 145.32693686922724, 147.3509877386846, 142.84766947778226, 140.31562955508014, 153.14695924536846, 132.48422786427352, 148.64035489470845, 144.78540615184943, 150.42702845540904, 144.618460478743, 143.239863982246, 143.25072519848442, 150.93036652523406, 150.04604161004292, 149.30656753260828, 152.27358775649986, 144.10876254340812, 149.12505435874715, 154.0956681358484, 147.04613638199825, 141.33464065605642, 150.0259496926397, 149.58827881406577, 145.23521052107432, 115.15150647848006, 156.24503747569437, 149.65558501978745, 148.4876349091996, 146.45793629546168, 152.57158307535133, 116.44036092228417, 144.19208677171983, 142.70328928175934, 158.56562346463505, 146.99816863413022, 144.13558290903515, 141.13775824682938, 111.01866086215472, 150.71739095334488, 149.14895416798316, 141.65248822300654, 142.61720742838753, 122.46381919533722, 154.08974438915783, 115.19819708828135, 148.74539896066355, 137.567115148498, 149.65261088571592, 145.61914180521504, 143.1528590285457, 115.42393247777775, 133.15682548697922, 148.6942560707249, 144.35683692943, 109.61276138189744, 146.33870355254726, 125.73214149550793, 112.96418850327395, 145.07916739911968, 144.38607988753532, 149.6652127310731, 150.47965150889658, 147.77639600434543, 111.57473315099821, 148.81204531443652, 149.42128441999964, 150.03082435178857, 145.2485584914254, 155.4661716904604, 157.7701297536456, 150.41394997445207, 145.22442357136939, 153.0007849288257, 148.69678723344404, 144.87417136544477, 153.58048381157826, 146.7146276621987, 143.9966292980656, 156.18844972710087, 160.68943688839093, 164.23603639174937, 165.1476317508663, 150.3354247110429, 131.8773614217907, 170.64863213192783, 160.48016145317843, 156.76896963106176, 176.0075057029842, 161.3923115161953, 163.74008299142216, 161.15309336232954, 175.9436840339089, 168.78621926415963, 156.6122659920198, 174.94362382857304, 173.15419108941092, 159.98531709181432, 165.97494222388355, 108.53781856141534, 162.95909028617444, 173.29231636942944, 166.9749567414735, 165.5304451499492, 166.15110242224088, 169.39255730818402, 166.32342861871447, 120.25977533982345, 165.55163849188224, 164.06055835832734, 161.80236509636916, 121.05689947444293, 175.17241303338756, 165.23412651595152, 168.35801016352735, 161.3879677914824, 174.80131128246848, 166.4151978084709, 127.81721323714581, 166.71958849408702, 162.70830220030874, 162.7480229694431, 176.96422522585564, 102.73109691579185, 165.92813527437514, 159.2477539515972, 175.08042381525243, 169.20240271733368, 158.32157803960456, 156.94129166610205, 174.7626862635681, 156.65919702474244, 162.57507918581751, 159.65037666944187, 168.45052509272418, 165.330287894342, 163.67164990645338, 160.04675330526112, 176.785292934128, 164.91701478049225, 165.35844102755195, 159.25589715373232, 179.81638303168828, 170.5260939100347, 160.52363979940185, 163.3397890383411, 178.1466730274312, 166.81311694607177, 162.31755095277555, 163.77314742019257, 174.4860932112346, 169.10050932671973, 161.8591969529388, 174.55604653786497, 177.98430360775862, 170.29513588745448, 156.23295060091567, 174.84679396610892, 165.15878427537626, 165.07898498790215, 124.16938478658322, 174.27671403650643, 169.42313504455217, 152.31835118754955, 159.44356952596155, 173.68801407299594, 167.96418282429124, 159.2439723833954, 115.8005563866431, 167.56292500666117, 185.11598032275668, 176.3529827664774, 174.3349516648973, 167.12743895553677, 131.3111132779883, 179.01929201790287, 158.12462485078402, 178.67202358853592, 175.75692164715613, 123.82368222196392, 162.3106281654359, 173.45535709810665, 175.80890556884938, 171.44538922293765, 174.5619932539511, 182.14774601370874, 169.51286393135425, 166.12671749655212, 176.16440391535244, 178.15400295457474, 178.64461721801305, 168.02171620773842, 181.3048338309132, 193.8089122226981, 160.2340048798105, 174.90071083548284, 185.53573663825176, 172.60341600990412, 166.77460409901227, 181.3820419769817, 137.4789819802808, 155.68351568113468, 170.4320922549088, 180.82123063353072, 167.0176659629647, 173.20125254330114, 168.0496140652104, 183.75942961016824, 175.51864185420607, 164.05665005738814, 163.06832333507168, 182.38401337620277, 171.2632044531532, 167.9213200634568, 169.9677621089288, 176.1715441475298, 189.36942918808904, 174.2906044614011, 160.98566537190965, 179.8727925542181, 175.42606620051203, 151.52828397751688, 164.51353902108315, 183.39524442833152, 175.02757448345506, 163.55325671841564, 154.4316564804388, 181.00130108667702, 171.7924932472297, 165.10589124011784, 184.32703326595714, 128.0817378276107, 168.71279714201845, 173.05449510679406, 182.10921231143575, 162.40841559726795, 181.54863620552032, 173.39346484532643, 161.63893153035366, 162.02356302708293, 179.84987510590466, 171.60270199958995, 163.56136536514597, 173.63491229176967, 171.4600544046055, 172.28464865774492, 174.85697903450657, 168.58391067323177, 173.24316872388505, 168.76444865827096, 178.12084939340443, 184.1112537016229, 163.62792863595894, 168.42254220564277, 182.71366651518025, 173.66998912309415, 171.27192376886578, 184.0287247505443, 183.149340711896, 158.92920682796395, 177.2700843471661, 175.3730837968958, 130.12604900701987, 163.64058559602262, 180.98411587576334, 175.68266065005912, 165.86996622587932, 184.89387538141003, 179.4347631032478, 185.35249734210564, 177.86851548909002, 170.88345705288975, 184.7019921704931, 187.6161809772963, 170.119255402361, 161.42761846974116, 166.96444326583173, 177.46183083952343, 133.55613931932479, 170.19458743012063, 170.5921974867795, 181.01571803052414, 166.3056468025872, 163.3830598474697, 178.49913422452295]
Elapsed: 0.2758555133368285~0.03698832490784889
Time per graph: 0.006362816570826816~0.0008097136715329293
Speed: 159.36850754371636~17.573401711662225
Total Time: 0.2417
best val loss: 0.27216389775276184 test_score: 0.8605

Testing...
Test loss: 0.4057 score: 0.8605 time: 0.26s
test Score 0.8605
Epoch Time List: [0.9325647850055248, 1.02826225804165, 0.9196861139498651, 0.9314751740312204, 0.9515257851453498, 0.9060573930619285, 1.0266906770411879, 0.9305605189874768, 1.0373830730095506, 0.9067316161235794, 0.926039990154095, 1.025372585048899, 0.9212236140156165, 0.9370648699114099, 0.9933580511715263, 0.9888336119474843, 0.9226645348826423, 0.9630238921381533, 0.8757127522258088, 0.9851632419740781, 1.002686772029847, 0.9287080579670146, 1.07062229514122, 0.8849836111767218, 1.009458996122703, 0.991390666924417, 0.9575355302076787, 0.8906843890435994, 0.989829248865135, 0.9328214880079031, 1.0097140250727534, 0.8893363010138273, 0.9348840320017189, 0.9077081199502572, 0.920971833053045, 0.8925532691646367, 0.9147983019938692, 1.0107704949332401, 0.9301677360199392, 0.9184605869231746, 0.9082575778011233, 0.9114818859379739, 0.9133814490633085, 1.0705526669044048, 0.8824890641262755, 0.9308982848888263, 0.8923204911407083, 0.9995000790804625, 0.8803525590337813, 1.0743921048706397, 0.9201343930326402, 1.038186733960174, 0.8592302310280502, 0.9669520899187773, 0.9378374150255695, 1.079749378026463, 1.0372315250569955, 0.8875996749848127, 1.0160543880192563, 0.9457559660077095, 1.0626029400154948, 1.0315705239772797, 0.8772147349081933, 1.0309126090724021, 0.9130487130023539, 1.0392172060674056, 0.9302638811059296, 0.9363078250316903, 0.9439260609215125, 1.0754015928832814, 0.9885327169904485, 0.9114736530464143, 0.978687408962287, 1.1030115720350295, 1.0838215029798448, 1.012900240952149, 1.0748679880052805, 0.919471194036305, 0.9506512050284073, 0.9188900798326358, 0.9759108148282394, 0.9296601270325482, 1.1232495300937444, 0.9711103270528838, 0.9205958190141246, 0.9196172878146172, 1.027683955966495, 0.8773189899511635, 0.9962631680537015, 0.9278033240698278, 0.9694702250417322, 0.870298410882242, 0.9199930350296199, 1.1599836990935728, 0.9403317060787231, 0.9296959170605987, 0.9240487399511039, 1.0749881839146838, 0.9157702560769394, 0.9257234390825033, 0.9369361890712753, 0.9465842420468107, 0.9618215300142765, 0.9265642800601199, 0.9187861719401553, 0.9705855370266363, 0.8879208141006529, 0.9412116081221029, 1.07617719785776, 0.9386053641792387, 0.9160215689335018, 0.9222140460042283, 0.9593352690571919, 0.8720107978442684, 1.030374413006939, 0.9478116859681904, 1.0596928290324286, 1.0637568728998303, 0.9143080001231283, 1.019871161901392, 0.9600928550353274, 1.071581907919608, 0.9238150808960199, 0.9128794240532443, 0.890344938961789, 1.0952084149466828, 0.895997968968004, 0.9378464069450274, 0.9274057899601758, 1.0120289791375399, 0.87762713606935, 1.0227652138564736, 0.9631845079129562, 0.9739103970350698, 0.8822119371034205, 0.9664030810818076, 1.1450551762245595, 1.027005627984181, 0.9136834340170026, 0.9373034070013091, 0.905511713004671, 1.082392704905942, 0.8990892110159621, 0.9475315739400685, 0.891457170015201, 0.94129718397744, 0.9271345708984882, 1.0317898190114647, 0.8721982089336962, 0.9860168199520558, 0.9406517121242359, 0.9417162740137428, 0.9736892351647839, 0.9295245709363371, 0.922848965972662, 0.9500798321096227, 0.9025943240849301, 1.0237721930025145, 0.9095130999339744, 0.9340823710663244, 0.9059591939439997, 0.9390786540461704, 1.0283276481786743, 0.9276418079389259, 0.9004697980126366, 0.9320883211912587, 0.9600274590775371, 0.9756710979854688, 0.9104571641655639, 0.9421529739629477, 0.9476328949676827, 0.8715875080088153, 0.9999120089923963, 0.9333694879896939, 0.9502982039703056, 0.874875235138461, 0.930463369935751, 0.9848591670161113, 1.0188550850143656, 0.8704537468729541, 0.9475270219845697, 0.9408008870668709, 0.9954641199437901, 0.8938691469375044, 0.9277224390534684, 0.9694159800419584, 1.0695392459165305, 0.9195247720927, 0.8696056031621993, 1.0179123601410538, 0.9019823530688882, 0.9780400418676436, 1.0486241669859737, 0.9107350461417809, 0.9279081849381328, 0.8914675860432908, 0.9143786932108924, 0.994648227118887, 0.9616359759820625, 0.9032043629558757, 0.9271547470707446, 0.8954885349376127, 0.8990711669903249, 0.9678916339762509, 0.9433452610392123, 0.9391041989438236, 0.9034576669801027, 0.918145335977897, 0.9869100878713652, 0.9511202579597011, 0.8687360010808334, 0.9052092260681093, 0.9424695989582688, 0.9464083130005747, 0.9304294551257044, 0.9070610599592328, 0.9439360230462626, 0.8878970709629357, 1.0422052560606971, 0.9491254100576043, 0.9650763710960746, 0.8486233310541138, 0.9329196979524568, 0.927886595018208, 0.987414441886358, 0.9030469540739432, 0.9626508528599516, 0.9387754129711539, 0.9190401359228417, 0.9940380469197407, 0.9238298571435735, 0.9374251392437145, 0.927299048868008, 0.928043233929202, 1.0506816810229793, 1.0038534610066563, 1.0373335549375042, 0.8912229900015518, 0.9273048270260915, 0.961500785080716, 1.0145502322120592, 0.8968430679524317, 0.9981644318904728, 1.0474737408803776, 0.9207474761642516, 0.9973143829265609, 0.9373799350578338, 1.0749585980083793, 0.8697846711147577, 1.0589672289788723, 0.9344343569828197, 0.9229768089717254, 0.9009931959444657, 0.9185728180455044, 0.8737671830458567, 1.0305904048727825, 0.9314339922275394, 0.9420903729042038, 0.9158051238628104, 0.9356874589575455, 1.118029338074848, 0.9089428230654448, 0.9444711619289592, 0.9202774788718671, 0.9117579660378397, 0.9378762429114431, 0.99705210106913, 1.0329467710107565, 0.8803756308043376, 0.9176400339929387, 0.9784288939554244, 0.9891879939241335, 0.8979562311433256, 0.9233630289090797, 0.9090585849480703, 0.9042447730898857, 0.9964631419861689, 0.9465807209489867, 0.8804292009444907, 0.9301268191775307, 1.0011361410142854, 0.9505997690139338, 0.876575883012265, 0.9427981299813837, 1.0381373829441145, 0.8586102781118825, 1.0940618368331343, 0.9176190379075706, 1.0343342020642012, 0.9305872558616102, 0.9012096349615604, 1.0061329840682447, 0.9952880708733574, 0.9499976730439812, 0.9137074400205165, 0.934963820152916, 0.9855610820231959, 1.0112341509666294, 0.9279964950401336, 0.9039694799575955, 0.9253451869590208, 0.9451356920180842, 1.0032821049680933]
Total Epoch List: [97, 89, 117]
Total Time List: [0.3060347510036081, 0.27061543404124677, 0.24169461498968303]
========================training times:6========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcd630>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.30s
Epoch 2/1000, LR 0.000000
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.28s
Epoch 3/1000, LR 0.000030
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.30s
Epoch 4/1000, LR 0.000060
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.31s
Epoch 5/1000, LR 0.000090
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.42s
Epoch 6/1000, LR 0.000120
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.30s
Epoch 7/1000, LR 0.000150
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.29s
Epoch 8/1000, LR 0.000180
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.29s
Epoch 9/1000, LR 0.000210
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.32s
Epoch 10/1000, LR 0.000240
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.41s
Epoch 11/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.28s
Epoch 12/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.29s
Epoch 13/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.29s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.33s
Epoch 15/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.42s
Epoch 16/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.33s
Val loss: 0.6918 score: 0.5814 time: 0.28s
Test loss: 0.6912 score: 0.5455 time: 0.28s
Epoch 17/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.38s
Val loss: 0.6917 score: 0.5581 time: 0.27s
Test loss: 0.6908 score: 0.5909 time: 0.29s
Epoch 18/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.46s
Val loss: 0.6915 score: 0.5581 time: 0.26s
Test loss: 0.6905 score: 0.6591 time: 0.30s
Epoch 19/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.40s
Val loss: 0.6913 score: 0.6047 time: 0.24s
Test loss: 0.6901 score: 0.7955 time: 0.31s
Epoch 20/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.33s
Val loss: 0.6910 score: 0.6977 time: 0.29s
Test loss: 0.6897 score: 0.7727 time: 0.29s
Epoch 21/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.38s
Val loss: 0.6907 score: 0.6744 time: 0.27s
Test loss: 0.6893 score: 0.7955 time: 0.29s
Epoch 22/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.40s
Val loss: 0.6904 score: 0.6512 time: 0.26s
Test loss: 0.6888 score: 0.7727 time: 0.49s
Epoch 23/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.39s
Val loss: 0.6901 score: 0.6512 time: 0.25s
Test loss: 0.6882 score: 0.7500 time: 0.44s
Epoch 24/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.38s
Val loss: 0.6897 score: 0.6512 time: 0.24s
Test loss: 0.6877 score: 0.7500 time: 0.30s
Epoch 25/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.33s
Val loss: 0.6892 score: 0.6512 time: 0.32s
Test loss: 0.6870 score: 0.7500 time: 0.29s
Epoch 26/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.41s
Val loss: 0.6887 score: 0.6512 time: 0.28s
Test loss: 0.6863 score: 0.7727 time: 0.29s
Epoch 27/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.37s
Val loss: 0.6882 score: 0.6512 time: 0.24s
Test loss: 0.6856 score: 0.7955 time: 0.29s
Epoch 28/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.39s
Val loss: 0.6875 score: 0.6512 time: 0.26s
Test loss: 0.6848 score: 0.8182 time: 0.42s
Epoch 29/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.37s
Val loss: 0.6869 score: 0.6744 time: 0.25s
Test loss: 0.6839 score: 0.7955 time: 0.29s
Epoch 30/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.36s
Val loss: 0.6861 score: 0.7209 time: 0.27s
Test loss: 0.6829 score: 0.7727 time: 0.29s
Epoch 31/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.38s
Val loss: 0.6853 score: 0.6744 time: 0.26s
Test loss: 0.6818 score: 0.7727 time: 0.30s
Epoch 32/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.43s
Val loss: 0.6843 score: 0.6744 time: 0.25s
Test loss: 0.6806 score: 0.7727 time: 0.30s
Epoch 33/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.33s
Val loss: 0.6833 score: 0.6744 time: 0.27s
Test loss: 0.6793 score: 0.7955 time: 0.29s
Epoch 34/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.50s
Val loss: 0.6822 score: 0.6744 time: 0.28s
Test loss: 0.6779 score: 0.7727 time: 0.29s
Epoch 35/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.38s
Val loss: 0.6809 score: 0.6744 time: 0.26s
Test loss: 0.6765 score: 0.7273 time: 0.29s
Epoch 36/1000, LR 0.000270
Train loss: 0.6760;  Loss pred: 0.6760; Loss self: 0.0000; time: 0.41s
Val loss: 0.6796 score: 0.6977 time: 0.26s
Test loss: 0.6748 score: 0.7273 time: 0.31s
Epoch 37/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.36s
Val loss: 0.6781 score: 0.6977 time: 0.27s
Test loss: 0.6731 score: 0.7273 time: 0.29s
Epoch 38/1000, LR 0.000270
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 0.37s
Val loss: 0.6766 score: 0.6977 time: 0.27s
Test loss: 0.6713 score: 0.7273 time: 0.28s
Epoch 39/1000, LR 0.000269
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.38s
Val loss: 0.6748 score: 0.7209 time: 0.37s
Test loss: 0.6693 score: 0.7273 time: 0.29s
Epoch 40/1000, LR 0.000269
Train loss: 0.6678;  Loss pred: 0.6678; Loss self: 0.0000; time: 0.41s
Val loss: 0.6730 score: 0.7209 time: 0.24s
Test loss: 0.6671 score: 0.7273 time: 0.30s
Epoch 41/1000, LR 0.000269
Train loss: 0.6653;  Loss pred: 0.6653; Loss self: 0.0000; time: 0.35s
Val loss: 0.6709 score: 0.7209 time: 0.26s
Test loss: 0.6648 score: 0.7500 time: 0.28s
Epoch 42/1000, LR 0.000269
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.38s
Val loss: 0.6687 score: 0.7209 time: 0.27s
Test loss: 0.6623 score: 0.7500 time: 0.28s
Epoch 43/1000, LR 0.000269
Train loss: 0.6602;  Loss pred: 0.6602; Loss self: 0.0000; time: 0.38s
Val loss: 0.6662 score: 0.7209 time: 0.25s
Test loss: 0.6596 score: 0.7500 time: 0.30s
Epoch 44/1000, LR 0.000269
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.39s
Val loss: 0.6636 score: 0.7209 time: 0.38s
Test loss: 0.6566 score: 0.7727 time: 0.31s
Epoch 45/1000, LR 0.000269
Train loss: 0.6542;  Loss pred: 0.6542; Loss self: 0.0000; time: 0.37s
Val loss: 0.6607 score: 0.7209 time: 0.26s
Test loss: 0.6535 score: 0.7955 time: 0.29s
Epoch 46/1000, LR 0.000269
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 0.36s
Val loss: 0.6576 score: 0.7907 time: 0.27s
Test loss: 0.6502 score: 0.8182 time: 0.29s
Epoch 47/1000, LR 0.000269
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.41s
Val loss: 0.6542 score: 0.8140 time: 0.26s
Test loss: 0.6466 score: 0.8182 time: 0.30s
Epoch 48/1000, LR 0.000269
Train loss: 0.6397;  Loss pred: 0.6397; Loss self: 0.0000; time: 0.42s
Val loss: 0.6505 score: 0.8372 time: 0.26s
Test loss: 0.6427 score: 0.8182 time: 0.31s
Epoch 49/1000, LR 0.000269
Train loss: 0.6344;  Loss pred: 0.6344; Loss self: 0.0000; time: 0.33s
Val loss: 0.6465 score: 0.8372 time: 0.27s
Test loss: 0.6385 score: 0.8409 time: 0.28s
Epoch 50/1000, LR 0.000269
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.38s
Val loss: 0.6422 score: 0.8372 time: 0.28s
Test loss: 0.6340 score: 0.8409 time: 0.30s
Epoch 51/1000, LR 0.000269
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.59s
Val loss: 0.6375 score: 0.8372 time: 0.26s
Test loss: 0.6292 score: 0.8636 time: 0.29s
Epoch 52/1000, LR 0.000269
Train loss: 0.6212;  Loss pred: 0.6212; Loss self: 0.0000; time: 0.38s
Val loss: 0.6325 score: 0.8372 time: 0.24s
Test loss: 0.6242 score: 0.8864 time: 0.30s
Epoch 53/1000, LR 0.000269
Train loss: 0.6130;  Loss pred: 0.6130; Loss self: 0.0000; time: 0.34s
Val loss: 0.6270 score: 0.8372 time: 0.26s
Test loss: 0.6187 score: 0.8864 time: 0.28s
Epoch 54/1000, LR 0.000269
Train loss: 0.6108;  Loss pred: 0.6108; Loss self: 0.0000; time: 0.37s
Val loss: 0.6212 score: 0.8372 time: 0.26s
Test loss: 0.6129 score: 0.8864 time: 0.29s
Epoch 55/1000, LR 0.000269
Train loss: 0.6026;  Loss pred: 0.6026; Loss self: 0.0000; time: 0.40s
Val loss: 0.6149 score: 0.8605 time: 0.24s
Test loss: 0.6068 score: 0.8864 time: 0.33s
Epoch 56/1000, LR 0.000269
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.38s
Val loss: 0.6081 score: 0.8605 time: 0.25s
Test loss: 0.6002 score: 0.8864 time: 0.28s
Epoch 57/1000, LR 0.000269
Train loss: 0.5824;  Loss pred: 0.5824; Loss self: 0.0000; time: 0.35s
Val loss: 0.6010 score: 0.8605 time: 0.27s
Test loss: 0.5934 score: 0.8864 time: 0.37s
Epoch 58/1000, LR 0.000269
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.40s
Val loss: 0.5934 score: 0.8605 time: 0.28s
Test loss: 0.5860 score: 0.8864 time: 0.29s
Epoch 59/1000, LR 0.000268
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 0.38s
Val loss: 0.5856 score: 0.8605 time: 0.25s
Test loss: 0.5784 score: 0.8864 time: 0.37s
Epoch 60/1000, LR 0.000268
Train loss: 0.5530;  Loss pred: 0.5530; Loss self: 0.0000; time: 0.43s
Val loss: 0.5773 score: 0.8605 time: 0.26s
Test loss: 0.5703 score: 0.8864 time: 0.30s
Epoch 61/1000, LR 0.000268
Train loss: 0.5468;  Loss pred: 0.5468; Loss self: 0.0000; time: 0.35s
Val loss: 0.5686 score: 0.8605 time: 0.28s
Test loss: 0.5620 score: 0.8864 time: 0.28s
Epoch 62/1000, LR 0.000268
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.39s
Val loss: 0.5596 score: 0.8605 time: 0.29s
Test loss: 0.5536 score: 0.8864 time: 0.30s
Epoch 63/1000, LR 0.000268
Train loss: 0.5263;  Loss pred: 0.5263; Loss self: 0.0000; time: 0.38s
Val loss: 0.5501 score: 0.8605 time: 0.24s
Test loss: 0.5448 score: 0.8864 time: 0.37s
Epoch 64/1000, LR 0.000268
Train loss: 0.5097;  Loss pred: 0.5097; Loss self: 0.0000; time: 0.43s
Val loss: 0.5403 score: 0.8605 time: 0.27s
Test loss: 0.5358 score: 0.8864 time: 0.30s
Epoch 65/1000, LR 0.000268
Train loss: 0.5031;  Loss pred: 0.5031; Loss self: 0.0000; time: 0.34s
Val loss: 0.5302 score: 0.8605 time: 0.32s
Test loss: 0.5264 score: 0.8864 time: 0.30s
Epoch 66/1000, LR 0.000268
Train loss: 0.4925;  Loss pred: 0.4925; Loss self: 0.0000; time: 0.36s
Val loss: 0.5199 score: 0.8605 time: 0.27s
Test loss: 0.5168 score: 0.8864 time: 0.29s
Epoch 67/1000, LR 0.000268
Train loss: 0.4841;  Loss pred: 0.4841; Loss self: 0.0000; time: 0.36s
Val loss: 0.5097 score: 0.8605 time: 0.24s
Test loss: 0.5075 score: 0.8864 time: 0.29s
Epoch 68/1000, LR 0.000268
Train loss: 0.4628;  Loss pred: 0.4628; Loss self: 0.0000; time: 0.37s
Val loss: 0.4991 score: 0.8605 time: 0.26s
Test loss: 0.4979 score: 0.8864 time: 0.40s
Epoch 69/1000, LR 0.000268
Train loss: 0.4578;  Loss pred: 0.4578; Loss self: 0.0000; time: 0.42s
Val loss: 0.4884 score: 0.8605 time: 0.26s
Test loss: 0.4883 score: 0.8864 time: 0.28s
Epoch 70/1000, LR 0.000268
Train loss: 0.4207;  Loss pred: 0.4207; Loss self: 0.0000; time: 0.37s
Val loss: 0.4775 score: 0.8605 time: 0.26s
Test loss: 0.4789 score: 0.8636 time: 0.29s
Epoch 71/1000, LR 0.000268
Train loss: 0.4268;  Loss pred: 0.4268; Loss self: 0.0000; time: 0.38s
Val loss: 0.4664 score: 0.8605 time: 0.24s
Test loss: 0.4688 score: 0.8636 time: 0.30s
Epoch 72/1000, LR 0.000267
Train loss: 0.4082;  Loss pred: 0.4082; Loss self: 0.0000; time: 0.36s
Val loss: 0.4552 score: 0.8605 time: 0.27s
Test loss: 0.4587 score: 0.8864 time: 0.29s
Epoch 73/1000, LR 0.000267
Train loss: 0.4098;  Loss pred: 0.4098; Loss self: 0.0000; time: 0.36s
Val loss: 0.4443 score: 0.8605 time: 0.26s
Test loss: 0.4486 score: 0.8864 time: 0.29s
Epoch 74/1000, LR 0.000267
Train loss: 0.3722;  Loss pred: 0.3722; Loss self: 0.0000; time: 0.37s
Val loss: 0.4334 score: 0.8605 time: 0.33s
Test loss: 0.4395 score: 0.8864 time: 0.29s
Epoch 75/1000, LR 0.000267
Train loss: 0.3736;  Loss pred: 0.3736; Loss self: 0.0000; time: 0.40s
Val loss: 0.4228 score: 0.8605 time: 0.24s
Test loss: 0.4306 score: 0.8864 time: 0.30s
Epoch 76/1000, LR 0.000267
Train loss: 0.3473;  Loss pred: 0.3473; Loss self: 0.0000; time: 0.34s
Val loss: 0.4123 score: 0.8605 time: 0.26s
Test loss: 0.4209 score: 0.8864 time: 0.28s
Epoch 77/1000, LR 0.000267
Train loss: 0.3356;  Loss pred: 0.3356; Loss self: 0.0000; time: 0.36s
Val loss: 0.4020 score: 0.8605 time: 0.27s
Test loss: 0.4121 score: 0.8864 time: 0.29s
Epoch 78/1000, LR 0.000267
Train loss: 0.3236;  Loss pred: 0.3236; Loss self: 0.0000; time: 0.39s
Val loss: 0.3922 score: 0.8837 time: 0.24s
Test loss: 0.4035 score: 0.8864 time: 0.32s
Epoch 79/1000, LR 0.000267
Train loss: 0.3008;  Loss pred: 0.3008; Loss self: 0.0000; time: 0.48s
Val loss: 0.3827 score: 0.9070 time: 0.25s
Test loss: 0.3969 score: 0.8864 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.2836;  Loss pred: 0.2836; Loss self: 0.0000; time: 0.33s
Val loss: 0.3737 score: 0.9070 time: 0.27s
Test loss: 0.3915 score: 0.8864 time: 0.28s
Epoch 81/1000, LR 0.000267
Train loss: 0.2833;  Loss pred: 0.2833; Loss self: 0.0000; time: 0.37s
Val loss: 0.3652 score: 0.9070 time: 0.24s
Test loss: 0.3848 score: 0.8864 time: 0.30s
Epoch 82/1000, LR 0.000267
Train loss: 0.2546;  Loss pred: 0.2546; Loss self: 0.0000; time: 0.49s
Val loss: 0.3573 score: 0.9070 time: 0.25s
Test loss: 0.3806 score: 0.8864 time: 0.30s
Epoch 83/1000, LR 0.000266
Train loss: 0.2444;  Loss pred: 0.2444; Loss self: 0.0000; time: 0.32s
Val loss: 0.3500 score: 0.9070 time: 0.27s
Test loss: 0.3769 score: 0.8864 time: 0.38s
Epoch 84/1000, LR 0.000266
Train loss: 0.2461;  Loss pred: 0.2461; Loss self: 0.0000; time: 0.37s
Val loss: 0.3434 score: 0.9070 time: 0.26s
Test loss: 0.3721 score: 0.8864 time: 0.29s
Epoch 85/1000, LR 0.000266
Train loss: 0.2373;  Loss pred: 0.2373; Loss self: 0.0000; time: 0.38s
Val loss: 0.3377 score: 0.9070 time: 0.24s
Test loss: 0.3667 score: 0.8864 time: 0.32s
Epoch 86/1000, LR 0.000266
Train loss: 0.2349;  Loss pred: 0.2349; Loss self: 0.0000; time: 0.34s
Val loss: 0.3328 score: 0.9070 time: 0.26s
Test loss: 0.3614 score: 0.8864 time: 0.28s
Epoch 87/1000, LR 0.000266
Train loss: 0.2061;  Loss pred: 0.2061; Loss self: 0.0000; time: 0.38s
Val loss: 0.3286 score: 0.9070 time: 0.26s
Test loss: 0.3579 score: 0.8864 time: 0.29s
Epoch 88/1000, LR 0.000266
Train loss: 0.1965;  Loss pred: 0.1965; Loss self: 0.0000; time: 0.41s
Val loss: 0.3256 score: 0.9070 time: 0.24s
Test loss: 0.3528 score: 0.8864 time: 0.31s
Epoch 89/1000, LR 0.000266
Train loss: 0.1842;  Loss pred: 0.1842; Loss self: 0.0000; time: 0.35s
Val loss: 0.3236 score: 0.9070 time: 0.27s
Test loss: 0.3487 score: 0.8864 time: 0.28s
Epoch 90/1000, LR 0.000266
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.39s
Val loss: 0.3216 score: 0.8837 time: 0.27s
Test loss: 0.3490 score: 0.8864 time: 0.38s
Epoch 91/1000, LR 0.000266
Train loss: 0.1381;  Loss pred: 0.1381; Loss self: 0.0000; time: 0.37s
Val loss: 0.3201 score: 0.8605 time: 0.25s
Test loss: 0.3531 score: 0.8864 time: 0.26s
Epoch 92/1000, LR 0.000266
Train loss: 0.1641;  Loss pred: 0.1641; Loss self: 0.0000; time: 0.32s
Val loss: 0.3200 score: 0.8605 time: 0.24s
Test loss: 0.3584 score: 0.8636 time: 0.28s
Epoch 93/1000, LR 0.000265
Train loss: 0.1257;  Loss pred: 0.1257; Loss self: 0.0000; time: 0.32s
Val loss: 0.3211 score: 0.8605 time: 0.23s
Test loss: 0.3644 score: 0.8636 time: 0.28s
     INFO: Early stopping counter 1 of 2
Epoch 94/1000, LR 0.000265
Train loss: 0.1211;  Loss pred: 0.1211; Loss self: 0.0000; time: 0.43s
Val loss: 0.3233 score: 0.8605 time: 0.24s
Test loss: 0.3723 score: 0.8636 time: 0.28s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 091,   Train_Loss: 0.1641,   Val_Loss: 0.3200,   Val_Precision: 0.8636,   Val_Recall: 0.8636,   Val_accuracy: 0.8636,   Val_Score: 0.8605,   Val_Loss: 0.3200,   Test_Precision: 0.8636,   Test_Recall: 0.8636,   Test_accuracy: 0.8636,   Test_Score: 0.8636,   Test_loss: 0.3584


[0.3029502850258723, 0.2876396829960868, 0.304128899006173, 0.31033104192465544, 0.42547445197124034, 0.3021622379310429, 0.29891157790552825, 0.2975473899859935, 0.3259642239427194, 0.416392654995434, 0.2850428440142423, 0.2978367171017453, 0.2928726620739326, 0.3349638710496947, 0.42805258405860513, 0.2875070059671998, 0.29873804398812354, 0.3013893309980631, 0.3156214809278026, 0.2898801430128515, 0.2948170639574528, 0.4922329419059679, 0.4484705409267917, 0.30928799707908183, 0.29047588096000254, 0.29346120497211814, 0.29282217798754573, 0.41942866099998355, 0.2991079770727083, 0.29753297299612314, 0.30416760803200305, 0.3056240259902552, 0.2907377310330048, 0.2966747839236632, 0.2980822160607204, 0.315154833951965, 0.29478957294486463, 0.2856602050596848, 0.29594726697541773, 0.30737587600015104, 0.28874787501990795, 0.28601291100494564, 0.30415513401385397, 0.31472878402564675, 0.2987792060011998, 0.2934072690550238, 0.30247484298888594, 0.3165629480499774, 0.2819108589319512, 0.30526562698651105, 0.29613089596387, 0.30321203998755664, 0.2811391110299155, 0.29760031297337264, 0.33164347999263555, 0.28688512404914945, 0.3743415790377185, 0.29239206993952394, 0.3741739368997514, 0.30708283092826605, 0.2798166039865464, 0.3086141800740734, 0.3761415350018069, 0.307702076039277, 0.3071471869479865, 0.2941260989755392, 0.2945724290329963, 0.40916355699300766, 0.282626096974127, 0.2941741900285706, 0.2991974810138345, 0.2943368839332834, 0.29249069397337735, 0.29242929304018617, 0.30809090402908623, 0.28247333492618054, 0.29357187496498227, 0.3231481109978631, 0.29975262098014355, 0.28673828893806785, 0.3073013140819967, 0.30200097302440554, 0.38430074299685657, 0.2947712269378826, 0.32206395093817264, 0.2797587619861588, 0.2939245000015944, 0.3102679819567129, 0.28031246399041265, 0.38355212297756225, 0.2679200100246817, 0.28315915004350245, 0.2879522569710389, 0.28525411197915673]
[0.006885233750588007, 0.006537265522638336, 0.006912020431958477, 0.0070529782255603505, 0.009669873908437281, 0.006867323589341884, 0.006793444952398369, 0.006762440681499852, 0.007408277816879986, 0.00946346943171441, 0.006478246454869143, 0.006769016297766939, 0.00665619686531665, 0.007612815251129426, 0.009728467819513753, 0.006534250135618177, 0.00678950099973008, 0.006849757522683252, 0.007173215475631878, 0.006588185068473897, 0.006700387817214836, 0.011187112316044724, 0.010192512293790722, 0.007029272660888223, 0.006601724567272785, 0.006669572840275412, 0.006655049499716948, 0.009532469568181445, 0.00679790856983428, 0.006762113022639162, 0.0069129001825455234, 0.006946000590687618, 0.0066076757052955645, 0.0067426087255378, 0.006774595819561827, 0.007162609862544658, 0.006699763021474196, 0.00649227738772011, 0.0067260742494413125, 0.0069858153636397965, 0.006562451704997908, 0.006500293431930582, 0.006912616682133044, 0.00715292690967379, 0.006790436500027267, 0.0066683470239778135, 0.006874428249747408, 0.007194612455681305, 0.0064070649757261644, 0.006937855158784342, 0.0067302476355425, 0.006891182726989923, 0.006389525250679898, 0.00676364347666756, 0.007537351818014445, 0.006520116455662487, 0.008507763159948147, 0.006645274316807362, 0.008503953111357987, 0.006979155248369683, 0.006359468272421509, 0.007013958638047122, 0.008548671250041067, 0.006993229000892659, 0.006980617885181511, 0.006684684067625891, 0.006694827932568098, 0.009299171749841083, 0.0064233203857756134, 0.006685777046103877, 0.00679994275031442, 0.00668947463484735, 0.006647515772122212, 0.006646120296367867, 0.007002066000661051, 0.006419848521049557, 0.006672088067385961, 0.0073442752499514345, 0.0068125595677305355, 0.006516779294046996, 0.006984120774590833, 0.006863658477827398, 0.008734107795383104, 0.006699346066770059, 0.007319635248594833, 0.006358153681503609, 0.00668010227276351, 0.007051545044470748, 0.006370737817963924, 0.008717093704035506, 0.006089091136924584, 0.006435435228261419, 0.00654436947661452, 0.0064830479995262895]
[145.23835155409196, 152.96915759915714, 144.675498263343, 141.7840758923584, 103.4139648012853, 145.61713700982494, 147.20072172616315, 147.87560395697378, 134.98413865115447, 105.66949121732824, 154.36275957800058, 147.73195336077038, 150.23594106879347, 131.3574501695206, 102.79110940719356, 153.0397488992659, 147.28622914110412, 145.99056925569397, 139.4074949228974, 151.7868714382734, 149.24509256475727, 89.38857247064419, 98.11123805160382, 142.26222942867597, 151.47557124048672, 149.9346395861097, 150.26184253663806, 104.90460974959869, 147.10406733587152, 147.88276928410662, 144.65708654739637, 143.96773898071396, 151.33914625964073, 148.31054873649643, 147.6102820942429, 139.61391436790197, 149.25901062392546, 154.02915499135344, 148.6751354377426, 143.1472130232445, 152.38207379696348, 153.83920902521484, 144.66301922753618, 139.8029104208483, 147.26593791076382, 149.96220148775015, 145.46664299489106, 138.99289310716645, 156.0777054374514, 144.13676519808192, 148.58294287999016, 145.1129711135669, 156.50615042073616, 147.84930687870894, 132.67259166674123, 153.37149371489153, 117.5397082875653, 150.48287735402883, 117.59236991375074, 143.28381650968404, 157.24585093640664, 142.57283961948588, 116.97724368511612, 142.9954603048683, 143.25379449902348, 149.59570114061597, 149.36903682547756, 107.5364588267863, 155.68272169865466, 149.57124551180598, 147.06006163857208, 149.4885704163851, 150.43213649732357, 150.46372250386503, 142.81499201886874, 155.76691517271402, 149.87811759981568, 136.1604741062248, 146.7877073305547, 153.45003334906377, 143.18194548383727, 145.6948948189125, 114.49366362624988, 149.26830022413333, 136.61882949590586, 157.27836257073844, 149.69830687731482, 141.81289259211627, 156.96769017557816, 114.71713325016323, 164.22812165446908, 155.38964569303224, 152.80310862236217, 154.2484337726744]
Elapsed: 0.31256126078028984~0.041162526656660633
Time per graph: 0.00710366501773386~0.0009355119694695598
Speed: 142.69878862882757~14.768303076117936
Total Time: 0.2858
best val loss: 0.31998497247695923 test_score: 0.8636

Testing...
Test loss: 0.3969 score: 0.8864 time: 0.28s
test Score 0.8864
Epoch Time List: [0.9506891799392179, 0.9376925469841808, 0.9916284169303253, 1.0019747042097151, 1.0723279350204393, 0.9293966119876131, 0.9475779830245301, 0.9655663911253214, 0.9614449680084363, 1.0240915280301124, 0.8942742899525911, 0.934016571030952, 1.0546305241296068, 0.9574806008022279, 1.0537259500706568, 0.8861662191338837, 0.9437019589822739, 1.0159149959217757, 0.9494206979870796, 0.8982097601983696, 0.9391746511682868, 1.1454832229064777, 1.0862312910612673, 0.9281877200119197, 0.9409670418826863, 0.9725973650347441, 0.9046103889122605, 1.06848587107379, 0.9176264851121232, 0.9221525969915092, 0.9380338110495359, 0.9866385679924861, 0.8806027569808066, 1.0659381888108328, 0.9407015610486269, 0.9741168030304834, 0.9207245991565287, 0.9156083188718185, 1.034397127921693, 0.9536616870900616, 0.8991695288568735, 0.9241292158840224, 0.9288279019529, 1.0827293259790167, 0.9236235078424215, 0.9263238019775599, 0.973009602050297, 0.9932471310021356, 0.8768088800134137, 0.9556978701148182, 1.1475144969299436, 0.9176663840189576, 0.8760489509440958, 0.923590298043564, 0.9647817779332399, 0.9066832349635661, 0.9860020389314741, 0.9750531648751348, 0.9955970339942724, 0.9913032818585634, 0.9055275210412219, 0.983195836073719, 0.9917594090802595, 1.009148359997198, 0.957982983905822, 0.9249326090794057, 0.8878631939878687, 1.0326691890368238, 0.9569697399856523, 0.920689285849221, 0.9159899080405012, 0.9190136938123032, 0.9140455160522833, 0.9933355211978778, 0.9528412299696356, 0.8778929160907865, 0.9195232960628346, 0.948364706011489, 1.034568462986499, 0.8798282169736922, 0.9182588618714362, 1.031822606921196, 0.9695370519766584, 0.9166886569000781, 0.9445694129681215, 0.8695876070996746, 0.9239599200664088, 0.9581652330234647, 0.8956631199689582, 1.0404320800444111, 0.883341277949512, 0.8442537899827585, 0.8281762121478096, 0.9585216050036252]
Total Epoch List: [94]
Total Time List: [0.285769677022472]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fce2c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.26s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.27s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.27s
Epoch 6/1000, LR 0.000120
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.27s
Epoch 9/1000, LR 0.000210
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.26s
Epoch 10/1000, LR 0.000240
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.26s
Epoch 13/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.28s
Epoch 14/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.27s
Epoch 15/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.26s
Epoch 16/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.27s
Epoch 18/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.39s
Epoch 19/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.27s
Epoch 20/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.39s
Val loss: 0.6899 score: 0.5227 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.43s
Val loss: 0.6894 score: 0.5682 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.36s
Epoch 23/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.37s
Val loss: 0.6889 score: 0.5909 time: 0.27s
Test loss: 0.6913 score: 0.5116 time: 0.34s
Epoch 24/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.42s
Val loss: 0.6883 score: 0.6364 time: 0.29s
Test loss: 0.6909 score: 0.6047 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.42s
Val loss: 0.6877 score: 0.6591 time: 0.36s
Test loss: 0.6905 score: 0.5814 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.46s
Val loss: 0.6870 score: 0.6818 time: 0.27s
Test loss: 0.6900 score: 0.5116 time: 0.27s
Epoch 27/1000, LR 0.000270
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.44s
Val loss: 0.6863 score: 0.6818 time: 0.25s
Test loss: 0.6895 score: 0.5116 time: 0.29s
Epoch 28/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.39s
Val loss: 0.6855 score: 0.7045 time: 0.27s
Test loss: 0.6890 score: 0.6047 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.52s
Val loss: 0.6846 score: 0.7045 time: 0.33s
Test loss: 0.6884 score: 0.6047 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 0.42s
Val loss: 0.6837 score: 0.7045 time: 0.27s
Test loss: 0.6878 score: 0.6512 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.53s
Val loss: 0.6827 score: 0.6591 time: 0.25s
Test loss: 0.6872 score: 0.6744 time: 0.27s
Epoch 32/1000, LR 0.000270
Train loss: 0.6779;  Loss pred: 0.6779; Loss self: 0.0000; time: 0.42s
Val loss: 0.6817 score: 0.6818 time: 0.27s
Test loss: 0.6864 score: 0.6744 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.40s
Val loss: 0.6806 score: 0.6591 time: 0.29s
Test loss: 0.6857 score: 0.6047 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6746;  Loss pred: 0.6746; Loss self: 0.0000; time: 0.42s
Val loss: 0.6795 score: 0.6364 time: 0.38s
Test loss: 0.6848 score: 0.6047 time: 0.27s
Epoch 35/1000, LR 0.000270
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.42s
Val loss: 0.6783 score: 0.6136 time: 0.25s
Test loss: 0.6840 score: 0.5814 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.40s
Val loss: 0.6771 score: 0.6136 time: 0.26s
Test loss: 0.6831 score: 0.5814 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.40s
Val loss: 0.6758 score: 0.6364 time: 0.27s
Test loss: 0.6822 score: 0.6047 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.39s
Val loss: 0.6743 score: 0.6364 time: 0.25s
Test loss: 0.6812 score: 0.6047 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6633;  Loss pred: 0.6633; Loss self: 0.0000; time: 0.42s
Val loss: 0.6727 score: 0.6364 time: 0.34s
Test loss: 0.6801 score: 0.6512 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.36s
Val loss: 0.6710 score: 0.6591 time: 0.26s
Test loss: 0.6790 score: 0.6279 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.43s
Val loss: 0.6691 score: 0.6364 time: 0.27s
Test loss: 0.6777 score: 0.6512 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.42s
Val loss: 0.6671 score: 0.6591 time: 0.24s
Test loss: 0.6764 score: 0.6744 time: 0.27s
Epoch 43/1000, LR 0.000269
Train loss: 0.6505;  Loss pred: 0.6505; Loss self: 0.0000; time: 0.36s
Val loss: 0.6649 score: 0.7045 time: 0.26s
Test loss: 0.6749 score: 0.6744 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6484;  Loss pred: 0.6484; Loss self: 0.0000; time: 0.41s
Val loss: 0.6625 score: 0.7500 time: 0.36s
Test loss: 0.6734 score: 0.6977 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.43s
Val loss: 0.6599 score: 0.7727 time: 0.26s
Test loss: 0.6716 score: 0.6744 time: 0.27s
Epoch 46/1000, LR 0.000269
Train loss: 0.6382;  Loss pred: 0.6382; Loss self: 0.0000; time: 0.41s
Val loss: 0.6572 score: 0.7727 time: 0.25s
Test loss: 0.6698 score: 0.6977 time: 0.38s
Epoch 47/1000, LR 0.000269
Train loss: 0.6353;  Loss pred: 0.6353; Loss self: 0.0000; time: 0.37s
Val loss: 0.6542 score: 0.8409 time: 0.26s
Test loss: 0.6678 score: 0.6512 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.41s
Val loss: 0.6511 score: 0.7955 time: 0.27s
Test loss: 0.6656 score: 0.6512 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.42s
Val loss: 0.6477 score: 0.8636 time: 0.33s
Test loss: 0.6633 score: 0.6279 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 0.6185;  Loss pred: 0.6185; Loss self: 0.0000; time: 0.46s
Val loss: 0.6441 score: 0.8636 time: 0.26s
Test loss: 0.6608 score: 0.6512 time: 0.34s
Epoch 51/1000, LR 0.000269
Train loss: 0.6136;  Loss pred: 0.6136; Loss self: 0.0000; time: 0.41s
Val loss: 0.6402 score: 0.8636 time: 0.25s
Test loss: 0.6581 score: 0.6512 time: 0.26s
Epoch 52/1000, LR 0.000269
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.39s
Val loss: 0.6360 score: 0.8636 time: 0.28s
Test loss: 0.6551 score: 0.6744 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.5968;  Loss pred: 0.5968; Loss self: 0.0000; time: 0.41s
Val loss: 0.6315 score: 0.8636 time: 0.26s
Test loss: 0.6520 score: 0.6977 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.5916;  Loss pred: 0.5916; Loss self: 0.0000; time: 0.47s
Val loss: 0.6267 score: 0.8636 time: 0.36s
Test loss: 0.6486 score: 0.6977 time: 0.26s
Epoch 55/1000, LR 0.000269
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.42s
Val loss: 0.6216 score: 0.8636 time: 0.25s
Test loss: 0.6450 score: 0.6977 time: 0.40s
Epoch 56/1000, LR 0.000269
Train loss: 0.5707;  Loss pred: 0.5707; Loss self: 0.0000; time: 0.37s
Val loss: 0.6162 score: 0.8636 time: 0.26s
Test loss: 0.6411 score: 0.6977 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.5611;  Loss pred: 0.5611; Loss self: 0.0000; time: 0.41s
Val loss: 0.6104 score: 0.8636 time: 0.41s
Test loss: 0.6369 score: 0.7209 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.42s
Val loss: 0.6043 score: 0.8636 time: 0.32s
Test loss: 0.6325 score: 0.7442 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5426;  Loss pred: 0.5426; Loss self: 0.0000; time: 0.54s
Val loss: 0.5979 score: 0.8409 time: 0.43s
Test loss: 0.6279 score: 0.7442 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.47s
Val loss: 0.5913 score: 0.8409 time: 0.26s
Test loss: 0.6230 score: 0.7442 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.41s
Val loss: 0.5844 score: 0.8409 time: 0.24s
Test loss: 0.6181 score: 0.7674 time: 0.27s
Epoch 62/1000, LR 0.000268
Train loss: 0.5087;  Loss pred: 0.5087; Loss self: 0.0000; time: 0.36s
Val loss: 0.5773 score: 0.8182 time: 0.27s
Test loss: 0.6132 score: 0.7674 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.4955;  Loss pred: 0.4955; Loss self: 0.0000; time: 0.40s
Val loss: 0.5701 score: 0.8182 time: 0.28s
Test loss: 0.6081 score: 0.7674 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.4803;  Loss pred: 0.4803; Loss self: 0.0000; time: 0.49s
Val loss: 0.5626 score: 0.8182 time: 0.36s
Test loss: 0.6027 score: 0.7674 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.4575;  Loss pred: 0.4575; Loss self: 0.0000; time: 0.42s
Val loss: 0.5546 score: 0.8182 time: 0.24s
Test loss: 0.5968 score: 0.7674 time: 0.32s
Epoch 66/1000, LR 0.000268
Train loss: 0.4561;  Loss pred: 0.4561; Loss self: 0.0000; time: 0.42s
Val loss: 0.5464 score: 0.8182 time: 0.26s
Test loss: 0.5905 score: 0.7674 time: 0.26s
Epoch 67/1000, LR 0.000268
Train loss: 0.4409;  Loss pred: 0.4409; Loss self: 0.0000; time: 0.39s
Val loss: 0.5381 score: 0.8182 time: 0.33s
Test loss: 0.5839 score: 0.7674 time: 0.24s
Epoch 68/1000, LR 0.000268
Train loss: 0.4213;  Loss pred: 0.4213; Loss self: 0.0000; time: 0.41s
Val loss: 0.5293 score: 0.8182 time: 0.26s
Test loss: 0.5770 score: 0.7674 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4023;  Loss pred: 0.4023; Loss self: 0.0000; time: 0.42s
Val loss: 0.5204 score: 0.7955 time: 0.33s
Test loss: 0.5695 score: 0.7907 time: 0.33s
Epoch 70/1000, LR 0.000268
Train loss: 0.3909;  Loss pred: 0.3909; Loss self: 0.0000; time: 0.42s
Val loss: 0.5116 score: 0.8182 time: 0.25s
Test loss: 0.5621 score: 0.7907 time: 0.27s
Epoch 71/1000, LR 0.000268
Train loss: 0.3761;  Loss pred: 0.3761; Loss self: 0.0000; time: 0.39s
Val loss: 0.5031 score: 0.8182 time: 0.26s
Test loss: 0.5545 score: 0.7907 time: 0.27s
Epoch 72/1000, LR 0.000267
Train loss: 0.3722;  Loss pred: 0.3722; Loss self: 0.0000; time: 0.41s
Val loss: 0.4948 score: 0.8182 time: 0.27s
Test loss: 0.5475 score: 0.8140 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.3503;  Loss pred: 0.3503; Loss self: 0.0000; time: 0.40s
Val loss: 0.4867 score: 0.8182 time: 0.26s
Test loss: 0.5407 score: 0.8140 time: 0.26s
Epoch 74/1000, LR 0.000267
Train loss: 0.3258;  Loss pred: 0.3258; Loss self: 0.0000; time: 0.42s
Val loss: 0.4789 score: 0.8182 time: 0.37s
Test loss: 0.5342 score: 0.8140 time: 0.27s
Epoch 75/1000, LR 0.000267
Train loss: 0.3182;  Loss pred: 0.3182; Loss self: 0.0000; time: 0.41s
Val loss: 0.4716 score: 0.7955 time: 0.27s
Test loss: 0.5278 score: 0.8140 time: 0.26s
Epoch 76/1000, LR 0.000267
Train loss: 0.3035;  Loss pred: 0.3035; Loss self: 0.0000; time: 0.40s
Val loss: 0.4647 score: 0.7955 time: 0.27s
Test loss: 0.5219 score: 0.8140 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.2925;  Loss pred: 0.2925; Loss self: 0.0000; time: 0.41s
Val loss: 0.4585 score: 0.7955 time: 0.27s
Test loss: 0.5160 score: 0.8140 time: 0.35s
Epoch 78/1000, LR 0.000267
Train loss: 0.2801;  Loss pred: 0.2801; Loss self: 0.0000; time: 0.45s
Val loss: 0.4529 score: 0.7955 time: 0.24s
Test loss: 0.5108 score: 0.8140 time: 0.27s
Epoch 79/1000, LR 0.000267
Train loss: 0.2597;  Loss pred: 0.2597; Loss self: 0.0000; time: 0.36s
Val loss: 0.4478 score: 0.7955 time: 0.33s
Test loss: 0.5058 score: 0.8140 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.2448;  Loss pred: 0.2448; Loss self: 0.0000; time: 0.39s
Val loss: 0.4433 score: 0.7955 time: 0.27s
Test loss: 0.5013 score: 0.8140 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.2272;  Loss pred: 0.2272; Loss self: 0.0000; time: 0.43s
Val loss: 0.4393 score: 0.7955 time: 0.26s
Test loss: 0.4973 score: 0.8140 time: 0.26s
Epoch 82/1000, LR 0.000267
Train loss: 0.2121;  Loss pred: 0.2121; Loss self: 0.0000; time: 0.42s
Val loss: 0.4362 score: 0.7955 time: 0.26s
Test loss: 0.4941 score: 0.8140 time: 0.26s
Epoch 83/1000, LR 0.000266
Train loss: 0.1961;  Loss pred: 0.1961; Loss self: 0.0000; time: 0.37s
Val loss: 0.4338 score: 0.7955 time: 0.28s
Test loss: 0.4918 score: 0.8140 time: 0.25s
Epoch 84/1000, LR 0.000266
Train loss: 0.1867;  Loss pred: 0.1867; Loss self: 0.0000; time: 0.42s
Val loss: 0.4330 score: 0.8182 time: 0.35s
Test loss: 0.4916 score: 0.8140 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.1861;  Loss pred: 0.1861; Loss self: 0.0000; time: 0.45s
Val loss: 0.4331 score: 0.8182 time: 0.25s
Test loss: 0.4928 score: 0.8140 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 86/1000, LR 0.000266
Train loss: 0.1774;  Loss pred: 0.1774; Loss self: 0.0000; time: 0.41s
Val loss: 0.4343 score: 0.8182 time: 0.25s
Test loss: 0.4950 score: 0.8140 time: 0.26s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 083,   Train_Loss: 0.1867,   Val_Loss: 0.4330,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4330,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4916


[0.3029502850258723, 0.2876396829960868, 0.304128899006173, 0.31033104192465544, 0.42547445197124034, 0.3021622379310429, 0.29891157790552825, 0.2975473899859935, 0.3259642239427194, 0.416392654995434, 0.2850428440142423, 0.2978367171017453, 0.2928726620739326, 0.3349638710496947, 0.42805258405860513, 0.2875070059671998, 0.29873804398812354, 0.3013893309980631, 0.3156214809278026, 0.2898801430128515, 0.2948170639574528, 0.4922329419059679, 0.4484705409267917, 0.30928799707908183, 0.29047588096000254, 0.29346120497211814, 0.29282217798754573, 0.41942866099998355, 0.2991079770727083, 0.29753297299612314, 0.30416760803200305, 0.3056240259902552, 0.2907377310330048, 0.2966747839236632, 0.2980822160607204, 0.315154833951965, 0.29478957294486463, 0.2856602050596848, 0.29594726697541773, 0.30737587600015104, 0.28874787501990795, 0.28601291100494564, 0.30415513401385397, 0.31472878402564675, 0.2987792060011998, 0.2934072690550238, 0.30247484298888594, 0.3165629480499774, 0.2819108589319512, 0.30526562698651105, 0.29613089596387, 0.30321203998755664, 0.2811391110299155, 0.29760031297337264, 0.33164347999263555, 0.28688512404914945, 0.3743415790377185, 0.29239206993952394, 0.3741739368997514, 0.30708283092826605, 0.2798166039865464, 0.3086141800740734, 0.3761415350018069, 0.307702076039277, 0.3071471869479865, 0.2941260989755392, 0.2945724290329963, 0.40916355699300766, 0.282626096974127, 0.2941741900285706, 0.2991974810138345, 0.2943368839332834, 0.29249069397337735, 0.29242929304018617, 0.30809090402908623, 0.28247333492618054, 0.29357187496498227, 0.3231481109978631, 0.29975262098014355, 0.28673828893806785, 0.3073013140819967, 0.30200097302440554, 0.38430074299685657, 0.2947712269378826, 0.32206395093817264, 0.2797587619861588, 0.2939245000015944, 0.3102679819567129, 0.28031246399041265, 0.38355212297756225, 0.2679200100246817, 0.28315915004350245, 0.2879522569710389, 0.28525411197915673, 0.22669296199455857, 0.224986603949219, 0.26687032904010266, 0.27635664492845535, 0.2729991429951042, 0.2448850319487974, 0.24381489399820566, 0.2746166540309787, 0.2685133380582556, 0.24908805009908974, 0.2557556920219213, 0.26585677301045507, 0.289259301032871, 0.2722853679442778, 0.2605551399756223, 0.255488493014127, 0.2789634370710701, 0.3940313550410792, 0.27239574992563576, 0.2545887219021097, 0.2583161690272391, 0.360246357973665, 0.3406761749647558, 0.2481761509552598, 0.2574336469406262, 0.27016996999736875, 0.2917936349986121, 0.24867082596756518, 0.24793208402115852, 0.2577837440185249, 0.2704603460151702, 0.25697939400561154, 0.24568698601797223, 0.2773065899964422, 0.2596143480623141, 0.258240032941103, 0.24350098508875817, 0.2603400810621679, 0.26373176102060825, 0.2457664159592241, 0.24707684305030853, 0.27579528000205755, 0.24274503404740244, 0.2474248759681359, 0.2742076040012762, 0.3870495680021122, 0.25012400397099555, 0.24692912900354713, 0.258586446987465, 0.3443039550911635, 0.2628253319999203, 0.2418423229828477, 0.25701176398433745, 0.2597413921030238, 0.40806537005119026, 0.2484028770122677, 0.24094587995205075, 0.2517034780466929, 0.24183891399297863, 0.2549323550192639, 0.27646025200374424, 0.2528859960148111, 0.24190502800047398, 0.23906909301877022, 0.3270769310183823, 0.2612674389965832, 0.243757382966578, 0.25450413196813315, 0.333453370956704, 0.2747772519942373, 0.27433049003593624, 0.2557203669566661, 0.2634301530197263, 0.2767341419821605, 0.26554796705022454, 0.2470284530427307, 0.3560750219039619, 0.27390799298882484, 0.30614030093420297, 0.24145184503868222, 0.25976177700795233, 0.26630522997584194, 0.24942886794451624, 0.2529650100041181, 0.2546210669679567, 0.2621787670068443]
[0.006885233750588007, 0.006537265522638336, 0.006912020431958477, 0.0070529782255603505, 0.009669873908437281, 0.006867323589341884, 0.006793444952398369, 0.006762440681499852, 0.007408277816879986, 0.00946346943171441, 0.006478246454869143, 0.006769016297766939, 0.00665619686531665, 0.007612815251129426, 0.009728467819513753, 0.006534250135618177, 0.00678950099973008, 0.006849757522683252, 0.007173215475631878, 0.006588185068473897, 0.006700387817214836, 0.011187112316044724, 0.010192512293790722, 0.007029272660888223, 0.006601724567272785, 0.006669572840275412, 0.006655049499716948, 0.009532469568181445, 0.00679790856983428, 0.006762113022639162, 0.0069129001825455234, 0.006946000590687618, 0.0066076757052955645, 0.0067426087255378, 0.006774595819561827, 0.007162609862544658, 0.006699763021474196, 0.00649227738772011, 0.0067260742494413125, 0.0069858153636397965, 0.006562451704997908, 0.006500293431930582, 0.006912616682133044, 0.00715292690967379, 0.006790436500027267, 0.0066683470239778135, 0.006874428249747408, 0.007194612455681305, 0.0064070649757261644, 0.006937855158784342, 0.0067302476355425, 0.006891182726989923, 0.006389525250679898, 0.00676364347666756, 0.007537351818014445, 0.006520116455662487, 0.008507763159948147, 0.006645274316807362, 0.008503953111357987, 0.006979155248369683, 0.006359468272421509, 0.007013958638047122, 0.008548671250041067, 0.006993229000892659, 0.006980617885181511, 0.006684684067625891, 0.006694827932568098, 0.009299171749841083, 0.0064233203857756134, 0.006685777046103877, 0.00679994275031442, 0.00668947463484735, 0.006647515772122212, 0.006646120296367867, 0.007002066000661051, 0.006419848521049557, 0.006672088067385961, 0.0073442752499514345, 0.0068125595677305355, 0.006516779294046996, 0.006984120774590833, 0.006863658477827398, 0.008734107795383104, 0.006699346066770059, 0.007319635248594833, 0.006358153681503609, 0.00668010227276351, 0.007051545044470748, 0.006370737817963924, 0.008717093704035506, 0.006089091136924584, 0.006435435228261419, 0.00654436947661452, 0.0064830479995262895, 0.005271929348710664, 0.005232246603470209, 0.006206286721862853, 0.006426898719266404, 0.006348817278955912, 0.005695000742995288, 0.005670113813911759, 0.0063864338146739225, 0.006244496233912922, 0.005792745351141622, 0.005947806791207472, 0.006182715651405932, 0.006726960489136535, 0.006332217859169252, 0.006059421859898193, 0.0059415928607936515, 0.006487521792350467, 0.009163519884676261, 0.006334784881991529, 0.005920667951211853, 0.006007352768075328, 0.008377822278457325, 0.007922701743366413, 0.005771538394308368, 0.005986828998619214, 0.006283022558078343, 0.006785898488339817, 0.005783042464361981, 0.00576586241909671, 0.005994970791128485, 0.006289775488724889, 0.005976264976874687, 0.005713650837627261, 0.006448990465033539, 0.006037542978193351, 0.006005582161421, 0.0056628136067153065, 0.006054420489817858, 0.006133296767921122, 0.005715498045563351, 0.0057459730941932215, 0.006413843720978083, 0.0056452333499395916, 0.005754066882979905, 0.006376921023285493, 0.009001152744235168, 0.005816837301651059, 0.005742537883803422, 0.0060136383020340705, 0.008007068723050315, 0.006112217023253961, 0.005624240069368551, 0.005977017767077615, 0.006040497490767996, 0.009489892326771866, 0.005776811093308552, 0.005603392557024436, 0.005853569256899835, 0.005624160790534387, 0.0059286594190526495, 0.006429308186133587, 0.0058810696747630484, 0.005625698325592418, 0.005559746349273726, 0.007606440256241449, 0.006075986953408912, 0.005668776348059954, 0.005918700743444957, 0.007754729557132652, 0.006390168651028774, 0.006379778838045028, 0.005946985278062002, 0.006126282628365728, 0.006435677720515361, 0.006175534117447082, 0.005744847745179783, 0.008280814462882835, 0.006369953325321508, 0.007119541882190767, 0.005615159186946098, 0.006040971558324473, 0.006193144883159115, 0.005800671347546889, 0.005882907209398095, 0.005921420162045505, 0.006097180628066146]
[145.23835155409196, 152.96915759915714, 144.675498263343, 141.7840758923584, 103.4139648012853, 145.61713700982494, 147.20072172616315, 147.87560395697378, 134.98413865115447, 105.66949121732824, 154.36275957800058, 147.73195336077038, 150.23594106879347, 131.3574501695206, 102.79110940719356, 153.0397488992659, 147.28622914110412, 145.99056925569397, 139.4074949228974, 151.7868714382734, 149.24509256475727, 89.38857247064419, 98.11123805160382, 142.26222942867597, 151.47557124048672, 149.9346395861097, 150.26184253663806, 104.90460974959869, 147.10406733587152, 147.88276928410662, 144.65708654739637, 143.96773898071396, 151.33914625964073, 148.31054873649643, 147.6102820942429, 139.61391436790197, 149.25901062392546, 154.02915499135344, 148.6751354377426, 143.1472130232445, 152.38207379696348, 153.83920902521484, 144.66301922753618, 139.8029104208483, 147.26593791076382, 149.96220148775015, 145.46664299489106, 138.99289310716645, 156.0777054374514, 144.13676519808192, 148.58294287999016, 145.1129711135669, 156.50615042073616, 147.84930687870894, 132.67259166674123, 153.37149371489153, 117.5397082875653, 150.48287735402883, 117.59236991375074, 143.28381650968404, 157.24585093640664, 142.57283961948588, 116.97724368511612, 142.9954603048683, 143.25379449902348, 149.59570114061597, 149.36903682547756, 107.5364588267863, 155.68272169865466, 149.57124551180598, 147.06006163857208, 149.4885704163851, 150.43213649732357, 150.46372250386503, 142.81499201886874, 155.76691517271402, 149.87811759981568, 136.1604741062248, 146.7877073305547, 153.45003334906377, 143.18194548383727, 145.6948948189125, 114.49366362624988, 149.26830022413333, 136.61882949590586, 157.27836257073844, 149.69830687731482, 141.81289259211627, 156.96769017557816, 114.71713325016323, 164.22812165446908, 155.38964569303224, 152.80310862236217, 154.2484337726744, 189.68387735404045, 191.12249016259383, 161.12694189221156, 155.59604152500862, 157.5096519653585, 175.59260220114555, 176.363302892876, 156.5819092499368, 160.14102059492808, 172.62971861915562, 168.1292004101883, 161.741224468669, 148.65554831411814, 157.92255134620302, 165.03224616495038, 168.30503594391766, 154.14206410514333, 109.12837125745257, 157.85855694055078, 168.89986201562243, 166.46267309525524, 119.3627612000547, 126.21956908036938, 173.26402974052033, 167.0333327092919, 159.15906568157368, 147.36442074963193, 172.91935969042308, 173.43459266179678, 166.8064841082839, 158.98818674730273, 167.32859133079373, 175.01944525809972, 155.06302969774964, 165.6302909332226, 166.51175075479892, 176.5906613656045, 165.1685742147857, 163.0444502914457, 174.96288022987758, 174.03492560913352, 155.91274803426367, 177.14059597035094, 173.79012450444824, 156.81549079069256, 111.09688152336432, 171.91472756443756, 174.13903403588446, 166.28868411686102, 124.8896487076793, 163.60675614028347, 177.80179858365696, 167.30751671982011, 165.54927827192242, 105.37527356120862, 173.1058855565364, 178.4633130417386, 170.83593891389603, 177.80430489879072, 168.67219540160255, 155.5377298846477, 170.0370944917075, 177.75570998018176, 179.8643206322948, 131.4675414928096, 164.58231521365485, 176.40491326531762, 168.95599952532046, 128.95356216261845, 156.49039244668552, 156.74524546785582, 168.1524256817866, 163.2311241028663, 155.38379070975625, 161.9293134782959, 174.06901703166116, 120.7610681874722, 156.98702155710495, 140.4584756361161, 178.08934114009818, 165.53628672891162, 161.46885287946006, 172.39383858954554, 169.9839831575916, 168.87840630018025, 164.01023046567866]
Elapsed: 0.29191069249128404~0.04411558234833609
Time per graph: 0.006702348854879089~0.000977241913845063
Speed: 151.84919789017187~18.535773651273885
Total Time: 0.2628
best val loss: 0.4329672157764435 test_score: 0.8140

Testing...
Test loss: 0.6633 score: 0.6279 time: 0.25s
test Score 0.6279
Epoch Time List: [0.9506891799392179, 0.9376925469841808, 0.9916284169303253, 1.0019747042097151, 1.0723279350204393, 0.9293966119876131, 0.9475779830245301, 0.9655663911253214, 0.9614449680084363, 1.0240915280301124, 0.8942742899525911, 0.934016571030952, 1.0546305241296068, 0.9574806008022279, 1.0537259500706568, 0.8861662191338837, 0.9437019589822739, 1.0159149959217757, 0.9494206979870796, 0.8982097601983696, 0.9391746511682868, 1.1454832229064777, 1.0862312910612673, 0.9281877200119197, 0.9409670418826863, 0.9725973650347441, 0.9046103889122605, 1.06848587107379, 0.9176264851121232, 0.9221525969915092, 0.9380338110495359, 0.9866385679924861, 0.8806027569808066, 1.0659381888108328, 0.9407015610486269, 0.9741168030304834, 0.9207245991565287, 0.9156083188718185, 1.034397127921693, 0.9536616870900616, 0.8991695288568735, 0.9241292158840224, 0.9288279019529, 1.0827293259790167, 0.9236235078424215, 0.9263238019775599, 0.973009602050297, 0.9932471310021356, 0.8768088800134137, 0.9556978701148182, 1.1475144969299436, 0.9176663840189576, 0.8760489509440958, 0.923590298043564, 0.9647817779332399, 0.9066832349635661, 0.9860020389314741, 0.9750531648751348, 0.9955970339942724, 0.9913032818585634, 0.9055275210412219, 0.983195836073719, 0.9917594090802595, 1.009148359997198, 0.957982983905822, 0.9249326090794057, 0.8878631939878687, 1.0326691890368238, 0.9569697399856523, 0.920689285849221, 0.9159899080405012, 0.9190136938123032, 0.9140455160522833, 0.9933355211978778, 0.9528412299696356, 0.8778929160907865, 0.9195232960628346, 0.948364706011489, 1.034568462986499, 0.8798282169736922, 0.9182588618714362, 1.031822606921196, 0.9695370519766584, 0.9166886569000781, 0.9445694129681215, 0.8695876070996746, 0.9239599200664088, 0.9581652330234647, 0.8956631199689582, 1.0404320800444111, 0.883341277949512, 0.8442537899827585, 0.8281762121478096, 0.9585216050036252, 0.885658337152563, 0.7863408090779558, 0.8994813640601933, 1.0838532330235466, 1.0552535069873556, 0.8746268769027665, 1.0468341179657727, 0.9128017139155418, 0.9644069231580943, 0.9274565340019763, 1.0227081759367138, 0.9237673070747405, 1.0866627290379256, 0.9226326540810987, 0.9300655031111091, 0.9869607751024887, 0.9706462899921462, 1.0756063910666853, 0.9378962380578741, 0.9654810050269589, 0.8909439590061083, 1.038820162997581, 0.9771023108623922, 0.9452420911984518, 1.038212611922063, 1.0003549329703674, 0.9734704490983859, 0.9095916139194742, 1.1017340440303087, 0.93647998617962, 1.0432929310481995, 0.9365685139782727, 0.9248152010841295, 1.070479383924976, 0.9300120869884267, 0.9210346358595416, 0.905403648968786, 0.9040993578964844, 1.017135726986453, 0.8584460159763694, 0.9347377570811659, 0.9345078041078523, 0.8580960468389094, 1.0095790250925347, 0.955179848941043, 1.0422580430749804, 0.8819647278869525, 0.9279362078523263, 0.9998388169333339, 1.0621746309334412, 0.9229559010127559, 0.9153215420665219, 0.9298446238972247, 1.0828651430783793, 1.0706632679793984, 0.8751225979067385, 1.0567581630311906, 0.9851760121528059, 1.208147800178267, 0.9803117380943149, 0.9277963999193162, 0.8805534361163154, 0.9156786979874596, 1.0933936319779605, 0.9845798721071333, 0.9351453210692853, 0.9593785299221054, 0.9293103198288009, 1.0848409639438614, 0.9358905410626903, 0.9170975009910762, 0.9289421429857612, 0.9191532320110127, 1.0540680669946596, 0.9379114719340578, 0.9134927459526807, 1.025480933021754, 0.966841424931772, 1.0018527389038354, 0.8957566510653123, 0.9410631221253425, 0.9401467679999769, 0.9016617919551209, 1.020318229100667, 0.9449822971364483, 0.9114433099748567]
Total Epoch List: [94, 86]
Total Time List: [0.285769677022472, 0.2627735889982432]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcc2b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.44s
Epoch 3/1000, LR 0.000030
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.26s
Epoch 6/1000, LR 0.000120
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.33s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.41s
Val loss: 0.6928 score: 0.7273 time: 0.27s
Test loss: 0.6928 score: 0.5814 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.26s
Epoch 24/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.29s
Epoch 32/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4884 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4884 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.4884 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.4884 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.4884 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.5000 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.4884 time: 0.22s
Epoch 38/1000, LR 0.000270
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.4884 time: 0.35s
Epoch 39/1000, LR 0.000269
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6828 score: 0.4884 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.4884 time: 0.28s
Epoch 41/1000, LR 0.000269
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6808 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.4884 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6796 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6790 score: 0.4884 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6746;  Loss pred: 0.6746; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.5000 time: 0.30s
Test loss: 0.6774 score: 0.5116 time: 0.26s
Epoch 44/1000, LR 0.000269
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6767 score: 0.5000 time: 0.38s
Test loss: 0.6757 score: 0.5116 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.40s
Val loss: 0.6751 score: 0.5227 time: 0.29s
Test loss: 0.6738 score: 0.5116 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6681;  Loss pred: 0.6681; Loss self: 0.0000; time: 0.40s
Val loss: 0.6732 score: 0.5455 time: 0.27s
Test loss: 0.6718 score: 0.5349 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.40s
Val loss: 0.6713 score: 0.5455 time: 0.28s
Test loss: 0.6695 score: 0.5349 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6641;  Loss pred: 0.6641; Loss self: 0.0000; time: 0.38s
Val loss: 0.6691 score: 0.5455 time: 0.30s
Test loss: 0.6671 score: 0.5349 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.40s
Val loss: 0.6668 score: 0.5682 time: 0.49s
Test loss: 0.6644 score: 0.5349 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.41s
Val loss: 0.6642 score: 0.5682 time: 0.27s
Test loss: 0.6614 score: 0.5581 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.49s
Val loss: 0.6614 score: 0.5909 time: 0.28s
Test loss: 0.6583 score: 0.5349 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.36s
Val loss: 0.6583 score: 0.5909 time: 0.29s
Test loss: 0.6549 score: 0.5349 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.6416;  Loss pred: 0.6416; Loss self: 0.0000; time: 0.44s
Val loss: 0.6548 score: 0.6364 time: 0.30s
Test loss: 0.6512 score: 0.5349 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.44s
Val loss: 0.6511 score: 0.6364 time: 0.27s
Test loss: 0.6472 score: 0.5349 time: 0.26s
Epoch 55/1000, LR 0.000269
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 0.50s
Val loss: 0.6471 score: 0.6591 time: 0.26s
Test loss: 0.6428 score: 0.5814 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6269;  Loss pred: 0.6269; Loss self: 0.0000; time: 0.34s
Val loss: 0.6427 score: 0.6818 time: 0.28s
Test loss: 0.6381 score: 0.6047 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 0.38s
Val loss: 0.6380 score: 0.7045 time: 0.29s
Test loss: 0.6329 score: 0.6279 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.53s
Val loss: 0.6329 score: 0.6818 time: 0.28s
Test loss: 0.6274 score: 0.6744 time: 0.28s
Epoch 59/1000, LR 0.000268
Train loss: 0.6034;  Loss pred: 0.6034; Loss self: 0.0000; time: 0.37s
Val loss: 0.6275 score: 0.7273 time: 0.28s
Test loss: 0.6215 score: 0.6977 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5992;  Loss pred: 0.5992; Loss self: 0.0000; time: 0.49s
Val loss: 0.6216 score: 0.7727 time: 0.29s
Test loss: 0.6152 score: 0.7209 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.42s
Val loss: 0.6154 score: 0.8182 time: 0.49s
Test loss: 0.6086 score: 0.7442 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.52s
Val loss: 0.6086 score: 0.8409 time: 0.32s
Test loss: 0.6016 score: 0.7442 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5683;  Loss pred: 0.5683; Loss self: 0.0000; time: 0.49s
Val loss: 0.6015 score: 0.8409 time: 0.31s
Test loss: 0.5943 score: 0.7674 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5594;  Loss pred: 0.5594; Loss self: 0.0000; time: 0.44s
Val loss: 0.5939 score: 0.8409 time: 0.27s
Test loss: 0.5867 score: 0.7907 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.65s
Val loss: 0.5859 score: 0.8409 time: 0.31s
Test loss: 0.5788 score: 0.7907 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.5388;  Loss pred: 0.5388; Loss self: 0.0000; time: 0.60s
Val loss: 0.5776 score: 0.8409 time: 0.28s
Test loss: 0.5705 score: 0.7907 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.43s
Val loss: 0.5689 score: 0.8409 time: 0.27s
Test loss: 0.5621 score: 0.8140 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.40s
Val loss: 0.5599 score: 0.8409 time: 0.29s
Test loss: 0.5539 score: 0.8140 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 0.40s
Val loss: 0.5506 score: 0.8409 time: 0.28s
Test loss: 0.5457 score: 0.8140 time: 0.38s
Epoch 70/1000, LR 0.000268
Train loss: 0.4873;  Loss pred: 0.4873; Loss self: 0.0000; time: 0.42s
Val loss: 0.5411 score: 0.8409 time: 0.28s
Test loss: 0.5375 score: 0.8140 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.4857;  Loss pred: 0.4857; Loss self: 0.0000; time: 0.46s
Val loss: 0.5314 score: 0.8636 time: 0.29s
Test loss: 0.5287 score: 0.8140 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.4667;  Loss pred: 0.4667; Loss self: 0.0000; time: 0.37s
Val loss: 0.5214 score: 0.8864 time: 0.29s
Test loss: 0.5190 score: 0.8140 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.4480;  Loss pred: 0.4480; Loss self: 0.0000; time: 0.41s
Val loss: 0.5113 score: 0.8864 time: 0.33s
Test loss: 0.5097 score: 0.8140 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.4400;  Loss pred: 0.4400; Loss self: 0.0000; time: 0.43s
Val loss: 0.5011 score: 0.8864 time: 0.30s
Test loss: 0.5009 score: 0.7907 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.4328;  Loss pred: 0.4328; Loss self: 0.0000; time: 0.44s
Val loss: 0.4907 score: 0.8864 time: 0.43s
Test loss: 0.4920 score: 0.7907 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.4208;  Loss pred: 0.4208; Loss self: 0.0000; time: 0.47s
Val loss: 0.4804 score: 0.9091 time: 0.30s
Test loss: 0.4828 score: 0.7907 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.3966;  Loss pred: 0.3966; Loss self: 0.0000; time: 0.36s
Val loss: 0.4699 score: 0.9091 time: 0.29s
Test loss: 0.4743 score: 0.7907 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.3854;  Loss pred: 0.3854; Loss self: 0.0000; time: 0.41s
Val loss: 0.4595 score: 0.9091 time: 0.33s
Test loss: 0.4659 score: 0.8140 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.4056;  Loss pred: 0.4056; Loss self: 0.0000; time: 0.42s
Val loss: 0.4495 score: 0.9318 time: 0.32s
Test loss: 0.4573 score: 0.8140 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.3570;  Loss pred: 0.3570; Loss self: 0.0000; time: 0.46s
Val loss: 0.4400 score: 0.9318 time: 0.38s
Test loss: 0.4478 score: 0.8140 time: 0.27s
Epoch 81/1000, LR 0.000267
Train loss: 0.3484;  Loss pred: 0.3484; Loss self: 0.0000; time: 0.50s
Val loss: 0.4314 score: 0.9318 time: 0.27s
Test loss: 0.4384 score: 0.8605 time: 0.26s
Epoch 82/1000, LR 0.000267
Train loss: 0.3365;  Loss pred: 0.3365; Loss self: 0.0000; time: 0.37s
Val loss: 0.4232 score: 0.9318 time: 0.30s
Test loss: 0.4296 score: 0.8605 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.3280;  Loss pred: 0.3280; Loss self: 0.0000; time: 0.40s
Val loss: 0.4159 score: 0.9318 time: 0.30s
Test loss: 0.4205 score: 0.8605 time: 0.25s
Epoch 84/1000, LR 0.000266
Train loss: 0.3073;  Loss pred: 0.3073; Loss self: 0.0000; time: 0.41s
Val loss: 0.4067 score: 0.9318 time: 0.27s
Test loss: 0.4137 score: 0.8605 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.2893;  Loss pred: 0.2893; Loss self: 0.0000; time: 0.46s
Val loss: 0.3954 score: 0.9318 time: 0.28s
Test loss: 0.4096 score: 0.8605 time: 0.25s
Epoch 86/1000, LR 0.000266
Train loss: 0.2942;  Loss pred: 0.2942; Loss self: 0.0000; time: 0.46s
Val loss: 0.3838 score: 0.9545 time: 0.30s
Test loss: 0.4069 score: 0.8605 time: 0.31s
Epoch 87/1000, LR 0.000266
Train loss: 0.2572;  Loss pred: 0.2572; Loss self: 0.0000; time: 0.39s
Val loss: 0.3724 score: 0.9318 time: 0.32s
Test loss: 0.4051 score: 0.8605 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.2922;  Loss pred: 0.2922; Loss self: 0.0000; time: 0.40s
Val loss: 0.3616 score: 0.9318 time: 0.28s
Test loss: 0.4044 score: 0.8605 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.2481;  Loss pred: 0.2481; Loss self: 0.0000; time: 0.41s
Val loss: 0.3522 score: 0.9318 time: 0.28s
Test loss: 0.4020 score: 0.8605 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.2281;  Loss pred: 0.2281; Loss self: 0.0000; time: 0.38s
Val loss: 0.3436 score: 0.9318 time: 0.32s
Test loss: 0.3981 score: 0.8605 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.2109;  Loss pred: 0.2109; Loss self: 0.0000; time: 0.53s
Val loss: 0.3348 score: 0.9318 time: 0.30s
Test loss: 0.3957 score: 0.8605 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.2052;  Loss pred: 0.2052; Loss self: 0.0000; time: 0.41s
Val loss: 0.3277 score: 0.9318 time: 0.30s
Test loss: 0.3903 score: 0.8605 time: 0.24s
Epoch 93/1000, LR 0.000265
Train loss: 0.2084;  Loss pred: 0.2084; Loss self: 0.0000; time: 0.50s
Val loss: 0.3220 score: 0.9318 time: 0.27s
Test loss: 0.3846 score: 0.8605 time: 0.26s
Epoch 94/1000, LR 0.000265
Train loss: 0.1892;  Loss pred: 0.1892; Loss self: 0.0000; time: 0.41s
Val loss: 0.3187 score: 0.9318 time: 0.29s
Test loss: 0.3780 score: 0.8605 time: 0.25s
Epoch 95/1000, LR 0.000265
Train loss: 0.1924;  Loss pred: 0.1924; Loss self: 0.0000; time: 0.39s
Val loss: 0.3174 score: 0.9318 time: 0.29s
Test loss: 0.3721 score: 0.8605 time: 0.23s
Epoch 96/1000, LR 0.000265
Train loss: 0.1704;  Loss pred: 0.1704; Loss self: 0.0000; time: 0.51s
Val loss: 0.3150 score: 0.9318 time: 0.30s
Test loss: 0.3691 score: 0.8605 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.1706;  Loss pred: 0.1706; Loss self: 0.0000; time: 0.42s
Val loss: 0.3104 score: 0.9318 time: 0.29s
Test loss: 0.3685 score: 0.8372 time: 0.24s
Epoch 98/1000, LR 0.000265
Train loss: 0.1500;  Loss pred: 0.1500; Loss self: 0.0000; time: 0.52s
Val loss: 0.3036 score: 0.9318 time: 0.27s
Test loss: 0.3699 score: 0.8372 time: 0.25s
Epoch 99/1000, LR 0.000265
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.42s
Val loss: 0.3031 score: 0.9318 time: 0.28s
Test loss: 0.3687 score: 0.8372 time: 0.34s
Epoch 100/1000, LR 0.000265
Train loss: 0.1606;  Loss pred: 0.1606; Loss self: 0.0000; time: 0.36s
Val loss: 0.3027 score: 0.9318 time: 0.29s
Test loss: 0.3683 score: 0.8372 time: 0.36s
Epoch 101/1000, LR 0.000265
Train loss: 0.1757;  Loss pred: 0.1757; Loss self: 0.0000; time: 0.39s
Val loss: 0.2981 score: 0.9318 time: 0.29s
Test loss: 0.3706 score: 0.8372 time: 0.23s
Epoch 102/1000, LR 0.000264
Train loss: 0.1441;  Loss pred: 0.1441; Loss self: 0.0000; time: 0.40s
Val loss: 0.2881 score: 0.9318 time: 0.31s
Test loss: 0.3761 score: 0.8372 time: 0.25s
Epoch 103/1000, LR 0.000264
Train loss: 0.1119;  Loss pred: 0.1119; Loss self: 0.0000; time: 0.47s
Val loss: 0.2762 score: 0.9318 time: 0.27s
Test loss: 0.3837 score: 0.8605 time: 0.26s
Epoch 104/1000, LR 0.000264
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 0.43s
Val loss: 0.2629 score: 0.9318 time: 0.28s
Test loss: 0.3949 score: 0.8605 time: 0.36s
Epoch 105/1000, LR 0.000264
Train loss: 0.1270;  Loss pred: 0.1270; Loss self: 0.0000; time: 0.41s
Val loss: 0.2560 score: 0.9318 time: 0.32s
Test loss: 0.4025 score: 0.8605 time: 0.25s
Epoch 106/1000, LR 0.000264
Train loss: 0.1178;  Loss pred: 0.1178; Loss self: 0.0000; time: 0.39s
Val loss: 0.2512 score: 0.9318 time: 0.42s
Test loss: 0.4085 score: 0.8605 time: 0.25s
Epoch 107/1000, LR 0.000264
Train loss: 0.0826;  Loss pred: 0.0826; Loss self: 0.0000; time: 0.39s
Val loss: 0.2474 score: 0.9318 time: 0.31s
Test loss: 0.4140 score: 0.8605 time: 0.24s
Epoch 108/1000, LR 0.000264
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.41s
Val loss: 0.2437 score: 0.9318 time: 0.30s
Test loss: 0.4202 score: 0.8605 time: 0.24s
Epoch 109/1000, LR 0.000264
Train loss: 0.0887;  Loss pred: 0.0887; Loss self: 0.0000; time: 0.44s
Val loss: 0.2408 score: 0.9318 time: 0.28s
Test loss: 0.4259 score: 0.8605 time: 0.25s
Epoch 110/1000, LR 0.000263
Train loss: 0.0851;  Loss pred: 0.0851; Loss self: 0.0000; time: 0.36s
Val loss: 0.2383 score: 0.9318 time: 0.29s
Test loss: 0.4315 score: 0.8605 time: 0.24s
Epoch 111/1000, LR 0.000263
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.40s
Val loss: 0.2345 score: 0.9318 time: 0.39s
Test loss: 0.4394 score: 0.8605 time: 0.23s
Epoch 112/1000, LR 0.000263
Train loss: 0.1299;  Loss pred: 0.1299; Loss self: 0.0000; time: 0.47s
Val loss: 0.2345 score: 0.9318 time: 0.30s
Test loss: 0.4424 score: 0.8605 time: 0.24s
Epoch 113/1000, LR 0.000263
Train loss: 0.0955;  Loss pred: 0.0955; Loss self: 0.0000; time: 0.41s
Val loss: 0.2377 score: 0.9318 time: 0.27s
Test loss: 0.4420 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 114/1000, LR 0.000263
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.41s
Val loss: 0.2408 score: 0.9318 time: 0.28s
Test loss: 0.4436 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 111,   Train_Loss: 0.1299,   Val_Loss: 0.2345,   Val_Precision: 1.0000,   Val_Recall: 0.8636,   Val_accuracy: 0.9268,   Val_Score: 0.9318,   Val_Loss: 0.2345,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.4424


[0.3029502850258723, 0.2876396829960868, 0.304128899006173, 0.31033104192465544, 0.42547445197124034, 0.3021622379310429, 0.29891157790552825, 0.2975473899859935, 0.3259642239427194, 0.416392654995434, 0.2850428440142423, 0.2978367171017453, 0.2928726620739326, 0.3349638710496947, 0.42805258405860513, 0.2875070059671998, 0.29873804398812354, 0.3013893309980631, 0.3156214809278026, 0.2898801430128515, 0.2948170639574528, 0.4922329419059679, 0.4484705409267917, 0.30928799707908183, 0.29047588096000254, 0.29346120497211814, 0.29282217798754573, 0.41942866099998355, 0.2991079770727083, 0.29753297299612314, 0.30416760803200305, 0.3056240259902552, 0.2907377310330048, 0.2966747839236632, 0.2980822160607204, 0.315154833951965, 0.29478957294486463, 0.2856602050596848, 0.29594726697541773, 0.30737587600015104, 0.28874787501990795, 0.28601291100494564, 0.30415513401385397, 0.31472878402564675, 0.2987792060011998, 0.2934072690550238, 0.30247484298888594, 0.3165629480499774, 0.2819108589319512, 0.30526562698651105, 0.29613089596387, 0.30321203998755664, 0.2811391110299155, 0.29760031297337264, 0.33164347999263555, 0.28688512404914945, 0.3743415790377185, 0.29239206993952394, 0.3741739368997514, 0.30708283092826605, 0.2798166039865464, 0.3086141800740734, 0.3761415350018069, 0.307702076039277, 0.3071471869479865, 0.2941260989755392, 0.2945724290329963, 0.40916355699300766, 0.282626096974127, 0.2941741900285706, 0.2991974810138345, 0.2943368839332834, 0.29249069397337735, 0.29242929304018617, 0.30809090402908623, 0.28247333492618054, 0.29357187496498227, 0.3231481109978631, 0.29975262098014355, 0.28673828893806785, 0.3073013140819967, 0.30200097302440554, 0.38430074299685657, 0.2947712269378826, 0.32206395093817264, 0.2797587619861588, 0.2939245000015944, 0.3102679819567129, 0.28031246399041265, 0.38355212297756225, 0.2679200100246817, 0.28315915004350245, 0.2879522569710389, 0.28525411197915673, 0.22669296199455857, 0.224986603949219, 0.26687032904010266, 0.27635664492845535, 0.2729991429951042, 0.2448850319487974, 0.24381489399820566, 0.2746166540309787, 0.2685133380582556, 0.24908805009908974, 0.2557556920219213, 0.26585677301045507, 0.289259301032871, 0.2722853679442778, 0.2605551399756223, 0.255488493014127, 0.2789634370710701, 0.3940313550410792, 0.27239574992563576, 0.2545887219021097, 0.2583161690272391, 0.360246357973665, 0.3406761749647558, 0.2481761509552598, 0.2574336469406262, 0.27016996999736875, 0.2917936349986121, 0.24867082596756518, 0.24793208402115852, 0.2577837440185249, 0.2704603460151702, 0.25697939400561154, 0.24568698601797223, 0.2773065899964422, 0.2596143480623141, 0.258240032941103, 0.24350098508875817, 0.2603400810621679, 0.26373176102060825, 0.2457664159592241, 0.24707684305030853, 0.27579528000205755, 0.24274503404740244, 0.2474248759681359, 0.2742076040012762, 0.3870495680021122, 0.25012400397099555, 0.24692912900354713, 0.258586446987465, 0.3443039550911635, 0.2628253319999203, 0.2418423229828477, 0.25701176398433745, 0.2597413921030238, 0.40806537005119026, 0.2484028770122677, 0.24094587995205075, 0.2517034780466929, 0.24183891399297863, 0.2549323550192639, 0.27646025200374424, 0.2528859960148111, 0.24190502800047398, 0.23906909301877022, 0.3270769310183823, 0.2612674389965832, 0.243757382966578, 0.25450413196813315, 0.333453370956704, 0.2747772519942373, 0.27433049003593624, 0.2557203669566661, 0.2634301530197263, 0.2767341419821605, 0.26554796705022454, 0.2470284530427307, 0.3560750219039619, 0.27390799298882484, 0.30614030093420297, 0.24145184503868222, 0.25976177700795233, 0.26630522997584194, 0.24942886794451624, 0.2529650100041181, 0.2546210669679567, 0.2621787670068443, 0.25751252204645425, 0.4478620960144326, 0.23906087805517018, 0.23468613997101784, 0.2613236049655825, 0.25172769103664905, 0.33609817002434283, 0.25500058999750763, 0.2516278700204566, 0.2505382049130276, 0.24059987301006913, 0.24990737799089402, 0.26072615990415215, 0.2557706009829417, 0.24803316604811698, 0.2534004390472546, 0.2512747159926221, 0.23294462100602686, 0.2562631869222969, 0.24854241800494492, 0.23759912489913404, 0.246002311934717, 0.2668397349771112, 0.25351940002292395, 0.23199007799848914, 0.2485253110062331, 0.25780564500018954, 0.23938874900341034, 0.260694928932935, 0.2575079179368913, 0.294717327109538, 0.23974230501335114, 0.25410875701345503, 0.2542629239615053, 0.26458740094676614, 0.2478872979991138, 0.2292193090543151, 0.3504497309913859, 0.26089316711295396, 0.28043120889924467, 0.2409760729642585, 0.25089875503908843, 0.26074538892135024, 0.24487474199850112, 0.24766616302076727, 0.25534420902840793, 0.26507665892131627, 0.23985342495143414, 0.23252164490986615, 0.251931075938046, 0.2556701679714024, 0.23826328909490258, 0.24036964599508792, 0.26033713505603373, 0.2558755479985848, 0.22921680100262165, 0.24851121800020337, 0.2832111220341176, 0.24468390399124473, 0.2381078889593482, 0.23232475598342717, 0.2560730220284313, 0.24166980700101703, 0.2587057159980759, 0.25310059101320803, 0.24863641406409442, 0.26565536099951714, 0.25785257504321635, 0.3814026890322566, 0.2533308109268546, 0.2660021469928324, 0.2378052689600736, 0.24790750700049102, 0.2518259179778397, 0.2501834479626268, 0.25696857704315335, 0.24337986204773188, 0.2545575100230053, 0.25224090996198356, 0.27328245202079415, 0.2667831829749048, 0.25642025598790497, 0.2529407129622996, 0.24962174601387233, 0.2591564580798149, 0.3142237769206986, 0.23905498895328492, 0.24645589792635292, 0.25040955701842904, 0.23977666709106416, 0.24455468903761357, 0.24591498204972595, 0.2654887930257246, 0.25890411098953336, 0.238928175996989, 0.23697105702012777, 0.24478500906843692, 0.25680555996950716, 0.3436874389881268, 0.3642074189847335, 0.23926827206742018, 0.25169275200460106, 0.26078497094567865, 0.36481080192606896, 0.25172385096084327, 0.2517558439867571, 0.2408975639846176, 0.24504436098504812, 0.257686844910495, 0.24271826504264027, 0.23375101597048342, 0.24633282294962555, 0.25134973996318877, 0.2569973269710317]
[0.006885233750588007, 0.006537265522638336, 0.006912020431958477, 0.0070529782255603505, 0.009669873908437281, 0.006867323589341884, 0.006793444952398369, 0.006762440681499852, 0.007408277816879986, 0.00946346943171441, 0.006478246454869143, 0.006769016297766939, 0.00665619686531665, 0.007612815251129426, 0.009728467819513753, 0.006534250135618177, 0.00678950099973008, 0.006849757522683252, 0.007173215475631878, 0.006588185068473897, 0.006700387817214836, 0.011187112316044724, 0.010192512293790722, 0.007029272660888223, 0.006601724567272785, 0.006669572840275412, 0.006655049499716948, 0.009532469568181445, 0.00679790856983428, 0.006762113022639162, 0.0069129001825455234, 0.006946000590687618, 0.0066076757052955645, 0.0067426087255378, 0.006774595819561827, 0.007162609862544658, 0.006699763021474196, 0.00649227738772011, 0.0067260742494413125, 0.0069858153636397965, 0.006562451704997908, 0.006500293431930582, 0.006912616682133044, 0.00715292690967379, 0.006790436500027267, 0.0066683470239778135, 0.006874428249747408, 0.007194612455681305, 0.0064070649757261644, 0.006937855158784342, 0.0067302476355425, 0.006891182726989923, 0.006389525250679898, 0.00676364347666756, 0.007537351818014445, 0.006520116455662487, 0.008507763159948147, 0.006645274316807362, 0.008503953111357987, 0.006979155248369683, 0.006359468272421509, 0.007013958638047122, 0.008548671250041067, 0.006993229000892659, 0.006980617885181511, 0.006684684067625891, 0.006694827932568098, 0.009299171749841083, 0.0064233203857756134, 0.006685777046103877, 0.00679994275031442, 0.00668947463484735, 0.006647515772122212, 0.006646120296367867, 0.007002066000661051, 0.006419848521049557, 0.006672088067385961, 0.0073442752499514345, 0.0068125595677305355, 0.006516779294046996, 0.006984120774590833, 0.006863658477827398, 0.008734107795383104, 0.006699346066770059, 0.007319635248594833, 0.006358153681503609, 0.00668010227276351, 0.007051545044470748, 0.006370737817963924, 0.008717093704035506, 0.006089091136924584, 0.006435435228261419, 0.00654436947661452, 0.0064830479995262895, 0.005271929348710664, 0.005232246603470209, 0.006206286721862853, 0.006426898719266404, 0.006348817278955912, 0.005695000742995288, 0.005670113813911759, 0.0063864338146739225, 0.006244496233912922, 0.005792745351141622, 0.005947806791207472, 0.006182715651405932, 0.006726960489136535, 0.006332217859169252, 0.006059421859898193, 0.0059415928607936515, 0.006487521792350467, 0.009163519884676261, 0.006334784881991529, 0.005920667951211853, 0.006007352768075328, 0.008377822278457325, 0.007922701743366413, 0.005771538394308368, 0.005986828998619214, 0.006283022558078343, 0.006785898488339817, 0.005783042464361981, 0.00576586241909671, 0.005994970791128485, 0.006289775488724889, 0.005976264976874687, 0.005713650837627261, 0.006448990465033539, 0.006037542978193351, 0.006005582161421, 0.0056628136067153065, 0.006054420489817858, 0.006133296767921122, 0.005715498045563351, 0.0057459730941932215, 0.006413843720978083, 0.0056452333499395916, 0.005754066882979905, 0.006376921023285493, 0.009001152744235168, 0.005816837301651059, 0.005742537883803422, 0.0060136383020340705, 0.008007068723050315, 0.006112217023253961, 0.005624240069368551, 0.005977017767077615, 0.006040497490767996, 0.009489892326771866, 0.005776811093308552, 0.005603392557024436, 0.005853569256899835, 0.005624160790534387, 0.0059286594190526495, 0.006429308186133587, 0.0058810696747630484, 0.005625698325592418, 0.005559746349273726, 0.007606440256241449, 0.006075986953408912, 0.005668776348059954, 0.005918700743444957, 0.007754729557132652, 0.006390168651028774, 0.006379778838045028, 0.005946985278062002, 0.006126282628365728, 0.006435677720515361, 0.006175534117447082, 0.005744847745179783, 0.008280814462882835, 0.006369953325321508, 0.007119541882190767, 0.005615159186946098, 0.006040971558324473, 0.006193144883159115, 0.005800671347546889, 0.005882907209398095, 0.005921420162045505, 0.006097180628066146, 0.005988663303405913, 0.010415397581730991, 0.005559555303608609, 0.0054578172086283214, 0.006077293138734476, 0.005854132349689512, 0.007816236512194019, 0.005930246279011805, 0.005851810930708293, 0.005826469881698316, 0.0055953458839550965, 0.005811799488160326, 0.0060633990675384225, 0.005948153511231202, 0.005768213163909697, 0.005893033466215224, 0.005843598046340048, 0.00541731676758202, 0.005959608998192951, 0.005780056232673137, 0.005525561044165908, 0.005720983998481791, 0.006205575232025843, 0.005895800000533115, 0.00539511809298812, 0.005779658395493793, 0.005995480116283478, 0.005567180209381636, 0.006062672765882209, 0.005988556231090496, 0.0068538913281287915, 0.005575402442170957, 0.005909505977057094, 0.005913091254918728, 0.006153195370855027, 0.005764820883700321, 0.005330681605914304, 0.008149993743985718, 0.006067282956115208, 0.006521656020912666, 0.005604094720099035, 0.005834854768350894, 0.006063846253984889, 0.005694761441825608, 0.005759678209785285, 0.005938237419265301, 0.006164573463286424, 0.005577986626777538, 0.005407480114182934, 0.005858862231117349, 0.005945817859800056, 0.005541006723137269, 0.005589991767327626, 0.006054351978047296, 0.0059505941395019725, 0.005330623279130736, 0.00577933065116752, 0.0065863051635841295, 0.005690323348633598, 0.00553739276649647, 0.005402901301940166, 0.005955186558800728, 0.005620228069791094, 0.006016411999955253, 0.005886060256121117, 0.0057822421875370796, 0.006178031651151561, 0.005996571512632939, 0.00886982997749434, 0.00589141420760127, 0.006186096441693777, 0.0055303550920947344, 0.005765290860476536, 0.005856416697159063, 0.005818219720061088, 0.005976013419608218, 0.005659996791807718, 0.005919942093558263, 0.005866067673534502, 0.006355405860948701, 0.006204260069183832, 0.005963261767160581, 0.005882342161913944, 0.005805156884043542, 0.006026894373949184, 0.0073075296958302, 0.005559418347750812, 0.005731532509915184, 0.005823478070196024, 0.005576201560257306, 0.005687318349711944, 0.005718953070923859, 0.006174157977342432, 0.006021025836965893, 0.005556469209232303, 0.005510954814421576, 0.005692674629498533, 0.005972222324872259, 0.007992731139258764, 0.008469939976389151, 0.005564378420172563, 0.00585331981406049, 0.006064766766178573, 0.008483972137815557, 0.005854043045601006, 0.005854787069459467, 0.005602268929874828, 0.005698706069419724, 0.005992717323499883, 0.005644610814945122, 0.005436070138848451, 0.0057286703011540825, 0.005845342789841599, 0.005976682022582133]
[145.23835155409196, 152.96915759915714, 144.675498263343, 141.7840758923584, 103.4139648012853, 145.61713700982494, 147.20072172616315, 147.87560395697378, 134.98413865115447, 105.66949121732824, 154.36275957800058, 147.73195336077038, 150.23594106879347, 131.3574501695206, 102.79110940719356, 153.0397488992659, 147.28622914110412, 145.99056925569397, 139.4074949228974, 151.7868714382734, 149.24509256475727, 89.38857247064419, 98.11123805160382, 142.26222942867597, 151.47557124048672, 149.9346395861097, 150.26184253663806, 104.90460974959869, 147.10406733587152, 147.88276928410662, 144.65708654739637, 143.96773898071396, 151.33914625964073, 148.31054873649643, 147.6102820942429, 139.61391436790197, 149.25901062392546, 154.02915499135344, 148.6751354377426, 143.1472130232445, 152.38207379696348, 153.83920902521484, 144.66301922753618, 139.8029104208483, 147.26593791076382, 149.96220148775015, 145.46664299489106, 138.99289310716645, 156.0777054374514, 144.13676519808192, 148.58294287999016, 145.1129711135669, 156.50615042073616, 147.84930687870894, 132.67259166674123, 153.37149371489153, 117.5397082875653, 150.48287735402883, 117.59236991375074, 143.28381650968404, 157.24585093640664, 142.57283961948588, 116.97724368511612, 142.9954603048683, 143.25379449902348, 149.59570114061597, 149.36903682547756, 107.5364588267863, 155.68272169865466, 149.57124551180598, 147.06006163857208, 149.4885704163851, 150.43213649732357, 150.46372250386503, 142.81499201886874, 155.76691517271402, 149.87811759981568, 136.1604741062248, 146.7877073305547, 153.45003334906377, 143.18194548383727, 145.6948948189125, 114.49366362624988, 149.26830022413333, 136.61882949590586, 157.27836257073844, 149.69830687731482, 141.81289259211627, 156.96769017557816, 114.71713325016323, 164.22812165446908, 155.38964569303224, 152.80310862236217, 154.2484337726744, 189.68387735404045, 191.12249016259383, 161.12694189221156, 155.59604152500862, 157.5096519653585, 175.59260220114555, 176.363302892876, 156.5819092499368, 160.14102059492808, 172.62971861915562, 168.1292004101883, 161.741224468669, 148.65554831411814, 157.92255134620302, 165.03224616495038, 168.30503594391766, 154.14206410514333, 109.12837125745257, 157.85855694055078, 168.89986201562243, 166.46267309525524, 119.3627612000547, 126.21956908036938, 173.26402974052033, 167.0333327092919, 159.15906568157368, 147.36442074963193, 172.91935969042308, 173.43459266179678, 166.8064841082839, 158.98818674730273, 167.32859133079373, 175.01944525809972, 155.06302969774964, 165.6302909332226, 166.51175075479892, 176.5906613656045, 165.1685742147857, 163.0444502914457, 174.96288022987758, 174.03492560913352, 155.91274803426367, 177.14059597035094, 173.79012450444824, 156.81549079069256, 111.09688152336432, 171.91472756443756, 174.13903403588446, 166.28868411686102, 124.8896487076793, 163.60675614028347, 177.80179858365696, 167.30751671982011, 165.54927827192242, 105.37527356120862, 173.1058855565364, 178.4633130417386, 170.83593891389603, 177.80430489879072, 168.67219540160255, 155.5377298846477, 170.0370944917075, 177.75570998018176, 179.8643206322948, 131.4675414928096, 164.58231521365485, 176.40491326531762, 168.95599952532046, 128.95356216261845, 156.49039244668552, 156.74524546785582, 168.1524256817866, 163.2311241028663, 155.38379070975625, 161.9293134782959, 174.06901703166116, 120.7610681874722, 156.98702155710495, 140.4584756361161, 178.08934114009818, 165.53628672891162, 161.46885287946006, 172.39383858954554, 169.9839831575916, 168.87840630018025, 164.01023046567866, 166.98217103494085, 96.01169731187589, 179.870501396203, 183.22343196453872, 164.5469417340362, 170.8195066777124, 127.93881024965297, 168.62706082531136, 170.88727093904274, 171.630510464171, 178.7199613284935, 172.06374755997322, 164.92399541268745, 168.11940009816777, 173.36391211350443, 169.69189225430375, 171.1274444391873, 184.59322998871684, 167.7962430594383, 173.00869744956165, 180.97709753036517, 174.79510522409703, 161.14541563192762, 169.61226634376627, 185.35275461341084, 173.02060633543095, 166.79231364374658, 179.624147663629, 164.94375312939806, 166.9851565905566, 145.9025175809162, 179.35924991463375, 169.21888291210348, 169.1162806202531, 162.51718655587618, 173.4659272463155, 187.59327116639574, 122.69948068830719, 164.81842156250534, 153.3352873554432, 178.44095254377288, 171.3838715273851, 164.91183287222108, 175.59998082719042, 173.62081067325443, 168.40013785163265, 162.21722491516636, 179.27615588022852, 184.92902033558366, 170.68160345004208, 168.18544119237916, 180.4726198624442, 178.8911400272176, 165.17044328211142, 168.0504461498518, 187.59532378042476, 173.03041828866176, 151.83019540743857, 175.73693773309512, 180.5904045763949, 185.08574266216982, 167.92085187023642, 177.92872239029447, 166.21202138541, 169.8929260807457, 172.94329216361405, 161.8638518651169, 166.76195687707659, 112.74173265297387, 169.73853216936803, 161.65283057342637, 180.82021558243733, 173.45178659682472, 170.75287700840997, 171.8738803472861, 167.3356349433297, 176.6785453743367, 168.92057121439447, 170.4719508285977, 157.3463633761897, 161.17957481617137, 167.69346023127073, 170.00031152125788, 172.2606330844683, 165.9229344258011, 136.84515036190916, 179.87493249263613, 174.4734062434548, 171.71868562842187, 179.33354617723975, 175.82979156611637, 174.85717885746814, 161.96540543175962, 166.08465518625292, 179.97040248841094, 181.4567590688836, 175.6643520109439, 167.44185758714016, 125.11367923890143, 118.06459110543938, 179.71459244660574, 170.8432191929545, 164.88680250272887, 117.86931684307473, 170.82211254859928, 170.80040454696214, 178.49910679356537, 175.47843103650825, 166.86920907792415, 177.16013252008804, 183.95641970356084, 174.56057818487872, 171.07636557052948, 167.316915342263]
Elapsed: 0.27904742419661094~0.04304206368792984
Time per graph: 0.006436655450256021~0.0009551926219846938
Speed: 158.13976642794034~19.10128502596841
Total Time: 0.2578
best val loss: 0.23445166647434235 test_score: 0.8605

Testing...
Test loss: 0.4069 score: 0.8605 time: 0.24s
test Score 0.8605
Epoch Time List: [0.9506891799392179, 0.9376925469841808, 0.9916284169303253, 1.0019747042097151, 1.0723279350204393, 0.9293966119876131, 0.9475779830245301, 0.9655663911253214, 0.9614449680084363, 1.0240915280301124, 0.8942742899525911, 0.934016571030952, 1.0546305241296068, 0.9574806008022279, 1.0537259500706568, 0.8861662191338837, 0.9437019589822739, 1.0159149959217757, 0.9494206979870796, 0.8982097601983696, 0.9391746511682868, 1.1454832229064777, 1.0862312910612673, 0.9281877200119197, 0.9409670418826863, 0.9725973650347441, 0.9046103889122605, 1.06848587107379, 0.9176264851121232, 0.9221525969915092, 0.9380338110495359, 0.9866385679924861, 0.8806027569808066, 1.0659381888108328, 0.9407015610486269, 0.9741168030304834, 0.9207245991565287, 0.9156083188718185, 1.034397127921693, 0.9536616870900616, 0.8991695288568735, 0.9241292158840224, 0.9288279019529, 1.0827293259790167, 0.9236235078424215, 0.9263238019775599, 0.973009602050297, 0.9932471310021356, 0.8768088800134137, 0.9556978701148182, 1.1475144969299436, 0.9176663840189576, 0.8760489509440958, 0.923590298043564, 0.9647817779332399, 0.9066832349635661, 0.9860020389314741, 0.9750531648751348, 0.9955970339942724, 0.9913032818585634, 0.9055275210412219, 0.983195836073719, 0.9917594090802595, 1.009148359997198, 0.957982983905822, 0.9249326090794057, 0.8878631939878687, 1.0326691890368238, 0.9569697399856523, 0.920689285849221, 0.9159899080405012, 0.9190136938123032, 0.9140455160522833, 0.9933355211978778, 0.9528412299696356, 0.8778929160907865, 0.9195232960628346, 0.948364706011489, 1.034568462986499, 0.8798282169736922, 0.9182588618714362, 1.031822606921196, 0.9695370519766584, 0.9166886569000781, 0.9445694129681215, 0.8695876070996746, 0.9239599200664088, 0.9581652330234647, 0.8956631199689582, 1.0404320800444111, 0.883341277949512, 0.8442537899827585, 0.8281762121478096, 0.9585216050036252, 0.885658337152563, 0.7863408090779558, 0.8994813640601933, 1.0838532330235466, 1.0552535069873556, 0.8746268769027665, 1.0468341179657727, 0.9128017139155418, 0.9644069231580943, 0.9274565340019763, 1.0227081759367138, 0.9237673070747405, 1.0866627290379256, 0.9226326540810987, 0.9300655031111091, 0.9869607751024887, 0.9706462899921462, 1.0756063910666853, 0.9378962380578741, 0.9654810050269589, 0.8909439590061083, 1.038820162997581, 0.9771023108623922, 0.9452420911984518, 1.038212611922063, 1.0003549329703674, 0.9734704490983859, 0.9095916139194742, 1.1017340440303087, 0.93647998617962, 1.0432929310481995, 0.9365685139782727, 0.9248152010841295, 1.070479383924976, 0.9300120869884267, 0.9210346358595416, 0.905403648968786, 0.9040993578964844, 1.017135726986453, 0.8584460159763694, 0.9347377570811659, 0.9345078041078523, 0.8580960468389094, 1.0095790250925347, 0.955179848941043, 1.0422580430749804, 0.8819647278869525, 0.9279362078523263, 0.9998388169333339, 1.0621746309334412, 0.9229559010127559, 0.9153215420665219, 0.9298446238972247, 1.0828651430783793, 1.0706632679793984, 0.8751225979067385, 1.0567581630311906, 0.9851760121528059, 1.208147800178267, 0.9803117380943149, 0.9277963999193162, 0.8805534361163154, 0.9156786979874596, 1.0933936319779605, 0.9845798721071333, 0.9351453210692853, 0.9593785299221054, 0.9293103198288009, 1.0848409639438614, 0.9358905410626903, 0.9170975009910762, 0.9289421429857612, 0.9191532320110127, 1.0540680669946596, 0.9379114719340578, 0.9134927459526807, 1.025480933021754, 0.966841424931772, 1.0018527389038354, 0.8957566510653123, 0.9410631221253425, 0.9401467679999769, 0.9016617919551209, 1.020318229100667, 0.9449822971364483, 0.9114433099748567, 1.0470604948932305, 1.1860754208173603, 0.882747882977128, 0.9488346538273618, 1.0038815889274701, 0.9612834320869297, 1.007059894152917, 0.9523899939376861, 0.9259809370851144, 0.986682103946805, 0.9158109810668975, 0.919222516939044, 1.0612365399720147, 0.8977018391015008, 0.9259622441604733, 0.9246115391142666, 0.9299052408896387, 0.9916430721059442, 0.9526350270025432, 0.9110389200504869, 0.8663599550491199, 0.9419140190584585, 0.9362451088381931, 1.0054517890093848, 0.8658215259201825, 0.9473081841133535, 0.9456275899428874, 0.8868506338912994, 1.0578569830395281, 0.9314591238508001, 0.9659825981361791, 0.9074652149574831, 0.9254873610334471, 1.1219427980249748, 0.955203372053802, 0.928750229999423, 1.044168918975629, 1.0491299030836672, 1.0359359440626577, 0.9476704109692946, 0.9402638720348477, 0.93577489501331, 0.942465111031197, 0.9827400209615007, 0.9313326270785183, 0.9127805919852108, 0.9379848119569942, 0.906489324872382, 1.1181610350031406, 0.9282096789684147, 1.0195841860258952, 0.8847304070368409, 0.9776199818588793, 0.9631490160245448, 1.013863800209947, 0.8425112480763346, 0.9210263770073652, 1.0936407629633322, 0.8890423029661179, 1.0176989131141454, 1.1363654440501705, 1.0979956828523427, 1.0362876468570903, 0.9701312249526381, 1.211152113857679, 1.1272224269341677, 0.9605983109213412, 0.9472984499298036, 1.0607265341095626, 0.9508297080174088, 1.0073788369772956, 0.8938093320466578, 0.9834069940261543, 0.9703582390211523, 1.107907119905576, 1.0213155910605565, 0.8842033060500398, 0.9883053879020736, 0.9882645888719708, 1.1140965769300237, 1.0333508091280237, 0.9208911530440673, 0.9422298051649705, 0.9249965520575643, 0.9944422140251845, 1.0731783718802035, 0.9411833889316767, 0.9236296889139339, 0.9398451189044863, 0.9409008568618447, 1.0654456198681146, 0.9548682979075238, 1.0327662180643529, 0.9479610749986023, 0.9144164520548657, 1.041517443023622, 0.9501096459571272, 1.0465552300447598, 1.0367416959488764, 1.0144148711115122, 0.920749292941764, 0.9583758859662339, 1.0045142190065235, 1.0697230530204251, 0.9789115318562835, 1.0541720010805875, 0.9342327490448952, 0.9584323560120538, 0.9719881158089265, 0.8899301609490067, 1.0207797158509493, 1.0179650720674545, 0.9312909729778767, 0.936511822976172]
Total Epoch List: [94, 86, 114]
Total Time List: [0.285769677022472, 0.2627735889982432, 0.2577986069954932]
========================training times:7========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcef20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.28s
Epoch 2/1000, LR 0.000000
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.37s
Epoch 3/1000, LR 0.000030
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.30s
Epoch 4/1000, LR 0.000060
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.28s
Epoch 5/1000, LR 0.000090
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.4884 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.29s
Epoch 6/1000, LR 0.000120
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.30s
Epoch 7/1000, LR 0.000150
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.33s
Epoch 8/1000, LR 0.000180
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.31s
Epoch 9/1000, LR 0.000210
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.39s
Epoch 10/1000, LR 0.000240
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4884 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.28s
Epoch 11/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.31s
Epoch 12/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.33s
Epoch 13/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.30s
Epoch 14/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4884 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.32s
Epoch 15/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.39s
Epoch 16/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.30s
Epoch 17/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.28s
Epoch 18/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.29s
Epoch 19/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.30s
Epoch 20/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.28s
Epoch 21/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.28s
Epoch 22/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.30s
Epoch 23/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.28s
Epoch 24/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.28s
Epoch 25/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.31s
Epoch 26/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4884 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.31s
Epoch 27/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4884 time: 0.26s
Test loss: 0.6897 score: 0.5227 time: 0.28s
Epoch 28/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.39s
Val loss: 0.6911 score: 0.5116 time: 0.27s
Test loss: 0.6894 score: 0.5682 time: 0.28s
Epoch 29/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.39s
Val loss: 0.6907 score: 0.5349 time: 0.25s
Test loss: 0.6890 score: 0.6136 time: 0.29s
Epoch 30/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.45s
Val loss: 0.6904 score: 0.6279 time: 0.25s
Test loss: 0.6885 score: 0.6364 time: 0.30s
Epoch 31/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.34s
Val loss: 0.6900 score: 0.6744 time: 0.37s
Test loss: 0.6880 score: 0.6364 time: 0.36s
Epoch 32/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.41s
Val loss: 0.6895 score: 0.6279 time: 0.27s
Test loss: 0.6875 score: 0.6818 time: 0.27s
Epoch 33/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.37s
Val loss: 0.6891 score: 0.6047 time: 0.33s
Test loss: 0.6869 score: 0.6818 time: 0.29s
Epoch 34/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.40s
Val loss: 0.6885 score: 0.6744 time: 0.26s
Test loss: 0.6863 score: 0.7273 time: 0.30s
Epoch 35/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.36s
Val loss: 0.6879 score: 0.6744 time: 0.26s
Test loss: 0.6856 score: 0.7727 time: 0.28s
Epoch 36/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.39s
Val loss: 0.6873 score: 0.6977 time: 0.26s
Test loss: 0.6848 score: 0.8182 time: 0.29s
Epoch 37/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.36s
Val loss: 0.6866 score: 0.7442 time: 0.24s
Test loss: 0.6840 score: 0.7955 time: 0.30s
Epoch 38/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.57s
Val loss: 0.6859 score: 0.6744 time: 0.25s
Test loss: 0.6831 score: 0.7955 time: 0.31s
Epoch 39/1000, LR 0.000269
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.59s
Val loss: 0.6851 score: 0.6744 time: 0.27s
Test loss: 0.6821 score: 0.7727 time: 0.30s
Epoch 40/1000, LR 0.000269
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.59s
Val loss: 0.6842 score: 0.6744 time: 0.26s
Test loss: 0.6810 score: 0.7273 time: 0.30s
Epoch 41/1000, LR 0.000269
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.45s
Val loss: 0.6832 score: 0.6279 time: 0.27s
Test loss: 0.6798 score: 0.7273 time: 0.29s
Epoch 42/1000, LR 0.000269
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.37s
Val loss: 0.6821 score: 0.6279 time: 0.33s
Test loss: 0.6785 score: 0.7273 time: 0.30s
Epoch 43/1000, LR 0.000269
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.38s
Val loss: 0.6809 score: 0.6512 time: 0.25s
Test loss: 0.6772 score: 0.7273 time: 0.29s
Epoch 44/1000, LR 0.000269
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.45s
Val loss: 0.6796 score: 0.6512 time: 0.24s
Test loss: 0.6757 score: 0.7045 time: 0.41s
Epoch 45/1000, LR 0.000269
Train loss: 0.6752;  Loss pred: 0.6752; Loss self: 0.0000; time: 0.39s
Val loss: 0.6782 score: 0.6512 time: 0.26s
Test loss: 0.6740 score: 0.7045 time: 0.29s
Epoch 46/1000, LR 0.000269
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.37s
Val loss: 0.6766 score: 0.6512 time: 0.27s
Test loss: 0.6722 score: 0.7045 time: 0.29s
Epoch 47/1000, LR 0.000269
Train loss: 0.6717;  Loss pred: 0.6717; Loss self: 0.0000; time: 0.37s
Val loss: 0.6749 score: 0.6512 time: 0.24s
Test loss: 0.6703 score: 0.7045 time: 0.30s
Epoch 48/1000, LR 0.000269
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.37s
Val loss: 0.6730 score: 0.6512 time: 0.25s
Test loss: 0.6681 score: 0.7045 time: 0.28s
Epoch 49/1000, LR 0.000269
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.36s
Val loss: 0.6709 score: 0.6512 time: 0.26s
Test loss: 0.6658 score: 0.7045 time: 0.28s
Epoch 50/1000, LR 0.000269
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.38s
Val loss: 0.6686 score: 0.6512 time: 0.24s
Test loss: 0.6633 score: 0.7045 time: 0.39s
Epoch 51/1000, LR 0.000269
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.44s
Val loss: 0.6660 score: 0.6744 time: 0.24s
Test loss: 0.6604 score: 0.7045 time: 0.30s
Epoch 52/1000, LR 0.000269
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.37s
Val loss: 0.6632 score: 0.6744 time: 0.25s
Test loss: 0.6573 score: 0.7045 time: 0.30s
Epoch 53/1000, LR 0.000269
Train loss: 0.6536;  Loss pred: 0.6536; Loss self: 0.0000; time: 0.36s
Val loss: 0.6601 score: 0.6977 time: 0.26s
Test loss: 0.6539 score: 0.7045 time: 0.29s
Epoch 54/1000, LR 0.000269
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.37s
Val loss: 0.6567 score: 0.7209 time: 0.24s
Test loss: 0.6503 score: 0.7273 time: 0.30s
Epoch 55/1000, LR 0.000269
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.36s
Val loss: 0.6530 score: 0.7209 time: 0.25s
Test loss: 0.6463 score: 0.7273 time: 0.30s
Epoch 56/1000, LR 0.000269
Train loss: 0.6437;  Loss pred: 0.6437; Loss self: 0.0000; time: 0.45s
Val loss: 0.6490 score: 0.7442 time: 0.26s
Test loss: 0.6420 score: 0.7273 time: 0.28s
Epoch 57/1000, LR 0.000269
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 0.37s
Val loss: 0.6445 score: 0.7442 time: 0.29s
Test loss: 0.6374 score: 0.7273 time: 0.31s
Epoch 58/1000, LR 0.000269
Train loss: 0.6345;  Loss pred: 0.6345; Loss self: 0.0000; time: 0.39s
Val loss: 0.6397 score: 0.7674 time: 0.24s
Test loss: 0.6324 score: 0.7273 time: 0.30s
Epoch 59/1000, LR 0.000268
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.36s
Val loss: 0.6345 score: 0.7907 time: 0.27s
Test loss: 0.6271 score: 0.7500 time: 0.29s
Epoch 60/1000, LR 0.000268
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.37s
Val loss: 0.6288 score: 0.7907 time: 0.27s
Test loss: 0.6213 score: 0.7500 time: 0.29s
Epoch 61/1000, LR 0.000268
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.38s
Val loss: 0.6226 score: 0.8140 time: 0.35s
Test loss: 0.6150 score: 0.7727 time: 0.30s
Epoch 62/1000, LR 0.000268
Train loss: 0.6049;  Loss pred: 0.6049; Loss self: 0.0000; time: 0.38s
Val loss: 0.6159 score: 0.8140 time: 0.24s
Test loss: 0.6083 score: 0.7727 time: 0.30s
Epoch 63/1000, LR 0.000268
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.34s
Val loss: 0.6086 score: 0.8372 time: 0.26s
Test loss: 0.6011 score: 0.7727 time: 0.28s
Epoch 64/1000, LR 0.000268
Train loss: 0.5901;  Loss pred: 0.5901; Loss self: 0.0000; time: 0.36s
Val loss: 0.6009 score: 0.8372 time: 0.26s
Test loss: 0.5935 score: 0.7727 time: 0.32s
Epoch 65/1000, LR 0.000268
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 0.38s
Val loss: 0.5926 score: 0.8372 time: 0.25s
Test loss: 0.5853 score: 0.7727 time: 0.29s
Epoch 66/1000, LR 0.000268
Train loss: 0.5649;  Loss pred: 0.5649; Loss self: 0.0000; time: 0.60s
Val loss: 0.5838 score: 0.8140 time: 0.27s
Test loss: 0.5767 score: 0.7955 time: 0.30s
Epoch 67/1000, LR 0.000268
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.61s
Val loss: 0.5746 score: 0.8140 time: 0.26s
Test loss: 0.5676 score: 0.8409 time: 0.32s
Epoch 68/1000, LR 0.000268
Train loss: 0.5467;  Loss pred: 0.5467; Loss self: 0.0000; time: 0.44s
Val loss: 0.5650 score: 0.8372 time: 0.37s
Test loss: 0.5583 score: 0.8409 time: 0.30s
Epoch 69/1000, LR 0.000268
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.36s
Val loss: 0.5551 score: 0.8605 time: 0.27s
Test loss: 0.5487 score: 0.8409 time: 0.28s
Epoch 70/1000, LR 0.000268
Train loss: 0.5319;  Loss pred: 0.5319; Loss self: 0.0000; time: 0.38s
Val loss: 0.5450 score: 0.8605 time: 0.41s
Test loss: 0.5390 score: 0.8409 time: 0.41s
Epoch 71/1000, LR 0.000268
Train loss: 0.5089;  Loss pred: 0.5089; Loss self: 0.0000; time: 0.37s
Val loss: 0.5347 score: 0.8605 time: 0.43s
Test loss: 0.5293 score: 0.8409 time: 0.28s
Epoch 72/1000, LR 0.000267
Train loss: 0.5037;  Loss pred: 0.5037; Loss self: 0.0000; time: 0.38s
Val loss: 0.5243 score: 0.8372 time: 0.32s
Test loss: 0.5197 score: 0.8409 time: 0.31s
Epoch 73/1000, LR 0.000267
Train loss: 0.4932;  Loss pred: 0.4932; Loss self: 0.0000; time: 0.50s
Val loss: 0.5139 score: 0.8372 time: 0.26s
Test loss: 0.5101 score: 0.8409 time: 0.33s
Epoch 74/1000, LR 0.000267
Train loss: 0.4765;  Loss pred: 0.4765; Loss self: 0.0000; time: 0.43s
Val loss: 0.5035 score: 0.8605 time: 0.25s
Test loss: 0.5008 score: 0.8182 time: 0.28s
Epoch 75/1000, LR 0.000267
Train loss: 0.4538;  Loss pred: 0.4538; Loss self: 0.0000; time: 0.39s
Val loss: 0.4932 score: 0.8605 time: 0.29s
Test loss: 0.4915 score: 0.8182 time: 0.28s
Epoch 76/1000, LR 0.000267
Train loss: 0.4476;  Loss pred: 0.4476; Loss self: 0.0000; time: 0.38s
Val loss: 0.4826 score: 0.9070 time: 0.26s
Test loss: 0.4815 score: 0.8409 time: 0.29s
Epoch 77/1000, LR 0.000267
Train loss: 0.4415;  Loss pred: 0.4415; Loss self: 0.0000; time: 0.41s
Val loss: 0.4721 score: 0.8837 time: 0.25s
Test loss: 0.4711 score: 0.8636 time: 0.40s
Epoch 78/1000, LR 0.000267
Train loss: 0.4268;  Loss pred: 0.4268; Loss self: 0.0000; time: 0.39s
Val loss: 0.4618 score: 0.8837 time: 0.25s
Test loss: 0.4609 score: 0.8636 time: 0.29s
Epoch 79/1000, LR 0.000267
Train loss: 0.4137;  Loss pred: 0.4137; Loss self: 0.0000; time: 0.35s
Val loss: 0.4517 score: 0.8837 time: 0.27s
Test loss: 0.4507 score: 0.8636 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.4068;  Loss pred: 0.4068; Loss self: 0.0000; time: 0.41s
Val loss: 0.4419 score: 0.8837 time: 0.26s
Test loss: 0.4406 score: 0.8636 time: 0.30s
Epoch 81/1000, LR 0.000267
Train loss: 0.3821;  Loss pred: 0.3821; Loss self: 0.0000; time: 0.40s
Val loss: 0.4323 score: 0.8837 time: 0.25s
Test loss: 0.4317 score: 0.8636 time: 0.30s
Epoch 82/1000, LR 0.000267
Train loss: 0.3690;  Loss pred: 0.3690; Loss self: 0.0000; time: 0.38s
Val loss: 0.4229 score: 0.8837 time: 0.25s
Test loss: 0.4238 score: 0.8636 time: 0.29s
Epoch 83/1000, LR 0.000266
Train loss: 0.3676;  Loss pred: 0.3676; Loss self: 0.0000; time: 0.37s
Val loss: 0.4138 score: 0.9302 time: 0.27s
Test loss: 0.4166 score: 0.8636 time: 0.38s
Epoch 84/1000, LR 0.000266
Train loss: 0.3627;  Loss pred: 0.3627; Loss self: 0.0000; time: 0.39s
Val loss: 0.4051 score: 0.9302 time: 0.28s
Test loss: 0.4098 score: 0.8636 time: 0.29s
Epoch 85/1000, LR 0.000266
Train loss: 0.3392;  Loss pred: 0.3392; Loss self: 0.0000; time: 0.37s
Val loss: 0.3967 score: 0.9302 time: 0.25s
Test loss: 0.4039 score: 0.8636 time: 0.29s
Epoch 86/1000, LR 0.000266
Train loss: 0.3149;  Loss pred: 0.3149; Loss self: 0.0000; time: 0.37s
Val loss: 0.3889 score: 0.9302 time: 0.28s
Test loss: 0.3990 score: 0.8636 time: 0.30s
Epoch 87/1000, LR 0.000266
Train loss: 0.2883;  Loss pred: 0.2883; Loss self: 0.0000; time: 0.33s
Val loss: 0.3818 score: 0.9302 time: 0.26s
Test loss: 0.3956 score: 0.8636 time: 0.28s
Epoch 88/1000, LR 0.000266
Train loss: 0.2894;  Loss pred: 0.2894; Loss self: 0.0000; time: 0.38s
Val loss: 0.3762 score: 0.9070 time: 0.26s
Test loss: 0.3957 score: 0.8409 time: 0.29s
Epoch 89/1000, LR 0.000266
Train loss: 0.3052;  Loss pred: 0.3052; Loss self: 0.0000; time: 0.51s
Val loss: 0.3694 score: 0.9070 time: 0.25s
Test loss: 0.3916 score: 0.8409 time: 0.30s
Epoch 90/1000, LR 0.000266
Train loss: 0.2775;  Loss pred: 0.2775; Loss self: 0.0000; time: 0.37s
Val loss: 0.3621 score: 0.9070 time: 0.25s
Test loss: 0.3844 score: 0.8409 time: 0.29s
Epoch 91/1000, LR 0.000266
Train loss: 0.2774;  Loss pred: 0.2774; Loss self: 0.0000; time: 0.37s
Val loss: 0.3548 score: 0.9070 time: 0.27s
Test loss: 0.3765 score: 0.8409 time: 0.30s
Epoch 92/1000, LR 0.000266
Train loss: 0.2492;  Loss pred: 0.2492; Loss self: 0.0000; time: 0.38s
Val loss: 0.3486 score: 0.9070 time: 0.26s
Test loss: 0.3709 score: 0.8409 time: 0.29s
Epoch 93/1000, LR 0.000265
Train loss: 0.2395;  Loss pred: 0.2395; Loss self: 0.0000; time: 0.45s
Val loss: 0.3421 score: 0.9070 time: 0.26s
Test loss: 0.3638 score: 0.8409 time: 0.30s
Epoch 94/1000, LR 0.000265
Train loss: 0.2241;  Loss pred: 0.2241; Loss self: 0.0000; time: 0.36s
Val loss: 0.3364 score: 0.9070 time: 0.34s
Test loss: 0.3583 score: 0.8636 time: 0.30s
Epoch 95/1000, LR 0.000265
Train loss: 0.2055;  Loss pred: 0.2055; Loss self: 0.0000; time: 0.34s
Val loss: 0.3312 score: 0.9070 time: 0.27s
Test loss: 0.3537 score: 0.8636 time: 0.28s
Epoch 96/1000, LR 0.000265
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.42s
Val loss: 0.3260 score: 0.9302 time: 0.29s
Test loss: 0.3474 score: 0.8636 time: 0.29s
Epoch 97/1000, LR 0.000265
Train loss: 0.1879;  Loss pred: 0.1879; Loss self: 0.0000; time: 0.38s
Val loss: 0.3224 score: 0.9070 time: 0.25s
Test loss: 0.3456 score: 0.8636 time: 0.30s
Epoch 98/1000, LR 0.000265
Train loss: 0.2059;  Loss pred: 0.2059; Loss self: 0.0000; time: 0.36s
Val loss: 0.3193 score: 0.9070 time: 0.26s
Test loss: 0.3439 score: 0.8636 time: 0.28s
Epoch 99/1000, LR 0.000265
Train loss: 0.1629;  Loss pred: 0.1629; Loss self: 0.0000; time: 0.38s
Val loss: 0.3181 score: 0.8837 time: 0.35s
Test loss: 0.3462 score: 0.8636 time: 0.28s
Epoch 100/1000, LR 0.000265
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.38s
Val loss: 0.3183 score: 0.8837 time: 0.34s
Test loss: 0.3509 score: 0.8864 time: 0.28s
     INFO: Early stopping counter 1 of 2
Epoch 101/1000, LR 0.000265
Train loss: 0.1568;  Loss pred: 0.1568; Loss self: 0.0000; time: 0.40s
Val loss: 0.3208 score: 0.8837 time: 0.25s
Test loss: 0.3598 score: 0.8636 time: 0.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 098,   Train_Loss: 0.1629,   Val_Loss: 0.3181,   Val_Precision: 0.8696,   Val_Recall: 0.9091,   Val_accuracy: 0.8889,   Val_Score: 0.8837,   Val_Loss: 0.3181,   Test_Precision: 0.8636,   Test_Recall: 0.8636,   Test_accuracy: 0.8636,   Test_Score: 0.8636,   Test_loss: 0.3462


[0.28178107494022697, 0.37869441299699247, 0.3005223999498412, 0.28709915408398956, 0.295045844046399, 0.3043468809919432, 0.3348340039374307, 0.31117341096978635, 0.39004076400306076, 0.28136962198186666, 0.3099887960124761, 0.33538012101780623, 0.30485707498155534, 0.3202938500326127, 0.39149055001325905, 0.3061388259520754, 0.28561337594874203, 0.29803693294525146, 0.3022486809641123, 0.2879478669492528, 0.28647347795777023, 0.3048172980779782, 0.2832450020359829, 0.2887693130178377, 0.3175503280945122, 0.3133027689764276, 0.2864482579752803, 0.2893905460368842, 0.2909245820483193, 0.30703137000091374, 0.3616959910141304, 0.2782217509811744, 0.2936680620769039, 0.3083051999565214, 0.28327887901104987, 0.2990240210201591, 0.30344756692647934, 0.31060497800353914, 0.30938048602547497, 0.302175085991621, 0.2895508899819106, 0.30144570604898036, 0.2977182869799435, 0.40948671591468155, 0.29389159905258566, 0.29207602399401367, 0.29935133503749967, 0.28158441092818975, 0.2856432719854638, 0.3952722770627588, 0.30413152708206326, 0.30922711302991956, 0.29353495698887855, 0.30231305002234876, 0.30001335800625384, 0.2843158630421385, 0.31198322493582964, 0.30401960806921124, 0.29732444195542485, 0.2971146749332547, 0.30664394691120833, 0.30007821798790246, 0.28638898802455515, 0.32019256602507085, 0.295755124068819, 0.31047637294977903, 0.32664183899760246, 0.30357926001306623, 0.281759002013132, 0.4123047840548679, 0.28792573406826705, 0.318166704964824, 0.33656936895567924, 0.28665707807522267, 0.2870193941053003, 0.2960998020134866, 0.4077825389103964, 0.2972784029552713, 0.29998854000587016, 0.3024137889733538, 0.3087407990824431, 0.2943020099774003, 0.3837469480931759, 0.29404215805698186, 0.29830051597673446, 0.3072665430372581, 0.28829723596572876, 0.29659136303234845, 0.30717282905243337, 0.29634835198521614, 0.29958002199418843, 0.2967634389642626, 0.3058213379699737, 0.3061945870285854, 0.2872552250046283, 0.29647378507070243, 0.305782631970942, 0.28766227804590017, 0.2831084170611575, 0.28401155304163694, 0.3027241170639172]
[0.006404115339550613, 0.008606691204477102, 0.006830054544314573, 0.006524980774636127, 0.006705587364690887, 0.006916974567998709, 0.007609863725850697, 0.0070721229765860535, 0.008864562818251381, 0.006394764135951515, 0.007045199909374456, 0.0076222754776774145, 0.006928569885944439, 0.007279405682559379, 0.008897512500301342, 0.006957700589819896, 0.0064912130897441375, 0.006773566657846624, 0.006869288203729825, 0.006544269703392109, 0.006510760862676596, 0.006927665865408595, 0.006437386409908702, 0.006562938932223584, 0.007217052911238914, 0.007120517476736991, 0.0065101876812563705, 0.006577057864474641, 0.006611922319279984, 0.006977985681838949, 0.008220363432139327, 0.0063232216132085095, 0.006674274138111452, 0.007006936362648214, 0.006438156341160225, 0.0067960004777308895, 0.00689653561196544, 0.007059204045534981, 0.007031374682397159, 0.006867615590718659, 0.006580702045043422, 0.0068510387738404625, 0.006766324704089625, 0.009306516270788217, 0.006679354523922401, 0.006638091454409401, 0.006803439432670447, 0.0063996457029134035, 0.0064918925451241775, 0.008983460842335428, 0.006912080160955983, 0.007027888932498172, 0.0066712490224745125, 0.006870751136871563, 0.0068184854092330415, 0.006461724160048602, 0.007090527839450674, 0.006909536547027528, 0.00675737368080511, 0.006752606248483062, 0.006969180611618372, 0.006819959499725056, 0.006508840636921708, 0.0072771037732970644, 0.006721707365200432, 0.007056281203404069, 0.00742367815903642, 0.006899528636660596, 0.006403613682116636, 0.00937056327397427, 0.006543766683369706, 0.007231061476473274, 0.0076493038399018005, 0.006514933592618697, 0.006523168047847735, 0.006729540954851968, 0.009267784975236282, 0.006756327339892529, 0.006817921363769777, 0.0068730406584853136, 0.007016836342782798, 0.006688682044940916, 0.00872152154757218, 0.00668277631947686, 0.0067795571812894195, 0.006983330523574047, 0.006552209908312017, 0.006740712796189738, 0.006981200660282577, 0.006735189817845821, 0.006808636863504283, 0.00674462361282415, 0.006950484953863038, 0.006958967887013304, 0.006528527841014279, 0.006738040569788692, 0.006949605272066864, 0.006537779046497731, 0.0064342822059353984, 0.006454808023673567, 0.006880093569634482]
[156.1495924072741, 116.1886695179457, 146.41171509126718, 153.2571565401687, 149.12936714024872, 144.57187751224166, 131.40839784068686, 141.40025609152133, 112.80872170493113, 156.37793337489595, 141.94061387376445, 131.1944186389745, 144.32992904187026, 137.37385215332694, 112.39096320079706, 143.7256442829894, 154.05440958023098, 147.632709695354, 145.57549055184916, 152.80543824189692, 153.59187982660703, 144.3487632671816, 155.34254685422596, 152.3707610762715, 138.56071339628508, 140.43923117484636, 153.605402633648, 152.04366764072518, 151.24194624671523, 143.30783202989667, 121.649122724949, 158.14723271933264, 149.82902699333223, 142.7157245683987, 155.32396962882535, 147.14536929136432, 145.00033875921807, 141.6590303311193, 142.21970029608448, 145.61094557352115, 151.95947076090445, 145.96326674114465, 147.7907200338156, 107.45159315294516, 149.71506549299878, 150.6457099707089, 146.98447893839568, 156.25865031008755, 154.03828591572474, 111.31567416506157, 144.67424808651145, 142.2902395875705, 149.89696781384396, 145.54449434699316, 146.66013637660365, 154.75745717880943, 141.0332238505777, 144.7275071481022, 147.98648812934297, 148.09096861299219, 143.4888914104038, 146.62843672903256, 153.63719221015376, 137.41730654844383, 148.77172504967888, 141.7177081204733, 134.70411547714485, 144.93743741949373, 156.161825125819, 106.71717065049783, 152.81718441175397, 138.2922829868844, 130.73085092836862, 153.49350623204847, 153.29974525643897, 148.59854583082736, 107.9006475303453, 148.0094065448147, 146.6722695444933, 145.49601110905996, 142.5143684630118, 149.50628438921908, 114.65889232118806, 149.63840658343054, 147.50225910917203, 143.1981483082091, 152.62026308580525, 148.35226336378864, 143.24183599093442, 148.47391492224332, 146.87227708679973, 148.2662424777287, 143.8748528538582, 143.69947041517196, 153.17388917570102, 148.41109809930404, 143.89306454848372, 152.95714230900737, 155.41749149229656, 154.92327522869982, 145.34686045747003]
Elapsed: 0.3076095499460226~0.029299142221937254
Time per graph: 0.006991126135136878~0.0006658895959531193
Speed: 144.1257382962932~11.481547932144727
Total Time: 0.3032
best val loss: 0.3181094229221344 test_score: 0.8636

Testing...
Test loss: 0.4166 score: 0.8636 time: 0.29s
test Score 0.8636
Epoch Time List: [0.8706781240180135, 1.0295234689256176, 0.9154979998711497, 0.8924768589204177, 0.953969235997647, 0.9483574300538749, 1.0669248071499169, 0.9327782469335943, 1.0388739060144871, 0.951056034071371, 0.9196552491048351, 0.9675171020207927, 0.9410809099208564, 1.0708984368247911, 1.0257385168224573, 0.9556800830177963, 0.8918109810911119, 0.9261253050062805, 0.9436411019414663, 0.8893170630326495, 1.0137605470372364, 0.9240157080348581, 0.8818954710150138, 0.9263202119618654, 0.938967770896852, 1.0763065110659227, 0.8804489210015163, 0.9517074789619073, 0.925247959094122, 1.0000186810502782, 1.0717517390148714, 0.9488994468702003, 0.991239273105748, 0.9609572750050575, 0.8996746109332889, 0.9523984330007806, 0.9057460500625893, 1.123586878995411, 1.1672107491176575, 1.1492242398671806, 0.9990534069947898, 0.995803717058152, 0.9237391909118742, 1.0941665141144767, 0.9356556790880859, 0.9206605979707092, 0.9090922579634935, 0.8949215391185135, 0.8961892430670559, 1.0144473270047456, 0.9814178729429841, 0.92453745205421, 0.9131073819007725, 0.9112601579399779, 0.9099799250252545, 0.9933741078712046, 0.9733625119552016, 0.9377681639743969, 0.9185209851711988, 0.9367574600037187, 1.0311023460235447, 0.9179757511010394, 0.8822328639216721, 0.9367624400183558, 0.9220999181270599, 1.1768681730609387, 1.1918510519899428, 1.1098458240740001, 0.903132118168287, 1.199015511898324, 1.079015028080903, 1.0169245870783925, 1.0928513490362093, 0.964032699004747, 0.9675315279746428, 0.9258888129843399, 1.0643326170975342, 0.9320032831747085, 0.920220822095871, 0.9654542350908741, 0.9519244940020144, 0.917692034970969, 1.0144494229461998, 0.9611111261183396, 0.9146477250615135, 0.9554470779839903, 0.8795305701205507, 0.9295457489788532, 1.0586415810976177, 0.9158080370398238, 0.9302746799075976, 0.9295724249677733, 1.0095390460919589, 1.0015949570806697, 0.8897756548831239, 0.9950349730206653, 0.9240747910225764, 0.8983584009110928, 1.0135174121242017, 0.9960263160755858, 0.9452827371424064]
Total Epoch List: [101]
Total Time List: [0.3031989319715649]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcc070>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.26s
Epoch 2/1000, LR 0.000000
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 3/1000, LR 0.000030
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.26s
Epoch 4/1000, LR 0.000060
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.26s
Epoch 7/1000, LR 0.000150
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.26s
Epoch 13/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.26s
Epoch 16/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.27s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.41s
Val loss: 0.6919 score: 0.5455 time: 0.36s
Test loss: 0.6924 score: 0.5581 time: 0.24s
Epoch 19/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.43s
Val loss: 0.6918 score: 0.7045 time: 0.26s
Test loss: 0.6923 score: 0.6744 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.42s
Val loss: 0.6916 score: 0.7045 time: 0.24s
Test loss: 0.6923 score: 0.4884 time: 0.27s
Epoch 21/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.38s
Val loss: 0.6914 score: 0.6591 time: 0.26s
Test loss: 0.6922 score: 0.6047 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.49s
Val loss: 0.6911 score: 0.6364 time: 0.27s
Test loss: 0.6921 score: 0.6047 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.41s
Val loss: 0.6909 score: 0.6364 time: 0.35s
Test loss: 0.6920 score: 0.6047 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.42s
Val loss: 0.6906 score: 0.6591 time: 0.24s
Test loss: 0.6918 score: 0.5814 time: 0.27s
Epoch 25/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.40s
Val loss: 0.6903 score: 0.6591 time: 0.28s
Test loss: 0.6916 score: 0.5814 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.49s
Val loss: 0.6899 score: 0.6591 time: 0.27s
Test loss: 0.6914 score: 0.5814 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.42s
Val loss: 0.6895 score: 0.6591 time: 0.26s
Test loss: 0.6912 score: 0.6047 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.43s
Val loss: 0.6890 score: 0.6591 time: 0.38s
Test loss: 0.6909 score: 0.6047 time: 0.32s
Epoch 29/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.47s
Val loss: 0.6885 score: 0.6591 time: 0.25s
Test loss: 0.6906 score: 0.6047 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.40s
Val loss: 0.6879 score: 0.6364 time: 0.26s
Test loss: 0.6903 score: 0.6047 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.40s
Val loss: 0.6873 score: 0.6818 time: 0.27s
Test loss: 0.6899 score: 0.6047 time: 0.29s
Epoch 32/1000, LR 0.000270
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.40s
Val loss: 0.6865 score: 0.6818 time: 0.27s
Test loss: 0.6894 score: 0.6047 time: 0.26s
Epoch 33/1000, LR 0.000270
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.48s
Val loss: 0.6856 score: 0.7045 time: 0.34s
Test loss: 0.6889 score: 0.6279 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.36s
Val loss: 0.6847 score: 0.7273 time: 0.29s
Test loss: 0.6883 score: 0.6047 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.41s
Val loss: 0.6837 score: 0.7273 time: 0.26s
Test loss: 0.6876 score: 0.6279 time: 0.28s
Epoch 36/1000, LR 0.000270
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.40s
Val loss: 0.6826 score: 0.7500 time: 0.25s
Test loss: 0.6869 score: 0.5581 time: 0.27s
Epoch 37/1000, LR 0.000270
Train loss: 0.6762;  Loss pred: 0.6762; Loss self: 0.0000; time: 0.51s
Val loss: 0.6814 score: 0.7500 time: 0.24s
Test loss: 0.6860 score: 0.5814 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.38s
Val loss: 0.6801 score: 0.7955 time: 0.35s
Test loss: 0.6852 score: 0.6047 time: 0.33s
Epoch 39/1000, LR 0.000269
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.40s
Val loss: 0.6786 score: 0.8182 time: 0.26s
Test loss: 0.6842 score: 0.6279 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.48s
Val loss: 0.6771 score: 0.8409 time: 0.27s
Test loss: 0.6831 score: 0.6744 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.41s
Val loss: 0.6754 score: 0.8409 time: 0.24s
Test loss: 0.6820 score: 0.6512 time: 0.27s
Epoch 42/1000, LR 0.000269
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.39s
Val loss: 0.6736 score: 0.8182 time: 0.25s
Test loss: 0.6808 score: 0.6512 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.40s
Val loss: 0.6716 score: 0.8182 time: 0.36s
Test loss: 0.6794 score: 0.6512 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.48s
Val loss: 0.6695 score: 0.8182 time: 0.27s
Test loss: 0.6780 score: 0.6744 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.40s
Val loss: 0.6673 score: 0.8182 time: 0.24s
Test loss: 0.6764 score: 0.6512 time: 0.27s
Epoch 46/1000, LR 0.000269
Train loss: 0.6498;  Loss pred: 0.6498; Loss self: 0.0000; time: 0.50s
Val loss: 0.6649 score: 0.7955 time: 0.26s
Test loss: 0.6747 score: 0.6512 time: 0.27s
Epoch 47/1000, LR 0.000269
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.36s
Val loss: 0.6623 score: 0.8182 time: 0.29s
Test loss: 0.6729 score: 0.6279 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.42s
Val loss: 0.6595 score: 0.8636 time: 0.36s
Test loss: 0.6709 score: 0.6279 time: 0.26s
Epoch 49/1000, LR 0.000269
Train loss: 0.6376;  Loss pred: 0.6376; Loss self: 0.0000; time: 0.41s
Val loss: 0.6564 score: 0.8636 time: 0.26s
Test loss: 0.6688 score: 0.6744 time: 0.26s
Epoch 50/1000, LR 0.000269
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.41s
Val loss: 0.6531 score: 0.8636 time: 0.26s
Test loss: 0.6666 score: 0.7209 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.40s
Val loss: 0.6495 score: 0.8636 time: 0.27s
Test loss: 0.6642 score: 0.7209 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6201;  Loss pred: 0.6201; Loss self: 0.0000; time: 0.41s
Val loss: 0.6457 score: 0.8864 time: 0.25s
Test loss: 0.6616 score: 0.7442 time: 0.26s
Epoch 53/1000, LR 0.000269
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.41s
Val loss: 0.6415 score: 0.8636 time: 0.34s
Test loss: 0.6587 score: 0.7442 time: 0.27s
Epoch 54/1000, LR 0.000269
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.37s
Val loss: 0.6371 score: 0.8636 time: 0.27s
Test loss: 0.6556 score: 0.7674 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 0.41s
Val loss: 0.6324 score: 0.8636 time: 0.26s
Test loss: 0.6522 score: 0.7674 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.46s
Val loss: 0.6274 score: 0.8636 time: 0.28s
Test loss: 0.6487 score: 0.7674 time: 0.27s
Epoch 57/1000, LR 0.000269
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.42s
Val loss: 0.6222 score: 0.8636 time: 0.25s
Test loss: 0.6450 score: 0.7674 time: 0.27s
Epoch 58/1000, LR 0.000269
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 0.39s
Val loss: 0.6167 score: 0.8409 time: 0.35s
Test loss: 0.6411 score: 0.7674 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.41s
Val loss: 0.6109 score: 0.8409 time: 0.26s
Test loss: 0.6370 score: 0.7674 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5500;  Loss pred: 0.5500; Loss self: 0.0000; time: 0.43s
Val loss: 0.6047 score: 0.8182 time: 0.26s
Test loss: 0.6328 score: 0.7674 time: 0.27s
Epoch 61/1000, LR 0.000268
Train loss: 0.5438;  Loss pred: 0.5438; Loss self: 0.0000; time: 0.37s
Val loss: 0.5981 score: 0.8182 time: 0.26s
Test loss: 0.6281 score: 0.7907 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.5309;  Loss pred: 0.5309; Loss self: 0.0000; time: 0.41s
Val loss: 0.5912 score: 0.8182 time: 0.31s
Test loss: 0.6232 score: 0.7907 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.44s
Val loss: 0.5840 score: 0.8182 time: 0.33s
Test loss: 0.6179 score: 0.7907 time: 0.26s
Epoch 64/1000, LR 0.000268
Train loss: 0.5007;  Loss pred: 0.5007; Loss self: 0.0000; time: 0.42s
Val loss: 0.5764 score: 0.8182 time: 0.24s
Test loss: 0.6122 score: 0.7907 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.4894;  Loss pred: 0.4894; Loss self: 0.0000; time: 0.39s
Val loss: 0.5685 score: 0.8182 time: 0.27s
Test loss: 0.6064 score: 0.7674 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.4707;  Loss pred: 0.4707; Loss self: 0.0000; time: 0.41s
Val loss: 0.5605 score: 0.7955 time: 0.26s
Test loss: 0.6004 score: 0.7674 time: 0.28s
Epoch 67/1000, LR 0.000268
Train loss: 0.4628;  Loss pred: 0.4628; Loss self: 0.0000; time: 0.42s
Val loss: 0.5522 score: 0.7955 time: 0.25s
Test loss: 0.5942 score: 0.7674 time: 0.27s
Epoch 68/1000, LR 0.000268
Train loss: 0.4413;  Loss pred: 0.4413; Loss self: 0.0000; time: 0.40s
Val loss: 0.5437 score: 0.7955 time: 0.35s
Test loss: 0.5878 score: 0.7674 time: 0.30s
Epoch 69/1000, LR 0.000268
Train loss: 0.4267;  Loss pred: 0.4267; Loss self: 0.0000; time: 0.37s
Val loss: 0.5351 score: 0.7955 time: 0.26s
Test loss: 0.5813 score: 0.7674 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.4156;  Loss pred: 0.4156; Loss self: 0.0000; time: 0.41s
Val loss: 0.5266 score: 0.7955 time: 0.27s
Test loss: 0.5752 score: 0.7674 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.3935;  Loss pred: 0.3935; Loss self: 0.0000; time: 0.45s
Val loss: 0.5180 score: 0.7955 time: 0.24s
Test loss: 0.5688 score: 0.7907 time: 0.29s
Epoch 72/1000, LR 0.000267
Train loss: 0.3848;  Loss pred: 0.3848; Loss self: 0.0000; time: 0.41s
Val loss: 0.5096 score: 0.7955 time: 0.26s
Test loss: 0.5627 score: 0.7907 time: 0.26s
Epoch 73/1000, LR 0.000267
Train loss: 0.3503;  Loss pred: 0.3503; Loss self: 0.0000; time: 0.39s
Val loss: 0.5014 score: 0.7955 time: 0.35s
Test loss: 0.5566 score: 0.7907 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.3450;  Loss pred: 0.3450; Loss self: 0.0000; time: 0.41s
Val loss: 0.4934 score: 0.7955 time: 0.26s
Test loss: 0.5505 score: 0.7907 time: 0.26s
Epoch 75/1000, LR 0.000267
Train loss: 0.3269;  Loss pred: 0.3269; Loss self: 0.0000; time: 0.47s
Val loss: 0.4857 score: 0.7955 time: 0.26s
Test loss: 0.5447 score: 0.7907 time: 0.26s
Epoch 76/1000, LR 0.000267
Train loss: 0.3168;  Loss pred: 0.3168; Loss self: 0.0000; time: 0.41s
Val loss: 0.4785 score: 0.7955 time: 0.26s
Test loss: 0.5391 score: 0.7907 time: 0.27s
Epoch 77/1000, LR 0.000267
Train loss: 0.2876;  Loss pred: 0.2876; Loss self: 0.0000; time: 0.39s
Val loss: 0.4718 score: 0.7955 time: 0.28s
Test loss: 0.5336 score: 0.7907 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.2777;  Loss pred: 0.2777; Loss self: 0.0000; time: 0.43s
Val loss: 0.4661 score: 0.7955 time: 0.40s
Test loss: 0.5295 score: 0.7907 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.2599;  Loss pred: 0.2599; Loss self: 0.0000; time: 0.43s
Val loss: 0.4611 score: 0.7955 time: 0.27s
Test loss: 0.5258 score: 0.7907 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.2472;  Loss pred: 0.2472; Loss self: 0.0000; time: 0.46s
Val loss: 0.4575 score: 0.7955 time: 0.25s
Test loss: 0.5239 score: 0.7907 time: 0.27s
Epoch 81/1000, LR 0.000267
Train loss: 0.2235;  Loss pred: 0.2235; Loss self: 0.0000; time: 0.44s
Val loss: 0.4552 score: 0.7955 time: 0.25s
Test loss: 0.5235 score: 0.7907 time: 0.27s
Epoch 82/1000, LR 0.000267
Train loss: 0.2068;  Loss pred: 0.2068; Loss self: 0.0000; time: 0.39s
Val loss: 0.4538 score: 0.7955 time: 0.28s
Test loss: 0.5237 score: 0.7907 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.2013;  Loss pred: 0.2013; Loss self: 0.0000; time: 0.44s
Val loss: 0.4531 score: 0.8182 time: 0.37s
Test loss: 0.5242 score: 0.7907 time: 0.25s
Epoch 84/1000, LR 0.000266
Train loss: 0.1897;  Loss pred: 0.1897; Loss self: 0.0000; time: 0.44s
Val loss: 0.4537 score: 0.8182 time: 0.27s
Test loss: 0.5258 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 85/1000, LR 0.000266
Train loss: 0.1850;  Loss pred: 0.1850; Loss self: 0.0000; time: 0.49s
Val loss: 0.4558 score: 0.8182 time: 0.27s
Test loss: 0.5293 score: 0.7907 time: 0.26s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 082,   Train_Loss: 0.2013,   Val_Loss: 0.4531,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4531,   Test_Precision: 0.9333,   Test_Recall: 0.6364,   Test_accuracy: 0.7568,   Test_Score: 0.7907,   Test_loss: 0.5242


[0.28178107494022697, 0.37869441299699247, 0.3005223999498412, 0.28709915408398956, 0.295045844046399, 0.3043468809919432, 0.3348340039374307, 0.31117341096978635, 0.39004076400306076, 0.28136962198186666, 0.3099887960124761, 0.33538012101780623, 0.30485707498155534, 0.3202938500326127, 0.39149055001325905, 0.3061388259520754, 0.28561337594874203, 0.29803693294525146, 0.3022486809641123, 0.2879478669492528, 0.28647347795777023, 0.3048172980779782, 0.2832450020359829, 0.2887693130178377, 0.3175503280945122, 0.3133027689764276, 0.2864482579752803, 0.2893905460368842, 0.2909245820483193, 0.30703137000091374, 0.3616959910141304, 0.2782217509811744, 0.2936680620769039, 0.3083051999565214, 0.28327887901104987, 0.2990240210201591, 0.30344756692647934, 0.31060497800353914, 0.30938048602547497, 0.302175085991621, 0.2895508899819106, 0.30144570604898036, 0.2977182869799435, 0.40948671591468155, 0.29389159905258566, 0.29207602399401367, 0.29935133503749967, 0.28158441092818975, 0.2856432719854638, 0.3952722770627588, 0.30413152708206326, 0.30922711302991956, 0.29353495698887855, 0.30231305002234876, 0.30001335800625384, 0.2843158630421385, 0.31198322493582964, 0.30401960806921124, 0.29732444195542485, 0.2971146749332547, 0.30664394691120833, 0.30007821798790246, 0.28638898802455515, 0.32019256602507085, 0.295755124068819, 0.31047637294977903, 0.32664183899760246, 0.30357926001306623, 0.281759002013132, 0.4123047840548679, 0.28792573406826705, 0.318166704964824, 0.33656936895567924, 0.28665707807522267, 0.2870193941053003, 0.2960998020134866, 0.4077825389103964, 0.2972784029552713, 0.29998854000587016, 0.3024137889733538, 0.3087407990824431, 0.2943020099774003, 0.3837469480931759, 0.29404215805698186, 0.29830051597673446, 0.3072665430372581, 0.28829723596572876, 0.29659136303234845, 0.30717282905243337, 0.29634835198521614, 0.29958002199418843, 0.2967634389642626, 0.3058213379699737, 0.3061945870285854, 0.2872552250046283, 0.29647378507070243, 0.305782631970942, 0.28766227804590017, 0.2831084170611575, 0.28401155304163694, 0.3027241170639172, 0.26315413194242865, 0.24573173990938812, 0.2594443749403581, 0.2559352139942348, 0.2553417419549078, 0.2596940740477294, 0.2551981440046802, 0.2591241030022502, 0.247246581944637, 0.24851109308656305, 0.255798168014735, 0.2690067969961092, 0.26198142499197274, 0.24600741104222834, 0.26120822690427303, 0.2749569689622149, 0.25346704199910164, 0.24526556301862001, 0.2591354950563982, 0.2744623509934172, 0.25042889604810625, 0.2456864189589396, 0.2563212759559974, 0.27247606590390205, 0.2579169680830091, 0.25124818400945514, 0.2511310769477859, 0.3257969889091328, 0.26865075703244656, 0.26153128396254033, 0.2972547340905294, 0.2597943090368062, 0.26009927596896887, 0.2464166529243812, 0.2885289720725268, 0.2695499010151252, 0.26829211600124836, 0.33585561404470354, 0.26206232397817075, 0.2537927740486339, 0.2754407830070704, 0.26081023400183767, 0.25089325406588614, 0.2587801100453362, 0.2696063829353079, 0.27133648993913084, 0.24781193805392832, 0.26203142502345145, 0.2603026209399104, 0.264103636960499, 0.24718192499130964, 0.2623675720533356, 0.27611446497030556, 0.25326527596917003, 0.25962197000626475, 0.2774359530303627, 0.27533549710642546, 0.2458628339227289, 0.2513102199882269, 0.2753364280797541, 0.2455991799943149, 0.2556408980162814, 0.2609505510190502, 0.2683852339396253, 0.2455516739282757, 0.2800574420252815, 0.2760396999074146, 0.3030269810697064, 0.25138610089197755, 0.24928276299033314, 0.29157554206904024, 0.26772156299557537, 0.24562353699002415, 0.2599491219734773, 0.2670339070027694, 0.2752796980785206, 0.24848505097907037, 0.25274620798882097, 0.25715504598338157, 0.27376032795291394, 0.2772790709277615, 0.25580379797611386, 0.2537251920439303, 0.24635371298063546, 0.2626608199207112]
[0.006404115339550613, 0.008606691204477102, 0.006830054544314573, 0.006524980774636127, 0.006705587364690887, 0.006916974567998709, 0.007609863725850697, 0.0070721229765860535, 0.008864562818251381, 0.006394764135951515, 0.007045199909374456, 0.0076222754776774145, 0.006928569885944439, 0.007279405682559379, 0.008897512500301342, 0.006957700589819896, 0.0064912130897441375, 0.006773566657846624, 0.006869288203729825, 0.006544269703392109, 0.006510760862676596, 0.006927665865408595, 0.006437386409908702, 0.006562938932223584, 0.007217052911238914, 0.007120517476736991, 0.0065101876812563705, 0.006577057864474641, 0.006611922319279984, 0.006977985681838949, 0.008220363432139327, 0.0063232216132085095, 0.006674274138111452, 0.007006936362648214, 0.006438156341160225, 0.0067960004777308895, 0.00689653561196544, 0.007059204045534981, 0.007031374682397159, 0.006867615590718659, 0.006580702045043422, 0.0068510387738404625, 0.006766324704089625, 0.009306516270788217, 0.006679354523922401, 0.006638091454409401, 0.006803439432670447, 0.0063996457029134035, 0.0064918925451241775, 0.008983460842335428, 0.006912080160955983, 0.007027888932498172, 0.0066712490224745125, 0.006870751136871563, 0.0068184854092330415, 0.006461724160048602, 0.007090527839450674, 0.006909536547027528, 0.00675737368080511, 0.006752606248483062, 0.006969180611618372, 0.006819959499725056, 0.006508840636921708, 0.0072771037732970644, 0.006721707365200432, 0.007056281203404069, 0.00742367815903642, 0.006899528636660596, 0.006403613682116636, 0.00937056327397427, 0.006543766683369706, 0.007231061476473274, 0.0076493038399018005, 0.006514933592618697, 0.006523168047847735, 0.006729540954851968, 0.009267784975236282, 0.006756327339892529, 0.006817921363769777, 0.0068730406584853136, 0.007016836342782798, 0.006688682044940916, 0.00872152154757218, 0.00668277631947686, 0.0067795571812894195, 0.006983330523574047, 0.006552209908312017, 0.006740712796189738, 0.006981200660282577, 0.006735189817845821, 0.006808636863504283, 0.00674462361282415, 0.006950484953863038, 0.006958967887013304, 0.006528527841014279, 0.006738040569788692, 0.006949605272066864, 0.006537779046497731, 0.0064342822059353984, 0.006454808023673567, 0.006880093569634482, 0.006119863533544852, 0.005714691625799724, 0.006033590114892049, 0.005951981720796158, 0.005938180045462971, 0.006039397070877427, 0.0059348405582483775, 0.006026141930284889, 0.005749920510340395, 0.005779327746199141, 0.005948794604993837, 0.00625597202316533, 0.006092591278883087, 0.005721102582377403, 0.00607460992800635, 0.006394348115400347, 0.0058945823720721316, 0.005703850302758605, 0.006026406861776703, 0.006382845371939935, 0.0058239278150722384, 0.005713637650207898, 0.005960959905953428, 0.006336652695439583, 0.005998069025186258, 0.005842981023475701, 0.005840257603436882, 0.007576674160677506, 0.006247692024010385, 0.006082122882849775, 0.006912900792803009, 0.006041728117135029, 0.006048820371371369, 0.005730619835450725, 0.006709976094709926, 0.006268602349188959, 0.006239351534912753, 0.0078105956754582215, 0.006094472650655134, 0.005902157536014741, 0.006405599604815591, 0.006065354279112504, 0.005834726838741538, 0.006018142094077586, 0.006269915882216463, 0.006310150928816996, 0.005763068326835542, 0.006093754070312824, 0.006053549324183963, 0.006141945045593, 0.005748416860263015, 0.006101571443100828, 0.006421266627216408, 0.005889890138817908, 0.006037720232703831, 0.006451998907682854, 0.006403151095498267, 0.005717740323784393, 0.0058444237206564395, 0.006403172746040794, 0.005711608837077091, 0.005945137163169335, 0.0060686174655593065, 0.0062415170683633794, 0.005710504044843621, 0.006512963768029802, 0.006419527904823596, 0.007047139094644334, 0.005846188392836687, 0.005797273557914724, 0.006780826559745122, 0.006226082860362218, 0.005712175278837771, 0.006045328417987844, 0.0062100908605295215, 0.006401853443686526, 0.005778722115792334, 0.005877818790437697, 0.005980349906590269, 0.006366519254718928, 0.006448350486692128, 0.005948925534328229, 0.005900585861486751, 0.005729156115828732, 0.006108391160946773]
[156.1495924072741, 116.1886695179457, 146.41171509126718, 153.2571565401687, 149.12936714024872, 144.57187751224166, 131.40839784068686, 141.40025609152133, 112.80872170493113, 156.37793337489595, 141.94061387376445, 131.1944186389745, 144.32992904187026, 137.37385215332694, 112.39096320079706, 143.7256442829894, 154.05440958023098, 147.632709695354, 145.57549055184916, 152.80543824189692, 153.59187982660703, 144.3487632671816, 155.34254685422596, 152.3707610762715, 138.56071339628508, 140.43923117484636, 153.605402633648, 152.04366764072518, 151.24194624671523, 143.30783202989667, 121.649122724949, 158.14723271933264, 149.82902699333223, 142.7157245683987, 155.32396962882535, 147.14536929136432, 145.00033875921807, 141.6590303311193, 142.21970029608448, 145.61094557352115, 151.95947076090445, 145.96326674114465, 147.7907200338156, 107.45159315294516, 149.71506549299878, 150.6457099707089, 146.98447893839568, 156.25865031008755, 154.03828591572474, 111.31567416506157, 144.67424808651145, 142.2902395875705, 149.89696781384396, 145.54449434699316, 146.66013637660365, 154.75745717880943, 141.0332238505777, 144.7275071481022, 147.98648812934297, 148.09096861299219, 143.4888914104038, 146.62843672903256, 153.63719221015376, 137.41730654844383, 148.77172504967888, 141.7177081204733, 134.70411547714485, 144.93743741949373, 156.161825125819, 106.71717065049783, 152.81718441175397, 138.2922829868844, 130.73085092836862, 153.49350623204847, 153.29974525643897, 148.59854583082736, 107.9006475303453, 148.0094065448147, 146.6722695444933, 145.49601110905996, 142.5143684630118, 149.50628438921908, 114.65889232118806, 149.63840658343054, 147.50225910917203, 143.1981483082091, 152.62026308580525, 148.35226336378864, 143.24183599093442, 148.47391492224332, 146.87227708679973, 148.2662424777287, 143.8748528538582, 143.69947041517196, 153.17388917570102, 148.41109809930404, 143.89306454848372, 152.95714230900737, 155.41749149229656, 154.92327522869982, 145.34686045747003, 163.40233642771489, 174.98756984285365, 165.73880243071363, 168.011268668049, 168.40176490843245, 165.57944249469858, 168.49652323181238, 165.94365210258573, 173.91544773560705, 173.0305052620808, 168.1012820917585, 159.84726215160256, 164.13377399301322, 174.79148216644109, 164.61962362218605, 156.38810742749038, 169.64730270593682, 175.32016917000098, 165.93635692648544, 156.6699397726551, 171.7054248873097, 175.01984921350652, 167.75821608886574, 157.81202601172836, 166.72032212382666, 171.14551561647028, 171.22532393288932, 131.9840313563887, 160.0591060117751, 164.41627689236205, 144.6570737773491, 165.5155579020324, 165.32149057243095, 174.50119336372762, 149.03182751848988, 159.525193064668, 160.27306594354013, 128.03120806036773, 164.08310567978407, 169.42956772977305, 156.11341040551795, 164.87083094943668, 171.38762921345, 166.1642387912531, 159.49177290182277, 158.47481483101012, 173.51867846916412, 164.10245449053127, 165.19234360658373, 162.81487258136983, 173.96093990898316, 163.89220536468855, 155.7325147910103, 169.78245373532528, 165.6254283832851, 154.99072679774153, 156.17310681658748, 174.8942665059912, 171.10326831123072, 156.17257876079003, 175.08201778603396, 168.20469781506318, 164.78217743583485, 160.21745819918348, 175.11589032196972, 153.53992984095842, 155.77469477912945, 141.90155559154172, 171.05162078343153, 172.49487884434066, 147.47464651825396, 160.6146308084667, 175.06465596473524, 165.41698496056972, 161.0282397566615, 156.2047630106551, 173.04863946773943, 170.13113803828819, 167.2142960896005, 157.07169961966108, 155.07841921182228, 168.09758236668247, 169.47469683087255, 174.5457759890958, 163.7092277903507]
Elapsed: 0.28719404255420533~0.032868893228496274
Time per graph: 0.006590646183475039~0.0007040561527254744
Speed: 153.25094844215988~14.436944297144167
Total Time: 0.2633
best val loss: 0.45313408970832825 test_score: 0.7907

Testing...
Test loss: 0.6616 score: 0.7442 time: 0.25s
test Score 0.7442
Epoch Time List: [0.8706781240180135, 1.0295234689256176, 0.9154979998711497, 0.8924768589204177, 0.953969235997647, 0.9483574300538749, 1.0669248071499169, 0.9327782469335943, 1.0388739060144871, 0.951056034071371, 0.9196552491048351, 0.9675171020207927, 0.9410809099208564, 1.0708984368247911, 1.0257385168224573, 0.9556800830177963, 0.8918109810911119, 0.9261253050062805, 0.9436411019414663, 0.8893170630326495, 1.0137605470372364, 0.9240157080348581, 0.8818954710150138, 0.9263202119618654, 0.938967770896852, 1.0763065110659227, 0.8804489210015163, 0.9517074789619073, 0.925247959094122, 1.0000186810502782, 1.0717517390148714, 0.9488994468702003, 0.991239273105748, 0.9609572750050575, 0.8996746109332889, 0.9523984330007806, 0.9057460500625893, 1.123586878995411, 1.1672107491176575, 1.1492242398671806, 0.9990534069947898, 0.995803717058152, 0.9237391909118742, 1.0941665141144767, 0.9356556790880859, 0.9206605979707092, 0.9090922579634935, 0.8949215391185135, 0.8961892430670559, 1.0144473270047456, 0.9814178729429841, 0.92453745205421, 0.9131073819007725, 0.9112601579399779, 0.9099799250252545, 0.9933741078712046, 0.9733625119552016, 0.9377681639743969, 0.9185209851711988, 0.9367574600037187, 1.0311023460235447, 0.9179757511010394, 0.8822328639216721, 0.9367624400183558, 0.9220999181270599, 1.1768681730609387, 1.1918510519899428, 1.1098458240740001, 0.903132118168287, 1.199015511898324, 1.079015028080903, 1.0169245870783925, 1.0928513490362093, 0.964032699004747, 0.9675315279746428, 0.9258888129843399, 1.0643326170975342, 0.9320032831747085, 0.920220822095871, 0.9654542350908741, 0.9519244940020144, 0.917692034970969, 1.0144494229461998, 0.9611111261183396, 0.9146477250615135, 0.9554470779839903, 0.8795305701205507, 0.9295457489788532, 1.0586415810976177, 0.9158080370398238, 0.9302746799075976, 0.9295724249677733, 1.0095390460919589, 1.0015949570806697, 0.8897756548831239, 0.9950349730206653, 0.9240747910225764, 0.8983584009110928, 1.0135174121242017, 0.9960263160755858, 0.9452827371424064, 0.9778026950079948, 0.8946950959507376, 1.0648322949418798, 0.8932018599007279, 0.8979506629984826, 0.928911785944365, 0.9140475739259273, 1.0170003800885752, 0.8679085109615698, 0.9449964389204979, 1.0181939739268273, 0.9284387000370771, 0.9808508040150627, 0.9369705439312384, 0.9337823790265247, 0.9337962581776083, 0.9002168299630284, 1.0108719459967688, 0.9518404511036351, 0.9341480721486732, 0.8810477949446067, 0.9980326029472053, 1.008760603144765, 0.9274619449861348, 0.9318189831683412, 1.0139435458695516, 0.9277943609049544, 1.137218720978126, 0.9833172338549048, 0.9125186888268217, 0.9644567780196667, 0.9269165571313351, 1.0704027902102098, 0.8886703789466992, 0.9547667980659753, 0.9174608680186793, 1.017217315849848, 1.0607032789848745, 0.9221883360296488, 0.9995441280771047, 0.9264817788498476, 0.9054369420045987, 1.0030469910707325, 1.0097964729648083, 0.9101820340147242, 1.0248971560504287, 0.8933760789223015, 1.0326269040815532, 0.922569768037647, 0.9328555350657552, 0.9122854759916663, 0.9254105569561943, 1.0260544999036938, 0.8927290819119662, 0.9344176249578595, 1.0103657199069858, 0.9435897328658029, 0.9812658040318638, 0.9131419589975849, 0.9558018329553306, 0.8781277920352295, 0.9638999211601913, 1.021214202977717, 0.9232479850761592, 0.9006772260181606, 0.9481131010688841, 0.9421384509187192, 1.044968482106924, 0.8808042739983648, 0.9287259579868987, 0.9806221929611638, 0.9272256240947172, 0.9806260401383042, 0.9268117097672075, 0.9962164082098752, 0.9394945349777117, 0.918266900931485, 1.0731759861810133, 0.952902555000037, 0.9801587570691481, 0.9614609059644863, 0.9235276099061593, 1.0574245419120416, 0.9493938130326569, 1.0180996470153332]
Total Epoch List: [101, 85]
Total Time List: [0.3031989319715649, 0.2632948210230097]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcc370>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.4884 time: 0.35s
Epoch 2/1000, LR 0.000000
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.4884 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4884 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4884 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4884 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.34s
Epoch 8/1000, LR 0.000180
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4884 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4884 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4884 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.27s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.30s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.26s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.23s
Epoch 20/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.26s
Epoch 22/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.29s
Epoch 27/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4884 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4884 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.4884 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4884 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6856 score: 0.4884 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.4884 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.4884 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6822 score: 0.4884 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6811 score: 0.5000 time: 0.27s
Test loss: 0.6808 score: 0.5116 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6798 score: 0.5000 time: 0.30s
Test loss: 0.6793 score: 0.5116 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6746;  Loss pred: 0.6746; Loss self: 0.0000; time: 0.41s
Val loss: 0.6783 score: 0.5227 time: 0.42s
Test loss: 0.6776 score: 0.5116 time: 0.36s
Epoch 39/1000, LR 0.000269
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.40s
Val loss: 0.6767 score: 0.5227 time: 0.31s
Test loss: 0.6757 score: 0.5116 time: 0.23s
Epoch 40/1000, LR 0.000269
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.52s
Val loss: 0.6749 score: 0.5455 time: 0.27s
Test loss: 0.6737 score: 0.5349 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.41s
Val loss: 0.6729 score: 0.5682 time: 0.28s
Test loss: 0.6714 score: 0.5349 time: 0.28s
Epoch 42/1000, LR 0.000269
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.37s
Val loss: 0.6707 score: 0.5682 time: 0.29s
Test loss: 0.6689 score: 0.5581 time: 0.24s
Epoch 43/1000, LR 0.000269
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.40s
Val loss: 0.6684 score: 0.5909 time: 0.27s
Test loss: 0.6662 score: 0.5581 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.56s
Val loss: 0.6658 score: 0.5909 time: 0.36s
Test loss: 0.6632 score: 0.5349 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6550;  Loss pred: 0.6550; Loss self: 0.0000; time: 0.51s
Val loss: 0.6630 score: 0.6364 time: 0.27s
Test loss: 0.6600 score: 0.5349 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6492;  Loss pred: 0.6492; Loss self: 0.0000; time: 0.40s
Val loss: 0.6599 score: 0.6364 time: 0.28s
Test loss: 0.6566 score: 0.5349 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6437;  Loss pred: 0.6437; Loss self: 0.0000; time: 0.39s
Val loss: 0.6567 score: 0.6364 time: 0.29s
Test loss: 0.6527 score: 0.5814 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 0.39s
Val loss: 0.6532 score: 0.6591 time: 0.28s
Test loss: 0.6486 score: 0.5814 time: 0.26s
Epoch 49/1000, LR 0.000269
Train loss: 0.6354;  Loss pred: 0.6354; Loss self: 0.0000; time: 0.49s
Val loss: 0.6493 score: 0.6818 time: 0.36s
Test loss: 0.6443 score: 0.6279 time: 0.27s
Epoch 50/1000, LR 0.000269
Train loss: 0.6308;  Loss pred: 0.6308; Loss self: 0.0000; time: 0.52s
Val loss: 0.6452 score: 0.6818 time: 0.28s
Test loss: 0.6396 score: 0.6744 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6231;  Loss pred: 0.6231; Loss self: 0.0000; time: 0.35s
Val loss: 0.6408 score: 0.7273 time: 0.28s
Test loss: 0.6346 score: 0.6977 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.40s
Val loss: 0.6361 score: 0.7955 time: 0.29s
Test loss: 0.6295 score: 0.6977 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.6109;  Loss pred: 0.6109; Loss self: 0.0000; time: 0.43s
Val loss: 0.6312 score: 0.7955 time: 0.26s
Test loss: 0.6241 score: 0.7209 time: 0.26s
Epoch 54/1000, LR 0.000269
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.36s
Val loss: 0.6258 score: 0.7955 time: 0.29s
Test loss: 0.6185 score: 0.7442 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.5945;  Loss pred: 0.5945; Loss self: 0.0000; time: 0.48s
Val loss: 0.6201 score: 0.8409 time: 0.32s
Test loss: 0.6125 score: 0.7674 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.5882;  Loss pred: 0.5882; Loss self: 0.0000; time: 0.41s
Val loss: 0.6141 score: 0.8409 time: 0.27s
Test loss: 0.6063 score: 0.7674 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5750;  Loss pred: 0.5750; Loss self: 0.0000; time: 0.42s
Val loss: 0.6077 score: 0.8409 time: 0.27s
Test loss: 0.6000 score: 0.7674 time: 0.26s
Epoch 58/1000, LR 0.000269
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.37s
Val loss: 0.6009 score: 0.8409 time: 0.30s
Test loss: 0.5933 score: 0.7674 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.47s
Val loss: 0.5937 score: 0.8409 time: 0.29s
Test loss: 0.5866 score: 0.7674 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.56s
Val loss: 0.5861 score: 0.8409 time: 0.30s
Test loss: 0.5793 score: 0.7674 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.5428;  Loss pred: 0.5428; Loss self: 0.0000; time: 0.43s
Val loss: 0.5782 score: 0.8409 time: 0.28s
Test loss: 0.5716 score: 0.7674 time: 0.26s
Epoch 62/1000, LR 0.000268
Train loss: 0.5258;  Loss pred: 0.5258; Loss self: 0.0000; time: 0.42s
Val loss: 0.5699 score: 0.8409 time: 0.27s
Test loss: 0.5636 score: 0.8140 time: 0.28s
Epoch 63/1000, LR 0.000268
Train loss: 0.5135;  Loss pred: 0.5135; Loss self: 0.0000; time: 0.36s
Val loss: 0.5611 score: 0.8636 time: 0.29s
Test loss: 0.5554 score: 0.8140 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 0.40s
Val loss: 0.5519 score: 0.8864 time: 0.31s
Test loss: 0.5471 score: 0.8140 time: 0.29s
Epoch 65/1000, LR 0.000268
Train loss: 0.4978;  Loss pred: 0.4978; Loss self: 0.0000; time: 0.54s
Val loss: 0.5424 score: 0.9091 time: 0.30s
Test loss: 0.5383 score: 0.8140 time: 0.29s
Epoch 66/1000, LR 0.000268
Train loss: 0.4801;  Loss pred: 0.4801; Loss self: 0.0000; time: 0.45s
Val loss: 0.5326 score: 0.9091 time: 0.29s
Test loss: 0.5289 score: 0.7907 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.4724;  Loss pred: 0.4724; Loss self: 0.0000; time: 0.42s
Val loss: 0.5226 score: 0.9091 time: 0.27s
Test loss: 0.5191 score: 0.7907 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.4516;  Loss pred: 0.4516; Loss self: 0.0000; time: 0.39s
Val loss: 0.5123 score: 0.9318 time: 0.29s
Test loss: 0.5094 score: 0.7907 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4366;  Loss pred: 0.4366; Loss self: 0.0000; time: 0.43s
Val loss: 0.5020 score: 0.9318 time: 0.30s
Test loss: 0.4991 score: 0.8140 time: 0.34s
Epoch 70/1000, LR 0.000268
Train loss: 0.4170;  Loss pred: 0.4170; Loss self: 0.0000; time: 0.40s
Val loss: 0.4916 score: 0.9318 time: 0.29s
Test loss: 0.4890 score: 0.8140 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.4157;  Loss pred: 0.4157; Loss self: 0.0000; time: 0.41s
Val loss: 0.4817 score: 0.9318 time: 0.27s
Test loss: 0.4782 score: 0.8372 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.3945;  Loss pred: 0.3945; Loss self: 0.0000; time: 0.39s
Val loss: 0.4717 score: 0.9091 time: 0.28s
Test loss: 0.4678 score: 0.8605 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.3741;  Loss pred: 0.3741; Loss self: 0.0000; time: 0.38s
Val loss: 0.4611 score: 0.9091 time: 0.30s
Test loss: 0.4582 score: 0.8605 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.3661;  Loss pred: 0.3661; Loss self: 0.0000; time: 0.41s
Val loss: 0.4503 score: 0.9091 time: 0.28s
Test loss: 0.4491 score: 0.8605 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.3408;  Loss pred: 0.3408; Loss self: 0.0000; time: 0.42s
Val loss: 0.4395 score: 0.9091 time: 0.39s
Test loss: 0.4402 score: 0.8605 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3222;  Loss pred: 0.3222; Loss self: 0.0000; time: 0.39s
Val loss: 0.4295 score: 0.9091 time: 0.31s
Test loss: 0.4310 score: 0.8605 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.3072;  Loss pred: 0.3072; Loss self: 0.0000; time: 0.38s
Val loss: 0.4196 score: 0.9318 time: 0.30s
Test loss: 0.4221 score: 0.8605 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.3040;  Loss pred: 0.3040; Loss self: 0.0000; time: 0.40s
Val loss: 0.4100 score: 0.9318 time: 0.31s
Test loss: 0.4138 score: 0.8605 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.3038;  Loss pred: 0.3038; Loss self: 0.0000; time: 0.51s
Val loss: 0.4000 score: 0.9318 time: 0.27s
Test loss: 0.4065 score: 0.8605 time: 0.26s
Epoch 80/1000, LR 0.000267
Train loss: 0.2818;  Loss pred: 0.2818; Loss self: 0.0000; time: 0.38s
Val loss: 0.3896 score: 0.9318 time: 0.42s
Test loss: 0.4006 score: 0.8605 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.2432;  Loss pred: 0.2432; Loss self: 0.0000; time: 0.38s
Val loss: 0.3780 score: 0.9318 time: 0.28s
Test loss: 0.3963 score: 0.8605 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.2517;  Loss pred: 0.2517; Loss self: 0.0000; time: 0.39s
Val loss: 0.3675 score: 0.9318 time: 0.31s
Test loss: 0.3921 score: 0.8605 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2273;  Loss pred: 0.2273; Loss self: 0.0000; time: 0.41s
Val loss: 0.3570 score: 0.9318 time: 0.28s
Test loss: 0.3889 score: 0.8605 time: 0.25s
Epoch 84/1000, LR 0.000266
Train loss: 0.2173;  Loss pred: 0.2173; Loss self: 0.0000; time: 0.50s
Val loss: 0.3462 score: 0.9091 time: 0.28s
Test loss: 0.3870 score: 0.8605 time: 0.26s
Epoch 85/1000, LR 0.000266
Train loss: 0.2064;  Loss pred: 0.2064; Loss self: 0.0000; time: 0.35s
Val loss: 0.3359 score: 0.9091 time: 0.29s
Test loss: 0.3860 score: 0.8605 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.2190;  Loss pred: 0.2190; Loss self: 0.0000; time: 0.51s
Val loss: 0.3263 score: 0.9091 time: 0.29s
Test loss: 0.3854 score: 0.8605 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.1935;  Loss pred: 0.1935; Loss self: 0.0000; time: 0.40s
Val loss: 0.3189 score: 0.9091 time: 0.40s
Test loss: 0.3836 score: 0.8605 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.1845;  Loss pred: 0.1845; Loss self: 0.0000; time: 0.49s
Val loss: 0.3130 score: 0.9318 time: 0.29s
Test loss: 0.3814 score: 0.8605 time: 0.26s
Epoch 89/1000, LR 0.000266
Train loss: 0.2128;  Loss pred: 0.2128; Loss self: 0.0000; time: 0.37s
Val loss: 0.3139 score: 0.9318 time: 0.29s
Test loss: 0.3756 score: 0.8372 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 90/1000, LR 0.000266
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 0.41s
Val loss: 0.3192 score: 0.9318 time: 0.32s
Test loss: 0.3709 score: 0.8372 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.1845,   Val_Loss: 0.3130,   Val_Precision: 0.9524,   Val_Recall: 0.9091,   Val_accuracy: 0.9302,   Val_Score: 0.9318,   Val_Loss: 0.3130,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.3814


[0.28178107494022697, 0.37869441299699247, 0.3005223999498412, 0.28709915408398956, 0.295045844046399, 0.3043468809919432, 0.3348340039374307, 0.31117341096978635, 0.39004076400306076, 0.28136962198186666, 0.3099887960124761, 0.33538012101780623, 0.30485707498155534, 0.3202938500326127, 0.39149055001325905, 0.3061388259520754, 0.28561337594874203, 0.29803693294525146, 0.3022486809641123, 0.2879478669492528, 0.28647347795777023, 0.3048172980779782, 0.2832450020359829, 0.2887693130178377, 0.3175503280945122, 0.3133027689764276, 0.2864482579752803, 0.2893905460368842, 0.2909245820483193, 0.30703137000091374, 0.3616959910141304, 0.2782217509811744, 0.2936680620769039, 0.3083051999565214, 0.28327887901104987, 0.2990240210201591, 0.30344756692647934, 0.31060497800353914, 0.30938048602547497, 0.302175085991621, 0.2895508899819106, 0.30144570604898036, 0.2977182869799435, 0.40948671591468155, 0.29389159905258566, 0.29207602399401367, 0.29935133503749967, 0.28158441092818975, 0.2856432719854638, 0.3952722770627588, 0.30413152708206326, 0.30922711302991956, 0.29353495698887855, 0.30231305002234876, 0.30001335800625384, 0.2843158630421385, 0.31198322493582964, 0.30401960806921124, 0.29732444195542485, 0.2971146749332547, 0.30664394691120833, 0.30007821798790246, 0.28638898802455515, 0.32019256602507085, 0.295755124068819, 0.31047637294977903, 0.32664183899760246, 0.30357926001306623, 0.281759002013132, 0.4123047840548679, 0.28792573406826705, 0.318166704964824, 0.33656936895567924, 0.28665707807522267, 0.2870193941053003, 0.2960998020134866, 0.4077825389103964, 0.2972784029552713, 0.29998854000587016, 0.3024137889733538, 0.3087407990824431, 0.2943020099774003, 0.3837469480931759, 0.29404215805698186, 0.29830051597673446, 0.3072665430372581, 0.28829723596572876, 0.29659136303234845, 0.30717282905243337, 0.29634835198521614, 0.29958002199418843, 0.2967634389642626, 0.3058213379699737, 0.3061945870285854, 0.2872552250046283, 0.29647378507070243, 0.305782631970942, 0.28766227804590017, 0.2831084170611575, 0.28401155304163694, 0.3027241170639172, 0.26315413194242865, 0.24573173990938812, 0.2594443749403581, 0.2559352139942348, 0.2553417419549078, 0.2596940740477294, 0.2551981440046802, 0.2591241030022502, 0.247246581944637, 0.24851109308656305, 0.255798168014735, 0.2690067969961092, 0.26198142499197274, 0.24600741104222834, 0.26120822690427303, 0.2749569689622149, 0.25346704199910164, 0.24526556301862001, 0.2591354950563982, 0.2744623509934172, 0.25042889604810625, 0.2456864189589396, 0.2563212759559974, 0.27247606590390205, 0.2579169680830091, 0.25124818400945514, 0.2511310769477859, 0.3257969889091328, 0.26865075703244656, 0.26153128396254033, 0.2972547340905294, 0.2597943090368062, 0.26009927596896887, 0.2464166529243812, 0.2885289720725268, 0.2695499010151252, 0.26829211600124836, 0.33585561404470354, 0.26206232397817075, 0.2537927740486339, 0.2754407830070704, 0.26081023400183767, 0.25089325406588614, 0.2587801100453362, 0.2696063829353079, 0.27133648993913084, 0.24781193805392832, 0.26203142502345145, 0.2603026209399104, 0.264103636960499, 0.24718192499130964, 0.2623675720533356, 0.27611446497030556, 0.25326527596917003, 0.25962197000626475, 0.2774359530303627, 0.27533549710642546, 0.2458628339227289, 0.2513102199882269, 0.2753364280797541, 0.2455991799943149, 0.2556408980162814, 0.2609505510190502, 0.2683852339396253, 0.2455516739282757, 0.2800574420252815, 0.2760396999074146, 0.3030269810697064, 0.25138610089197755, 0.24928276299033314, 0.29157554206904024, 0.26772156299557537, 0.24562353699002415, 0.2599491219734773, 0.2670339070027694, 0.2752796980785206, 0.24848505097907037, 0.25274620798882097, 0.25715504598338157, 0.27376032795291394, 0.2772790709277615, 0.25580379797611386, 0.2537251920439303, 0.24635371298063546, 0.2626608199207112, 0.36001771898008883, 0.26389258191920817, 0.24254836200270802, 0.23813713097479194, 0.25034255802165717, 0.2490041609853506, 0.3467976429965347, 0.2378402929753065, 0.24763590702787042, 0.2579934080131352, 0.2301821740111336, 0.23939433204941452, 0.24694931600242853, 0.2699953499250114, 0.30190551397390664, 0.24129517597611994, 0.2514344019582495, 0.2652156950207427, 0.23931275901850313, 0.25125488406047225, 0.26033631106838584, 0.24301067390479147, 0.23351632500998676, 0.23869940696749836, 0.2571081669302657, 0.29095014289487153, 0.2357505599502474, 0.24526456790044904, 0.23745063203386962, 0.26171134400647134, 0.2565955249592662, 0.23679482098668814, 0.2504883479559794, 0.2545784650137648, 0.254186772974208, 0.26276112196501344, 0.23851256200578064, 0.3609748979797587, 0.23925274307839572, 0.25762042694259435, 0.28416981699410826, 0.24100701697170734, 0.25835606397595257, 0.2488554830197245, 0.25403817801270634, 0.2530856600496918, 0.23472520196810365, 0.2638899330049753, 0.27263439900707453, 0.25979088293388486, 0.23739075707271695, 0.24372744804713875, 0.2652477329829708, 0.23806908400729299, 0.23623854003380984, 0.25477244902867824, 0.2630739730084315, 0.2403788799419999, 0.242856579949148, 0.2467263820581138, 0.2613808090100065, 0.28192873392254114, 0.23783576698042452, 0.2935673639876768, 0.29637483204714954, 0.2516156289493665, 0.2691709039499983, 0.2587288620416075, 0.34829065192025155, 0.2431640620343387, 0.2488243639236316, 0.2537748279282823, 0.23518130893353373, 0.2530448680045083, 0.25901181902736425, 0.2564399379771203, 0.23849637305829674, 0.24558775196783245, 0.26865163596812636, 0.264602851937525, 0.2492194129154086, 0.24752474401611835, 0.2525901379995048, 0.26189003500621766, 0.23620179598219693, 0.23500126996077597, 0.23680051299743354, 0.26129423605743796, 0.24770446796901524, 0.25313100207131356]
[0.006404115339550613, 0.008606691204477102, 0.006830054544314573, 0.006524980774636127, 0.006705587364690887, 0.006916974567998709, 0.007609863725850697, 0.0070721229765860535, 0.008864562818251381, 0.006394764135951515, 0.007045199909374456, 0.0076222754776774145, 0.006928569885944439, 0.007279405682559379, 0.008897512500301342, 0.006957700589819896, 0.0064912130897441375, 0.006773566657846624, 0.006869288203729825, 0.006544269703392109, 0.006510760862676596, 0.006927665865408595, 0.006437386409908702, 0.006562938932223584, 0.007217052911238914, 0.007120517476736991, 0.0065101876812563705, 0.006577057864474641, 0.006611922319279984, 0.006977985681838949, 0.008220363432139327, 0.0063232216132085095, 0.006674274138111452, 0.007006936362648214, 0.006438156341160225, 0.0067960004777308895, 0.00689653561196544, 0.007059204045534981, 0.007031374682397159, 0.006867615590718659, 0.006580702045043422, 0.0068510387738404625, 0.006766324704089625, 0.009306516270788217, 0.006679354523922401, 0.006638091454409401, 0.006803439432670447, 0.0063996457029134035, 0.0064918925451241775, 0.008983460842335428, 0.006912080160955983, 0.007027888932498172, 0.0066712490224745125, 0.006870751136871563, 0.0068184854092330415, 0.006461724160048602, 0.007090527839450674, 0.006909536547027528, 0.00675737368080511, 0.006752606248483062, 0.006969180611618372, 0.006819959499725056, 0.006508840636921708, 0.0072771037732970644, 0.006721707365200432, 0.007056281203404069, 0.00742367815903642, 0.006899528636660596, 0.006403613682116636, 0.00937056327397427, 0.006543766683369706, 0.007231061476473274, 0.0076493038399018005, 0.006514933592618697, 0.006523168047847735, 0.006729540954851968, 0.009267784975236282, 0.006756327339892529, 0.006817921363769777, 0.0068730406584853136, 0.007016836342782798, 0.006688682044940916, 0.00872152154757218, 0.00668277631947686, 0.0067795571812894195, 0.006983330523574047, 0.006552209908312017, 0.006740712796189738, 0.006981200660282577, 0.006735189817845821, 0.006808636863504283, 0.00674462361282415, 0.006950484953863038, 0.006958967887013304, 0.006528527841014279, 0.006738040569788692, 0.006949605272066864, 0.006537779046497731, 0.0064342822059353984, 0.006454808023673567, 0.006880093569634482, 0.006119863533544852, 0.005714691625799724, 0.006033590114892049, 0.005951981720796158, 0.005938180045462971, 0.006039397070877427, 0.0059348405582483775, 0.006026141930284889, 0.005749920510340395, 0.005779327746199141, 0.005948794604993837, 0.00625597202316533, 0.006092591278883087, 0.005721102582377403, 0.00607460992800635, 0.006394348115400347, 0.0058945823720721316, 0.005703850302758605, 0.006026406861776703, 0.006382845371939935, 0.0058239278150722384, 0.005713637650207898, 0.005960959905953428, 0.006336652695439583, 0.005998069025186258, 0.005842981023475701, 0.005840257603436882, 0.007576674160677506, 0.006247692024010385, 0.006082122882849775, 0.006912900792803009, 0.006041728117135029, 0.006048820371371369, 0.005730619835450725, 0.006709976094709926, 0.006268602349188959, 0.006239351534912753, 0.0078105956754582215, 0.006094472650655134, 0.005902157536014741, 0.006405599604815591, 0.006065354279112504, 0.005834726838741538, 0.006018142094077586, 0.006269915882216463, 0.006310150928816996, 0.005763068326835542, 0.006093754070312824, 0.006053549324183963, 0.006141945045593, 0.005748416860263015, 0.006101571443100828, 0.006421266627216408, 0.005889890138817908, 0.006037720232703831, 0.006451998907682854, 0.006403151095498267, 0.005717740323784393, 0.0058444237206564395, 0.006403172746040794, 0.005711608837077091, 0.005945137163169335, 0.0060686174655593065, 0.0062415170683633794, 0.005710504044843621, 0.006512963768029802, 0.006419527904823596, 0.007047139094644334, 0.005846188392836687, 0.005797273557914724, 0.006780826559745122, 0.006226082860362218, 0.005712175278837771, 0.006045328417987844, 0.0062100908605295215, 0.006401853443686526, 0.005778722115792334, 0.005877818790437697, 0.005980349906590269, 0.006366519254718928, 0.006448350486692128, 0.005948925534328229, 0.005900585861486751, 0.005729156115828732, 0.006108391160946773, 0.008372505092560205, 0.006137036788818794, 0.005640659581458326, 0.005538072813367254, 0.005821919953992027, 0.005790794441519782, 0.00806506146503569, 0.005531169604076896, 0.005758974582043498, 0.005999846697979888, 0.005353073814212409, 0.005567310047660803, 0.005743007348893687, 0.006278961626163056, 0.007021058464509456, 0.005611515720374882, 0.005847311673447663, 0.006167806860947505, 0.005565413000430305, 0.005843136838615634, 0.006054332815543856, 0.005651411021041663, 0.005430612209534576, 0.005551148999244148, 0.005979259696052691, 0.006766282392903989, 0.005482571161633661, 0.005703827160475559, 0.005522107721717898, 0.006086310325731891, 0.005967337789750377, 0.005506856302016003, 0.005825310417580916, 0.0059204294189247626, 0.005911320301725767, 0.0061107237666282194, 0.005546803767576294, 0.008394765069296713, 0.005564017280892924, 0.005991172719595217, 0.00660860039521182, 0.00560481434817924, 0.006008280557580292, 0.005787336814412197, 0.005907864604946659, 0.005885713024411437, 0.005458725627165201, 0.006136975186162217, 0.00634033486062964, 0.006041648440322903, 0.00552071528076086, 0.005668080187142762, 0.006168551929836529, 0.005536490325751, 0.005493919535669996, 0.0059249406750855405, 0.006117999372289105, 0.005590206510279068, 0.005647827440677861, 0.005737822838560786, 0.006078623465348988, 0.006556482184245143, 0.005531064348381965, 0.006827147999713414, 0.006892437954584873, 0.00585152625463643, 0.006259788463953449, 0.006016950280037384, 0.008099782602796547, 0.005654978186845086, 0.005786613114503061, 0.005901740184378659, 0.005469332765896133, 0.005884764372197868, 0.006023530675054982, 0.005963719487840006, 0.005546427280425505, 0.0057113430690193595, 0.006247712464375031, 0.006153554696221511, 0.00579580030035834, 0.005756389395723683, 0.0058741892558024375, 0.006090465930377154, 0.005493065022841789, 0.005465145813041302, 0.005506988674358919, 0.00607661014087065, 0.0057605690225352385, 0.005886767490030548]
[156.1495924072741, 116.1886695179457, 146.41171509126718, 153.2571565401687, 149.12936714024872, 144.57187751224166, 131.40839784068686, 141.40025609152133, 112.80872170493113, 156.37793337489595, 141.94061387376445, 131.1944186389745, 144.32992904187026, 137.37385215332694, 112.39096320079706, 143.7256442829894, 154.05440958023098, 147.632709695354, 145.57549055184916, 152.80543824189692, 153.59187982660703, 144.3487632671816, 155.34254685422596, 152.3707610762715, 138.56071339628508, 140.43923117484636, 153.605402633648, 152.04366764072518, 151.24194624671523, 143.30783202989667, 121.649122724949, 158.14723271933264, 149.82902699333223, 142.7157245683987, 155.32396962882535, 147.14536929136432, 145.00033875921807, 141.6590303311193, 142.21970029608448, 145.61094557352115, 151.95947076090445, 145.96326674114465, 147.7907200338156, 107.45159315294516, 149.71506549299878, 150.6457099707089, 146.98447893839568, 156.25865031008755, 154.03828591572474, 111.31567416506157, 144.67424808651145, 142.2902395875705, 149.89696781384396, 145.54449434699316, 146.66013637660365, 154.75745717880943, 141.0332238505777, 144.7275071481022, 147.98648812934297, 148.09096861299219, 143.4888914104038, 146.62843672903256, 153.63719221015376, 137.41730654844383, 148.77172504967888, 141.7177081204733, 134.70411547714485, 144.93743741949373, 156.161825125819, 106.71717065049783, 152.81718441175397, 138.2922829868844, 130.73085092836862, 153.49350623204847, 153.29974525643897, 148.59854583082736, 107.9006475303453, 148.0094065448147, 146.6722695444933, 145.49601110905996, 142.5143684630118, 149.50628438921908, 114.65889232118806, 149.63840658343054, 147.50225910917203, 143.1981483082091, 152.62026308580525, 148.35226336378864, 143.24183599093442, 148.47391492224332, 146.87227708679973, 148.2662424777287, 143.8748528538582, 143.69947041517196, 153.17388917570102, 148.41109809930404, 143.89306454848372, 152.95714230900737, 155.41749149229656, 154.92327522869982, 145.34686045747003, 163.40233642771489, 174.98756984285365, 165.73880243071363, 168.011268668049, 168.40176490843245, 165.57944249469858, 168.49652323181238, 165.94365210258573, 173.91544773560705, 173.0305052620808, 168.1012820917585, 159.84726215160256, 164.13377399301322, 174.79148216644109, 164.61962362218605, 156.38810742749038, 169.64730270593682, 175.32016917000098, 165.93635692648544, 156.6699397726551, 171.7054248873097, 175.01984921350652, 167.75821608886574, 157.81202601172836, 166.72032212382666, 171.14551561647028, 171.22532393288932, 131.9840313563887, 160.0591060117751, 164.41627689236205, 144.6570737773491, 165.5155579020324, 165.32149057243095, 174.50119336372762, 149.03182751848988, 159.525193064668, 160.27306594354013, 128.03120806036773, 164.08310567978407, 169.42956772977305, 156.11341040551795, 164.87083094943668, 171.38762921345, 166.1642387912531, 159.49177290182277, 158.47481483101012, 173.51867846916412, 164.10245449053127, 165.19234360658373, 162.81487258136983, 173.96093990898316, 163.89220536468855, 155.7325147910103, 169.78245373532528, 165.6254283832851, 154.99072679774153, 156.17310681658748, 174.8942665059912, 171.10326831123072, 156.17257876079003, 175.08201778603396, 168.20469781506318, 164.78217743583485, 160.21745819918348, 175.11589032196972, 153.53992984095842, 155.77469477912945, 141.90155559154172, 171.05162078343153, 172.49487884434066, 147.47464651825396, 160.6146308084667, 175.06465596473524, 165.41698496056972, 161.0282397566615, 156.2047630106551, 173.04863946773943, 170.13113803828819, 167.2142960896005, 157.07169961966108, 155.07841921182228, 168.09758236668247, 169.47469683087255, 174.5457759890958, 163.7092277903507, 119.43856575119894, 162.94508806300828, 177.28423166807414, 180.5682290031107, 171.7646425753949, 172.68787730229846, 123.99161548058638, 180.79358826077643, 173.64202355016522, 166.67092516492028, 186.80855798121078, 179.61995855074866, 174.12479895096018, 159.26200214908454, 142.42866728070572, 178.20497167442562, 171.01876141491616, 162.1321845098079, 179.6811844732246, 171.14095179001862, 165.1709660612985, 176.94696002055804, 184.14130146216127, 180.1428857586351, 167.24478461107265, 147.79164420461242, 182.39617334980957, 175.32088050098358, 181.0902739305682, 164.30315683578752, 167.57891629959693, 181.5918094020195, 171.66467163397468, 168.90666693930703, 169.16694561586473, 163.6467361626102, 180.28400533032655, 119.1218565076267, 179.7262570398628, 166.9122301764592, 151.3179705531201, 178.41804168319266, 166.43696818357776, 172.79104915920934, 169.26589671041197, 169.902949031396, 183.1929406789612, 162.9467236978278, 157.72037628635482, 165.51774070895024, 181.13594872115576, 176.42657954422708, 162.1126013648556, 180.61984057821948, 182.0194113705831, 168.77806122262356, 163.45212530249754, 178.8842680786902, 177.05923392729906, 174.2821324631259, 164.5109300979852, 152.52081404307688, 180.79702874773747, 146.4740474414759, 145.0865436278315, 170.89558458490288, 159.74980716336177, 166.1971519554898, 123.4601036890469, 176.83534170410306, 172.8126591863706, 169.4415492310055, 182.83765914472625, 169.9303382008676, 166.01559018222682, 167.6805896117339, 180.29624286776604, 175.09016494288454, 160.0585823534745, 162.50769666742923, 172.53872600444367, 173.72000593686067, 170.2362583929714, 164.1910506406978, 182.04772669569797, 182.9777345764009, 181.5874444514655, 164.56543645512218, 173.59396200063202, 169.87251521204735]
Elapsed: 0.2773582265438998~0.033761556351227545
Time per graph: 0.006390694875839865~0.0007288684209927428
Speed: 158.27117573462183~15.948602768297901
Total Time: 0.2540
best val loss: 0.31298840045928955 test_score: 0.8605

Testing...
Test loss: 0.5094 score: 0.7907 time: 0.39s
test Score 0.7907
Epoch Time List: [0.8706781240180135, 1.0295234689256176, 0.9154979998711497, 0.8924768589204177, 0.953969235997647, 0.9483574300538749, 1.0669248071499169, 0.9327782469335943, 1.0388739060144871, 0.951056034071371, 0.9196552491048351, 0.9675171020207927, 0.9410809099208564, 1.0708984368247911, 1.0257385168224573, 0.9556800830177963, 0.8918109810911119, 0.9261253050062805, 0.9436411019414663, 0.8893170630326495, 1.0137605470372364, 0.9240157080348581, 0.8818954710150138, 0.9263202119618654, 0.938967770896852, 1.0763065110659227, 0.8804489210015163, 0.9517074789619073, 0.925247959094122, 1.0000186810502782, 1.0717517390148714, 0.9488994468702003, 0.991239273105748, 0.9609572750050575, 0.8996746109332889, 0.9523984330007806, 0.9057460500625893, 1.123586878995411, 1.1672107491176575, 1.1492242398671806, 0.9990534069947898, 0.995803717058152, 0.9237391909118742, 1.0941665141144767, 0.9356556790880859, 0.9206605979707092, 0.9090922579634935, 0.8949215391185135, 0.8961892430670559, 1.0144473270047456, 0.9814178729429841, 0.92453745205421, 0.9131073819007725, 0.9112601579399779, 0.9099799250252545, 0.9933741078712046, 0.9733625119552016, 0.9377681639743969, 0.9185209851711988, 0.9367574600037187, 1.0311023460235447, 0.9179757511010394, 0.8822328639216721, 0.9367624400183558, 0.9220999181270599, 1.1768681730609387, 1.1918510519899428, 1.1098458240740001, 0.903132118168287, 1.199015511898324, 1.079015028080903, 1.0169245870783925, 1.0928513490362093, 0.964032699004747, 0.9675315279746428, 0.9258888129843399, 1.0643326170975342, 0.9320032831747085, 0.920220822095871, 0.9654542350908741, 0.9519244940020144, 0.917692034970969, 1.0144494229461998, 0.9611111261183396, 0.9146477250615135, 0.9554470779839903, 0.8795305701205507, 0.9295457489788532, 1.0586415810976177, 0.9158080370398238, 0.9302746799075976, 0.9295724249677733, 1.0095390460919589, 1.0015949570806697, 0.8897756548831239, 0.9950349730206653, 0.9240747910225764, 0.8983584009110928, 1.0135174121242017, 0.9960263160755858, 0.9452827371424064, 0.9778026950079948, 0.8946950959507376, 1.0648322949418798, 0.8932018599007279, 0.8979506629984826, 0.928911785944365, 0.9140475739259273, 1.0170003800885752, 0.8679085109615698, 0.9449964389204979, 1.0181939739268273, 0.9284387000370771, 0.9808508040150627, 0.9369705439312384, 0.9337823790265247, 0.9337962581776083, 0.9002168299630284, 1.0108719459967688, 0.9518404511036351, 0.9341480721486732, 0.8810477949446067, 0.9980326029472053, 1.008760603144765, 0.9274619449861348, 0.9318189831683412, 1.0139435458695516, 0.9277943609049544, 1.137218720978126, 0.9833172338549048, 0.9125186888268217, 0.9644567780196667, 0.9269165571313351, 1.0704027902102098, 0.8886703789466992, 0.9547667980659753, 0.9174608680186793, 1.017217315849848, 1.0607032789848745, 0.9221883360296488, 0.9995441280771047, 0.9264817788498476, 0.9054369420045987, 1.0030469910707325, 1.0097964729648083, 0.9101820340147242, 1.0248971560504287, 0.8933760789223015, 1.0326269040815532, 0.922569768037647, 0.9328555350657552, 0.9122854759916663, 0.9254105569561943, 1.0260544999036938, 0.8927290819119662, 0.9344176249578595, 1.0103657199069858, 0.9435897328658029, 0.9812658040318638, 0.9131419589975849, 0.9558018329553306, 0.8781277920352295, 0.9638999211601913, 1.021214202977717, 0.9232479850761592, 0.9006772260181606, 0.9481131010688841, 0.9421384509187192, 1.044968482106924, 0.8808042739983648, 0.9287259579868987, 0.9806221929611638, 0.9272256240947172, 0.9806260401383042, 0.9268117097672075, 0.9962164082098752, 0.9394945349777117, 0.918266900931485, 1.0731759861810133, 0.952902555000037, 0.9801587570691481, 0.9614609059644863, 0.9235276099061593, 1.0574245419120416, 0.9493938130326569, 1.0180996470153332, 1.0582891349913552, 1.0873771959450096, 0.9571748591260985, 0.8636922589503229, 0.9547791600925848, 0.991004325915128, 1.0247716769808903, 0.8648291490972042, 0.9092313470318913, 0.9438237629365176, 0.8923590540653095, 0.9785657840548083, 1.0914158979430795, 1.0262803849764168, 0.969760867068544, 0.9187112401705235, 0.9157953570829704, 1.0579780309926718, 0.9117142669856548, 0.9188787429593503, 1.0099117180798203, 0.8955084200715646, 0.9491796338697895, 1.0856645349413157, 0.9397839138982818, 0.9589219420449808, 0.9259903999045491, 1.0389947600197047, 1.0452464210102335, 0.9511891399743035, 0.9440752799855545, 0.9198110109427944, 1.0049152692081407, 1.083227850147523, 0.9520211609778926, 0.980090783094056, 0.9078475409187376, 1.181666204938665, 0.9505871779983863, 1.0449860260123387, 0.9754183960612863, 0.8980440431041643, 0.9189556367928162, 1.1685023399768397, 1.033573065069504, 0.9292046390473843, 0.9142962781479582, 0.9291870561428368, 1.1262337529333308, 1.0534413770074025, 0.8646728509338573, 0.9317094699945301, 0.9551623999141157, 0.8760185440769419, 1.0255065150558949, 0.9273500739363953, 0.9454487259499729, 0.9069054210558534, 0.9972895249957219, 1.0948959639063105, 0.9678861210122705, 0.9660659901564941, 0.8824598119826987, 1.0009981599869207, 1.1250978120369837, 0.9870110070332885, 0.959897740976885, 0.9342153990874067, 1.073822666076012, 0.9290969809517264, 0.9305846279021353, 0.9243827101308852, 0.9183532579336315, 0.9373918040655553, 1.0620135598583147, 0.9582312030252069, 0.9158722448628396, 0.952781718922779, 1.0398277071071789, 1.0613807479385287, 0.9047619808698073, 0.941920273937285, 0.9357727819588035, 1.0327367290155962, 0.8739448898704723, 1.0352507199859247, 1.0337154698790982, 1.0340552018024027, 0.9011719089467078, 0.9781658749561757]
Total Epoch List: [101, 85, 90]
Total Time List: [0.3031989319715649, 0.2632948210230097, 0.2539871400222182]
========================training times:8========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcdab0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.30s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.30s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.28s
Epoch 4/1000, LR 0.000060
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.29s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.29s
Epoch 6/1000, LR 0.000120
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.31s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.28s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.29s
Epoch 9/1000, LR 0.000210
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.30s
Epoch 10/1000, LR 0.000240
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.30s
Epoch 11/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.27s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.28s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.29s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.30s
Epoch 15/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.28s
Epoch 16/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.31s
Epoch 17/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.39s
Epoch 18/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.31s
Epoch 19/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5000 time: 0.28s
Epoch 20/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4884 time: 0.27s
Test loss: 0.6896 score: 0.5227 time: 0.29s
Epoch 21/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.4884 time: 0.24s
Test loss: 0.6890 score: 0.5682 time: 0.29s
Epoch 22/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.36s
Val loss: 0.6907 score: 0.5116 time: 0.32s
Test loss: 0.6885 score: 0.6136 time: 0.30s
Epoch 23/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.53s
Val loss: 0.6902 score: 0.5349 time: 0.25s
Test loss: 0.6878 score: 0.6136 time: 0.29s
Epoch 24/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.47s
Val loss: 0.6897 score: 0.6279 time: 0.26s
Test loss: 0.6871 score: 0.6364 time: 0.29s
Epoch 25/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.37s
Val loss: 0.6892 score: 0.6512 time: 0.24s
Test loss: 0.6863 score: 0.6364 time: 0.29s
Epoch 26/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.43s
Val loss: 0.6885 score: 0.6744 time: 0.32s
Test loss: 0.6854 score: 0.6818 time: 0.29s
Epoch 27/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.33s
Val loss: 0.6878 score: 0.5814 time: 0.29s
Test loss: 0.6844 score: 0.6818 time: 0.27s
Epoch 28/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.40s
Val loss: 0.6870 score: 0.6047 time: 0.35s
Test loss: 0.6833 score: 0.6818 time: 0.29s
Epoch 29/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.37s
Val loss: 0.6862 score: 0.6512 time: 0.24s
Test loss: 0.6822 score: 0.6818 time: 0.29s
Epoch 30/1000, LR 0.000270
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.38s
Val loss: 0.6852 score: 0.6512 time: 0.38s
Test loss: 0.6809 score: 0.7727 time: 0.30s
Epoch 31/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.32s
Val loss: 0.6842 score: 0.6744 time: 0.28s
Test loss: 0.6794 score: 0.8182 time: 0.29s
Epoch 32/1000, LR 0.000270
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.43s
Val loss: 0.6829 score: 0.7209 time: 0.27s
Test loss: 0.6778 score: 0.7727 time: 0.29s
Epoch 33/1000, LR 0.000270
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.38s
Val loss: 0.6815 score: 0.6744 time: 0.36s
Test loss: 0.6760 score: 0.7955 time: 0.30s
Epoch 34/1000, LR 0.000270
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 0.38s
Val loss: 0.6800 score: 0.6512 time: 0.26s
Test loss: 0.6741 score: 0.7727 time: 0.30s
Epoch 35/1000, LR 0.000270
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.39s
Val loss: 0.6783 score: 0.6279 time: 0.25s
Test loss: 0.6721 score: 0.7273 time: 0.42s
Epoch 36/1000, LR 0.000270
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.36s
Val loss: 0.6766 score: 0.6279 time: 0.32s
Test loss: 0.6699 score: 0.7273 time: 0.32s
Epoch 37/1000, LR 0.000270
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.39s
Val loss: 0.6747 score: 0.6279 time: 0.38s
Test loss: 0.6676 score: 0.7500 time: 0.28s
Epoch 38/1000, LR 0.000270
Train loss: 0.6678;  Loss pred: 0.6678; Loss self: 0.0000; time: 0.49s
Val loss: 0.6728 score: 0.6279 time: 0.26s
Test loss: 0.6652 score: 0.7500 time: 0.31s
Epoch 39/1000, LR 0.000269
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 0.43s
Val loss: 0.6707 score: 0.6512 time: 0.26s
Test loss: 0.6627 score: 0.7500 time: 0.31s
Epoch 40/1000, LR 0.000269
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.57s
Val loss: 0.6684 score: 0.6512 time: 0.24s
Test loss: 0.6600 score: 0.7273 time: 0.32s
Epoch 41/1000, LR 0.000269
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.39s
Val loss: 0.6659 score: 0.6512 time: 0.25s
Test loss: 0.6571 score: 0.7273 time: 0.30s
Epoch 42/1000, LR 0.000269
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.36s
Val loss: 0.6632 score: 0.6512 time: 0.29s
Test loss: 0.6540 score: 0.7273 time: 0.29s
Epoch 43/1000, LR 0.000269
Train loss: 0.6525;  Loss pred: 0.6525; Loss self: 0.0000; time: 0.46s
Val loss: 0.6602 score: 0.6512 time: 0.26s
Test loss: 0.6507 score: 0.7273 time: 0.28s
Epoch 44/1000, LR 0.000269
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.38s
Val loss: 0.6569 score: 0.6744 time: 0.24s
Test loss: 0.6470 score: 0.7273 time: 0.31s
Epoch 45/1000, LR 0.000269
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.39s
Val loss: 0.6533 score: 0.7209 time: 0.24s
Test loss: 0.6431 score: 0.7273 time: 0.30s
Epoch 46/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.35s
Val loss: 0.6494 score: 0.7209 time: 0.26s
Test loss: 0.6389 score: 0.7273 time: 0.37s
Epoch 47/1000, LR 0.000269
Train loss: 0.6359;  Loss pred: 0.6359; Loss self: 0.0000; time: 0.42s
Val loss: 0.6452 score: 0.7209 time: 0.27s
Test loss: 0.6343 score: 0.7273 time: 0.29s
Epoch 48/1000, LR 0.000269
Train loss: 0.6294;  Loss pred: 0.6294; Loss self: 0.0000; time: 0.36s
Val loss: 0.6405 score: 0.7907 time: 0.24s
Test loss: 0.6294 score: 0.7273 time: 0.29s
Epoch 49/1000, LR 0.000269
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.36s
Val loss: 0.6354 score: 0.7907 time: 0.25s
Test loss: 0.6241 score: 0.7500 time: 0.29s
Epoch 50/1000, LR 0.000269
Train loss: 0.6154;  Loss pred: 0.6154; Loss self: 0.0000; time: 0.36s
Val loss: 0.6299 score: 0.7674 time: 0.26s
Test loss: 0.6183 score: 0.7727 time: 0.29s
Epoch 51/1000, LR 0.000269
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.37s
Val loss: 0.6240 score: 0.8140 time: 0.24s
Test loss: 0.6122 score: 0.7727 time: 0.29s
Epoch 52/1000, LR 0.000269
Train loss: 0.6028;  Loss pred: 0.6028; Loss self: 0.0000; time: 0.37s
Val loss: 0.6175 score: 0.8140 time: 0.25s
Test loss: 0.6056 score: 0.7727 time: 0.39s
Epoch 53/1000, LR 0.000269
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 0.32s
Val loss: 0.6105 score: 0.8140 time: 0.28s
Test loss: 0.5985 score: 0.7955 time: 0.28s
Epoch 54/1000, LR 0.000269
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.46s
Val loss: 0.6030 score: 0.8605 time: 0.27s
Test loss: 0.5909 score: 0.7955 time: 0.29s
Epoch 55/1000, LR 0.000269
Train loss: 0.5772;  Loss pred: 0.5772; Loss self: 0.0000; time: 0.38s
Val loss: 0.5950 score: 0.8605 time: 0.24s
Test loss: 0.5830 score: 0.8182 time: 0.39s
Epoch 56/1000, LR 0.000269
Train loss: 0.5642;  Loss pred: 0.5642; Loss self: 0.0000; time: 0.39s
Val loss: 0.5866 score: 0.8605 time: 0.24s
Test loss: 0.5746 score: 0.8409 time: 0.31s
Epoch 57/1000, LR 0.000269
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.36s
Val loss: 0.5776 score: 0.8372 time: 0.29s
Test loss: 0.5659 score: 0.8409 time: 0.29s
Epoch 58/1000, LR 0.000269
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.46s
Val loss: 0.5682 score: 0.8372 time: 0.29s
Test loss: 0.5568 score: 0.8636 time: 0.27s
Epoch 59/1000, LR 0.000268
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.44s
Val loss: 0.5582 score: 0.8372 time: 0.26s
Test loss: 0.5473 score: 0.8864 time: 0.29s
Epoch 60/1000, LR 0.000268
Train loss: 0.5271;  Loss pred: 0.5271; Loss self: 0.0000; time: 0.38s
Val loss: 0.5479 score: 0.8372 time: 0.24s
Test loss: 0.5374 score: 0.8864 time: 0.30s
Epoch 61/1000, LR 0.000268
Train loss: 0.5083;  Loss pred: 0.5083; Loss self: 0.0000; time: 0.37s
Val loss: 0.5373 score: 0.8372 time: 0.25s
Test loss: 0.5273 score: 0.8864 time: 0.29s
Epoch 62/1000, LR 0.000268
Train loss: 0.5016;  Loss pred: 0.5016; Loss self: 0.0000; time: 0.37s
Val loss: 0.5265 score: 0.8372 time: 0.26s
Test loss: 0.5168 score: 0.8864 time: 0.29s
Epoch 63/1000, LR 0.000268
Train loss: 0.4868;  Loss pred: 0.4868; Loss self: 0.0000; time: 0.47s
Val loss: 0.5156 score: 0.8605 time: 0.37s
Test loss: 0.5063 score: 0.8864 time: 0.28s
Epoch 64/1000, LR 0.000268
Train loss: 0.4717;  Loss pred: 0.4717; Loss self: 0.0000; time: 0.38s
Val loss: 0.5047 score: 0.8605 time: 0.27s
Test loss: 0.4960 score: 0.8864 time: 0.37s
Epoch 65/1000, LR 0.000268
Train loss: 0.4537;  Loss pred: 0.4537; Loss self: 0.0000; time: 0.38s
Val loss: 0.4937 score: 0.8605 time: 0.24s
Test loss: 0.4856 score: 0.8864 time: 0.30s
Epoch 66/1000, LR 0.000268
Train loss: 0.4501;  Loss pred: 0.4501; Loss self: 0.0000; time: 0.34s
Val loss: 0.4828 score: 0.8605 time: 0.25s
Test loss: 0.4752 score: 0.8864 time: 0.28s
Epoch 67/1000, LR 0.000268
Train loss: 0.4269;  Loss pred: 0.4269; Loss self: 0.0000; time: 0.43s
Val loss: 0.4720 score: 0.8605 time: 0.27s
Test loss: 0.4652 score: 0.8864 time: 0.29s
Epoch 68/1000, LR 0.000268
Train loss: 0.4282;  Loss pred: 0.4282; Loss self: 0.0000; time: 0.47s
Val loss: 0.4615 score: 0.8605 time: 0.26s
Test loss: 0.4563 score: 0.8636 time: 0.34s
Epoch 69/1000, LR 0.000268
Train loss: 0.4006;  Loss pred: 0.4006; Loss self: 0.0000; time: 0.35s
Val loss: 0.4514 score: 0.8837 time: 0.24s
Test loss: 0.4482 score: 0.8636 time: 0.29s
Epoch 70/1000, LR 0.000268
Train loss: 0.3789;  Loss pred: 0.3789; Loss self: 0.0000; time: 0.35s
Val loss: 0.4416 score: 0.8837 time: 0.26s
Test loss: 0.4402 score: 0.8636 time: 0.28s
Epoch 71/1000, LR 0.000268
Train loss: 0.3757;  Loss pred: 0.3757; Loss self: 0.0000; time: 0.35s
Val loss: 0.4318 score: 0.8837 time: 0.26s
Test loss: 0.4320 score: 0.8636 time: 0.32s
Epoch 72/1000, LR 0.000267
Train loss: 0.3687;  Loss pred: 0.3687; Loss self: 0.0000; time: 0.38s
Val loss: 0.4224 score: 0.8837 time: 0.24s
Test loss: 0.4241 score: 0.8636 time: 0.37s
Epoch 73/1000, LR 0.000267
Train loss: 0.3543;  Loss pred: 0.3543; Loss self: 0.0000; time: 0.40s
Val loss: 0.4130 score: 0.9070 time: 0.24s
Test loss: 0.4156 score: 0.8636 time: 0.30s
Epoch 74/1000, LR 0.000267
Train loss: 0.3226;  Loss pred: 0.3226; Loss self: 0.0000; time: 0.31s
Val loss: 0.4038 score: 0.9070 time: 0.26s
Test loss: 0.4069 score: 0.8636 time: 0.27s
Epoch 75/1000, LR 0.000267
Train loss: 0.3472;  Loss pred: 0.3472; Loss self: 0.0000; time: 0.36s
Val loss: 0.3946 score: 0.8837 time: 0.25s
Test loss: 0.3965 score: 0.8636 time: 0.30s
Epoch 76/1000, LR 0.000267
Train loss: 0.3251;  Loss pred: 0.3251; Loss self: 0.0000; time: 0.39s
Val loss: 0.3862 score: 0.8837 time: 0.27s
Test loss: 0.3879 score: 0.8864 time: 0.30s
Epoch 77/1000, LR 0.000267
Train loss: 0.2838;  Loss pred: 0.2838; Loss self: 0.0000; time: 0.32s
Val loss: 0.3782 score: 0.8837 time: 0.27s
Test loss: 0.3799 score: 0.8864 time: 0.27s
Epoch 78/1000, LR 0.000267
Train loss: 0.2822;  Loss pred: 0.2822; Loss self: 0.0000; time: 0.37s
Val loss: 0.3707 score: 0.9070 time: 0.27s
Test loss: 0.3740 score: 0.8864 time: 0.31s
Epoch 79/1000, LR 0.000267
Train loss: 0.2636;  Loss pred: 0.2636; Loss self: 0.0000; time: 0.38s
Val loss: 0.3640 score: 0.9070 time: 0.23s
Test loss: 0.3714 score: 0.8864 time: 0.40s
Epoch 80/1000, LR 0.000267
Train loss: 0.2835;  Loss pred: 0.2835; Loss self: 0.0000; time: 0.34s
Val loss: 0.3578 score: 0.9070 time: 0.28s
Test loss: 0.3693 score: 0.8864 time: 0.29s
Epoch 81/1000, LR 0.000267
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 0.37s
Val loss: 0.3514 score: 0.9302 time: 0.27s
Test loss: 0.3641 score: 0.8864 time: 0.28s
Epoch 82/1000, LR 0.000267
Train loss: 0.2437;  Loss pred: 0.2437; Loss self: 0.0000; time: 0.39s
Val loss: 0.3453 score: 0.9302 time: 0.25s
Test loss: 0.3584 score: 0.8864 time: 0.30s
Epoch 83/1000, LR 0.000266
Train loss: 0.2369;  Loss pred: 0.2369; Loss self: 0.0000; time: 0.39s
Val loss: 0.3392 score: 0.9302 time: 0.24s
Test loss: 0.3516 score: 0.8864 time: 0.30s
Epoch 84/1000, LR 0.000266
Train loss: 0.2292;  Loss pred: 0.2292; Loss self: 0.0000; time: 0.32s
Val loss: 0.3343 score: 0.9302 time: 0.29s
Test loss: 0.3492 score: 0.8864 time: 0.27s
Epoch 85/1000, LR 0.000266
Train loss: 0.2192;  Loss pred: 0.2192; Loss self: 0.0000; time: 0.41s
Val loss: 0.3293 score: 0.9302 time: 0.28s
Test loss: 0.3450 score: 0.8864 time: 0.40s
Epoch 86/1000, LR 0.000266
Train loss: 0.2143;  Loss pred: 0.2143; Loss self: 0.0000; time: 0.44s
Val loss: 0.3244 score: 0.9302 time: 0.24s
Test loss: 0.3396 score: 0.8864 time: 0.28s
Epoch 87/1000, LR 0.000266
Train loss: 0.2166;  Loss pred: 0.2166; Loss self: 0.0000; time: 0.45s
Val loss: 0.3217 score: 0.9302 time: 0.25s
Test loss: 0.3412 score: 0.8636 time: 0.33s
Epoch 88/1000, LR 0.000266
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.49s
Val loss: 0.3191 score: 0.9070 time: 0.24s
Test loss: 0.3413 score: 0.8636 time: 0.32s
Epoch 89/1000, LR 0.000266
Train loss: 0.1845;  Loss pred: 0.1845; Loss self: 0.0000; time: 0.34s
Val loss: 0.3182 score: 0.8837 time: 0.26s
Test loss: 0.3452 score: 0.8636 time: 0.27s
Epoch 90/1000, LR 0.000266
Train loss: 0.1843;  Loss pred: 0.1843; Loss self: 0.0000; time: 0.67s
Val loss: 0.3161 score: 0.8837 time: 0.25s
Test loss: 0.3437 score: 0.8636 time: 0.30s
Epoch 91/1000, LR 0.000266
Train loss: 0.1699;  Loss pred: 0.1699; Loss self: 0.0000; time: 0.46s
Val loss: 0.3144 score: 0.8837 time: 0.24s
Test loss: 0.3421 score: 0.8636 time: 0.36s
Epoch 92/1000, LR 0.000266
Train loss: 0.1526;  Loss pred: 0.1526; Loss self: 0.0000; time: 0.31s
Val loss: 0.3122 score: 0.8837 time: 0.22s
Test loss: 0.3380 score: 0.8636 time: 0.25s
Epoch 93/1000, LR 0.000265
Train loss: 0.1452;  Loss pred: 0.1452; Loss self: 0.0000; time: 0.38s
Val loss: 0.3089 score: 0.8837 time: 0.24s
Test loss: 0.3301 score: 0.8636 time: 0.27s
Epoch 94/1000, LR 0.000265
Train loss: 0.1523;  Loss pred: 0.1523; Loss self: 0.0000; time: 0.29s
Val loss: 0.3057 score: 0.8837 time: 0.22s
Test loss: 0.3215 score: 0.8636 time: 0.26s
Epoch 95/1000, LR 0.000265
Train loss: 0.1284;  Loss pred: 0.1284; Loss self: 0.0000; time: 0.62s
Val loss: 0.3052 score: 0.8837 time: 0.25s
Test loss: 0.3191 score: 0.8636 time: 0.30s
Epoch 96/1000, LR 0.000265
Train loss: 0.1257;  Loss pred: 0.1257; Loss self: 0.0000; time: 0.36s
Val loss: 0.3067 score: 0.8837 time: 0.36s
Test loss: 0.3211 score: 0.8636 time: 0.40s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.37s
Val loss: 0.3103 score: 0.8837 time: 0.27s
Test loss: 0.3266 score: 0.8864 time: 0.28s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.1284,   Val_Loss: 0.3052,   Val_Precision: 0.8696,   Val_Recall: 0.9091,   Val_accuracy: 0.8889,   Val_Score: 0.8837,   Val_Loss: 0.3052,   Test_Precision: 0.8636,   Test_Recall: 0.8636,   Test_accuracy: 0.8636,   Test_Score: 0.8636,   Test_loss: 0.3191


[0.30145648296456784, 0.30784547701478004, 0.2891621949383989, 0.29467664391268045, 0.2960956799797714, 0.31403666199184954, 0.2837912170216441, 0.2955553939100355, 0.29974595399107784, 0.3004386250395328, 0.2790172069799155, 0.28815585596021265, 0.2951386340428144, 0.307339645922184, 0.2826175729278475, 0.3099046640563756, 0.39349576900713146, 0.3098684990545735, 0.28780107805505395, 0.289998703985475, 0.2962815349455923, 0.30202430894132704, 0.2949671399546787, 0.2910416550002992, 0.29538421297911555, 0.2978494520066306, 0.27712576603516936, 0.2955536359222606, 0.2990617490140721, 0.3029566069599241, 0.29411642299965024, 0.2914770740317181, 0.30013961996883154, 0.3087229479569942, 0.42288610292598605, 0.32130664808209985, 0.2857426719274372, 0.31479454203508794, 0.3094706259435043, 0.3215484319953248, 0.30249289399944246, 0.29721524694468826, 0.28834371897391975, 0.3185790580464527, 0.3070441969903186, 0.3747744830325246, 0.2912926949793473, 0.28941438102629036, 0.2944973179837689, 0.2944468220230192, 0.29239818395581096, 0.39803462906274945, 0.27975055295974016, 0.29094268300104886, 0.3957817319314927, 0.3106901290593669, 0.2960074780276045, 0.2772509370697662, 0.2951457779854536, 0.30785213503986597, 0.2976641490822658, 0.2940414589829743, 0.28574819304049015, 0.37744537903927267, 0.30744399200193584, 0.2802602360025048, 0.2937864949926734, 0.3442324351053685, 0.2946819399949163, 0.2820565849542618, 0.32442133605945855, 0.3750028039794415, 0.3022641170537099, 0.2773243800038472, 0.30118676205165684, 0.3054999919841066, 0.2785948159871623, 0.312978454050608, 0.3994534839875996, 0.29462140891700983, 0.2884692510124296, 0.30846140207722783, 0.299586525070481, 0.2779186828993261, 0.4006521450355649, 0.28628904500510544, 0.33035338507033885, 0.3284180200425908, 0.2723340759985149, 0.3044337809551507, 0.3628275890368968, 0.25909386295825243, 0.26953748508822173, 0.26118090201634914, 0.30382765899412334, 0.40241453400813043, 0.2854039289522916]
[0.006851283703740178, 0.006996488113972274, 0.0065718680667817934, 0.006697196452560919, 0.006729447272267531, 0.007137196863451126, 0.006449800386855548, 0.006717168043409897, 0.006812408045251769, 0.0068281505690802905, 0.0063413001586344435, 0.006548996726368469, 0.006707696228245782, 0.006984991952776909, 0.00642312665745108, 0.007043287819463082, 0.008943085659252987, 0.007042465887603943, 0.006540933592160317, 0.006590879636033523, 0.006733671248763461, 0.006864188839575614, 0.006703798635333607, 0.006614583068188618, 0.006713277567707171, 0.0067693057274234225, 0.0062983128644356675, 0.006717128089142286, 0.006796857932138003, 0.006885377430907366, 0.00668446415908296, 0.00662447895526632, 0.006821354999291626, 0.007016430635386231, 0.00961104779377241, 0.007302423820047724, 0.006494151634714482, 0.007154421409888362, 0.007033423316897824, 0.007307918908984654, 0.006874838499987329, 0.006754891976015642, 0.006553266340316358, 0.00724043313741938, 0.006978277204325423, 0.008517601887102832, 0.0066202885222578925, 0.0065775995687793265, 0.006693120863267474, 0.006691973227795891, 0.006645413271722977, 0.009046241569607942, 0.006357967112721367, 0.006612333704569292, 0.008995039362079378, 0.007061139296803793, 0.006727442682445558, 0.006301157660676505, 0.006707858590578491, 0.006996639432724227, 0.006765094297324223, 0.006682760431431234, 0.006494277114556594, 0.008578304069074378, 0.006987363454589451, 0.006369550818238746, 0.006676965795288032, 0.00782346443421292, 0.00669731681806628, 0.006410376930778677, 0.007373212183169512, 0.008522790999532763, 0.006869639023947952, 0.0063028268182692545, 0.006845153682992201, 0.006943181636002423, 0.0063317003633445975, 0.007113146682968363, 0.009078488272445446, 0.006695941111750223, 0.006556119341191582, 0.007010486410846087, 0.00680878466069275, 0.0063163337022574114, 0.009105730568990111, 0.006506569204661487, 0.007508031478871338, 0.007464045910058881, 0.006189410818148066, 0.006918949567162516, 0.008246081569020382, 0.005888496885414828, 0.006125851933823221, 0.005935929591280662, 0.006905174068048258, 0.009145784863821145, 0.0064864529307339]
[145.95804862876878, 142.9288499758842, 152.1637363742292, 149.31621120620005, 148.6006145142205, 140.1110294604455, 155.04355794296558, 148.8722618724841, 146.7909722021125, 146.45254082829837, 157.6963674615496, 152.69514427662833, 149.0824816706888, 143.164087626822, 155.68741725495335, 141.97914747096493, 111.81822897618646, 141.99571797148312, 152.88337450766312, 151.72481599160457, 148.5073985730496, 145.68363769867176, 149.16915832306114, 151.18110842227983, 148.95853626107944, 147.72563690672996, 158.77267794152374, 148.87314738220076, 147.1268062366933, 145.23532079899627, 149.60062260804906, 150.95526859588273, 146.59843976802946, 142.5226089967543, 104.04692822857062, 136.94083288546577, 153.98470135105885, 139.77370673439322, 142.178275776107, 136.83786211291414, 145.4579623945847, 148.04085743349634, 152.5956596404298, 138.11328424978979, 143.30184524343042, 117.4039375465738, 151.05081850102562, 152.03114594365317, 149.40713314891732, 149.43275562525915, 150.4797307723688, 110.54314571475005, 157.28297776173542, 151.23253675309434, 111.1723873289233, 141.62020574394384, 148.64489334251456, 158.70099652333377, 149.07887315998997, 142.92575880398596, 147.81759958549682, 149.6387623438765, 153.98172611984026, 116.57315851102753, 143.11549792692958, 156.99694194079945, 149.76862704698968, 127.82061047365201, 149.31352766565558, 155.99706706771278, 135.62609825371, 117.33245600588143, 145.56805627107087, 158.65896824285556, 146.08875801936313, 144.02619035842417, 157.93545850482562, 140.5847572909453, 110.1504975266805, 149.3442046921787, 152.529255182571, 142.64345459009473, 146.8690889540126, 158.3196910009057, 109.82095202833428, 153.6908266930554, 133.19070422308982, 133.97559608420352, 161.5662668679036, 144.53060978302565, 121.26971963955457, 169.82262527418368, 163.24260050730405, 168.46561008218637, 144.81894158573314, 109.33998720610577, 154.16746420248154]
Elapsed: 0.3074274108656346~0.03337033796752821
Time per graph: 0.006986986610582603~0.0007584167719892774
Speed: 144.56450046598067~13.382411666180179
Total Time: 0.2859
best val loss: 0.3051639497280121 test_score: 0.8636

Testing...
Test loss: 0.3641 score: 0.8864 time: 0.29s
test Score 0.8864
Epoch Time List: [0.9308716820087284, 1.0008250470273197, 0.8910605830606073, 0.8986643840325996, 1.2048928680596873, 1.0245232820743695, 0.9141289790859446, 0.954673737869598, 0.9244506049435586, 1.0201225739438087, 0.895354763022624, 1.0292730309301987, 0.9313434320501983, 1.07732690195553, 0.8731405141297728, 0.9367791190743446, 1.0198822050588205, 0.9779862450668588, 0.9396700381767005, 0.9832438109442592, 0.9091758731519803, 0.9700436940183863, 1.070584623143077, 1.0124020529910922, 0.9038736498914659, 1.0413906988687813, 0.8869846428278834, 1.0408057861495763, 0.9045546590350568, 1.0586517611518502, 0.8917156427633017, 0.9853319189278409, 1.0319082760252059, 0.9413575540529564, 1.0604381560115144, 0.9912750431103632, 1.0486875601345673, 1.0661472898209468, 0.9961663308786228, 1.1315396119607612, 0.9327207549940795, 0.948064200929366, 1.0030511829536408, 0.93272296898067, 0.937905844883062, 0.9779747619759291, 0.9714287479873747, 0.8812050978885964, 0.9050406600581482, 0.9097144490806386, 0.8997924060095102, 1.0092458518920466, 0.8795682069612667, 1.0117517131147906, 1.0196289740270004, 0.9411605299683288, 0.9474473600275815, 1.0213568268809468, 0.9930250510806218, 0.9277740260586143, 0.9139159169280902, 0.9188103140331805, 1.1166098470566794, 1.02467159088701, 0.9198671649210155, 0.8718791791470721, 0.9910839608637616, 1.0751292039640248, 0.881617454928346, 0.8883409349946305, 0.9296751819783822, 0.997202032012865, 0.9393958690343425, 0.8502499189926311, 0.909675857052207, 0.9551714769331738, 0.8625468100653961, 0.9520921648945659, 1.0122107131173834, 0.9101800330681726, 0.9200177169404924, 0.9417069969931617, 0.9301504408940673, 0.8853813878959045, 1.0821252471068874, 0.9609704029280692, 1.0267642788821831, 1.0573590929852799, 0.8624735639896244, 1.2169232710730284, 1.0591598349856213, 0.7794137632008642, 0.8893354000756517, 0.7734952939208597, 1.1660273919114843, 1.1124680308857933, 0.9150287689408287]
Total Epoch List: [97]
Total Time List: [0.28585293295327574]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcfee0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.26s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.26s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.27s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 10/1000, LR 0.000240
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.28s
Epoch 11/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 12/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 15/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.35s
Epoch 16/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.27s
Epoch 18/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.24s
Epoch 19/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.26s
Epoch 20/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.34s
Epoch 21/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.27s
Epoch 22/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.28s
Epoch 24/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.38s
Val loss: 0.6883 score: 0.5455 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.40s
Val loss: 0.6876 score: 0.5682 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.26s
Epoch 27/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.50s
Val loss: 0.6869 score: 0.5909 time: 0.26s
Test loss: 0.6903 score: 0.5116 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.41s
Val loss: 0.6861 score: 0.6591 time: 0.26s
Test loss: 0.6897 score: 0.5581 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.42s
Val loss: 0.6852 score: 0.6364 time: 0.27s
Test loss: 0.6892 score: 0.6047 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.41s
Val loss: 0.6843 score: 0.6591 time: 0.25s
Test loss: 0.6885 score: 0.6047 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.39s
Val loss: 0.6833 score: 0.6818 time: 0.25s
Test loss: 0.6878 score: 0.5814 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6779;  Loss pred: 0.6779; Loss self: 0.0000; time: 0.41s
Val loss: 0.6822 score: 0.7045 time: 0.35s
Test loss: 0.6871 score: 0.6047 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.41s
Val loss: 0.6810 score: 0.7045 time: 0.31s
Test loss: 0.6863 score: 0.6047 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.42s
Val loss: 0.6798 score: 0.7273 time: 0.24s
Test loss: 0.6855 score: 0.5814 time: 0.27s
Epoch 35/1000, LR 0.000270
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 0.42s
Val loss: 0.6784 score: 0.7273 time: 0.26s
Test loss: 0.6846 score: 0.5581 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.40s
Val loss: 0.6769 score: 0.7273 time: 0.27s
Test loss: 0.6836 score: 0.6279 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.41s
Val loss: 0.6753 score: 0.7500 time: 0.28s
Test loss: 0.6825 score: 0.6512 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.54s
Val loss: 0.6735 score: 0.7727 time: 0.25s
Test loss: 0.6814 score: 0.6744 time: 0.27s
Epoch 39/1000, LR 0.000269
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.39s
Val loss: 0.6716 score: 0.8182 time: 0.25s
Test loss: 0.6801 score: 0.6744 time: 0.35s
Epoch 40/1000, LR 0.000269
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.40s
Val loss: 0.6695 score: 0.8182 time: 0.27s
Test loss: 0.6787 score: 0.6512 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6550;  Loss pred: 0.6550; Loss self: 0.0000; time: 0.42s
Val loss: 0.6673 score: 0.8182 time: 0.34s
Test loss: 0.6772 score: 0.6512 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.46s
Val loss: 0.6649 score: 0.8182 time: 0.26s
Test loss: 0.6756 score: 0.6512 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6463;  Loss pred: 0.6463; Loss self: 0.0000; time: 0.45s
Val loss: 0.6623 score: 0.8409 time: 0.26s
Test loss: 0.6739 score: 0.6512 time: 0.29s
Epoch 44/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.46s
Val loss: 0.6595 score: 0.8409 time: 0.25s
Test loss: 0.6721 score: 0.6744 time: 0.26s
Epoch 45/1000, LR 0.000269
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.40s
Val loss: 0.6566 score: 0.8409 time: 0.37s
Test loss: 0.6701 score: 0.6744 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.41s
Val loss: 0.6533 score: 0.8409 time: 0.27s
Test loss: 0.6680 score: 0.6977 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.45s
Val loss: 0.6499 score: 0.8409 time: 0.24s
Test loss: 0.6657 score: 0.6977 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.37s
Val loss: 0.6461 score: 0.8182 time: 0.26s
Test loss: 0.6631 score: 0.6977 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 0.41s
Val loss: 0.6421 score: 0.8182 time: 0.26s
Test loss: 0.6604 score: 0.7209 time: 0.34s
Epoch 50/1000, LR 0.000269
Train loss: 0.6073;  Loss pred: 0.6073; Loss self: 0.0000; time: 0.49s
Val loss: 0.6378 score: 0.8182 time: 0.28s
Test loss: 0.6575 score: 0.7442 time: 0.24s
Epoch 51/1000, LR 0.000269
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.42s
Val loss: 0.6331 score: 0.8182 time: 0.24s
Test loss: 0.6544 score: 0.7442 time: 0.31s
Epoch 52/1000, LR 0.000269
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.42s
Val loss: 0.6281 score: 0.8182 time: 0.25s
Test loss: 0.6510 score: 0.7442 time: 0.27s
Epoch 53/1000, LR 0.000269
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.40s
Val loss: 0.6229 score: 0.8182 time: 0.27s
Test loss: 0.6474 score: 0.7442 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.42s
Val loss: 0.6175 score: 0.8182 time: 0.27s
Test loss: 0.6438 score: 0.7442 time: 0.36s
Epoch 55/1000, LR 0.000269
Train loss: 0.5590;  Loss pred: 0.5590; Loss self: 0.0000; time: 0.55s
Val loss: 0.6119 score: 0.8182 time: 0.27s
Test loss: 0.6399 score: 0.7442 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.5541;  Loss pred: 0.5541; Loss self: 0.0000; time: 0.47s
Val loss: 0.6060 score: 0.8182 time: 0.39s
Test loss: 0.6359 score: 0.7442 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.41s
Val loss: 0.5998 score: 0.8182 time: 0.27s
Test loss: 0.6317 score: 0.7442 time: 0.29s
Epoch 58/1000, LR 0.000269
Train loss: 0.5309;  Loss pred: 0.5309; Loss self: 0.0000; time: 0.45s
Val loss: 0.5932 score: 0.8182 time: 0.25s
Test loss: 0.6271 score: 0.7442 time: 0.31s
Epoch 59/1000, LR 0.000268
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.40s
Val loss: 0.5861 score: 0.8182 time: 0.28s
Test loss: 0.6221 score: 0.7442 time: 0.27s
Epoch 60/1000, LR 0.000268
Train loss: 0.5081;  Loss pred: 0.5081; Loss self: 0.0000; time: 0.49s
Val loss: 0.5787 score: 0.8182 time: 0.27s
Test loss: 0.6167 score: 0.7442 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.4922;  Loss pred: 0.4922; Loss self: 0.0000; time: 0.42s
Val loss: 0.5709 score: 0.8182 time: 0.27s
Test loss: 0.6110 score: 0.7442 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.4802;  Loss pred: 0.4802; Loss self: 0.0000; time: 0.53s
Val loss: 0.5629 score: 0.8182 time: 0.27s
Test loss: 0.6053 score: 0.7907 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.4614;  Loss pred: 0.4614; Loss self: 0.0000; time: 0.44s
Val loss: 0.5548 score: 0.8182 time: 0.25s
Test loss: 0.5993 score: 0.7907 time: 0.27s
Epoch 64/1000, LR 0.000268
Train loss: 0.4470;  Loss pred: 0.4470; Loss self: 0.0000; time: 0.37s
Val loss: 0.5464 score: 0.8182 time: 0.30s
Test loss: 0.5931 score: 0.7674 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.4336;  Loss pred: 0.4336; Loss self: 0.0000; time: 0.42s
Val loss: 0.5379 score: 0.8182 time: 0.36s
Test loss: 0.5870 score: 0.7674 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.4208;  Loss pred: 0.4208; Loss self: 0.0000; time: 0.51s
Val loss: 0.5297 score: 0.8182 time: 0.27s
Test loss: 0.5812 score: 0.7674 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.4088;  Loss pred: 0.4088; Loss self: 0.0000; time: 0.46s
Val loss: 0.5217 score: 0.8182 time: 0.27s
Test loss: 0.5756 score: 0.7674 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.3798;  Loss pred: 0.3798; Loss self: 0.0000; time: 0.42s
Val loss: 0.5139 score: 0.8182 time: 0.24s
Test loss: 0.5702 score: 0.7674 time: 0.27s
Epoch 69/1000, LR 0.000268
Train loss: 0.3730;  Loss pred: 0.3730; Loss self: 0.0000; time: 0.39s
Val loss: 0.5063 score: 0.8182 time: 0.27s
Test loss: 0.5651 score: 0.7674 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.3483;  Loss pred: 0.3483; Loss self: 0.0000; time: 0.44s
Val loss: 0.4985 score: 0.8182 time: 0.36s
Test loss: 0.5596 score: 0.7674 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.3334;  Loss pred: 0.3334; Loss self: 0.0000; time: 0.41s
Val loss: 0.4905 score: 0.8182 time: 0.26s
Test loss: 0.5533 score: 0.7674 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.3162;  Loss pred: 0.3162; Loss self: 0.0000; time: 0.42s
Val loss: 0.4827 score: 0.7955 time: 0.26s
Test loss: 0.5469 score: 0.7674 time: 0.27s
Epoch 73/1000, LR 0.000267
Train loss: 0.3012;  Loss pred: 0.3012; Loss self: 0.0000; time: 0.39s
Val loss: 0.4754 score: 0.7955 time: 0.29s
Test loss: 0.5408 score: 0.7907 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.2801;  Loss pred: 0.2801; Loss self: 0.0000; time: 0.42s
Val loss: 0.4688 score: 0.7955 time: 0.28s
Test loss: 0.5352 score: 0.7907 time: 0.26s
Epoch 75/1000, LR 0.000267
Train loss: 0.2630;  Loss pred: 0.2630; Loss self: 0.0000; time: 0.43s
Val loss: 0.4629 score: 0.7955 time: 0.35s
Test loss: 0.5302 score: 0.7907 time: 0.26s
Epoch 76/1000, LR 0.000267
Train loss: 0.2436;  Loss pred: 0.2436; Loss self: 0.0000; time: 0.53s
Val loss: 0.4582 score: 0.7955 time: 0.24s
Test loss: 0.5269 score: 0.7907 time: 0.28s
Epoch 77/1000, LR 0.000267
Train loss: 0.2449;  Loss pred: 0.2449; Loss self: 0.0000; time: 0.40s
Val loss: 0.4551 score: 0.7955 time: 0.25s
Test loss: 0.5256 score: 0.7907 time: 0.27s
Epoch 78/1000, LR 0.000267
Train loss: 0.2138;  Loss pred: 0.2138; Loss self: 0.0000; time: 0.39s
Val loss: 0.4539 score: 0.7955 time: 0.27s
Test loss: 0.5270 score: 0.7907 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.2126;  Loss pred: 0.2126; Loss self: 0.0000; time: 0.46s
Val loss: 0.4535 score: 0.7955 time: 0.26s
Test loss: 0.5292 score: 0.7907 time: 0.26s
Epoch 80/1000, LR 0.000267
Train loss: 0.1898;  Loss pred: 0.1898; Loss self: 0.0000; time: 0.42s
Val loss: 0.4540 score: 0.7955 time: 0.34s
Test loss: 0.5318 score: 0.7907 time: 0.31s
     INFO: Early stopping counter 1 of 2
Epoch 81/1000, LR 0.000267
Train loss: 0.1751;  Loss pred: 0.1751; Loss self: 0.0000; time: 0.44s
Val loss: 0.4553 score: 0.7955 time: 0.26s
Test loss: 0.5349 score: 0.7907 time: 0.26s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 078,   Train_Loss: 0.2126,   Val_Loss: 0.4535,   Val_Precision: 0.9333,   Val_Recall: 0.6364,   Val_accuracy: 0.7568,   Val_Score: 0.7955,   Val_Loss: 0.4535,   Test_Precision: 0.9333,   Test_Recall: 0.6364,   Test_accuracy: 0.7568,   Test_Score: 0.7907,   Test_loss: 0.5292


[0.30145648296456784, 0.30784547701478004, 0.2891621949383989, 0.29467664391268045, 0.2960956799797714, 0.31403666199184954, 0.2837912170216441, 0.2955553939100355, 0.29974595399107784, 0.3004386250395328, 0.2790172069799155, 0.28815585596021265, 0.2951386340428144, 0.307339645922184, 0.2826175729278475, 0.3099046640563756, 0.39349576900713146, 0.3098684990545735, 0.28780107805505395, 0.289998703985475, 0.2962815349455923, 0.30202430894132704, 0.2949671399546787, 0.2910416550002992, 0.29538421297911555, 0.2978494520066306, 0.27712576603516936, 0.2955536359222606, 0.2990617490140721, 0.3029566069599241, 0.29411642299965024, 0.2914770740317181, 0.30013961996883154, 0.3087229479569942, 0.42288610292598605, 0.32130664808209985, 0.2857426719274372, 0.31479454203508794, 0.3094706259435043, 0.3215484319953248, 0.30249289399944246, 0.29721524694468826, 0.28834371897391975, 0.3185790580464527, 0.3070441969903186, 0.3747744830325246, 0.2912926949793473, 0.28941438102629036, 0.2944973179837689, 0.2944468220230192, 0.29239818395581096, 0.39803462906274945, 0.27975055295974016, 0.29094268300104886, 0.3957817319314927, 0.3106901290593669, 0.2960074780276045, 0.2772509370697662, 0.2951457779854536, 0.30785213503986597, 0.2976641490822658, 0.2940414589829743, 0.28574819304049015, 0.37744537903927267, 0.30744399200193584, 0.2802602360025048, 0.2937864949926734, 0.3442324351053685, 0.2946819399949163, 0.2820565849542618, 0.32442133605945855, 0.3750028039794415, 0.3022641170537099, 0.2773243800038472, 0.30118676205165684, 0.3054999919841066, 0.2785948159871623, 0.312978454050608, 0.3994534839875996, 0.29462140891700983, 0.2884692510124296, 0.30846140207722783, 0.299586525070481, 0.2779186828993261, 0.4006521450355649, 0.28628904500510544, 0.33035338507033885, 0.3284180200425908, 0.2723340759985149, 0.3044337809551507, 0.3628275890368968, 0.25909386295825243, 0.26953748508822173, 0.26118090201634914, 0.30382765899412334, 0.40241453400813043, 0.2854039289522916, 0.2457797189708799, 0.2574185929261148, 0.25812872708775103, 0.26476061798166484, 0.2528502930654213, 0.2606846489943564, 0.2712188920704648, 0.24564111605286598, 0.26762486004736274, 0.279506957042031, 0.25983521598391235, 0.2540749440668151, 0.2627197790425271, 0.2594620370073244, 0.3532676340546459, 0.25024959596339613, 0.26964775496162474, 0.24715873296372592, 0.2607457370031625, 0.34021114802453667, 0.27338172297459096, 0.25192768895067275, 0.2802162889856845, 0.25862007797695696, 0.2485951610142365, 0.2609712720150128, 0.261869756039232, 0.2559766940539703, 0.2643348230049014, 0.2594114859821275, 0.2625567920040339, 0.24709448602516204, 0.26184623898006976, 0.2716337409801781, 0.2651116179767996, 0.24847453099209815, 0.25974752788897604, 0.2725740310270339, 0.3497953669866547, 0.2513725010212511, 0.2438586700009182, 0.2604019680293277, 0.29434823198243976, 0.2691963160177693, 0.2476244040299207, 0.25173892301972955, 0.27282291394658387, 0.24620607495307922, 0.34440573409665376, 0.24539543502032757, 0.31642732699401677, 0.2697936649201438, 0.253028137027286, 0.36487662699073553, 0.26280226500239223, 0.25040096999146044, 0.29677770601119846, 0.314601395977661, 0.2694430819246918, 0.25190760800614953, 0.257754473015666, 0.2516479860059917, 0.2789101640228182, 0.26811414503026754, 0.2478720819344744, 0.2544181600678712, 0.2621311970287934, 0.2699275210034102, 0.2447691559791565, 0.24577933608088642, 0.26455645996611565, 0.2722261209273711, 0.25615393894258887, 0.2617406650679186, 0.2615066410508007, 0.2826032870216295, 0.2722165189916268, 0.25447728496510535, 0.2604347469750792, 0.3140432209474966, 0.2673930840101093]
[0.006851283703740178, 0.006996488113972274, 0.0065718680667817934, 0.006697196452560919, 0.006729447272267531, 0.007137196863451126, 0.006449800386855548, 0.006717168043409897, 0.006812408045251769, 0.0068281505690802905, 0.0063413001586344435, 0.006548996726368469, 0.006707696228245782, 0.006984991952776909, 0.00642312665745108, 0.007043287819463082, 0.008943085659252987, 0.007042465887603943, 0.006540933592160317, 0.006590879636033523, 0.006733671248763461, 0.006864188839575614, 0.006703798635333607, 0.006614583068188618, 0.006713277567707171, 0.0067693057274234225, 0.0062983128644356675, 0.006717128089142286, 0.006796857932138003, 0.006885377430907366, 0.00668446415908296, 0.00662447895526632, 0.006821354999291626, 0.007016430635386231, 0.00961104779377241, 0.007302423820047724, 0.006494151634714482, 0.007154421409888362, 0.007033423316897824, 0.007307918908984654, 0.006874838499987329, 0.006754891976015642, 0.006553266340316358, 0.00724043313741938, 0.006978277204325423, 0.008517601887102832, 0.0066202885222578925, 0.0065775995687793265, 0.006693120863267474, 0.006691973227795891, 0.006645413271722977, 0.009046241569607942, 0.006357967112721367, 0.006612333704569292, 0.008995039362079378, 0.007061139296803793, 0.006727442682445558, 0.006301157660676505, 0.006707858590578491, 0.006996639432724227, 0.006765094297324223, 0.006682760431431234, 0.006494277114556594, 0.008578304069074378, 0.006987363454589451, 0.006369550818238746, 0.006676965795288032, 0.00782346443421292, 0.00669731681806628, 0.006410376930778677, 0.007373212183169512, 0.008522790999532763, 0.006869639023947952, 0.0063028268182692545, 0.006845153682992201, 0.006943181636002423, 0.0063317003633445975, 0.007113146682968363, 0.009078488272445446, 0.006695941111750223, 0.006556119341191582, 0.007010486410846087, 0.00680878466069275, 0.0063163337022574114, 0.009105730568990111, 0.006506569204661487, 0.007508031478871338, 0.007464045910058881, 0.006189410818148066, 0.006918949567162516, 0.008246081569020382, 0.005888496885414828, 0.006125851933823221, 0.005935929591280662, 0.006905174068048258, 0.009145784863821145, 0.0064864529307339, 0.00571580741792744, 0.0059864789052584836, 0.006002993653203512, 0.006157223673992206, 0.005880239373614448, 0.0060624336975431715, 0.0063074160946619716, 0.005712584094252697, 0.0062238339545898315, 0.0065001617916751395, 0.006042679441486334, 0.005908719629460816, 0.006109762303314583, 0.006034000860635451, 0.008215526373363858, 0.005819758045660375, 0.006270878022363366, 0.005747877510784324, 0.006063854348910756, 0.007911887163361319, 0.006357714487781185, 0.005858783463969133, 0.0065166578833880115, 0.006014420418068767, 0.00578128281428457, 0.0060690993491863444, 0.006089994326493767, 0.0059529463733481456, 0.006147321465230266, 0.006032825255398314, 0.006105971907070557, 0.005746383395934001, 0.0060894474181411576, 0.006317063743725072, 0.006165386464576735, 0.005778477464932515, 0.006040640183464559, 0.006338930954117068, 0.00813477597643383, 0.005845872116773281, 0.00567113186048647, 0.006055859721612272, 0.006845307720521855, 0.006260379442273704, 0.005758707070463272, 0.005854393558598361, 0.006344718928990322, 0.005725722673327424, 0.008009435676666366, 0.005706870581868083, 0.007358775046372483, 0.006274271277212646, 0.0058843752797043255, 0.00848550295327292, 0.006111680581450982, 0.005823278371894429, 0.006901807116539499, 0.00731631153436421, 0.006266118184295158, 0.005858316465259292, 0.005994290070131768, 0.0058522787443253885, 0.006486282884251586, 0.006235212675122501, 0.005764467021731963, 0.0059167013969272375, 0.006096074349506823, 0.006277384209381633, 0.0056923059530036395, 0.0057157985135089864, 0.00615247581316548, 0.006330840021566769, 0.0059570683475020664, 0.006086992210881828, 0.006081549791879085, 0.006572169465619291, 0.006330616720735507, 0.005918076394537334, 0.00605662202267626, 0.007303330719709223, 0.006218443814188589]
[145.95804862876878, 142.9288499758842, 152.1637363742292, 149.31621120620005, 148.6006145142205, 140.1110294604455, 155.04355794296558, 148.8722618724841, 146.7909722021125, 146.45254082829837, 157.6963674615496, 152.69514427662833, 149.0824816706888, 143.164087626822, 155.68741725495335, 141.97914747096493, 111.81822897618646, 141.99571797148312, 152.88337450766312, 151.72481599160457, 148.5073985730496, 145.68363769867176, 149.16915832306114, 151.18110842227983, 148.95853626107944, 147.72563690672996, 158.77267794152374, 148.87314738220076, 147.1268062366933, 145.23532079899627, 149.60062260804906, 150.95526859588273, 146.59843976802946, 142.5226089967543, 104.04692822857062, 136.94083288546577, 153.98470135105885, 139.77370673439322, 142.178275776107, 136.83786211291414, 145.4579623945847, 148.04085743349634, 152.5956596404298, 138.11328424978979, 143.30184524343042, 117.4039375465738, 151.05081850102562, 152.03114594365317, 149.40713314891732, 149.43275562525915, 150.4797307723688, 110.54314571475005, 157.28297776173542, 151.23253675309434, 111.1723873289233, 141.62020574394384, 148.64489334251456, 158.70099652333377, 149.07887315998997, 142.92575880398596, 147.81759958549682, 149.6387623438765, 153.98172611984026, 116.57315851102753, 143.11549792692958, 156.99694194079945, 149.76862704698968, 127.82061047365201, 149.31352766565558, 155.99706706771278, 135.62609825371, 117.33245600588143, 145.56805627107087, 158.65896824285556, 146.08875801936313, 144.02619035842417, 157.93545850482562, 140.5847572909453, 110.1504975266805, 149.3442046921787, 152.529255182571, 142.64345459009473, 146.8690889540126, 158.3196910009057, 109.82095202833428, 153.6908266930554, 133.19070422308982, 133.97559608420352, 161.5662668679036, 144.53060978302565, 121.26971963955457, 169.82262527418368, 163.24260050730405, 168.46561008218637, 144.81894158573314, 109.33998720610577, 154.16746420248154, 174.9534102327404, 167.0431009322703, 166.58355110309796, 162.4108612821633, 170.06110405762664, 164.9502575847146, 158.54352796643775, 175.05212763626142, 160.67266692784108, 153.8423245527082, 165.48950009402242, 169.24140299600782, 163.6724884464808, 165.7275202799173, 121.72074612798653, 171.82844925068164, 159.4673020960979, 173.97726345486188, 164.91161272361845, 126.39209576077371, 157.2892274294305, 170.68389814197585, 153.45289224851862, 166.2670599141622, 172.97199118665662, 164.7690938086333, 164.2037654533804, 167.98404307438182, 162.6724754278882, 165.75981528806534, 163.7740912043873, 174.02249921360544, 164.21851300019213, 158.3013945352902, 162.195834072285, 173.05596605137555, 165.54536764784726, 157.75530720215696, 122.92901524233316, 171.06087509693342, 176.33164324171082, 165.12932035581673, 146.08547063590072, 159.73472681981886, 173.65008981426672, 170.81188512366026, 157.61139479808867, 174.6504427569252, 124.85274123784627, 175.22738349406563, 135.89218228554918, 159.38105890191147, 169.94157450308768, 117.84805279153109, 163.62111642990817, 171.72457439548438, 144.8895895110715, 136.68089382239523, 159.5884358686868, 170.6975042966955, 166.82542691465343, 170.8736107229413, 154.1715059064039, 160.37945329272242, 173.4765757580039, 169.01309241655107, 164.03999404648022, 159.30202241014447, 175.67572935399465, 174.95368278580028, 162.536193618207, 157.9569214501361, 167.86780705971296, 164.28475104868406, 164.43177055548182, 152.15675816505603, 157.96249308927005, 168.97382415053775, 165.10853678105647, 136.9238280968624, 160.8119378224992]
Elapsed: 0.2896724230625531~0.035874901322492735
Time per graph: 0.0066480211136540285~0.0007840484970843926
Speed: 152.24880326109863~15.68847864805999
Total Time: 0.2680
best val loss: 0.4534991383552551 test_score: 0.7907

Testing...
Test loss: 0.6739 score: 0.6512 time: 0.26s
test Score 0.6512
Epoch Time List: [0.9308716820087284, 1.0008250470273197, 0.8910605830606073, 0.8986643840325996, 1.2048928680596873, 1.0245232820743695, 0.9141289790859446, 0.954673737869598, 0.9244506049435586, 1.0201225739438087, 0.895354763022624, 1.0292730309301987, 0.9313434320501983, 1.07732690195553, 0.8731405141297728, 0.9367791190743446, 1.0198822050588205, 0.9779862450668588, 0.9396700381767005, 0.9832438109442592, 0.9091758731519803, 0.9700436940183863, 1.070584623143077, 1.0124020529910922, 0.9038736498914659, 1.0413906988687813, 0.8869846428278834, 1.0408057861495763, 0.9045546590350568, 1.0586517611518502, 0.8917156427633017, 0.9853319189278409, 1.0319082760252059, 0.9413575540529564, 1.0604381560115144, 0.9912750431103632, 1.0486875601345673, 1.0661472898209468, 0.9961663308786228, 1.1315396119607612, 0.9327207549940795, 0.948064200929366, 1.0030511829536408, 0.93272296898067, 0.937905844883062, 0.9779747619759291, 0.9714287479873747, 0.8812050978885964, 0.9050406600581482, 0.9097144490806386, 0.8997924060095102, 1.0092458518920466, 0.8795682069612667, 1.0117517131147906, 1.0196289740270004, 0.9411605299683288, 0.9474473600275815, 1.0213568268809468, 0.9930250510806218, 0.9277740260586143, 0.9139159169280902, 0.9188103140331805, 1.1166098470566794, 1.02467159088701, 0.9198671649210155, 0.8718791791470721, 0.9910839608637616, 1.0751292039640248, 0.881617454928346, 0.8883409349946305, 0.9296751819783822, 0.997202032012865, 0.9393958690343425, 0.8502499189926311, 0.909675857052207, 0.9551714769331738, 0.8625468100653961, 0.9520921648945659, 1.0122107131173834, 0.9101800330681726, 0.9200177169404924, 0.9417069969931617, 0.9301504408940673, 0.8853813878959045, 1.0821252471068874, 0.9609704029280692, 1.0267642788821831, 1.0573590929852799, 0.8624735639896244, 1.2169232710730284, 1.0591598349856213, 0.7794137632008642, 0.8893354000756517, 0.7734952939208597, 1.1660273919114843, 1.1124680308857933, 0.9150287689408287, 0.875480373040773, 0.9419887799303979, 0.9514530629385263, 0.9920173179125413, 0.9031781021039933, 0.9413556950166821, 1.0924774199957028, 0.8815334849059582, 0.9518148498609662, 1.0448861229233444, 0.9223755898419768, 0.9091171999461949, 0.9226683520246297, 0.8959400691092014, 1.00779413303826, 0.9107025689445436, 0.945757043082267, 0.8864471209235489, 0.9207758679986, 1.0097442279802635, 1.0009616289753467, 0.9194208001717925, 0.9992680719587952, 1.00663001392968, 0.8954323008656502, 0.9138492540223524, 1.020027780905366, 0.924952573957853, 0.9437024970538914, 0.9137854530708864, 0.9063519220799208, 1.0014743119245395, 0.975165749900043, 0.930427851038985, 0.9385789200896397, 0.9164196059573442, 0.938798816059716, 1.0617888600099832, 0.9929897749098018, 0.9154046860057861, 1.0006382499122992, 0.9772866650018841, 0.9963314129272476, 0.9807768198661506, 1.0066104959696531, 0.9217078820802271, 0.9616657790029421, 0.8691334829200059, 1.0130974921630695, 1.0124363211216405, 0.9731926150852814, 0.941939493175596, 0.9201779130380601, 1.0516325869830325, 1.0730497400509194, 1.1009889020351693, 0.9770325820427388, 1.0137342170346528, 0.9378196591278538, 1.0142532208701596, 0.938931715907529, 1.044322469853796, 0.9578687680186704, 0.9350975108100101, 1.0194682870060205, 1.032431696075946, 0.9828459190903232, 0.9330404520733282, 0.9064392760628834, 1.0432648899732158, 0.9292009288910776, 0.952873517991975, 0.9302756269462407, 0.9575413159327582, 1.0348740429617465, 1.0499134819256142, 0.9167365629691631, 0.9144320950144902, 0.9809326099930331, 1.0717271878384054, 0.9641983619658276]
Total Epoch List: [97, 81]
Total Time List: [0.28585293295327574, 0.2680231999838725]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fccaf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.26s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.35s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.42s
Val loss: 0.6930 score: 0.5682 time: 0.28s
Test loss: 0.6930 score: 0.5349 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.26s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.36s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.26s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.26s
Epoch 26/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4884 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4884 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4884 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4884 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.4884 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6862 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.4884 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.4884 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.4884 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.4884 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6824 score: 0.4884 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6812 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6811 score: 0.4884 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6797 score: 0.4884 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6746;  Loss pred: 0.6746; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6785 score: 0.5000 time: 0.28s
Test loss: 0.6782 score: 0.5116 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.5000 time: 0.29s
Test loss: 0.6765 score: 0.5116 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6705;  Loss pred: 0.6705; Loss self: 0.0000; time: 0.48s
Val loss: 0.6753 score: 0.5227 time: 0.29s
Test loss: 0.6746 score: 0.5116 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6701;  Loss pred: 0.6701; Loss self: 0.0000; time: 0.41s
Val loss: 0.6734 score: 0.5455 time: 0.27s
Test loss: 0.6725 score: 0.5349 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.36s
Val loss: 0.6714 score: 0.5455 time: 0.29s
Test loss: 0.6701 score: 0.5349 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6631;  Loss pred: 0.6631; Loss self: 0.0000; time: 0.49s
Val loss: 0.6691 score: 0.5682 time: 0.29s
Test loss: 0.6675 score: 0.5581 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.41s
Val loss: 0.6666 score: 0.5682 time: 0.28s
Test loss: 0.6646 score: 0.5349 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.42s
Val loss: 0.6639 score: 0.5909 time: 0.27s
Test loss: 0.6614 score: 0.5349 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.39s
Val loss: 0.6608 score: 0.6364 time: 0.31s
Test loss: 0.6579 score: 0.5349 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6461;  Loss pred: 0.6461; Loss self: 0.0000; time: 0.39s
Val loss: 0.6575 score: 0.6364 time: 0.30s
Test loss: 0.6541 score: 0.5349 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6432;  Loss pred: 0.6432; Loss self: 0.0000; time: 0.41s
Val loss: 0.6539 score: 0.6364 time: 0.32s
Test loss: 0.6500 score: 0.5814 time: 0.35s
Epoch 51/1000, LR 0.000269
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.40s
Val loss: 0.6500 score: 0.6818 time: 0.27s
Test loss: 0.6456 score: 0.6047 time: 0.32s
Epoch 52/1000, LR 0.000269
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.42s
Val loss: 0.6458 score: 0.7045 time: 0.27s
Test loss: 0.6408 score: 0.6047 time: 0.26s
Epoch 53/1000, LR 0.000269
Train loss: 0.6276;  Loss pred: 0.6276; Loss self: 0.0000; time: 0.38s
Val loss: 0.6412 score: 0.6818 time: 0.38s
Test loss: 0.6357 score: 0.6744 time: 0.26s
Epoch 54/1000, LR 0.000269
Train loss: 0.6189;  Loss pred: 0.6189; Loss self: 0.0000; time: 0.38s
Val loss: 0.6364 score: 0.7273 time: 0.29s
Test loss: 0.6303 score: 0.7442 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.40s
Val loss: 0.6311 score: 0.7727 time: 0.30s
Test loss: 0.6245 score: 0.7442 time: 0.34s
Epoch 56/1000, LR 0.000269
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.40s
Val loss: 0.6255 score: 0.8182 time: 0.27s
Test loss: 0.6183 score: 0.7907 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5980;  Loss pred: 0.5980; Loss self: 0.0000; time: 0.37s
Val loss: 0.6195 score: 0.8182 time: 0.27s
Test loss: 0.6117 score: 0.7907 time: 0.27s
Epoch 58/1000, LR 0.000269
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.42s
Val loss: 0.6131 score: 0.8182 time: 0.29s
Test loss: 0.6048 score: 0.7907 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.46s
Val loss: 0.6061 score: 0.8409 time: 0.30s
Test loss: 0.5975 score: 0.8140 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.42s
Val loss: 0.5987 score: 0.8636 time: 0.28s
Test loss: 0.5901 score: 0.8140 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.35s
Val loss: 0.5907 score: 0.8636 time: 0.40s
Test loss: 0.5824 score: 0.8140 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.43s
Val loss: 0.5823 score: 0.8636 time: 0.30s
Test loss: 0.5744 score: 0.8140 time: 0.23s
Epoch 63/1000, LR 0.000268
Train loss: 0.5366;  Loss pred: 0.5366; Loss self: 0.0000; time: 0.39s
Val loss: 0.5735 score: 0.8636 time: 0.28s
Test loss: 0.5660 score: 0.8140 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.43s
Val loss: 0.5644 score: 0.8636 time: 0.27s
Test loss: 0.5572 score: 0.8140 time: 0.28s
Epoch 65/1000, LR 0.000268
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.36s
Val loss: 0.5550 score: 0.8864 time: 0.30s
Test loss: 0.5478 score: 0.7907 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.4981;  Loss pred: 0.4981; Loss self: 0.0000; time: 0.42s
Val loss: 0.5454 score: 0.9091 time: 0.41s
Test loss: 0.5379 score: 0.8140 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.4779;  Loss pred: 0.4779; Loss self: 0.0000; time: 0.48s
Val loss: 0.5354 score: 0.9091 time: 0.29s
Test loss: 0.5279 score: 0.8140 time: 0.24s
Epoch 68/1000, LR 0.000268
Train loss: 0.4571;  Loss pred: 0.4571; Loss self: 0.0000; time: 0.39s
Val loss: 0.5250 score: 0.9091 time: 0.27s
Test loss: 0.5183 score: 0.8140 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4571;  Loss pred: 0.4571; Loss self: 0.0000; time: 0.53s
Val loss: 0.5144 score: 0.9091 time: 0.28s
Test loss: 0.5085 score: 0.8140 time: 0.27s
Epoch 70/1000, LR 0.000268
Train loss: 0.4398;  Loss pred: 0.4398; Loss self: 0.0000; time: 0.36s
Val loss: 0.5036 score: 0.9091 time: 0.27s
Test loss: 0.4984 score: 0.8140 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.4254;  Loss pred: 0.4254; Loss self: 0.0000; time: 0.40s
Val loss: 0.4927 score: 0.9091 time: 0.41s
Test loss: 0.4884 score: 0.8140 time: 0.23s
Epoch 72/1000, LR 0.000267
Train loss: 0.4089;  Loss pred: 0.4089; Loss self: 0.0000; time: 0.39s
Val loss: 0.4816 score: 0.9091 time: 0.34s
Test loss: 0.4789 score: 0.8140 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.3917;  Loss pred: 0.3917; Loss self: 0.0000; time: 0.49s
Val loss: 0.4703 score: 0.9318 time: 0.27s
Test loss: 0.4696 score: 0.8372 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.3785;  Loss pred: 0.3785; Loss self: 0.0000; time: 0.37s
Val loss: 0.4592 score: 0.9318 time: 0.28s
Test loss: 0.4603 score: 0.8372 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.3540;  Loss pred: 0.3540; Loss self: 0.0000; time: 0.40s
Val loss: 0.4481 score: 0.9091 time: 0.30s
Test loss: 0.4510 score: 0.8605 time: 0.29s
Epoch 76/1000, LR 0.000267
Train loss: 0.3748;  Loss pred: 0.3748; Loss self: 0.0000; time: 0.39s
Val loss: 0.4373 score: 0.9091 time: 0.28s
Test loss: 0.4417 score: 0.8605 time: 0.35s
Epoch 77/1000, LR 0.000267
Train loss: 0.3500;  Loss pred: 0.3500; Loss self: 0.0000; time: 0.49s
Val loss: 0.4272 score: 0.8864 time: 0.31s
Test loss: 0.4319 score: 0.8605 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.3227;  Loss pred: 0.3227; Loss self: 0.0000; time: 0.42s
Val loss: 0.4176 score: 0.9091 time: 0.27s
Test loss: 0.4226 score: 0.8605 time: 0.26s
Epoch 79/1000, LR 0.000267
Train loss: 0.3106;  Loss pred: 0.3106; Loss self: 0.0000; time: 0.37s
Val loss: 0.4076 score: 0.9091 time: 0.29s
Test loss: 0.4146 score: 0.8605 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.2992;  Loss pred: 0.2992; Loss self: 0.0000; time: 0.40s
Val loss: 0.3979 score: 0.9091 time: 0.40s
Test loss: 0.4072 score: 0.8605 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.2737;  Loss pred: 0.2737; Loss self: 0.0000; time: 0.40s
Val loss: 0.3867 score: 0.9091 time: 0.27s
Test loss: 0.4019 score: 0.8605 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.2647;  Loss pred: 0.2647; Loss self: 0.0000; time: 0.40s
Val loss: 0.3754 score: 0.9091 time: 0.37s
Test loss: 0.3977 score: 0.8605 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.2515;  Loss pred: 0.2515; Loss self: 0.0000; time: 0.35s
Val loss: 0.3660 score: 0.9091 time: 0.27s
Test loss: 0.3928 score: 0.8605 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2335;  Loss pred: 0.2335; Loss self: 0.0000; time: 0.39s
Val loss: 0.3572 score: 0.9091 time: 0.38s
Test loss: 0.3883 score: 0.8372 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.2260;  Loss pred: 0.2260; Loss self: 0.0000; time: 0.42s
Val loss: 0.3492 score: 0.9091 time: 0.27s
Test loss: 0.3843 score: 0.8372 time: 0.25s
Epoch 86/1000, LR 0.000266
Train loss: 0.1885;  Loss pred: 0.1885; Loss self: 0.0000; time: 0.41s
Val loss: 0.3411 score: 0.9318 time: 0.28s
Test loss: 0.3816 score: 0.8372 time: 0.26s
Epoch 87/1000, LR 0.000266
Train loss: 0.2127;  Loss pred: 0.2127; Loss self: 0.0000; time: 0.38s
Val loss: 0.3331 score: 0.9318 time: 0.37s
Test loss: 0.3799 score: 0.8372 time: 0.31s
Epoch 88/1000, LR 0.000266
Train loss: 0.2003;  Loss pred: 0.2003; Loss self: 0.0000; time: 0.39s
Val loss: 0.3258 score: 0.9318 time: 0.33s
Test loss: 0.3789 score: 0.8372 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.1757;  Loss pred: 0.1757; Loss self: 0.0000; time: 0.44s
Val loss: 0.3195 score: 0.9318 time: 0.29s
Test loss: 0.3785 score: 0.8372 time: 0.23s
Epoch 90/1000, LR 0.000266
Train loss: 0.1652;  Loss pred: 0.1652; Loss self: 0.0000; time: 0.44s
Val loss: 0.3138 score: 0.9318 time: 0.27s
Test loss: 0.3788 score: 0.8372 time: 0.25s
Epoch 91/1000, LR 0.000266
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.41s
Val loss: 0.3083 score: 0.9318 time: 0.28s
Test loss: 0.3803 score: 0.8372 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.39s
Val loss: 0.3006 score: 0.9318 time: 0.34s
Test loss: 0.3838 score: 0.8372 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.1287;  Loss pred: 0.1287; Loss self: 0.0000; time: 0.53s
Val loss: 0.2935 score: 0.9318 time: 0.29s
Test loss: 0.3880 score: 0.8372 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.1260;  Loss pred: 0.1260; Loss self: 0.0000; time: 0.40s
Val loss: 0.2878 score: 0.9318 time: 0.32s
Test loss: 0.3923 score: 0.8372 time: 0.24s
Epoch 95/1000, LR 0.000265
Train loss: 0.1198;  Loss pred: 0.1198; Loss self: 0.0000; time: 0.43s
Val loss: 0.2866 score: 0.9318 time: 0.27s
Test loss: 0.3949 score: 0.8372 time: 0.27s
Epoch 96/1000, LR 0.000265
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.39s
Val loss: 0.2877 score: 0.9318 time: 0.28s
Test loss: 0.3976 score: 0.8372 time: 0.26s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.1076;  Loss pred: 0.1076; Loss self: 0.0000; time: 0.41s
Val loss: 0.2922 score: 0.9318 time: 0.30s
Test loss: 0.3999 score: 0.8372 time: 0.26s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.1198,   Val_Loss: 0.2866,   Val_Precision: 1.0000,   Val_Recall: 0.8636,   Val_accuracy: 0.9268,   Val_Score: 0.9318,   Val_Loss: 0.2866,   Test_Precision: 0.8500,   Test_Recall: 0.8095,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.3949


[0.30145648296456784, 0.30784547701478004, 0.2891621949383989, 0.29467664391268045, 0.2960956799797714, 0.31403666199184954, 0.2837912170216441, 0.2955553939100355, 0.29974595399107784, 0.3004386250395328, 0.2790172069799155, 0.28815585596021265, 0.2951386340428144, 0.307339645922184, 0.2826175729278475, 0.3099046640563756, 0.39349576900713146, 0.3098684990545735, 0.28780107805505395, 0.289998703985475, 0.2962815349455923, 0.30202430894132704, 0.2949671399546787, 0.2910416550002992, 0.29538421297911555, 0.2978494520066306, 0.27712576603516936, 0.2955536359222606, 0.2990617490140721, 0.3029566069599241, 0.29411642299965024, 0.2914770740317181, 0.30013961996883154, 0.3087229479569942, 0.42288610292598605, 0.32130664808209985, 0.2857426719274372, 0.31479454203508794, 0.3094706259435043, 0.3215484319953248, 0.30249289399944246, 0.29721524694468826, 0.28834371897391975, 0.3185790580464527, 0.3070441969903186, 0.3747744830325246, 0.2912926949793473, 0.28941438102629036, 0.2944973179837689, 0.2944468220230192, 0.29239818395581096, 0.39803462906274945, 0.27975055295974016, 0.29094268300104886, 0.3957817319314927, 0.3106901290593669, 0.2960074780276045, 0.2772509370697662, 0.2951457779854536, 0.30785213503986597, 0.2976641490822658, 0.2940414589829743, 0.28574819304049015, 0.37744537903927267, 0.30744399200193584, 0.2802602360025048, 0.2937864949926734, 0.3442324351053685, 0.2946819399949163, 0.2820565849542618, 0.32442133605945855, 0.3750028039794415, 0.3022641170537099, 0.2773243800038472, 0.30118676205165684, 0.3054999919841066, 0.2785948159871623, 0.312978454050608, 0.3994534839875996, 0.29462140891700983, 0.2884692510124296, 0.30846140207722783, 0.299586525070481, 0.2779186828993261, 0.4006521450355649, 0.28628904500510544, 0.33035338507033885, 0.3284180200425908, 0.2723340759985149, 0.3044337809551507, 0.3628275890368968, 0.25909386295825243, 0.26953748508822173, 0.26118090201634914, 0.30382765899412334, 0.40241453400813043, 0.2854039289522916, 0.2457797189708799, 0.2574185929261148, 0.25812872708775103, 0.26476061798166484, 0.2528502930654213, 0.2606846489943564, 0.2712188920704648, 0.24564111605286598, 0.26762486004736274, 0.279506957042031, 0.25983521598391235, 0.2540749440668151, 0.2627197790425271, 0.2594620370073244, 0.3532676340546459, 0.25024959596339613, 0.26964775496162474, 0.24715873296372592, 0.2607457370031625, 0.34021114802453667, 0.27338172297459096, 0.25192768895067275, 0.2802162889856845, 0.25862007797695696, 0.2485951610142365, 0.2609712720150128, 0.261869756039232, 0.2559766940539703, 0.2643348230049014, 0.2594114859821275, 0.2625567920040339, 0.24709448602516204, 0.26184623898006976, 0.2716337409801781, 0.2651116179767996, 0.24847453099209815, 0.25974752788897604, 0.2725740310270339, 0.3497953669866547, 0.2513725010212511, 0.2438586700009182, 0.2604019680293277, 0.29434823198243976, 0.2691963160177693, 0.2476244040299207, 0.25173892301972955, 0.27282291394658387, 0.24620607495307922, 0.34440573409665376, 0.24539543502032757, 0.31642732699401677, 0.2697936649201438, 0.253028137027286, 0.36487662699073553, 0.26280226500239223, 0.25040096999146044, 0.29677770601119846, 0.314601395977661, 0.2694430819246918, 0.25190760800614953, 0.257754473015666, 0.2516479860059917, 0.2789101640228182, 0.26811414503026754, 0.2478720819344744, 0.2544181600678712, 0.2621311970287934, 0.2699275210034102, 0.2447691559791565, 0.24577933608088642, 0.26455645996611565, 0.2722261209273711, 0.25615393894258887, 0.2617406650679186, 0.2615066410508007, 0.2826032870216295, 0.2722165189916268, 0.25447728496510535, 0.2604347469750792, 0.3140432209474966, 0.2673930840101093, 0.26769693894311786, 0.23360550100915134, 0.35580419609323144, 0.2507946180412546, 0.25635715504176915, 0.23556200799066573, 0.25443795998580754, 0.2654150730231777, 0.2387067690724507, 0.2479104120284319, 0.25363047199789435, 0.36257016903255135, 0.24505739693995565, 0.25381682498846203, 0.25566263194195926, 0.26150778494775295, 0.24250877206213772, 0.25375722104217857, 0.25040004192851484, 0.2550749310757965, 0.24100828997325152, 0.2514037189539522, 0.2577219909289852, 0.2538353760028258, 0.26345492398831993, 0.2416851760353893, 0.25297332101035863, 0.256019469932653, 0.25274283497128636, 0.24971483496483415, 0.23281830502673984, 0.25428384402766824, 0.23795330699067563, 0.2559631100157276, 0.24570904904976487, 0.2590455689933151, 0.23890965501777828, 0.23336024209856987, 0.25831150100566447, 0.256510074948892, 0.23764326295349747, 0.25371414597611874, 0.2573245920939371, 0.25893662800081074, 0.24655685399193317, 0.24195098702330142, 0.2644430290674791, 0.25478495901916176, 0.2396400619763881, 0.35044452000875026, 0.3252716528950259, 0.2672530689742416, 0.2657253179932013, 0.23220373806543648, 0.34796968195587397, 0.2577553780283779, 0.2747798199998215, 0.23319055291358382, 0.2507748659700155, 0.25722891406621784, 0.2564614610746503, 0.2344819949939847, 0.24998853204306215, 0.2825357320252806, 0.25953375501558185, 0.23991598701104522, 0.2488872678950429, 0.2495075420010835, 0.2768094629282132, 0.24834377702791244, 0.23867778794374317, 0.2394226840697229, 0.2565505390521139, 0.24212479405105114, 0.2921240460127592, 0.3532803049311042, 0.24276259494945407, 0.25981350895017385, 0.23661970999091864, 0.2462622419698164, 0.2467649170430377, 0.25176187697798014, 0.23631675401702523, 0.25333388603758067, 0.2543669600272551, 0.2675471230177209, 0.31835722201503813, 0.2404334960738197, 0.2392141108866781, 0.25248851103242487, 0.2547422719653696, 0.2342584440484643, 0.23276898497715592, 0.2434655309189111, 0.2733889080118388, 0.2657369659282267, 0.26758957793936133]
[0.006851283703740178, 0.006996488113972274, 0.0065718680667817934, 0.006697196452560919, 0.006729447272267531, 0.007137196863451126, 0.006449800386855548, 0.006717168043409897, 0.006812408045251769, 0.0068281505690802905, 0.0063413001586344435, 0.006548996726368469, 0.006707696228245782, 0.006984991952776909, 0.00642312665745108, 0.007043287819463082, 0.008943085659252987, 0.007042465887603943, 0.006540933592160317, 0.006590879636033523, 0.006733671248763461, 0.006864188839575614, 0.006703798635333607, 0.006614583068188618, 0.006713277567707171, 0.0067693057274234225, 0.0062983128644356675, 0.006717128089142286, 0.006796857932138003, 0.006885377430907366, 0.00668446415908296, 0.00662447895526632, 0.006821354999291626, 0.007016430635386231, 0.00961104779377241, 0.007302423820047724, 0.006494151634714482, 0.007154421409888362, 0.007033423316897824, 0.007307918908984654, 0.006874838499987329, 0.006754891976015642, 0.006553266340316358, 0.00724043313741938, 0.006978277204325423, 0.008517601887102832, 0.0066202885222578925, 0.0065775995687793265, 0.006693120863267474, 0.006691973227795891, 0.006645413271722977, 0.009046241569607942, 0.006357967112721367, 0.006612333704569292, 0.008995039362079378, 0.007061139296803793, 0.006727442682445558, 0.006301157660676505, 0.006707858590578491, 0.006996639432724227, 0.006765094297324223, 0.006682760431431234, 0.006494277114556594, 0.008578304069074378, 0.006987363454589451, 0.006369550818238746, 0.006676965795288032, 0.00782346443421292, 0.00669731681806628, 0.006410376930778677, 0.007373212183169512, 0.008522790999532763, 0.006869639023947952, 0.0063028268182692545, 0.006845153682992201, 0.006943181636002423, 0.0063317003633445975, 0.007113146682968363, 0.009078488272445446, 0.006695941111750223, 0.006556119341191582, 0.007010486410846087, 0.00680878466069275, 0.0063163337022574114, 0.009105730568990111, 0.006506569204661487, 0.007508031478871338, 0.007464045910058881, 0.006189410818148066, 0.006918949567162516, 0.008246081569020382, 0.005888496885414828, 0.006125851933823221, 0.005935929591280662, 0.006905174068048258, 0.009145784863821145, 0.0064864529307339, 0.00571580741792744, 0.0059864789052584836, 0.006002993653203512, 0.006157223673992206, 0.005880239373614448, 0.0060624336975431715, 0.0063074160946619716, 0.005712584094252697, 0.0062238339545898315, 0.0065001617916751395, 0.006042679441486334, 0.005908719629460816, 0.006109762303314583, 0.006034000860635451, 0.008215526373363858, 0.005819758045660375, 0.006270878022363366, 0.005747877510784324, 0.006063854348910756, 0.007911887163361319, 0.006357714487781185, 0.005858783463969133, 0.0065166578833880115, 0.006014420418068767, 0.00578128281428457, 0.0060690993491863444, 0.006089994326493767, 0.0059529463733481456, 0.006147321465230266, 0.006032825255398314, 0.006105971907070557, 0.005746383395934001, 0.0060894474181411576, 0.006317063743725072, 0.006165386464576735, 0.005778477464932515, 0.006040640183464559, 0.006338930954117068, 0.00813477597643383, 0.005845872116773281, 0.00567113186048647, 0.006055859721612272, 0.006845307720521855, 0.006260379442273704, 0.005758707070463272, 0.005854393558598361, 0.006344718928990322, 0.005725722673327424, 0.008009435676666366, 0.005706870581868083, 0.007358775046372483, 0.006274271277212646, 0.0058843752797043255, 0.00848550295327292, 0.006111680581450982, 0.005823278371894429, 0.006901807116539499, 0.00731631153436421, 0.006266118184295158, 0.005858316465259292, 0.005994290070131768, 0.0058522787443253885, 0.006486282884251586, 0.006235212675122501, 0.005764467021731963, 0.0059167013969272375, 0.006096074349506823, 0.006277384209381633, 0.0056923059530036395, 0.0057157985135089864, 0.00615247581316548, 0.006330840021566769, 0.0059570683475020664, 0.006086992210881828, 0.006081549791879085, 0.006572169465619291, 0.006330616720735507, 0.005918076394537334, 0.00605662202267626, 0.007303330719709223, 0.006218443814188589, 0.006225510207979485, 0.005432686069980264, 0.008274516188214684, 0.0058324329777035945, 0.005961794303296957, 0.005478186232341063, 0.005917161860135059, 0.006172443558678551, 0.005551320210987225, 0.005765358419265858, 0.005898383069718473, 0.008431864396105845, 0.0056990092311617595, 0.005902716860196791, 0.005945642603301379, 0.00608157639413379, 0.0056397388851659935, 0.00590133072191113, 0.005823256789035228, 0.0059319751412975925, 0.0056048439528663145, 0.005846598115208191, 0.005993534672767097, 0.005903148279135483, 0.006126858697402789, 0.0056205854891951, 0.005883100488612992, 0.0059539411612244885, 0.00587774034816945, 0.005807321743368236, 0.005414379186668369, 0.005913577768085308, 0.005533797836992456, 0.005952630465482037, 0.0057141639313898806, 0.006024315557984072, 0.005556038488785542, 0.005426982374385346, 0.006007244209434057, 0.005965350580206791, 0.005526587510546453, 0.0059003289761888076, 0.005984292839393886, 0.006021782046530482, 0.005733880325393795, 0.005626767140076777, 0.006149837885290211, 0.005925231605096785, 0.005573024697125305, 0.008149872558343029, 0.00756445704407037, 0.006215187650563759, 0.006179658557981426, 0.005400086931754337, 0.008092318185020325, 0.0059943111169390205, 0.006390228372088871, 0.005423036114269391, 0.005831973627209663, 0.00598206776898181, 0.005964220024991867, 0.0054530696510229, 0.00581368679169912, 0.006570598419192572, 0.006035668721292601, 0.0055794415583964, 0.005788075997559137, 0.005802500976769384, 0.006437429370423562, 0.005775436675067731, 0.0055506462312498416, 0.005567969396970299, 0.0059662916058631135, 0.005630809163977933, 0.006793582465413005, 0.008215821044909399, 0.00564564174301056, 0.006042174626748229, 0.0055027839532771775, 0.0057270288830189855, 0.005738719001000877, 0.005854927371580934, 0.005495738465512215, 0.005891485721804202, 0.005915510698308258, 0.006222026116691183, 0.0074036563259311195, 0.005591476652879528, 0.005563118857829724, 0.005871825837963369, 0.0059242388829155715, 0.005447870791824751, 0.005413232208771068, 0.0056619890911374674, 0.006357881581670669, 0.006179929440191319, 0.006223013440450263]
[145.95804862876878, 142.9288499758842, 152.1637363742292, 149.31621120620005, 148.6006145142205, 140.1110294604455, 155.04355794296558, 148.8722618724841, 146.7909722021125, 146.45254082829837, 157.6963674615496, 152.69514427662833, 149.0824816706888, 143.164087626822, 155.68741725495335, 141.97914747096493, 111.81822897618646, 141.99571797148312, 152.88337450766312, 151.72481599160457, 148.5073985730496, 145.68363769867176, 149.16915832306114, 151.18110842227983, 148.95853626107944, 147.72563690672996, 158.77267794152374, 148.87314738220076, 147.1268062366933, 145.23532079899627, 149.60062260804906, 150.95526859588273, 146.59843976802946, 142.5226089967543, 104.04692822857062, 136.94083288546577, 153.98470135105885, 139.77370673439322, 142.178275776107, 136.83786211291414, 145.4579623945847, 148.04085743349634, 152.5956596404298, 138.11328424978979, 143.30184524343042, 117.4039375465738, 151.05081850102562, 152.03114594365317, 149.40713314891732, 149.43275562525915, 150.4797307723688, 110.54314571475005, 157.28297776173542, 151.23253675309434, 111.1723873289233, 141.62020574394384, 148.64489334251456, 158.70099652333377, 149.07887315998997, 142.92575880398596, 147.81759958549682, 149.6387623438765, 153.98172611984026, 116.57315851102753, 143.11549792692958, 156.99694194079945, 149.76862704698968, 127.82061047365201, 149.31352766565558, 155.99706706771278, 135.62609825371, 117.33245600588143, 145.56805627107087, 158.65896824285556, 146.08875801936313, 144.02619035842417, 157.93545850482562, 140.5847572909453, 110.1504975266805, 149.3442046921787, 152.529255182571, 142.64345459009473, 146.8690889540126, 158.3196910009057, 109.82095202833428, 153.6908266930554, 133.19070422308982, 133.97559608420352, 161.5662668679036, 144.53060978302565, 121.26971963955457, 169.82262527418368, 163.24260050730405, 168.46561008218637, 144.81894158573314, 109.33998720610577, 154.16746420248154, 174.9534102327404, 167.0431009322703, 166.58355110309796, 162.4108612821633, 170.06110405762664, 164.9502575847146, 158.54352796643775, 175.05212763626142, 160.67266692784108, 153.8423245527082, 165.48950009402242, 169.24140299600782, 163.6724884464808, 165.7275202799173, 121.72074612798653, 171.82844925068164, 159.4673020960979, 173.97726345486188, 164.91161272361845, 126.39209576077371, 157.2892274294305, 170.68389814197585, 153.45289224851862, 166.2670599141622, 172.97199118665662, 164.7690938086333, 164.2037654533804, 167.98404307438182, 162.6724754278882, 165.75981528806534, 163.7740912043873, 174.02249921360544, 164.21851300019213, 158.3013945352902, 162.195834072285, 173.05596605137555, 165.54536764784726, 157.75530720215696, 122.92901524233316, 171.06087509693342, 176.33164324171082, 165.12932035581673, 146.08547063590072, 159.73472681981886, 173.65008981426672, 170.81188512366026, 157.61139479808867, 174.6504427569252, 124.85274123784627, 175.22738349406563, 135.89218228554918, 159.38105890191147, 169.94157450308768, 117.84805279153109, 163.62111642990817, 171.72457439548438, 144.8895895110715, 136.68089382239523, 159.5884358686868, 170.6975042966955, 166.82542691465343, 170.8736107229413, 154.1715059064039, 160.37945329272242, 173.4765757580039, 169.01309241655107, 164.03999404648022, 159.30202241014447, 175.67572935399465, 174.95368278580028, 162.536193618207, 157.9569214501361, 167.86780705971296, 164.28475104868406, 164.43177055548182, 152.15675816505603, 157.96249308927005, 168.97382415053775, 165.10853678105647, 136.9238280968624, 160.8119378224992, 160.62940491499958, 184.0710078069416, 120.85298732320938, 171.45503494387864, 167.7347370819194, 182.5421695407856, 168.9999401127038, 162.0103919126137, 180.137329859083, 173.44975407918122, 169.53798832325236, 118.59773272229543, 175.46909637065934, 169.4135130795789, 168.1903987038743, 164.43105129199515, 177.31317359927155, 169.4533058937176, 171.72521086188877, 168.57791480583558, 178.4170992822379, 171.03963369720876, 166.84645281918753, 169.4011318561102, 163.21577653225555, 177.917407701098, 169.97839862425357, 167.95597620490085, 170.13340854899073, 172.19641758990983, 184.69338136905228, 169.10236733451785, 180.70772179554112, 167.9929580374214, 175.00372968067188, 165.99396070391634, 179.98435432339554, 184.2644643033798, 166.4656812901918, 167.63474108596895, 180.94348421909325, 169.4820753275911, 167.10412188005228, 166.06379843590673, 174.4019657283868, 177.72194496507188, 162.6059123268759, 168.76977418736118, 179.43577399106508, 122.70130518498715, 132.19719461344295, 160.8961878905287, 161.82123827997515, 185.18220403446875, 123.57398425720558, 166.824841168846, 156.48892993680516, 184.3985507248873, 171.468539455391, 167.16627738408368, 167.66651729978113, 183.38295015403253, 172.00789031631646, 152.1931392244308, 165.68172412647584, 179.2294066590084, 172.768982373712, 172.33947982146876, 155.3415101677773, 173.1470806903571, 180.1592027915693, 179.598688265803, 167.60830111241856, 177.59436892255488, 147.19774214725848, 121.71638044862351, 177.12778201663681, 165.50332649656949, 181.72619686521602, 174.61060882110536, 174.25491644138566, 170.79631164237352, 181.95916823105188, 169.7364717865702, 169.04711207537568, 160.7193510997011, 135.06839809642773, 178.8436332797732, 179.75528216381102, 170.30477871715078, 168.79805486639276, 183.55795102567996, 184.7325149620773, 176.61637701938855, 157.28509365178024, 161.81414523869418, 160.6938518724532]
Elapsed: 0.27854408753485504~0.03631235379202035
Time per graph: 0.006420455507049354~0.0007953745297708866
Speed: 157.8314799322425~16.99833472588904
Total Time: 0.2684
best val loss: 0.28664976358413696 test_score: 0.8372

Testing...
Test loss: 0.4696 score: 0.8372 time: 0.34s
test Score 0.8372
Epoch Time List: [0.9308716820087284, 1.0008250470273197, 0.8910605830606073, 0.8986643840325996, 1.2048928680596873, 1.0245232820743695, 0.9141289790859446, 0.954673737869598, 0.9244506049435586, 1.0201225739438087, 0.895354763022624, 1.0292730309301987, 0.9313434320501983, 1.07732690195553, 0.8731405141297728, 0.9367791190743446, 1.0198822050588205, 0.9779862450668588, 0.9396700381767005, 0.9832438109442592, 0.9091758731519803, 0.9700436940183863, 1.070584623143077, 1.0124020529910922, 0.9038736498914659, 1.0413906988687813, 0.8869846428278834, 1.0408057861495763, 0.9045546590350568, 1.0586517611518502, 0.8917156427633017, 0.9853319189278409, 1.0319082760252059, 0.9413575540529564, 1.0604381560115144, 0.9912750431103632, 1.0486875601345673, 1.0661472898209468, 0.9961663308786228, 1.1315396119607612, 0.9327207549940795, 0.948064200929366, 1.0030511829536408, 0.93272296898067, 0.937905844883062, 0.9779747619759291, 0.9714287479873747, 0.8812050978885964, 0.9050406600581482, 0.9097144490806386, 0.8997924060095102, 1.0092458518920466, 0.8795682069612667, 1.0117517131147906, 1.0196289740270004, 0.9411605299683288, 0.9474473600275815, 1.0213568268809468, 0.9930250510806218, 0.9277740260586143, 0.9139159169280902, 0.9188103140331805, 1.1166098470566794, 1.02467159088701, 0.9198671649210155, 0.8718791791470721, 0.9910839608637616, 1.0751292039640248, 0.881617454928346, 0.8883409349946305, 0.9296751819783822, 0.997202032012865, 0.9393958690343425, 0.8502499189926311, 0.909675857052207, 0.9551714769331738, 0.8625468100653961, 0.9520921648945659, 1.0122107131173834, 0.9101800330681726, 0.9200177169404924, 0.9417069969931617, 0.9301504408940673, 0.8853813878959045, 1.0821252471068874, 0.9609704029280692, 1.0267642788821831, 1.0573590929852799, 0.8624735639896244, 1.2169232710730284, 1.0591598349856213, 0.7794137632008642, 0.8893354000756517, 0.7734952939208597, 1.1660273919114843, 1.1124680308857933, 0.9150287689408287, 0.875480373040773, 0.9419887799303979, 0.9514530629385263, 0.9920173179125413, 0.9031781021039933, 0.9413556950166821, 1.0924774199957028, 0.8815334849059582, 0.9518148498609662, 1.0448861229233444, 0.9223755898419768, 0.9091171999461949, 0.9226683520246297, 0.8959400691092014, 1.00779413303826, 0.9107025689445436, 0.945757043082267, 0.8864471209235489, 0.9207758679986, 1.0097442279802635, 1.0009616289753467, 0.9194208001717925, 0.9992680719587952, 1.00663001392968, 0.8954323008656502, 0.9138492540223524, 1.020027780905366, 0.924952573957853, 0.9437024970538914, 0.9137854530708864, 0.9063519220799208, 1.0014743119245395, 0.975165749900043, 0.930427851038985, 0.9385789200896397, 0.9164196059573442, 0.938798816059716, 1.0617888600099832, 0.9929897749098018, 0.9154046860057861, 1.0006382499122992, 0.9772866650018841, 0.9963314129272476, 0.9807768198661506, 1.0066104959696531, 0.9217078820802271, 0.9616657790029421, 0.8691334829200059, 1.0130974921630695, 1.0124363211216405, 0.9731926150852814, 0.941939493175596, 0.9201779130380601, 1.0516325869830325, 1.0730497400509194, 1.1009889020351693, 0.9770325820427388, 1.0137342170346528, 0.9378196591278538, 1.0142532208701596, 0.938931715907529, 1.044322469853796, 0.9578687680186704, 0.9350975108100101, 1.0194682870060205, 1.032431696075946, 0.9828459190903232, 0.9330404520733282, 0.9064392760628834, 1.0432648899732158, 0.9292009288910776, 0.952873517991975, 0.9302756269462407, 0.9575413159327582, 1.0348740429617465, 1.0499134819256142, 0.9167365629691631, 0.9144320950144902, 0.9809326099930331, 1.0717271878384054, 0.9641983619658276, 0.922922695055604, 0.9274713679915294, 1.0307733670342714, 0.9674812880111858, 0.9473253699252382, 0.9160568369552493, 0.9019743909593672, 1.0561362971784547, 0.8684166789753363, 0.9872708327602595, 0.9047934769187123, 1.0283030670834705, 0.9990366959245875, 0.9687580410391092, 0.9594123690621927, 1.0235916380770504, 0.8719170229742303, 0.9662883060518652, 1.0482529590371996, 0.982194370822981, 0.8816375669557601, 0.9729865550762042, 0.9443008800735697, 1.0824008350027725, 0.9614160300698131, 0.9160493650706485, 0.9823196430224925, 0.9380508760223165, 0.9556776869576424, 1.013812375953421, 0.9649362810887396, 0.9097779630683362, 0.9115507279057056, 0.9927661380497739, 1.0389080761233345, 0.9571547718951479, 0.9117984490003437, 1.0000124579528347, 0.9365467131137848, 0.9909255030797794, 0.8862457049544901, 1.0209135169861838, 0.9257725139614195, 0.8995426660403609, 1.0181334579829127, 0.926978825009428, 0.9519190060673282, 0.9465154211502522, 0.926937406999059, 1.0707042560679838, 0.9925260000163689, 0.9564039380056784, 1.0242761018453166, 0.9051199927926064, 1.0443908949382603, 0.9210058629978448, 0.9163968700449914, 0.9455832779640332, 0.9988668828736991, 0.9522531337570399, 1.002440877025947, 0.9621184809366241, 0.9137469140114263, 0.9736572981346399, 0.9129452488850802, 1.0582616288447753, 1.0124549539759755, 0.9108178010210395, 1.08655322692357, 0.8733019010396674, 1.0503200091188774, 0.9697805201867595, 1.013121358002536, 0.8876753869699314, 0.9888821560889482, 1.013915470102802, 1.0354660619050264, 0.939874749048613, 0.89501389907673, 1.0435314000351354, 0.9185649640858173, 1.0252738980343565, 0.8552873369771987, 1.0191702388692647, 0.9362253349972889, 0.9508824530057609, 1.0669608319876716, 0.9548406400717795, 0.9600240361178294, 0.9619631811510772, 0.940477850032039, 0.9565325820585713, 1.0452587900217623, 0.9588660441804677, 0.9623085168423131, 0.9240624901140109, 0.9656586230266839]
Total Epoch List: [97, 81, 97]
Total Time List: [0.28585293295327574, 0.2680231999838725, 0.26836599491070956]
========================training times:9========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcdcf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.28s
Epoch 2/1000, LR 0.000000
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.29s
Epoch 3/1000, LR 0.000030
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.31s
Epoch 4/1000, LR 0.000060
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.28s
Epoch 5/1000, LR 0.000090
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5116 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.28s
Epoch 6/1000, LR 0.000120
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.29s
Epoch 7/1000, LR 0.000150
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.30s
Epoch 8/1000, LR 0.000180
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.29s
Epoch 9/1000, LR 0.000210
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.29s
Epoch 10/1000, LR 0.000240
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.30s
Epoch 11/1000, LR 0.000270
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.29s
Epoch 12/1000, LR 0.000270
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.28s
Epoch 13/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.29s
Epoch 14/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.31s
Epoch 15/1000, LR 0.000270
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.28s
Epoch 16/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.29s
Epoch 17/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.42s
Epoch 18/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.32s
Epoch 19/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.28s
Epoch 20/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.29s
Epoch 21/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.30s
Epoch 22/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.28s
Epoch 23/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.28s
Epoch 24/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.29s
Epoch 25/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.30s
Epoch 26/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.28s
Epoch 27/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.29s
Epoch 28/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5116 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.29s
Epoch 29/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.30s
Epoch 30/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.27s
Epoch 31/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.29s
Epoch 32/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.39s
Val loss: 0.6892 score: 0.5581 time: 0.24s
Test loss: 0.6884 score: 0.5227 time: 0.30s
Epoch 33/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.38s
Val loss: 0.6887 score: 0.5814 time: 0.35s
Test loss: 0.6878 score: 0.5455 time: 0.30s
Epoch 34/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.33s
Val loss: 0.6882 score: 0.5814 time: 0.27s
Test loss: 0.6871 score: 0.5455 time: 0.28s
Epoch 35/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.41s
Val loss: 0.6876 score: 0.6047 time: 0.33s
Test loss: 0.6864 score: 0.5909 time: 0.29s
Epoch 36/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.38s
Val loss: 0.6870 score: 0.5814 time: 0.24s
Test loss: 0.6856 score: 0.6591 time: 0.29s
Epoch 37/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.33s
Val loss: 0.6862 score: 0.6279 time: 0.26s
Test loss: 0.6847 score: 0.6591 time: 0.29s
Epoch 38/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.37s
Val loss: 0.6854 score: 0.6512 time: 0.26s
Test loss: 0.6838 score: 0.7273 time: 0.29s
Epoch 39/1000, LR 0.000269
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.37s
Val loss: 0.6845 score: 0.6977 time: 0.25s
Test loss: 0.6827 score: 0.7273 time: 0.37s
Epoch 40/1000, LR 0.000269
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.50s
Val loss: 0.6834 score: 0.7442 time: 0.26s
Test loss: 0.6816 score: 0.7273 time: 0.32s
Epoch 41/1000, LR 0.000269
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.36s
Val loss: 0.6823 score: 0.7442 time: 0.25s
Test loss: 0.6803 score: 0.7273 time: 0.29s
Epoch 42/1000, LR 0.000269
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.38s
Val loss: 0.6811 score: 0.7674 time: 0.25s
Test loss: 0.6790 score: 0.7273 time: 0.30s
Epoch 43/1000, LR 0.000269
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.35s
Val loss: 0.6797 score: 0.8140 time: 0.26s
Test loss: 0.6775 score: 0.7500 time: 0.30s
Epoch 44/1000, LR 0.000269
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 0.37s
Val loss: 0.6782 score: 0.7907 time: 0.25s
Test loss: 0.6758 score: 0.7727 time: 0.29s
Epoch 45/1000, LR 0.000269
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.37s
Val loss: 0.6765 score: 0.8605 time: 0.25s
Test loss: 0.6741 score: 0.8182 time: 0.29s
Epoch 46/1000, LR 0.000269
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.36s
Val loss: 0.6747 score: 0.8372 time: 0.26s
Test loss: 0.6721 score: 0.8636 time: 0.41s
Epoch 47/1000, LR 0.000269
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.37s
Val loss: 0.6727 score: 0.8372 time: 0.27s
Test loss: 0.6700 score: 0.8864 time: 0.30s
Epoch 48/1000, LR 0.000269
Train loss: 0.6675;  Loss pred: 0.6675; Loss self: 0.0000; time: 0.39s
Val loss: 0.6705 score: 0.8605 time: 0.25s
Test loss: 0.6677 score: 0.8864 time: 0.30s
Epoch 49/1000, LR 0.000269
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.48s
Val loss: 0.6680 score: 0.8605 time: 0.26s
Test loss: 0.6652 score: 0.8864 time: 0.30s
Epoch 50/1000, LR 0.000269
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.38s
Val loss: 0.6652 score: 0.8605 time: 0.27s
Test loss: 0.6623 score: 0.8864 time: 0.28s
Epoch 51/1000, LR 0.000269
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.44s
Val loss: 0.6622 score: 0.8605 time: 0.28s
Test loss: 0.6593 score: 0.8864 time: 0.29s
Epoch 52/1000, LR 0.000269
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.37s
Val loss: 0.6589 score: 0.8605 time: 0.24s
Test loss: 0.6559 score: 0.8864 time: 0.41s
Epoch 53/1000, LR 0.000269
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.39s
Val loss: 0.6553 score: 0.8605 time: 0.25s
Test loss: 0.6521 score: 0.8864 time: 0.31s
Epoch 54/1000, LR 0.000269
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.36s
Val loss: 0.6514 score: 0.8605 time: 0.28s
Test loss: 0.6481 score: 0.8864 time: 0.37s
Epoch 55/1000, LR 0.000269
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.37s
Val loss: 0.6472 score: 0.8605 time: 0.26s
Test loss: 0.6437 score: 0.8864 time: 0.28s
Epoch 56/1000, LR 0.000269
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.37s
Val loss: 0.6427 score: 0.8605 time: 0.26s
Test loss: 0.6390 score: 0.8864 time: 0.36s
Epoch 57/1000, LR 0.000269
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.38s
Val loss: 0.6378 score: 0.8605 time: 0.24s
Test loss: 0.6341 score: 0.8864 time: 0.40s
Epoch 58/1000, LR 0.000269
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 0.53s
Val loss: 0.6325 score: 0.8605 time: 0.25s
Test loss: 0.6288 score: 0.8864 time: 0.30s
Epoch 59/1000, LR 0.000268
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 0.38s
Val loss: 0.6268 score: 0.8605 time: 0.25s
Test loss: 0.6231 score: 0.8864 time: 0.29s
Epoch 60/1000, LR 0.000268
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.37s
Val loss: 0.6207 score: 0.8605 time: 0.27s
Test loss: 0.6172 score: 0.8864 time: 0.32s
Epoch 61/1000, LR 0.000268
Train loss: 0.6028;  Loss pred: 0.6028; Loss self: 0.0000; time: 0.47s
Val loss: 0.6141 score: 0.8605 time: 0.27s
Test loss: 0.6109 score: 0.8864 time: 0.28s
Epoch 62/1000, LR 0.000268
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 0.38s
Val loss: 0.6072 score: 0.8605 time: 0.34s
Test loss: 0.6041 score: 0.8864 time: 0.30s
Epoch 63/1000, LR 0.000268
Train loss: 0.5855;  Loss pred: 0.5855; Loss self: 0.0000; time: 0.42s
Val loss: 0.5999 score: 0.8605 time: 0.36s
Test loss: 0.5969 score: 0.8864 time: 0.30s
Epoch 64/1000, LR 0.000268
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.37s
Val loss: 0.5922 score: 0.8837 time: 0.26s
Test loss: 0.5894 score: 0.8636 time: 0.30s
Epoch 65/1000, LR 0.000268
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 0.36s
Val loss: 0.5840 score: 0.8837 time: 0.37s
Test loss: 0.5814 score: 0.8864 time: 0.28s
Epoch 66/1000, LR 0.000268
Train loss: 0.5569;  Loss pred: 0.5569; Loss self: 0.0000; time: 0.44s
Val loss: 0.5753 score: 0.8837 time: 0.29s
Test loss: 0.5729 score: 0.8864 time: 0.28s
Epoch 67/1000, LR 0.000268
Train loss: 0.5482;  Loss pred: 0.5482; Loss self: 0.0000; time: 0.38s
Val loss: 0.5663 score: 0.8837 time: 0.28s
Test loss: 0.5642 score: 0.8864 time: 0.37s
Epoch 68/1000, LR 0.000268
Train loss: 0.5306;  Loss pred: 0.5306; Loss self: 0.0000; time: 0.48s
Val loss: 0.5568 score: 0.8837 time: 0.25s
Test loss: 0.5551 score: 0.8636 time: 0.29s
Epoch 69/1000, LR 0.000268
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.44s
Val loss: 0.5470 score: 0.8837 time: 0.25s
Test loss: 0.5460 score: 0.8636 time: 0.31s
Epoch 70/1000, LR 0.000268
Train loss: 0.5098;  Loss pred: 0.5098; Loss self: 0.0000; time: 0.36s
Val loss: 0.5368 score: 0.8837 time: 0.27s
Test loss: 0.5365 score: 0.8636 time: 0.29s
Epoch 71/1000, LR 0.000268
Train loss: 0.4998;  Loss pred: 0.4998; Loss self: 0.0000; time: 0.37s
Val loss: 0.5264 score: 0.8837 time: 0.28s
Test loss: 0.5271 score: 0.8636 time: 0.29s
Epoch 72/1000, LR 0.000267
Train loss: 0.4871;  Loss pred: 0.4871; Loss self: 0.0000; time: 0.40s
Val loss: 0.5155 score: 0.8837 time: 0.26s
Test loss: 0.5170 score: 0.8636 time: 0.42s
Epoch 73/1000, LR 0.000267
Train loss: 0.4623;  Loss pred: 0.4623; Loss self: 0.0000; time: 0.36s
Val loss: 0.5045 score: 0.9070 time: 0.24s
Test loss: 0.5073 score: 0.8636 time: 0.29s
Epoch 74/1000, LR 0.000267
Train loss: 0.4551;  Loss pred: 0.4551; Loss self: 0.0000; time: 0.37s
Val loss: 0.4931 score: 0.9070 time: 0.29s
Test loss: 0.4974 score: 0.8636 time: 0.34s
Epoch 75/1000, LR 0.000267
Train loss: 0.4414;  Loss pred: 0.4414; Loss self: 0.0000; time: 0.34s
Val loss: 0.4817 score: 0.9302 time: 0.28s
Test loss: 0.4878 score: 0.8636 time: 0.28s
Epoch 76/1000, LR 0.000267
Train loss: 0.4297;  Loss pred: 0.4297; Loss self: 0.0000; time: 0.38s
Val loss: 0.4700 score: 0.9302 time: 0.32s
Test loss: 0.4773 score: 0.8636 time: 0.29s
Epoch 77/1000, LR 0.000267
Train loss: 0.4107;  Loss pred: 0.4107; Loss self: 0.0000; time: 0.38s
Val loss: 0.4580 score: 0.9302 time: 0.25s
Test loss: 0.4664 score: 0.8636 time: 0.32s
Epoch 78/1000, LR 0.000267
Train loss: 0.3902;  Loss pred: 0.3902; Loss self: 0.0000; time: 0.46s
Val loss: 0.4466 score: 0.9302 time: 0.30s
Test loss: 0.4566 score: 0.8636 time: 0.30s
Epoch 79/1000, LR 0.000267
Train loss: 0.3673;  Loss pred: 0.3673; Loss self: 0.0000; time: 0.32s
Val loss: 0.4352 score: 0.9302 time: 0.27s
Test loss: 0.4472 score: 0.8636 time: 0.37s
Epoch 80/1000, LR 0.000267
Train loss: 0.3593;  Loss pred: 0.3593; Loss self: 0.0000; time: 0.35s
Val loss: 0.4241 score: 0.9302 time: 0.26s
Test loss: 0.4374 score: 0.8864 time: 0.28s
Epoch 81/1000, LR 0.000267
Train loss: 0.3377;  Loss pred: 0.3377; Loss self: 0.0000; time: 0.38s
Val loss: 0.4132 score: 0.9302 time: 0.24s
Test loss: 0.4281 score: 0.8864 time: 0.29s
Epoch 82/1000, LR 0.000267
Train loss: 0.3410;  Loss pred: 0.3410; Loss self: 0.0000; time: 0.34s
Val loss: 0.4028 score: 0.9302 time: 0.25s
Test loss: 0.4195 score: 0.8864 time: 0.29s
Epoch 83/1000, LR 0.000266
Train loss: 0.3239;  Loss pred: 0.3239; Loss self: 0.0000; time: 0.35s
Val loss: 0.3928 score: 0.9302 time: 0.27s
Test loss: 0.4105 score: 0.8864 time: 0.28s
Epoch 84/1000, LR 0.000266
Train loss: 0.3071;  Loss pred: 0.3071; Loss self: 0.0000; time: 0.38s
Val loss: 0.3834 score: 0.9302 time: 0.25s
Test loss: 0.4022 score: 0.8864 time: 0.29s
Epoch 85/1000, LR 0.000266
Train loss: 0.3076;  Loss pred: 0.3076; Loss self: 0.0000; time: 0.34s
Val loss: 0.3747 score: 0.9302 time: 0.25s
Test loss: 0.3950 score: 0.8864 time: 0.38s
Epoch 86/1000, LR 0.000266
Train loss: 0.2879;  Loss pred: 0.2879; Loss self: 0.0000; time: 0.34s
Val loss: 0.3667 score: 0.9302 time: 0.27s
Test loss: 0.3893 score: 0.8636 time: 0.29s
Epoch 87/1000, LR 0.000266
Train loss: 0.2485;  Loss pred: 0.2485; Loss self: 0.0000; time: 0.37s
Val loss: 0.3594 score: 0.9302 time: 0.25s
Test loss: 0.3864 score: 0.8636 time: 0.29s
Epoch 88/1000, LR 0.000266
Train loss: 0.2434;  Loss pred: 0.2434; Loss self: 0.0000; time: 0.50s
Val loss: 0.3530 score: 0.9070 time: 0.26s
Test loss: 0.3863 score: 0.8636 time: 0.30s
Epoch 89/1000, LR 0.000266
Train loss: 0.2231;  Loss pred: 0.2231; Loss self: 0.0000; time: 0.33s
Val loss: 0.3490 score: 0.8837 time: 0.27s
Test loss: 0.3924 score: 0.8182 time: 0.29s
Epoch 90/1000, LR 0.000266
Train loss: 0.2062;  Loss pred: 0.2062; Loss self: 0.0000; time: 0.37s
Val loss: 0.3471 score: 0.8837 time: 0.25s
Test loss: 0.4003 score: 0.8182 time: 0.28s
Epoch 91/1000, LR 0.000266
Train loss: 0.1968;  Loss pred: 0.1968; Loss self: 0.0000; time: 0.64s
Val loss: 0.3452 score: 0.8837 time: 0.26s
Test loss: 0.4062 score: 0.7955 time: 0.30s
Epoch 92/1000, LR 0.000266
Train loss: 0.1862;  Loss pred: 0.1862; Loss self: 0.0000; time: 0.36s
Val loss: 0.3413 score: 0.8837 time: 0.27s
Test loss: 0.4054 score: 0.7955 time: 0.30s
Epoch 93/1000, LR 0.000265
Train loss: 0.1749;  Loss pred: 0.1749; Loss self: 0.0000; time: 0.37s
Val loss: 0.3377 score: 0.8837 time: 0.35s
Test loss: 0.4034 score: 0.7955 time: 0.28s
Epoch 94/1000, LR 0.000265
Train loss: 0.1763;  Loss pred: 0.1763; Loss self: 0.0000; time: 0.38s
Val loss: 0.3339 score: 0.8837 time: 0.35s
Test loss: 0.3980 score: 0.8409 time: 0.29s
Epoch 95/1000, LR 0.000265
Train loss: 0.1587;  Loss pred: 0.1587; Loss self: 0.0000; time: 0.44s
Val loss: 0.3301 score: 0.8837 time: 0.25s
Test loss: 0.3876 score: 0.8409 time: 0.28s
Epoch 96/1000, LR 0.000265
Train loss: 0.1463;  Loss pred: 0.1463; Loss self: 0.0000; time: 0.43s
Val loss: 0.3281 score: 0.8837 time: 0.35s
Test loss: 0.3753 score: 0.8409 time: 0.34s
Epoch 97/1000, LR 0.000265
Train loss: 0.1441;  Loss pred: 0.1441; Loss self: 0.0000; time: 0.39s
Val loss: 0.3284 score: 0.8837 time: 0.25s
Test loss: 0.3681 score: 0.8409 time: 0.38s
     INFO: Early stopping counter 1 of 2
Epoch 98/1000, LR 0.000265
Train loss: 0.1387;  Loss pred: 0.1387; Loss self: 0.0000; time: 0.34s
Val loss: 0.3308 score: 0.8605 time: 0.27s
Test loss: 0.3667 score: 0.8409 time: 0.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 095,   Train_Loss: 0.1463,   Val_Loss: 0.3281,   Val_Precision: 0.8696,   Val_Recall: 0.9091,   Val_accuracy: 0.8889,   Val_Score: 0.8837,   Val_Loss: 0.3281,   Test_Precision: 0.8261,   Test_Recall: 0.8636,   Test_accuracy: 0.8444,   Test_Score: 0.8409,   Test_loss: 0.3753


[0.281553523032926, 0.2972645499976352, 0.3178476900793612, 0.2826410139678046, 0.28397698898334056, 0.2961928049335256, 0.3086709020426497, 0.29353999497834593, 0.29644098808057606, 0.307812706916593, 0.28942857892252505, 0.2881169399479404, 0.2980497410753742, 0.31561325502116233, 0.2820465490221977, 0.29386050708126277, 0.4255499739665538, 0.3235310820164159, 0.2818759479559958, 0.2947484280448407, 0.3091699290089309, 0.2859099640045315, 0.27992139593698084, 0.2970595830120146, 0.30454535596072674, 0.2801138189388439, 0.298020399059169, 0.2943908349843696, 0.3041476389626041, 0.27667484106495976, 0.2913504899479449, 0.30606822099070996, 0.3074198120739311, 0.28033759794197977, 0.2958713100524619, 0.29830188991036266, 0.29009157605469227, 0.29830680903978646, 0.3717687289463356, 0.32011596602387726, 0.29919712396804243, 0.3023414280032739, 0.3014788019936532, 0.2967679549474269, 0.2967267179628834, 0.4138873389456421, 0.3055095650488511, 0.2999637379543856, 0.3061135020107031, 0.2809046730399132, 0.29844643792603165, 0.4178837299114093, 0.31337156903464347, 0.37097399996127933, 0.28775370796211064, 0.3639340860536322, 0.4033061460359022, 0.3016314379638061, 0.29917888692580163, 0.3216994170797989, 0.28437897097319365, 0.3006288791075349, 0.30764961696695536, 0.30094404297415167, 0.28084238700103015, 0.2849791720509529, 0.3715573340887204, 0.29248019598890096, 0.3094281710218638, 0.29589773004408926, 0.29745451000053436, 0.4280362830031663, 0.2917604480171576, 0.34621229697950184, 0.2877417110139504, 0.2947037579724565, 0.32662245398387313, 0.3055603870889172, 0.37326488306280226, 0.28512381203472614, 0.2992840240476653, 0.289886049926281, 0.282508322969079, 0.2971079220296815, 0.3887889670440927, 0.29787915700580925, 0.2912655199179426, 0.30339442507829517, 0.2894196549896151, 0.28450412407983094, 0.3021992640569806, 0.3027473579859361, 0.28538378805387765, 0.29809155501425266, 0.2855666489340365, 0.3409249889664352, 0.3884344630641863, 0.3050488489679992]
[0.006398943705293773, 0.006756012499946254, 0.0072238111381673, 0.006423659408359195, 0.006454022476894103, 0.006731654657580128, 0.007015247773696584, 0.006671363522235135, 0.006737295183649456, 0.0069957433390134775, 0.006577922248239206, 0.006548112271544101, 0.00677385775171305, 0.007173028523208235, 0.006410148841413585, 0.0066786478882105175, 0.009671590317421678, 0.007352979136736725, 0.00640627154445445, 0.006698827910110016, 0.007026589295657521, 0.006497953727375716, 0.006361849907658656, 0.006751354159363968, 0.00692148536274379, 0.006366223157700998, 0.006773190887708386, 0.006690700795099308, 0.006912446340059184, 0.006288064569658177, 0.006621602044271474, 0.006956095931607045, 0.006986813910771161, 0.006371309044135904, 0.006724347955737771, 0.006779588407053697, 0.00659299036487937, 0.0067797002054496925, 0.0084492892942349, 0.007275362864179028, 0.006799934635637328, 0.006871396090983498, 0.006851790954401208, 0.006744726248805157, 0.006743789044610987, 0.009406530430582776, 0.006943399205655706, 0.006817357680781491, 0.006957125045697798, 0.006384197114543481, 0.006782873589227992, 0.009497357497986575, 0.007122081114423715, 0.008431227271847258, 0.0065398569991388785, 0.00827122922849164, 0.009166048773543232, 0.006855259953722866, 0.006799520157404582, 0.007311350388177248, 0.006463158431208946, 0.0068324745251712475, 0.006992036749248986, 0.0068396373403216285, 0.006382781522750685, 0.006476799364794384, 0.008444484865652736, 0.006647277181565931, 0.007032458432315087, 0.006724948410092938, 0.006760329772739417, 0.009728097340981052, 0.006630919273117219, 0.007868461294988678, 0.006539584341226146, 0.006697812681192194, 0.0074232375905425715, 0.0069445542520208455, 0.00848329279688187, 0.006480086637152867, 0.006801909637446938, 0.006588319316506386, 0.006420643703842705, 0.006752452773401852, 0.008836112887365744, 0.00676998084104112, 0.006619670907225968, 0.006895327842688526, 0.006577719431582161, 0.006466002819996158, 0.0068681650922041045, 0.006880621772407639, 0.0064859951830426735, 0.006774808068505742, 0.006490151112137193, 0.007748295203782618, 0.008828055978731507, 0.006932928385636346]
[156.27579270195977, 148.01630399706266, 138.43108310465917, 155.67450520472588, 154.9421316055339, 148.55188670053917, 142.54664015567113, 149.8943951513177, 148.4275176820025, 142.9440663472107, 152.02368806771506, 152.71576884007695, 147.62636545580082, 139.41112833505593, 156.0026178392883, 149.73090612626095, 103.39561201208812, 135.99929789054224, 156.09703601553448, 149.2798461788783, 142.31655756769598, 153.89460158619244, 157.1869840557161, 148.11843319062496, 144.4776587093126, 157.07900512257962, 147.64090021657958, 149.46117463995148, 144.66658413024845, 159.03144583236374, 151.0208546684147, 143.75879945188927, 143.1267545938727, 156.9536170781719, 148.7133037407318, 147.5015797359569, 151.67624168343477, 147.49914741011335, 118.3531496172486, 137.45018890035016, 147.06023713215802, 145.53083343749884, 145.94724308651823, 148.26398627774583, 148.28459095990075, 106.30912294173545, 144.02167733427393, 146.68439692097357, 143.73753431647575, 156.63676764020272, 147.43013957802762, 105.2924458421196, 140.40839804180118, 118.60669482118027, 152.9085422099708, 120.90101390919402, 109.0982630254366, 145.8733887191153, 147.0692014804918, 136.77363919215813, 154.72311419309398, 146.35985781080336, 143.01984326775886, 146.20658234387844, 156.6715069966935, 154.39724834393516, 118.4204857856303, 150.43753595429658, 142.1977832680626, 148.70002549003647, 147.92177802219558, 102.7950240369565, 150.80865243740126, 127.08965101434599, 152.9149174965001, 149.3024734489891, 134.7121101544736, 143.9977230660993, 117.87875580194066, 154.3189244209498, 147.0175367362488, 151.78377852673268, 155.74762377820588, 148.0943345415217, 113.17193575354196, 147.71090546339204, 151.0649115363741, 145.02573667477637, 152.0283755489198, 154.6550516352225, 145.59929567433912, 145.33570265555872, 154.17834453754335, 147.60565759032062, 154.07961736513437, 129.06064801348984, 113.27522190720056, 144.23919365326228]
Elapsed: 0.3094604154716113~0.034412428307018836
Time per graph: 0.0070331912607184376~0.0007821006433413371
Speed: 143.64591380771608~13.2365092798779
Total Time: 0.3055
best val loss: 0.3281368613243103 test_score: 0.8409

Testing...
Test loss: 0.4878 score: 0.8636 time: 0.30s
test Score 0.8636
Epoch Time List: [0.8959221319528297, 0.9450483248801902, 0.9515445899451151, 0.8911865039262921, 1.0144796720705926, 0.9170008349465206, 0.9372958341846243, 0.8848399750422686, 0.9160824299324304, 0.9699058281257749, 0.8869341020472348, 1.0726877989945933, 0.9475527589675039, 1.0363334850408137, 0.8875768929719925, 0.9415446530329064, 1.055723047000356, 0.959961929009296, 0.8682471389183775, 0.9116258471040055, 1.0324713399168104, 0.9045113520696759, 1.0113898938288912, 0.9484419429209083, 1.0202515090350062, 0.8728961700107902, 0.9340287448139861, 0.9886812070617452, 1.0210816009202972, 0.8615542059997097, 0.940783723955974, 0.9316062801517546, 1.0257227130932733, 0.875392128014937, 1.0352282680105418, 0.9168708720244467, 0.8751894949236885, 0.9281157051445916, 0.9891729190712795, 1.071504418971017, 0.9007175880251452, 0.9258984220214188, 0.9111768008442596, 0.9133373709628358, 0.9101053100312129, 1.0293518870603293, 0.9431274197995663, 0.9319955081446096, 1.0461285449564457, 0.9277692619943991, 1.0085199099266902, 1.0307927960529923, 0.949213583022356, 1.0067617901368067, 0.9132569180801511, 0.9910698410822079, 1.0219824290834367, 1.07692709506955, 0.9320793189108372, 0.9598568410146981, 1.0229231860721484, 1.0110651049762964, 1.082693561911583, 0.9238902339711785, 1.0069632119266316, 1.010214045876637, 1.022734744939953, 1.0223020940320566, 1.0041615929221734, 0.9237181729404256, 0.9404964098939672, 1.0872164720203727, 0.8883214090019464, 0.9971735350554809, 0.898192677879706, 0.990469055948779, 0.948846113984473, 1.0678147989092395, 0.9579430789453909, 0.89433625980746, 0.9144327298272401, 0.8811952349497005, 0.9020719471154734, 0.9172243430512026, 0.9793091539759189, 0.9030843308428302, 0.9042978319339454, 1.0516563279088587, 0.8778257670346648, 0.8969112630002201, 1.196867095073685, 0.920303856022656, 1.0077678029192612, 1.0218852488324046, 0.9728509178385139, 1.1180121138459072, 1.0209090299904346, 0.9059694759780541]
Total Epoch List: [98]
Total Time List: [0.3055155740585178]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcd630>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.26s
Epoch 4/1000, LR 0.000060
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.26s
Epoch 5/1000, LR 0.000090
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.26s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.26s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.53s
Val loss: 0.6928 score: 0.5455 time: 0.25s
Test loss: 0.6929 score: 0.5814 time: 0.30s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.46s
Val loss: 0.6927 score: 0.6364 time: 0.25s
Test loss: 0.6929 score: 0.6047 time: 0.29s
Epoch 10/1000, LR 0.000240
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.38s
Val loss: 0.6926 score: 0.5227 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.27s
Epoch 13/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.27s
Epoch 14/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.28s
Epoch 17/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.26s
Epoch 18/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.37s
Epoch 19/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.38s
Val loss: 0.6907 score: 0.5227 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.50s
Val loss: 0.6904 score: 0.5227 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.42s
Val loss: 0.6899 score: 0.5682 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.29s
Epoch 22/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.42s
Val loss: 0.6894 score: 0.5909 time: 0.24s
Test loss: 0.6914 score: 0.5116 time: 0.28s
Epoch 23/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.51s
Val loss: 0.6888 score: 0.6364 time: 0.34s
Test loss: 0.6911 score: 0.5814 time: 0.28s
Epoch 24/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.39s
Val loss: 0.6881 score: 0.6364 time: 0.29s
Test loss: 0.6907 score: 0.6047 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.42s
Val loss: 0.6874 score: 0.6591 time: 0.27s
Test loss: 0.6902 score: 0.5116 time: 0.31s
Epoch 26/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.42s
Val loss: 0.6866 score: 0.6818 time: 0.25s
Test loss: 0.6897 score: 0.4884 time: 0.27s
Epoch 27/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.40s
Val loss: 0.6858 score: 0.7045 time: 0.31s
Test loss: 0.6891 score: 0.5814 time: 0.27s
Epoch 28/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.51s
Val loss: 0.6848 score: 0.6818 time: 0.25s
Test loss: 0.6885 score: 0.6279 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.40s
Val loss: 0.6838 score: 0.6818 time: 0.27s
Test loss: 0.6878 score: 0.6512 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.43s
Val loss: 0.6827 score: 0.6591 time: 0.27s
Test loss: 0.6871 score: 0.6977 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.42s
Val loss: 0.6816 score: 0.6591 time: 0.28s
Test loss: 0.6863 score: 0.6512 time: 0.27s
Epoch 32/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.38s
Val loss: 0.6804 score: 0.6364 time: 0.28s
Test loss: 0.6855 score: 0.6047 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.52s
Val loss: 0.6792 score: 0.6136 time: 0.27s
Test loss: 0.6846 score: 0.5581 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6722;  Loss pred: 0.6722; Loss self: 0.0000; time: 0.41s
Val loss: 0.6779 score: 0.6136 time: 0.30s
Test loss: 0.6837 score: 0.5814 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.42s
Val loss: 0.6765 score: 0.6136 time: 0.25s
Test loss: 0.6828 score: 0.5814 time: 0.28s
Epoch 36/1000, LR 0.000270
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.38s
Val loss: 0.6751 score: 0.5909 time: 0.32s
Test loss: 0.6818 score: 0.5814 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 0.45s
Val loss: 0.6735 score: 0.5909 time: 0.26s
Test loss: 0.6808 score: 0.5581 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6637;  Loss pred: 0.6637; Loss self: 0.0000; time: 0.52s
Val loss: 0.6718 score: 0.6136 time: 0.27s
Test loss: 0.6797 score: 0.5581 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.40s
Val loss: 0.6700 score: 0.6136 time: 0.26s
Test loss: 0.6784 score: 0.5814 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6577;  Loss pred: 0.6577; Loss self: 0.0000; time: 0.42s
Val loss: 0.6681 score: 0.6136 time: 0.24s
Test loss: 0.6771 score: 0.5814 time: 0.26s
Epoch 41/1000, LR 0.000269
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.44s
Val loss: 0.6659 score: 0.6136 time: 0.27s
Test loss: 0.6757 score: 0.5814 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.44s
Val loss: 0.6636 score: 0.6136 time: 0.26s
Test loss: 0.6742 score: 0.5814 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.53s
Val loss: 0.6612 score: 0.6136 time: 0.25s
Test loss: 0.6725 score: 0.6047 time: 0.26s
Epoch 44/1000, LR 0.000269
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.41s
Val loss: 0.6585 score: 0.6364 time: 0.25s
Test loss: 0.6706 score: 0.6047 time: 0.26s
Epoch 45/1000, LR 0.000269
Train loss: 0.6370;  Loss pred: 0.6370; Loss self: 0.0000; time: 0.40s
Val loss: 0.6556 score: 0.6364 time: 0.28s
Test loss: 0.6686 score: 0.6512 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.43s
Val loss: 0.6525 score: 0.6591 time: 0.26s
Test loss: 0.6665 score: 0.6512 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 0.43s
Val loss: 0.6491 score: 0.6591 time: 0.26s
Test loss: 0.6641 score: 0.6512 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.48s
Val loss: 0.6455 score: 0.6591 time: 0.28s
Test loss: 0.6615 score: 0.6744 time: 0.27s
Epoch 49/1000, LR 0.000269
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.40s
Val loss: 0.6416 score: 0.6818 time: 0.29s
Test loss: 0.6588 score: 0.6977 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.41s
Val loss: 0.6375 score: 0.7045 time: 0.33s
Test loss: 0.6558 score: 0.7209 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.5989;  Loss pred: 0.5989; Loss self: 0.0000; time: 0.46s
Val loss: 0.6330 score: 0.7045 time: 0.25s
Test loss: 0.6526 score: 0.7209 time: 0.27s
Epoch 52/1000, LR 0.000269
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.44s
Val loss: 0.6282 score: 0.7045 time: 0.28s
Test loss: 0.6491 score: 0.7442 time: 0.27s
Epoch 53/1000, LR 0.000269
Train loss: 0.5845;  Loss pred: 0.5845; Loss self: 0.0000; time: 0.51s
Val loss: 0.6230 score: 0.7273 time: 0.26s
Test loss: 0.6454 score: 0.7442 time: 0.28s
Epoch 54/1000, LR 0.000269
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.39s
Val loss: 0.6176 score: 0.7500 time: 0.29s
Test loss: 0.6414 score: 0.7674 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.5650;  Loss pred: 0.5650; Loss self: 0.0000; time: 0.41s
Val loss: 0.6118 score: 0.7727 time: 0.29s
Test loss: 0.6371 score: 0.7674 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.5554;  Loss pred: 0.5554; Loss self: 0.0000; time: 0.42s
Val loss: 0.6057 score: 0.8182 time: 0.25s
Test loss: 0.6326 score: 0.7442 time: 0.27s
Epoch 57/1000, LR 0.000269
Train loss: 0.5426;  Loss pred: 0.5426; Loss self: 0.0000; time: 0.37s
Val loss: 0.5992 score: 0.8182 time: 0.26s
Test loss: 0.6278 score: 0.6977 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.51s
Val loss: 0.5924 score: 0.8182 time: 0.27s
Test loss: 0.6228 score: 0.7209 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.42s
Val loss: 0.5853 score: 0.8182 time: 0.26s
Test loss: 0.6175 score: 0.7209 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5104;  Loss pred: 0.5104; Loss self: 0.0000; time: 0.51s
Val loss: 0.5779 score: 0.8182 time: 0.26s
Test loss: 0.6119 score: 0.7209 time: 0.26s
Epoch 61/1000, LR 0.000268
Train loss: 0.4965;  Loss pred: 0.4965; Loss self: 0.0000; time: 0.41s
Val loss: 0.5702 score: 0.8182 time: 0.25s
Test loss: 0.6061 score: 0.7209 time: 0.27s
Epoch 62/1000, LR 0.000268
Train loss: 0.4861;  Loss pred: 0.4861; Loss self: 0.0000; time: 0.38s
Val loss: 0.5623 score: 0.8182 time: 0.28s
Test loss: 0.6002 score: 0.7209 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.4711;  Loss pred: 0.4711; Loss self: 0.0000; time: 0.53s
Val loss: 0.5542 score: 0.8182 time: 0.26s
Test loss: 0.5941 score: 0.7209 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.4529;  Loss pred: 0.4529; Loss self: 0.0000; time: 0.40s
Val loss: 0.5459 score: 0.8182 time: 0.26s
Test loss: 0.5879 score: 0.7209 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.4503;  Loss pred: 0.4503; Loss self: 0.0000; time: 0.45s
Val loss: 0.5376 score: 0.8182 time: 0.26s
Test loss: 0.5817 score: 0.7442 time: 0.28s
Epoch 66/1000, LR 0.000268
Train loss: 0.4317;  Loss pred: 0.4317; Loss self: 0.0000; time: 0.39s
Val loss: 0.5293 score: 0.8182 time: 0.35s
Test loss: 0.5754 score: 0.7442 time: 0.29s
Epoch 67/1000, LR 0.000268
Train loss: 0.4223;  Loss pred: 0.4223; Loss self: 0.0000; time: 0.40s
Val loss: 0.5210 score: 0.7955 time: 0.27s
Test loss: 0.5691 score: 0.7442 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.4024;  Loss pred: 0.4024; Loss self: 0.0000; time: 0.51s
Val loss: 0.5128 score: 0.7955 time: 0.27s
Test loss: 0.5628 score: 0.7442 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.3815;  Loss pred: 0.3815; Loss self: 0.0000; time: 0.42s
Val loss: 0.5048 score: 0.7955 time: 0.25s
Test loss: 0.5563 score: 0.7674 time: 0.27s
Epoch 70/1000, LR 0.000268
Train loss: 0.3717;  Loss pred: 0.3717; Loss self: 0.0000; time: 0.40s
Val loss: 0.4969 score: 0.7955 time: 0.25s
Test loss: 0.5499 score: 0.7674 time: 0.26s
Epoch 71/1000, LR 0.000268
Train loss: 0.3624;  Loss pred: 0.3624; Loss self: 0.0000; time: 0.40s
Val loss: 0.4894 score: 0.7955 time: 0.27s
Test loss: 0.5435 score: 0.7907 time: 0.25s
Epoch 72/1000, LR 0.000267
Train loss: 0.3399;  Loss pred: 0.3399; Loss self: 0.0000; time: 0.41s
Val loss: 0.4819 score: 0.7955 time: 0.25s
Test loss: 0.5371 score: 0.7907 time: 0.27s
Epoch 73/1000, LR 0.000267
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.54s
Val loss: 0.4748 score: 0.7955 time: 0.24s
Test loss: 0.5308 score: 0.7907 time: 0.27s
Epoch 74/1000, LR 0.000267
Train loss: 0.3215;  Loss pred: 0.3215; Loss self: 0.0000; time: 0.38s
Val loss: 0.4680 score: 0.7955 time: 0.26s
Test loss: 0.5248 score: 0.8140 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.3026;  Loss pred: 0.3026; Loss self: 0.0000; time: 0.40s
Val loss: 0.4615 score: 0.7955 time: 0.27s
Test loss: 0.5191 score: 0.8140 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.2835;  Loss pred: 0.2835; Loss self: 0.0000; time: 0.41s
Val loss: 0.4553 score: 0.7955 time: 0.25s
Test loss: 0.5134 score: 0.8140 time: 0.26s
Epoch 77/1000, LR 0.000267
Train loss: 0.2807;  Loss pred: 0.2807; Loss self: 0.0000; time: 0.42s
Val loss: 0.4496 score: 0.7955 time: 0.26s
Test loss: 0.5080 score: 0.7907 time: 0.26s
Epoch 78/1000, LR 0.000267
Train loss: 0.2659;  Loss pred: 0.2659; Loss self: 0.0000; time: 0.50s
Val loss: 0.4443 score: 0.7955 time: 0.26s
Test loss: 0.5028 score: 0.8140 time: 0.26s
Epoch 79/1000, LR 0.000267
Train loss: 0.2539;  Loss pred: 0.2539; Loss self: 0.0000; time: 0.41s
Val loss: 0.4394 score: 0.8182 time: 0.28s
Test loss: 0.4981 score: 0.8140 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.2454;  Loss pred: 0.2454; Loss self: 0.0000; time: 0.43s
Val loss: 0.4351 score: 0.7955 time: 0.25s
Test loss: 0.4939 score: 0.8140 time: 0.27s
Epoch 81/1000, LR 0.000267
Train loss: 0.2260;  Loss pred: 0.2260; Loss self: 0.0000; time: 0.40s
Val loss: 0.4312 score: 0.7955 time: 0.26s
Test loss: 0.4900 score: 0.8140 time: 0.36s
Epoch 82/1000, LR 0.000267
Train loss: 0.2105;  Loss pred: 0.2105; Loss self: 0.0000; time: 0.41s
Val loss: 0.4278 score: 0.7955 time: 0.28s
Test loss: 0.4864 score: 0.8140 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.50s
Val loss: 0.4250 score: 0.7955 time: 0.29s
Test loss: 0.4833 score: 0.8140 time: 0.25s
Epoch 84/1000, LR 0.000266
Train loss: 0.2044;  Loss pred: 0.2044; Loss self: 0.0000; time: 0.41s
Val loss: 0.4228 score: 0.7955 time: 0.26s
Test loss: 0.4811 score: 0.8140 time: 0.27s
Epoch 85/1000, LR 0.000266
Train loss: 0.1956;  Loss pred: 0.1956; Loss self: 0.0000; time: 0.43s
Val loss: 0.4212 score: 0.7955 time: 0.25s
Test loss: 0.4795 score: 0.8140 time: 0.27s
Epoch 86/1000, LR 0.000266
Train loss: 0.1760;  Loss pred: 0.1760; Loss self: 0.0000; time: 0.40s
Val loss: 0.4202 score: 0.7955 time: 0.28s
Test loss: 0.4785 score: 0.8140 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 0.42s
Val loss: 0.4198 score: 0.7955 time: 0.29s
Test loss: 0.4781 score: 0.8140 time: 0.26s
Epoch 88/1000, LR 0.000266
Train loss: 0.1791;  Loss pred: 0.1791; Loss self: 0.0000; time: 0.52s
Val loss: 0.4201 score: 0.7955 time: 0.26s
Test loss: 0.4788 score: 0.8140 time: 0.34s
     INFO: Early stopping counter 1 of 2
Epoch 89/1000, LR 0.000266
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.43s
Val loss: 0.4210 score: 0.7955 time: 0.25s
Test loss: 0.4798 score: 0.8140 time: 0.27s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.1697,   Val_Loss: 0.4198,   Val_Precision: 0.8824,   Val_Recall: 0.6818,   Val_accuracy: 0.7692,   Val_Score: 0.7955,   Val_Loss: 0.4198,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4781


[0.281553523032926, 0.2972645499976352, 0.3178476900793612, 0.2826410139678046, 0.28397698898334056, 0.2961928049335256, 0.3086709020426497, 0.29353999497834593, 0.29644098808057606, 0.307812706916593, 0.28942857892252505, 0.2881169399479404, 0.2980497410753742, 0.31561325502116233, 0.2820465490221977, 0.29386050708126277, 0.4255499739665538, 0.3235310820164159, 0.2818759479559958, 0.2947484280448407, 0.3091699290089309, 0.2859099640045315, 0.27992139593698084, 0.2970595830120146, 0.30454535596072674, 0.2801138189388439, 0.298020399059169, 0.2943908349843696, 0.3041476389626041, 0.27667484106495976, 0.2913504899479449, 0.30606822099070996, 0.3074198120739311, 0.28033759794197977, 0.2958713100524619, 0.29830188991036266, 0.29009157605469227, 0.29830680903978646, 0.3717687289463356, 0.32011596602387726, 0.29919712396804243, 0.3023414280032739, 0.3014788019936532, 0.2967679549474269, 0.2967267179628834, 0.4138873389456421, 0.3055095650488511, 0.2999637379543856, 0.3061135020107031, 0.2809046730399132, 0.29844643792603165, 0.4178837299114093, 0.31337156903464347, 0.37097399996127933, 0.28775370796211064, 0.3639340860536322, 0.4033061460359022, 0.3016314379638061, 0.29917888692580163, 0.3216994170797989, 0.28437897097319365, 0.3006288791075349, 0.30764961696695536, 0.30094404297415167, 0.28084238700103015, 0.2849791720509529, 0.3715573340887204, 0.29248019598890096, 0.3094281710218638, 0.29589773004408926, 0.29745451000053436, 0.4280362830031663, 0.2917604480171576, 0.34621229697950184, 0.2877417110139504, 0.2947037579724565, 0.32662245398387313, 0.3055603870889172, 0.37326488306280226, 0.28512381203472614, 0.2992840240476653, 0.289886049926281, 0.282508322969079, 0.2971079220296815, 0.3887889670440927, 0.29787915700580925, 0.2912655199179426, 0.30339442507829517, 0.2894196549896151, 0.28450412407983094, 0.3021992640569806, 0.3027473579859361, 0.28538378805387765, 0.29809155501425266, 0.2855666489340365, 0.3409249889664352, 0.3884344630641863, 0.3050488489679992, 0.2585200140019879, 0.25622624706011266, 0.2681261310353875, 0.26367176801431924, 0.25527859304565936, 0.2613805599976331, 0.26266732707154006, 0.30516622599679977, 0.2970447379630059, 0.24695326504297554, 0.2586394640384242, 0.273246729047969, 0.2738999780267477, 0.25129100296180695, 0.25915148691274226, 0.2834449530346319, 0.26930630300194025, 0.3695639061043039, 0.24799152801278979, 0.2492189429467544, 0.29154523892793804, 0.28160478500649333, 0.2831706360448152, 0.2498933010501787, 0.3138926209649071, 0.2727994970045984, 0.2709843290504068, 0.2572206789627671, 0.25733065709937364, 0.26037387596443295, 0.2787395330378786, 0.25206362595781684, 0.24353612505365163, 0.2605370790697634, 0.27949934895150363, 0.2626549639971927, 0.2491441749734804, 0.24190075206570327, 0.26172154501546174, 0.2613358099479228, 0.24316464003641158, 0.2581982220290229, 0.26009069406427443, 0.2607005669269711, 0.2507710329955444, 0.25515134702436626, 0.27473054302390665, 0.2791736810468137, 0.2486170280026272, 0.2578836780739948, 0.27719052601605654, 0.27077171101700515, 0.2820285059278831, 0.25567720201797783, 0.2443979480303824, 0.27723167708609253, 0.25518276903312653, 0.2501868649851531, 0.2580405059270561, 0.2600373940076679, 0.27327543299179524, 0.2506697300123051, 0.2536152789834887, 0.2604575159493834, 0.28047181700821966, 0.2902171660680324, 0.2588135419646278, 0.25739768997300416, 0.27776156400796026, 0.2660973450401798, 0.25848692597355694, 0.2791263359831646, 0.271502329967916, 0.2593580209650099, 0.25901387503836304, 0.26831559801939875, 0.2643615179695189, 0.2602454840671271, 0.25609729799907655, 0.27609474293421954, 0.36723636696115136, 0.2414522550534457, 0.2540603559464216, 0.27302339498419315, 0.27497328410390764, 0.25054780091159046, 0.2609332030406222, 0.339938408927992, 0.27550135098863393]
[0.006398943705293773, 0.006756012499946254, 0.0072238111381673, 0.006423659408359195, 0.006454022476894103, 0.006731654657580128, 0.007015247773696584, 0.006671363522235135, 0.006737295183649456, 0.0069957433390134775, 0.006577922248239206, 0.006548112271544101, 0.00677385775171305, 0.007173028523208235, 0.006410148841413585, 0.0066786478882105175, 0.009671590317421678, 0.007352979136736725, 0.00640627154445445, 0.006698827910110016, 0.007026589295657521, 0.006497953727375716, 0.006361849907658656, 0.006751354159363968, 0.00692148536274379, 0.006366223157700998, 0.006773190887708386, 0.006690700795099308, 0.006912446340059184, 0.006288064569658177, 0.006621602044271474, 0.006956095931607045, 0.006986813910771161, 0.006371309044135904, 0.006724347955737771, 0.006779588407053697, 0.00659299036487937, 0.0067797002054496925, 0.0084492892942349, 0.007275362864179028, 0.006799934635637328, 0.006871396090983498, 0.006851790954401208, 0.006744726248805157, 0.006743789044610987, 0.009406530430582776, 0.006943399205655706, 0.006817357680781491, 0.006957125045697798, 0.006384197114543481, 0.006782873589227992, 0.009497357497986575, 0.007122081114423715, 0.008431227271847258, 0.0065398569991388785, 0.00827122922849164, 0.009166048773543232, 0.006855259953722866, 0.006799520157404582, 0.007311350388177248, 0.006463158431208946, 0.0068324745251712475, 0.006992036749248986, 0.0068396373403216285, 0.006382781522750685, 0.006476799364794384, 0.008444484865652736, 0.006647277181565931, 0.007032458432315087, 0.006724948410092938, 0.006760329772739417, 0.009728097340981052, 0.006630919273117219, 0.007868461294988678, 0.006539584341226146, 0.006697812681192194, 0.0074232375905425715, 0.0069445542520208455, 0.00848329279688187, 0.006480086637152867, 0.006801909637446938, 0.006588319316506386, 0.006420643703842705, 0.006752452773401852, 0.008836112887365744, 0.00676998084104112, 0.006619670907225968, 0.006895327842688526, 0.006577719431582161, 0.006466002819996158, 0.0068681650922041045, 0.006880621772407639, 0.0064859951830426735, 0.006774808068505742, 0.006490151112137193, 0.007748295203782618, 0.008828055978731507, 0.006932928385636346, 0.006012093348883439, 0.005958749931630527, 0.006235491419427617, 0.006131901581728354, 0.005936711466178125, 0.00607861767436356, 0.0061085424900358155, 0.007096888976669762, 0.00690801716193037, 0.005743099187045943, 0.0060148712567075395, 0.006354575094138813, 0.006369766930854598, 0.005843976813065278, 0.006026778765412611, 0.0065917430938286494, 0.00626293727911489, 0.008594509444286138, 0.005767244837506739, 0.005795789370854753, 0.006780121835533443, 0.006548948488523101, 0.00658536362894919, 0.005811472117446016, 0.007299828394532724, 0.006344174348944148, 0.006301961140707135, 0.005981876254948073, 0.005984433886031945, 0.006055206417777511, 0.006482314721811129, 0.0058619447897166705, 0.005663630815201201, 0.006059001838831708, 0.0064999848593372935, 0.0061082549766789, 0.005794050580778614, 0.005625598885248913, 0.00608654755849911, 0.006077576975533089, 0.005654991628753758, 0.006004609814628439, 0.006048620792192428, 0.006062803882022583, 0.005831884488268474, 0.005933752256380611, 0.006389082395904805, 0.006492411187135203, 0.005781791348898307, 0.005997294838930112, 0.006446291302698989, 0.00629701653527919, 0.006558802463439142, 0.005945981442278554, 0.005683673210008893, 0.006447248304327733, 0.005934483000770385, 0.005818299185701235, 0.00600094199830363, 0.006047381255992277, 0.006355242627716169, 0.005829528604937327, 0.005898029743802062, 0.00605715153370659, 0.006522600395539992, 0.0067492364201868, 0.006018919580572739, 0.0059859927900698644, 0.006459571255999076, 0.006188310349771623, 0.006011323859850161, 0.006491310139143363, 0.006314007673672465, 0.006031581882907208, 0.006023578489264257, 0.006239897628358111, 0.006147942278360905, 0.00605222055970063, 0.005955751116257594, 0.006420807975214408, 0.00854038062700352, 0.005615168722173156, 0.005908380370847014, 0.006349381278702166, 0.006394727537300178, 0.005826693044455592, 0.0060682140242005154, 0.007905544393674233, 0.0064070081625263705]
[156.27579270195977, 148.01630399706266, 138.43108310465917, 155.67450520472588, 154.9421316055339, 148.55188670053917, 142.54664015567113, 149.8943951513177, 148.4275176820025, 142.9440663472107, 152.02368806771506, 152.71576884007695, 147.62636545580082, 139.41112833505593, 156.0026178392883, 149.73090612626095, 103.39561201208812, 135.99929789054224, 156.09703601553448, 149.2798461788783, 142.31655756769598, 153.89460158619244, 157.1869840557161, 148.11843319062496, 144.4776587093126, 157.07900512257962, 147.64090021657958, 149.46117463995148, 144.66658413024845, 159.03144583236374, 151.0208546684147, 143.75879945188927, 143.1267545938727, 156.9536170781719, 148.7133037407318, 147.5015797359569, 151.67624168343477, 147.49914741011335, 118.3531496172486, 137.45018890035016, 147.06023713215802, 145.53083343749884, 145.94724308651823, 148.26398627774583, 148.28459095990075, 106.30912294173545, 144.02167733427393, 146.68439692097357, 143.73753431647575, 156.63676764020272, 147.43013957802762, 105.2924458421196, 140.40839804180118, 118.60669482118027, 152.9085422099708, 120.90101390919402, 109.0982630254366, 145.8733887191153, 147.0692014804918, 136.77363919215813, 154.72311419309398, 146.35985781080336, 143.01984326775886, 146.20658234387844, 156.6715069966935, 154.39724834393516, 118.4204857856303, 150.43753595429658, 142.1977832680626, 148.70002549003647, 147.92177802219558, 102.7950240369565, 150.80865243740126, 127.08965101434599, 152.9149174965001, 149.3024734489891, 134.7121101544736, 143.9977230660993, 117.87875580194066, 154.3189244209498, 147.0175367362488, 151.78377852673268, 155.74762377820588, 148.0943345415217, 113.17193575354196, 147.71090546339204, 151.0649115363741, 145.02573667477637, 152.0283755489198, 154.6550516352225, 145.59929567433912, 145.33570265555872, 154.17834453754335, 147.60565759032062, 154.07961736513437, 129.06064801348984, 113.27522190720056, 144.23919365326228, 166.33141602595362, 167.82043406314995, 160.37228387234225, 163.08154765232504, 168.44342287764402, 164.51108682447304, 163.70517216360344, 140.906811884389, 144.75933926611162, 174.12201451362472, 166.254597533545, 157.36693408853677, 156.9916153691726, 171.116353125207, 165.92611723844107, 151.70494143441732, 159.66948979909392, 116.35335402008629, 173.39302009455074, 172.53905137213806, 147.4899749970824, 152.69626898920944, 151.85190315140844, 172.07344022145506, 136.98952166450374, 157.62492406382694, 158.68076265030592, 167.1716293316537, 167.1001834165241, 165.147136365838, 154.26588231442804, 170.59184892942903, 176.56518099943895, 165.04368650147487, 153.8465122058692, 163.71287770696614, 172.59083020735702, 177.7588520614465, 164.29675286174734, 164.53925701406456, 176.8349213666969, 166.53871456623187, 165.32694548992092, 164.940186002915, 171.47116031046542, 168.5274269623731, 156.5169985350271, 154.0259806682474, 172.95677752026546, 166.74184392415083, 155.1279569977409, 158.80536352373787, 152.46685741403545, 168.18081416964378, 175.9425574009093, 155.10493047533896, 168.50667528581428, 171.87153291421498, 166.6405041546284, 165.36083267598056, 157.35040478845758, 171.54045683094319, 169.54814462420248, 165.09410313333598, 153.31308670753117, 148.16490899756076, 166.14277473114925, 167.05666629918022, 154.80903613707935, 161.59499822708545, 166.35270754234926, 154.0521063644583, 158.37801467516465, 165.79398562653725, 166.01427237684155, 160.25903942003094, 162.65604892221089, 165.22861157086854, 167.90493431974846, 155.74363909654335, 117.09080000932701, 178.08903872312936, 169.251120820551, 157.4956607747461, 156.37882836556238, 171.6239370034351, 164.7931328743385, 126.49350256007271, 156.0790894334816]
Elapsed: 0.28955150067801266~0.035893670743577195
Time per graph: 0.006648038537898016~0.0007803849924803125
Speed: 152.1865669162742~15.260208163180403
Total Time: 0.2761
best val loss: 0.4198153614997864 test_score: 0.8140

Testing...
Test loss: 0.6326 score: 0.7442 time: 0.25s
test Score 0.7442
Epoch Time List: [0.8959221319528297, 0.9450483248801902, 0.9515445899451151, 0.8911865039262921, 1.0144796720705926, 0.9170008349465206, 0.9372958341846243, 0.8848399750422686, 0.9160824299324304, 0.9699058281257749, 0.8869341020472348, 1.0726877989945933, 0.9475527589675039, 1.0363334850408137, 0.8875768929719925, 0.9415446530329064, 1.055723047000356, 0.959961929009296, 0.8682471389183775, 0.9116258471040055, 1.0324713399168104, 0.9045113520696759, 1.0113898938288912, 0.9484419429209083, 1.0202515090350062, 0.8728961700107902, 0.9340287448139861, 0.9886812070617452, 1.0210816009202972, 0.8615542059997097, 0.940783723955974, 0.9316062801517546, 1.0257227130932733, 0.875392128014937, 1.0352282680105418, 0.9168708720244467, 0.8751894949236885, 0.9281157051445916, 0.9891729190712795, 1.071504418971017, 0.9007175880251452, 0.9258984220214188, 0.9111768008442596, 0.9133373709628358, 0.9101053100312129, 1.0293518870603293, 0.9431274197995663, 0.9319955081446096, 1.0461285449564457, 0.9277692619943991, 1.0085199099266902, 1.0307927960529923, 0.949213583022356, 1.0067617901368067, 0.9132569180801511, 0.9910698410822079, 1.0219824290834367, 1.07692709506955, 0.9320793189108372, 0.9598568410146981, 1.0229231860721484, 1.0110651049762964, 1.082693561911583, 0.9238902339711785, 1.0069632119266316, 1.010214045876637, 1.022734744939953, 1.0223020940320566, 1.0041615929221734, 0.9237181729404256, 0.9404964098939672, 1.0872164720203727, 0.8883214090019464, 0.9971735350554809, 0.898192677879706, 0.990469055948779, 0.948846113984473, 1.0678147989092395, 0.9579430789453909, 0.89433625980746, 0.9144327298272401, 0.8811952349497005, 0.9020719471154734, 0.9172243430512026, 0.9793091539759189, 0.9030843308428302, 0.9042978319339454, 1.0516563279088587, 0.8778257670346648, 0.8969112630002201, 1.196867095073685, 0.920303856022656, 1.0077678029192612, 1.0218852488324046, 0.9728509178385139, 1.1180121138459072, 1.0209090299904346, 0.9059694759780541, 0.9416965851560235, 1.0056875880109146, 1.0731785258976743, 0.9377523149596527, 0.9252205360680819, 0.9309580411063507, 1.0954112310428172, 1.085586010129191, 1.0008392919553444, 0.8881242859642953, 0.9560214199591428, 0.934721295023337, 1.0352150758262724, 0.882185201975517, 0.9472387199057266, 0.9313297349726781, 0.9275467720581219, 1.1271359820384532, 0.9185158389154822, 1.016359896864742, 0.9578211259795353, 0.9374358110362664, 1.1307424809783697, 0.9214600180275738, 0.9975256220204756, 0.935347385937348, 0.9821607460035011, 1.0165873371297494, 0.927227703970857, 0.9632312279427424, 0.9773070799419656, 0.9113484399858862, 1.0337238669162616, 0.9693035479867831, 0.9469543668674305, 0.9595885600429028, 0.9601389419985935, 1.0332315941341221, 0.9188196160830557, 0.9136959470342845, 0.9516393379308283, 0.948270213091746, 1.0449424921534956, 0.9198603939730674, 0.9186982722021639, 0.9400300530251116, 0.9532837299630046, 1.0347587529104203, 0.9361557101365179, 0.995509588974528, 0.9829584941035137, 0.9905066171195358, 1.0473033860325813, 0.9373371979454532, 0.9462466649711132, 0.9360340611310676, 0.8794138040393591, 1.0247984199086204, 0.9322369621368125, 1.0214198529720306, 0.9342660179827362, 0.9099525929195806, 1.0427855281159282, 0.9150646219495684, 0.9802385019138455, 1.0248644150560722, 0.9308781590079889, 1.028140897047706, 0.9371334729949012, 0.9091535758925602, 0.9217030690051615, 0.9318020069040358, 1.045660668052733, 0.8930109140928835, 0.9274504368659109, 0.92201759503223, 0.9328521669376642, 1.0126767969923094, 0.9408966939663514, 0.948050299892202, 1.0196472248062491, 0.9251686588395387, 1.03792579297442, 0.9429419310763478, 0.9471104099648073, 0.9247657130472362, 0.9675748719600961, 1.105650938116014, 0.952889489941299]
Total Epoch List: [98, 89]
Total Time List: [0.3055155740585178, 0.2761488629039377]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7babd3fcef20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7004 score: 0.4884 time: 0.26s
Epoch 2/1000, LR 0.000000
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.4884 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7002 score: 0.4884 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7001 score: 0.4884 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6999 score: 0.4884 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6997 score: 0.4884 time: 0.26s
Epoch 7/1000, LR 0.000150
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.4884 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6991 score: 0.4884 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.4884 time: 0.32s
Epoch 10/1000, LR 0.000240
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4884 time: 0.27s
Epoch 11/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.4884 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4884 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4884 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4884 time: 0.26s
Epoch 18/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4884 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.27s
Epoch 23/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.33s
Epoch 32/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4884 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.4884 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6859 score: 0.4884 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.4884 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6833 score: 0.4884 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6819 score: 0.4884 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6803 score: 0.4884 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6791 score: 0.5000 time: 0.29s
Test loss: 0.6784 score: 0.5116 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.42s
Val loss: 0.6773 score: 0.5227 time: 0.27s
Test loss: 0.6764 score: 0.5349 time: 0.26s
Epoch 44/1000, LR 0.000269
Train loss: 0.6701;  Loss pred: 0.6701; Loss self: 0.0000; time: 0.40s
Val loss: 0.6753 score: 0.5455 time: 0.28s
Test loss: 0.6741 score: 0.5349 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6671;  Loss pred: 0.6671; Loss self: 0.0000; time: 0.38s
Val loss: 0.6731 score: 0.6364 time: 0.29s
Test loss: 0.6716 score: 0.5814 time: 0.35s
Epoch 46/1000, LR 0.000269
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.41s
Val loss: 0.6707 score: 0.6818 time: 0.29s
Test loss: 0.6689 score: 0.6047 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.41s
Val loss: 0.6681 score: 0.7045 time: 0.27s
Test loss: 0.6659 score: 0.6512 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.40s
Val loss: 0.6652 score: 0.7727 time: 0.28s
Test loss: 0.6627 score: 0.7209 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.38s
Val loss: 0.6620 score: 0.8182 time: 0.30s
Test loss: 0.6593 score: 0.7674 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6455;  Loss pred: 0.6455; Loss self: 0.0000; time: 0.39s
Val loss: 0.6586 score: 0.8409 time: 0.28s
Test loss: 0.6556 score: 0.7442 time: 0.36s
Epoch 51/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.41s
Val loss: 0.6547 score: 0.8409 time: 0.27s
Test loss: 0.6517 score: 0.7907 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.38s
Val loss: 0.6506 score: 0.8636 time: 0.28s
Test loss: 0.6475 score: 0.7907 time: 0.27s
Epoch 53/1000, LR 0.000269
Train loss: 0.6315;  Loss pred: 0.6315; Loss self: 0.0000; time: 0.41s
Val loss: 0.6462 score: 0.9091 time: 0.29s
Test loss: 0.6429 score: 0.7907 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.6252;  Loss pred: 0.6252; Loss self: 0.0000; time: 0.40s
Val loss: 0.6415 score: 0.9091 time: 0.31s
Test loss: 0.6380 score: 0.7907 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6150;  Loss pred: 0.6150; Loss self: 0.0000; time: 0.42s
Val loss: 0.6364 score: 0.9318 time: 0.27s
Test loss: 0.6326 score: 0.7907 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.48s
Val loss: 0.6308 score: 0.9318 time: 0.37s
Test loss: 0.6268 score: 0.8140 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.35s
Val loss: 0.6249 score: 0.9091 time: 0.29s
Test loss: 0.6206 score: 0.8140 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.40s
Val loss: 0.6184 score: 0.9091 time: 0.28s
Test loss: 0.6139 score: 0.8372 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.5803;  Loss pred: 0.5803; Loss self: 0.0000; time: 0.43s
Val loss: 0.6115 score: 0.9091 time: 0.27s
Test loss: 0.6069 score: 0.8605 time: 0.26s
Epoch 60/1000, LR 0.000268
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.41s
Val loss: 0.6042 score: 0.9091 time: 0.29s
Test loss: 0.5995 score: 0.8372 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.38s
Val loss: 0.5965 score: 0.8636 time: 0.39s
Test loss: 0.5918 score: 0.8605 time: 0.26s
Epoch 62/1000, LR 0.000268
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.40s
Val loss: 0.5887 score: 0.8864 time: 0.29s
Test loss: 0.5835 score: 0.8605 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5415;  Loss pred: 0.5415; Loss self: 0.0000; time: 0.42s
Val loss: 0.5801 score: 0.8864 time: 0.27s
Test loss: 0.5750 score: 0.8605 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.38s
Val loss: 0.5711 score: 0.8864 time: 0.28s
Test loss: 0.5663 score: 0.8605 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.5259;  Loss pred: 0.5259; Loss self: 0.0000; time: 0.41s
Val loss: 0.5614 score: 0.8864 time: 0.29s
Test loss: 0.5575 score: 0.8605 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5060;  Loss pred: 0.5060; Loss self: 0.0000; time: 0.40s
Val loss: 0.5511 score: 0.9091 time: 0.38s
Test loss: 0.5488 score: 0.8605 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.4882;  Loss pred: 0.4882; Loss self: 0.0000; time: 0.41s
Val loss: 0.5409 score: 0.9091 time: 0.27s
Test loss: 0.5394 score: 0.8605 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.4805;  Loss pred: 0.4805; Loss self: 0.0000; time: 0.40s
Val loss: 0.5303 score: 0.9091 time: 0.30s
Test loss: 0.5298 score: 0.8605 time: 0.26s
Epoch 69/1000, LR 0.000268
Train loss: 0.4569;  Loss pred: 0.4569; Loss self: 0.0000; time: 0.37s
Val loss: 0.5195 score: 0.9091 time: 0.29s
Test loss: 0.5198 score: 0.8605 time: 0.23s
Epoch 70/1000, LR 0.000268
Train loss: 0.4510;  Loss pred: 0.4510; Loss self: 0.0000; time: 0.41s
Val loss: 0.5087 score: 0.9091 time: 0.29s
Test loss: 0.5093 score: 0.8605 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.4448;  Loss pred: 0.4448; Loss self: 0.0000; time: 0.41s
Val loss: 0.4980 score: 0.9091 time: 0.27s
Test loss: 0.4988 score: 0.8605 time: 0.33s
Epoch 72/1000, LR 0.000267
Train loss: 0.4002;  Loss pred: 0.4002; Loss self: 0.0000; time: 0.42s
Val loss: 0.4873 score: 0.9091 time: 0.28s
Test loss: 0.4879 score: 0.8605 time: 0.26s
Epoch 73/1000, LR 0.000267
Train loss: 0.4030;  Loss pred: 0.4030; Loss self: 0.0000; time: 0.35s
Val loss: 0.4764 score: 0.9091 time: 0.29s
Test loss: 0.4773 score: 0.8605 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.3842;  Loss pred: 0.3842; Loss self: 0.0000; time: 0.40s
Val loss: 0.4656 score: 0.8864 time: 0.29s
Test loss: 0.4669 score: 0.8605 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.3595;  Loss pred: 0.3595; Loss self: 0.0000; time: 0.42s
Val loss: 0.4557 score: 0.8636 time: 0.27s
Test loss: 0.4564 score: 0.8605 time: 0.28s
Epoch 76/1000, LR 0.000267
Train loss: 0.3522;  Loss pred: 0.3522; Loss self: 0.0000; time: 0.36s
Val loss: 0.4449 score: 0.8636 time: 0.27s
Test loss: 0.4467 score: 0.8605 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.3405;  Loss pred: 0.3405; Loss self: 0.0000; time: 0.40s
Val loss: 0.4343 score: 0.8636 time: 0.38s
Test loss: 0.4375 score: 0.8605 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.3236;  Loss pred: 0.3236; Loss self: 0.0000; time: 0.40s
Val loss: 0.4228 score: 0.8636 time: 0.26s
Test loss: 0.4293 score: 0.8605 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.3212;  Loss pred: 0.3212; Loss self: 0.0000; time: 0.41s
Val loss: 0.4096 score: 0.9091 time: 0.27s
Test loss: 0.4223 score: 0.8605 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.2847;  Loss pred: 0.2847; Loss self: 0.0000; time: 0.38s
Val loss: 0.3968 score: 0.9091 time: 0.29s
Test loss: 0.4162 score: 0.8605 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.2584;  Loss pred: 0.2584; Loss self: 0.0000; time: 0.41s
Val loss: 0.3842 score: 0.9091 time: 0.29s
Test loss: 0.4110 score: 0.8372 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.2627;  Loss pred: 0.2627; Loss self: 0.0000; time: 0.42s
Val loss: 0.3726 score: 0.9318 time: 0.37s
Test loss: 0.4063 score: 0.8372 time: 0.30s
Epoch 83/1000, LR 0.000266
Train loss: 0.2353;  Loss pred: 0.2353; Loss self: 0.0000; time: 0.41s
Val loss: 0.3624 score: 0.9318 time: 0.27s
Test loss: 0.4020 score: 0.8372 time: 0.26s
Epoch 84/1000, LR 0.000266
Train loss: 0.2263;  Loss pred: 0.2263; Loss self: 0.0000; time: 0.41s
Val loss: 0.3536 score: 0.9318 time: 0.28s
Test loss: 0.3981 score: 0.8372 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.2171;  Loss pred: 0.2171; Loss self: 0.0000; time: 0.45s
Val loss: 0.3497 score: 0.9091 time: 0.29s
Test loss: 0.3938 score: 0.8372 time: 0.25s
Epoch 86/1000, LR 0.000266
Train loss: 0.2071;  Loss pred: 0.2071; Loss self: 0.0000; time: 0.40s
Val loss: 0.3495 score: 0.9091 time: 0.29s
Test loss: 0.3907 score: 0.8372 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.1935;  Loss pred: 0.1935; Loss self: 0.0000; time: 0.39s
Val loss: 0.3543 score: 0.8636 time: 0.26s
Test loss: 0.3887 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 88/1000, LR 0.000266
Train loss: 0.1886;  Loss pred: 0.1886; Loss self: 0.0000; time: 0.51s
Val loss: 0.3598 score: 0.8182 time: 0.28s
Test loss: 0.3885 score: 0.8605 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 085,   Train_Loss: 0.2071,   Val_Loss: 0.3495,   Val_Precision: 1.0000,   Val_Recall: 0.8182,   Val_accuracy: 0.9000,   Val_Score: 0.9091,   Val_Loss: 0.3495,   Test_Precision: 0.8500,   Test_Recall: 0.8095,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.3907


[0.281553523032926, 0.2972645499976352, 0.3178476900793612, 0.2826410139678046, 0.28397698898334056, 0.2961928049335256, 0.3086709020426497, 0.29353999497834593, 0.29644098808057606, 0.307812706916593, 0.28942857892252505, 0.2881169399479404, 0.2980497410753742, 0.31561325502116233, 0.2820465490221977, 0.29386050708126277, 0.4255499739665538, 0.3235310820164159, 0.2818759479559958, 0.2947484280448407, 0.3091699290089309, 0.2859099640045315, 0.27992139593698084, 0.2970595830120146, 0.30454535596072674, 0.2801138189388439, 0.298020399059169, 0.2943908349843696, 0.3041476389626041, 0.27667484106495976, 0.2913504899479449, 0.30606822099070996, 0.3074198120739311, 0.28033759794197977, 0.2958713100524619, 0.29830188991036266, 0.29009157605469227, 0.29830680903978646, 0.3717687289463356, 0.32011596602387726, 0.29919712396804243, 0.3023414280032739, 0.3014788019936532, 0.2967679549474269, 0.2967267179628834, 0.4138873389456421, 0.3055095650488511, 0.2999637379543856, 0.3061135020107031, 0.2809046730399132, 0.29844643792603165, 0.4178837299114093, 0.31337156903464347, 0.37097399996127933, 0.28775370796211064, 0.3639340860536322, 0.4033061460359022, 0.3016314379638061, 0.29917888692580163, 0.3216994170797989, 0.28437897097319365, 0.3006288791075349, 0.30764961696695536, 0.30094404297415167, 0.28084238700103015, 0.2849791720509529, 0.3715573340887204, 0.29248019598890096, 0.3094281710218638, 0.29589773004408926, 0.29745451000053436, 0.4280362830031663, 0.2917604480171576, 0.34621229697950184, 0.2877417110139504, 0.2947037579724565, 0.32662245398387313, 0.3055603870889172, 0.37326488306280226, 0.28512381203472614, 0.2992840240476653, 0.289886049926281, 0.282508322969079, 0.2971079220296815, 0.3887889670440927, 0.29787915700580925, 0.2912655199179426, 0.30339442507829517, 0.2894196549896151, 0.28450412407983094, 0.3021992640569806, 0.3027473579859361, 0.28538378805387765, 0.29809155501425266, 0.2855666489340365, 0.3409249889664352, 0.3884344630641863, 0.3050488489679992, 0.2585200140019879, 0.25622624706011266, 0.2681261310353875, 0.26367176801431924, 0.25527859304565936, 0.2613805599976331, 0.26266732707154006, 0.30516622599679977, 0.2970447379630059, 0.24695326504297554, 0.2586394640384242, 0.273246729047969, 0.2738999780267477, 0.25129100296180695, 0.25915148691274226, 0.2834449530346319, 0.26930630300194025, 0.3695639061043039, 0.24799152801278979, 0.2492189429467544, 0.29154523892793804, 0.28160478500649333, 0.2831706360448152, 0.2498933010501787, 0.3138926209649071, 0.2727994970045984, 0.2709843290504068, 0.2572206789627671, 0.25733065709937364, 0.26037387596443295, 0.2787395330378786, 0.25206362595781684, 0.24353612505365163, 0.2605370790697634, 0.27949934895150363, 0.2626549639971927, 0.2491441749734804, 0.24190075206570327, 0.26172154501546174, 0.2613358099479228, 0.24316464003641158, 0.2581982220290229, 0.26009069406427443, 0.2607005669269711, 0.2507710329955444, 0.25515134702436626, 0.27473054302390665, 0.2791736810468137, 0.2486170280026272, 0.2578836780739948, 0.27719052601605654, 0.27077171101700515, 0.2820285059278831, 0.25567720201797783, 0.2443979480303824, 0.27723167708609253, 0.25518276903312653, 0.2501868649851531, 0.2580405059270561, 0.2600373940076679, 0.27327543299179524, 0.2506697300123051, 0.2536152789834887, 0.2604575159493834, 0.28047181700821966, 0.2902171660680324, 0.2588135419646278, 0.25739768997300416, 0.27776156400796026, 0.2660973450401798, 0.25848692597355694, 0.2791263359831646, 0.271502329967916, 0.2593580209650099, 0.25901387503836304, 0.26831559801939875, 0.2643615179695189, 0.2602454840671271, 0.25609729799907655, 0.27609474293421954, 0.36723636696115136, 0.2414522550534457, 0.2540603559464216, 0.27302339498419315, 0.27497328410390764, 0.25054780091159046, 0.2609332030406222, 0.339938408927992, 0.27550135098863393, 0.26117057306692004, 0.23745630902703851, 0.24931455799378455, 0.2362932930700481, 0.25683801202103496, 0.2622578439768404, 0.23881326196715236, 0.24647852790076286, 0.32419889396987855, 0.27504474099259824, 0.23797661706339568, 0.2341090840054676, 0.2622051159851253, 0.2591971440706402, 0.23856492503546178, 0.23418300598859787, 0.2648925369139761, 0.2572445090627298, 0.25156964897178113, 0.2520351819694042, 0.25125626707449555, 0.2738517780089751, 0.24065998091828078, 0.23430025298148394, 0.2579868830507621, 0.25761779898311943, 0.2469258219935, 0.23487476201262325, 0.232830811990425, 0.24334187898784876, 0.33902344200760126, 0.2559220469556749, 0.2408653220627457, 0.25112124101724476, 0.25112459994852543, 0.25735801295377314, 0.2533526160987094, 0.23736695700790733, 0.24993488600011915, 0.2556823450140655, 0.23555250803474337, 0.2369254119694233, 0.26360354491043836, 0.25152495596557856, 0.3529178319731727, 0.24006333795841783, 0.25504269590601325, 0.25878403102979064, 0.23386460507754236, 0.36779214907437563, 0.2559368379879743, 0.27324742602650076, 0.24068132601678371, 0.25372251705266535, 0.2507172030163929, 0.2535202989820391, 0.24974117998499423, 0.23645199893508106, 0.2628306309925392, 0.2543023460311815, 0.2653547250665724, 0.25397086702287197, 0.25020973291248083, 0.25355973199475557, 0.2352769629796967, 0.24708804592955858, 0.2566285900538787, 0.26238577405456454, 0.23845186200924218, 0.25864217698108405, 0.33044201496522874, 0.26361910905689, 0.23863195604644716, 0.24235229298938066, 0.2793496010126546, 0.23698923795018345, 0.2349582799943164, 0.24864440993405879, 0.2590354810236022, 0.2375976339681074, 0.24415611708536744, 0.3080667000031099, 0.2601803661091253, 0.25349383405409753, 0.2533251299755648, 0.25413212494459003, 0.2580503289354965, 0.25502769998274744]
[0.006398943705293773, 0.006756012499946254, 0.0072238111381673, 0.006423659408359195, 0.006454022476894103, 0.006731654657580128, 0.007015247773696584, 0.006671363522235135, 0.006737295183649456, 0.0069957433390134775, 0.006577922248239206, 0.006548112271544101, 0.00677385775171305, 0.007173028523208235, 0.006410148841413585, 0.0066786478882105175, 0.009671590317421678, 0.007352979136736725, 0.00640627154445445, 0.006698827910110016, 0.007026589295657521, 0.006497953727375716, 0.006361849907658656, 0.006751354159363968, 0.00692148536274379, 0.006366223157700998, 0.006773190887708386, 0.006690700795099308, 0.006912446340059184, 0.006288064569658177, 0.006621602044271474, 0.006956095931607045, 0.006986813910771161, 0.006371309044135904, 0.006724347955737771, 0.006779588407053697, 0.00659299036487937, 0.0067797002054496925, 0.0084492892942349, 0.007275362864179028, 0.006799934635637328, 0.006871396090983498, 0.006851790954401208, 0.006744726248805157, 0.006743789044610987, 0.009406530430582776, 0.006943399205655706, 0.006817357680781491, 0.006957125045697798, 0.006384197114543481, 0.006782873589227992, 0.009497357497986575, 0.007122081114423715, 0.008431227271847258, 0.0065398569991388785, 0.00827122922849164, 0.009166048773543232, 0.006855259953722866, 0.006799520157404582, 0.007311350388177248, 0.006463158431208946, 0.0068324745251712475, 0.006992036749248986, 0.0068396373403216285, 0.006382781522750685, 0.006476799364794384, 0.008444484865652736, 0.006647277181565931, 0.007032458432315087, 0.006724948410092938, 0.006760329772739417, 0.009728097340981052, 0.006630919273117219, 0.007868461294988678, 0.006539584341226146, 0.006697812681192194, 0.0074232375905425715, 0.0069445542520208455, 0.00848329279688187, 0.006480086637152867, 0.006801909637446938, 0.006588319316506386, 0.006420643703842705, 0.006752452773401852, 0.008836112887365744, 0.00676998084104112, 0.006619670907225968, 0.006895327842688526, 0.006577719431582161, 0.006466002819996158, 0.0068681650922041045, 0.006880621772407639, 0.0064859951830426735, 0.006774808068505742, 0.006490151112137193, 0.007748295203782618, 0.008828055978731507, 0.006932928385636346, 0.006012093348883439, 0.005958749931630527, 0.006235491419427617, 0.006131901581728354, 0.005936711466178125, 0.00607861767436356, 0.0061085424900358155, 0.007096888976669762, 0.00690801716193037, 0.005743099187045943, 0.0060148712567075395, 0.006354575094138813, 0.006369766930854598, 0.005843976813065278, 0.006026778765412611, 0.0065917430938286494, 0.00626293727911489, 0.008594509444286138, 0.005767244837506739, 0.005795789370854753, 0.006780121835533443, 0.006548948488523101, 0.00658536362894919, 0.005811472117446016, 0.007299828394532724, 0.006344174348944148, 0.006301961140707135, 0.005981876254948073, 0.005984433886031945, 0.006055206417777511, 0.006482314721811129, 0.0058619447897166705, 0.005663630815201201, 0.006059001838831708, 0.0064999848593372935, 0.0061082549766789, 0.005794050580778614, 0.005625598885248913, 0.00608654755849911, 0.006077576975533089, 0.005654991628753758, 0.006004609814628439, 0.006048620792192428, 0.006062803882022583, 0.005831884488268474, 0.005933752256380611, 0.006389082395904805, 0.006492411187135203, 0.005781791348898307, 0.005997294838930112, 0.006446291302698989, 0.00629701653527919, 0.006558802463439142, 0.005945981442278554, 0.005683673210008893, 0.006447248304327733, 0.005934483000770385, 0.005818299185701235, 0.00600094199830363, 0.006047381255992277, 0.006355242627716169, 0.005829528604937327, 0.005898029743802062, 0.00605715153370659, 0.006522600395539992, 0.0067492364201868, 0.006018919580572739, 0.0059859927900698644, 0.006459571255999076, 0.006188310349771623, 0.006011323859850161, 0.006491310139143363, 0.006314007673672465, 0.006031581882907208, 0.006023578489264257, 0.006239897628358111, 0.006147942278360905, 0.00605222055970063, 0.005955751116257594, 0.006420807975214408, 0.00854038062700352, 0.005615168722173156, 0.005908380370847014, 0.006349381278702166, 0.006394727537300178, 0.005826693044455592, 0.0060682140242005154, 0.007905544393674233, 0.0064070081625263705, 0.0060737342573702335, 0.0055222397448148495, 0.0057980129765996405, 0.005495192862094142, 0.005972977023744999, 0.006099019627368381, 0.005553796789933776, 0.005732058788389834, 0.007539509162090199, 0.006396389325409261, 0.005534339931706877, 0.005444397302452735, 0.006097793395002914, 0.00602784055978233, 0.0055480215124526, 0.005446116418339485, 0.006160291556138979, 0.0059824304433192975, 0.005850456952832119, 0.0058612833016140516, 0.005843169001732455, 0.006368646000208723, 0.0055967437422856, 0.00544884309259265, 0.005999694954668886, 0.005991111604258591, 0.005742460976593023, 0.005462203767735424, 0.0054146700462889535, 0.005659113464833692, 0.00788426609320003, 0.005951675510597091, 0.0056015191177382715, 0.005840028860866157, 0.005840106975547103, 0.0059850700686923984, 0.005891921304621149, 0.005520161790881566, 0.005812439209305097, 0.005946101046838733, 0.005477965303133566, 0.005509893301614496, 0.006130314997917171, 0.00584941758059485, 0.008207391441236575, 0.005582868324614368, 0.005931225486186354, 0.006018233279762573, 0.005438711745989358, 0.00855330579242734, 0.005952019488092425, 0.006354591302941878, 0.005597240139925202, 0.005900523652387566, 0.005830632628288207, 0.005895820906559049, 0.00580793441825568, 0.005498883696164676, 0.006112340255640447, 0.00591400804723678, 0.006171040117827265, 0.005906299233090046, 0.005818830997964671, 0.005896737953366408, 0.005471557278597597, 0.005746233626268804, 0.00596810674543904, 0.006101994745454989, 0.005545392139749818, 0.006014934348397304, 0.00768469802244718, 0.006130676954811396, 0.0055495803731731895, 0.005636099836962341, 0.006496502349131503, 0.005511377626748452, 0.005464146046379451, 0.005782428138001367, 0.00602408095403726, 0.005525526371351335, 0.005678049234543429, 0.0071643418605374385, 0.00605070618858431, 0.005895205443118548, 0.0058912820924549945, 0.005910049417316047, 0.006001170440360384, 0.005930876743784824]
[156.27579270195977, 148.01630399706266, 138.43108310465917, 155.67450520472588, 154.9421316055339, 148.55188670053917, 142.54664015567113, 149.8943951513177, 148.4275176820025, 142.9440663472107, 152.02368806771506, 152.71576884007695, 147.62636545580082, 139.41112833505593, 156.0026178392883, 149.73090612626095, 103.39561201208812, 135.99929789054224, 156.09703601553448, 149.2798461788783, 142.31655756769598, 153.89460158619244, 157.1869840557161, 148.11843319062496, 144.4776587093126, 157.07900512257962, 147.64090021657958, 149.46117463995148, 144.66658413024845, 159.03144583236374, 151.0208546684147, 143.75879945188927, 143.1267545938727, 156.9536170781719, 148.7133037407318, 147.5015797359569, 151.67624168343477, 147.49914741011335, 118.3531496172486, 137.45018890035016, 147.06023713215802, 145.53083343749884, 145.94724308651823, 148.26398627774583, 148.28459095990075, 106.30912294173545, 144.02167733427393, 146.68439692097357, 143.73753431647575, 156.63676764020272, 147.43013957802762, 105.2924458421196, 140.40839804180118, 118.60669482118027, 152.9085422099708, 120.90101390919402, 109.0982630254366, 145.8733887191153, 147.0692014804918, 136.77363919215813, 154.72311419309398, 146.35985781080336, 143.01984326775886, 146.20658234387844, 156.6715069966935, 154.39724834393516, 118.4204857856303, 150.43753595429658, 142.1977832680626, 148.70002549003647, 147.92177802219558, 102.7950240369565, 150.80865243740126, 127.08965101434599, 152.9149174965001, 149.3024734489891, 134.7121101544736, 143.9977230660993, 117.87875580194066, 154.3189244209498, 147.0175367362488, 151.78377852673268, 155.74762377820588, 148.0943345415217, 113.17193575354196, 147.71090546339204, 151.0649115363741, 145.02573667477637, 152.0283755489198, 154.6550516352225, 145.59929567433912, 145.33570265555872, 154.17834453754335, 147.60565759032062, 154.07961736513437, 129.06064801348984, 113.27522190720056, 144.23919365326228, 166.33141602595362, 167.82043406314995, 160.37228387234225, 163.08154765232504, 168.44342287764402, 164.51108682447304, 163.70517216360344, 140.906811884389, 144.75933926611162, 174.12201451362472, 166.254597533545, 157.36693408853677, 156.9916153691726, 171.116353125207, 165.92611723844107, 151.70494143441732, 159.66948979909392, 116.35335402008629, 173.39302009455074, 172.53905137213806, 147.4899749970824, 152.69626898920944, 151.85190315140844, 172.07344022145506, 136.98952166450374, 157.62492406382694, 158.68076265030592, 167.1716293316537, 167.1001834165241, 165.147136365838, 154.26588231442804, 170.59184892942903, 176.56518099943895, 165.04368650147487, 153.8465122058692, 163.71287770696614, 172.59083020735702, 177.7588520614465, 164.29675286174734, 164.53925701406456, 176.8349213666969, 166.53871456623187, 165.32694548992092, 164.940186002915, 171.47116031046542, 168.5274269623731, 156.5169985350271, 154.0259806682474, 172.95677752026546, 166.74184392415083, 155.1279569977409, 158.80536352373787, 152.46685741403545, 168.18081416964378, 175.9425574009093, 155.10493047533896, 168.50667528581428, 171.87153291421498, 166.6405041546284, 165.36083267598056, 157.35040478845758, 171.54045683094319, 169.54814462420248, 165.09410313333598, 153.31308670753117, 148.16490899756076, 166.14277473114925, 167.05666629918022, 154.80903613707935, 161.59499822708545, 166.35270754234926, 154.0521063644583, 158.37801467516465, 165.79398562653725, 166.01427237684155, 160.25903942003094, 162.65604892221089, 165.22861157086854, 167.90493431974846, 155.74363909654335, 117.09080000932701, 178.08903872312936, 169.251120820551, 157.4956607747461, 156.37882836556238, 171.6239370034351, 164.7931328743385, 126.49350256007271, 156.0790894334816, 164.64335738537457, 181.085944509917, 172.47288062926512, 181.97723448398023, 167.4207009376724, 163.96077748506644, 180.05700205173048, 174.4573872873529, 132.63462892626384, 156.33820099529618, 180.69002127442224, 183.67505978108795, 163.99374908626632, 165.89688962113337, 180.2444344809205, 183.61708108782935, 162.32997917175848, 167.15614322214486, 170.92681957362575, 170.6111014501935, 171.14000976242647, 157.0192471001256, 178.6753237323708, 183.52519663475636, 166.6751405789077, 166.91393284831847, 174.1413662323737, 183.07628981307485, 184.68346020185828, 176.70612300214546, 126.8348871256988, 168.01991274885162, 178.5230004541644, 171.23203049576816, 171.2297401720659, 167.08242151264858, 169.72392336871175, 181.15411067332153, 172.04481010297818, 168.17743124827214, 182.54953156201424, 181.4917177628435, 163.1237547075083, 170.95719124540705, 121.84139225718884, 179.11939559654115, 168.59922158228005, 166.16172114209758, 183.86707123013554, 116.91386047314582, 168.01020258764174, 157.36653268905695, 178.65947770705498, 169.47648359910627, 171.50797584954108, 169.6116649146362, 172.17825271180214, 181.85509191574158, 163.6034576244677, 169.09006413463254, 162.0472369173457, 169.31075797810908, 171.8558247094276, 169.58528730772352, 182.76332478718157, 174.02703493093597, 167.55732473522232, 163.8808359749638, 180.32989819275707, 166.25285366023203, 130.12873076846702, 163.11412383508372, 180.19380435213174, 177.4276590066518, 153.92898305250162, 181.4428383833989, 183.01121373990378, 172.93773067893915, 166.00042523163853, 180.978233166126, 176.11682440446643, 139.58016234655003, 165.26996499791557, 169.6293724873145, 169.74233864657523, 169.2033229147065, 166.63416077546827, 168.60913541120803]
Elapsed: 0.2789026026868007~0.03624605333276666
Time per graph: 0.006427819280788144~0.0007896639408987753
Speed: 157.5898153791971~16.665614453376147
Total Time: 0.2558
best val loss: 0.3494734466075897 test_score: 0.8372

Testing...
Test loss: 0.6326 score: 0.7907 time: 0.25s
test Score 0.7907
Epoch Time List: [0.8959221319528297, 0.9450483248801902, 0.9515445899451151, 0.8911865039262921, 1.0144796720705926, 0.9170008349465206, 0.9372958341846243, 0.8848399750422686, 0.9160824299324304, 0.9699058281257749, 0.8869341020472348, 1.0726877989945933, 0.9475527589675039, 1.0363334850408137, 0.8875768929719925, 0.9415446530329064, 1.055723047000356, 0.959961929009296, 0.8682471389183775, 0.9116258471040055, 1.0324713399168104, 0.9045113520696759, 1.0113898938288912, 0.9484419429209083, 1.0202515090350062, 0.8728961700107902, 0.9340287448139861, 0.9886812070617452, 1.0210816009202972, 0.8615542059997097, 0.940783723955974, 0.9316062801517546, 1.0257227130932733, 0.875392128014937, 1.0352282680105418, 0.9168708720244467, 0.8751894949236885, 0.9281157051445916, 0.9891729190712795, 1.071504418971017, 0.9007175880251452, 0.9258984220214188, 0.9111768008442596, 0.9133373709628358, 0.9101053100312129, 1.0293518870603293, 0.9431274197995663, 0.9319955081446096, 1.0461285449564457, 0.9277692619943991, 1.0085199099266902, 1.0307927960529923, 0.949213583022356, 1.0067617901368067, 0.9132569180801511, 0.9910698410822079, 1.0219824290834367, 1.07692709506955, 0.9320793189108372, 0.9598568410146981, 1.0229231860721484, 1.0110651049762964, 1.082693561911583, 0.9238902339711785, 1.0069632119266316, 1.010214045876637, 1.022734744939953, 1.0223020940320566, 1.0041615929221734, 0.9237181729404256, 0.9404964098939672, 1.0872164720203727, 0.8883214090019464, 0.9971735350554809, 0.898192677879706, 0.990469055948779, 0.948846113984473, 1.0678147989092395, 0.9579430789453909, 0.89433625980746, 0.9144327298272401, 0.8811952349497005, 0.9020719471154734, 0.9172243430512026, 0.9793091539759189, 0.9030843308428302, 0.9042978319339454, 1.0516563279088587, 0.8778257670346648, 0.8969112630002201, 1.196867095073685, 0.920303856022656, 1.0077678029192612, 1.0218852488324046, 0.9728509178385139, 1.1180121138459072, 1.0209090299904346, 0.9059694759780541, 0.9416965851560235, 1.0056875880109146, 1.0731785258976743, 0.9377523149596527, 0.9252205360680819, 0.9309580411063507, 1.0954112310428172, 1.085586010129191, 1.0008392919553444, 0.8881242859642953, 0.9560214199591428, 0.934721295023337, 1.0352150758262724, 0.882185201975517, 0.9472387199057266, 0.9313297349726781, 0.9275467720581219, 1.1271359820384532, 0.9185158389154822, 1.016359896864742, 0.9578211259795353, 0.9374358110362664, 1.1307424809783697, 0.9214600180275738, 0.9975256220204756, 0.935347385937348, 0.9821607460035011, 1.0165873371297494, 0.927227703970857, 0.9632312279427424, 0.9773070799419656, 0.9113484399858862, 1.0337238669162616, 0.9693035479867831, 0.9469543668674305, 0.9595885600429028, 0.9601389419985935, 1.0332315941341221, 0.9188196160830557, 0.9136959470342845, 0.9516393379308283, 0.948270213091746, 1.0449424921534956, 0.9198603939730674, 0.9186982722021639, 0.9400300530251116, 0.9532837299630046, 1.0347587529104203, 0.9361557101365179, 0.995509588974528, 0.9829584941035137, 0.9905066171195358, 1.0473033860325813, 0.9373371979454532, 0.9462466649711132, 0.9360340611310676, 0.8794138040393591, 1.0247984199086204, 0.9322369621368125, 1.0214198529720306, 0.9342660179827362, 0.9099525929195806, 1.0427855281159282, 0.9150646219495684, 0.9802385019138455, 1.0248644150560722, 0.9308781590079889, 1.028140897047706, 0.9371334729949012, 0.9091535758925602, 0.9217030690051615, 0.9318020069040358, 1.045660668052733, 0.8930109140928835, 0.9274504368659109, 0.92201759503223, 0.9328521669376642, 1.0126767969923094, 0.9408966939663514, 0.948050299892202, 1.0196472248062491, 0.9251686588395387, 1.03792579297442, 0.9429419310763478, 0.9471104099648073, 0.9247657130472362, 0.9675748719600961, 1.105650938116014, 0.952889489941299, 0.9582429568981752, 0.9165968380402774, 1.1282263320172206, 1.0007729440694675, 0.9403280250262469, 0.943514236016199, 0.8914006698178127, 1.0521095939911902, 1.0055205159587786, 0.9510618119966239, 0.8755995890824124, 0.9108940839068964, 0.944815909024328, 1.048357842140831, 0.8651485620066524, 0.9668803089298308, 0.9415387250483036, 0.947410250082612, 1.0382848411099985, 0.9403817458078265, 0.9285612779203802, 1.020727365044877, 0.879599082050845, 0.9170391960069537, 1.0624424220295623, 0.9396532459650189, 0.9058426019037142, 0.9492259089602157, 0.9868638451443985, 1.0445944239618257, 1.0421076558995992, 0.9410907840356231, 0.9796160130063072, 0.9622539560077712, 1.0603638470638543, 0.9266909619327635, 0.944102433975786, 0.9086592680541798, 0.9126753041055053, 1.0332581920083612, 0.8770655628759414, 0.9160617151064798, 0.9449676929507405, 0.930286799906753, 1.0238485998706892, 0.9266082809772342, 0.9255849800538272, 0.9341334680793807, 0.9084239811636508, 1.0386017869459465, 0.9358691780362278, 0.9308222620747983, 0.9396488260244951, 0.955094680073671, 0.9399687930708751, 1.1031712351832539, 0.8858715740498155, 0.9160397199448198, 0.9576250759419054, 0.9435568010667339, 1.0306867179460824, 0.9332163068465889, 0.9332892538513988, 0.9116808320395648, 0.9350439250702038, 1.024953322019428, 0.9364783770870417, 0.9619246840011328, 0.8949674039613456, 0.9456845798995346, 1.0092743269633502, 0.9541293820366263, 0.8800206501036882, 0.932309067924507, 0.9666581051424146, 0.8606492270482704, 1.0152860898524523, 0.9129517209948972, 0.9369934380520135, 0.9024899388896301, 0.933977515087463, 1.0856036299373955, 0.9337655070703477, 0.94230585207697, 0.996899844147265, 0.9442824749276042, 0.9075337210670114, 1.0401594571303576]
Total Epoch List: [98, 89, 88]
Total Time List: [0.3055155740585178, 0.2761488629039377, 0.25580803595948964]
T-times Epoch Time: 1.1483783091646902 ~ 0.5622433683346754
T-times Total Epoch: 84.83333333333333 ~ 13.83895468114072
T-times Total Time: 0.27605703343482063 ~ 0.013509525136584547
T-times Inference Elapsed: 0.3156226584934785 ~ 0.12542466955168582
T-times Time Per Graph: 0.0073047626353420474 ~ 0.002926844110511746
T-times Speed: 158.14180904338966 ~ 6.804546222660619
T-times cross validation test micro f1 score:0.7603796556684874 ~ 0.0574705381183497
T-times cross validation test precision:0.8020148415097264 ~ 0.11193739721076354
T-times cross validation test recall:0.7445887445887446 ~ 0.11766081926250102
T-times cross validation test f1_score:0.7603796556684874 ~ 0.10873397319890299
