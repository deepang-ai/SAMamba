Namespace(seed=60, model='GPSTransformer', dataset='mining/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/averVolume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Processing...
Loading necessary files...
This might take a while.
Processing graphs...
  0%|          | 0/130 [00:00<?, ?it/s]100%|##########| 130/130 [00:00<00:00, 111028.21it/s]
Converting graphs into PyG objects...
  0%|          | 0/130 [00:00<?, ?it/s]100%|##########| 130/130 [00:00<00:00, 49596.10it/s]
Saving...
Done!
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 390], edge_attr=[390, 2], x=[103, 14887], y=[1, 1], num_nodes=115)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156e8ad2aa0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7972;  Loss pred: 0.7972; Loss self: 0.0000; time: 0.24s
Val loss: 1.0911 score: 0.4884 time: 0.04s
Test loss: 1.3625 score: 0.4545 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7977;  Loss pred: 0.7977; Loss self: 0.0000; time: 0.13s
Val loss: 0.9055 score: 0.4651 time: 0.04s
Test loss: 1.0693 score: 0.4773 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7561;  Loss pred: 0.7561; Loss self: 0.0000; time: 0.13s
Val loss: 0.8286 score: 0.4419 time: 0.04s
Test loss: 0.9403 score: 0.4773 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.13s
Val loss: 0.7872 score: 0.4419 time: 0.04s
Test loss: 0.8743 score: 0.4773 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.13s
Val loss: 0.7643 score: 0.4651 time: 0.04s
Test loss: 0.8339 score: 0.4773 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.5642;  Loss pred: 0.5642; Loss self: 0.0000; time: 0.13s
Val loss: 0.7502 score: 0.4884 time: 0.04s
Test loss: 0.8172 score: 0.5000 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.4939;  Loss pred: 0.4939; Loss self: 0.0000; time: 0.13s
Val loss: 0.7455 score: 0.4884 time: 0.05s
Test loss: 0.8316 score: 0.4773 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.4337;  Loss pred: 0.4337; Loss self: 0.0000; time: 0.13s
Val loss: 0.7478 score: 0.4884 time: 0.04s
Test loss: 0.8445 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3752;  Loss pred: 0.3752; Loss self: 0.0000; time: 0.13s
Val loss: 0.7503 score: 0.4884 time: 0.04s
Test loss: 0.8610 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3308;  Loss pred: 0.3308; Loss self: 0.0000; time: 0.13s
Val loss: 0.7486 score: 0.4884 time: 0.04s
Test loss: 0.8798 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2842;  Loss pred: 0.2842; Loss self: 0.0000; time: 0.13s
Val loss: 0.7427 score: 0.4884 time: 0.04s
Test loss: 0.8795 score: 0.4773 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.2591;  Loss pred: 0.2591; Loss self: 0.0000; time: 0.13s
Val loss: 0.7364 score: 0.4884 time: 0.04s
Test loss: 0.8711 score: 0.4773 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.2330;  Loss pred: 0.2330; Loss self: 0.0000; time: 0.13s
Val loss: 0.7280 score: 0.4884 time: 0.04s
Test loss: 0.8536 score: 0.4773 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.2133;  Loss pred: 0.2133; Loss self: 0.0000; time: 0.13s
Val loss: 0.7168 score: 0.5116 time: 0.04s
Test loss: 0.8297 score: 0.5227 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.1945;  Loss pred: 0.1945; Loss self: 0.0000; time: 0.13s
Val loss: 0.7043 score: 0.5349 time: 0.04s
Test loss: 0.8021 score: 0.5227 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.1736;  Loss pred: 0.1736; Loss self: 0.0000; time: 0.13s
Val loss: 0.6913 score: 0.5349 time: 0.04s
Test loss: 0.7750 score: 0.5682 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.1587;  Loss pred: 0.1587; Loss self: 0.0000; time: 0.13s
Val loss: 0.6791 score: 0.6279 time: 0.04s
Test loss: 0.7489 score: 0.6136 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.13s
Val loss: 0.6670 score: 0.5814 time: 0.04s
Test loss: 0.7215 score: 0.6364 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 0.1389;  Loss pred: 0.1389; Loss self: 0.0000; time: 0.13s
Val loss: 0.6572 score: 0.5814 time: 0.04s
Test loss: 0.6964 score: 0.6364 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.1260;  Loss pred: 0.1260; Loss self: 0.0000; time: 0.13s
Val loss: 0.6501 score: 0.5814 time: 0.04s
Test loss: 0.6717 score: 0.6364 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 0.1127;  Loss pred: 0.1127; Loss self: 0.0000; time: 0.13s
Val loss: 0.6449 score: 0.5814 time: 0.04s
Test loss: 0.6515 score: 0.6591 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 0.1037;  Loss pred: 0.1037; Loss self: 0.0000; time: 0.13s
Val loss: 0.6404 score: 0.6279 time: 0.04s
Test loss: 0.6381 score: 0.6591 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 0.0919;  Loss pred: 0.0919; Loss self: 0.0000; time: 0.13s
Val loss: 0.6367 score: 0.6279 time: 0.04s
Test loss: 0.6283 score: 0.6136 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.0812;  Loss pred: 0.0812; Loss self: 0.0000; time: 0.14s
Val loss: 0.6332 score: 0.6047 time: 0.04s
Test loss: 0.6211 score: 0.5682 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.14s
Val loss: 0.6299 score: 0.6047 time: 0.04s
Test loss: 0.6180 score: 0.5455 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.0676;  Loss pred: 0.0676; Loss self: 0.0000; time: 0.14s
Val loss: 0.6269 score: 0.5814 time: 0.04s
Test loss: 0.6155 score: 0.5909 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.0608;  Loss pred: 0.0608; Loss self: 0.0000; time: 0.14s
Val loss: 0.6230 score: 0.5814 time: 0.04s
Test loss: 0.6143 score: 0.5909 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.0602;  Loss pred: 0.0602; Loss self: 0.0000; time: 0.14s
Val loss: 0.6195 score: 0.6047 time: 0.04s
Test loss: 0.6135 score: 0.6136 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.14s
Val loss: 0.6149 score: 0.5814 time: 0.04s
Test loss: 0.6142 score: 0.6136 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.0460;  Loss pred: 0.0460; Loss self: 0.0000; time: 0.14s
Val loss: 0.6113 score: 0.6279 time: 0.04s
Test loss: 0.6160 score: 0.6136 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.0404;  Loss pred: 0.0404; Loss self: 0.0000; time: 0.14s
Val loss: 0.6093 score: 0.6279 time: 0.04s
Test loss: 0.6203 score: 0.5682 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.0384;  Loss pred: 0.0384; Loss self: 0.0000; time: 0.13s
Val loss: 0.6086 score: 0.6279 time: 0.04s
Test loss: 0.6278 score: 0.5682 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.14s
Val loss: 0.6094 score: 0.6047 time: 0.04s
Test loss: 0.6351 score: 0.5455 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.13s
Val loss: 0.6117 score: 0.6047 time: 0.04s
Test loss: 0.6429 score: 0.5455 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0293;  Loss pred: 0.0293; Loss self: 0.0000; time: 0.13s
Val loss: 0.6144 score: 0.6047 time: 0.04s
Test loss: 0.6489 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.14s
Val loss: 0.6172 score: 0.6279 time: 0.04s
Test loss: 0.6524 score: 0.5455 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.13s
Val loss: 0.6194 score: 0.6279 time: 0.04s
Test loss: 0.6525 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0219;  Loss pred: 0.0219; Loss self: 0.0000; time: 0.13s
Val loss: 0.6212 score: 0.6279 time: 0.04s
Test loss: 0.6520 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.14s
Val loss: 0.6215 score: 0.6279 time: 0.04s
Test loss: 0.6486 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.13s
Val loss: 0.6219 score: 0.6279 time: 0.04s
Test loss: 0.6470 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.13s
Val loss: 0.6229 score: 0.6512 time: 0.04s
Test loss: 0.6477 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.14s
Val loss: 0.6242 score: 0.6512 time: 0.04s
Test loss: 0.6510 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.14s
Val loss: 0.6256 score: 0.6744 time: 0.04s
Test loss: 0.6550 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.14s
Val loss: 0.6277 score: 0.6512 time: 0.04s
Test loss: 0.6589 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.14s
Val loss: 0.6299 score: 0.6512 time: 0.04s
Test loss: 0.6632 score: 0.5455 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.13s
Val loss: 0.6313 score: 0.6512 time: 0.04s
Test loss: 0.6650 score: 0.5455 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.13s
Val loss: 0.6322 score: 0.6512 time: 0.04s
Test loss: 0.6652 score: 0.5455 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.13s
Val loss: 0.6320 score: 0.6977 time: 0.04s
Test loss: 0.6624 score: 0.5455 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.12s
Val loss: 0.6326 score: 0.6744 time: 0.04s
Test loss: 0.6605 score: 0.5455 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.12s
Val loss: 0.6315 score: 0.6744 time: 0.04s
Test loss: 0.6574 score: 0.5682 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.12s
Val loss: 0.6302 score: 0.6744 time: 0.04s
Test loss: 0.6541 score: 0.5682 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.13s
Val loss: 0.6282 score: 0.6744 time: 0.04s
Test loss: 0.6505 score: 0.5682 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 031,   Train_Loss: 0.0384,   Val_Loss: 0.6086,   Val_Precision: 0.5882,   Val_Recall: 0.9091,   Val_accuracy: 0.7143,   Val_Score: 0.6279,   Val_Loss: 0.6086,   Test_Precision: 0.5429,   Test_Recall: 0.8636,   Test_accuracy: 0.6667,   Test_Score: 0.5682,   Test_loss: 0.6278


[0.050254620029591024, 0.049901769030839205, 0.049728312995284796, 0.04956449696328491, 0.05023940501268953, 0.059270345023833215, 0.050475060008466244, 0.04962450498715043, 0.053454627050086856, 0.05074190301820636, 0.04724080197047442, 0.04762235493399203, 0.04745094198733568, 0.04709047498181462, 0.04692380304913968, 0.047160531976260245, 0.04738903103861958, 0.047152053914032876, 0.047188278986141086, 0.046896621002815664, 0.0477995719993487, 0.047150520025752485, 0.05142352601978928, 0.051110928994603455, 0.05145676003303379, 0.05147393501829356, 0.05122588307131082, 0.05111315904650837, 0.05126563704106957, 0.05031250894535333, 0.05021487397607416, 0.05054542399011552, 0.050449160975404084, 0.05007476406171918, 0.05014289903920144, 0.04973494296427816, 0.05026197899132967, 0.05060527706518769, 0.05076677002944052, 0.05236837605480105, 0.05187987699173391, 0.0500694460934028, 0.05112693505361676, 0.050846106954850256, 0.05170604703016579, 0.04681206296663731, 0.04720892000477761, 0.047394069959409535, 0.04654827399645001, 0.046930355951189995, 0.04731699300464243, 0.054005624959245324]
[0.0011421504552179779, 0.0011341311143372548, 0.001130188931711018, 0.001126465840074657, 0.0011418046593793076, 0.0013470532959962095, 0.0011471604547378693, 0.0011278296587988734, 0.001214877887501974, 0.001153225068595599, 0.001073654590238055, 0.0010823262484998188, 0.0010784304997121746, 0.0010702380677685142, 0.0010664500692986292, 0.0010718302721877328, 0.0010770234326958996, 0.0010716375889552926, 0.001072460886048661, 0.0010658322955185379, 0.001086353909076107, 0.0010716027278580111, 0.0011687165004497563, 0.001161612022604624, 0.001169471818932586, 0.0011698621595066718, 0.001164224615257064, 0.001161662705602463, 0.0011651281145697628, 0.0011434661123943938, 0.0011412471358198673, 0.0011487596361389892, 0.0011465718403500928, 0.0011380628195845268, 0.0011396113418000327, 0.0011303396128245038, 0.0011423177043484016, 0.0011501199332997203, 0.0011537902279418301, 0.001190190364881842, 0.001179088113448498, 0.0011379419566682454, 0.0011619757966731083, 0.0011555933398829604, 0.001175137432503768, 0.0010639105219690298, 0.0010729300001085821, 0.001077137953622944, 0.0010579153181011366, 0.0010665989988906817, 0.0010753862046509642, 0.0012274005672555756]
[875.5413924946967, 881.7322683051192, 884.8078157039441, 887.7322013899014, 875.8065504335974, 742.361124813887, 871.7176362468875, 886.6587185382137, 823.1279952392541, 867.1334219414802, 931.3982439904402, 923.9358293177045, 927.2734777687508, 934.371547897784, 937.6904074446437, 932.9835384840188, 928.4849053812117, 933.151291356689, 932.4349381956171, 938.2339080966675, 920.5103342891758, 933.1816483883582, 855.6394982146403, 860.8726326349055, 855.0868723905905, 854.8015609135505, 858.940780752344, 860.8350730183583, 858.2747145958808, 874.5340060021728, 876.2344006073707, 870.5041233525805, 872.1651490190629, 878.6861171381299, 877.492144313416, 884.6898654654685, 875.4132026434957, 869.4745400429478, 866.7086752709232, 840.2017269727071, 848.1130363321906, 878.7794440130122, 860.6031234584519, 865.3563199847457, 850.9642977412292, 939.9286682015818, 932.0272523825399, 928.3861891937879, 945.2552419742921, 937.5594774043965, 929.8984826800598, 814.7299477268165]
Elapsed: 0.04974445285132298~0.0023370003427926815
Time per graph: 0.0011305557466209767~5.3113644154379134e-05
Speed: 886.3928030799939~39.99246432490113
Total Time: 0.0544
best val loss: 0.6085994839668274 test_score: 0.5682

Testing...
Test loss: 0.6624 score: 0.5455 time: 0.04s
test Score 0.5455
Epoch Time List: [0.33400091400835663, 0.2175844869343564, 0.2175415629753843, 0.2177837701747194, 0.21986207808367908, 0.23004207713529468, 0.2276550258975476, 0.2194843019824475, 0.22553159296512604, 0.22351702605374157, 0.20814168499782681, 0.20758564793504775, 0.20640734792687, 0.20702418591827154, 0.20707415195647627, 0.20717559906188399, 0.20690349396318197, 0.2071675998158753, 0.20803135796450078, 0.20796925306785852, 0.20898377988487482, 0.20895638200454414, 0.2129916010890156, 0.22737903194501996, 0.2276928760111332, 0.22789440886117518, 0.2272513690404594, 0.2269542181165889, 0.2286695409566164, 0.2296322068432346, 0.22446757915895432, 0.2221090029925108, 0.2228707659523934, 0.2218319260282442, 0.22174544306471944, 0.22269886010326445, 0.22229193709790707, 0.2233355069765821, 0.22461244591977447, 0.22366146394051611, 0.2243869920494035, 0.22331803909037262, 0.22507333499379456, 0.2257941319840029, 0.22617686598096043, 0.20697875204496086, 0.20737928908783942, 0.20725469302851707, 0.20536676596384495, 0.20641541993245482, 0.20673725905362517, 0.21321470709517598]
Total Epoch List: [52]
Total Time List: [0.05443351995199919]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156e8ad27d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7386;  Loss pred: 0.7386; Loss self: 0.0000; time: 0.15s
Val loss: 0.7126 score: 0.4545 time: 0.04s
Test loss: 0.6989 score: 0.4884 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7686;  Loss pred: 0.7686; Loss self: 0.0000; time: 0.15s
Val loss: 0.7127 score: 0.4545 time: 0.04s
Test loss: 0.7019 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.15s
Val loss: 0.7095 score: 0.4545 time: 0.04s
Test loss: 0.6949 score: 0.4651 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 0.15s
Val loss: 0.7055 score: 0.4545 time: 0.04s
Test loss: 0.6803 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.15s
Val loss: 0.7030 score: 0.4773 time: 0.04s
Test loss: 0.6699 score: 0.4884 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.4862;  Loss pred: 0.4862; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7013 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6643 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4089;  Loss pred: 0.4089; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6639 score: 0.5116 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.3529;  Loss pred: 0.3529; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6681 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.2910;  Loss pred: 0.2910; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6724 score: 0.5116 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.2446;  Loss pred: 0.2446; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6771 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.1993;  Loss pred: 0.1993; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6823 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.1790;  Loss pred: 0.1790; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.1396;  Loss pred: 0.1396; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.04s
Test loss: 0.6917 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1253;  Loss pred: 0.1253; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.04s
Test loss: 0.6962 score: 0.4651 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.15s
Val loss: 0.6907 score: 0.5227 time: 0.04s
Test loss: 0.7000 score: 0.5116 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.0862;  Loss pred: 0.0862; Loss self: 0.0000; time: 0.15s
Val loss: 0.6872 score: 0.5227 time: 0.04s
Test loss: 0.7036 score: 0.5116 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.0753;  Loss pred: 0.0753; Loss self: 0.0000; time: 0.15s
Val loss: 0.6836 score: 0.5682 time: 0.04s
Test loss: 0.7070 score: 0.5349 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 0.0618;  Loss pred: 0.0618; Loss self: 0.0000; time: 0.15s
Val loss: 0.6800 score: 0.5909 time: 0.04s
Test loss: 0.7107 score: 0.5349 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 0.0571;  Loss pred: 0.0571; Loss self: 0.0000; time: 0.15s
Val loss: 0.6768 score: 0.6136 time: 0.04s
Test loss: 0.7137 score: 0.5581 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.0484;  Loss pred: 0.0484; Loss self: 0.0000; time: 0.15s
Val loss: 0.6739 score: 0.6136 time: 0.04s
Test loss: 0.7153 score: 0.5581 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.15s
Val loss: 0.6702 score: 0.6136 time: 0.04s
Test loss: 0.7168 score: 0.5814 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.15s
Val loss: 0.6661 score: 0.6136 time: 0.04s
Test loss: 0.7187 score: 0.5349 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.15s
Val loss: 0.6616 score: 0.5682 time: 0.04s
Test loss: 0.7207 score: 0.5349 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.15s
Val loss: 0.6570 score: 0.6136 time: 0.04s
Test loss: 0.7228 score: 0.5349 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.15s
Val loss: 0.6534 score: 0.6364 time: 0.04s
Test loss: 0.7245 score: 0.5349 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.15s
Val loss: 0.6503 score: 0.6364 time: 0.04s
Test loss: 0.7265 score: 0.5349 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.15s
Val loss: 0.6473 score: 0.6591 time: 0.04s
Test loss: 0.7273 score: 0.5116 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.15s
Val loss: 0.6457 score: 0.6364 time: 0.04s
Test loss: 0.7264 score: 0.5116 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.15s
Val loss: 0.6444 score: 0.6364 time: 0.04s
Test loss: 0.7250 score: 0.5349 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.15s
Val loss: 0.6433 score: 0.6591 time: 0.04s
Test loss: 0.7228 score: 0.5581 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 0.0139;  Loss pred: 0.0139; Loss self: 0.0000; time: 0.15s
Val loss: 0.6426 score: 0.6591 time: 0.04s
Test loss: 0.7200 score: 0.5581 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.15s
Val loss: 0.6420 score: 0.6591 time: 0.04s
Test loss: 0.7178 score: 0.5581 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.15s
Val loss: 0.6397 score: 0.6818 time: 0.05s
Test loss: 0.7157 score: 0.5581 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.15s
Val loss: 0.6362 score: 0.6818 time: 0.04s
Test loss: 0.7129 score: 0.5581 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.15s
Val loss: 0.6323 score: 0.6818 time: 0.04s
Test loss: 0.7083 score: 0.5349 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.15s
Val loss: 0.6283 score: 0.6818 time: 0.04s
Test loss: 0.7051 score: 0.5581 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.15s
Val loss: 0.6242 score: 0.6818 time: 0.07s
Test loss: 0.7041 score: 0.5814 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.15s
Val loss: 0.6200 score: 0.6818 time: 0.04s
Test loss: 0.7044 score: 0.5814 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.15s
Val loss: 0.6160 score: 0.6591 time: 0.05s
Test loss: 0.7061 score: 0.5814 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.15s
Val loss: 0.6109 score: 0.6591 time: 0.04s
Test loss: 0.7066 score: 0.5814 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.15s
Val loss: 0.6049 score: 0.6818 time: 0.04s
Test loss: 0.7045 score: 0.6047 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.15s
Val loss: 0.5991 score: 0.7045 time: 0.04s
Test loss: 0.7014 score: 0.6047 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.15s
Val loss: 0.5926 score: 0.7045 time: 0.04s
Test loss: 0.6985 score: 0.6047 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.15s
Val loss: 0.5861 score: 0.7045 time: 0.04s
Test loss: 0.6953 score: 0.6279 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.15s
Val loss: 0.5800 score: 0.7045 time: 0.04s
Test loss: 0.6924 score: 0.6279 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.15s
Val loss: 0.5741 score: 0.7045 time: 0.04s
Test loss: 0.6899 score: 0.6047 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.15s
Val loss: 0.5677 score: 0.7273 time: 0.04s
Test loss: 0.6888 score: 0.6047 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.15s
Val loss: 0.5612 score: 0.7045 time: 0.04s
Test loss: 0.6873 score: 0.6047 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.15s
Val loss: 0.5547 score: 0.7273 time: 0.04s
Test loss: 0.6847 score: 0.6279 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.15s
Val loss: 0.5494 score: 0.7273 time: 0.04s
Test loss: 0.6837 score: 0.6279 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.15s
Val loss: 0.5458 score: 0.7273 time: 0.04s
Test loss: 0.6842 score: 0.6279 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.15s
Val loss: 0.5432 score: 0.7500 time: 0.04s
Test loss: 0.6851 score: 0.6279 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.15s
Val loss: 0.5405 score: 0.7500 time: 0.04s
Test loss: 0.6862 score: 0.6279 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.15s
Val loss: 0.5369 score: 0.7500 time: 0.06s
Test loss: 0.6877 score: 0.6279 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.15s
Val loss: 0.5324 score: 0.7500 time: 0.04s
Test loss: 0.6892 score: 0.6279 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.14s
Val loss: 0.5255 score: 0.7500 time: 0.04s
Test loss: 0.6889 score: 0.6279 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.14s
Val loss: 0.5174 score: 0.7500 time: 0.04s
Test loss: 0.6875 score: 0.6279 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.15s
Val loss: 0.5099 score: 0.7500 time: 0.04s
Test loss: 0.6841 score: 0.6279 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.15s
Val loss: 0.5027 score: 0.7500 time: 0.04s
Test loss: 0.6787 score: 0.6279 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.14s
Val loss: 0.4976 score: 0.7727 time: 0.04s
Test loss: 0.6734 score: 0.6279 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.14s
Val loss: 0.4944 score: 0.7727 time: 0.04s
Test loss: 0.6694 score: 0.6512 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.14s
Val loss: 0.4921 score: 0.7727 time: 0.04s
Test loss: 0.6671 score: 0.6512 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.14s
Val loss: 0.4889 score: 0.7727 time: 0.04s
Test loss: 0.6643 score: 0.6512 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.14s
Val loss: 0.4834 score: 0.7955 time: 0.04s
Test loss: 0.6607 score: 0.6744 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.14s
Val loss: 0.4776 score: 0.8182 time: 0.04s
Test loss: 0.6583 score: 0.6744 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.14s
Val loss: 0.4715 score: 0.8182 time: 0.04s
Test loss: 0.6565 score: 0.6744 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.14s
Val loss: 0.4633 score: 0.8409 time: 0.04s
Test loss: 0.6548 score: 0.6744 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.14s
Val loss: 0.4549 score: 0.8409 time: 0.04s
Test loss: 0.6530 score: 0.6744 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.14s
Val loss: 0.4462 score: 0.8182 time: 0.04s
Test loss: 0.6507 score: 0.6744 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.15s
Val loss: 0.4378 score: 0.8182 time: 0.04s
Test loss: 0.6466 score: 0.6744 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.15s
Val loss: 0.4300 score: 0.8182 time: 0.04s
Test loss: 0.6437 score: 0.6512 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.16s
Val loss: 0.4223 score: 0.8182 time: 0.04s
Test loss: 0.6404 score: 0.6512 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.16s
Val loss: 0.4137 score: 0.8182 time: 0.04s
Test loss: 0.6366 score: 0.6512 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.16s
Val loss: 0.4048 score: 0.8182 time: 0.04s
Test loss: 0.6341 score: 0.6512 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.15s
Val loss: 0.3974 score: 0.8182 time: 0.04s
Test loss: 0.6332 score: 0.6512 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.15s
Val loss: 0.3910 score: 0.8182 time: 0.04s
Test loss: 0.6344 score: 0.6977 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.15s
Val loss: 0.3829 score: 0.8409 time: 0.04s
Test loss: 0.6373 score: 0.6977 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.15s
Val loss: 0.3742 score: 0.8409 time: 0.04s
Test loss: 0.6433 score: 0.6744 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.16s
Val loss: 0.3676 score: 0.8409 time: 0.04s
Test loss: 0.6496 score: 0.6744 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.16s
Val loss: 0.3612 score: 0.8409 time: 0.04s
Test loss: 0.6548 score: 0.6744 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.15s
Val loss: 0.3566 score: 0.8409 time: 0.05s
Test loss: 0.6600 score: 0.6744 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.15s
Val loss: 0.3529 score: 0.8409 time: 0.04s
Test loss: 0.6666 score: 0.7209 time: 0.05s
Epoch 83/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.17s
Val loss: 0.3496 score: 0.8409 time: 0.04s
Test loss: 0.6726 score: 0.7209 time: 0.06s
Epoch 84/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.17s
Val loss: 0.3465 score: 0.8409 time: 0.05s
Test loss: 0.6741 score: 0.7209 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.15s
Val loss: 0.3431 score: 0.8409 time: 0.04s
Test loss: 0.6770 score: 0.7442 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.16s
Val loss: 0.3405 score: 0.8636 time: 0.12s
Test loss: 0.6806 score: 0.7674 time: 0.04s
Epoch 87/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.15s
Val loss: 0.3387 score: 0.8636 time: 0.04s
Test loss: 0.6850 score: 0.7674 time: 0.04s
Epoch 88/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.15s
Val loss: 0.3375 score: 0.8636 time: 0.04s
Test loss: 0.6909 score: 0.7674 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.15s
Val loss: 0.3379 score: 0.8864 time: 0.04s
Test loss: 0.6991 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.15s
Val loss: 0.3392 score: 0.8636 time: 0.04s
Test loss: 0.7106 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.15s
Val loss: 0.3416 score: 0.8636 time: 0.04s
Test loss: 0.7279 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.15s
Val loss: 0.3462 score: 0.8636 time: 0.04s
Test loss: 0.7525 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.16s
Val loss: 0.3516 score: 0.8636 time: 0.12s
Test loss: 0.7763 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.14s
Val loss: 0.3574 score: 0.8864 time: 0.04s
Test loss: 0.7939 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.13s
Val loss: 0.3643 score: 0.8864 time: 0.05s
Test loss: 0.8146 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.14s
Val loss: 0.3712 score: 0.8636 time: 0.05s
Test loss: 0.8347 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.14s
Val loss: 0.3799 score: 0.8636 time: 0.05s
Test loss: 0.8608 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.14s
Val loss: 0.3893 score: 0.8636 time: 0.05s
Test loss: 0.8887 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.14s
Val loss: 0.4000 score: 0.8636 time: 0.04s
Test loss: 0.9191 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.15s
Val loss: 0.4169 score: 0.8864 time: 0.14s
Test loss: 0.9629 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.15s
Val loss: 0.4349 score: 0.8636 time: 0.05s
Test loss: 1.0038 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.15s
Val loss: 0.4517 score: 0.8636 time: 0.05s
Test loss: 1.0382 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.15s
Val loss: 0.4655 score: 0.8636 time: 0.05s
Test loss: 1.0651 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.16s
Val loss: 0.4747 score: 0.8636 time: 0.04s
Test loss: 1.0827 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.16s
Val loss: 0.4777 score: 0.8864 time: 0.05s
Test loss: 1.0869 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.16s
Val loss: 0.4813 score: 0.8864 time: 0.05s
Test loss: 1.0900 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.16s
Val loss: 0.4881 score: 0.8864 time: 0.05s
Test loss: 1.0981 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.16s
Val loss: 0.4973 score: 0.8864 time: 0.05s
Test loss: 1.1128 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.0013,   Val_Loss: 0.3375,   Val_Precision: 0.8636,   Val_Recall: 0.8636,   Val_accuracy: 0.8636,   Val_Score: 0.8636,   Val_Loss: 0.3375,   Test_Precision: 0.8750,   Test_Recall: 0.6364,   Test_accuracy: 0.7368,   Test_Score: 0.7674,   Test_loss: 0.6909


[0.050254620029591024, 0.049901769030839205, 0.049728312995284796, 0.04956449696328491, 0.05023940501268953, 0.059270345023833215, 0.050475060008466244, 0.04962450498715043, 0.053454627050086856, 0.05074190301820636, 0.04724080197047442, 0.04762235493399203, 0.04745094198733568, 0.04709047498181462, 0.04692380304913968, 0.047160531976260245, 0.04738903103861958, 0.047152053914032876, 0.047188278986141086, 0.046896621002815664, 0.0477995719993487, 0.047150520025752485, 0.05142352601978928, 0.051110928994603455, 0.05145676003303379, 0.05147393501829356, 0.05122588307131082, 0.05111315904650837, 0.05126563704106957, 0.05031250894535333, 0.05021487397607416, 0.05054542399011552, 0.050449160975404084, 0.05007476406171918, 0.05014289903920144, 0.04973494296427816, 0.05026197899132967, 0.05060527706518769, 0.05076677002944052, 0.05236837605480105, 0.05187987699173391, 0.0500694460934028, 0.05112693505361676, 0.050846106954850256, 0.05170604703016579, 0.04681206296663731, 0.04720892000477761, 0.047394069959409535, 0.04654827399645001, 0.046930355951189995, 0.04731699300464243, 0.054005624959245324, 0.043736454914323986, 0.04374476906377822, 0.052174384938552976, 0.04359889100305736, 0.04360083304345608, 0.04405400506220758, 0.044084839057177305, 0.04361324699129909, 0.04411290097050369, 0.04378838394768536, 0.044331828015856445, 0.043593921000137925, 0.043846601038239896, 0.043803154956549406, 0.04487714101560414, 0.04864808497950435, 0.04445979301817715, 0.04484008299186826, 0.053725956939160824, 0.05340338300447911, 0.04409461689647287, 0.0443103089928627, 0.04407455900218338, 0.04350926203187555, 0.05298479995690286, 0.04382390796672553, 0.04369012906681746, 0.04424480104353279, 0.04368880798574537, 0.04438119393307716, 0.04353541892487556, 0.04382630798500031, 0.05316124495584518, 0.04374263691715896, 0.0438473760150373, 0.04463361599482596, 0.0451551740989089, 0.044003815040923655, 0.052838349947705865, 0.04388562496751547, 0.04434161097742617, 0.04394391900859773, 0.04406682401895523, 0.04403962998185307, 0.04389169893693179, 0.04416217003017664, 0.044260826078243554, 0.04422647994942963, 0.04421739303506911, 0.04398024594411254, 0.04461695207282901, 0.0445683830184862, 0.04448549204971641, 0.04787752602715045, 0.04151197604369372, 0.04148066497873515, 0.04202516109216958, 0.041728331008926034, 0.041562533006072044, 0.0414263759739697, 0.04151404008734971, 0.04155736602842808, 0.04149818106088787, 0.0512294489890337, 0.04147782898508012, 0.04141035198699683, 0.042056570993736386, 0.04229494708124548, 0.04562648793216795, 0.04510218102950603, 0.045032616006210446, 0.04562673298642039, 0.054862243006937206, 0.04508557205554098, 0.0549349159700796, 0.04548680398147553, 0.04534198099281639, 0.04507282702252269, 0.044977469951845706, 0.045215356978587806, 0.04514755099080503, 0.04958600795362145, 0.05976638908032328, 0.046552216983400285, 0.044847447075881064, 0.045335044036619365, 0.04534972901456058, 0.04515201097819954, 0.04566882201470435, 0.04569228005129844, 0.046808904968202114, 0.04505160893313587, 0.050778185948729515, 0.0412588519975543, 0.04133885505143553, 0.041746301925741136, 0.041800247970968485, 0.04139587702229619, 0.04249800299294293, 0.04602652706671506, 0.04577423899900168, 0.045884749037213624, 0.046459745964966714, 0.0461688709910959, 0.04623241594526917, 0.04581981699448079, 0.09566195402294397, 0.0460699659306556]
[0.0011421504552179779, 0.0011341311143372548, 0.001130188931711018, 0.001126465840074657, 0.0011418046593793076, 0.0013470532959962095, 0.0011471604547378693, 0.0011278296587988734, 0.001214877887501974, 0.001153225068595599, 0.001073654590238055, 0.0010823262484998188, 0.0010784304997121746, 0.0010702380677685142, 0.0010664500692986292, 0.0010718302721877328, 0.0010770234326958996, 0.0010716375889552926, 0.001072460886048661, 0.0010658322955185379, 0.001086353909076107, 0.0010716027278580111, 0.0011687165004497563, 0.001161612022604624, 0.001169471818932586, 0.0011698621595066718, 0.001164224615257064, 0.001161662705602463, 0.0011651281145697628, 0.0011434661123943938, 0.0011412471358198673, 0.0011487596361389892, 0.0011465718403500928, 0.0011380628195845268, 0.0011396113418000327, 0.0011303396128245038, 0.0011423177043484016, 0.0011501199332997203, 0.0011537902279418301, 0.001190190364881842, 0.001179088113448498, 0.0011379419566682454, 0.0011619757966731083, 0.0011555933398829604, 0.001175137432503768, 0.0010639105219690298, 0.0010729300001085821, 0.001077137953622944, 0.0010579153181011366, 0.0010665989988906817, 0.0010753862046509642, 0.0012274005672555756, 0.0010171268584726507, 0.00101732021078554, 0.0012133577892686738, 0.00101392769774552, 0.0010139728614757227, 0.0010245117456327345, 0.0010252288152831932, 0.0010142615579371882, 0.0010258814179186905, 0.0010183345104112875, 0.001030972744554801, 0.0010138121162822774, 0.001019688396238137, 0.001018678022245335, 0.001043654442223352, 0.0011313508134768453, 0.0010339486748413291, 0.0010427926277178665, 0.0012494408590502518, 0.001241939139639049, 0.0010254562068947178, 0.0010304723021595976, 0.0010249897442368228, 0.0010118433030668734, 0.0012322046501605317, 0.0010191606503889658, 0.0010160495131818015, 0.0010289488614775068, 0.0010160187903661714, 0.0010321207891413292, 0.0010124516029040827, 0.0010192164647674492, 0.0012363080222289576, 0.001017270625980441, 0.0010197064189543558, 0.0010379910696471154, 0.0010501203278816024, 0.0010233445358354338, 0.0012287988359931596, 0.001020595929477104, 0.0010312002552889806, 0.00102195160485111, 0.0010248098609059355, 0.0010241774414384433, 0.001020737184579809, 0.0010270272100041079, 0.0010293215367033384, 0.0010285227895216193, 0.0010283114659318396, 0.0010227964173049428, 0.0010376035365774188, 0.0010364740236857256, 0.001034546326737591, 0.0011134308378407082, 0.0009653947917138074, 0.0009646666274124453, 0.000977329327724874, 0.0009704263025331636, 0.0009665705350249313, 0.0009634040924179, 0.0009654427927290631, 0.0009664503727541413, 0.000965073978160183, 0.0011913825346286907, 0.0009646006740716308, 0.0009630314415580658, 0.000978059790552009, 0.000983603420494081, 0.0010610811147015802, 0.0010488879309187447, 0.0010472701396793127, 0.0010610868136376836, 0.0012758661164404001, 0.0010485016757102554, 0.0012775561853506884, 0.0010578326507319892, 0.0010544646742515439, 0.001048205279593551, 0.0010459876732987373, 0.0010515199297346002, 0.0010499430462977914, 0.001153162975665615, 0.001389916025123797, 0.0010826096972883787, 0.0010429638854856061, 0.0010543033496888225, 0.0010546448608037345, 0.001050046766934873, 0.0010620656282489384, 0.0010626111639836847, 0.001088579185307026, 0.0010477118356543224, 0.001180888045319291, 0.0009595081859896349, 0.0009613687221264077, 0.0009708442308311892, 0.0009720987900225229, 0.0009626948144720044, 0.0009883256509986727, 0.0010703843503887223, 0.001064517186023295, 0.0010670871869119447, 0.001080459208487598, 0.0010736946742115326, 0.001075172463843469, 0.00106557713940653, 0.0022246966051847434, 0.0010713945565268743]
[875.5413924946967, 881.7322683051192, 884.8078157039441, 887.7322013899014, 875.8065504335974, 742.361124813887, 871.7176362468875, 886.6587185382137, 823.1279952392541, 867.1334219414802, 931.3982439904402, 923.9358293177045, 927.2734777687508, 934.371547897784, 937.6904074446437, 932.9835384840188, 928.4849053812117, 933.151291356689, 932.4349381956171, 938.2339080966675, 920.5103342891758, 933.1816483883582, 855.6394982146403, 860.8726326349055, 855.0868723905905, 854.8015609135505, 858.940780752344, 860.8350730183583, 858.2747145958808, 874.5340060021728, 876.2344006073707, 870.5041233525805, 872.1651490190629, 878.6861171381299, 877.492144313416, 884.6898654654685, 875.4132026434957, 869.4745400429478, 866.7086752709232, 840.2017269727071, 848.1130363321906, 878.7794440130122, 860.6031234584519, 865.3563199847457, 850.9642977412292, 939.9286682015818, 932.0272523825399, 928.3861891937879, 945.2552419742921, 937.5594774043965, 929.8984826800598, 814.7299477268165, 983.1615315926579, 982.974671492896, 824.1592124304317, 986.2636184251713, 986.2196889023372, 976.0747051098023, 975.3920150242516, 985.9389741969582, 974.771530640258, 981.9955916019359, 969.9577464890443, 986.3760591726528, 980.6917521953059, 981.6644495734132, 958.1715552033174, 883.8991302148089, 967.165996081441, 958.9634347420375, 800.3580103504368, 805.1924350259505, 975.1757249860486, 970.4288003707272, 975.6195177782687, 988.2953190173058, 811.5535027965688, 981.1995779255674, 984.204004850569, 971.8655974448158, 984.2337656369541, 968.8788468566241, 987.7015327267328, 981.1458454295736, 808.8599135651369, 983.0225846108594, 980.6744190405672, 963.3994253341398, 952.2718239511565, 977.1879997225217, 813.8028542253329, 979.8197025068911, 969.7437475127106, 978.5199174335577, 975.7907668023378, 976.3933079755335, 979.6841097854727, 973.684037052923, 971.5137246644541, 972.2681988068678, 972.4680052009492, 977.7116766159476, 963.7592440158254, 964.8095149012773, 966.6072694428951, 898.124936021454, 1035.845654630848, 1036.6275473655915, 1023.1965537429446, 1030.4749545531056, 1034.5856445688223, 1037.9860412366047, 1035.794153243666, 1034.714278344423, 1036.1899943736955, 839.3609700780636, 1036.6984254520025, 1038.3876962335971, 1022.432380576251, 1016.6699089940972, 942.4350185341309, 953.3907012583101, 954.8634703804432, 942.4299568587966, 783.7812973589641, 953.7419187457185, 782.7444393183386, 945.329111658663, 948.3485074640336, 954.0116038985772, 956.0342110402643, 951.0043240477586, 952.4326138699658, 867.1801133944591, 719.4679260647649, 923.6939245091819, 958.8059700978025, 948.4936193128381, 948.1864816919601, 952.3385352816604, 941.5614001638788, 941.0780103712086, 918.628624814241, 954.4609175627712, 846.8203264176647, 1042.200592555239, 1040.1836225627828, 1030.0313564657527, 1028.702031381842, 1038.7507909746619, 1011.8122493224078, 934.2438532820837, 939.3930066415263, 937.1305477801781, 925.5323959890878, 931.3634723338362, 930.0833434900793, 938.4585714338307, 449.4994947488392, 933.3629650328697]
Elapsed: 0.046961705576541134~0.00536279906547824
Time per graph: 0.0010835877897416122~0.00012065108839166871
Speed: 930.8325787821155~75.6349705138591
Total Time: 0.0467
best val loss: 0.3375363051891327 test_score: 0.7674

Testing...
Test loss: 0.6991 score: 0.7674 time: 0.04s
test Score 0.7674
Epoch Time List: [0.33400091400835663, 0.2175844869343564, 0.2175415629753843, 0.2177837701747194, 0.21986207808367908, 0.23004207713529468, 0.2276550258975476, 0.2194843019824475, 0.22553159296512604, 0.22351702605374157, 0.20814168499782681, 0.20758564793504775, 0.20640734792687, 0.20702418591827154, 0.20707415195647627, 0.20717559906188399, 0.20690349396318197, 0.2071675998158753, 0.20803135796450078, 0.20796925306785852, 0.20898377988487482, 0.20895638200454414, 0.2129916010890156, 0.22737903194501996, 0.2276928760111332, 0.22789440886117518, 0.2272513690404594, 0.2269542181165889, 0.2286695409566164, 0.2296322068432346, 0.22446757915895432, 0.2221090029925108, 0.2228707659523934, 0.2218319260282442, 0.22174544306471944, 0.22269886010326445, 0.22229193709790707, 0.2233355069765821, 0.22461244591977447, 0.22366146394051611, 0.2243869920494035, 0.22331803909037262, 0.22507333499379456, 0.2257941319840029, 0.22617686598096043, 0.20697875204496086, 0.20737928908783942, 0.20725469302851707, 0.20536676596384495, 0.20641541993245482, 0.20673725905362517, 0.21321470709517598, 0.2302238050615415, 0.22617497097235173, 0.2359006949700415, 0.22536377096548676, 0.2262525340775028, 0.22743978793732822, 0.22722067590802908, 0.22637442511040717, 0.22676172200590372, 0.22856041206978261, 0.22676314401905984, 0.2282341248355806, 0.22671531105879694, 0.2273897900013253, 0.2287310070823878, 0.2363632870838046, 0.2350811039796099, 0.23298506799619645, 0.24244028818793595, 0.24143488588742912, 0.2329239200334996, 0.23157335992436856, 0.23105441988445818, 0.22734174702782184, 0.23662375402636826, 0.22831206989940256, 0.22804131102748215, 0.22842964006122202, 0.2266191201051697, 0.2283192010363564, 0.2265421551419422, 0.2286931350827217, 0.24528197688050568, 0.22840711404569447, 0.22936648596078157, 0.2301264419220388, 0.2603375620674342, 0.22948237508535385, 0.24610781704541296, 0.22867795289494097, 0.22773774107918143, 0.2283632050966844, 0.22730543091893196, 0.22841643798165023, 0.2282014508964494, 0.22697117109782994, 0.22983082209248096, 0.23072522191796452, 0.22946483793202788, 0.2292210179148242, 0.23219848005101085, 0.23080493696033955, 0.23130967188626528, 0.2538482059026137, 0.22026140696834773, 0.21719384717289358, 0.21919157297816128, 0.2239738218486309, 0.2207043890375644, 0.21783505205530673, 0.21710747794713825, 0.21764143893960863, 0.2178534580161795, 0.22592585301026702, 0.2184116969583556, 0.21657122601754963, 0.21774883090984076, 0.217411695048213, 0.22099014115519822, 0.23412559705320746, 0.2342178849503398, 0.24536418716888875, 0.24756121798418462, 0.236585081089288, 0.2444703959627077, 0.234201070968993, 0.23495269194245338, 0.23375335801392794, 0.23678811604622751, 0.23853173514362425, 0.24526267894543707, 0.23914972390048206, 0.2622597098816186, 0.26261673402041197, 0.23154001811053604, 0.3172417060704902, 0.23388910805806518, 0.22892234311439097, 0.23539021401666105, 0.23614056792575866, 0.22852431505452842, 0.23462205310352147, 0.3261468099663034, 0.2152346670627594, 0.21389295905828476, 0.22456086799502373, 0.22530293394811451, 0.22800330095924437, 0.21765952405985445, 0.33286676078569144, 0.24578854092396796, 0.24547686893492937, 0.24684059107676148, 0.23929520009551197, 0.24845604819711298, 0.2467084019444883, 0.3032047109445557, 0.2475157561711967]
Total Epoch List: [52, 108]
Total Time List: [0.05443351995199919, 0.04674800799693912]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156ebfdbb80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7827;  Loss pred: 0.7827; Loss self: 0.0000; time: 0.16s
Val loss: 0.7415 score: 0.4318 time: 0.05s
Test loss: 0.7192 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7734;  Loss pred: 0.7734; Loss self: 0.0000; time: 0.15s
Val loss: 0.7304 score: 0.4318 time: 0.05s
Test loss: 0.7048 score: 0.4884 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7287;  Loss pred: 0.7287; Loss self: 0.0000; time: 0.15s
Val loss: 0.7231 score: 0.4318 time: 0.05s
Test loss: 0.7035 score: 0.4651 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.15s
Val loss: 0.7275 score: 0.3636 time: 0.05s
Test loss: 0.7001 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.14s
Val loss: 0.7337 score: 0.3182 time: 0.05s
Test loss: 0.6955 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5209;  Loss pred: 0.5209; Loss self: 0.0000; time: 0.15s
Val loss: 0.7236 score: 0.3864 time: 0.05s
Test loss: 0.6899 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4720;  Loss pred: 0.4720; Loss self: 0.0000; time: 0.14s
Val loss: 0.7063 score: 0.4545 time: 0.05s
Test loss: 0.6822 score: 0.4884 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.4258;  Loss pred: 0.4258; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6793 score: 0.4884 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3896;  Loss pred: 0.3896; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6674 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.3533;  Loss pred: 0.3533; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6602 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4884 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.3233;  Loss pred: 0.3233; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6607 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2981;  Loss pred: 0.2981; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6639 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7053 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2702;  Loss pred: 0.2702; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6710 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7166 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.2436;  Loss pred: 0.2436; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6790 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7254 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2312;  Loss pred: 0.2312; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6809 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7309 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2089;  Loss pred: 0.2089; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6810 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7350 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1824;  Loss pred: 0.1824; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7371 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1681;  Loss pred: 0.1681; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7366 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1469;  Loss pred: 0.1469; Loss self: 0.0000; time: 0.14s
Val loss: 0.6863 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7364 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1292;  Loss pred: 0.1292; Loss self: 0.0000; time: 0.14s
Val loss: 0.6896 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7375 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 0.14s
Val loss: 0.6912 score: 0.5455 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7382 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1123;  Loss pred: 0.1123; Loss self: 0.0000; time: 0.14s
Val loss: 0.6923 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7358 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1025;  Loss pred: 0.1025; Loss self: 0.0000; time: 0.14s
Val loss: 0.6912 score: 0.5227 time: 0.05s
Test loss: 0.7284 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0930;  Loss pred: 0.0930; Loss self: 0.0000; time: 0.14s
Val loss: 0.6908 score: 0.5000 time: 0.05s
Test loss: 0.7212 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.14s
Val loss: 0.6919 score: 0.5000 time: 0.05s
Test loss: 0.7150 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0833;  Loss pred: 0.0833; Loss self: 0.0000; time: 0.14s
Val loss: 0.6914 score: 0.5000 time: 0.05s
Test loss: 0.7102 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0728;  Loss pred: 0.0728; Loss self: 0.0000; time: 0.15s
Val loss: 0.6896 score: 0.5455 time: 0.05s
Test loss: 0.7056 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.14s
Val loss: 0.6884 score: 0.5227 time: 0.05s
Test loss: 0.7026 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0583;  Loss pred: 0.0583; Loss self: 0.0000; time: 0.14s
Val loss: 0.6878 score: 0.5227 time: 0.05s
Test loss: 0.6988 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 0.14s
Val loss: 0.6878 score: 0.5455 time: 0.05s
Test loss: 0.6944 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 009,   Train_Loss: 0.3533,   Val_Loss: 0.6602,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6602,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6881


[0.050254620029591024, 0.049901769030839205, 0.049728312995284796, 0.04956449696328491, 0.05023940501268953, 0.059270345023833215, 0.050475060008466244, 0.04962450498715043, 0.053454627050086856, 0.05074190301820636, 0.04724080197047442, 0.04762235493399203, 0.04745094198733568, 0.04709047498181462, 0.04692380304913968, 0.047160531976260245, 0.04738903103861958, 0.047152053914032876, 0.047188278986141086, 0.046896621002815664, 0.0477995719993487, 0.047150520025752485, 0.05142352601978928, 0.051110928994603455, 0.05145676003303379, 0.05147393501829356, 0.05122588307131082, 0.05111315904650837, 0.05126563704106957, 0.05031250894535333, 0.05021487397607416, 0.05054542399011552, 0.050449160975404084, 0.05007476406171918, 0.05014289903920144, 0.04973494296427816, 0.05026197899132967, 0.05060527706518769, 0.05076677002944052, 0.05236837605480105, 0.05187987699173391, 0.0500694460934028, 0.05112693505361676, 0.050846106954850256, 0.05170604703016579, 0.04681206296663731, 0.04720892000477761, 0.047394069959409535, 0.04654827399645001, 0.046930355951189995, 0.04731699300464243, 0.054005624959245324, 0.043736454914323986, 0.04374476906377822, 0.052174384938552976, 0.04359889100305736, 0.04360083304345608, 0.04405400506220758, 0.044084839057177305, 0.04361324699129909, 0.04411290097050369, 0.04378838394768536, 0.044331828015856445, 0.043593921000137925, 0.043846601038239896, 0.043803154956549406, 0.04487714101560414, 0.04864808497950435, 0.04445979301817715, 0.04484008299186826, 0.053725956939160824, 0.05340338300447911, 0.04409461689647287, 0.0443103089928627, 0.04407455900218338, 0.04350926203187555, 0.05298479995690286, 0.04382390796672553, 0.04369012906681746, 0.04424480104353279, 0.04368880798574537, 0.04438119393307716, 0.04353541892487556, 0.04382630798500031, 0.05316124495584518, 0.04374263691715896, 0.0438473760150373, 0.04463361599482596, 0.0451551740989089, 0.044003815040923655, 0.052838349947705865, 0.04388562496751547, 0.04434161097742617, 0.04394391900859773, 0.04406682401895523, 0.04403962998185307, 0.04389169893693179, 0.04416217003017664, 0.044260826078243554, 0.04422647994942963, 0.04421739303506911, 0.04398024594411254, 0.04461695207282901, 0.0445683830184862, 0.04448549204971641, 0.04787752602715045, 0.04151197604369372, 0.04148066497873515, 0.04202516109216958, 0.041728331008926034, 0.041562533006072044, 0.0414263759739697, 0.04151404008734971, 0.04155736602842808, 0.04149818106088787, 0.0512294489890337, 0.04147782898508012, 0.04141035198699683, 0.042056570993736386, 0.04229494708124548, 0.04562648793216795, 0.04510218102950603, 0.045032616006210446, 0.04562673298642039, 0.054862243006937206, 0.04508557205554098, 0.0549349159700796, 0.04548680398147553, 0.04534198099281639, 0.04507282702252269, 0.044977469951845706, 0.045215356978587806, 0.04514755099080503, 0.04958600795362145, 0.05976638908032328, 0.046552216983400285, 0.044847447075881064, 0.045335044036619365, 0.04534972901456058, 0.04515201097819954, 0.04566882201470435, 0.04569228005129844, 0.046808904968202114, 0.04505160893313587, 0.050778185948729515, 0.0412588519975543, 0.04133885505143553, 0.041746301925741136, 0.041800247970968485, 0.04139587702229619, 0.04249800299294293, 0.04602652706671506, 0.04577423899900168, 0.045884749037213624, 0.046459745964966714, 0.0461688709910959, 0.04623241594526917, 0.04581981699448079, 0.09566195402294397, 0.0460699659306556, 0.0527486870996654, 0.05285371502395719, 0.05241515499074012, 0.04895639605820179, 0.049788046977482736, 0.0493700560182333, 0.04929204098880291, 0.049334065057337284, 0.048670100048184395, 0.04915307799819857, 0.04916609497740865, 0.0490249409340322, 0.048935031052678823, 0.049749102094210684, 0.04922999895643443, 0.049258423037827015, 0.0493214710149914, 0.04937814502045512, 0.04980615305248648, 0.04998265497852117, 0.04993407300207764, 0.050165829015895724, 0.049944303929805756, 0.049803596921265125, 0.04993785603437573, 0.04991546494420618, 0.049845103989355266, 0.050123722990974784, 0.049935069982893765, 0.04991503502242267]
[0.0011421504552179779, 0.0011341311143372548, 0.001130188931711018, 0.001126465840074657, 0.0011418046593793076, 0.0013470532959962095, 0.0011471604547378693, 0.0011278296587988734, 0.001214877887501974, 0.001153225068595599, 0.001073654590238055, 0.0010823262484998188, 0.0010784304997121746, 0.0010702380677685142, 0.0010664500692986292, 0.0010718302721877328, 0.0010770234326958996, 0.0010716375889552926, 0.001072460886048661, 0.0010658322955185379, 0.001086353909076107, 0.0010716027278580111, 0.0011687165004497563, 0.001161612022604624, 0.001169471818932586, 0.0011698621595066718, 0.001164224615257064, 0.001161662705602463, 0.0011651281145697628, 0.0011434661123943938, 0.0011412471358198673, 0.0011487596361389892, 0.0011465718403500928, 0.0011380628195845268, 0.0011396113418000327, 0.0011303396128245038, 0.0011423177043484016, 0.0011501199332997203, 0.0011537902279418301, 0.001190190364881842, 0.001179088113448498, 0.0011379419566682454, 0.0011619757966731083, 0.0011555933398829604, 0.001175137432503768, 0.0010639105219690298, 0.0010729300001085821, 0.001077137953622944, 0.0010579153181011366, 0.0010665989988906817, 0.0010753862046509642, 0.0012274005672555756, 0.0010171268584726507, 0.00101732021078554, 0.0012133577892686738, 0.00101392769774552, 0.0010139728614757227, 0.0010245117456327345, 0.0010252288152831932, 0.0010142615579371882, 0.0010258814179186905, 0.0010183345104112875, 0.001030972744554801, 0.0010138121162822774, 0.001019688396238137, 0.001018678022245335, 0.001043654442223352, 0.0011313508134768453, 0.0010339486748413291, 0.0010427926277178665, 0.0012494408590502518, 0.001241939139639049, 0.0010254562068947178, 0.0010304723021595976, 0.0010249897442368228, 0.0010118433030668734, 0.0012322046501605317, 0.0010191606503889658, 0.0010160495131818015, 0.0010289488614775068, 0.0010160187903661714, 0.0010321207891413292, 0.0010124516029040827, 0.0010192164647674492, 0.0012363080222289576, 0.001017270625980441, 0.0010197064189543558, 0.0010379910696471154, 0.0010501203278816024, 0.0010233445358354338, 0.0012287988359931596, 0.001020595929477104, 0.0010312002552889806, 0.00102195160485111, 0.0010248098609059355, 0.0010241774414384433, 0.001020737184579809, 0.0010270272100041079, 0.0010293215367033384, 0.0010285227895216193, 0.0010283114659318396, 0.0010227964173049428, 0.0010376035365774188, 0.0010364740236857256, 0.001034546326737591, 0.0011134308378407082, 0.0009653947917138074, 0.0009646666274124453, 0.000977329327724874, 0.0009704263025331636, 0.0009665705350249313, 0.0009634040924179, 0.0009654427927290631, 0.0009664503727541413, 0.000965073978160183, 0.0011913825346286907, 0.0009646006740716308, 0.0009630314415580658, 0.000978059790552009, 0.000983603420494081, 0.0010610811147015802, 0.0010488879309187447, 0.0010472701396793127, 0.0010610868136376836, 0.0012758661164404001, 0.0010485016757102554, 0.0012775561853506884, 0.0010578326507319892, 0.0010544646742515439, 0.001048205279593551, 0.0010459876732987373, 0.0010515199297346002, 0.0010499430462977914, 0.001153162975665615, 0.001389916025123797, 0.0010826096972883787, 0.0010429638854856061, 0.0010543033496888225, 0.0010546448608037345, 0.001050046766934873, 0.0010620656282489384, 0.0010626111639836847, 0.001088579185307026, 0.0010477118356543224, 0.001180888045319291, 0.0009595081859896349, 0.0009613687221264077, 0.0009708442308311892, 0.0009720987900225229, 0.0009626948144720044, 0.0009883256509986727, 0.0010703843503887223, 0.001064517186023295, 0.0010670871869119447, 0.001080459208487598, 0.0010736946742115326, 0.001075172463843469, 0.00106557713940653, 0.0022246966051847434, 0.0010713945565268743, 0.0012267136534805908, 0.0012291561633478417, 0.0012189570928079097, 0.0011385208385628323, 0.0011578615576158776, 0.0011481408376333325, 0.0011463265346233235, 0.0011473038385427276, 0.0011318627918182418, 0.0011430948371674086, 0.0011433975576141545, 0.0011401149054426094, 0.001138023977969275, 0.0011569558626560623, 0.0011448836966612658, 0.0011455447218099305, 0.0011470109538370093, 0.0011483289539640726, 0.0011582826291275925, 0.0011623873250818876, 0.0011612575116762241, 0.0011666471864161796, 0.0011614954402280409, 0.001158223184215468, 0.0011613454891715286, 0.0011608247661443298, 0.001159188464868727, 0.0011656679765342972, 0.001161280697276599, 0.001160814767963318]
[875.5413924946967, 881.7322683051192, 884.8078157039441, 887.7322013899014, 875.8065504335974, 742.361124813887, 871.7176362468875, 886.6587185382137, 823.1279952392541, 867.1334219414802, 931.3982439904402, 923.9358293177045, 927.2734777687508, 934.371547897784, 937.6904074446437, 932.9835384840188, 928.4849053812117, 933.151291356689, 932.4349381956171, 938.2339080966675, 920.5103342891758, 933.1816483883582, 855.6394982146403, 860.8726326349055, 855.0868723905905, 854.8015609135505, 858.940780752344, 860.8350730183583, 858.2747145958808, 874.5340060021728, 876.2344006073707, 870.5041233525805, 872.1651490190629, 878.6861171381299, 877.492144313416, 884.6898654654685, 875.4132026434957, 869.4745400429478, 866.7086752709232, 840.2017269727071, 848.1130363321906, 878.7794440130122, 860.6031234584519, 865.3563199847457, 850.9642977412292, 939.9286682015818, 932.0272523825399, 928.3861891937879, 945.2552419742921, 937.5594774043965, 929.8984826800598, 814.7299477268165, 983.1615315926579, 982.974671492896, 824.1592124304317, 986.2636184251713, 986.2196889023372, 976.0747051098023, 975.3920150242516, 985.9389741969582, 974.771530640258, 981.9955916019359, 969.9577464890443, 986.3760591726528, 980.6917521953059, 981.6644495734132, 958.1715552033174, 883.8991302148089, 967.165996081441, 958.9634347420375, 800.3580103504368, 805.1924350259505, 975.1757249860486, 970.4288003707272, 975.6195177782687, 988.2953190173058, 811.5535027965688, 981.1995779255674, 984.204004850569, 971.8655974448158, 984.2337656369541, 968.8788468566241, 987.7015327267328, 981.1458454295736, 808.8599135651369, 983.0225846108594, 980.6744190405672, 963.3994253341398, 952.2718239511565, 977.1879997225217, 813.8028542253329, 979.8197025068911, 969.7437475127106, 978.5199174335577, 975.7907668023378, 976.3933079755335, 979.6841097854727, 973.684037052923, 971.5137246644541, 972.2681988068678, 972.4680052009492, 977.7116766159476, 963.7592440158254, 964.8095149012773, 966.6072694428951, 898.124936021454, 1035.845654630848, 1036.6275473655915, 1023.1965537429446, 1030.4749545531056, 1034.5856445688223, 1037.9860412366047, 1035.794153243666, 1034.714278344423, 1036.1899943736955, 839.3609700780636, 1036.6984254520025, 1038.3876962335971, 1022.432380576251, 1016.6699089940972, 942.4350185341309, 953.3907012583101, 954.8634703804432, 942.4299568587966, 783.7812973589641, 953.7419187457185, 782.7444393183386, 945.329111658663, 948.3485074640336, 954.0116038985772, 956.0342110402643, 951.0043240477586, 952.4326138699658, 867.1801133944591, 719.4679260647649, 923.6939245091819, 958.8059700978025, 948.4936193128381, 948.1864816919601, 952.3385352816604, 941.5614001638788, 941.0780103712086, 918.628624814241, 954.4609175627712, 846.8203264176647, 1042.200592555239, 1040.1836225627828, 1030.0313564657527, 1028.702031381842, 1038.7507909746619, 1011.8122493224078, 934.2438532820837, 939.3930066415263, 937.1305477801781, 925.5323959890878, 931.3634723338362, 930.0833434900793, 938.4585714338307, 449.4994947488392, 933.3629650328697, 815.1861660320407, 813.5662740170532, 820.3734207710835, 878.332627852742, 863.6611116609431, 870.9732876162685, 872.3517861589014, 871.6086937093938, 883.4993138996863, 874.8180531354704, 874.5864405086085, 877.1045753601347, 878.7161073569213, 864.3372079071939, 873.4511661893879, 872.9471499113769, 871.8312555383847, 870.8306069858852, 863.3471441708407, 860.2984378976709, 861.1354414892387, 857.1571694026001, 860.9590407033077, 863.3914547975123, 861.0702063460651, 861.4564654073432, 862.6724905455693, 857.8772172957409, 861.1182484520497, 861.4638851937833]
Elapsed: 0.0474201384392616~0.005049989545659227
Time per graph: 0.0010955982135416663~0.00011452382243954481
Speed: 920.0701844813246~74.02910756854722
Total Time: 0.0506
best val loss: 0.6601649522781372 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7382 score: 0.4884 time: 0.05s
test Score 0.4884
Epoch Time List: [0.33400091400835663, 0.2175844869343564, 0.2175415629753843, 0.2177837701747194, 0.21986207808367908, 0.23004207713529468, 0.2276550258975476, 0.2194843019824475, 0.22553159296512604, 0.22351702605374157, 0.20814168499782681, 0.20758564793504775, 0.20640734792687, 0.20702418591827154, 0.20707415195647627, 0.20717559906188399, 0.20690349396318197, 0.2071675998158753, 0.20803135796450078, 0.20796925306785852, 0.20898377988487482, 0.20895638200454414, 0.2129916010890156, 0.22737903194501996, 0.2276928760111332, 0.22789440886117518, 0.2272513690404594, 0.2269542181165889, 0.2286695409566164, 0.2296322068432346, 0.22446757915895432, 0.2221090029925108, 0.2228707659523934, 0.2218319260282442, 0.22174544306471944, 0.22269886010326445, 0.22229193709790707, 0.2233355069765821, 0.22461244591977447, 0.22366146394051611, 0.2243869920494035, 0.22331803909037262, 0.22507333499379456, 0.2257941319840029, 0.22617686598096043, 0.20697875204496086, 0.20737928908783942, 0.20725469302851707, 0.20536676596384495, 0.20641541993245482, 0.20673725905362517, 0.21321470709517598, 0.2302238050615415, 0.22617497097235173, 0.2359006949700415, 0.22536377096548676, 0.2262525340775028, 0.22743978793732822, 0.22722067590802908, 0.22637442511040717, 0.22676172200590372, 0.22856041206978261, 0.22676314401905984, 0.2282341248355806, 0.22671531105879694, 0.2273897900013253, 0.2287310070823878, 0.2363632870838046, 0.2350811039796099, 0.23298506799619645, 0.24244028818793595, 0.24143488588742912, 0.2329239200334996, 0.23157335992436856, 0.23105441988445818, 0.22734174702782184, 0.23662375402636826, 0.22831206989940256, 0.22804131102748215, 0.22842964006122202, 0.2266191201051697, 0.2283192010363564, 0.2265421551419422, 0.2286931350827217, 0.24528197688050568, 0.22840711404569447, 0.22936648596078157, 0.2301264419220388, 0.2603375620674342, 0.22948237508535385, 0.24610781704541296, 0.22867795289494097, 0.22773774107918143, 0.2283632050966844, 0.22730543091893196, 0.22841643798165023, 0.2282014508964494, 0.22697117109782994, 0.22983082209248096, 0.23072522191796452, 0.22946483793202788, 0.2292210179148242, 0.23219848005101085, 0.23080493696033955, 0.23130967188626528, 0.2538482059026137, 0.22026140696834773, 0.21719384717289358, 0.21919157297816128, 0.2239738218486309, 0.2207043890375644, 0.21783505205530673, 0.21710747794713825, 0.21764143893960863, 0.2178534580161795, 0.22592585301026702, 0.2184116969583556, 0.21657122601754963, 0.21774883090984076, 0.217411695048213, 0.22099014115519822, 0.23412559705320746, 0.2342178849503398, 0.24536418716888875, 0.24756121798418462, 0.236585081089288, 0.2444703959627077, 0.234201070968993, 0.23495269194245338, 0.23375335801392794, 0.23678811604622751, 0.23853173514362425, 0.24526267894543707, 0.23914972390048206, 0.2622597098816186, 0.26261673402041197, 0.23154001811053604, 0.3172417060704902, 0.23388910805806518, 0.22892234311439097, 0.23539021401666105, 0.23614056792575866, 0.22852431505452842, 0.23462205310352147, 0.3261468099663034, 0.2152346670627594, 0.21389295905828476, 0.22456086799502373, 0.22530293394811451, 0.22800330095924437, 0.21765952405985445, 0.33286676078569144, 0.24578854092396796, 0.24547686893492937, 0.24684059107676148, 0.23929520009551197, 0.24845604819711298, 0.2467084019444883, 0.3032047109445557, 0.2475157561711967, 0.257282038917765, 0.25517578690778464, 0.25306263705715537, 0.24648129800334573, 0.2380451540229842, 0.24676609702873975, 0.2353425519540906, 0.23487905005458742, 0.23451219301205128, 0.2335385570768267, 0.23427164391614497, 0.23392004915513098, 0.23413915606215596, 0.23743736010510474, 0.23659581295214593, 0.23441567597910762, 0.2356550699332729, 0.23535744100809097, 0.2355578759452328, 0.23882383690215647, 0.23855374509003013, 0.23924323299434036, 0.23891703796107322, 0.2394033740274608, 0.23886528494767845, 0.24132751894649118, 0.2427135600009933, 0.23985486710444093, 0.23905107809696347, 0.23910264705773443]
Total Epoch List: [52, 108, 30]
Total Time List: [0.05443351995199919, 0.04674800799693912, 0.05060887394938618]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156ebfd80d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.14s
Val loss: 0.7856 score: 0.4651 time: 0.05s
Test loss: 0.8973 score: 0.4545 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.14s
Val loss: 0.7120 score: 0.3953 time: 0.05s
Test loss: 0.7581 score: 0.4091 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 0.14s
Val loss: 0.6906 score: 0.5349 time: 0.05s
Test loss: 0.7021 score: 0.4773 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.5852;  Loss pred: 0.5852; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5116 time: 0.05s
Test loss: 0.6829 score: 0.5000 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.5375;  Loss pred: 0.5375; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6726 score: 0.5000 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.4889;  Loss pred: 0.4889; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6795 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6672 score: 0.5000 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.4394;  Loss pred: 0.4394; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6628 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.3957;  Loss pred: 0.3957; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6588 score: 0.5000 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.3552;  Loss pred: 0.3552; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6756 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6559 score: 0.5000 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.3155;  Loss pred: 0.3155; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6738 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6540 score: 0.5000 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.2792;  Loss pred: 0.2792; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6713 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6517 score: 0.5000 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.2448;  Loss pred: 0.2448; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6688 score: 0.5116 time: 0.05s
Test loss: 0.6484 score: 0.5227 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.2238;  Loss pred: 0.2238; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6671 score: 0.5116 time: 0.05s
Test loss: 0.6458 score: 0.5227 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.1972;  Loss pred: 0.1972; Loss self: 0.0000; time: 0.13s
Val loss: 0.6642 score: 0.5349 time: 0.05s
Test loss: 0.6439 score: 0.5227 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.1776;  Loss pred: 0.1776; Loss self: 0.0000; time: 0.13s
Val loss: 0.6617 score: 0.5581 time: 0.05s
Test loss: 0.6434 score: 0.5455 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.1593;  Loss pred: 0.1593; Loss self: 0.0000; time: 0.13s
Val loss: 0.6600 score: 0.5581 time: 0.05s
Test loss: 0.6454 score: 0.5227 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.1471;  Loss pred: 0.1471; Loss self: 0.0000; time: 0.13s
Val loss: 0.6587 score: 0.5581 time: 0.05s
Test loss: 0.6461 score: 0.5227 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.1286;  Loss pred: 0.1286; Loss self: 0.0000; time: 0.13s
Val loss: 0.6581 score: 0.5581 time: 0.05s
Test loss: 0.6464 score: 0.5227 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.1150;  Loss pred: 0.1150; Loss self: 0.0000; time: 0.13s
Val loss: 0.6579 score: 0.5581 time: 0.05s
Test loss: 0.6466 score: 0.5227 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.1026;  Loss pred: 0.1026; Loss self: 0.0000; time: 0.13s
Val loss: 0.6588 score: 0.5349 time: 0.05s
Test loss: 0.6496 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0968;  Loss pred: 0.0968; Loss self: 0.0000; time: 0.13s
Val loss: 0.6602 score: 0.5349 time: 0.05s
Test loss: 0.6540 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0848;  Loss pred: 0.0848; Loss self: 0.0000; time: 0.13s
Val loss: 0.6603 score: 0.5349 time: 0.05s
Test loss: 0.6579 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.14s
Val loss: 0.6607 score: 0.5349 time: 0.05s
Test loss: 0.6622 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.13s
Val loss: 0.6610 score: 0.5349 time: 0.05s
Test loss: 0.6651 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6636 score: 0.5116 time: 0.05s
Test loss: 0.6676 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6669 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6677 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0441;  Loss pred: 0.0441; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6703 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6682 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6736 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6679 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6765 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6680 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6792 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6691 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6801 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6700 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6709 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6799 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6711 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6793 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6716 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6785 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6716 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6774 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6711 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.14s
Val loss: 0.6758 score: 0.5349 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6687 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.14s
Val loss: 0.6741 score: 0.5349 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6667 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.14s
Val loss: 0.6724 score: 0.5349 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6652 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 018,   Train_Loss: 0.1150,   Val_Loss: 0.6579,   Val_Precision: 0.5366,   Val_Recall: 1.0000,   Val_accuracy: 0.6984,   Val_Score: 0.5581,   Val_Loss: 0.6579,   Test_Precision: 0.5122,   Test_Recall: 0.9545,   Test_accuracy: 0.6667,   Test_Score: 0.5227,   Test_loss: 0.6466


[0.056123649002984166, 0.056504828040488064, 0.05611734604462981, 0.059435631963424385, 0.0566912479698658, 0.05746407597325742, 0.05644895904697478, 0.05992711696308106, 0.06094945000950247, 0.05649067007470876, 0.05716392502654344, 0.05736346391495317, 0.05581190204247832, 0.0559559470275417, 0.05580962297972292, 0.05622370296623558, 0.05590877798385918, 0.055651732021942735, 0.05784153298009187, 0.056331088999286294, 0.056007246021181345, 0.055912434007041156, 0.05644990096334368, 0.05626495706383139, 0.056307432940229774, 0.056075685075484216, 0.05567603500094265, 0.05625650903675705, 0.056011097971349955, 0.05590795399621129, 0.056491896975785494, 0.05664781399536878, 0.056382075999863446, 0.057250619982369244, 0.0569297110196203, 0.056195655022747815, 0.0566090940264985, 0.05643652204889804, 0.056486974004656076]
[0.0012755374773405492, 0.0012842006372838196, 0.0012753942282870412, 0.0013508098173505541, 0.0012884374538605864, 0.0013060017266649413, 0.0012829308874312449, 0.0013619799309791151, 0.001385214772943238, 0.00128387886533429, 0.0012991801142396237, 0.0013037150889762086, 0.0012684523191472347, 0.0012717260688077658, 0.00126840052226643, 0.0012778114310508086, 0.0012706540450877087, 0.0012648120914077895, 0.001314580295002088, 0.0012802520227110522, 0.0012728919550268488, 0.0012707371365236627, 0.0012829522946214472, 0.001278749024177986, 0.001279714385005222, 0.0012744473880791868, 0.0012653644318396057, 0.0012785570235626603, 0.0012729794993488626, 0.0012706353180957112, 0.0012839067494496703, 0.0012874503180765632, 0.0012814108181787146, 0.0013011504541447555, 0.0012938570686277342, 0.001277173977789723, 0.0012865703187840568, 0.0012826482283840464, 0.0012837948637421835]
[783.9832366862045, 778.6945209084109, 784.0712917001999, 740.2966629020922, 776.1339108884704, 765.6957717457543, 779.4652149986467, 734.2252093840392, 721.9097135928228, 778.8896811068114, 769.7162148954798, 767.0387559794892, 788.3623096469942, 786.3328624988343, 788.3945035067939, 782.5880843604989, 786.9962747657044, 790.6312777947574, 760.6990640297189, 781.096207825087, 785.6126327539773, 786.9448143584484, 779.4522089342876, 782.0142819994147, 781.42436446545, 784.6538110193576, 790.2861617077265, 782.1317169049921, 785.5586052340251, 787.0078737451516, 778.8727650420382, 776.7290014685686, 780.3898529757323, 768.5506290333647, 772.8828973826304, 782.9786837112041, 777.2602751671626, 779.6369868766413, 778.9406456145658]
Elapsed: 0.05667985349189108~0.0011168335240935123
Time per graph: 0.0012881784884520699~2.5382580093034357e-05
Speed: 776.5781789131166~14.633140796974951
Total Time: 0.0568
best val loss: 0.657867431640625 test_score: 0.5227

Testing...
Test loss: 0.6434 score: 0.5455 time: 0.05s
test Score 0.5455
Epoch Time List: [0.23565922200214118, 0.23609857191331685, 0.23612849693745375, 0.25938629906158894, 0.23611598904244602, 0.23974205378908664, 0.23982832196634263, 0.24852330808062106, 0.2584526720456779, 0.23566971812397242, 0.2370977341197431, 0.24397330486681312, 0.23316360812168568, 0.23309161805082113, 0.23313851689454168, 0.23484586214181036, 0.23417882912326604, 0.23372186196502298, 0.23520005901809782, 0.234176215948537, 0.23398939496837556, 0.23333884682506323, 0.2350447919452563, 0.2349197849398479, 0.23425694613251835, 0.23412002401892096, 0.23328502906952053, 0.23292306100483984, 0.23227752896491438, 0.23409773595631123, 0.23330931505188346, 0.23546543100383133, 0.2438744029495865, 0.23736636014655232, 0.23627156193833798, 0.2352258978644386, 0.23495958792045712, 0.2354052651207894, 0.23537716292776167]
Total Epoch List: [39]
Total Time List: [0.05683992896229029]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156e8ad1b40>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8779;  Loss pred: 0.8779; Loss self: 0.0000; time: 0.15s
Val loss: 0.7135 score: 0.4773 time: 0.05s
Test loss: 0.7326 score: 0.4419 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8691;  Loss pred: 0.8691; Loss self: 0.0000; time: 0.15s
Val loss: 0.7290 score: 0.5000 time: 0.05s
Test loss: 0.7562 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.8037;  Loss pred: 0.8037; Loss self: 0.0000; time: 0.15s
Val loss: 0.7276 score: 0.5227 time: 0.05s
Test loss: 0.7444 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.15s
Val loss: 0.7252 score: 0.4773 time: 0.05s
Test loss: 0.7331 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6099;  Loss pred: 0.6099; Loss self: 0.0000; time: 0.15s
Val loss: 0.7231 score: 0.5000 time: 0.05s
Test loss: 0.7216 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5509;  Loss pred: 0.5509; Loss self: 0.0000; time: 0.15s
Val loss: 0.7205 score: 0.5227 time: 0.05s
Test loss: 0.7194 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4662;  Loss pred: 0.4662; Loss self: 0.0000; time: 0.15s
Val loss: 0.7152 score: 0.5227 time: 0.05s
Test loss: 0.7123 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4078;  Loss pred: 0.4078; Loss self: 0.0000; time: 0.15s
Val loss: 0.6996 score: 0.5682 time: 0.05s
Test loss: 0.6937 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.3456;  Loss pred: 0.3456; Loss self: 0.0000; time: 0.15s
Val loss: 0.6880 score: 0.5909 time: 0.05s
Test loss: 0.6806 score: 0.6279 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.2926;  Loss pred: 0.2926; Loss self: 0.0000; time: 0.15s
Val loss: 0.6757 score: 0.7045 time: 0.05s
Test loss: 0.6662 score: 0.6744 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.2570;  Loss pred: 0.2570; Loss self: 0.0000; time: 0.15s
Val loss: 0.6656 score: 0.6818 time: 0.05s
Test loss: 0.6568 score: 0.6977 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.2239;  Loss pred: 0.2239; Loss self: 0.0000; time: 0.15s
Val loss: 0.6564 score: 0.6364 time: 0.05s
Test loss: 0.6493 score: 0.7209 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.2025;  Loss pred: 0.2025; Loss self: 0.0000; time: 0.15s
Val loss: 0.6512 score: 0.5909 time: 0.05s
Test loss: 0.6430 score: 0.6512 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.1757;  Loss pred: 0.1757; Loss self: 0.0000; time: 0.15s
Val loss: 0.6471 score: 0.5455 time: 0.05s
Test loss: 0.6390 score: 0.6047 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.1621;  Loss pred: 0.1621; Loss self: 0.0000; time: 0.15s
Val loss: 0.6430 score: 0.5455 time: 0.05s
Test loss: 0.6347 score: 0.5814 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.1470;  Loss pred: 0.1470; Loss self: 0.0000; time: 0.15s
Val loss: 0.6405 score: 0.5455 time: 0.05s
Test loss: 0.6316 score: 0.5814 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.1320;  Loss pred: 0.1320; Loss self: 0.0000; time: 0.15s
Val loss: 0.6390 score: 0.5455 time: 0.05s
Test loss: 0.6296 score: 0.6047 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.1206;  Loss pred: 0.1206; Loss self: 0.0000; time: 0.15s
Val loss: 0.6386 score: 0.5455 time: 0.05s
Test loss: 0.6289 score: 0.6047 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.1042;  Loss pred: 0.1042; Loss self: 0.0000; time: 0.15s
Val loss: 0.6397 score: 0.5455 time: 0.05s
Test loss: 0.6292 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0932;  Loss pred: 0.0932; Loss self: 0.0000; time: 0.15s
Val loss: 0.6418 score: 0.5455 time: 0.05s
Test loss: 0.6307 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0860;  Loss pred: 0.0860; Loss self: 0.0000; time: 0.15s
Val loss: 0.6441 score: 0.5455 time: 0.05s
Test loss: 0.6323 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.15s
Val loss: 0.6466 score: 0.5455 time: 0.05s
Test loss: 0.6334 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.15s
Val loss: 0.6495 score: 0.5455 time: 0.05s
Test loss: 0.6346 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0659;  Loss pred: 0.0659; Loss self: 0.0000; time: 0.15s
Val loss: 0.6527 score: 0.5455 time: 0.05s
Test loss: 0.6358 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.15s
Val loss: 0.6559 score: 0.5455 time: 0.05s
Test loss: 0.6377 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0541;  Loss pred: 0.0541; Loss self: 0.0000; time: 0.15s
Val loss: 0.6590 score: 0.5455 time: 0.05s
Test loss: 0.6404 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0516;  Loss pred: 0.0516; Loss self: 0.0000; time: 0.15s
Val loss: 0.6621 score: 0.5455 time: 0.05s
Test loss: 0.6435 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.15s
Val loss: 0.6650 score: 0.5455 time: 0.05s
Test loss: 0.6464 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.15s
Val loss: 0.6684 score: 0.5455 time: 0.06s
Test loss: 0.6489 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.15s
Val loss: 0.6720 score: 0.5455 time: 0.05s
Test loss: 0.6515 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0379;  Loss pred: 0.0379; Loss self: 0.0000; time: 0.15s
Val loss: 0.6751 score: 0.5455 time: 0.05s
Test loss: 0.6543 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0357;  Loss pred: 0.0357; Loss self: 0.0000; time: 0.15s
Val loss: 0.6770 score: 0.5455 time: 0.05s
Test loss: 0.6574 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.15s
Val loss: 0.6790 score: 0.5455 time: 0.05s
Test loss: 0.6603 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.15s
Val loss: 0.6818 score: 0.5455 time: 0.05s
Test loss: 0.6646 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.15s
Val loss: 0.6841 score: 0.5455 time: 0.05s
Test loss: 0.6684 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.15s
Val loss: 0.6868 score: 0.5455 time: 0.05s
Test loss: 0.6723 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.15s
Val loss: 0.6886 score: 0.5455 time: 0.05s
Test loss: 0.6752 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.15s
Val loss: 0.6910 score: 0.5455 time: 0.05s
Test loss: 0.6783 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 017,   Train_Loss: 0.1206,   Val_Loss: 0.6386,   Val_Precision: 0.5263,   Val_Recall: 0.9091,   Val_accuracy: 0.6667,   Val_Score: 0.5455,   Val_Loss: 0.6386,   Test_Precision: 0.5676,   Test_Recall: 0.9545,   Test_accuracy: 0.7119,   Test_Score: 0.6047,   Test_loss: 0.6289


[0.056123649002984166, 0.056504828040488064, 0.05611734604462981, 0.059435631963424385, 0.0566912479698658, 0.05746407597325742, 0.05644895904697478, 0.05992711696308106, 0.06094945000950247, 0.05649067007470876, 0.05716392502654344, 0.05736346391495317, 0.05581190204247832, 0.0559559470275417, 0.05580962297972292, 0.05622370296623558, 0.05590877798385918, 0.055651732021942735, 0.05784153298009187, 0.056331088999286294, 0.056007246021181345, 0.055912434007041156, 0.05644990096334368, 0.05626495706383139, 0.056307432940229774, 0.056075685075484216, 0.05567603500094265, 0.05625650903675705, 0.056011097971349955, 0.05590795399621129, 0.056491896975785494, 0.05664781399536878, 0.056382075999863446, 0.057250619982369244, 0.0569297110196203, 0.056195655022747815, 0.0566090940264985, 0.05643652204889804, 0.056486974004656076, 0.05235527001786977, 0.051889650989323854, 0.052337844972498715, 0.05222141300328076, 0.0519215629901737, 0.0521815400570631, 0.05188386305235326, 0.05282504309434444, 0.05668381508439779, 0.051663429010659456, 0.05163468699902296, 0.052217481075786054, 0.052228663000278175, 0.052058286033570766, 0.052769772009924054, 0.05235717201139778, 0.052032161969691515, 0.05231033801101148, 0.05283720896113664, 0.05290720006451011, 0.05276582308579236, 0.052867816062644124, 0.05261450994294137, 0.052471423987299204, 0.05323339800816029, 0.052732185926288366, 0.05289485305547714, 0.05295724200550467, 0.053480362985283136, 0.053574623074382544, 0.0528071029111743, 0.052869664039462805, 0.05259112303610891, 0.05278907099273056, 0.052734158001840115, 0.05258288700133562, 0.052646290976554155, 0.05247969401534647]
[0.0012755374773405492, 0.0012842006372838196, 0.0012753942282870412, 0.0013508098173505541, 0.0012884374538605864, 0.0013060017266649413, 0.0012829308874312449, 0.0013619799309791151, 0.001385214772943238, 0.00128387886533429, 0.0012991801142396237, 0.0013037150889762086, 0.0012684523191472347, 0.0012717260688077658, 0.00126840052226643, 0.0012778114310508086, 0.0012706540450877087, 0.0012648120914077895, 0.001314580295002088, 0.0012802520227110522, 0.0012728919550268488, 0.0012707371365236627, 0.0012829522946214472, 0.001278749024177986, 0.001279714385005222, 0.0012744473880791868, 0.0012653644318396057, 0.0012785570235626603, 0.0012729794993488626, 0.0012706353180957112, 0.0012839067494496703, 0.0012874503180765632, 0.0012814108181787146, 0.0013011504541447555, 0.0012938570686277342, 0.001277173977789723, 0.0012865703187840568, 0.0012826482283840464, 0.0012837948637421835, 0.0012175644190202271, 0.0012067360695191594, 0.001217159185406947, 0.0012144514651925757, 0.001207478209073807, 0.0012135241873735606, 0.0012066014663337967, 0.00122848937428708, 0.0013182282577766929, 0.0012014750932711502, 0.001200806674395883, 0.0012143600250182802, 0.001214620069773911, 0.001210657814734204, 0.001227204000230792, 0.0012176086514278552, 0.0012100502783649188, 0.001216519488628174, 0.0012287723014217822, 0.001230400001500235, 0.0012271121647858688, 0.001229484094480096, 0.0012235932544870085, 0.0012202656741232374, 0.0012379860001897744, 0.00122632990526252, 0.0012301128617552824, 0.001231563767569876, 0.0012437293717507706, 0.0012459214668461057, 0.0012280721607249836, 0.0012295270706851815, 0.0012230493729327653, 0.0012276528137844316, 0.0012263757674846538, 0.0012228578372403633, 0.0012243323482919572, 0.0012204580003568946]
[783.9832366862045, 778.6945209084109, 784.0712917001999, 740.2966629020922, 776.1339108884704, 765.6957717457543, 779.4652149986467, 734.2252093840392, 721.9097135928228, 778.8896811068114, 769.7162148954798, 767.0387559794892, 788.3623096469942, 786.3328624988343, 788.3945035067939, 782.5880843604989, 786.9962747657044, 790.6312777947574, 760.6990640297189, 781.096207825087, 785.6126327539773, 786.9448143584484, 779.4522089342876, 782.0142819994147, 781.42436446545, 784.6538110193576, 790.2861617077265, 782.1317169049921, 785.5586052340251, 787.0078737451516, 778.8727650420382, 776.7290014685686, 780.3898529757323, 768.5506290333647, 772.8828973826304, 782.9786837112041, 777.2602751671626, 779.6369868766413, 778.9406456145658, 821.3117797945335, 828.6816191700176, 821.5852223681477, 823.4170147272455, 828.1722953551663, 824.0462039444863, 828.7740632691705, 814.00785463067, 758.5939643613674, 832.3102206616604, 832.7735191038081, 823.4790172584499, 823.302714062793, 825.9972288037036, 814.8604468466015, 821.2819437734187, 826.4119416188644, 822.0172462075923, 813.8204277903437, 812.7438221559601, 814.9214299203855, 813.349277546257, 817.2650481137623, 819.4936735547376, 807.7635771702646, 815.4412574534176, 812.9335373122366, 811.9758199554717, 804.0334358207863, 802.618805927933, 814.2843979214194, 813.3208481881799, 817.6284801995258, 814.562544696447, 815.4107627640428, 817.7565449935792, 816.7716889903963, 819.3645334026843]
Elapsed: 0.05468731061948536~0.002240925591024043
Time per graph: 0.0012566245716251108~3.897034168162114e-05
Speed: 796.5328985382738~24.213415439981077
Total Time: 0.0529
best val loss: 0.6385979652404785 test_score: 0.6047

Testing...
Test loss: 0.6662 score: 0.6744 time: 0.05s
test Score 0.6744
Epoch Time List: [0.23565922200214118, 0.23609857191331685, 0.23612849693745375, 0.25938629906158894, 0.23611598904244602, 0.23974205378908664, 0.23982832196634263, 0.24852330808062106, 0.2584526720456779, 0.23566971812397242, 0.2370977341197431, 0.24397330486681312, 0.23316360812168568, 0.23309161805082113, 0.23313851689454168, 0.23484586214181036, 0.23417882912326604, 0.23372186196502298, 0.23520005901809782, 0.234176215948537, 0.23398939496837556, 0.23333884682506323, 0.2350447919452563, 0.2349197849398479, 0.23425694613251835, 0.23412002401892096, 0.23328502906952053, 0.23292306100483984, 0.23227752896491438, 0.23409773595631123, 0.23330931505188346, 0.23546543100383133, 0.2438744029495865, 0.23736636014655232, 0.23627156193833798, 0.2352258978644386, 0.23495958792045712, 0.2354052651207894, 0.23537716292776167, 0.24268977984320372, 0.24063222901895642, 0.24154602992348373, 0.24165221583098173, 0.24101999995764345, 0.24164402903988957, 0.2427365940529853, 0.24418378109112382, 0.25470841815695167, 0.24176442110911012, 0.2415379349840805, 0.241914507932961, 0.24224176805000752, 0.24271906807553023, 0.24620309798046947, 0.2436727179447189, 0.24329067196231335, 0.24335770087782294, 0.24456462101079524, 0.24655416596215218, 0.24625273188576102, 0.2470087178517133, 0.24647439492400736, 0.2472205669619143, 0.2466516668209806, 0.24647055997047573, 0.24661359295714647, 0.24799146002624184, 0.2574385979678482, 0.24989197205286473, 0.24740068288519979, 0.24702512694057077, 0.24687545304186642, 0.24613819585647434, 0.24669945717323571, 0.24646701384335756, 0.24588977394159883, 0.24653836619108915]
Total Epoch List: [39, 38]
Total Time List: [0.05683992896229029, 0.05294805997982621]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156ebfdbd30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8873;  Loss pred: 0.8873; Loss self: 0.0000; time: 0.15s
Val loss: 0.9265 score: 0.4773 time: 0.05s
Test loss: 0.8212 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8909;  Loss pred: 0.8909; Loss self: 0.0000; time: 0.15s
Val loss: 0.8352 score: 0.4091 time: 0.05s
Test loss: 0.7704 score: 0.4651 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.8355;  Loss pred: 0.8355; Loss self: 0.0000; time: 0.15s
Val loss: 0.8035 score: 0.4318 time: 0.05s
Test loss: 0.7477 score: 0.4884 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.7983;  Loss pred: 0.7983; Loss self: 0.0000; time: 0.15s
Val loss: 0.7855 score: 0.4545 time: 0.05s
Test loss: 0.7308 score: 0.4884 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.7082;  Loss pred: 0.7082; Loss self: 0.0000; time: 0.15s
Val loss: 0.7897 score: 0.4773 time: 0.05s
Test loss: 0.7243 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6154;  Loss pred: 0.6154; Loss self: 0.0000; time: 0.15s
Val loss: 0.7989 score: 0.4773 time: 0.07s
Test loss: 0.7268 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.15s
Val loss: 0.8184 score: 0.4773 time: 0.05s
Test loss: 0.7291 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4657;  Loss pred: 0.4657; Loss self: 0.0000; time: 0.15s
Val loss: 0.8329 score: 0.4773 time: 0.05s
Test loss: 0.7319 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.4174;  Loss pred: 0.4174; Loss self: 0.0000; time: 0.15s
Val loss: 0.8256 score: 0.4545 time: 0.05s
Test loss: 0.7388 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3657;  Loss pred: 0.3657; Loss self: 0.0000; time: 0.15s
Val loss: 0.8148 score: 0.4318 time: 0.05s
Test loss: 0.7422 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3238;  Loss pred: 0.3238; Loss self: 0.0000; time: 0.15s
Val loss: 0.8062 score: 0.3409 time: 0.05s
Test loss: 0.7415 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2915;  Loss pred: 0.2915; Loss self: 0.0000; time: 0.15s
Val loss: 0.7938 score: 0.3636 time: 0.05s
Test loss: 0.7371 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2533;  Loss pred: 0.2533; Loss self: 0.0000; time: 0.15s
Val loss: 0.7825 score: 0.5000 time: 0.05s
Test loss: 0.7326 score: 0.4186 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.2278;  Loss pred: 0.2278; Loss self: 0.0000; time: 0.15s
Val loss: 0.7755 score: 0.5227 time: 0.05s
Test loss: 0.7312 score: 0.4186 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.2007;  Loss pred: 0.2007; Loss self: 0.0000; time: 0.15s
Val loss: 0.7711 score: 0.5227 time: 0.05s
Test loss: 0.7277 score: 0.3953 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.1746;  Loss pred: 0.1746; Loss self: 0.0000; time: 0.15s
Val loss: 0.7649 score: 0.5455 time: 0.05s
Test loss: 0.7220 score: 0.4186 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.1584;  Loss pred: 0.1584; Loss self: 0.0000; time: 0.15s
Val loss: 0.7580 score: 0.5227 time: 0.05s
Test loss: 0.7153 score: 0.4186 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.17s
Val loss: 0.7532 score: 0.5455 time: 0.05s
Test loss: 0.7085 score: 0.5116 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.1268;  Loss pred: 0.1268; Loss self: 0.0000; time: 0.14s
Val loss: 0.7468 score: 0.5455 time: 0.05s
Test loss: 0.7004 score: 0.5116 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.15s
Val loss: 0.7422 score: 0.5682 time: 0.05s
Test loss: 0.6929 score: 0.6047 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.1027;  Loss pred: 0.1027; Loss self: 0.0000; time: 0.15s
Val loss: 0.7371 score: 0.5455 time: 0.05s
Test loss: 0.6861 score: 0.5814 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.15s
Val loss: 0.7331 score: 0.5455 time: 0.06s
Test loss: 0.6807 score: 0.6279 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.16s
Val loss: 0.7315 score: 0.5455 time: 0.06s
Test loss: 0.6765 score: 0.6047 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.0768;  Loss pred: 0.0768; Loss self: 0.0000; time: 0.14s
Val loss: 0.7330 score: 0.5682 time: 0.05s
Test loss: 0.6734 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0680;  Loss pred: 0.0680; Loss self: 0.0000; time: 0.15s
Val loss: 0.7344 score: 0.5909 time: 0.05s
Test loss: 0.6725 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.16s
Val loss: 0.7351 score: 0.6136 time: 0.05s
Test loss: 0.6719 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0608;  Loss pred: 0.0608; Loss self: 0.0000; time: 0.15s
Val loss: 0.7347 score: 0.6136 time: 0.05s
Test loss: 0.6708 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0510;  Loss pred: 0.0510; Loss self: 0.0000; time: 0.15s
Val loss: 0.7328 score: 0.5682 time: 0.05s
Test loss: 0.6698 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.15s
Val loss: 0.7333 score: 0.5682 time: 0.05s
Test loss: 0.6697 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0472;  Loss pred: 0.0472; Loss self: 0.0000; time: 0.14s
Val loss: 0.7328 score: 0.5682 time: 0.05s
Test loss: 0.6677 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0454;  Loss pred: 0.0454; Loss self: 0.0000; time: 0.14s
Val loss: 0.7314 score: 0.5682 time: 0.05s
Test loss: 0.6662 score: 0.6047 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.0406;  Loss pred: 0.0406; Loss self: 0.0000; time: 0.14s
Val loss: 0.7326 score: 0.5682 time: 0.05s
Test loss: 0.6639 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.14s
Val loss: 0.7354 score: 0.5682 time: 0.05s
Test loss: 0.6619 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.16s
Val loss: 0.7387 score: 0.5682 time: 0.05s
Test loss: 0.6619 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.15s
Val loss: 0.7406 score: 0.5682 time: 0.05s
Test loss: 0.6619 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.15s
Val loss: 0.7434 score: 0.5682 time: 0.05s
Test loss: 0.6620 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.14s
Val loss: 0.7395 score: 0.5455 time: 0.05s
Test loss: 0.6614 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.14s
Val loss: 0.7351 score: 0.5455 time: 0.05s
Test loss: 0.6638 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.15s
Val loss: 0.7292 score: 0.5455 time: 0.05s
Test loss: 0.6651 score: 0.5581 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.0268;  Loss pred: 0.0268; Loss self: 0.0000; time: 0.15s
Val loss: 0.7198 score: 0.5455 time: 0.05s
Test loss: 0.6652 score: 0.5581 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.15s
Val loss: 0.7107 score: 0.5455 time: 0.05s
Test loss: 0.6666 score: 0.5581 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.15s
Val loss: 0.7009 score: 0.5455 time: 0.05s
Test loss: 0.6653 score: 0.5581 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.15s
Val loss: 0.6929 score: 0.5455 time: 0.05s
Test loss: 0.6653 score: 0.5814 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.0228;  Loss pred: 0.0228; Loss self: 0.0000; time: 0.16s
Val loss: 0.6841 score: 0.5682 time: 0.06s
Test loss: 0.6684 score: 0.5814 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0196;  Loss pred: 0.0196; Loss self: 0.0000; time: 0.15s
Val loss: 0.6742 score: 0.5682 time: 0.06s
Test loss: 0.6727 score: 0.5814 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.15s
Val loss: 0.6650 score: 0.5455 time: 0.05s
Test loss: 0.6765 score: 0.5814 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.14s
Val loss: 0.6587 score: 0.5455 time: 0.05s
Test loss: 0.6811 score: 0.5814 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.14s
Val loss: 0.6554 score: 0.5455 time: 0.05s
Test loss: 0.6852 score: 0.5814 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.14s
Val loss: 0.6523 score: 0.5227 time: 0.05s
Test loss: 0.6883 score: 0.5814 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.14s
Val loss: 0.6496 score: 0.5455 time: 0.05s
Test loss: 0.6898 score: 0.5814 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.15s
Val loss: 0.6458 score: 0.5455 time: 0.05s
Test loss: 0.6907 score: 0.5814 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.14s
Val loss: 0.6421 score: 0.5227 time: 0.05s
Test loss: 0.6923 score: 0.5814 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.14s
Val loss: 0.6385 score: 0.5227 time: 0.05s
Test loss: 0.6933 score: 0.5814 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.14s
Val loss: 0.6352 score: 0.5455 time: 0.05s
Test loss: 0.6946 score: 0.5581 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.14s
Val loss: 0.6308 score: 0.5682 time: 0.05s
Test loss: 0.6943 score: 0.5581 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.15s
Val loss: 0.6261 score: 0.5909 time: 0.05s
Test loss: 0.6936 score: 0.5581 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.14s
Val loss: 0.6213 score: 0.6136 time: 0.05s
Test loss: 0.6917 score: 0.5581 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.14s
Val loss: 0.6178 score: 0.6136 time: 0.05s
Test loss: 0.6904 score: 0.5581 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.15s
Val loss: 0.6120 score: 0.6136 time: 0.05s
Test loss: 0.6881 score: 0.5581 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.15s
Val loss: 0.6065 score: 0.6136 time: 0.05s
Test loss: 0.6840 score: 0.5581 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.14s
Val loss: 0.6010 score: 0.6136 time: 0.05s
Test loss: 0.6797 score: 0.5581 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.14s
Val loss: 0.5972 score: 0.6136 time: 0.05s
Test loss: 0.6803 score: 0.5581 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.14s
Val loss: 0.5923 score: 0.6136 time: 0.05s
Test loss: 0.6790 score: 0.5581 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.16s
Val loss: 0.5844 score: 0.6136 time: 0.05s
Test loss: 0.6744 score: 0.5581 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.14s
Val loss: 0.5733 score: 0.6136 time: 0.05s
Test loss: 0.6657 score: 0.6279 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.15s
Val loss: 0.5564 score: 0.6136 time: 0.05s
Test loss: 0.6482 score: 0.6279 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.15s
Val loss: 0.5397 score: 0.6364 time: 0.05s
Test loss: 0.6300 score: 0.6279 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.14s
Val loss: 0.5277 score: 0.6591 time: 0.05s
Test loss: 0.6157 score: 0.6279 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.15s
Val loss: 0.5148 score: 0.6591 time: 0.05s
Test loss: 0.5999 score: 0.6279 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.15s
Val loss: 0.5023 score: 0.6818 time: 0.05s
Test loss: 0.5833 score: 0.6279 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.14s
Val loss: 0.4911 score: 0.7045 time: 0.05s
Test loss: 0.5690 score: 0.6744 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.14s
Val loss: 0.4801 score: 0.7045 time: 0.05s
Test loss: 0.5527 score: 0.6977 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.14s
Val loss: 0.4692 score: 0.6818 time: 0.05s
Test loss: 0.5380 score: 0.6977 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.14s
Val loss: 0.4577 score: 0.7273 time: 0.05s
Test loss: 0.5205 score: 0.7209 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.14s
Val loss: 0.4456 score: 0.7273 time: 0.05s
Test loss: 0.5063 score: 0.7209 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.14s
Val loss: 0.4323 score: 0.7500 time: 0.05s
Test loss: 0.4932 score: 0.7209 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.14s
Val loss: 0.4202 score: 0.7727 time: 0.05s
Test loss: 0.4805 score: 0.7209 time: 0.05s
Epoch 78/1000, LR 0.000267
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.14s
Val loss: 0.4084 score: 0.7727 time: 0.05s
Test loss: 0.4693 score: 0.7209 time: 0.05s
Epoch 79/1000, LR 0.000267
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.15s
Val loss: 0.3948 score: 0.7727 time: 0.05s
Test loss: 0.4591 score: 0.7674 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.15s
Val loss: 0.3823 score: 0.7727 time: 0.05s
Test loss: 0.4502 score: 0.7674 time: 0.05s
Epoch 81/1000, LR 0.000267
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.14s
Val loss: 0.3710 score: 0.7955 time: 0.06s
Test loss: 0.4431 score: 0.7442 time: 0.05s
Epoch 82/1000, LR 0.000267
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.15s
Val loss: 0.3606 score: 0.7955 time: 0.06s
Test loss: 0.4360 score: 0.7442 time: 0.05s
Epoch 83/1000, LR 0.000266
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.15s
Val loss: 0.3509 score: 0.8182 time: 0.05s
Test loss: 0.4317 score: 0.7674 time: 0.05s
Epoch 84/1000, LR 0.000266
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.14s
Val loss: 0.3417 score: 0.8182 time: 0.05s
Test loss: 0.4272 score: 0.7674 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.15s
Val loss: 0.3336 score: 0.8182 time: 0.05s
Test loss: 0.4193 score: 0.7674 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.15s
Val loss: 0.3263 score: 0.8409 time: 0.07s
Test loss: 0.4106 score: 0.7907 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.15s
Val loss: 0.3183 score: 0.8409 time: 0.05s
Test loss: 0.4031 score: 0.8140 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.14s
Val loss: 0.3109 score: 0.8636 time: 0.05s
Test loss: 0.3950 score: 0.8140 time: 0.09s
Epoch 89/1000, LR 0.000266
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.15s
Val loss: 0.3071 score: 0.8636 time: 0.05s
Test loss: 0.3881 score: 0.8372 time: 0.05s
Epoch 90/1000, LR 0.000266
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.14s
Val loss: 0.3074 score: 0.8636 time: 0.05s
Test loss: 0.3829 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.14s
Val loss: 0.3059 score: 0.8864 time: 0.05s
Test loss: 0.3802 score: 0.8140 time: 0.05s
Epoch 92/1000, LR 0.000266
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.14s
Val loss: 0.3065 score: 0.8864 time: 0.05s
Test loss: 0.3797 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.15s
Val loss: 0.3086 score: 0.8864 time: 0.05s
Test loss: 0.3810 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.15s
Val loss: 0.3116 score: 0.8636 time: 0.05s
Test loss: 0.3841 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.14s
Val loss: 0.3162 score: 0.8636 time: 0.05s
Test loss: 0.3881 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.16s
Val loss: 0.3229 score: 0.8636 time: 0.11s
Test loss: 0.3934 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.3303 score: 0.8636 time: 0.05s
Test loss: 0.3986 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.14s
Val loss: 0.3388 score: 0.8636 time: 0.05s
Test loss: 0.4037 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.14s
Val loss: 0.3506 score: 0.8409 time: 0.05s
Test loss: 0.4095 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.14s
Val loss: 0.3630 score: 0.8409 time: 0.05s
Test loss: 0.4160 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.14s
Val loss: 0.3684 score: 0.8182 time: 0.05s
Test loss: 0.4219 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.3715 score: 0.8409 time: 0.05s
Test loss: 0.4255 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.15s
Val loss: 0.3714 score: 0.8409 time: 0.10s
Test loss: 0.4286 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.14s
Val loss: 0.3734 score: 0.8409 time: 0.05s
Test loss: 0.4291 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.14s
Val loss: 0.3744 score: 0.8409 time: 0.05s
Test loss: 0.4303 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.16s
Val loss: 0.3759 score: 0.8409 time: 0.05s
Test loss: 0.4326 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.14s
Val loss: 0.3749 score: 0.8409 time: 0.05s
Test loss: 0.4367 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.3755 score: 0.8409 time: 0.05s
Test loss: 0.4434 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.14s
Val loss: 0.3754 score: 0.8409 time: 0.05s
Test loss: 0.4521 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.16s
Val loss: 0.3725 score: 0.8409 time: 0.05s
Test loss: 0.4615 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.14s
Val loss: 0.3707 score: 0.8182 time: 0.05s
Test loss: 0.4700 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.0045,   Val_Loss: 0.3059,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.3059,   Test_Precision: 0.7826,   Test_Recall: 0.8571,   Test_accuracy: 0.8182,   Test_Score: 0.8140,   Test_loss: 0.3802


[0.056123649002984166, 0.056504828040488064, 0.05611734604462981, 0.059435631963424385, 0.0566912479698658, 0.05746407597325742, 0.05644895904697478, 0.05992711696308106, 0.06094945000950247, 0.05649067007470876, 0.05716392502654344, 0.05736346391495317, 0.05581190204247832, 0.0559559470275417, 0.05580962297972292, 0.05622370296623558, 0.05590877798385918, 0.055651732021942735, 0.05784153298009187, 0.056331088999286294, 0.056007246021181345, 0.055912434007041156, 0.05644990096334368, 0.05626495706383139, 0.056307432940229774, 0.056075685075484216, 0.05567603500094265, 0.05625650903675705, 0.056011097971349955, 0.05590795399621129, 0.056491896975785494, 0.05664781399536878, 0.056382075999863446, 0.057250619982369244, 0.0569297110196203, 0.056195655022747815, 0.0566090940264985, 0.05643652204889804, 0.056486974004656076, 0.05235527001786977, 0.051889650989323854, 0.052337844972498715, 0.05222141300328076, 0.0519215629901737, 0.0521815400570631, 0.05188386305235326, 0.05282504309434444, 0.05668381508439779, 0.051663429010659456, 0.05163468699902296, 0.052217481075786054, 0.052228663000278175, 0.052058286033570766, 0.052769772009924054, 0.05235717201139778, 0.052032161969691515, 0.05231033801101148, 0.05283720896113664, 0.05290720006451011, 0.05276582308579236, 0.052867816062644124, 0.05261450994294137, 0.052471423987299204, 0.05323339800816029, 0.052732185926288366, 0.05289485305547714, 0.05295724200550467, 0.053480362985283136, 0.053574623074382544, 0.0528071029111743, 0.052869664039462805, 0.05259112303610891, 0.05278907099273056, 0.052734158001840115, 0.05258288700133562, 0.052646290976554155, 0.05247969401534647, 0.05028055200818926, 0.050594347063452005, 0.05097423796541989, 0.05095987406093627, 0.05123306100722402, 0.05162746901623905, 0.05085364100523293, 0.051206614007242024, 0.051117594935931265, 0.05110922199673951, 0.05092535703442991, 0.05146336508914828, 0.05108640796970576, 0.05090596491936594, 0.0510371180716902, 0.05113908997736871, 0.05069096793886274, 0.05139231402426958, 0.05006331403274089, 0.050381003995426, 0.05503429600503296, 0.05367212207056582, 0.05544281401671469, 0.04984593892004341, 0.05009658192284405, 0.050009026075713336, 0.049950344953686, 0.049988015089184046, 0.05069836194161326, 0.050048397039063275, 0.049860258935950696, 0.049817248014733195, 0.05002341698855162, 0.04968266899231821, 0.0497756419936195, 0.05020575097296387, 0.04990302794612944, 0.050125756999477744, 0.05033937096595764, 0.05001613509375602, 0.04996980307623744, 0.050412919954396784, 0.05140839295927435, 0.053554199053905904, 0.05376087292097509, 0.05059127905406058, 0.04984578397125006, 0.049777916981838644, 0.050618003006093204, 0.05008095607627183, 0.04990773100871593, 0.05031343596056104, 0.04982958303298801, 0.05008933797944337, 0.04992368503008038, 0.04959018505178392, 0.049960530013777316, 0.05217296502087265, 0.04993362899404019, 0.049772865953855217, 0.049823983921669424, 0.050010053906589746, 0.049992428976111114, 0.050227426923811436, 0.05170926603022963, 0.050046112039126456, 0.049698148970492184, 0.05016982601955533, 0.049590208916924894, 0.04998173704370856, 0.04964295600075275, 0.0499047179473564, 0.050183198996819556, 0.049592465977184474, 0.04981209698598832, 0.04970463004428893, 0.0503105940297246, 0.049950361950322986, 0.0500944999512285, 0.04980091005563736, 0.051773271057754755, 0.05287868401501328, 0.050140975043177605, 0.04990499501582235, 0.05180318595375866, 0.051302006002515554, 0.0497440539766103, 0.09656863601412624, 0.05036259698681533, 0.050523745943792164, 0.05017178098205477, 0.051055726944468915, 0.05095673305913806, 0.051592868054285645, 0.05079287604894489, 0.04922664898913354, 0.048161115031689405, 0.048783355043269694, 0.049164919066242874, 0.04947552608791739, 0.0492891730973497, 0.048826438025571406, 0.04887732304632664, 0.04832919593900442, 0.049732681014575064, 0.049016540055163205, 0.04959958896506578, 0.049273094977252185, 0.049334223032929, 0.049280300037935376, 0.04862948798108846]
[0.0012755374773405492, 0.0012842006372838196, 0.0012753942282870412, 0.0013508098173505541, 0.0012884374538605864, 0.0013060017266649413, 0.0012829308874312449, 0.0013619799309791151, 0.001385214772943238, 0.00128387886533429, 0.0012991801142396237, 0.0013037150889762086, 0.0012684523191472347, 0.0012717260688077658, 0.00126840052226643, 0.0012778114310508086, 0.0012706540450877087, 0.0012648120914077895, 0.001314580295002088, 0.0012802520227110522, 0.0012728919550268488, 0.0012707371365236627, 0.0012829522946214472, 0.001278749024177986, 0.001279714385005222, 0.0012744473880791868, 0.0012653644318396057, 0.0012785570235626603, 0.0012729794993488626, 0.0012706353180957112, 0.0012839067494496703, 0.0012874503180765632, 0.0012814108181787146, 0.0013011504541447555, 0.0012938570686277342, 0.001277173977789723, 0.0012865703187840568, 0.0012826482283840464, 0.0012837948637421835, 0.0012175644190202271, 0.0012067360695191594, 0.001217159185406947, 0.0012144514651925757, 0.001207478209073807, 0.0012135241873735606, 0.0012066014663337967, 0.00122848937428708, 0.0013182282577766929, 0.0012014750932711502, 0.001200806674395883, 0.0012143600250182802, 0.001214620069773911, 0.001210657814734204, 0.001227204000230792, 0.0012176086514278552, 0.0012100502783649188, 0.001216519488628174, 0.0012287723014217822, 0.001230400001500235, 0.0012271121647858688, 0.001229484094480096, 0.0012235932544870085, 0.0012202656741232374, 0.0012379860001897744, 0.00122632990526252, 0.0012301128617552824, 0.001231563767569876, 0.0012437293717507706, 0.0012459214668461057, 0.0012280721607249836, 0.0012295270706851815, 0.0012230493729327653, 0.0012276528137844316, 0.0012263757674846538, 0.0012228578372403633, 0.0012243323482919572, 0.0012204580003568946, 0.0011693151629811457, 0.0011766127224058607, 0.0011854473945446485, 0.0011851133502543318, 0.0011914665350517214, 0.0012006388143311407, 0.0011826428140751845, 0.0011908514885405122, 0.001188781277579797, 0.0011885865580637095, 0.0011843106287076723, 0.001196822443933681, 0.0011880559992954829, 0.00118385964928758, 0.0011869097225974466, 0.0011892811622643887, 0.0011788597195084359, 0.0011951700935876647, 0.0011642631170404858, 0.0011716512557075813, 0.0012798673489542549, 0.0012481888853619959, 0.0012893677678305743, 0.0011592078818614747, 0.0011650367889033501, 0.001163000606411938, 0.0011616359291554883, 0.0011625119788182337, 0.0011790316730607734, 0.0011639162102107738, 0.0011595409054872255, 0.0011585406515054232, 0.001163335278803526, 0.001155410906798098, 0.0011575730696190582, 0.0011675756040224155, 0.0011605355336309173, 0.001165715279057622, 0.0011706830457199452, 0.0011631659324129308, 0.0011620884436334287, 0.0011723934873115532, 0.0011955440223087059, 0.0012454464896257188, 0.0012502528586273277, 0.001176541373350246, 0.0011592042784011642, 0.001157625976321829, 0.0011771628606068187, 0.0011646733971226008, 0.0011606449071794402, 0.001170079906059559, 0.00115882751239507, 0.0011648683251033341, 0.0011610159309321019, 0.001153260117483347, 0.0011618727910180772, 0.001213324767927271, 0.0011612471859079115, 0.0011575085105547725, 0.00115869730050394, 0.0011630245094555755, 0.0011626146273514212, 0.0011680796959025914, 0.0012025410704704564, 0.0011638630706773594, 0.001155770906290516, 0.0011667401399896588, 0.0011532606724866253, 0.0011623659777606642, 0.0011544873488547151, 0.0011605748359850327, 0.00116705113946092, 0.001153313162260104, 0.0011584208601392633, 0.0011559216289369519, 0.001170013814644758, 0.001161636324426116, 0.0011649883709588024, 0.0011581606989683106, 0.0012040295594826687, 0.0012297368375584483, 0.001166069187050642, 0.0011605812794377292, 0.0012047252547385734, 0.0011930699070352455, 0.0011568384645723325, 0.0022457822328866566, 0.0011712231857398914, 0.0011749708359021433, 0.0011667856042338319, 0.0011873424870806725, 0.001185040303700885, 0.0011998341407973407, 0.0011812296755568579, 0.001144805790444966, 0.001120025930969521, 0.0011344966289132488, 0.0011433702108428576, 0.0011505936299515671, 0.001146259839473249, 0.0011354985587342188, 0.0011366819313099218, 0.0011239347892791727, 0.001156573977083141, 0.0011399195361665863, 0.0011534788131410646, 0.0011458859297035391, 0.0011473075123936978, 0.001146053489254311, 0.001130918325141592]
[783.9832366862045, 778.6945209084109, 784.0712917001999, 740.2966629020922, 776.1339108884704, 765.6957717457543, 779.4652149986467, 734.2252093840392, 721.9097135928228, 778.8896811068114, 769.7162148954798, 767.0387559794892, 788.3623096469942, 786.3328624988343, 788.3945035067939, 782.5880843604989, 786.9962747657044, 790.6312777947574, 760.6990640297189, 781.096207825087, 785.6126327539773, 786.9448143584484, 779.4522089342876, 782.0142819994147, 781.42436446545, 784.6538110193576, 790.2861617077265, 782.1317169049921, 785.5586052340251, 787.0078737451516, 778.8727650420382, 776.7290014685686, 780.3898529757323, 768.5506290333647, 772.8828973826304, 782.9786837112041, 777.2602751671626, 779.6369868766413, 778.9406456145658, 821.3117797945335, 828.6816191700176, 821.5852223681477, 823.4170147272455, 828.1722953551663, 824.0462039444863, 828.7740632691705, 814.00785463067, 758.5939643613674, 832.3102206616604, 832.7735191038081, 823.4790172584499, 823.302714062793, 825.9972288037036, 814.8604468466015, 821.2819437734187, 826.4119416188644, 822.0172462075923, 813.8204277903437, 812.7438221559601, 814.9214299203855, 813.349277546257, 817.2650481137623, 819.4936735547376, 807.7635771702646, 815.4412574534176, 812.9335373122366, 811.9758199554717, 804.0334358207863, 802.618805927933, 814.2843979214194, 813.3208481881799, 817.6284801995258, 814.562544696447, 815.4107627640428, 817.7565449935792, 816.7716889903963, 819.3645334026843, 855.2014304257546, 849.8973204668955, 843.5633707593729, 843.8011433972914, 839.3017936979574, 832.889948303967, 845.563840661384, 839.7352731410559, 841.1976356457002, 841.3354443693776, 844.3730688216542, 835.5458280956274, 841.7111656293979, 844.6947242452071, 842.5240614017289, 840.8440591928676, 848.2773509446762, 836.700989562245, 858.9123758742468, 853.496290068056, 781.3309721644771, 801.160795234916, 775.5739091279985, 862.6580405873224, 858.341993596014, 859.844779518367, 860.8549158142879, 860.2061898893832, 848.1536356050503, 859.1683758910015, 862.4102826107818, 863.1548653045421, 859.59741634285, 865.492955031231, 863.8763515197245, 856.475586295996, 861.6711604437852, 857.8424062592813, 854.2021716774939, 859.7225659158956, 860.5197009561191, 852.9559493657089, 836.4392957014729, 802.924901494981, 799.8382032078821, 849.9488608313554, 862.6607222147704, 863.8368699857098, 849.5001273523932, 858.6098063805383, 861.5899607315437, 854.642486227858, 862.9411964280995, 858.4661274151232, 861.3146239924263, 867.1070687697131, 860.6794200970676, 824.1816424042057, 861.1430986746877, 863.924533497139, 863.0381718893109, 859.8271075715429, 860.1302413320935, 856.1059690600015, 831.572429878658, 859.2076037072021, 865.2233713076689, 857.0888801416083, 867.1066514770079, 860.3142376263735, 866.185325453786, 861.6419803305959, 856.8604803915589, 867.0671875801174, 863.2441234524919, 865.1105533163646, 854.6907630348126, 860.8546228907147, 858.3776670465693, 863.4380366133989, 830.5443933034888, 813.1821130002303, 857.5820466788223, 861.6371965645297, 830.0647770657062, 838.1738522640134, 864.4249224282894, 445.27914833248633, 853.8082341396567, 851.0849541488401, 857.0554833479015, 842.2169768881995, 843.8531557762183, 833.4485292571002, 846.5754126339383, 873.5106062062436, 892.8364713256025, 881.4481898971485, 874.6073585937057, 869.1165794495957, 872.4025439638006, 880.6704264906651, 879.753581415332, 889.7313345388505, 864.6226007280418, 877.2549011335318, 866.9426682202138, 872.6872143885365, 871.6059026874485, 872.5596225448936, 884.2371529126996]
Elapsed: 0.05239803752146087~0.004211292427981819
Time per graph: 0.0012123443954706844~9.216973667556497e-05
Speed: 828.1253618145656~45.00756919027281
Total Time: 0.0494
best val loss: 0.3058655560016632 test_score: 0.8140

Testing...
Test loss: 0.3802 score: 0.8140 time: 0.04s
test Score 0.8140
Epoch Time List: [0.23565922200214118, 0.23609857191331685, 0.23612849693745375, 0.25938629906158894, 0.23611598904244602, 0.23974205378908664, 0.23982832196634263, 0.24852330808062106, 0.2584526720456779, 0.23566971812397242, 0.2370977341197431, 0.24397330486681312, 0.23316360812168568, 0.23309161805082113, 0.23313851689454168, 0.23484586214181036, 0.23417882912326604, 0.23372186196502298, 0.23520005901809782, 0.234176215948537, 0.23398939496837556, 0.23333884682506323, 0.2350447919452563, 0.2349197849398479, 0.23425694613251835, 0.23412002401892096, 0.23328502906952053, 0.23292306100483984, 0.23227752896491438, 0.23409773595631123, 0.23330931505188346, 0.23546543100383133, 0.2438744029495865, 0.23736636014655232, 0.23627156193833798, 0.2352258978644386, 0.23495958792045712, 0.2354052651207894, 0.23537716292776167, 0.24268977984320372, 0.24063222901895642, 0.24154602992348373, 0.24165221583098173, 0.24101999995764345, 0.24164402903988957, 0.2427365940529853, 0.24418378109112382, 0.25470841815695167, 0.24176442110911012, 0.2415379349840805, 0.241914507932961, 0.24224176805000752, 0.24271906807553023, 0.24620309798046947, 0.2436727179447189, 0.24329067196231335, 0.24335770087782294, 0.24456462101079524, 0.24655416596215218, 0.24625273188576102, 0.2470087178517133, 0.24647439492400736, 0.2472205669619143, 0.2466516668209806, 0.24647055997047573, 0.24661359295714647, 0.24799146002624184, 0.2574385979678482, 0.24989197205286473, 0.24740068288519979, 0.24702512694057077, 0.24687545304186642, 0.24613819585647434, 0.24669945717323571, 0.24646701384335756, 0.24588977394159883, 0.24653836619108915, 0.24459600902628154, 0.24369326804298908, 0.24462250620126724, 0.24657176691107452, 0.2456414169864729, 0.25938032497651875, 0.24588359019253403, 0.2464892651187256, 0.2466648609843105, 0.24649973085615784, 0.24491044995374978, 0.24557409493718296, 0.24620129505638033, 0.24508318887092173, 0.2444668608950451, 0.252016682876274, 0.24408382503315806, 0.2659192020073533, 0.24091139901429415, 0.2420933070825413, 0.24740540399216115, 0.2567786549916491, 0.27600517007522285, 0.2407403930556029, 0.2438784479163587, 0.2512318770168349, 0.24149368703365326, 0.24091069400310516, 0.2430244730785489, 0.23988261097110808, 0.24000706488732249, 0.2409463080111891, 0.2401619420852512, 0.2566626010229811, 0.2436873250408098, 0.24792757001705468, 0.2400798429735005, 0.24124253599438816, 0.24227917415555567, 0.24187705502845347, 0.24170650297310203, 0.24153102387208492, 0.24573520896956325, 0.26586921501439065, 0.2602415719302371, 0.24947600602172315, 0.24031406897120178, 0.24046450410969555, 0.24149774597026408, 0.24122359906323254, 0.24141556606628, 0.24052001000382006, 0.23971115506719798, 0.2401955040404573, 0.240615950897336, 0.2415996299823746, 0.23903249902650714, 0.24198177200742066, 0.241700679063797, 0.2418559470679611, 0.2398641249164939, 0.23931264004204422, 0.2407124500023201, 0.25646831712219864, 0.24311519402544945, 0.24724480893928558, 0.24044008401688188, 0.24006578896660358, 0.24106126301921904, 0.24120820593088865, 0.23989196214824915, 0.23959704011213034, 0.24065779009833932, 0.2400400839978829, 0.24018919689115137, 0.2395833149785176, 0.2403974700719118, 0.23997893300838768, 0.24580504593905061, 0.2441169418161735, 0.2477023630635813, 0.25415606109891087, 0.24300359294284135, 0.24033588892780244, 0.24312077893409878, 0.2644723510602489, 0.2436701589031145, 0.2841706669423729, 0.2464348430512473, 0.24204959394410253, 0.2408437340054661, 0.2416189240757376, 0.24265210411977023, 0.2467693059006706, 0.24014053109567612, 0.31113752687815577, 0.23260312690399587, 0.23334160808008164, 0.23571358900517225, 0.23521795601118356, 0.23453230794984847, 0.2327660860028118, 0.3018461500760168, 0.2333013580646366, 0.2355591938830912, 0.2555606459500268, 0.23530150693841279, 0.23791108804289252, 0.23551971185952425, 0.2520386279793456, 0.23347682994790375]
Total Epoch List: [39, 38, 111]
Total Time List: [0.05683992896229029, 0.05294805997982621, 0.049352887901477516]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156ebfdbe80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7427;  Loss pred: 0.7427; Loss self: 0.0000; time: 0.14s
Val loss: 1.1521 score: 0.4419 time: 0.04s
Test loss: 1.4906 score: 0.4091 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7524;  Loss pred: 0.7524; Loss self: 0.0000; time: 0.12s
Val loss: 1.1341 score: 0.4186 time: 0.04s
Test loss: 1.4844 score: 0.3864 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7158;  Loss pred: 0.7158; Loss self: 0.0000; time: 0.13s
Val loss: 1.1069 score: 0.4651 time: 0.04s
Test loss: 1.4278 score: 0.4091 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.12s
Val loss: 1.0780 score: 0.4186 time: 0.04s
Test loss: 1.3441 score: 0.3182 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.12s
Val loss: 1.0365 score: 0.4186 time: 0.04s
Test loss: 1.2631 score: 0.3409 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.13s
Val loss: 0.9849 score: 0.4186 time: 0.04s
Test loss: 1.2007 score: 0.3409 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.4969;  Loss pred: 0.4969; Loss self: 0.0000; time: 0.12s
Val loss: 0.9392 score: 0.4186 time: 0.04s
Test loss: 1.1547 score: 0.3636 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.4309;  Loss pred: 0.4309; Loss self: 0.0000; time: 0.12s
Val loss: 0.9019 score: 0.4651 time: 0.04s
Test loss: 1.1158 score: 0.3864 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.3848;  Loss pred: 0.3848; Loss self: 0.0000; time: 0.12s
Val loss: 0.8757 score: 0.4419 time: 0.04s
Test loss: 1.0888 score: 0.3864 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.3217;  Loss pred: 0.3217; Loss self: 0.0000; time: 0.12s
Val loss: 0.8593 score: 0.4419 time: 0.04s
Test loss: 1.0712 score: 0.3636 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.2696;  Loss pred: 0.2696; Loss self: 0.0000; time: 0.13s
Val loss: 0.8398 score: 0.4186 time: 0.04s
Test loss: 1.0502 score: 0.4091 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.2272;  Loss pred: 0.2272; Loss self: 0.0000; time: 0.14s
Val loss: 0.8176 score: 0.4419 time: 0.10s
Test loss: 1.0209 score: 0.4091 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 0.13s
Val loss: 0.7978 score: 0.4651 time: 0.04s
Test loss: 0.9900 score: 0.4318 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.1703;  Loss pred: 0.1703; Loss self: 0.0000; time: 0.12s
Val loss: 0.7831 score: 0.4419 time: 0.04s
Test loss: 0.9645 score: 0.4318 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.1474;  Loss pred: 0.1474; Loss self: 0.0000; time: 0.12s
Val loss: 0.7748 score: 0.4651 time: 0.04s
Test loss: 0.9537 score: 0.4091 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.1303;  Loss pred: 0.1303; Loss self: 0.0000; time: 0.12s
Val loss: 0.7662 score: 0.5116 time: 0.04s
Test loss: 0.9437 score: 0.4318 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.1114;  Loss pred: 0.1114; Loss self: 0.0000; time: 0.12s
Val loss: 0.7591 score: 0.4884 time: 0.04s
Test loss: 0.9308 score: 0.4318 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.12s
Val loss: 0.7531 score: 0.5116 time: 0.04s
Test loss: 0.9174 score: 0.4545 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.23s
Val loss: 0.7465 score: 0.5116 time: 0.04s
Test loss: 0.9039 score: 0.4545 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.0868;  Loss pred: 0.0868; Loss self: 0.0000; time: 0.13s
Val loss: 0.7402 score: 0.5349 time: 0.04s
Test loss: 0.8946 score: 0.4545 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.0670;  Loss pred: 0.0670; Loss self: 0.0000; time: 0.12s
Val loss: 0.7334 score: 0.5349 time: 0.04s
Test loss: 0.8835 score: 0.4545 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.12s
Val loss: 0.7272 score: 0.5116 time: 0.04s
Test loss: 0.8686 score: 0.5000 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.0572;  Loss pred: 0.0572; Loss self: 0.0000; time: 0.12s
Val loss: 0.7224 score: 0.5349 time: 0.04s
Test loss: 0.8531 score: 0.5227 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.12s
Val loss: 0.7191 score: 0.5349 time: 0.04s
Test loss: 0.8393 score: 0.5227 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.12s
Val loss: 0.7165 score: 0.5349 time: 0.04s
Test loss: 0.8277 score: 0.5455 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.12s
Val loss: 0.7164 score: 0.5349 time: 0.04s
Test loss: 0.8155 score: 0.5455 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.0343;  Loss pred: 0.0343; Loss self: 0.0000; time: 0.15s
Val loss: 0.7173 score: 0.5349 time: 0.04s
Test loss: 0.8008 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.16s
Val loss: 0.7188 score: 0.5349 time: 0.04s
Test loss: 0.7864 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.14s
Val loss: 0.7210 score: 0.5349 time: 0.04s
Test loss: 0.7738 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0278;  Loss pred: 0.0278; Loss self: 0.0000; time: 0.14s
Val loss: 0.7242 score: 0.5581 time: 0.11s
Test loss: 0.7620 score: 0.4773 time: 0.09s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.15s
Val loss: 0.7268 score: 0.5581 time: 0.05s
Test loss: 0.7514 score: 0.4773 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.15s
Val loss: 0.7294 score: 0.5581 time: 0.06s
Test loss: 0.7448 score: 0.4773 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.15s
Val loss: 0.7344 score: 0.5581 time: 0.04s
Test loss: 0.7429 score: 0.4773 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.15s
Val loss: 0.7389 score: 0.5581 time: 0.05s
Test loss: 0.7405 score: 0.4773 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.18s
Val loss: 0.7405 score: 0.5581 time: 0.08s
Test loss: 0.7384 score: 0.4773 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.14s
Val loss: 0.7414 score: 0.5581 time: 0.05s
Test loss: 0.7366 score: 0.4773 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.16s
Val loss: 0.7414 score: 0.5581 time: 0.09s
Test loss: 0.7346 score: 0.4773 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.22s
Val loss: 0.7404 score: 0.5581 time: 0.04s
Test loss: 0.7309 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.13s
Val loss: 0.7388 score: 0.5581 time: 0.04s
Test loss: 0.7262 score: 0.4773 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.13s
Val loss: 0.7381 score: 0.5581 time: 0.04s
Test loss: 0.7218 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0135;  Loss pred: 0.0135; Loss self: 0.0000; time: 0.14s
Val loss: 0.7380 score: 0.5581 time: 0.04s
Test loss: 0.7167 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.13s
Val loss: 0.7377 score: 0.5581 time: 0.04s
Test loss: 0.7118 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.13s
Val loss: 0.7377 score: 0.5581 time: 0.04s
Test loss: 0.7077 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.13s
Val loss: 0.7368 score: 0.5581 time: 0.04s
Test loss: 0.7046 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.14s
Val loss: 0.7350 score: 0.5581 time: 0.04s
Test loss: 0.7018 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.13s
Val loss: 0.7322 score: 0.5581 time: 0.04s
Test loss: 0.6980 score: 0.5455 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 025,   Train_Loss: 0.0400,   Val_Loss: 0.7164,   Val_Precision: 0.5263,   Val_Recall: 0.9091,   Val_accuracy: 0.6667,   Val_Score: 0.5349,   Val_Loss: 0.7164,   Test_Precision: 0.5278,   Test_Recall: 0.8636,   Test_accuracy: 0.6552,   Test_Score: 0.5455,   Test_loss: 0.8155


[0.05634695792105049, 0.055449633044190705, 0.054845440899953246, 0.055756825953722, 0.05552349297795445, 0.05554869002662599, 0.05555638193618506, 0.05512944399379194, 0.05528483004309237, 0.05525871098507196, 0.056264269980601966, 0.055052267969585955, 0.0546871799742803, 0.05448615492787212, 0.05493574100546539, 0.0541823951061815, 0.05465300497598946, 0.05449016997590661, 0.05619608098641038, 0.054530161898583174, 0.055166018079034984, 0.05431446898728609, 0.05462204199284315, 0.05421569396276027, 0.054416383034549654, 0.054967593983747065, 0.05501973000355065, 0.060359888011589646, 0.059467777027748525, 0.09962164901662618, 0.07356239401269704, 0.2383197050075978, 0.06387459801044315, 0.07665849500335753, 0.05965879198629409, 0.0792895199265331, 0.0684199120150879, 0.05790492706000805, 0.05867801303975284, 0.058604560093954206, 0.05815091100521386, 0.058425317984074354, 0.05841778090689331, 0.05844970198813826, 0.0586075980681926, 0.058674469008110464]
[0.0012806126800238747, 0.001260218932822516, 0.0012464872931807556, 0.0012672005898573182, 0.001261897567680783, 0.0012624702278778634, 0.001262645044004206, 0.0012529419089498167, 0.001256473410070281, 0.0012558797951152717, 0.0012787334086500448, 0.0012511879083996807, 0.001242890453960916, 0.0012383217029061846, 0.0012485395683060315, 0.0012314180705950341, 0.0012421137494543059, 0.0012384129539978776, 0.0012771836587820542, 0.0012393218613314357, 0.001253773138159886, 0.0012344197497110474, 0.00124141004529189, 0.001232174862790006, 0.0012367359780579466, 0.0012492634996306151, 0.0012504484091716056, 0.0013718156366270375, 0.0013515403869942847, 0.002264128386741504, 0.00167187259119766, 0.005416356931990859, 0.0014516954093282534, 0.0017422385228035803, 0.0013558816360521384, 0.0018020345437848432, 0.0015549980003429068, 0.0013160210695456374, 0.0013335912054489281, 0.001331921820317141, 0.0013216116137548604, 0.00132784813600169, 0.00132767683879303, 0.0013284023179122332, 0.0013319908651861954, 0.001333510659275238]
[780.8762286980922, 793.512915855261, 802.2544677918253, 789.1410468113781, 792.4573480539159, 792.097887077258, 791.988219292982, 798.1215991395594, 795.878362395321, 796.2545491132886, 782.0238317349488, 799.2404604349476, 804.5761368696184, 807.5445965722206, 800.9357695861894, 812.0718900257729, 805.0792453101232, 807.4850935399, 782.9727487694436, 806.8928913475906, 797.5924587662334, 810.0972138805132, 805.535611535085, 811.5731218016459, 808.58001848568, 800.4716381257293, 799.7131210414973, 728.9609283494959, 739.896498560371, 441.67106682460854, 598.131702897074, 184.62594185653782, 688.8497363663446, 573.9742216185285, 737.5275049167687, 554.9283189098492, 643.0876437008155, 759.8662537714953, 749.8549749834087, 750.7948174930359, 756.6519464511042, 753.098169050506, 753.1953339707909, 752.7839921053717, 750.7558994109263, 749.9002674215587]
Elapsed: 0.06287056029996956~0.027424690785843142
Time per graph: 0.0014288763704538537~0.0006232884269509807
Speed: 744.4244280590133~113.1069393305558
Total Time: 0.0592
best val loss: 0.7164089679718018 test_score: 0.5455

Testing...
Test loss: 0.7620 score: 0.4773 time: 0.05s
test Score 0.4773
Epoch Time List: [0.22510718100238591, 0.21324360405560583, 0.21399372396990657, 0.21300871390849352, 0.2130030218977481, 0.21418408409226686, 0.2125342229846865, 0.21161293401382864, 0.21294486697297543, 0.2126772899646312, 0.2161224300507456, 0.29766715492587537, 0.21386800485197455, 0.2115746820345521, 0.21327114303130656, 0.21143510600086302, 0.21261684398632497, 0.21168283000588417, 0.3178139260271564, 0.213214545045048, 0.21209195908159018, 0.21046793705318123, 0.21201773511711508, 0.20954467705450952, 0.2099147809203714, 0.21205098810605705, 0.23859599907882512, 0.2593261309666559, 0.23381322401110083, 0.34521425887942314, 0.2635196888586506, 0.4397176749771461, 0.2508827759884298, 0.27088973799254745, 0.32057094294577837, 0.26476964994799346, 0.31752500508446246, 0.3132835761643946, 0.22949904017150402, 0.22933115810155869, 0.23035897687077522, 0.22716045600827783, 0.2276837658137083, 0.2282171129481867, 0.22978435608092695, 0.22926328494213521]
Total Epoch List: [46]
Total Time List: [0.05915001092944294]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156ebfdbbe0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9133;  Loss pred: 0.9133; Loss self: 0.0000; time: 0.15s
Val loss: 1.3204 score: 0.4545 time: 0.04s
Test loss: 1.3408 score: 0.4419 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.9182;  Loss pred: 0.9182; Loss self: 0.0000; time: 0.15s
Val loss: 1.1642 score: 0.4545 time: 0.04s
Test loss: 1.1559 score: 0.4419 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.8552;  Loss pred: 0.8552; Loss self: 0.0000; time: 0.15s
Val loss: 1.0636 score: 0.4545 time: 0.04s
Test loss: 1.0417 score: 0.4419 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.7754;  Loss pred: 0.7754; Loss self: 0.0000; time: 0.15s
Val loss: 0.9952 score: 0.4545 time: 0.04s
Test loss: 0.9702 score: 0.4419 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.15s
Val loss: 0.9463 score: 0.4545 time: 0.04s
Test loss: 0.9351 score: 0.4419 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.15s
Val loss: 0.9155 score: 0.4545 time: 0.04s
Test loss: 0.9230 score: 0.4419 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4652;  Loss pred: 0.4652; Loss self: 0.0000; time: 0.15s
Val loss: 0.9059 score: 0.4545 time: 0.04s
Test loss: 0.9236 score: 0.4419 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.3895;  Loss pred: 0.3895; Loss self: 0.0000; time: 0.15s
Val loss: 0.8906 score: 0.4318 time: 0.04s
Test loss: 0.9136 score: 0.4186 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3363;  Loss pred: 0.3363; Loss self: 0.0000; time: 0.15s
Val loss: 0.8698 score: 0.4091 time: 0.04s
Test loss: 0.8890 score: 0.4186 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.2832;  Loss pred: 0.2832; Loss self: 0.0000; time: 0.15s
Val loss: 0.8444 score: 0.3864 time: 0.04s
Test loss: 0.8591 score: 0.3953 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.2381;  Loss pred: 0.2381; Loss self: 0.0000; time: 0.15s
Val loss: 0.8178 score: 0.4091 time: 0.04s
Test loss: 0.8277 score: 0.3488 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.2111;  Loss pred: 0.2111; Loss self: 0.0000; time: 0.15s
Val loss: 0.7919 score: 0.3636 time: 0.04s
Test loss: 0.8011 score: 0.3023 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.1818;  Loss pred: 0.1818; Loss self: 0.0000; time: 0.15s
Val loss: 0.7691 score: 0.4091 time: 0.04s
Test loss: 0.7795 score: 0.2326 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.1636;  Loss pred: 0.1636; Loss self: 0.0000; time: 0.15s
Val loss: 0.7503 score: 0.4545 time: 0.04s
Test loss: 0.7609 score: 0.2558 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.1445;  Loss pred: 0.1445; Loss self: 0.0000; time: 0.16s
Val loss: 0.7368 score: 0.4318 time: 0.04s
Test loss: 0.7459 score: 0.2326 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.1280;  Loss pred: 0.1280; Loss self: 0.0000; time: 0.15s
Val loss: 0.7268 score: 0.4318 time: 0.04s
Test loss: 0.7355 score: 0.3023 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.1236;  Loss pred: 0.1236; Loss self: 0.0000; time: 0.15s
Val loss: 0.7199 score: 0.4773 time: 0.04s
Test loss: 0.7279 score: 0.3488 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.15s
Val loss: 0.7143 score: 0.4545 time: 0.04s
Test loss: 0.7215 score: 0.3953 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 0.0963;  Loss pred: 0.0963; Loss self: 0.0000; time: 0.15s
Val loss: 0.7094 score: 0.4318 time: 0.04s
Test loss: 0.7153 score: 0.3953 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.0888;  Loss pred: 0.0888; Loss self: 0.0000; time: 0.15s
Val loss: 0.7049 score: 0.4545 time: 0.04s
Test loss: 0.7100 score: 0.4651 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.15s
Val loss: 0.7003 score: 0.5000 time: 0.05s
Test loss: 0.7055 score: 0.4651 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.0663;  Loss pred: 0.0663; Loss self: 0.0000; time: 0.15s
Val loss: 0.6946 score: 0.4773 time: 0.04s
Test loss: 0.7011 score: 0.4651 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 0.0626;  Loss pred: 0.0626; Loss self: 0.0000; time: 0.15s
Val loss: 0.6891 score: 0.4545 time: 0.04s
Test loss: 0.6963 score: 0.4651 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.15s
Val loss: 0.6837 score: 0.4545 time: 0.04s
Test loss: 0.6914 score: 0.4884 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.0501;  Loss pred: 0.0501; Loss self: 0.0000; time: 0.15s
Val loss: 0.6771 score: 0.4545 time: 0.04s
Test loss: 0.6876 score: 0.4884 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.0459;  Loss pred: 0.0459; Loss self: 0.0000; time: 0.15s
Val loss: 0.6702 score: 0.4545 time: 0.04s
Test loss: 0.6830 score: 0.4651 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.0424;  Loss pred: 0.0424; Loss self: 0.0000; time: 0.15s
Val loss: 0.6631 score: 0.5227 time: 0.04s
Test loss: 0.6778 score: 0.4884 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6561 score: 0.5000 time: 0.05s
Test loss: 0.6737 score: 0.4884 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.0317;  Loss pred: 0.0317; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6498 score: 0.5000 time: 0.04s
Test loss: 0.6698 score: 0.4884 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6457 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6657 score: 0.5116 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 0.0274;  Loss pred: 0.0274; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6438 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6613 score: 0.5116 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6427 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6574 score: 0.5116 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6411 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6537 score: 0.5116 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6387 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6495 score: 0.5116 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6355 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6445 score: 0.5116 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6314 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6382 score: 0.5116 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6277 score: 0.5000 time: 0.04s
Test loss: 0.6319 score: 0.5349 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6285 score: 0.5000 time: 0.04s
Test loss: 0.6293 score: 0.5581 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6313 score: 0.5000 time: 0.04s
Test loss: 0.6276 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6377 score: 0.5000 time: 0.04s
Test loss: 0.6289 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6434 score: 0.5000 time: 0.04s
Test loss: 0.6285 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0126;  Loss pred: 0.0126; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6466 score: 0.5000 time: 0.04s
Test loss: 0.6275 score: 0.5581 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6459 score: 0.5000 time: 0.04s
Test loss: 0.6242 score: 0.5581 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6427 score: 0.5000 time: 0.04s
Test loss: 0.6187 score: 0.5581 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6375 score: 0.5000 time: 0.04s
Test loss: 0.6122 score: 0.5814 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6287 score: 0.5000 time: 0.04s
Test loss: 0.6034 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.15s
Val loss: 0.6167 score: 0.5227 time: 0.04s
Test loss: 0.5931 score: 0.6047 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.15s
Val loss: 0.6053 score: 0.5682 time: 0.04s
Test loss: 0.5844 score: 0.6047 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.15s
Val loss: 0.5921 score: 0.5909 time: 0.04s
Test loss: 0.5750 score: 0.6279 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.15s
Val loss: 0.5791 score: 0.5909 time: 0.04s
Test loss: 0.5662 score: 0.6744 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.15s
Val loss: 0.5655 score: 0.5909 time: 0.04s
Test loss: 0.5571 score: 0.6512 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.15s
Val loss: 0.5518 score: 0.6364 time: 0.04s
Test loss: 0.5488 score: 0.6744 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.15s
Val loss: 0.5403 score: 0.6818 time: 0.04s
Test loss: 0.5420 score: 0.6744 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.15s
Val loss: 0.5273 score: 0.7045 time: 0.04s
Test loss: 0.5344 score: 0.6744 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.15s
Val loss: 0.5186 score: 0.7500 time: 0.04s
Test loss: 0.5304 score: 0.6744 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.15s
Val loss: 0.5124 score: 0.7273 time: 0.04s
Test loss: 0.5284 score: 0.6977 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.15s
Val loss: 0.5036 score: 0.7045 time: 0.04s
Test loss: 0.5250 score: 0.6744 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.15s
Val loss: 0.4905 score: 0.7045 time: 0.04s
Test loss: 0.5190 score: 0.6744 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.15s
Val loss: 0.4752 score: 0.7500 time: 0.04s
Test loss: 0.5116 score: 0.6977 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.15s
Val loss: 0.4589 score: 0.7500 time: 0.04s
Test loss: 0.5031 score: 0.6977 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.15s
Val loss: 0.4415 score: 0.7955 time: 0.04s
Test loss: 0.4949 score: 0.6977 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.15s
Val loss: 0.4273 score: 0.7955 time: 0.04s
Test loss: 0.4890 score: 0.7442 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.15s
Val loss: 0.4131 score: 0.8409 time: 0.04s
Test loss: 0.4843 score: 0.7442 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.16s
Val loss: 0.3968 score: 0.8409 time: 0.04s
Test loss: 0.4801 score: 0.7674 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.15s
Val loss: 0.3815 score: 0.8409 time: 0.04s
Test loss: 0.4770 score: 0.7907 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.15s
Val loss: 0.3682 score: 0.8636 time: 0.04s
Test loss: 0.4752 score: 0.8140 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.15s
Val loss: 0.3592 score: 0.8636 time: 0.04s
Test loss: 0.4737 score: 0.8140 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.15s
Val loss: 0.3504 score: 0.8636 time: 0.04s
Test loss: 0.4724 score: 0.8140 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.15s
Val loss: 0.3428 score: 0.8636 time: 0.04s
Test loss: 0.4712 score: 0.8140 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.15s
Val loss: 0.3363 score: 0.9091 time: 0.04s
Test loss: 0.4704 score: 0.8372 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.15s
Val loss: 0.3300 score: 0.9091 time: 0.04s
Test loss: 0.4690 score: 0.8140 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.15s
Val loss: 0.3235 score: 0.9091 time: 0.04s
Test loss: 0.4692 score: 0.7907 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.15s
Val loss: 0.3190 score: 0.9091 time: 0.04s
Test loss: 0.4680 score: 0.7907 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.15s
Val loss: 0.3137 score: 0.9091 time: 0.04s
Test loss: 0.4690 score: 0.8140 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.15s
Val loss: 0.3090 score: 0.8864 time: 0.04s
Test loss: 0.4714 score: 0.8140 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.15s
Val loss: 0.3062 score: 0.8636 time: 0.04s
Test loss: 0.4741 score: 0.8140 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.15s
Val loss: 0.3059 score: 0.8636 time: 0.04s
Test loss: 0.4758 score: 0.8140 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.15s
Val loss: 0.3064 score: 0.8636 time: 0.04s
Test loss: 0.4793 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.15s
Val loss: 0.3080 score: 0.8636 time: 0.04s
Test loss: 0.4857 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.15s
Val loss: 0.3099 score: 0.8636 time: 0.04s
Test loss: 0.4914 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.15s
Val loss: 0.3136 score: 0.8636 time: 0.04s
Test loss: 0.4995 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.15s
Val loss: 0.3170 score: 0.8636 time: 0.04s
Test loss: 0.5069 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.15s
Val loss: 0.3211 score: 0.8636 time: 0.04s
Test loss: 0.5137 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.15s
Val loss: 0.3266 score: 0.8636 time: 0.04s
Test loss: 0.5231 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.15s
Val loss: 0.3360 score: 0.8636 time: 0.04s
Test loss: 0.5342 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.15s
Val loss: 0.3463 score: 0.8636 time: 0.04s
Test loss: 0.5447 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.15s
Val loss: 0.3547 score: 0.8636 time: 0.04s
Test loss: 0.5548 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.15s
Val loss: 0.3606 score: 0.8636 time: 0.04s
Test loss: 0.5634 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.15s
Val loss: 0.3664 score: 0.8636 time: 0.04s
Test loss: 0.5743 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.15s
Val loss: 0.3716 score: 0.8636 time: 0.04s
Test loss: 0.5824 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.15s
Val loss: 0.3783 score: 0.8636 time: 0.04s
Test loss: 0.5914 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.15s
Val loss: 0.3866 score: 0.8864 time: 0.04s
Test loss: 0.6033 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.15s
Val loss: 0.3944 score: 0.8864 time: 0.04s
Test loss: 0.6141 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.15s
Val loss: 0.4023 score: 0.8864 time: 0.04s
Test loss: 0.6243 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.15s
Val loss: 0.4115 score: 0.8636 time: 0.05s
Test loss: 0.6362 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.15s
Val loss: 0.4190 score: 0.8636 time: 0.05s
Test loss: 0.6436 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.15s
Val loss: 0.4281 score: 0.8636 time: 0.05s
Test loss: 0.6527 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 076,   Train_Loss: 0.0029,   Val_Loss: 0.3059,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3059,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4758


[0.05634695792105049, 0.055449633044190705, 0.054845440899953246, 0.055756825953722, 0.05552349297795445, 0.05554869002662599, 0.05555638193618506, 0.05512944399379194, 0.05528483004309237, 0.05525871098507196, 0.056264269980601966, 0.055052267969585955, 0.0546871799742803, 0.05448615492787212, 0.05493574100546539, 0.0541823951061815, 0.05465300497598946, 0.05449016997590661, 0.05619608098641038, 0.054530161898583174, 0.055166018079034984, 0.05431446898728609, 0.05462204199284315, 0.05421569396276027, 0.054416383034549654, 0.054967593983747065, 0.05501973000355065, 0.060359888011589646, 0.059467777027748525, 0.09962164901662618, 0.07356239401269704, 0.2383197050075978, 0.06387459801044315, 0.07665849500335753, 0.05965879198629409, 0.0792895199265331, 0.0684199120150879, 0.05790492706000805, 0.05867801303975284, 0.058604560093954206, 0.05815091100521386, 0.058425317984074354, 0.05841778090689331, 0.05844970198813826, 0.0586075980681926, 0.058674469008110464, 0.0422240870539099, 0.04279062000568956, 0.042377146892249584, 0.04276011197362095, 0.05241102597210556, 0.04239049204625189, 0.042746506980620325, 0.04317852796521038, 0.042605520924553275, 0.04265572305303067, 0.04269029991701245, 0.0424938159994781, 0.04281620401889086, 0.04573768295813352, 0.04306335700675845, 0.04271447297651321, 0.042605043039657176, 0.04283484700135887, 0.042733918060548604, 0.04326092905830592, 0.06566467101220042, 0.043595410068519413, 0.04381072404794395, 0.04372496600262821, 0.04359633498825133, 0.04331413400359452, 0.043282691040076315, 0.05211375106591731, 0.046821063035167754, 0.043794889003038406, 0.043249586946330965, 0.04380450095050037, 0.043183133006095886, 0.043764095986261964, 0.04371455893851817, 0.04328102804720402, 0.04334682400804013, 0.043194131925702095, 0.043590951012447476, 0.04389983299188316, 0.043778833001852036, 0.044643713044933975, 0.04270769807044417, 0.042900166008621454, 0.04280517401639372, 0.05337121395859867, 0.04347223602235317, 0.04388771904632449, 0.04302542598452419, 0.04279339499771595, 0.04302323202136904, 0.04249427793547511, 0.042771194013766944, 0.04291411896701902, 0.04300701804459095, 0.04337902495171875, 0.04344772605691105, 0.04302315297536552, 0.042936595040373504, 0.043195809004828334, 0.04308990004938096, 0.04367505304981023, 0.04825416405219585, 0.04320831492077559, 0.04306625097524375, 0.04279756301548332, 0.04285144805908203, 0.04305475705768913, 0.04288655205164105, 0.04296511597931385, 0.04311405494809151, 0.043010429944843054, 0.04286277701612562, 0.043035076931118965, 0.04312910791486502, 0.053509572986513376, 0.04297267901711166, 0.04287529503926635, 0.04284580599050969, 0.04328124795574695, 0.04301011702045798, 0.04304052295628935, 0.04287408594973385, 0.044134367955848575, 0.0437637030845508, 0.04310668900143355, 0.04266948101576418, 0.042681250954046845, 0.04285715299192816, 0.04281779401935637, 0.0423721739789471, 0.04276943299919367, 0.04230525891762227, 0.04255582997575402, 0.05285515100695193, 0.05248296493664384, 0.052687404095195234]
[0.0012806126800238747, 0.001260218932822516, 0.0012464872931807556, 0.0012672005898573182, 0.001261897567680783, 0.0012624702278778634, 0.001262645044004206, 0.0012529419089498167, 0.001256473410070281, 0.0012558797951152717, 0.0012787334086500448, 0.0012511879083996807, 0.001242890453960916, 0.0012383217029061846, 0.0012485395683060315, 0.0012314180705950341, 0.0012421137494543059, 0.0012384129539978776, 0.0012771836587820542, 0.0012393218613314357, 0.001253773138159886, 0.0012344197497110474, 0.00124141004529189, 0.001232174862790006, 0.0012367359780579466, 0.0012492634996306151, 0.0012504484091716056, 0.0013718156366270375, 0.0013515403869942847, 0.002264128386741504, 0.00167187259119766, 0.005416356931990859, 0.0014516954093282534, 0.0017422385228035803, 0.0013558816360521384, 0.0018020345437848432, 0.0015549980003429068, 0.0013160210695456374, 0.0013335912054489281, 0.001331921820317141, 0.0013216116137548604, 0.00132784813600169, 0.00132767683879303, 0.0013284023179122332, 0.0013319908651861954, 0.001333510659275238, 0.0009819555128816255, 0.000995130697806734, 0.0009855150440058044, 0.0009944212086888593, 0.001218861069118734, 0.0009858253964244626, 0.0009941048135027983, 0.0010041518131444274, 0.0009908260680128668, 0.0009919935593728064, 0.0009927976724886617, 0.0009882282790576304, 0.000995725674857927, 0.0010636670455379889, 0.0010014734187618244, 0.0009933598366630978, 0.000990814954410632, 0.000996159232589741, 0.000993812047919735, 0.0010060681176350215, 0.0015270853723767539, 0.0010138467457795211, 0.0010188540476266034, 0.0010168596744797257, 0.0010138682555407286, 0.0010073054419440586, 0.0010065742102343329, 0.0012119476992073794, 0.001088861931050413, 0.001018485790768335, 0.0010058043475890922, 0.0010187093244302412, 0.001004258907118509, 0.0010177696740991155, 0.0010166176497329806, 0.0010065355359814888, 0.0010080656746055843, 0.0010045146959465603, 0.0010137430468011041, 0.0010209263486484455, 0.0010181123953919079, 0.0010382258847659064, 0.0009932022807080038, 0.0009976782792702663, 0.000995469163171947, 0.0012411910222929923, 0.0010109822330779809, 0.0010206446289842905, 0.001000591301965679, 0.0009951952325050221, 0.0010005402795667218, 0.0009882390217552351, 0.0009946789305527197, 0.000998002766674861, 0.0010001632103393244, 0.0010088145337609012, 0.0010104122338816524, 0.0010005384412875703, 0.0009985254660551978, 0.0010045536977867054, 0.001002090698822813, 0.0010156989081351216, 0.0011221898616789732, 0.001004844533041293, 0.0010015407203545058, 0.0009952921631507748, 0.0009965453036995821, 0.0010012734199462588, 0.0009973616756195592, 0.0009991887437049733, 0.001002652440653291, 0.0010002425568568151, 0.0009968087678168748, 0.001000815742584162, 0.0010030025096480237, 0.0012444086741049622, 0.0009993646283049224, 0.0009970998846341012, 0.000996414092802551, 0.00100654065013365, 0.0010002352795455344, 0.0010009423943323104, 0.0009970717662728804, 0.0010263806501360133, 0.0010177605368500186, 0.0010024811395682222, 0.0009923135119945157, 0.0009925872314894616, 0.0009966779765564688, 0.0009957626516129388, 0.000985399394859235, 0.000994637976725434, 0.0009838432306423783, 0.000989670464552419, 0.0012291895583012077, 0.0012205340682940428, 0.0012252884673301218]
[780.8762286980922, 793.512915855261, 802.2544677918253, 789.1410468113781, 792.4573480539159, 792.097887077258, 791.988219292982, 798.1215991395594, 795.878362395321, 796.2545491132886, 782.0238317349488, 799.2404604349476, 804.5761368696184, 807.5445965722206, 800.9357695861894, 812.0718900257729, 805.0792453101232, 807.4850935399, 782.9727487694436, 806.8928913475906, 797.5924587662334, 810.0972138805132, 805.535611535085, 811.5731218016459, 808.58001848568, 800.4716381257293, 799.7131210414973, 728.9609283494959, 739.896498560371, 441.67106682460854, 598.131702897074, 184.62594185653782, 688.8497363663446, 573.9742216185285, 737.5275049167687, 554.9283189098492, 643.0876437008155, 759.8662537714953, 749.8549749834087, 750.7948174930359, 756.6519464511042, 753.098169050506, 753.1953339707909, 752.7839921053717, 750.7558994109263, 749.9002674215587, 1018.3760739480157, 1004.8931283136958, 1014.6978537591054, 1005.6100888259375, 820.438051010214, 1014.3784118637518, 1005.9301458127234, 995.8653531367669, 1009.2588722514455, 1008.0710610986788, 1007.2545773533938, 1011.911945035205, 1004.292673424016, 940.1438205638995, 998.528749006992, 1006.6845498396713, 1009.2701927322358, 1003.8555757800627, 1006.2264812478554, 993.9684823237558, 654.8422361243636, 986.3423679790235, 981.4948493648099, 983.4198612622218, 986.3214421943488, 992.7475404779316, 993.4687277227156, 825.1181141347978, 918.3900837044704, 981.8497313012197, 994.2291484392514, 981.6342856774132, 995.7591542496457, 982.5405742071806, 983.6539826577423, 993.5068999077953, 991.9988599862403, 995.5055949258103, 986.443264055452, 979.5025873549557, 982.2098272510123, 963.1815336847189, 1006.84424454518, 1002.3271236609779, 1004.551458744956, 805.6777579268878, 989.1370661929984, 979.7729509390205, 999.4090474657163, 1004.8279647430422, 999.4600121776648, 1011.900945000002, 1005.3495346928919, 1002.0012302488834, 999.8368162939436, 991.2624833743819, 989.6950635270397, 999.4618484754295, 1001.4767114058969, 995.4669443786445, 997.9136630793311, 984.543738297459, 891.1148052111629, 995.1788233084872, 998.4616498129399, 1004.7301054137929, 1003.4666726014287, 998.7281995897512, 1002.6453035492885, 1000.8119149661541, 997.3545761764041, 999.7575019627465, 1003.2014487494068, 999.1849223094197, 997.0064784293737, 803.5945271108363, 1000.6357756489294, 1002.9085504978904, 1003.5988122040338, 993.5018519790716, 999.7647757979089, 999.058492938608, 1002.9368334619138, 974.2974011323017, 982.5493952585472, 997.52500124911, 1007.7460277549126, 1007.4681280147187, 1003.3330960668046, 1004.2553799142772, 1014.8169414523039, 1005.3909295643615, 1016.4220973976438, 1010.4373484079395, 813.5441708291459, 819.3134677491745, 816.1343444119566]
Elapsed: 0.050160347243080605~0.018067449198200563
Time per graph: 0.001155830434659237~0.0004056935039244118
Speed: 903.4904391664213~136.18806043823406
Total Time: 0.0534
best val loss: 0.30588698387145996 test_score: 0.8140

Testing...
Test loss: 0.4704 score: 0.8372 time: 0.05s
test Score 0.8372
Epoch Time List: [0.22510718100238591, 0.21324360405560583, 0.21399372396990657, 0.21300871390849352, 0.2130030218977481, 0.21418408409226686, 0.2125342229846865, 0.21161293401382864, 0.21294486697297543, 0.2126772899646312, 0.2161224300507456, 0.29766715492587537, 0.21386800485197455, 0.2115746820345521, 0.21327114303130656, 0.21143510600086302, 0.21261684398632497, 0.21168283000588417, 0.3178139260271564, 0.213214545045048, 0.21209195908159018, 0.21046793705318123, 0.21201773511711508, 0.20954467705450952, 0.2099147809203714, 0.21205098810605705, 0.23859599907882512, 0.2593261309666559, 0.23381322401110083, 0.34521425887942314, 0.2635196888586506, 0.4397176749771461, 0.2508827759884298, 0.27088973799254745, 0.32057094294577837, 0.26476964994799346, 0.31752500508446246, 0.3132835761643946, 0.22949904017150402, 0.22933115810155869, 0.23035897687077522, 0.22716045600827783, 0.2276837658137083, 0.2282171129481867, 0.22978435608092695, 0.22926328494213521, 0.22433554206509143, 0.22509928396902978, 0.2253035989124328, 0.22402816906105727, 0.23521590803284198, 0.2248864999273792, 0.2256264319876209, 0.2260058110114187, 0.22627552191261202, 0.2260861418908462, 0.22605193604249507, 0.2250866701360792, 0.22608480008784682, 0.2301158361369744, 0.2338142580119893, 0.22711884696036577, 0.2260465620784089, 0.2258883158210665, 0.2263171070953831, 0.22747127094771713, 0.2660194670315832, 0.23157436703331769, 0.23141660611145198, 0.23274277593009174, 0.23095772811211646, 0.23044460488017648, 0.23043718305416405, 0.2506836369866505, 0.24932171497493982, 0.23388530500233173, 0.22825072996784002, 0.2297104662284255, 0.2283400190062821, 0.23009397485293448, 0.22885676205623895, 0.22894724004436284, 0.22988194797653705, 0.2295683390693739, 0.23009657801594585, 0.23102158890105784, 0.23124358511995524, 0.23362787393853068, 0.2272378409979865, 0.22696025285404176, 0.2266634500119835, 0.23741285607684404, 0.22690177103504539, 0.22824977606069297, 0.2281102209817618, 0.22676356695592403, 0.22612082597333938, 0.22649046208243817, 0.22736662800889462, 0.22663232206832618, 0.2267109618987888, 0.2268860440235585, 0.22829464101232588, 0.22681483218912035, 0.22664691100362688, 0.227487982949242, 0.22819755203090608, 0.22904631192795932, 0.23583295207936317, 0.23317440191749483, 0.22739225497934967, 0.22673536895308644, 0.22695568995550275, 0.22705535613931715, 0.22775262698996812, 0.22620711708441377, 0.22735665703658015, 0.22658058698289096, 0.22724364011082798, 0.22723198600579053, 0.22620159306097776, 0.23858366895001382, 0.22743743297178298, 0.22743180487304926, 0.22584853996522725, 0.22670133388601243, 0.22826621704734862, 0.22823386208619922, 0.2280220709508285, 0.22962302702944726, 0.23272266401909292, 0.22697250405326486, 0.22614944900851697, 0.22575361700728536, 0.2258227369748056, 0.22783063200768083, 0.226085250149481, 0.22511971893254668, 0.22643752512522042, 0.22609740495681763, 0.24666852003429085, 0.24579159601125866, 0.24610766302794218]
Total Epoch List: [46, 97]
Total Time List: [0.05915001092944294, 0.053368323016911745]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7156e8ad2530>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7271;  Loss pred: 0.7271; Loss self: 0.0000; time: 0.15s
Val loss: 0.6560 score: 0.5000 time: 0.05s
Test loss: 0.7212 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7463;  Loss pred: 0.7463; Loss self: 0.0000; time: 0.15s
Val loss: 0.6402 score: 0.5455 time: 0.05s
Test loss: 0.6958 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.15s
Val loss: 0.6464 score: 0.6591 time: 0.05s
Test loss: 0.6866 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6381;  Loss pred: 0.6381; Loss self: 0.0000; time: 0.16s
Val loss: 0.6720 score: 0.6364 time: 0.05s
Test loss: 0.6943 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5770;  Loss pred: 0.5770; Loss self: 0.0000; time: 0.15s
Val loss: 0.7115 score: 0.5227 time: 0.05s
Test loss: 0.7139 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5106;  Loss pred: 0.5106; Loss self: 0.0000; time: 0.15s
Val loss: 0.7537 score: 0.4091 time: 0.05s
Test loss: 0.7372 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4556;  Loss pred: 0.4556; Loss self: 0.0000; time: 0.15s
Val loss: 0.7926 score: 0.4091 time: 0.05s
Test loss: 0.7627 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.3998;  Loss pred: 0.3998; Loss self: 0.0000; time: 0.15s
Val loss: 0.8186 score: 0.4545 time: 0.05s
Test loss: 0.7855 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3484;  Loss pred: 0.3484; Loss self: 0.0000; time: 0.15s
Val loss: 0.8337 score: 0.4773 time: 0.05s
Test loss: 0.8074 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3129;  Loss pred: 0.3129; Loss self: 0.0000; time: 0.15s
Val loss: 0.8453 score: 0.4091 time: 0.05s
Test loss: 0.8250 score: 0.3721 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2720;  Loss pred: 0.2720; Loss self: 0.0000; time: 0.15s
Val loss: 0.8459 score: 0.3864 time: 0.05s
Test loss: 0.8365 score: 0.3953 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2477;  Loss pred: 0.2477; Loss self: 0.0000; time: 0.15s
Val loss: 0.8483 score: 0.4091 time: 0.05s
Test loss: 0.8387 score: 0.3488 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2150;  Loss pred: 0.2150; Loss self: 0.0000; time: 0.15s
Val loss: 0.8476 score: 0.4091 time: 0.05s
Test loss: 0.8364 score: 0.3023 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.1890;  Loss pred: 0.1890; Loss self: 0.0000; time: 0.15s
Val loss: 0.8483 score: 0.4318 time: 0.05s
Test loss: 0.8373 score: 0.3023 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1700;  Loss pred: 0.1700; Loss self: 0.0000; time: 0.15s
Val loss: 0.8527 score: 0.4318 time: 0.05s
Test loss: 0.8389 score: 0.3256 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1558;  Loss pred: 0.1558; Loss self: 0.0000; time: 0.15s
Val loss: 0.8603 score: 0.4318 time: 0.05s
Test loss: 0.8426 score: 0.3023 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1371;  Loss pred: 0.1371; Loss self: 0.0000; time: 0.15s
Val loss: 0.8692 score: 0.4545 time: 0.05s
Test loss: 0.8465 score: 0.3256 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 0.15s
Val loss: 0.8779 score: 0.4318 time: 0.05s
Test loss: 0.8511 score: 0.3023 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1126;  Loss pred: 0.1126; Loss self: 0.0000; time: 0.15s
Val loss: 0.8854 score: 0.4545 time: 0.05s
Test loss: 0.8574 score: 0.3256 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1045;  Loss pred: 0.1045; Loss self: 0.0000; time: 0.15s
Val loss: 0.8943 score: 0.4545 time: 0.05s
Test loss: 0.8636 score: 0.3721 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0925;  Loss pred: 0.0925; Loss self: 0.0000; time: 0.15s
Val loss: 0.9070 score: 0.4545 time: 0.05s
Test loss: 0.8703 score: 0.3953 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0859;  Loss pred: 0.0859; Loss self: 0.0000; time: 0.15s
Val loss: 0.9197 score: 0.4545 time: 0.05s
Test loss: 0.8761 score: 0.4186 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 001,   Train_Loss: 0.7463,   Val_Loss: 0.6402,   Val_Precision: 0.5263,   Val_Recall: 0.9091,   Val_accuracy: 0.6667,   Val_Score: 0.5455,   Val_Loss: 0.6402,   Test_Precision: 0.5000,   Test_Recall: 0.9048,   Test_accuracy: 0.6441,   Test_Score: 0.5116,   Test_loss: 0.6958


[0.05634695792105049, 0.055449633044190705, 0.054845440899953246, 0.055756825953722, 0.05552349297795445, 0.05554869002662599, 0.05555638193618506, 0.05512944399379194, 0.05528483004309237, 0.05525871098507196, 0.056264269980601966, 0.055052267969585955, 0.0546871799742803, 0.05448615492787212, 0.05493574100546539, 0.0541823951061815, 0.05465300497598946, 0.05449016997590661, 0.05619608098641038, 0.054530161898583174, 0.055166018079034984, 0.05431446898728609, 0.05462204199284315, 0.05421569396276027, 0.054416383034549654, 0.054967593983747065, 0.05501973000355065, 0.060359888011589646, 0.059467777027748525, 0.09962164901662618, 0.07356239401269704, 0.2383197050075978, 0.06387459801044315, 0.07665849500335753, 0.05965879198629409, 0.0792895199265331, 0.0684199120150879, 0.05790492706000805, 0.05867801303975284, 0.058604560093954206, 0.05815091100521386, 0.058425317984074354, 0.05841778090689331, 0.05844970198813826, 0.0586075980681926, 0.058674469008110464, 0.0422240870539099, 0.04279062000568956, 0.042377146892249584, 0.04276011197362095, 0.05241102597210556, 0.04239049204625189, 0.042746506980620325, 0.04317852796521038, 0.042605520924553275, 0.04265572305303067, 0.04269029991701245, 0.0424938159994781, 0.04281620401889086, 0.04573768295813352, 0.04306335700675845, 0.04271447297651321, 0.042605043039657176, 0.04283484700135887, 0.042733918060548604, 0.04326092905830592, 0.06566467101220042, 0.043595410068519413, 0.04381072404794395, 0.04372496600262821, 0.04359633498825133, 0.04331413400359452, 0.043282691040076315, 0.05211375106591731, 0.046821063035167754, 0.043794889003038406, 0.043249586946330965, 0.04380450095050037, 0.043183133006095886, 0.043764095986261964, 0.04371455893851817, 0.04328102804720402, 0.04334682400804013, 0.043194131925702095, 0.043590951012447476, 0.04389983299188316, 0.043778833001852036, 0.044643713044933975, 0.04270769807044417, 0.042900166008621454, 0.04280517401639372, 0.05337121395859867, 0.04347223602235317, 0.04388771904632449, 0.04302542598452419, 0.04279339499771595, 0.04302323202136904, 0.04249427793547511, 0.042771194013766944, 0.04291411896701902, 0.04300701804459095, 0.04337902495171875, 0.04344772605691105, 0.04302315297536552, 0.042936595040373504, 0.043195809004828334, 0.04308990004938096, 0.04367505304981023, 0.04825416405219585, 0.04320831492077559, 0.04306625097524375, 0.04279756301548332, 0.04285144805908203, 0.04305475705768913, 0.04288655205164105, 0.04296511597931385, 0.04311405494809151, 0.043010429944843054, 0.04286277701612562, 0.043035076931118965, 0.04312910791486502, 0.053509572986513376, 0.04297267901711166, 0.04287529503926635, 0.04284580599050969, 0.04328124795574695, 0.04301011702045798, 0.04304052295628935, 0.04287408594973385, 0.044134367955848575, 0.0437637030845508, 0.04310668900143355, 0.04266948101576418, 0.042681250954046845, 0.04285715299192816, 0.04281779401935637, 0.0423721739789471, 0.04276943299919367, 0.04230525891762227, 0.04255582997575402, 0.05285515100695193, 0.05248296493664384, 0.052687404095195234, 0.05083843891043216, 0.050719802966341376, 0.05165862699504942, 0.04999908397439867, 0.05014008900616318, 0.05046175606548786, 0.05014285200741142, 0.05059064400848001, 0.05075458693318069, 0.0506452489644289, 0.050695897080004215, 0.05043207900598645, 0.050370178068988025, 0.0504073390038684, 0.05021815199870616, 0.05068212398327887, 0.05033028102479875, 0.050186509964987636, 0.05028089601546526, 0.05020958301611245, 0.05058460996951908, 0.05075731500983238]
[0.0012806126800238747, 0.001260218932822516, 0.0012464872931807556, 0.0012672005898573182, 0.001261897567680783, 0.0012624702278778634, 0.001262645044004206, 0.0012529419089498167, 0.001256473410070281, 0.0012558797951152717, 0.0012787334086500448, 0.0012511879083996807, 0.001242890453960916, 0.0012383217029061846, 0.0012485395683060315, 0.0012314180705950341, 0.0012421137494543059, 0.0012384129539978776, 0.0012771836587820542, 0.0012393218613314357, 0.001253773138159886, 0.0012344197497110474, 0.00124141004529189, 0.001232174862790006, 0.0012367359780579466, 0.0012492634996306151, 0.0012504484091716056, 0.0013718156366270375, 0.0013515403869942847, 0.002264128386741504, 0.00167187259119766, 0.005416356931990859, 0.0014516954093282534, 0.0017422385228035803, 0.0013558816360521384, 0.0018020345437848432, 0.0015549980003429068, 0.0013160210695456374, 0.0013335912054489281, 0.001331921820317141, 0.0013216116137548604, 0.00132784813600169, 0.00132767683879303, 0.0013284023179122332, 0.0013319908651861954, 0.001333510659275238, 0.0009819555128816255, 0.000995130697806734, 0.0009855150440058044, 0.0009944212086888593, 0.001218861069118734, 0.0009858253964244626, 0.0009941048135027983, 0.0010041518131444274, 0.0009908260680128668, 0.0009919935593728064, 0.0009927976724886617, 0.0009882282790576304, 0.000995725674857927, 0.0010636670455379889, 0.0010014734187618244, 0.0009933598366630978, 0.000990814954410632, 0.000996159232589741, 0.000993812047919735, 0.0010060681176350215, 0.0015270853723767539, 0.0010138467457795211, 0.0010188540476266034, 0.0010168596744797257, 0.0010138682555407286, 0.0010073054419440586, 0.0010065742102343329, 0.0012119476992073794, 0.001088861931050413, 0.001018485790768335, 0.0010058043475890922, 0.0010187093244302412, 0.001004258907118509, 0.0010177696740991155, 0.0010166176497329806, 0.0010065355359814888, 0.0010080656746055843, 0.0010045146959465603, 0.0010137430468011041, 0.0010209263486484455, 0.0010181123953919079, 0.0010382258847659064, 0.0009932022807080038, 0.0009976782792702663, 0.000995469163171947, 0.0012411910222929923, 0.0010109822330779809, 0.0010206446289842905, 0.001000591301965679, 0.0009951952325050221, 0.0010005402795667218, 0.0009882390217552351, 0.0009946789305527197, 0.000998002766674861, 0.0010001632103393244, 0.0010088145337609012, 0.0010104122338816524, 0.0010005384412875703, 0.0009985254660551978, 0.0010045536977867054, 0.001002090698822813, 0.0010156989081351216, 0.0011221898616789732, 0.001004844533041293, 0.0010015407203545058, 0.0009952921631507748, 0.0009965453036995821, 0.0010012734199462588, 0.0009973616756195592, 0.0009991887437049733, 0.001002652440653291, 0.0010002425568568151, 0.0009968087678168748, 0.001000815742584162, 0.0010030025096480237, 0.0012444086741049622, 0.0009993646283049224, 0.0009970998846341012, 0.000996414092802551, 0.00100654065013365, 0.0010002352795455344, 0.0010009423943323104, 0.0009970717662728804, 0.0010263806501360133, 0.0010177605368500186, 0.0010024811395682222, 0.0009923135119945157, 0.0009925872314894616, 0.0009966779765564688, 0.0009957626516129388, 0.000985399394859235, 0.000994637976725434, 0.0009838432306423783, 0.000989670464552419, 0.0012291895583012077, 0.0012205340682940428, 0.0012252884673301218, 0.0011822892769867944, 0.0011795303015428226, 0.0012013634184895212, 0.0011627693947534575, 0.0011660485815386786, 0.001173529210825299, 0.001166112837381661, 0.0011765266048483723, 0.0011803392310042022, 0.0011777964875448583, 0.0011789743506977724, 0.0011728390466508476, 0.0011713994899764657, 0.0011722636977643814, 0.0011678639999699108, 0.0011786540461227644, 0.0011704716517395059, 0.0011671281387206427, 0.0011693231631503549, 0.0011676647213049406, 0.0011763862783609087, 0.0011804026746472648]
[780.8762286980922, 793.512915855261, 802.2544677918253, 789.1410468113781, 792.4573480539159, 792.097887077258, 791.988219292982, 798.1215991395594, 795.878362395321, 796.2545491132886, 782.0238317349488, 799.2404604349476, 804.5761368696184, 807.5445965722206, 800.9357695861894, 812.0718900257729, 805.0792453101232, 807.4850935399, 782.9727487694436, 806.8928913475906, 797.5924587662334, 810.0972138805132, 805.535611535085, 811.5731218016459, 808.58001848568, 800.4716381257293, 799.7131210414973, 728.9609283494959, 739.896498560371, 441.67106682460854, 598.131702897074, 184.62594185653782, 688.8497363663446, 573.9742216185285, 737.5275049167687, 554.9283189098492, 643.0876437008155, 759.8662537714953, 749.8549749834087, 750.7948174930359, 756.6519464511042, 753.098169050506, 753.1953339707909, 752.7839921053717, 750.7558994109263, 749.9002674215587, 1018.3760739480157, 1004.8931283136958, 1014.6978537591054, 1005.6100888259375, 820.438051010214, 1014.3784118637518, 1005.9301458127234, 995.8653531367669, 1009.2588722514455, 1008.0710610986788, 1007.2545773533938, 1011.911945035205, 1004.292673424016, 940.1438205638995, 998.528749006992, 1006.6845498396713, 1009.2701927322358, 1003.8555757800627, 1006.2264812478554, 993.9684823237558, 654.8422361243636, 986.3423679790235, 981.4948493648099, 983.4198612622218, 986.3214421943488, 992.7475404779316, 993.4687277227156, 825.1181141347978, 918.3900837044704, 981.8497313012197, 994.2291484392514, 981.6342856774132, 995.7591542496457, 982.5405742071806, 983.6539826577423, 993.5068999077953, 991.9988599862403, 995.5055949258103, 986.443264055452, 979.5025873549557, 982.2098272510123, 963.1815336847189, 1006.84424454518, 1002.3271236609779, 1004.551458744956, 805.6777579268878, 989.1370661929984, 979.7729509390205, 999.4090474657163, 1004.8279647430422, 999.4600121776648, 1011.900945000002, 1005.3495346928919, 1002.0012302488834, 999.8368162939436, 991.2624833743819, 989.6950635270397, 999.4618484754295, 1001.4767114058969, 995.4669443786445, 997.9136630793311, 984.543738297459, 891.1148052111629, 995.1788233084872, 998.4616498129399, 1004.7301054137929, 1003.4666726014287, 998.7281995897512, 1002.6453035492885, 1000.8119149661541, 997.3545761764041, 999.7575019627465, 1003.2014487494068, 999.1849223094197, 997.0064784293737, 803.5945271108363, 1000.6357756489294, 1002.9085504978904, 1003.5988122040338, 993.5018519790716, 999.7647757979089, 999.058492938608, 1002.9368334619138, 974.2974011323017, 982.5493952585472, 997.52500124911, 1007.7460277549126, 1007.4681280147187, 1003.3330960668046, 1004.2553799142772, 1014.8169414523039, 1005.3909295643615, 1016.4220973976438, 1010.4373484079395, 813.5441708291459, 819.3134677491745, 816.1343444119566, 845.8166875611183, 847.7950915648394, 832.3875894750513, 860.0157559289995, 857.5972012078893, 852.1304717219075, 857.549945376947, 849.959529923998, 847.2140667130295, 849.0431161707073, 848.1948732880855, 852.6319130110769, 853.6797297223433, 853.0503861094525, 856.2640855662682, 848.4253740862682, 854.3564455523907, 856.8039505038054, 855.1955793861385, 856.4102192643411, 850.0609182498519, 847.1685311106452]
Elapsed: 0.05020627727111181~0.01682075985371196
Time per graph: 0.0011583238106684384~0.00037774496891250525
Speed: 896.5508137108691~128.03003564704906
Total Time: 0.0514
best val loss: 0.6401925683021545 test_score: 0.5116

Testing...
Test loss: 0.6866 score: 0.5581 time: 0.05s
test Score 0.5581
Epoch Time List: [0.22510718100238591, 0.21324360405560583, 0.21399372396990657, 0.21300871390849352, 0.2130030218977481, 0.21418408409226686, 0.2125342229846865, 0.21161293401382864, 0.21294486697297543, 0.2126772899646312, 0.2161224300507456, 0.29766715492587537, 0.21386800485197455, 0.2115746820345521, 0.21327114303130656, 0.21143510600086302, 0.21261684398632497, 0.21168283000588417, 0.3178139260271564, 0.213214545045048, 0.21209195908159018, 0.21046793705318123, 0.21201773511711508, 0.20954467705450952, 0.2099147809203714, 0.21205098810605705, 0.23859599907882512, 0.2593261309666559, 0.23381322401110083, 0.34521425887942314, 0.2635196888586506, 0.4397176749771461, 0.2508827759884298, 0.27088973799254745, 0.32057094294577837, 0.26476964994799346, 0.31752500508446246, 0.3132835761643946, 0.22949904017150402, 0.22933115810155869, 0.23035897687077522, 0.22716045600827783, 0.2276837658137083, 0.2282171129481867, 0.22978435608092695, 0.22926328494213521, 0.22433554206509143, 0.22509928396902978, 0.2253035989124328, 0.22402816906105727, 0.23521590803284198, 0.2248864999273792, 0.2256264319876209, 0.2260058110114187, 0.22627552191261202, 0.2260861418908462, 0.22605193604249507, 0.2250866701360792, 0.22608480008784682, 0.2301158361369744, 0.2338142580119893, 0.22711884696036577, 0.2260465620784089, 0.2258883158210665, 0.2263171070953831, 0.22747127094771713, 0.2660194670315832, 0.23157436703331769, 0.23141660611145198, 0.23274277593009174, 0.23095772811211646, 0.23044460488017648, 0.23043718305416405, 0.2506836369866505, 0.24932171497493982, 0.23388530500233173, 0.22825072996784002, 0.2297104662284255, 0.2283400190062821, 0.23009397485293448, 0.22885676205623895, 0.22894724004436284, 0.22988194797653705, 0.2295683390693739, 0.23009657801594585, 0.23102158890105784, 0.23124358511995524, 0.23362787393853068, 0.2272378409979865, 0.22696025285404176, 0.2266634500119835, 0.23741285607684404, 0.22690177103504539, 0.22824977606069297, 0.2281102209817618, 0.22676356695592403, 0.22612082597333938, 0.22649046208243817, 0.22736662800889462, 0.22663232206832618, 0.2267109618987888, 0.2268860440235585, 0.22829464101232588, 0.22681483218912035, 0.22664691100362688, 0.227487982949242, 0.22819755203090608, 0.22904631192795932, 0.23583295207936317, 0.23317440191749483, 0.22739225497934967, 0.22673536895308644, 0.22695568995550275, 0.22705535613931715, 0.22775262698996812, 0.22620711708441377, 0.22735665703658015, 0.22658058698289096, 0.22724364011082798, 0.22723198600579053, 0.22620159306097776, 0.23858366895001382, 0.22743743297178298, 0.22743180487304926, 0.22584853996522725, 0.22670133388601243, 0.22826621704734862, 0.22823386208619922, 0.2280220709508285, 0.22962302702944726, 0.23272266401909292, 0.22697250405326486, 0.22614944900851697, 0.22575361700728536, 0.2258227369748056, 0.22783063200768083, 0.226085250149481, 0.22511971893254668, 0.22643752512522042, 0.22609740495681763, 0.24666852003429085, 0.24579159601125866, 0.24610766302794218, 0.24520311306696385, 0.24459764582570642, 0.24572089593857527, 0.2539651180850342, 0.24359316600020975, 0.24348469509277493, 0.2423013140214607, 0.24296972306910902, 0.24306291504763067, 0.24456977401860058, 0.24294404301326722, 0.24333420197945088, 0.24498983600642532, 0.24412385805044323, 0.24313717102631927, 0.24322548985946923, 0.24404276499990374, 0.24278633506037295, 0.24408508301712573, 0.2442715469514951, 0.24375200597569346, 0.24336201697587967]
Total Epoch List: [46, 97, 22]
Total Time List: [0.05915001092944294, 0.053368323016911745, 0.05139878101181239]
T-times Epoch Time: 0.23661317315469774 ~ 0.005109365646561665
T-times Total Epoch: 60.333333333333336 ~ 3.7810443393719013
T-times Total Time: 0.05276093263334284 ~ 0.0016625842671817114
T-times Inference Elapsed: 0.050008151077278094 ~ 0.0020370420237282552
T-times Time Per Graph: 0.0011554221398935962 ~ 4.770557283109964e-05
T-times Speed: 881.5821200022532 ~ 39.000076472678955
T-times cross validation test micro f1 score:0.7050206175746411 ~ 0.0160716280060556
T-times cross validation test precision:0.6370975998699041 ~ 0.014054552888438102
T-times cross validation test recall:0.8573833573833575 ~ 0.04624488815725721
T-times cross validation test f1_score:0.7050206175746411 ~ 0.019644561773182544
