Namespace(seed=15, model='AEtransGAT', dataset='mining/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/averVolume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 382], edge_attr=[382, 2], x=[103, 14887], y=[1, 1], num_nodes=115)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351dbab3d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.3501;  Loss pred: 2.3501; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.3281;  Loss pred: 2.3281; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.3342;  Loss pred: 2.3342; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 2.2931;  Loss pred: 2.2931; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 2.2829;  Loss pred: 2.2829; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 2.3255;  Loss pred: 2.3255; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 2.2922;  Loss pred: 2.2922; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 2.2651;  Loss pred: 2.2651; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 2.2196;  Loss pred: 2.2196; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.71s
Epoch 10/1000, LR 0.000240
Train loss: 2.1474;  Loss pred: 2.1474; Loss self: 0.0000; time: 2.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.31s
Epoch 11/1000, LR 0.000270
Train loss: 2.1198;  Loss pred: 2.1198; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.13s
Epoch 12/1000, LR 0.000270
Train loss: 2.1302;  Loss pred: 2.1302; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 2.0965;  Loss pred: 2.0965; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 2.0258;  Loss pred: 2.0258; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 2.0355;  Loss pred: 2.0355; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 16/1000, LR 0.000270
Train loss: 1.9894;  Loss pred: 1.9894; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 013,   Train_Loss: 2.0258,   Val_Loss: 0.6935,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6935,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6931


[0.05545937595888972, 0.057529293932020664, 0.05470380000770092, 0.05550090002361685, 0.057805703952908516, 0.060653330059722066, 0.055280742002651095, 0.05565055401530117, 0.7109664009185508, 0.31898007704876363, 0.1344758050981909, 0.06007817306090146, 0.05518483591731638, 0.05276909493841231, 0.0538266699295491, 0.05469510005787015]
[0.0012604403627020392, 0.0013074839530004697, 0.0012432681819932027, 0.0012613840914458376, 0.001313765998929739, 0.0013784847740845923, 0.0012563805000602522, 0.001264785318529572, 0.016158327293603426, 0.007249547205653719, 0.003056268297686157, 0.001365413024111397, 0.0012542008163026449, 0.0011992976122366433, 0.0012233334074897523, 0.0012430704558606851]
[793.3735142028248, 764.8277424018532, 804.3316916522418, 792.7799365645789, 761.170559151821, 725.434200507632, 795.9372180259428, 790.6480138167567, 61.8875940454473, 137.93964941977728, 327.1964050921449, 732.3791280303587, 797.320482494962, 833.821388283296, 817.4386425463305, 804.4596308160293]
Elapsed: 0.11834749105764786~0.16623907475156044
Time per graph: 0.002689715705855633~0.0037781607898081924
Speed: 671.3091123157499~244.47224299137483
Total Time: 0.0551
best val loss: 0.6934881210327148 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.05s
test Score 0.5000
Epoch Time List: [0.480402200948447, 0.23165051499381661, 0.23635498306248337, 0.24537496489938349, 0.2494306720327586, 0.2522010870743543, 0.23866619099862874, 0.2368397390237078, 0.8923679920844734, 3.482520340010524, 0.6687864249106497, 0.2591526519972831, 0.24343741813208908, 0.22690718597732484, 0.23128005699254572, 0.2292608447605744]
Total Epoch List: [16]
Total Time List: [0.05510198697447777]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351db2dfc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.4514;  Loss pred: 2.4514; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.4423;  Loss pred: 2.4423; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.4852;  Loss pred: 2.4852; Loss self: 0.0000; time: 2.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5116 time: 1.29s
Epoch 4/1000, LR 0.000060
Train loss: 2.5084;  Loss pred: 2.5084; Loss self: 0.0000; time: 2.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5116 time: 0.51s
Epoch 5/1000, LR 0.000090
Train loss: 2.4796;  Loss pred: 2.4796; Loss self: 0.0000; time: 0.95s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.26s
Epoch 6/1000, LR 0.000120
Train loss: 2.4159;  Loss pred: 2.4159; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 2.3663;  Loss pred: 2.3663; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 2.3757;  Loss pred: 2.3757; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 2.3001;  Loss pred: 2.3001; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 2.2908;  Loss pred: 2.2908; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 2.2926;  Loss pred: 2.2926; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 2.2280;  Loss pred: 2.2280; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 2.1723;  Loss pred: 2.1723; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 2.1119;  Loss pred: 2.1119; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 1.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 1.82s
Epoch 15/1000, LR 0.000270
Train loss: 2.1279;  Loss pred: 2.1279; Loss self: 0.0000; time: 2.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 1.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.84s
Epoch 16/1000, LR 0.000270
Train loss: 2.0837;  Loss pred: 2.0837; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 2.0247;  Loss pred: 2.0247; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 1.9956;  Loss pred: 1.9956; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 1.9790;  Loss pred: 1.9790; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 1.9348;  Loss pred: 1.9348; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 1.9122;  Loss pred: 1.9122; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 1.8856;  Loss pred: 1.8856; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 1.8381;  Loss pred: 1.8381; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 1.8090;  Loss pred: 1.8090; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 1.7972;  Loss pred: 1.7972; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 1.7776;  Loss pred: 1.7776; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 1.7195;  Loss pred: 1.7195; Loss self: 0.0000; time: 0.14s
Val loss: 0.6930 score: 0.5000 time: 0.04s
Test loss: 0.6930 score: 0.5581 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 1.7131;  Loss pred: 1.7131; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 1.6822;  Loss pred: 1.6822; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 1.6523;  Loss pred: 1.6523; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 1.6120;  Loss pred: 1.6120; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.14s
Epoch 32/1000, LR 0.000270
Train loss: 1.5874;  Loss pred: 1.5874; Loss self: 0.0000; time: 5.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.93s
Epoch 33/1000, LR 0.000270
Train loss: 1.5850;  Loss pred: 1.5850; Loss self: 0.0000; time: 1.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.43s
Epoch 34/1000, LR 0.000270
Train loss: 1.5676;  Loss pred: 1.5676; Loss self: 0.0000; time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 1.5489;  Loss pred: 1.5489; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 1.5139;  Loss pred: 1.5139; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 1.4824;  Loss pred: 1.4824; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.10s
Epoch 38/1000, LR 0.000270
Train loss: 1.5106;  Loss pred: 1.5106; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 1.4715;  Loss pred: 1.4715; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 1.4192;  Loss pred: 1.4192; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 1.4358;  Loss pred: 1.4358; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 1.4233;  Loss pred: 1.4233; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 1.4194;  Loss pred: 1.4194; Loss self: 0.0000; time: 2.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 2.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 3.57s
Epoch 44/1000, LR 0.000269
Train loss: 1.3856;  Loss pred: 1.3856; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 1.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 1.3630;  Loss pred: 1.3630; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 1.3580;  Loss pred: 1.3580; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 1.3478;  Loss pred: 1.3478; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 1.3327;  Loss pred: 1.3327; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 1.3209;  Loss pred: 1.3209; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 1.3069;  Loss pred: 1.3069; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 1.2963;  Loss pred: 1.2963; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 1.2807;  Loss pred: 1.2807; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 1.2700;  Loss pred: 1.2700; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 1.2641;  Loss pred: 1.2641; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 1.2463;  Loss pred: 1.2463; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 1.2454;  Loss pred: 1.2454; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 1.2343;  Loss pred: 1.2343; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 1.2269;  Loss pred: 1.2269; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 1.2134;  Loss pred: 1.2134; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 1.2091;  Loss pred: 1.2091; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 1.1991;  Loss pred: 1.1991; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 1.1927;  Loss pred: 1.1927; Loss self: 0.0000; time: 3.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 1.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 1.24s
Epoch 63/1000, LR 0.000268
Train loss: 1.1826;  Loss pred: 1.1826; Loss self: 0.0000; time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 1.1790;  Loss pred: 1.1790; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 1.1697;  Loss pred: 1.1697; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 1.1633;  Loss pred: 1.1633; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 1.1496;  Loss pred: 1.1496; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 1.1493;  Loss pred: 1.1493; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.1426;  Loss pred: 1.1426; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 1.1358;  Loss pred: 1.1358; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 1.1280;  Loss pred: 1.1280; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 1.1210;  Loss pred: 1.1210; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 1.1265;  Loss pred: 1.1265; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4884 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 1.1222;  Loss pred: 1.1222; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4884 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 1.1080;  Loss pred: 1.1080; Loss self: 0.0000; time: 1.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 1.78s
Epoch 76/1000, LR 0.000267
Train loss: 1.1029;  Loss pred: 1.1029; Loss self: 0.0000; time: 2.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.38s
Epoch 77/1000, LR 0.000267
Train loss: 1.1054;  Loss pred: 1.1054; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4884 time: 0.40s
Epoch 78/1000, LR 0.000267
Train loss: 1.0961;  Loss pred: 1.0961; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.05s
Epoch 79/1000, LR 0.000267
Train loss: 1.0924;  Loss pred: 1.0924; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4884 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 1.0923;  Loss pred: 1.0923; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.05s
Epoch 81/1000, LR 0.000267
Train loss: 1.0875;  Loss pred: 1.0875; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.05s
Epoch 82/1000, LR 0.000267
Train loss: 1.0867;  Loss pred: 1.0867; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4884 time: 0.05s
Epoch 83/1000, LR 0.000266
Train loss: 1.0827;  Loss pred: 1.0827; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.05s
Epoch 84/1000, LR 0.000266
Train loss: 1.0784;  Loss pred: 1.0784; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4884 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 1.0809;  Loss pred: 1.0809; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4884 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 1.0717;  Loss pred: 1.0717; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.4884 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 1.0688;  Loss pred: 1.0688; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4884 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 1.0640;  Loss pred: 1.0640; Loss self: 0.0000; time: 3.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4884 time: 1.79s
Epoch 89/1000, LR 0.000266
Train loss: 1.0635;  Loss pred: 1.0635; Loss self: 0.0000; time: 1.94s
Val loss: 0.6862 score: 0.5227 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4884 time: 0.26s
Epoch 90/1000, LR 0.000266
Train loss: 1.0611;  Loss pred: 1.0611; Loss self: 0.0000; time: 2.12s
Val loss: 0.6860 score: 0.5227 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4884 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 1.0531;  Loss pred: 1.0531; Loss self: 0.0000; time: 0.15s
Val loss: 0.6857 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.4884 time: 0.05s
Epoch 92/1000, LR 0.000266
Train loss: 1.0536;  Loss pred: 1.0536; Loss self: 0.0000; time: 0.15s
Val loss: 0.6854 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.4884 time: 0.05s
Epoch 93/1000, LR 0.000265
Train loss: 1.0492;  Loss pred: 1.0492; Loss self: 0.0000; time: 0.15s
Val loss: 0.6851 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.4884 time: 0.05s
Epoch 94/1000, LR 0.000265
Train loss: 1.0467;  Loss pred: 1.0467; Loss self: 0.0000; time: 0.15s
Val loss: 0.6848 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.4884 time: 0.05s
Epoch 95/1000, LR 0.000265
Train loss: 1.0444;  Loss pred: 1.0444; Loss self: 0.0000; time: 0.15s
Val loss: 0.6845 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.4884 time: 0.05s
Epoch 96/1000, LR 0.000265
Train loss: 1.0433;  Loss pred: 1.0433; Loss self: 0.0000; time: 0.16s
Val loss: 0.6842 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.4884 time: 0.05s
Epoch 97/1000, LR 0.000265
Train loss: 1.0451;  Loss pred: 1.0451; Loss self: 0.0000; time: 0.15s
Val loss: 0.6839 score: 0.5227 time: 0.05s
Test loss: 0.6858 score: 0.5116 time: 0.05s
Epoch 98/1000, LR 0.000265
Train loss: 1.0393;  Loss pred: 1.0393; Loss self: 0.0000; time: 0.15s
Val loss: 0.6835 score: 0.5227 time: 0.05s
Test loss: 0.6855 score: 0.5116 time: 0.05s
Epoch 99/1000, LR 0.000265
Train loss: 1.0360;  Loss pred: 1.0360; Loss self: 0.0000; time: 0.15s
Val loss: 0.6832 score: 0.5227 time: 0.05s
Test loss: 0.6852 score: 0.5116 time: 0.05s
Epoch 100/1000, LR 0.000265
Train loss: 1.0361;  Loss pred: 1.0361; Loss self: 0.0000; time: 0.15s
Val loss: 0.6828 score: 0.5455 time: 0.05s
Test loss: 0.6849 score: 0.5116 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 1.0321;  Loss pred: 1.0321; Loss self: 0.0000; time: 0.15s
Val loss: 0.6824 score: 0.5455 time: 0.05s
Test loss: 0.6846 score: 0.5116 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 1.0318;  Loss pred: 1.0318; Loss self: 0.0000; time: 0.25s
Val loss: 0.6821 score: 0.5455 time: 0.44s
Test loss: 0.6843 score: 0.5116 time: 1.49s
Epoch 103/1000, LR 0.000264
Train loss: 1.0301;  Loss pred: 1.0301; Loss self: 0.0000; time: 4.09s
Val loss: 0.6817 score: 0.5455 time: 0.06s
Test loss: 0.6840 score: 0.5349 time: 0.21s
Epoch 104/1000, LR 0.000264
Train loss: 1.0253;  Loss pred: 1.0253; Loss self: 0.0000; time: 0.33s
Val loss: 0.6813 score: 0.5455 time: 0.05s
Test loss: 0.6837 score: 0.5349 time: 0.05s
Epoch 105/1000, LR 0.000264
Train loss: 1.0248;  Loss pred: 1.0248; Loss self: 0.0000; time: 0.25s
Val loss: 0.6809 score: 0.5455 time: 0.06s
Test loss: 0.6833 score: 0.5349 time: 0.05s
Epoch 106/1000, LR 0.000264
Train loss: 1.0220;  Loss pred: 1.0220; Loss self: 0.0000; time: 0.16s
Val loss: 0.6805 score: 0.5455 time: 0.04s
Test loss: 0.6830 score: 0.5349 time: 0.05s
Epoch 107/1000, LR 0.000264
Train loss: 1.0221;  Loss pred: 1.0221; Loss self: 0.0000; time: 0.16s
Val loss: 0.6800 score: 0.5455 time: 0.05s
Test loss: 0.6826 score: 0.5349 time: 0.06s
Epoch 108/1000, LR 0.000264
Train loss: 1.0204;  Loss pred: 1.0204; Loss self: 0.0000; time: 0.16s
Val loss: 0.6796 score: 0.5455 time: 0.05s
Test loss: 0.6823 score: 0.5349 time: 0.05s
Epoch 109/1000, LR 0.000264
Train loss: 1.0163;  Loss pred: 1.0163; Loss self: 0.0000; time: 0.15s
Val loss: 0.6791 score: 0.5455 time: 0.05s
Test loss: 0.6819 score: 0.5349 time: 0.05s
Epoch 110/1000, LR 0.000263
Train loss: 1.0172;  Loss pred: 1.0172; Loss self: 0.0000; time: 0.15s
Val loss: 0.6787 score: 0.5455 time: 0.05s
Test loss: 0.6815 score: 0.5349 time: 0.10s
Epoch 111/1000, LR 0.000263
Train loss: 1.0146;  Loss pred: 1.0146; Loss self: 0.0000; time: 0.17s
Val loss: 0.6782 score: 0.5455 time: 0.06s
Test loss: 0.6811 score: 0.5349 time: 0.05s
Epoch 112/1000, LR 0.000263
Train loss: 1.0104;  Loss pred: 1.0104; Loss self: 0.0000; time: 0.17s
Val loss: 0.6777 score: 0.5455 time: 0.05s
Test loss: 0.6807 score: 0.5581 time: 0.05s
Epoch 113/1000, LR 0.000263
Train loss: 1.0089;  Loss pred: 1.0089; Loss self: 0.0000; time: 0.15s
Val loss: 0.6772 score: 0.5909 time: 0.05s
Test loss: 0.6803 score: 0.5581 time: 0.05s
Epoch 114/1000, LR 0.000263
Train loss: 1.0112;  Loss pred: 1.0112; Loss self: 0.0000; time: 0.15s
Val loss: 0.6767 score: 0.5909 time: 0.05s
Test loss: 0.6799 score: 0.5581 time: 0.05s
Epoch 115/1000, LR 0.000263
Train loss: 1.0072;  Loss pred: 1.0072; Loss self: 0.0000; time: 0.15s
Val loss: 0.6761 score: 0.5909 time: 0.05s
Test loss: 0.6795 score: 0.5581 time: 1.54s
Epoch 116/1000, LR 0.000263
Train loss: 1.0079;  Loss pred: 1.0079; Loss self: 0.0000; time: 5.91s
Val loss: 0.6756 score: 0.6136 time: 0.33s
Test loss: 0.6790 score: 0.5581 time: 0.31s
Epoch 117/1000, LR 0.000262
Train loss: 1.0049;  Loss pred: 1.0049; Loss self: 0.0000; time: 0.86s
Val loss: 0.6751 score: 0.6364 time: 0.07s
Test loss: 0.6786 score: 0.5581 time: 0.19s
Epoch 118/1000, LR 0.000262
Train loss: 1.0021;  Loss pred: 1.0021; Loss self: 0.0000; time: 0.16s
Val loss: 0.6745 score: 0.6591 time: 0.05s
Test loss: 0.6781 score: 0.5581 time: 0.05s
Epoch 119/1000, LR 0.000262
Train loss: 1.0011;  Loss pred: 1.0011; Loss self: 0.0000; time: 0.16s
Val loss: 0.6739 score: 0.6591 time: 0.05s
Test loss: 0.6777 score: 0.5581 time: 0.05s
Epoch 120/1000, LR 0.000262
Train loss: 1.0002;  Loss pred: 1.0002; Loss self: 0.0000; time: 0.16s
Val loss: 0.6733 score: 0.6591 time: 0.05s
Test loss: 0.6772 score: 0.5581 time: 0.05s
Epoch 121/1000, LR 0.000262
Train loss: 0.9981;  Loss pred: 0.9981; Loss self: 0.0000; time: 0.16s
Val loss: 0.6727 score: 0.6591 time: 0.05s
Test loss: 0.6767 score: 0.5581 time: 0.05s
Epoch 122/1000, LR 0.000262
Train loss: 0.9979;  Loss pred: 0.9979; Loss self: 0.0000; time: 0.16s
Val loss: 0.6721 score: 0.6818 time: 0.05s
Test loss: 0.6762 score: 0.5581 time: 0.05s
Epoch 123/1000, LR 0.000262
Train loss: 0.9925;  Loss pred: 0.9925; Loss self: 0.0000; time: 0.16s
Val loss: 0.6715 score: 0.6818 time: 0.05s
Test loss: 0.6757 score: 0.5581 time: 0.05s
Epoch 124/1000, LR 0.000261
Train loss: 0.9924;  Loss pred: 0.9924; Loss self: 0.0000; time: 0.16s
Val loss: 0.6708 score: 0.6818 time: 0.05s
Test loss: 0.6752 score: 0.5814 time: 0.05s
Epoch 125/1000, LR 0.000261
Train loss: 0.9930;  Loss pred: 0.9930; Loss self: 0.0000; time: 0.16s
Val loss: 0.6701 score: 0.6818 time: 0.05s
Test loss: 0.6746 score: 0.6279 time: 0.05s
Epoch 126/1000, LR 0.000261
Train loss: 0.9880;  Loss pred: 0.9880; Loss self: 0.0000; time: 0.16s
Val loss: 0.6695 score: 0.6818 time: 0.05s
Test loss: 0.6741 score: 0.6279 time: 0.05s
Epoch 127/1000, LR 0.000261
Train loss: 0.9877;  Loss pred: 0.9877; Loss self: 0.0000; time: 0.16s
Val loss: 0.6688 score: 0.6818 time: 0.05s
Test loss: 0.6735 score: 0.6279 time: 0.05s
Epoch 128/1000, LR 0.000261
Train loss: 0.9844;  Loss pred: 0.9844; Loss self: 0.0000; time: 0.16s
Val loss: 0.6680 score: 0.6818 time: 0.05s
Test loss: 0.6729 score: 0.6279 time: 0.05s
Epoch 129/1000, LR 0.000261
Train loss: 0.9868;  Loss pred: 0.9868; Loss self: 0.0000; time: 0.16s
Val loss: 0.6673 score: 0.6818 time: 0.05s
Test loss: 0.6723 score: 0.6279 time: 0.05s
Epoch 130/1000, LR 0.000260
Train loss: 0.9851;  Loss pred: 0.9851; Loss self: 0.0000; time: 0.15s
Val loss: 0.6665 score: 0.6818 time: 0.05s
Test loss: 0.6717 score: 0.6279 time: 0.38s
Epoch 131/1000, LR 0.000260
Train loss: 0.9815;  Loss pred: 0.9815; Loss self: 0.0000; time: 3.57s
Val loss: 0.6658 score: 0.6818 time: 0.74s
Test loss: 0.6710 score: 0.6279 time: 1.56s
Epoch 132/1000, LR 0.000260
Train loss: 0.9821;  Loss pred: 0.9821; Loss self: 0.0000; time: 1.83s
Val loss: 0.6650 score: 0.6818 time: 0.35s
Test loss: 0.6703 score: 0.6279 time: 1.01s
Epoch 133/1000, LR 0.000260
Train loss: 0.9785;  Loss pred: 0.9785; Loss self: 0.0000; time: 2.69s
Val loss: 0.6642 score: 0.6818 time: 0.05s
Test loss: 0.6697 score: 0.6279 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 0.9759;  Loss pred: 0.9759; Loss self: 0.0000; time: 0.15s
Val loss: 0.6633 score: 0.6818 time: 0.04s
Test loss: 0.6690 score: 0.6279 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9799;  Loss pred: 0.9799; Loss self: 0.0000; time: 0.15s
Val loss: 0.6625 score: 0.6818 time: 0.04s
Test loss: 0.6683 score: 0.6279 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9739;  Loss pred: 0.9739; Loss self: 0.0000; time: 0.15s
Val loss: 0.6616 score: 0.7273 time: 0.05s
Test loss: 0.6676 score: 0.6279 time: 0.04s
Epoch 137/1000, LR 0.000259
Train loss: 0.9729;  Loss pred: 0.9729; Loss self: 0.0000; time: 0.15s
Val loss: 0.6607 score: 0.7500 time: 0.05s
Test loss: 0.6668 score: 0.6279 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9740;  Loss pred: 0.9740; Loss self: 0.0000; time: 0.15s
Val loss: 0.6598 score: 0.7500 time: 0.04s
Test loss: 0.6661 score: 0.6279 time: 0.04s
Epoch 139/1000, LR 0.000259
Train loss: 0.9715;  Loss pred: 0.9715; Loss self: 0.0000; time: 0.14s
Val loss: 0.6589 score: 0.7500 time: 0.04s
Test loss: 0.6653 score: 0.6279 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9693;  Loss pred: 0.9693; Loss self: 0.0000; time: 0.14s
Val loss: 0.6579 score: 0.7500 time: 0.04s
Test loss: 0.6645 score: 0.6279 time: 0.04s
Epoch 141/1000, LR 0.000259
Train loss: 0.9697;  Loss pred: 0.9697; Loss self: 0.0000; time: 0.15s
Val loss: 0.6570 score: 0.7727 time: 0.04s
Test loss: 0.6637 score: 0.6279 time: 0.04s
Epoch 142/1000, LR 0.000259
Train loss: 0.9674;  Loss pred: 0.9674; Loss self: 0.0000; time: 0.15s
Val loss: 0.6560 score: 0.7727 time: 0.04s
Test loss: 0.6629 score: 0.6279 time: 0.05s
Epoch 143/1000, LR 0.000258
Train loss: 0.9655;  Loss pred: 0.9655; Loss self: 0.0000; time: 0.15s
Val loss: 0.6550 score: 0.7727 time: 0.05s
Test loss: 0.6620 score: 0.6279 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9635;  Loss pred: 0.9635; Loss self: 0.0000; time: 0.14s
Val loss: 0.6540 score: 0.7727 time: 0.05s
Test loss: 0.6612 score: 0.6279 time: 0.04s
Epoch 145/1000, LR 0.000258
Train loss: 0.9628;  Loss pred: 0.9628; Loss self: 0.0000; time: 0.15s
Val loss: 0.6529 score: 0.7727 time: 0.04s
Test loss: 0.6603 score: 0.6279 time: 0.06s
Epoch 146/1000, LR 0.000258
Train loss: 0.9588;  Loss pred: 0.9588; Loss self: 0.0000; time: 0.15s
Val loss: 0.6519 score: 0.7955 time: 0.04s
Test loss: 0.6595 score: 0.6279 time: 0.04s
Epoch 147/1000, LR 0.000258
Train loss: 0.9604;  Loss pred: 0.9604; Loss self: 0.0000; time: 0.15s
Val loss: 0.6508 score: 0.7955 time: 0.04s
Test loss: 0.6586 score: 0.6279 time: 0.05s
Epoch 148/1000, LR 0.000257
Train loss: 0.9576;  Loss pred: 0.9576; Loss self: 0.0000; time: 0.15s
Val loss: 0.6497 score: 0.7955 time: 0.05s
Test loss: 0.6577 score: 0.6279 time: 0.05s
Epoch 149/1000, LR 0.000257
Train loss: 0.9563;  Loss pred: 0.9563; Loss self: 0.0000; time: 0.16s
Val loss: 0.6486 score: 0.7955 time: 0.05s
Test loss: 0.6568 score: 0.6279 time: 0.05s
Epoch 150/1000, LR 0.000257
Train loss: 0.9542;  Loss pred: 0.9542; Loss self: 0.0000; time: 0.16s
Val loss: 0.6474 score: 0.7955 time: 0.05s
Test loss: 0.6558 score: 0.6279 time: 0.05s
Epoch 151/1000, LR 0.000257
Train loss: 0.9524;  Loss pred: 0.9524; Loss self: 0.0000; time: 0.16s
Val loss: 0.6463 score: 0.7955 time: 0.05s
Test loss: 0.6548 score: 0.6279 time: 0.05s
Epoch 152/1000, LR 0.000257
Train loss: 0.9498;  Loss pred: 0.9498; Loss self: 0.0000; time: 0.15s
Val loss: 0.6451 score: 0.7955 time: 0.05s
Test loss: 0.6538 score: 0.6279 time: 2.23s
Epoch 153/1000, LR 0.000257
Train loss: 0.9487;  Loss pred: 0.9487; Loss self: 0.0000; time: 5.16s
Val loss: 0.6439 score: 0.7955 time: 2.00s
Test loss: 0.6528 score: 0.6279 time: 0.35s
Epoch 154/1000, LR 0.000256
Train loss: 0.9485;  Loss pred: 0.9485; Loss self: 0.0000; time: 0.16s
Val loss: 0.6426 score: 0.7955 time: 0.05s
Test loss: 0.6517 score: 0.6512 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9450;  Loss pred: 0.9450; Loss self: 0.0000; time: 0.15s
Val loss: 0.6413 score: 0.7955 time: 0.05s
Test loss: 0.6507 score: 0.6512 time: 0.05s
Epoch 156/1000, LR 0.000256
Train loss: 0.9440;  Loss pred: 0.9440; Loss self: 0.0000; time: 0.15s
Val loss: 0.6400 score: 0.7955 time: 0.05s
Test loss: 0.6496 score: 0.6512 time: 0.05s
Epoch 157/1000, LR 0.000256
Train loss: 0.9424;  Loss pred: 0.9424; Loss self: 0.0000; time: 0.16s
Val loss: 0.6387 score: 0.7955 time: 0.05s
Test loss: 0.6485 score: 0.6744 time: 0.05s
Epoch 158/1000, LR 0.000256
Train loss: 0.9391;  Loss pred: 0.9391; Loss self: 0.0000; time: 0.15s
Val loss: 0.6374 score: 0.7955 time: 0.05s
Test loss: 0.6475 score: 0.6744 time: 0.05s
Epoch 159/1000, LR 0.000255
Train loss: 0.9384;  Loss pred: 0.9384; Loss self: 0.0000; time: 0.15s
Val loss: 0.6361 score: 0.7955 time: 0.05s
Test loss: 0.6464 score: 0.6744 time: 0.05s
Epoch 160/1000, LR 0.000255
Train loss: 0.9376;  Loss pred: 0.9376; Loss self: 0.0000; time: 0.15s
Val loss: 0.6348 score: 0.7955 time: 0.05s
Test loss: 0.6452 score: 0.6744 time: 0.05s
Epoch 161/1000, LR 0.000255
Train loss: 0.9352;  Loss pred: 0.9352; Loss self: 0.0000; time: 0.15s
Val loss: 0.6334 score: 0.7955 time: 0.05s
Test loss: 0.6441 score: 0.6744 time: 0.05s
Epoch 162/1000, LR 0.000255
Train loss: 0.9345;  Loss pred: 0.9345; Loss self: 0.0000; time: 0.15s
Val loss: 0.6320 score: 0.7955 time: 0.05s
Test loss: 0.6430 score: 0.6744 time: 0.05s
Epoch 163/1000, LR 0.000255
Train loss: 0.9308;  Loss pred: 0.9308; Loss self: 0.0000; time: 0.15s
Val loss: 0.6306 score: 0.7955 time: 0.05s
Test loss: 0.6418 score: 0.6744 time: 0.05s
Epoch 164/1000, LR 0.000254
Train loss: 0.9303;  Loss pred: 0.9303; Loss self: 0.0000; time: 0.15s
Val loss: 0.6292 score: 0.7955 time: 0.05s
Test loss: 0.6406 score: 0.6744 time: 0.05s
Epoch 165/1000, LR 0.000254
Train loss: 0.9264;  Loss pred: 0.9264; Loss self: 0.0000; time: 0.15s
Val loss: 0.6277 score: 0.7955 time: 0.05s
Test loss: 0.6394 score: 0.6744 time: 0.05s
Epoch 166/1000, LR 0.000254
Train loss: 0.9268;  Loss pred: 0.9268; Loss self: 0.0000; time: 0.15s
Val loss: 0.6263 score: 0.7955 time: 0.05s
Test loss: 0.6382 score: 0.6744 time: 0.05s
Epoch 167/1000, LR 0.000254
Train loss: 0.9245;  Loss pred: 0.9245; Loss self: 0.0000; time: 0.16s
Val loss: 0.6248 score: 0.7955 time: 1.96s
Test loss: 0.6370 score: 0.6744 time: 1.97s
Epoch 168/1000, LR 0.000254
Train loss: 0.9230;  Loss pred: 0.9230; Loss self: 0.0000; time: 3.96s
Val loss: 0.6232 score: 0.7955 time: 0.06s
Test loss: 0.6357 score: 0.6744 time: 0.05s
Epoch 169/1000, LR 0.000253
Train loss: 0.9207;  Loss pred: 0.9207; Loss self: 0.0000; time: 0.17s
Val loss: 0.6217 score: 0.7955 time: 0.05s
Test loss: 0.6344 score: 0.6977 time: 0.05s
Epoch 170/1000, LR 0.000253
Train loss: 0.9184;  Loss pred: 0.9184; Loss self: 0.0000; time: 0.16s
Val loss: 0.6201 score: 0.7955 time: 0.05s
Test loss: 0.6331 score: 0.7209 time: 0.05s
Epoch 171/1000, LR 0.000253
Train loss: 0.9186;  Loss pred: 0.9186; Loss self: 0.0000; time: 0.17s
Val loss: 0.6185 score: 0.7955 time: 0.06s
Test loss: 0.6317 score: 0.7209 time: 0.05s
Epoch 172/1000, LR 0.000253
Train loss: 0.9152;  Loss pred: 0.9152; Loss self: 0.0000; time: 0.16s
Val loss: 0.6168 score: 0.7955 time: 0.05s
Test loss: 0.6303 score: 0.7209 time: 0.05s
Epoch 173/1000, LR 0.000253
Train loss: 0.9138;  Loss pred: 0.9138; Loss self: 0.0000; time: 0.16s
Val loss: 0.6151 score: 0.7955 time: 0.05s
Test loss: 0.6289 score: 0.7209 time: 0.05s
Epoch 174/1000, LR 0.000252
Train loss: 0.9097;  Loss pred: 0.9097; Loss self: 0.0000; time: 0.16s
Val loss: 0.6134 score: 0.7955 time: 0.05s
Test loss: 0.6274 score: 0.7209 time: 0.05s
Epoch 175/1000, LR 0.000252
Train loss: 0.9070;  Loss pred: 0.9070; Loss self: 0.0000; time: 0.16s
Val loss: 0.6116 score: 0.7955 time: 0.04s
Test loss: 0.6259 score: 0.7209 time: 0.04s
Epoch 176/1000, LR 0.000252
Train loss: 0.9064;  Loss pred: 0.9064; Loss self: 0.0000; time: 0.15s
Val loss: 0.6098 score: 0.7955 time: 0.05s
Test loss: 0.6244 score: 0.7209 time: 0.05s
Epoch 177/1000, LR 0.000252
Train loss: 0.9032;  Loss pred: 0.9032; Loss self: 0.0000; time: 0.15s
Val loss: 0.6080 score: 0.7955 time: 0.04s
Test loss: 0.6229 score: 0.7209 time: 0.04s
Epoch 178/1000, LR 0.000251
Train loss: 0.9008;  Loss pred: 0.9008; Loss self: 0.0000; time: 0.17s
Val loss: 0.6062 score: 0.7955 time: 0.04s
Test loss: 0.6213 score: 0.7209 time: 0.05s
Epoch 179/1000, LR 0.000251
Train loss: 0.8994;  Loss pred: 0.8994; Loss self: 0.0000; time: 0.15s
Val loss: 0.6043 score: 0.7955 time: 0.04s
Test loss: 0.6198 score: 0.7209 time: 0.04s
Epoch 180/1000, LR 0.000251
Train loss: 0.8975;  Loss pred: 0.8975; Loss self: 0.0000; time: 0.15s
Val loss: 0.6025 score: 0.7955 time: 0.04s
Test loss: 0.6183 score: 0.7209 time: 0.04s
Epoch 181/1000, LR 0.000251
Train loss: 0.8956;  Loss pred: 0.8956; Loss self: 0.0000; time: 0.15s
Val loss: 0.6007 score: 0.7955 time: 0.04s
Test loss: 0.6168 score: 0.7209 time: 0.05s
Epoch 182/1000, LR 0.000251
Train loss: 0.8928;  Loss pred: 0.8928; Loss self: 0.0000; time: 0.15s
Val loss: 0.5988 score: 0.7955 time: 0.05s
Test loss: 0.6152 score: 0.7209 time: 0.05s
Epoch 183/1000, LR 0.000250
Train loss: 0.8909;  Loss pred: 0.8909; Loss self: 0.0000; time: 0.16s
Val loss: 0.5970 score: 0.7955 time: 0.05s
Test loss: 0.6137 score: 0.7209 time: 0.05s
Epoch 184/1000, LR 0.000250
Train loss: 0.8877;  Loss pred: 0.8877; Loss self: 0.0000; time: 0.16s
Val loss: 0.5950 score: 0.7955 time: 0.05s
Test loss: 0.6120 score: 0.7209 time: 0.04s
Epoch 185/1000, LR 0.000250
Train loss: 0.8865;  Loss pred: 0.8865; Loss self: 0.0000; time: 0.14s
Val loss: 0.5931 score: 0.7955 time: 0.05s
Test loss: 0.6103 score: 0.7442 time: 0.04s
Epoch 186/1000, LR 0.000250
Train loss: 0.8842;  Loss pred: 0.8842; Loss self: 0.0000; time: 0.15s
Val loss: 0.5910 score: 0.7955 time: 0.04s
Test loss: 0.6085 score: 0.7674 time: 0.05s
Epoch 187/1000, LR 0.000249
Train loss: 0.8822;  Loss pred: 0.8822; Loss self: 0.0000; time: 0.15s
Val loss: 0.5889 score: 0.7955 time: 0.04s
Test loss: 0.6066 score: 0.7674 time: 0.14s
Epoch 188/1000, LR 0.000249
Train loss: 0.8782;  Loss pred: 0.8782; Loss self: 0.0000; time: 4.94s
Val loss: 0.5868 score: 0.8182 time: 0.17s
Test loss: 0.6047 score: 0.7674 time: 0.74s
Epoch 189/1000, LR 0.000249
Train loss: 0.8768;  Loss pred: 0.8768; Loss self: 0.0000; time: 0.77s
Val loss: 0.5847 score: 0.8182 time: 0.05s
Test loss: 0.6029 score: 0.7674 time: 0.05s
Epoch 190/1000, LR 0.000249
Train loss: 0.8738;  Loss pred: 0.8738; Loss self: 0.0000; time: 0.16s
Val loss: 0.5826 score: 0.8182 time: 0.05s
Test loss: 0.6010 score: 0.7674 time: 0.05s
Epoch 191/1000, LR 0.000249
Train loss: 0.8727;  Loss pred: 0.8727; Loss self: 0.0000; time: 0.16s
Val loss: 0.5805 score: 0.8182 time: 0.05s
Test loss: 0.5992 score: 0.7674 time: 0.05s
Epoch 192/1000, LR 0.000248
Train loss: 0.8701;  Loss pred: 0.8701; Loss self: 0.0000; time: 0.18s
Val loss: 0.5784 score: 0.8182 time: 0.11s
Test loss: 0.5974 score: 0.7674 time: 0.05s
Epoch 193/1000, LR 0.000248
Train loss: 0.8679;  Loss pred: 0.8679; Loss self: 0.0000; time: 0.15s
Val loss: 0.5764 score: 0.8182 time: 0.05s
Test loss: 0.5957 score: 0.7674 time: 0.05s
Epoch 194/1000, LR 0.000248
Train loss: 0.8653;  Loss pred: 0.8653; Loss self: 0.0000; time: 0.15s
Val loss: 0.5744 score: 0.8182 time: 0.05s
Test loss: 0.5940 score: 0.7674 time: 0.05s
Epoch 195/1000, LR 0.000248
Train loss: 0.8625;  Loss pred: 0.8625; Loss self: 0.0000; time: 0.16s
Val loss: 0.5724 score: 0.8182 time: 0.04s
Test loss: 0.5922 score: 0.7674 time: 0.05s
Epoch 196/1000, LR 0.000247
Train loss: 0.8614;  Loss pred: 0.8614; Loss self: 0.0000; time: 0.15s
Val loss: 0.5704 score: 0.8182 time: 0.05s
Test loss: 0.5905 score: 0.7674 time: 0.05s
Epoch 197/1000, LR 0.000247
Train loss: 0.8579;  Loss pred: 0.8579; Loss self: 0.0000; time: 0.15s
Val loss: 0.5683 score: 0.8182 time: 0.04s
Test loss: 0.5887 score: 0.7674 time: 0.05s
Epoch 198/1000, LR 0.000247
Train loss: 0.8547;  Loss pred: 0.8547; Loss self: 0.0000; time: 0.15s
Val loss: 0.5663 score: 0.8182 time: 0.04s
Test loss: 0.5870 score: 0.7674 time: 0.04s
Epoch 199/1000, LR 0.000247
Train loss: 0.8532;  Loss pred: 0.8532; Loss self: 0.0000; time: 0.15s
Val loss: 0.5643 score: 0.8182 time: 0.92s
Test loss: 0.5853 score: 0.7674 time: 2.01s
Epoch 200/1000, LR 0.000246
Train loss: 0.8504;  Loss pred: 0.8504; Loss self: 0.0000; time: 5.36s
Val loss: 0.5623 score: 0.8182 time: 0.05s
Test loss: 0.5836 score: 0.7674 time: 0.05s
Epoch 201/1000, LR 0.000246
Train loss: 0.8468;  Loss pred: 0.8468; Loss self: 0.0000; time: 0.15s
Val loss: 0.5603 score: 0.8182 time: 0.04s
Test loss: 0.5819 score: 0.7674 time: 0.05s
Epoch 202/1000, LR 0.000246
Train loss: 0.8469;  Loss pred: 0.8469; Loss self: 0.0000; time: 0.14s
Val loss: 0.5583 score: 0.8182 time: 0.04s
Test loss: 0.5802 score: 0.7674 time: 0.05s
Epoch 203/1000, LR 0.000246
Train loss: 0.8417;  Loss pred: 0.8417; Loss self: 0.0000; time: 0.15s
Val loss: 0.5562 score: 0.8182 time: 0.05s
Test loss: 0.5784 score: 0.7674 time: 0.05s
Epoch 204/1000, LR 0.000245
Train loss: 0.8415;  Loss pred: 0.8415; Loss self: 0.0000; time: 0.14s
Val loss: 0.5542 score: 0.8182 time: 0.04s
Test loss: 0.5767 score: 0.7674 time: 0.05s
Epoch 205/1000, LR 0.000245
Train loss: 0.8385;  Loss pred: 0.8385; Loss self: 0.0000; time: 0.15s
Val loss: 0.5522 score: 0.8182 time: 0.04s
Test loss: 0.5749 score: 0.7674 time: 0.05s
Epoch 206/1000, LR 0.000245
Train loss: 0.8352;  Loss pred: 0.8352; Loss self: 0.0000; time: 0.15s
Val loss: 0.5501 score: 0.8182 time: 0.05s
Test loss: 0.5731 score: 0.7907 time: 0.05s
Epoch 207/1000, LR 0.000245
Train loss: 0.8332;  Loss pred: 0.8332; Loss self: 0.0000; time: 0.14s
Val loss: 0.5479 score: 0.8182 time: 0.04s
Test loss: 0.5712 score: 0.7907 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.8303;  Loss pred: 0.8303; Loss self: 0.0000; time: 0.14s
Val loss: 0.5459 score: 0.8182 time: 0.04s
Test loss: 0.5693 score: 0.7907 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.8276;  Loss pred: 0.8276; Loss self: 0.0000; time: 0.15s
Val loss: 0.5437 score: 0.8182 time: 0.04s
Test loss: 0.5675 score: 0.7907 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.8259;  Loss pred: 0.8259; Loss self: 0.0000; time: 0.14s
Val loss: 0.5417 score: 0.8182 time: 0.04s
Test loss: 0.5656 score: 0.7907 time: 0.04s
Epoch 211/1000, LR 0.000244
Train loss: 0.8224;  Loss pred: 0.8224; Loss self: 0.0000; time: 0.15s
Val loss: 0.5397 score: 0.8182 time: 1.26s
Test loss: 0.5639 score: 0.7907 time: 1.95s
Epoch 212/1000, LR 0.000243
Train loss: 0.8208;  Loss pred: 0.8208; Loss self: 0.0000; time: 1.59s
Val loss: 0.5377 score: 0.8182 time: 0.05s
Test loss: 0.5621 score: 0.7907 time: 0.05s
Epoch 213/1000, LR 0.000243
Train loss: 0.8174;  Loss pred: 0.8174; Loss self: 0.0000; time: 0.16s
Val loss: 0.5358 score: 0.8182 time: 0.05s
Test loss: 0.5605 score: 0.7907 time: 0.05s
Epoch 214/1000, LR 0.000243
Train loss: 0.8149;  Loss pred: 0.8149; Loss self: 0.0000; time: 0.16s
Val loss: 0.5339 score: 0.8182 time: 0.05s
Test loss: 0.5589 score: 0.7907 time: 0.05s
Epoch 215/1000, LR 0.000243
Train loss: 0.8115;  Loss pred: 0.8115; Loss self: 0.0000; time: 0.16s
Val loss: 0.5321 score: 0.8182 time: 0.05s
Test loss: 0.5573 score: 0.7907 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.8125;  Loss pred: 0.8125; Loss self: 0.0000; time: 0.16s
Val loss: 0.5301 score: 0.8182 time: 0.05s
Test loss: 0.5556 score: 0.7907 time: 0.05s
Epoch 217/1000, LR 0.000242
Train loss: 0.8067;  Loss pred: 0.8067; Loss self: 0.0000; time: 0.16s
Val loss: 0.5283 score: 0.8182 time: 0.05s
Test loss: 0.5539 score: 0.7907 time: 0.05s
Epoch 218/1000, LR 0.000242
Train loss: 0.8056;  Loss pred: 0.8056; Loss self: 0.0000; time: 0.16s
Val loss: 0.5264 score: 0.8182 time: 0.05s
Test loss: 0.5523 score: 0.7907 time: 0.05s
Epoch 219/1000, LR 0.000242
Train loss: 0.8036;  Loss pred: 0.8036; Loss self: 0.0000; time: 0.16s
Val loss: 0.5245 score: 0.8182 time: 0.05s
Test loss: 0.5506 score: 0.7907 time: 0.05s
Epoch 220/1000, LR 0.000241
Train loss: 0.8011;  Loss pred: 0.8011; Loss self: 0.0000; time: 0.16s
Val loss: 0.5225 score: 0.8182 time: 0.05s
Test loss: 0.5488 score: 0.7907 time: 0.05s
Epoch 221/1000, LR 0.000241
Train loss: 0.7973;  Loss pred: 0.7973; Loss self: 0.0000; time: 0.16s
Val loss: 0.5205 score: 0.8182 time: 0.05s
Test loss: 0.5470 score: 0.7907 time: 0.05s
Epoch 222/1000, LR 0.000241
Train loss: 0.7949;  Loss pred: 0.7949; Loss self: 0.0000; time: 3.00s
Val loss: 0.5186 score: 0.8182 time: 0.49s
Test loss: 0.5451 score: 0.7907 time: 2.00s
Epoch 223/1000, LR 0.000241
Train loss: 0.7930;  Loss pred: 0.7930; Loss self: 0.0000; time: 0.70s
Val loss: 0.5165 score: 0.8182 time: 0.06s
Test loss: 0.5432 score: 0.7907 time: 0.05s
Epoch 224/1000, LR 0.000240
Train loss: 0.7900;  Loss pred: 0.7900; Loss self: 0.0000; time: 0.15s
Val loss: 0.5145 score: 0.8182 time: 0.04s
Test loss: 0.5413 score: 0.7907 time: 0.04s
Epoch 225/1000, LR 0.000240
Train loss: 0.7873;  Loss pred: 0.7873; Loss self: 0.0000; time: 0.15s
Val loss: 0.5125 score: 0.8182 time: 0.04s
Test loss: 0.5393 score: 0.7907 time: 0.04s
Epoch 226/1000, LR 0.000240
Train loss: 0.7843;  Loss pred: 0.7843; Loss self: 0.0000; time: 0.15s
Val loss: 0.5104 score: 0.8182 time: 0.04s
Test loss: 0.5372 score: 0.7907 time: 0.04s
Epoch 227/1000, LR 0.000240
Train loss: 0.7818;  Loss pred: 0.7818; Loss self: 0.0000; time: 0.14s
Val loss: 0.5086 score: 0.8182 time: 0.04s
Test loss: 0.5354 score: 0.7907 time: 0.04s
Epoch 228/1000, LR 0.000239
Train loss: 0.7809;  Loss pred: 0.7809; Loss self: 0.0000; time: 0.14s
Val loss: 0.5068 score: 0.8182 time: 0.05s
Test loss: 0.5338 score: 0.7907 time: 0.05s
Epoch 229/1000, LR 0.000239
Train loss: 0.7771;  Loss pred: 0.7771; Loss self: 0.0000; time: 0.16s
Val loss: 0.5051 score: 0.8182 time: 0.05s
Test loss: 0.5323 score: 0.7907 time: 0.05s
Epoch 230/1000, LR 0.000239
Train loss: 0.7781;  Loss pred: 0.7781; Loss self: 0.0000; time: 0.16s
Val loss: 0.5035 score: 0.8182 time: 0.05s
Test loss: 0.5309 score: 0.7907 time: 0.05s
Epoch 231/1000, LR 0.000238
Train loss: 0.7724;  Loss pred: 0.7724; Loss self: 0.0000; time: 0.16s
Val loss: 0.5020 score: 0.8182 time: 0.05s
Test loss: 0.5296 score: 0.7907 time: 0.05s
Epoch 232/1000, LR 0.000238
Train loss: 0.7720;  Loss pred: 0.7720; Loss self: 0.0000; time: 0.16s
Val loss: 0.5004 score: 0.8182 time: 0.05s
Test loss: 0.5282 score: 0.7907 time: 0.05s
Epoch 233/1000, LR 0.000238
Train loss: 0.7693;  Loss pred: 0.7693; Loss self: 0.0000; time: 0.17s
Val loss: 0.4989 score: 0.8182 time: 0.05s
Test loss: 0.5268 score: 0.7907 time: 0.05s
Epoch 234/1000, LR 0.000238
Train loss: 0.7645;  Loss pred: 0.7645; Loss self: 0.0000; time: 0.16s
Val loss: 0.4972 score: 0.8182 time: 0.05s
Test loss: 0.5252 score: 0.7907 time: 0.05s
Epoch 235/1000, LR 0.000237
Train loss: 0.7633;  Loss pred: 0.7633; Loss self: 0.0000; time: 0.16s
Val loss: 0.4955 score: 0.8182 time: 0.05s
Test loss: 0.5236 score: 0.7907 time: 0.05s
Epoch 236/1000, LR 0.000237
Train loss: 0.7607;  Loss pred: 0.7607; Loss self: 0.0000; time: 0.16s
Val loss: 0.4939 score: 0.8182 time: 0.05s
Test loss: 0.5221 score: 0.7907 time: 0.05s
Epoch 237/1000, LR 0.000237
Train loss: 0.7599;  Loss pred: 0.7599; Loss self: 0.0000; time: 0.16s
Val loss: 0.4924 score: 0.8182 time: 0.05s
Test loss: 0.5205 score: 0.7907 time: 0.05s
Epoch 238/1000, LR 0.000236
Train loss: 0.7576;  Loss pred: 0.7576; Loss self: 0.0000; time: 4.47s
Val loss: 0.4907 score: 0.8182 time: 2.49s
Test loss: 0.5188 score: 0.7907 time: 1.85s
Epoch 239/1000, LR 0.000236
Train loss: 0.7563;  Loss pred: 0.7563; Loss self: 0.0000; time: 3.32s
Val loss: 0.4889 score: 0.8182 time: 0.05s
Test loss: 0.5170 score: 0.7907 time: 0.90s
Epoch 240/1000, LR 0.000236
Train loss: 0.7514;  Loss pred: 0.7514; Loss self: 0.0000; time: 0.38s
Val loss: 0.4870 score: 0.8182 time: 0.05s
Test loss: 0.5149 score: 0.7907 time: 0.05s
Epoch 241/1000, LR 0.000236
Train loss: 0.7490;  Loss pred: 0.7490; Loss self: 0.0000; time: 0.16s
Val loss: 0.4852 score: 0.8182 time: 0.05s
Test loss: 0.5130 score: 0.7907 time: 0.05s
Epoch 242/1000, LR 0.000235
Train loss: 0.7466;  Loss pred: 0.7466; Loss self: 0.0000; time: 0.16s
Val loss: 0.4834 score: 0.8182 time: 0.05s
Test loss: 0.5110 score: 0.7907 time: 0.05s
Epoch 243/1000, LR 0.000235
Train loss: 0.7434;  Loss pred: 0.7434; Loss self: 0.0000; time: 0.15s
Val loss: 0.4817 score: 0.8182 time: 0.05s
Test loss: 0.5091 score: 0.7907 time: 0.05s
Epoch 244/1000, LR 0.000235
Train loss: 0.7418;  Loss pred: 0.7418; Loss self: 0.0000; time: 0.15s
Val loss: 0.4799 score: 0.8182 time: 0.05s
Test loss: 0.5071 score: 0.7907 time: 0.05s
Epoch 245/1000, LR 0.000234
Train loss: 0.7415;  Loss pred: 0.7415; Loss self: 0.0000; time: 0.15s
Val loss: 0.4781 score: 0.8182 time: 0.05s
Test loss: 0.5050 score: 0.7907 time: 0.05s
Epoch 246/1000, LR 0.000234
Train loss: 0.7389;  Loss pred: 0.7389; Loss self: 0.0000; time: 0.17s
Val loss: 0.4765 score: 0.8182 time: 0.05s
Test loss: 0.5033 score: 0.7907 time: 0.05s
Epoch 247/1000, LR 0.000234
Train loss: 0.7364;  Loss pred: 0.7364; Loss self: 0.0000; time: 0.17s
Val loss: 0.4751 score: 0.8182 time: 0.04s
Test loss: 0.5018 score: 0.7907 time: 0.04s
Epoch 248/1000, LR 0.000234
Train loss: 0.7338;  Loss pred: 0.7338; Loss self: 0.0000; time: 0.15s
Val loss: 0.4738 score: 0.8182 time: 0.05s
Test loss: 0.5006 score: 0.7907 time: 0.04s
Epoch 249/1000, LR 0.000233
Train loss: 0.7303;  Loss pred: 0.7303; Loss self: 0.0000; time: 0.15s
Val loss: 0.4726 score: 0.8182 time: 0.04s
Test loss: 0.4994 score: 0.7907 time: 0.04s
Epoch 250/1000, LR 0.000233
Train loss: 0.7281;  Loss pred: 0.7281; Loss self: 0.0000; time: 0.14s
Val loss: 0.4714 score: 0.8182 time: 0.04s
Test loss: 0.4982 score: 0.7907 time: 0.05s
Epoch 251/1000, LR 0.000233
Train loss: 0.7249;  Loss pred: 0.7249; Loss self: 0.0000; time: 0.15s
Val loss: 0.4702 score: 0.8182 time: 0.04s
Test loss: 0.4971 score: 0.7907 time: 0.04s
Epoch 252/1000, LR 0.000232
Train loss: 0.7218;  Loss pred: 0.7218; Loss self: 0.0000; time: 0.15s
Val loss: 0.4691 score: 0.8182 time: 0.04s
Test loss: 0.4960 score: 0.7907 time: 0.04s
Epoch 253/1000, LR 0.000232
Train loss: 0.7235;  Loss pred: 0.7235; Loss self: 0.0000; time: 0.14s
Val loss: 0.4678 score: 0.8182 time: 0.04s
Test loss: 0.4947 score: 0.7907 time: 0.05s
Epoch 254/1000, LR 0.000232
Train loss: 0.7203;  Loss pred: 0.7203; Loss self: 0.0000; time: 0.14s
Val loss: 0.4666 score: 0.8182 time: 0.05s
Test loss: 0.4934 score: 0.7907 time: 0.05s
Epoch 255/1000, LR 0.000232
Train loss: 0.7171;  Loss pred: 0.7171; Loss self: 0.0000; time: 6.53s
Val loss: 0.4653 score: 0.8182 time: 3.05s
Test loss: 0.4919 score: 0.7907 time: 2.38s
Epoch 256/1000, LR 0.000231
Train loss: 0.7152;  Loss pred: 0.7152; Loss self: 0.0000; time: 3.52s
Val loss: 0.4640 score: 0.8182 time: 0.06s
Test loss: 0.4904 score: 0.7907 time: 0.05s
Epoch 257/1000, LR 0.000231
Train loss: 0.7126;  Loss pred: 0.7126; Loss self: 0.0000; time: 0.16s
Val loss: 0.4627 score: 0.8182 time: 0.05s
Test loss: 0.4889 score: 0.7907 time: 0.05s
Epoch 258/1000, LR 0.000231
Train loss: 0.7125;  Loss pred: 0.7125; Loss self: 0.0000; time: 0.15s
Val loss: 0.4613 score: 0.8182 time: 0.05s
Test loss: 0.4873 score: 0.8140 time: 0.05s
Epoch 259/1000, LR 0.000230
Train loss: 0.7080;  Loss pred: 0.7080; Loss self: 0.0000; time: 0.15s
Val loss: 0.4599 score: 0.8182 time: 0.05s
Test loss: 0.4856 score: 0.8140 time: 0.05s
Epoch 260/1000, LR 0.000230
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.15s
Val loss: 0.4585 score: 0.8182 time: 0.05s
Test loss: 0.4840 score: 0.8140 time: 0.05s
Epoch 261/1000, LR 0.000230
Train loss: 0.7046;  Loss pred: 0.7046; Loss self: 0.0000; time: 0.16s
Val loss: 0.4571 score: 0.8182 time: 0.05s
Test loss: 0.4823 score: 0.8140 time: 0.05s
Epoch 262/1000, LR 0.000229
Train loss: 0.7022;  Loss pred: 0.7022; Loss self: 0.0000; time: 0.16s
Val loss: 0.4558 score: 0.8182 time: 0.05s
Test loss: 0.4806 score: 0.8140 time: 0.05s
Epoch 263/1000, LR 0.000229
Train loss: 0.7010;  Loss pred: 0.7010; Loss self: 0.0000; time: 0.16s
Val loss: 0.4544 score: 0.8182 time: 0.05s
Test loss: 0.4790 score: 0.8140 time: 0.05s
Epoch 264/1000, LR 0.000229
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.16s
Val loss: 0.4531 score: 0.8409 time: 0.05s
Test loss: 0.4774 score: 0.8140 time: 0.05s
Epoch 265/1000, LR 0.000228
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.16s
Val loss: 0.4518 score: 0.8409 time: 0.05s
Test loss: 0.4759 score: 0.8140 time: 0.05s
Epoch 266/1000, LR 0.000228
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.16s
Val loss: 0.4508 score: 0.8409 time: 0.05s
Test loss: 0.4746 score: 0.8140 time: 0.05s
Epoch 267/1000, LR 0.000228
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.16s
Val loss: 0.4497 score: 0.8409 time: 0.05s
Test loss: 0.4733 score: 0.8140 time: 0.05s
Epoch 268/1000, LR 0.000228
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.16s
Val loss: 0.4488 score: 0.8409 time: 0.05s
Test loss: 0.4724 score: 0.8140 time: 0.05s
Epoch 269/1000, LR 0.000227
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.16s
Val loss: 0.4480 score: 0.8409 time: 0.05s
Test loss: 0.4715 score: 0.8140 time: 0.05s
Epoch 270/1000, LR 0.000227
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.16s
Val loss: 0.4472 score: 0.8409 time: 0.05s
Test loss: 0.4706 score: 0.8140 time: 0.05s
Epoch 271/1000, LR 0.000227
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 6.29s
Val loss: 0.4462 score: 0.8409 time: 1.76s
Test loss: 0.4695 score: 0.8140 time: 1.74s
Epoch 272/1000, LR 0.000226
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 1.18s
Val loss: 0.4454 score: 0.8409 time: 1.53s
Test loss: 0.4686 score: 0.8140 time: 1.43s
Epoch 273/1000, LR 0.000226
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.16s
Val loss: 0.4445 score: 0.8409 time: 0.05s
Test loss: 0.4675 score: 0.8140 time: 0.05s
Epoch 274/1000, LR 0.000226
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.15s
Val loss: 0.4436 score: 0.8409 time: 0.05s
Test loss: 0.4663 score: 0.8140 time: 0.05s
Epoch 275/1000, LR 0.000225
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.15s
Val loss: 0.4426 score: 0.8409 time: 0.05s
Test loss: 0.4651 score: 0.8140 time: 0.05s
Epoch 276/1000, LR 0.000225
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.15s
Val loss: 0.4418 score: 0.8409 time: 0.05s
Test loss: 0.4640 score: 0.8140 time: 0.05s
Epoch 277/1000, LR 0.000225
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.15s
Val loss: 0.4411 score: 0.8409 time: 0.05s
Test loss: 0.4632 score: 0.8140 time: 0.05s
Epoch 278/1000, LR 0.000224
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.15s
Val loss: 0.4402 score: 0.8409 time: 0.05s
Test loss: 0.4620 score: 0.8140 time: 0.05s
Epoch 279/1000, LR 0.000224
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.15s
Val loss: 0.4390 score: 0.8409 time: 0.05s
Test loss: 0.4605 score: 0.8140 time: 0.05s
Epoch 280/1000, LR 0.000224
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.15s
Val loss: 0.4378 score: 0.8409 time: 0.05s
Test loss: 0.4589 score: 0.8140 time: 0.05s
Epoch 281/1000, LR 0.000223
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.15s
Val loss: 0.4367 score: 0.8409 time: 0.05s
Test loss: 0.4573 score: 0.8140 time: 0.05s
Epoch 282/1000, LR 0.000223
Train loss: 0.6675;  Loss pred: 0.6675; Loss self: 0.0000; time: 0.15s
Val loss: 0.4356 score: 0.8409 time: 0.05s
Test loss: 0.4558 score: 0.8140 time: 0.05s
Epoch 283/1000, LR 0.000223
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.16s
Val loss: 0.4346 score: 0.8409 time: 0.05s
Test loss: 0.4545 score: 0.8140 time: 0.04s
Epoch 284/1000, LR 0.000222
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.15s
Val loss: 0.4336 score: 0.8409 time: 0.04s
Test loss: 0.4531 score: 0.8140 time: 0.04s
Epoch 285/1000, LR 0.000222
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 0.15s
Val loss: 0.4329 score: 0.8409 time: 2.63s
Test loss: 0.4522 score: 0.8140 time: 1.95s
Epoch 286/1000, LR 0.000222
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 4.66s
Val loss: 0.4322 score: 0.8409 time: 1.23s
Test loss: 0.4513 score: 0.8140 time: 1.03s
Epoch 287/1000, LR 0.000221
Train loss: 0.6585;  Loss pred: 0.6585; Loss self: 0.0000; time: 0.29s
Val loss: 0.4319 score: 0.8409 time: 0.05s
Test loss: 0.4509 score: 0.8140 time: 0.05s
Epoch 288/1000, LR 0.000221
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 0.16s
Val loss: 0.4317 score: 0.8409 time: 0.05s
Test loss: 0.4506 score: 0.8140 time: 0.05s
Epoch 289/1000, LR 0.000221
Train loss: 0.6568;  Loss pred: 0.6568; Loss self: 0.0000; time: 0.15s
Val loss: 0.4312 score: 0.8409 time: 0.05s
Test loss: 0.4500 score: 0.8140 time: 0.05s
Epoch 290/1000, LR 0.000220
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.15s
Val loss: 0.4306 score: 0.8409 time: 0.05s
Test loss: 0.4492 score: 0.8140 time: 0.05s
Epoch 291/1000, LR 0.000220
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.16s
Val loss: 0.4299 score: 0.8409 time: 0.05s
Test loss: 0.4482 score: 0.8140 time: 0.05s
Epoch 292/1000, LR 0.000220
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.16s
Val loss: 0.4295 score: 0.8409 time: 0.05s
Test loss: 0.4476 score: 0.8140 time: 0.05s
Epoch 293/1000, LR 0.000219
Train loss: 0.6510;  Loss pred: 0.6510; Loss self: 0.0000; time: 0.16s
Val loss: 0.4290 score: 0.8409 time: 0.05s
Test loss: 0.4469 score: 0.8140 time: 0.05s
Epoch 294/1000, LR 0.000219
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.16s
Val loss: 0.4286 score: 0.8409 time: 0.05s
Test loss: 0.4463 score: 0.8140 time: 0.05s
Epoch 295/1000, LR 0.000219
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.16s
Val loss: 0.4280 score: 0.8409 time: 0.05s
Test loss: 0.4454 score: 0.8140 time: 0.05s
Epoch 296/1000, LR 0.000218
Train loss: 0.6496;  Loss pred: 0.6496; Loss self: 0.0000; time: 0.16s
Val loss: 0.4270 score: 0.8409 time: 0.05s
Test loss: 0.4440 score: 0.8140 time: 0.05s
Epoch 297/1000, LR 0.000218
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 0.22s
Val loss: 0.4258 score: 0.8409 time: 1.25s
Test loss: 0.4421 score: 0.8140 time: 1.34s
Epoch 298/1000, LR 0.000218
Train loss: 0.6471;  Loss pred: 0.6471; Loss self: 0.0000; time: 6.39s
Val loss: 0.4245 score: 0.8409 time: 0.05s
Test loss: 0.4403 score: 0.8140 time: 0.05s
Epoch 299/1000, LR 0.000217
Train loss: 0.6428;  Loss pred: 0.6428; Loss self: 0.0000; time: 0.15s
Val loss: 0.4232 score: 0.8409 time: 0.05s
Test loss: 0.4383 score: 0.8140 time: 0.05s
Epoch 300/1000, LR 0.000217
Train loss: 0.6423;  Loss pred: 0.6423; Loss self: 0.0000; time: 0.15s
Val loss: 0.4221 score: 0.8409 time: 0.05s
Test loss: 0.4366 score: 0.8140 time: 0.05s
Epoch 301/1000, LR 0.000217
Train loss: 0.6415;  Loss pred: 0.6415; Loss self: 0.0000; time: 0.15s
Val loss: 0.4211 score: 0.8409 time: 0.05s
Test loss: 0.4351 score: 0.8140 time: 0.05s
Epoch 302/1000, LR 0.000216
Train loss: 0.6396;  Loss pred: 0.6396; Loss self: 0.0000; time: 0.15s
Val loss: 0.4205 score: 0.8409 time: 0.05s
Test loss: 0.4341 score: 0.8140 time: 0.05s
Epoch 303/1000, LR 0.000216
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 0.15s
Val loss: 0.4201 score: 0.8409 time: 0.05s
Test loss: 0.4334 score: 0.8140 time: 0.05s
Epoch 304/1000, LR 0.000216
Train loss: 0.6380;  Loss pred: 0.6380; Loss self: 0.0000; time: 0.15s
Val loss: 0.4201 score: 0.8409 time: 0.05s
Test loss: 0.4334 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 305/1000, LR 0.000215
Train loss: 0.6369;  Loss pred: 0.6369; Loss self: 0.0000; time: 0.15s
Val loss: 0.4199 score: 0.8409 time: 0.05s
Test loss: 0.4330 score: 0.8140 time: 0.05s
Epoch 306/1000, LR 0.000215
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.15s
Val loss: 0.4198 score: 0.8409 time: 0.05s
Test loss: 0.4327 score: 0.8140 time: 0.05s
Epoch 307/1000, LR 0.000215
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 0.15s
Val loss: 0.4196 score: 0.8409 time: 0.05s
Test loss: 0.4323 score: 0.8140 time: 0.05s
Epoch 308/1000, LR 0.000214
Train loss: 0.6324;  Loss pred: 0.6324; Loss self: 0.0000; time: 0.15s
Val loss: 0.4193 score: 0.8409 time: 0.05s
Test loss: 0.4319 score: 0.8140 time: 0.05s
Epoch 309/1000, LR 0.000214
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.15s
Val loss: 0.4192 score: 0.8409 time: 0.05s
Test loss: 0.4316 score: 0.8140 time: 0.05s
Epoch 310/1000, LR 0.000214
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.15s
Val loss: 0.4192 score: 0.8409 time: 0.05s
Test loss: 0.4314 score: 0.8140 time: 0.05s
Epoch 311/1000, LR 0.000213
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.15s
Val loss: 0.4188 score: 0.8409 time: 0.05s
Test loss: 0.4307 score: 0.8140 time: 0.05s
Epoch 312/1000, LR 0.000213
Train loss: 0.6300;  Loss pred: 0.6300; Loss self: 0.0000; time: 0.16s
Val loss: 0.4185 score: 0.8409 time: 0.05s
Test loss: 0.4301 score: 0.8140 time: 0.05s
Epoch 313/1000, LR 0.000213
Train loss: 0.6288;  Loss pred: 0.6288; Loss self: 0.0000; time: 2.47s
Val loss: 0.4178 score: 0.8409 time: 2.33s
Test loss: 0.4290 score: 0.8140 time: 0.05s
Epoch 314/1000, LR 0.000212
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.21s
Val loss: 0.4173 score: 0.8409 time: 0.04s
Test loss: 0.4280 score: 0.8140 time: 0.05s
Epoch 315/1000, LR 0.000212
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.16s
Val loss: 0.4167 score: 0.8409 time: 0.04s
Test loss: 0.4270 score: 0.8140 time: 0.05s
Epoch 316/1000, LR 0.000212
Train loss: 0.6260;  Loss pred: 0.6260; Loss self: 0.0000; time: 0.15s
Val loss: 0.4162 score: 0.8409 time: 0.04s
Test loss: 0.4261 score: 0.8140 time: 0.05s
Epoch 317/1000, LR 0.000211
Train loss: 0.6250;  Loss pred: 0.6250; Loss self: 0.0000; time: 0.15s
Val loss: 0.4159 score: 0.8409 time: 0.05s
Test loss: 0.4255 score: 0.8140 time: 0.05s
Epoch 318/1000, LR 0.000211
Train loss: 0.6223;  Loss pred: 0.6223; Loss self: 0.0000; time: 0.15s
Val loss: 0.4156 score: 0.8409 time: 0.05s
Test loss: 0.4250 score: 0.8140 time: 0.05s
Epoch 319/1000, LR 0.000210
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 0.15s
Val loss: 0.4153 score: 0.8409 time: 0.05s
Test loss: 0.4243 score: 0.8140 time: 0.09s
Epoch 320/1000, LR 0.000210
Train loss: 0.6199;  Loss pred: 0.6199; Loss self: 0.0000; time: 0.17s
Val loss: 0.4152 score: 0.8409 time: 0.05s
Test loss: 0.4239 score: 0.8140 time: 0.05s
Epoch 321/1000, LR 0.000210
Train loss: 0.6195;  Loss pred: 0.6195; Loss self: 0.0000; time: 0.18s
Val loss: 0.4152 score: 0.8409 time: 0.05s
Test loss: 0.4237 score: 0.8140 time: 0.05s
Epoch 322/1000, LR 0.000209
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.20s
Val loss: 0.4151 score: 0.8409 time: 0.05s
Test loss: 0.4235 score: 0.8140 time: 0.05s
Epoch 323/1000, LR 0.000209
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.16s
Val loss: 0.4149 score: 0.8409 time: 0.05s
Test loss: 0.4230 score: 0.8140 time: 0.05s
Epoch 324/1000, LR 0.000209
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.17s
Val loss: 0.4146 score: 0.8409 time: 0.05s
Test loss: 0.4223 score: 0.8140 time: 0.05s
Epoch 325/1000, LR 0.000208
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 4.84s
Val loss: 0.4142 score: 0.8409 time: 0.41s
Test loss: 0.4216 score: 0.8140 time: 1.75s
Epoch 326/1000, LR 0.000208
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 6.48s
Val loss: 0.4140 score: 0.8409 time: 0.06s
Test loss: 0.4210 score: 0.8140 time: 0.07s
Epoch 327/1000, LR 0.000208
Train loss: 0.6144;  Loss pred: 0.6144; Loss self: 0.0000; time: 0.15s
Val loss: 0.4139 score: 0.8409 time: 0.04s
Test loss: 0.4206 score: 0.8140 time: 0.05s
Epoch 328/1000, LR 0.000207
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.15s
Val loss: 0.4137 score: 0.8636 time: 0.04s
Test loss: 0.4201 score: 0.8140 time: 0.04s
Epoch 329/1000, LR 0.000207
Train loss: 0.6119;  Loss pred: 0.6119; Loss self: 0.0000; time: 0.15s
Val loss: 0.4133 score: 0.8636 time: 0.04s
Test loss: 0.4193 score: 0.8140 time: 0.04s
Epoch 330/1000, LR 0.000207
Train loss: 0.6110;  Loss pred: 0.6110; Loss self: 0.0000; time: 0.15s
Val loss: 0.4129 score: 0.8636 time: 0.04s
Test loss: 0.4184 score: 0.8140 time: 0.04s
Epoch 331/1000, LR 0.000206
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.15s
Val loss: 0.4122 score: 0.8864 time: 0.04s
Test loss: 0.4172 score: 0.8140 time: 0.04s
Epoch 332/1000, LR 0.000206
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 0.15s
Val loss: 0.4118 score: 0.8864 time: 0.04s
Test loss: 0.4164 score: 0.8140 time: 0.04s
Epoch 333/1000, LR 0.000205
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.15s
Val loss: 0.4117 score: 0.8636 time: 0.04s
Test loss: 0.4160 score: 0.8140 time: 0.04s
Epoch 334/1000, LR 0.000205
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.15s
Val loss: 0.4116 score: 0.8636 time: 0.04s
Test loss: 0.4156 score: 0.8140 time: 0.04s
Epoch 335/1000, LR 0.000205
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.15s
Val loss: 0.4117 score: 0.8636 time: 0.04s
Test loss: 0.4155 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 336/1000, LR 0.000204
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.15s
Val loss: 0.4119 score: 0.8636 time: 0.04s
Test loss: 0.4156 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 333,   Train_Loss: 0.6091,   Val_Loss: 0.4116,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.4116,   Test_Precision: 1.0000,   Test_Recall: 0.6364,   Test_accuracy: 0.7778,   Test_Score: 0.8140,   Test_loss: 0.4156


[0.05545937595888972, 0.057529293932020664, 0.05470380000770092, 0.05550090002361685, 0.057805703952908516, 0.060653330059722066, 0.055280742002651095, 0.05565055401530117, 0.7109664009185508, 0.31898007704876363, 0.1344758050981909, 0.06007817306090146, 0.05518483591731638, 0.05276909493841231, 0.0538266699295491, 0.05469510005787015, 0.05622987204696983, 0.05546813702676445, 1.297919764998369, 0.513117947964929, 0.27114238403737545, 0.05265923507977277, 0.05199010600335896, 0.05150219995994121, 0.05466046102810651, 0.053594280034303665, 0.054290933068841696, 0.05432291503529996, 0.0538062599953264, 1.8271140879951417, 0.8477141369367018, 0.08036299899686128, 0.05348086101002991, 0.049798041000030935, 0.05032224499154836, 0.04971343604847789, 0.04985185002442449, 0.04927924496587366, 0.049995991052128375, 0.04923918505664915, 0.050050979014486074, 0.049178126035258174, 0.049004415050148964, 0.04907885298598558, 0.049217629013583064, 0.04972954001277685, 0.1419258660171181, 0.9357607350684702, 0.4322917010867968, 0.0567030580714345, 0.056750652962364256, 0.05634796107187867, 0.10557014402002096, 0.05621543596498668, 0.05699615797493607, 0.05795564502477646, 0.057973846909590065, 0.05665381799917668, 3.5709817359456792, 0.09258924203459173, 0.052572960034012794, 0.05078897706698626, 0.05072854505851865, 0.05473598896060139, 0.05362162506207824, 0.05336401204112917, 0.050551208085380495, 0.05139677203260362, 0.050800247001461685, 0.05001360806636512, 0.05054433294571936, 0.050314499996602535, 0.05068053596187383, 0.049981547985225916, 0.050519780023023486, 0.05011103698052466, 0.05023378902114928, 1.2415582379326224, 0.054319899063557386, 0.05017784901428968, 0.04938702890649438, 0.04996224900241941, 0.04960649798158556, 0.04930786299519241, 0.04952123400289565, 0.04878442897461355, 0.04974715306889266, 0.05205671594012529, 0.049344186089001596, 0.04915229103062302, 1.7849164409562945, 0.387693680007942, 0.4082161580445245, 0.05592207401059568, 0.054509787005372345, 0.05587615899275988, 0.056411084020510316, 0.050510814995504916, 0.05051064502913505, 0.050930417026393116, 0.052005036966875196, 0.05417391995433718, 0.05350572895258665, 1.798725588945672, 0.26537680393084884, 0.08149278908967972, 0.0540065459208563, 0.05263013101648539, 0.05185056396294385, 0.05126343294978142, 0.05158588697668165, 0.05166915990412235, 0.05204587196931243, 0.05076477595139295, 0.05160653591156006, 0.05141747300513089, 0.051453712047077715, 1.4993561550509185, 0.21717809094116092, 0.0546054890146479, 0.05589847301598638, 0.05318957392591983, 0.06447987398132682, 0.05154588504228741, 0.05086784600280225, 0.10351475607603788, 0.05790265090763569, 0.05859625490847975, 0.05191781197208911, 0.05200030992273241, 1.541631220956333, 0.32009176607243717, 0.1973802630091086, 0.05458328896202147, 0.05388558504637331, 0.05621476692613214, 0.05402219796087593, 0.054226765991188586, 0.05298422195482999, 0.053424868965521455, 0.05351552402134985, 0.05355909699574113, 0.05387668404728174, 0.05284880299586803, 0.052429168950766325, 0.38165334006771445, 1.565964404027909, 1.0109545269515365, 0.048854355001822114, 0.048071552999317646, 0.04897273704409599, 0.04854643705766648, 0.048556186025962234, 0.04824847693089396, 0.048016497050412, 0.04766917589586228, 0.04780828708317131, 0.055876895086839795, 0.04822872101794928, 0.04871944501064718, 0.06286621594335884, 0.04880620399489999, 0.05628752999473363, 0.05238977400586009, 0.05259281606413424, 0.05225759407039732, 0.05211399798281491, 2.2372530420543626, 0.3573656240478158, 0.05237980803940445, 0.05212225508876145, 0.05183890892658383, 0.05174215289298445, 0.05201577104162425, 0.051447126083076, 0.05204572097864002, 0.05216061498504132, 0.0516660480061546, 0.05151507700793445, 0.052921624039299786, 0.052174380980432034, 0.05238828994333744, 1.9781540100229904, 0.054647520068101585, 0.05410671699792147, 0.05652543995529413, 0.05793883197475225, 0.054068783996626735, 0.05434737307950854, 0.05399795901030302, 0.04855033697094768, 0.04940653906669468, 0.04922254499979317, 0.0499601480551064, 0.04901986406184733, 0.04888689611107111, 0.04996061394922435, 0.05389843904413283, 0.05272371903993189, 0.04872848896775395, 0.04920637304894626, 0.0495099889812991, 0.14230923098511994, 0.7482448990922421, 0.05387521593365818, 0.054153622942976654, 0.05547530797775835, 0.04959046398289502, 0.050351866986602545, 0.05301779496949166, 0.05130091204773635, 0.05010366998612881, 0.04933218297082931, 0.04856491705868393, 2.0116327210562304, 0.04985905601643026, 0.049996405956335366, 0.04966721800155938, 0.05112981202546507, 0.050206927000544965, 0.05025013000704348, 0.05015570204705, 0.048190874978899956, 0.048838049988262355, 0.04907591105438769, 0.048574788961559534, 1.956354875001125, 0.053682955098338425, 0.05301607493311167, 0.05270198010839522, 0.052885398035869, 0.05326252500526607, 0.05263467796612531, 0.053776626009494066, 0.05263316503260285, 0.053022531094029546, 0.052324921009130776, 2.0086210949812084, 0.05765336798503995, 0.04933463898487389, 0.04908384697046131, 0.04924994194880128, 0.048615487990900874, 0.052271928056143224, 0.053431935026310384, 0.05321253300644457, 0.05333104496821761, 0.05302306998055428, 0.05280563700944185, 0.05298119189683348, 0.05416607309598476, 0.05366159102413803, 0.05355061206500977, 1.850088482024148, 0.9061089319875464, 0.05083067202940583, 0.052040801965631545, 0.05175290408078581, 0.05112950806505978, 0.051120463060215116, 0.05227689107414335, 0.051971185952425, 0.048650128999724984, 0.04871485393960029, 0.049198834924027324, 0.04945102892816067, 0.04904836101923138, 0.048837580950930715, 0.049392578075639904, 0.05253667791839689, 2.384068739018403, 0.051824813010171056, 0.05332548497244716, 0.0522444179514423, 0.0510998519603163, 0.051166116958484054, 0.051263703033328056, 0.05331534903962165, 0.05252336000557989, 0.052072918973863125, 0.055333961034193635, 0.05263735610060394, 0.05228745401836932, 0.05288647406268865, 0.05228657694533467, 0.051777904969640076, 1.7425204470055178, 1.4346817249897867, 0.05153802002314478, 0.05119078909046948, 0.05042924406006932, 0.05115442001260817, 0.0506798499263823, 0.05122986703645438, 0.05153047200292349, 0.05170172604266554, 0.05072360299527645, 0.051213309983722866, 0.04906692006625235, 0.04810817597899586, 1.9581554090837017, 1.0322526330128312, 0.05263629008550197, 0.051759189926087856, 0.052253557019867, 0.053675664006732404, 0.05401281698141247, 0.05236483609769493, 0.0524191630538553, 0.05264303297735751, 0.05308309104293585, 0.052821284043602645, 1.343839078093879, 0.051012675976380706, 0.05029693292453885, 0.04996269498951733, 0.05008294607978314, 0.05127258598804474, 0.05084754095878452, 0.05140564602334052, 0.05072511802427471, 0.0509243420092389, 0.051158690941520035, 0.05040607193950564, 0.051086653023958206, 0.0517140559386462, 0.05118823901284486, 0.05121310905087739, 0.05626375006977469, 0.04968244198244065, 0.05044862790964544, 0.04978307196870446, 0.05142125894781202, 0.05027606594376266, 0.09632291994057596, 0.05642897298093885, 0.05625422403682023, 0.05464953696355224, 0.052983022993430495, 0.05011435807682574, 1.755774887977168, 0.0770078090718016, 0.0500655040377751, 0.048775761970318854, 0.04821814794559032, 0.048547586891800165, 0.04884694400243461, 0.048847015015780926, 0.048819413990713656, 0.04862278199288994, 0.04920842102728784, 0.048961310996674]
[0.0012604403627020392, 0.0013074839530004697, 0.0012432681819932027, 0.0012613840914458376, 0.001313765998929739, 0.0013784847740845923, 0.0012563805000602522, 0.001264785318529572, 0.016158327293603426, 0.007249547205653719, 0.003056268297686157, 0.001365413024111397, 0.0012542008163026449, 0.0011992976122366433, 0.0012233334074897523, 0.0012430704558606851, 0.0013076714429527867, 0.0012899566750410338, 0.030184180581357415, 0.011932975534068118, 0.006305636838078499, 0.001224633373948204, 0.001209072232636255, 0.001197725580463749, 0.0012711735122815468, 0.0012463786054489224, 0.001262579838810272, 0.001263323605472092, 0.0012513083719843348, 0.0424910253022126, 0.019714282254341905, 0.0018689069534153786, 0.0012437409537216259, 0.0011580939767449055, 0.0011702847672453106, 0.001156126419732044, 0.0011593453494052207, 0.0011460289526947361, 0.0011626974663285668, 0.0011450973268988173, 0.001163976256150839, 0.0011436773496571668, 0.0011396375593057898, 0.001141368674092688, 0.0011445960235716992, 0.0011565009305296943, 0.003300601535281816, 0.021761877559731867, 0.010053295374111555, 0.0013186757691031278, 0.001319782627031727, 0.0013104176993460157, 0.0024551196283725805, 0.0013073357201159694, 0.0013254920459287458, 0.0013478056982506154, 0.0013482289978974434, 0.0013175306511436437, 0.08304608688245765, 0.0021532381868509705, 0.0012226269775351814, 0.0011811390015578202, 0.0011797336060120616, 0.0012729299758279392, 0.001247014536327401, 0.0012410235358402133, 0.0011756094903576859, 0.0011952737682000842, 0.0011814010930572484, 0.0011631071643340726, 0.0011754496033888223, 0.00117010465108378, 0.0011786171153924147, 0.0011623615810517655, 0.0011748786051865928, 0.0011653729530354572, 0.0011682276516546343, 0.028873447393781917, 0.0012632534665943578, 0.0011669267212625505, 0.0011485355559649856, 0.0011619127674981258, 0.0011536394879438502, 0.0011466944882602885, 0.0011516566047185036, 0.0011345216040607802, 0.0011569105364858758, 0.0012106213009331463, 0.0011475392113721302, 0.001143076535595884, 0.0415096846734022, 0.009016132093207953, 0.009493399024291267, 0.0013005133490836204, 0.0012676694652412174, 0.0012994455579711601, 0.0013118856748955887, 0.001174670116174533, 0.001174666163468257, 0.0011844283029393747, 0.0012094194643459347, 0.0012598586035892367, 0.0012443192779671313, 0.041830827649899344, 0.0061715535797871824, 0.0018951811416204585, 0.0012559661842059605, 0.0012239565352671021, 0.001205827068905671, 0.0011921728592972423, 0.0011996717901553871, 0.0012016083698633106, 0.0012103691155654053, 0.001180576184916115, 0.001200151997943257, 0.0011957551861658347, 0.0011965979545832026, 0.03486874779188182, 0.005050653277701417, 0.0012698950933639048, 0.0012999644887438694, 0.0012369668354865076, 0.001499531953054112, 0.001198741512611335, 0.0011829731628558663, 0.0024073199087450673, 0.0013465732769217603, 0.001362703602522785, 0.0012073909760950956, 0.0012093095330868004, 0.03585188885944961, 0.00744399455982412, 0.004590238674630432, 0.0012693788130702668, 0.0012531531406133327, 0.0013073201610728404, 0.0012563301851366494, 0.0012610875811904323, 0.0012321912082518603, 0.0012424388131516617, 0.00124454707026395, 0.0012455603952497938, 0.0012529461406344592, 0.0012290419301364658, 0.0012192829988550307, 0.008875659071342197, 0.03641777683785835, 0.023510570394221778, 0.001136147790740049, 0.001117943093007387, 0.0011389008614906045, 0.001128986908317825, 0.0011292136285107495, 0.0011220576030440456, 0.0011166627221026045, 0.0011085854859502857, 0.0011118206298411932, 0.0012994626764381348, 0.001121598163208123, 0.0011330103490848182, 0.0014620050219385776, 0.0011350279998813952, 0.0013090123254589218, 0.0012183668373455835, 0.0012230887456775405, 0.0012152928853580772, 0.001211953441460812, 0.052029140512892154, 0.008310828466228274, 0.0012181350706838244, 0.001212145467180499, 0.0012055560215484611, 0.0012033058812321966, 0.0012096690939912616, 0.0011964447926296744, 0.0012103656041544191, 0.0012130375577916587, 0.00120153600014313, 0.0011980250466961501, 0.0012307354427744137, 0.0012133576972193497, 0.0012183323242636614, 0.04600358162844164, 0.0012708725597232926, 0.0012582957441377085, 0.0013145451152393984, 0.0013474146970872615, 0.0012574135813169008, 0.001263892397197873, 0.0012557664886116982, 0.0011290776039755276, 0.0011489892806208065, 0.0011447103488323993, 0.0011618639082582884, 0.0011399968386476124, 0.001136904560722584, 0.0011618747430052175, 0.0012534520707937868, 0.0012261330009286488, 0.0011332206736686965, 0.0011443342569522387, 0.0011513950925883512, 0.0033095169996539523, 0.017401044164935865, 0.001252911998457167, 0.0012593865800692246, 0.0012901234413432174, 0.0011532666042533724, 0.001170973650851222, 0.0012329719760346897, 0.0011930444662264267, 0.001165201627584391, 0.0011472600690890537, 0.0011294166757833473, 0.04678215630363326, 0.001159512930614657, 0.0011627071152636131, 0.0011550515814316134, 0.0011890653959410482, 0.0011676029535010457, 0.0011686076745824065, 0.0011664116755127907, 0.0011207180227651152, 0.0011357686043781943, 0.0011413002570787834, 0.0011296462549199892, 0.045496625000026165, 0.0012484408162404285, 0.0012329319751886434, 0.0012256274443812841, 0.0012298929775783488, 0.00123866337221549, 0.001224062278281984, 0.0012506192095231177, 0.0012240270937814615, 0.0012330821184658033, 0.0012168586281193204, 0.04671211848793508, 0.0013407759996520918, 0.0011473171856947416, 0.0011414848132665421, 0.001145347487181425, 0.0011305927439744388, 0.001215626233863796, 0.001242603140146753, 0.0012375007675917342, 0.001240256859725991, 0.0012330946507105648, 0.0012280380699870197, 0.001232120741786825, 0.0012596761185112734, 0.0012479439773055356, 0.0012453630712792971, 0.0430253135354453, 0.02107230074389643, 0.0011821086518466473, 0.001210251208503059, 0.001203555908855484, 0.0011890583270944136, 0.0011888479781445376, 0.0012157416528870547, 0.0012086322314517443, 0.0011313983488308137, 0.0011329035799907043, 0.0011441589517215656, 0.001150023928561876, 0.0011406595585867763, 0.0011357576965332725, 0.0011486646064102304, 0.001221783207404579, 0.055443459046939605, 0.0012052282095388618, 0.001240127557498771, 0.0012149864639870304, 0.001188368650239914, 0.0011899096967089315, 0.0011921791403099549, 0.0012398918381307362, 0.001221473488501858, 0.0012109981156712354, 0.0012868363031207821, 0.0012241245604791614, 0.0012159873027527749, 0.0012299180014578755, 0.0012159669057054574, 0.0012041373248753507, 0.040523731325709716, 0.03336469127883225, 0.0011985586051894135, 0.0011904834672202204, 0.0011727731176760308, 0.001189637674711818, 0.0011786011610786583, 0.0011913922566617297, 0.0011983830698354299, 0.0012023657219224545, 0.0011796186743087548, 0.0011910072089237876, 0.00114109116433145, 0.001118794790209206, 0.04553849788566748, 0.02400587518634491, 0.0012240997694302784, 0.0012037020913043687, 0.0012151990004620234, 0.001248271255970521, 0.0012561120228235458, 0.0012177868859929053, 0.0012190503035780303, 0.0012242565808687792, 0.001234490489370601, 0.0012284019545023871, 0.03125207158357858, 0.0011863413017762954, 0.0011696961145241593, 0.0011619231392911008, 0.0011647196762740266, 0.0011923857206522033, 0.0011825009525298726, 0.0011954801400776865, 0.0011796539075412723, 0.001184287023470672, 0.0011897369986400008, 0.001172234231151294, 0.0011880616982315863, 0.0012026524636894464, 0.0011904241630894153, 0.0011910025360669162, 0.0013084593039482486, 0.0011554056274986199, 0.0011732239048754753, 0.001157745859737313, 0.0011958432313444655, 0.0011692108359014572, 0.00224006790559479, 0.001312301697231136, 0.001308237768298145, 0.0012709194642686566, 0.0012321633254286162, 0.0011654501878331567, 0.040831974139003906, 0.0017908792807395722, 0.0011643140473901186, 0.0011343200458213687, 0.0011213522778044259, 0.0011290136486465155, 0.0011359754419170839, 0.0011359770933902541, 0.001135335209086364, 0.0011307623719276731, 0.0011443818843555312, 0.001138635139457535]
[793.3735142028248, 764.8277424018532, 804.3316916522418, 792.7799365645789, 761.170559151821, 725.434200507632, 795.9372180259428, 790.6480138167567, 61.8875940454473, 137.93964941977728, 327.1964050921449, 732.3791280303587, 797.320482494962, 833.821388283296, 817.4386425463305, 804.4596308160293, 764.7180837274772, 775.2198343934224, 33.12993696498184, 83.80139531377101, 158.58826406893542, 816.570919324215, 827.0804448296734, 834.9157906544908, 786.6746674143367, 802.3244266454803, 792.0291210592268, 791.5628233878441, 799.1635174742673, 23.534381504979308, 50.72464658355789, 535.0721169786041, 804.0259484965227, 863.4877825811118, 854.4928789886434, 864.9573117027899, 862.5557522726341, 872.5783041070922, 860.0689594324873, 873.2882144683948, 859.124054048067, 874.37247952822, 877.4719574959867, 876.1410950716108, 873.670692022423, 864.6772117528565, 302.9750756977142, 45.95191739569374, 99.46987159803545, 758.3365247396113, 757.7005330408555, 763.1154558573694, 407.3121278668069, 764.9144627604097, 754.4368169326282, 741.9467073762562, 741.7137604661339, 758.9956249837371, 12.04150656027161, 464.4168053987851, 817.9109559777605, 846.6404027646929, 847.6489903346671, 785.5891675027763, 801.915271128365, 805.786490844404, 850.6225989173873, 836.6284165224012, 846.4525772633104, 859.7660049429252, 850.7383022776978, 854.6244125034245, 848.4519586049411, 860.3174918213898, 851.1517663062571, 858.0943957857364, 855.9975434442397, 34.63389689363371, 791.6067728639838, 856.9518392020836, 870.6739593793526, 860.6498077762219, 866.8219235303012, 872.0718641607438, 868.3143880761465, 881.4287858606759, 864.3710714551078, 826.0221418780593, 871.429917243773, 874.8320596736783, 24.090763586088173, 110.912305816074, 105.33634975641986, 768.9271322779033, 788.8491656693142, 769.5589814176686, 762.2615439257606, 851.3028349240985, 851.3056995252618, 844.2891794448999, 826.8429849860315, 793.7398666414467, 803.6522600804831, 23.90581435226291, 162.0337548838851, 527.6540474357815, 796.1997803565182, 817.0224768494508, 829.3063124777369, 838.8045342598022, 833.5613191925395, 832.2179048351298, 826.19424697801, 847.044022043401, 833.2277925743866, 836.2915850747679, 835.702581781797, 28.678976542794604, 197.9941890715385, 787.4666224207839, 769.2517823823639, 808.429111688102, 666.874752460786, 834.2082004164541, 845.3277144393174, 415.3997133356899, 742.6257576460897, 733.8352948863505, 828.2321301043406, 826.9181484474606, 27.892533191774262, 134.33647646615518, 217.85359561515864, 787.7868999414635, 797.9870676544516, 764.923566373641, 795.9690946144315, 792.966337085032, 811.5623559907755, 804.8686095561732, 803.5051657691942, 802.8514745761909, 798.1189035736414, 813.6418908742729, 820.1541405391949, 112.66768945968289, 27.459117135355807, 42.53405949886147, 880.1671826062637, 894.4999135062344, 878.039550072155, 885.7498635568646, 885.5720253029876, 891.2198422675326, 895.525551454842, 902.0504171068003, 899.4256565853027, 769.5488436351471, 891.5849123180499, 882.6044711839955, 683.9921785453432, 881.0355340172182, 763.9347472526003, 820.7708625578383, 817.6021597239385, 822.8469137341797, 825.1142047129, 19.219998449757455, 120.32494763471308, 820.9270253081459, 824.9834917306105, 829.4927669272165, 831.0438896683448, 826.6723560742834, 835.8095636005845, 826.1966438633359, 824.376783370587, 832.2680301554655, 834.7070896036331, 812.5223059683136, 824.1592749538729, 820.7941134652094, 21.737437925523427, 786.860958126069, 794.7257269675383, 760.7194218038572, 742.1620100788004, 795.2832821741044, 791.2065949736397, 796.3263943327087, 885.6787137385064, 870.3301387282686, 873.5834362116116, 860.68600022103, 877.1954150208945, 879.5813074796962, 860.6779741278095, 797.7967592863121, 815.5722089223763, 882.4406607078506, 873.8705443139917, 868.511605127643, 302.1588951211193, 57.46781575412925, 798.1406525210052, 794.0373637656462, 775.1196265055427, 867.1021915590821, 853.9901809686877, 811.0484418437949, 838.1917257141118, 858.220565717133, 871.6419467070086, 885.4128165819885, 21.375671388672963, 862.4310894660748, 860.0618219948507, 865.7622015118694, 840.9966377068619, 856.4555245441184, 855.7191791140194, 857.3302385372377, 892.2851062328139, 880.4610341799998, 876.1936167083246, 885.2328732509526, 21.979652336836523, 800.9991238602831, 811.0747552370001, 815.9086226278292, 813.0788761547312, 807.3218458146433, 816.9518967642206, 799.6039021192685, 816.9753799408468, 810.9759966710056, 821.7881493312992, 21.407721001955466, 745.8367395146413, 871.5985539730796, 876.0519530157737, 873.0974758244685, 884.4917901070561, 822.6212730055679, 804.7621703916656, 808.0803068478673, 806.2845951288907, 810.9677545221325, 814.3070027222933, 811.6087702166243, 793.854853088612, 801.3180224316823, 802.978683937329, 23.242131615756282, 47.45566287011394, 845.9459275912043, 826.2747378181795, 830.8712479762951, 841.0016373575239, 841.1504400762182, 822.5431757028912, 827.3815425217095, 883.8619934644588, 882.6876511487457, 874.0044366172584, 869.5471243372446, 876.6857669951524, 880.4694901494815, 870.5761406936424, 818.4758097341094, 18.036392699693913, 829.7183820337352, 806.3686626050893, 823.0544369345952, 841.4897176882905, 840.399908300448, 838.8001150062145, 806.5219636477343, 818.683343857511, 825.7651164434035, 777.0996183235127, 816.9103310929144, 822.3770081613363, 813.0623332731583, 822.3908029962693, 830.470062958573, 24.67689838239461, 29.971804373758307, 834.3355057235316, 839.9948655607965, 852.6798448293237, 840.5920737524083, 848.4634438038377, 839.3541207007597, 834.4577165441153, 831.6937033110913, 847.7315778219521, 839.6254804398836, 876.354169814194, 893.8189637198861, 21.959441932201592, 41.65646918671071, 816.9268755482414, 830.7703436124873, 822.9104859531617, 801.1079284386052, 796.1073390191382, 821.1617414361167, 820.3106935496456, 816.822237778262, 810.0507931088599, 814.0657838704674, 31.997878839028726, 842.9277464273656, 854.9229048322582, 860.6421252701006, 858.5756902459399, 838.6547932266639, 845.6652807429665, 836.4839920594719, 847.7062582569483, 844.3898988856599, 840.5218978170042, 853.0718293543293, 841.7071280797003, 831.495407187079, 840.0367121285389, 839.6287746811441, 764.2576249658823, 865.4969096566864, 852.3522201042596, 863.7474205495282, 836.2300122531258, 855.2777388767558, 446.415038357722, 762.0198938322867, 764.3870435730317, 786.8319182407395, 811.5807209666327, 858.0375295654917, 24.49061112244315, 558.3849289869689, 858.8748046470464, 881.5854076491202, 891.780415301755, 885.7288848534474, 880.3007205088779, 880.2994407356941, 880.7971355039081, 884.3591057025047, 873.8341751741018, 878.2444571984814]
Elapsed: 0.186977004015551~0.4530177163005833
Time per graph: 0.004345459167691614~0.010534285554290231
Speed: 730.9890808644634~252.20143482061457
Total Time: 0.0500
best val loss: 0.41159847378730774 test_score: 0.8140

Testing...
Test loss: 0.4172 score: 0.8140 time: 0.04s
test Score 0.8140
Epoch Time List: [0.480402200948447, 0.23165051499381661, 0.23635498306248337, 0.24537496489938349, 0.2494306720327586, 0.2522010870743543, 0.23866619099862874, 0.2368397390237078, 0.8923679920844734, 3.482520340010524, 0.6687864249106497, 0.2591526519972831, 0.24343741813208908, 0.22690718597732484, 0.23128005699254572, 0.2292608447605744, 0.28828608291223645, 0.3012420309241861, 4.807711353059858, 2.84745625697542, 1.284412236069329, 0.31760633597150445, 0.2498023509979248, 0.25499244104139507, 0.25318737095221877, 0.26072893710806966, 0.26677056087646633, 0.2661114679649472, 0.2589731839252636, 3.775720320059918, 4.195870227995329, 1.18387481989339, 0.31894658599048853, 0.23825160600245, 0.24877576704602689, 0.2390887641813606, 0.23418310389388353, 0.23681667598430067, 0.24010133708361536, 0.23607592401094735, 0.24074556794948876, 0.2354166880249977, 0.2333479248918593, 0.23109959112480283, 0.23545167688280344, 0.23509613098576665, 0.32459252688568085, 6.943038871977478, 3.2115896781906486, 1.3041555748786777, 0.3591682950500399, 0.2739344679284841, 0.32167562504764646, 0.27764329011552036, 0.2815652199788019, 0.27800144208595157, 0.2762110590701923, 0.27105708490125835, 8.558935633976944, 2.5695596809964627, 0.24475416890345514, 0.24042713386006653, 0.2421369361691177, 0.252992843859829, 0.25830665009561926, 0.2556055859895423, 0.24427121388725936, 0.24270237109158188, 0.24242074484936893, 0.23750618495978415, 0.2422159321140498, 0.24431246402673423, 0.24438434198964387, 0.23672560404520482, 0.2396555079612881, 0.24055274995043874, 0.24067654891405255, 6.5129730919143185, 0.9573900459799916, 0.25578214693814516, 0.2396165980026126, 0.23728616803418845, 0.24125806393567473, 0.239739004871808, 0.24093338486272842, 0.23408401396591216, 0.2398862399859354, 0.23462435218971223, 0.23656323098111898, 0.2345487530110404, 4.750608935020864, 3.1640275300014764, 1.6501905767945573, 0.2698421620298177, 0.26027152698952705, 0.2635450870729983, 0.2640966789331287, 0.24664243508595973, 0.24299498193431646, 0.24151444295421243, 0.24228266708087176, 0.24994208093266934, 0.2569977610837668, 5.9562670171726495, 2.5639751709531993, 2.252431231085211, 0.2516482979990542, 0.2437203839654103, 0.24449919606558979, 0.248160659102723, 0.24592142400797457, 0.25396329921204597, 0.24572805094067007, 0.24028735794126987, 0.2427210749592632, 0.24705357605125755, 0.24498226307332516, 2.1862613251432776, 4.368229487910867, 0.4297018718207255, 0.36224929604213685, 0.2564236130565405, 0.2658683229237795, 0.25815837597474456, 0.24034255696460605, 0.3001924640266225, 0.282580483937636, 0.2842074119253084, 0.248626826913096, 0.24542293592821807, 1.7346668120007962, 6.556002522818744, 1.1186421690508723, 0.2581495000049472, 0.25687140389345586, 0.26136599807068706, 0.25921133894007653, 0.25813911703880876, 0.2554723139619455, 0.25584166415501386, 0.255838253069669, 0.254921811982058, 0.25782274501398206, 0.25792451889719814, 0.2519736341200769, 0.5783722980413586, 5.864421446924098, 3.186518831877038, 2.779937923885882, 0.2347439369186759, 0.23399049893487245, 0.23652323288843036, 0.23714383493643254, 0.23261753492988646, 0.23073062603361905, 0.23104176297783852, 0.23484687402378768, 0.24074353813193738, 0.2345844869269058, 0.23310343897901475, 0.24611599498894066, 0.24307057401165366, 0.24101394903846085, 0.24686526495497674, 0.2570637691533193, 0.2525096479803324, 0.2507795401616022, 2.428092908114195, 7.505616341019049, 0.25318944407626987, 0.2494652319001034, 0.24726579699199647, 0.25000528106465936, 0.24785440508276224, 0.24810787907335907, 0.24881537887267768, 0.24890788504853845, 0.24752156110480428, 0.24664850602857769, 0.24764904798939824, 0.24894000787753612, 0.25034175999462605, 4.084463177016005, 4.062065158970654, 0.26752617093734443, 0.2624818111071363, 0.2820732139516622, 0.26189229311421514, 0.2602439569309354, 0.2602414949797094, 0.25114548997953534, 0.24213765701279044, 0.2377538449363783, 0.2562938790069893, 0.23618142597842962, 0.23400303395465016, 0.23578378383535892, 0.2421328390482813, 0.2521880710264668, 0.2556494759628549, 0.23713864001911134, 0.23745526594575495, 0.3263405760517344, 5.851404786924832, 0.8721186158945784, 0.26002403697930276, 0.26536631491035223, 0.3308978680288419, 0.24206786206923425, 0.24487479985691607, 0.2539507100591436, 0.24011229304596782, 0.24012731190305203, 0.23376119195017964, 3.0736863389611244, 5.454608349013142, 0.23844668397214264, 0.23313227505423129, 0.24111499590799212, 0.23444232600741088, 0.23559300310444087, 0.23887486895546317, 0.2308670450001955, 0.22800163482315838, 0.24415974714793265, 0.22985930694267154, 3.3599332800367847, 1.6888335380936041, 0.254239339963533, 0.2528514019213617, 0.251472377916798, 0.2518444269662723, 0.2540884871268645, 0.2598153919680044, 0.2573032259242609, 0.25390682904981077, 0.2515582471387461, 5.494140371098183, 0.8143191379494965, 0.23501959291752428, 0.23714191908948123, 0.23680471908301115, 0.23260392004158348, 0.2397349098464474, 0.25440652307588607, 0.2551693341229111, 0.2555427150800824, 0.2549610409187153, 0.262247726903297, 0.2565293119987473, 0.25978270289488137, 0.2597151338122785, 0.25979775190353394, 8.79942558100447, 4.2771238358691335, 0.47419394890312105, 0.25046966201625764, 0.2515510180965066, 0.24729631701484323, 0.24770437099505216, 0.25385651097167283, 0.2618143099825829, 0.256894304882735, 0.23773664701730013, 0.23643074498977512, 0.23371615109499544, 0.2343672189163044, 0.23391605110373348, 0.23348442290443927, 0.2394326999783516, 11.960624307044782, 3.6245032830629498, 0.25286101503297687, 0.24906757893040776, 0.24719527608249336, 0.24825661210343242, 0.24868838500697166, 0.2512796079972759, 0.26462450390681624, 0.25361922895535827, 0.26542040205094963, 0.2572052931645885, 0.25116379698738456, 0.2548625549534336, 0.2520213848911226, 0.25340388203039765, 9.786080386955291, 4.146915941033512, 0.25376143713947386, 0.24564530095085502, 0.24323290202300996, 0.24353831203188747, 0.24109426012728363, 0.24308621499221772, 0.2491076331352815, 0.24439024704042822, 0.2435111029772088, 0.24600026512052864, 0.2461735469987616, 0.2333271490642801, 4.722994087031111, 6.916646301862784, 0.386709550046362, 0.251659682020545, 0.2491323041031137, 0.25265534094069153, 0.25679113902151585, 0.25177766697015613, 0.25277117686346173, 0.25195907603483647, 0.25955879408866167, 0.255572073161602, 2.8071263069286942, 6.486098140012473, 0.24200535705313087, 0.24196343612857163, 0.2475049908971414, 0.24572852801065892, 0.24593147088307887, 0.24534385208971798, 0.2438277080655098, 0.24526742496527731, 0.24513256398495287, 0.24540515698026866, 0.2430530219571665, 0.2439451339887455, 0.24621825909707695, 0.24848339997697622, 4.855698894942179, 0.30135459813755006, 0.25282949989195913, 0.23958103300537914, 0.2444021450355649, 0.24284454516600817, 0.29054352606181055, 0.2758578910725191, 0.2796572250081226, 0.29920319211669266, 0.25604023691266775, 0.2569624480092898, 7.001272014924325, 6.603762212092988, 0.24224706320092082, 0.23581031209323555, 0.23293076606933028, 0.23368761804886162, 0.23432971618603915, 0.23606191703584045, 0.23691315506584942, 0.2350940831238404, 0.23594288108870387, 0.23629320797044784]
Total Epoch List: [16, 336]
Total Time List: [0.05510198697447777, 0.05002069193869829]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351db65c90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.1726;  Loss pred: 2.1726; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.1804;  Loss pred: 2.1804; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4884 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.1619;  Loss pred: 2.1619; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4884 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 2.1551;  Loss pred: 2.1551; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 2.1438;  Loss pred: 2.1438; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.1454;  Loss pred: 2.1454; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.1006;  Loss pred: 2.1006; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.0967;  Loss pred: 2.0967; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 2.0762;  Loss pred: 2.0762; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.0334;  Loss pred: 2.0334; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 2.0065;  Loss pred: 2.0065; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 2.0033;  Loss pred: 2.0033; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 1.9983;  Loss pred: 1.9983; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 1.9684;  Loss pred: 1.9684; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 1.9116;  Loss pred: 1.9116; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 1.8965;  Loss pred: 1.8965; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 1.9019;  Loss pred: 1.9019; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 1.8653;  Loss pred: 1.8653; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 1.8343;  Loss pred: 1.8343; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 1.8250;  Loss pred: 1.8250; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 1.7759;  Loss pred: 1.7759; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 1.7625;  Loss pred: 1.7625; Loss self: 0.0000; time: 4.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 1.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.77s
Epoch 23/1000, LR 0.000270
Train loss: 1.7506;  Loss pred: 1.7506; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 1.7541;  Loss pred: 1.7541; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 1.6981;  Loss pred: 1.6981; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 1.7020;  Loss pred: 1.7020; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 1.6750;  Loss pred: 1.6750; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 1.6389;  Loss pred: 1.6389; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 1.6492;  Loss pred: 1.6492; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 1.6301;  Loss pred: 1.6301; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 1.5943;  Loss pred: 1.5943; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 1.5800;  Loss pred: 1.5800; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 1.5518;  Loss pred: 1.5518; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 1.5558;  Loss pred: 1.5558; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 1.5315;  Loss pred: 1.5315; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 1.5321;  Loss pred: 1.5321; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 1.4961;  Loss pred: 1.4961; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 2.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 1.51s
Epoch 38/1000, LR 0.000270
Train loss: 1.4803;  Loss pred: 1.4803; Loss self: 0.0000; time: 6.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 1.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.11s
Epoch 39/1000, LR 0.000269
Train loss: 1.4788;  Loss pred: 1.4788; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 1.4817;  Loss pred: 1.4817; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 1.4501;  Loss pred: 1.4501; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 1.4357;  Loss pred: 1.4357; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 1.4239;  Loss pred: 1.4239; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 1.4322;  Loss pred: 1.4322; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.05s
Test loss: 0.6921 score: 0.5116 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 1.4122;  Loss pred: 1.4122; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.05s
Test loss: 0.6920 score: 0.5116 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 1.4020;  Loss pred: 1.4020; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
Test loss: 0.6919 score: 0.5116 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 1.3793;  Loss pred: 1.3793; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.05s
Test loss: 0.6918 score: 0.5116 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 1.3542;  Loss pred: 1.3542; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.05s
Test loss: 0.6917 score: 0.5116 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 1.3524;  Loss pred: 1.3524; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.12s
Test loss: 0.6915 score: 0.5116 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 1.3577;  Loss pred: 1.3577; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.05s
Test loss: 0.6914 score: 0.5116 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 1.3381;  Loss pred: 1.3381; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.05s
Test loss: 0.6912 score: 0.5116 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 1.3186;  Loss pred: 1.3186; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.05s
Test loss: 0.6911 score: 0.5116 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 1.3084;  Loss pred: 1.3084; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.05s
Test loss: 0.6909 score: 0.5116 time: 0.15s
Epoch 54/1000, LR 0.000269
Train loss: 1.3213;  Loss pred: 1.3213; Loss self: 0.0000; time: 6.53s
Val loss: 0.6907 score: 0.5227 time: 0.09s
Test loss: 0.6908 score: 0.5116 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 1.2937;  Loss pred: 1.2937; Loss self: 0.0000; time: 0.15s
Val loss: 0.6905 score: 0.5227 time: 0.05s
Test loss: 0.6906 score: 0.5116 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 1.2921;  Loss pred: 1.2921; Loss self: 0.0000; time: 0.15s
Val loss: 0.6904 score: 0.5227 time: 0.05s
Test loss: 0.6904 score: 0.5116 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 1.2807;  Loss pred: 1.2807; Loss self: 0.0000; time: 0.15s
Val loss: 0.6902 score: 0.5227 time: 0.05s
Test loss: 0.6903 score: 0.5116 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 1.2806;  Loss pred: 1.2806; Loss self: 0.0000; time: 0.15s
Val loss: 0.6901 score: 0.5227 time: 0.05s
Test loss: 0.6901 score: 0.5349 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 1.2746;  Loss pred: 1.2746; Loss self: 0.0000; time: 0.15s
Val loss: 0.6899 score: 0.5227 time: 0.05s
Test loss: 0.6899 score: 0.5349 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 1.2536;  Loss pred: 1.2536; Loss self: 0.0000; time: 0.15s
Val loss: 0.6898 score: 0.5227 time: 0.05s
Test loss: 0.6898 score: 0.5349 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 1.2539;  Loss pred: 1.2539; Loss self: 0.0000; time: 0.15s
Val loss: 0.6896 score: 0.5227 time: 0.05s
Test loss: 0.6896 score: 0.5349 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 1.2486;  Loss pred: 1.2486; Loss self: 0.0000; time: 0.15s
Val loss: 0.6895 score: 0.5227 time: 0.05s
Test loss: 0.6894 score: 0.5349 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 1.2418;  Loss pred: 1.2418; Loss self: 0.0000; time: 0.15s
Val loss: 0.6893 score: 0.5227 time: 0.05s
Test loss: 0.6892 score: 0.5349 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 1.2301;  Loss pred: 1.2301; Loss self: 0.0000; time: 0.15s
Val loss: 0.6891 score: 0.5227 time: 0.05s
Test loss: 0.6890 score: 0.5349 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 1.2345;  Loss pred: 1.2345; Loss self: 0.0000; time: 0.15s
Val loss: 0.6889 score: 0.5227 time: 0.05s
Test loss: 0.6888 score: 0.5349 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 1.2173;  Loss pred: 1.2173; Loss self: 0.0000; time: 0.15s
Val loss: 0.6887 score: 0.5455 time: 0.05s
Test loss: 0.6886 score: 0.5349 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 1.2205;  Loss pred: 1.2205; Loss self: 0.0000; time: 0.15s
Val loss: 0.6885 score: 0.5455 time: 0.05s
Test loss: 0.6884 score: 0.5349 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 1.2126;  Loss pred: 1.2126; Loss self: 0.0000; time: 0.15s
Val loss: 0.6883 score: 0.5455 time: 0.05s
Test loss: 0.6882 score: 0.5349 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.2017;  Loss pred: 1.2017; Loss self: 0.0000; time: 0.15s
Val loss: 0.6881 score: 0.5455 time: 0.05s
Test loss: 0.6880 score: 0.5349 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 1.1965;  Loss pred: 1.1965; Loss self: 0.0000; time: 0.15s
Val loss: 0.6879 score: 0.5455 time: 0.05s
Test loss: 0.6877 score: 0.5349 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 1.1904;  Loss pred: 1.1904; Loss self: 0.0000; time: 0.15s
Val loss: 0.6876 score: 0.5455 time: 0.17s
Test loss: 0.6874 score: 0.5349 time: 2.03s
Epoch 72/1000, LR 0.000267
Train loss: 1.1942;  Loss pred: 1.1942; Loss self: 0.0000; time: 4.24s
Val loss: 0.6874 score: 0.5455 time: 0.06s
Test loss: 0.6871 score: 0.5349 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 1.1772;  Loss pred: 1.1772; Loss self: 0.0000; time: 0.16s
Val loss: 0.6871 score: 0.5455 time: 0.05s
Test loss: 0.6869 score: 0.5349 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 1.1782;  Loss pred: 1.1782; Loss self: 0.0000; time: 0.16s
Val loss: 0.6869 score: 0.5455 time: 0.05s
Test loss: 0.6866 score: 0.5349 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 1.1736;  Loss pred: 1.1736; Loss self: 0.0000; time: 0.16s
Val loss: 0.6866 score: 0.5455 time: 0.05s
Test loss: 0.6863 score: 0.5349 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 1.1671;  Loss pred: 1.1671; Loss self: 0.0000; time: 0.16s
Val loss: 0.6863 score: 0.5455 time: 0.05s
Test loss: 0.6860 score: 0.5349 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 1.1574;  Loss pred: 1.1574; Loss self: 0.0000; time: 0.15s
Val loss: 0.6860 score: 0.5455 time: 0.05s
Test loss: 0.6857 score: 0.5349 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 1.1577;  Loss pred: 1.1577; Loss self: 0.0000; time: 0.15s
Val loss: 0.6857 score: 0.5455 time: 0.05s
Test loss: 0.6853 score: 0.5116 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 1.1526;  Loss pred: 1.1526; Loss self: 0.0000; time: 0.15s
Val loss: 0.6854 score: 0.5455 time: 0.05s
Test loss: 0.6850 score: 0.5116 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 1.1477;  Loss pred: 1.1477; Loss self: 0.0000; time: 0.15s
Val loss: 0.6851 score: 0.5455 time: 0.05s
Test loss: 0.6846 score: 0.5116 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 1.1393;  Loss pred: 1.1393; Loss self: 0.0000; time: 0.15s
Val loss: 0.6847 score: 0.5455 time: 0.05s
Test loss: 0.6842 score: 0.5116 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 1.1425;  Loss pred: 1.1425; Loss self: 0.0000; time: 0.15s
Val loss: 0.6843 score: 0.5455 time: 0.05s
Test loss: 0.6838 score: 0.5116 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 1.1336;  Loss pred: 1.1336; Loss self: 0.0000; time: 0.15s
Val loss: 0.6839 score: 0.5455 time: 0.05s
Test loss: 0.6833 score: 0.5116 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.1287;  Loss pred: 1.1287; Loss self: 0.0000; time: 0.15s
Val loss: 0.6835 score: 0.5682 time: 0.05s
Test loss: 0.6829 score: 0.5116 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 1.1312;  Loss pred: 1.1312; Loss self: 0.0000; time: 0.18s
Val loss: 0.6831 score: 0.5682 time: 0.06s
Test loss: 0.6824 score: 0.5116 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 1.1181;  Loss pred: 1.1181; Loss self: 0.0000; time: 0.16s
Val loss: 0.6827 score: 0.5682 time: 0.05s
Test loss: 0.6820 score: 0.5116 time: 0.04s
Epoch 87/1000, LR 0.000266
Train loss: 1.1149;  Loss pred: 1.1149; Loss self: 0.0000; time: 0.15s
Val loss: 0.6823 score: 0.5682 time: 0.05s
Test loss: 0.6815 score: 0.5349 time: 0.04s
Epoch 88/1000, LR 0.000266
Train loss: 1.1102;  Loss pred: 1.1102; Loss self: 0.0000; time: 2.86s
Val loss: 0.6818 score: 0.5682 time: 1.44s
Test loss: 0.6811 score: 0.5349 time: 1.50s
Epoch 89/1000, LR 0.000266
Train loss: 1.1087;  Loss pred: 1.1087; Loss self: 0.0000; time: 4.02s
Val loss: 0.6814 score: 0.5682 time: 0.05s
Test loss: 0.6806 score: 0.5581 time: 0.05s
Epoch 90/1000, LR 0.000266
Train loss: 1.1071;  Loss pred: 1.1071; Loss self: 0.0000; time: 0.16s
Val loss: 0.6809 score: 0.5682 time: 0.05s
Test loss: 0.6800 score: 0.5581 time: 0.05s
Epoch 91/1000, LR 0.000266
Train loss: 1.1036;  Loss pred: 1.1036; Loss self: 0.0000; time: 0.16s
Val loss: 0.6804 score: 0.5682 time: 0.05s
Test loss: 0.6795 score: 0.5581 time: 0.05s
Epoch 92/1000, LR 0.000266
Train loss: 1.1020;  Loss pred: 1.1020; Loss self: 0.0000; time: 0.16s
Val loss: 0.6799 score: 0.5682 time: 0.05s
Test loss: 0.6789 score: 0.5581 time: 0.05s
Epoch 93/1000, LR 0.000265
Train loss: 1.0911;  Loss pred: 1.0911; Loss self: 0.0000; time: 0.16s
Val loss: 0.6794 score: 0.5682 time: 0.05s
Test loss: 0.6783 score: 0.5581 time: 0.05s
Epoch 94/1000, LR 0.000265
Train loss: 1.0957;  Loss pred: 1.0957; Loss self: 0.0000; time: 0.16s
Val loss: 0.6788 score: 0.5682 time: 0.05s
Test loss: 0.6778 score: 0.5581 time: 0.04s
Epoch 95/1000, LR 0.000265
Train loss: 1.0873;  Loss pred: 1.0873; Loss self: 0.0000; time: 0.15s
Val loss: 0.6783 score: 0.5682 time: 0.05s
Test loss: 0.6772 score: 0.5581 time: 0.04s
Epoch 96/1000, LR 0.000265
Train loss: 1.0901;  Loss pred: 1.0901; Loss self: 0.0000; time: 0.15s
Val loss: 0.6777 score: 0.5682 time: 0.05s
Test loss: 0.6766 score: 0.5581 time: 0.04s
Epoch 97/1000, LR 0.000265
Train loss: 1.0789;  Loss pred: 1.0789; Loss self: 0.0000; time: 0.15s
Val loss: 0.6771 score: 0.5682 time: 0.05s
Test loss: 0.6759 score: 0.5581 time: 0.04s
Epoch 98/1000, LR 0.000265
Train loss: 1.0804;  Loss pred: 1.0804; Loss self: 0.0000; time: 0.15s
Val loss: 0.6765 score: 0.5909 time: 0.05s
Test loss: 0.6753 score: 0.5581 time: 0.04s
Epoch 99/1000, LR 0.000265
Train loss: 1.0736;  Loss pred: 1.0736; Loss self: 0.0000; time: 0.15s
Val loss: 0.6759 score: 0.5909 time: 0.05s
Test loss: 0.6746 score: 0.5581 time: 0.04s
Epoch 100/1000, LR 0.000265
Train loss: 1.0717;  Loss pred: 1.0717; Loss self: 0.0000; time: 0.15s
Val loss: 0.6752 score: 0.5909 time: 0.05s
Test loss: 0.6739 score: 0.5814 time: 0.04s
Epoch 101/1000, LR 0.000265
Train loss: 1.0663;  Loss pred: 1.0663; Loss self: 0.0000; time: 0.15s
Val loss: 0.6746 score: 0.6136 time: 0.05s
Test loss: 0.6732 score: 0.5814 time: 0.04s
Epoch 102/1000, LR 0.000264
Train loss: 1.0673;  Loss pred: 1.0673; Loss self: 0.0000; time: 0.15s
Val loss: 0.6739 score: 0.6364 time: 0.05s
Test loss: 0.6725 score: 0.5814 time: 0.04s
Epoch 103/1000, LR 0.000264
Train loss: 1.0655;  Loss pred: 1.0655; Loss self: 0.0000; time: 5.53s
Val loss: 0.6732 score: 0.6364 time: 0.08s
Test loss: 0.6717 score: 0.5581 time: 0.04s
Epoch 104/1000, LR 0.000264
Train loss: 1.0560;  Loss pred: 1.0560; Loss self: 0.0000; time: 0.15s
Val loss: 0.6724 score: 0.6364 time: 0.05s
Test loss: 0.6709 score: 0.5581 time: 0.04s
Epoch 105/1000, LR 0.000264
Train loss: 1.0569;  Loss pred: 1.0569; Loss self: 0.0000; time: 0.15s
Val loss: 0.6717 score: 0.6591 time: 0.05s
Test loss: 0.6701 score: 0.5814 time: 0.04s
Epoch 106/1000, LR 0.000264
Train loss: 1.0535;  Loss pred: 1.0535; Loss self: 0.0000; time: 0.15s
Val loss: 0.6709 score: 0.6591 time: 0.05s
Test loss: 0.6693 score: 0.6047 time: 0.04s
Epoch 107/1000, LR 0.000264
Train loss: 1.0538;  Loss pred: 1.0538; Loss self: 0.0000; time: 0.15s
Val loss: 0.6701 score: 0.6591 time: 0.05s
Test loss: 0.6684 score: 0.6047 time: 0.04s
Epoch 108/1000, LR 0.000264
Train loss: 1.0525;  Loss pred: 1.0525; Loss self: 0.0000; time: 0.15s
Val loss: 0.6693 score: 0.6818 time: 0.05s
Test loss: 0.6675 score: 0.6047 time: 0.04s
Epoch 109/1000, LR 0.000264
Train loss: 1.0519;  Loss pred: 1.0519; Loss self: 0.0000; time: 0.15s
Val loss: 0.6685 score: 0.6818 time: 0.05s
Test loss: 0.6666 score: 0.6047 time: 0.04s
Epoch 110/1000, LR 0.000263
Train loss: 1.0427;  Loss pred: 1.0427; Loss self: 0.0000; time: 0.15s
Val loss: 0.6676 score: 0.6818 time: 0.05s
Test loss: 0.6657 score: 0.6047 time: 0.04s
Epoch 111/1000, LR 0.000263
Train loss: 1.0461;  Loss pred: 1.0461; Loss self: 0.0000; time: 0.15s
Val loss: 0.6667 score: 0.6818 time: 0.05s
Test loss: 0.6648 score: 0.6047 time: 0.04s
Epoch 112/1000, LR 0.000263
Train loss: 1.0356;  Loss pred: 1.0356; Loss self: 0.0000; time: 0.15s
Val loss: 0.6658 score: 0.7045 time: 0.05s
Test loss: 0.6638 score: 0.6047 time: 0.04s
Epoch 113/1000, LR 0.000263
Train loss: 1.0385;  Loss pred: 1.0385; Loss self: 0.0000; time: 0.15s
Val loss: 0.6649 score: 0.7045 time: 0.05s
Test loss: 0.6628 score: 0.6047 time: 0.04s
Epoch 114/1000, LR 0.000263
Train loss: 1.0328;  Loss pred: 1.0328; Loss self: 0.0000; time: 0.15s
Val loss: 0.6639 score: 0.7045 time: 0.05s
Test loss: 0.6618 score: 0.6047 time: 0.04s
Epoch 115/1000, LR 0.000263
Train loss: 1.0337;  Loss pred: 1.0337; Loss self: 0.0000; time: 0.16s
Val loss: 0.6629 score: 0.7045 time: 0.05s
Test loss: 0.6608 score: 0.6047 time: 0.05s
Epoch 116/1000, LR 0.000263
Train loss: 1.0264;  Loss pred: 1.0264; Loss self: 0.0000; time: 0.16s
Val loss: 0.6619 score: 0.7045 time: 0.05s
Test loss: 0.6597 score: 0.6047 time: 0.04s
Epoch 117/1000, LR 0.000262
Train loss: 1.0270;  Loss pred: 1.0270; Loss self: 0.0000; time: 0.15s
Val loss: 0.6608 score: 0.7273 time: 0.05s
Test loss: 0.6586 score: 0.6279 time: 0.04s
Epoch 118/1000, LR 0.000262
Train loss: 1.0238;  Loss pred: 1.0238; Loss self: 0.0000; time: 0.15s
Val loss: 0.6598 score: 0.7500 time: 0.05s
Test loss: 0.6575 score: 0.6279 time: 0.04s
Epoch 119/1000, LR 0.000262
Train loss: 1.0199;  Loss pred: 1.0199; Loss self: 0.0000; time: 0.15s
Val loss: 0.6587 score: 0.7500 time: 0.05s
Test loss: 0.6564 score: 0.6279 time: 0.04s
Epoch 120/1000, LR 0.000262
Train loss: 1.0179;  Loss pred: 1.0179; Loss self: 0.0000; time: 0.15s
Val loss: 0.6575 score: 0.7500 time: 0.05s
Test loss: 0.6552 score: 0.6279 time: 0.04s
Epoch 121/1000, LR 0.000262
Train loss: 1.0138;  Loss pred: 1.0138; Loss self: 0.0000; time: 0.15s
Val loss: 0.6564 score: 0.7500 time: 0.05s
Test loss: 0.6540 score: 0.6279 time: 0.04s
Epoch 122/1000, LR 0.000262
Train loss: 1.0098;  Loss pred: 1.0098; Loss self: 0.0000; time: 0.15s
Val loss: 0.6551 score: 0.7500 time: 0.05s
Test loss: 0.6527 score: 0.6279 time: 0.04s
Epoch 123/1000, LR 0.000262
Train loss: 1.0110;  Loss pred: 1.0110; Loss self: 0.0000; time: 0.15s
Val loss: 0.6539 score: 0.7500 time: 0.05s
Test loss: 0.6514 score: 0.6279 time: 0.04s
Epoch 124/1000, LR 0.000261
Train loss: 1.0097;  Loss pred: 1.0097; Loss self: 0.0000; time: 0.15s
Val loss: 0.6526 score: 0.7727 time: 0.05s
Test loss: 0.6501 score: 0.6279 time: 0.04s
Epoch 125/1000, LR 0.000261
Train loss: 1.0040;  Loss pred: 1.0040; Loss self: 0.0000; time: 0.15s
Val loss: 0.6513 score: 0.7727 time: 0.05s
Test loss: 0.6487 score: 0.6512 time: 0.04s
Epoch 126/1000, LR 0.000261
Train loss: 1.0059;  Loss pred: 1.0059; Loss self: 0.0000; time: 0.15s
Val loss: 0.6499 score: 0.7727 time: 0.05s
Test loss: 0.6473 score: 0.6512 time: 0.04s
Epoch 127/1000, LR 0.000261
Train loss: 1.0009;  Loss pred: 1.0009; Loss self: 0.0000; time: 0.15s
Val loss: 0.6485 score: 0.7955 time: 0.05s
Test loss: 0.6458 score: 0.6744 time: 0.04s
Epoch 128/1000, LR 0.000261
Train loss: 1.0011;  Loss pred: 1.0011; Loss self: 0.0000; time: 0.16s
Val loss: 0.6472 score: 0.7955 time: 0.05s
Test loss: 0.6443 score: 0.6744 time: 0.04s
Epoch 129/1000, LR 0.000261
Train loss: 0.9980;  Loss pred: 0.9980; Loss self: 0.0000; time: 0.15s
Val loss: 0.6457 score: 0.8182 time: 0.05s
Test loss: 0.6428 score: 0.6744 time: 0.04s
Epoch 130/1000, LR 0.000260
Train loss: 0.9936;  Loss pred: 0.9936; Loss self: 0.0000; time: 0.15s
Val loss: 0.6442 score: 0.8182 time: 0.05s
Test loss: 0.6412 score: 0.6744 time: 0.04s
Epoch 131/1000, LR 0.000260
Train loss: 0.9949;  Loss pred: 0.9949; Loss self: 0.0000; time: 0.16s
Val loss: 0.6427 score: 0.8182 time: 0.05s
Test loss: 0.6397 score: 0.6744 time: 0.05s
Epoch 132/1000, LR 0.000260
Train loss: 0.9883;  Loss pred: 0.9883; Loss self: 0.0000; time: 0.16s
Val loss: 0.6411 score: 0.8182 time: 0.05s
Test loss: 0.6381 score: 0.6744 time: 0.05s
Epoch 133/1000, LR 0.000260
Train loss: 0.9881;  Loss pred: 0.9881; Loss self: 0.0000; time: 5.70s
Val loss: 0.6395 score: 0.8182 time: 0.11s
Test loss: 0.6366 score: 0.6744 time: 0.13s
Epoch 134/1000, LR 0.000260
Train loss: 0.9820;  Loss pred: 0.9820; Loss self: 0.0000; time: 0.24s
Val loss: 0.6379 score: 0.8182 time: 0.05s
Test loss: 0.6350 score: 0.6977 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9812;  Loss pred: 0.9812; Loss self: 0.0000; time: 0.14s
Val loss: 0.6362 score: 0.8182 time: 0.05s
Test loss: 0.6333 score: 0.6977 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9807;  Loss pred: 0.9807; Loss self: 0.0000; time: 0.14s
Val loss: 0.6345 score: 0.8182 time: 0.05s
Test loss: 0.6315 score: 0.7209 time: 0.04s
Epoch 137/1000, LR 0.000259
Train loss: 0.9785;  Loss pred: 0.9785; Loss self: 0.0000; time: 0.14s
Val loss: 0.6327 score: 0.8182 time: 0.05s
Test loss: 0.6298 score: 0.7674 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9712;  Loss pred: 0.9712; Loss self: 0.0000; time: 0.15s
Val loss: 0.6310 score: 0.8182 time: 0.05s
Test loss: 0.6280 score: 0.7674 time: 0.05s
Epoch 139/1000, LR 0.000259
Train loss: 0.9675;  Loss pred: 0.9675; Loss self: 0.0000; time: 0.15s
Val loss: 0.6292 score: 0.8636 time: 0.05s
Test loss: 0.6262 score: 0.7674 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9699;  Loss pred: 0.9699; Loss self: 0.0000; time: 0.14s
Val loss: 0.6274 score: 0.8636 time: 0.05s
Test loss: 0.6244 score: 0.7907 time: 0.04s
Epoch 141/1000, LR 0.000259
Train loss: 0.9654;  Loss pred: 0.9654; Loss self: 0.0000; time: 0.14s
Val loss: 0.6255 score: 0.8636 time: 0.05s
Test loss: 0.6226 score: 0.7907 time: 0.04s
Epoch 142/1000, LR 0.000259
Train loss: 0.9617;  Loss pred: 0.9617; Loss self: 0.0000; time: 0.14s
Val loss: 0.6236 score: 0.8636 time: 0.05s
Test loss: 0.6208 score: 0.7907 time: 0.04s
Epoch 143/1000, LR 0.000258
Train loss: 0.9635;  Loss pred: 0.9635; Loss self: 0.0000; time: 0.14s
Val loss: 0.6217 score: 0.8636 time: 0.05s
Test loss: 0.6189 score: 0.7907 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9546;  Loss pred: 0.9546; Loss self: 0.0000; time: 0.14s
Val loss: 0.6197 score: 0.8636 time: 0.05s
Test loss: 0.6171 score: 0.7907 time: 0.04s
Epoch 145/1000, LR 0.000258
Train loss: 0.9559;  Loss pred: 0.9559; Loss self: 0.0000; time: 0.15s
Val loss: 0.6177 score: 0.8636 time: 0.05s
Test loss: 0.6152 score: 0.7907 time: 0.04s
Epoch 146/1000, LR 0.000258
Train loss: 0.9549;  Loss pred: 0.9549; Loss self: 0.0000; time: 0.14s
Val loss: 0.6156 score: 0.8636 time: 0.05s
Test loss: 0.6133 score: 0.7907 time: 0.04s
Epoch 147/1000, LR 0.000258
Train loss: 0.9501;  Loss pred: 0.9501; Loss self: 0.0000; time: 0.15s
Val loss: 0.6136 score: 0.8636 time: 0.10s
Test loss: 0.6113 score: 0.8140 time: 0.04s
Epoch 148/1000, LR 0.000257
Train loss: 0.9488;  Loss pred: 0.9488; Loss self: 0.0000; time: 0.14s
Val loss: 0.6114 score: 0.8636 time: 0.05s
Test loss: 0.6093 score: 0.8140 time: 0.04s
Epoch 149/1000, LR 0.000257
Train loss: 0.9471;  Loss pred: 0.9471; Loss self: 0.0000; time: 0.14s
Val loss: 0.6092 score: 0.8636 time: 0.05s
Test loss: 0.6072 score: 0.8140 time: 0.04s
Epoch 150/1000, LR 0.000257
Train loss: 0.9458;  Loss pred: 0.9458; Loss self: 0.0000; time: 0.14s
Val loss: 0.6070 score: 0.8864 time: 0.05s
Test loss: 0.6051 score: 0.8372 time: 0.04s
Epoch 151/1000, LR 0.000257
Train loss: 0.9382;  Loss pred: 0.9382; Loss self: 0.0000; time: 0.14s
Val loss: 0.6047 score: 0.9091 time: 0.05s
Test loss: 0.6029 score: 0.8372 time: 0.04s
Epoch 152/1000, LR 0.000257
Train loss: 0.9385;  Loss pred: 0.9385; Loss self: 0.0000; time: 0.14s
Val loss: 0.6024 score: 0.9091 time: 0.05s
Test loss: 0.6006 score: 0.8372 time: 0.05s
Epoch 153/1000, LR 0.000257
Train loss: 0.9362;  Loss pred: 0.9362; Loss self: 0.0000; time: 0.17s
Val loss: 0.6000 score: 0.9091 time: 0.11s
Test loss: 0.5984 score: 0.8372 time: 0.04s
Epoch 154/1000, LR 0.000256
Train loss: 0.9339;  Loss pred: 0.9339; Loss self: 0.0000; time: 0.15s
Val loss: 0.5977 score: 0.9091 time: 0.05s
Test loss: 0.5962 score: 0.8372 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9302;  Loss pred: 0.9302; Loss self: 0.0000; time: 0.15s
Val loss: 0.5953 score: 0.9091 time: 0.05s
Test loss: 0.5939 score: 0.8372 time: 0.05s
Epoch 156/1000, LR 0.000256
Train loss: 0.9253;  Loss pred: 0.9253; Loss self: 0.0000; time: 0.15s
Val loss: 0.5929 score: 0.9091 time: 0.05s
Test loss: 0.5916 score: 0.8140 time: 0.05s
Epoch 157/1000, LR 0.000256
Train loss: 0.9244;  Loss pred: 0.9244; Loss self: 0.0000; time: 0.15s
Val loss: 0.5905 score: 0.9091 time: 0.05s
Test loss: 0.5894 score: 0.8140 time: 0.05s
Epoch 158/1000, LR 0.000256
Train loss: 0.9211;  Loss pred: 0.9211; Loss self: 0.0000; time: 0.15s
Val loss: 0.5881 score: 0.9091 time: 0.05s
Test loss: 0.5871 score: 0.8140 time: 0.05s
Epoch 159/1000, LR 0.000255
Train loss: 0.9184;  Loss pred: 0.9184; Loss self: 0.0000; time: 0.15s
Val loss: 0.5856 score: 0.9091 time: 0.05s
Test loss: 0.5849 score: 0.8140 time: 0.04s
Epoch 160/1000, LR 0.000255
Train loss: 0.9146;  Loss pred: 0.9146; Loss self: 0.0000; time: 0.15s
Val loss: 0.5831 score: 0.9091 time: 0.06s
Test loss: 0.5826 score: 0.8140 time: 0.09s
Epoch 161/1000, LR 0.000255
Train loss: 0.9140;  Loss pred: 0.9140; Loss self: 0.0000; time: 0.15s
Val loss: 0.5806 score: 0.9091 time: 0.05s
Test loss: 0.5803 score: 0.8140 time: 0.05s
Epoch 162/1000, LR 0.000255
Train loss: 0.9111;  Loss pred: 0.9111; Loss self: 0.0000; time: 0.15s
Val loss: 0.5780 score: 0.9091 time: 0.05s
Test loss: 0.5781 score: 0.8140 time: 0.05s
Epoch 163/1000, LR 0.000255
Train loss: 0.9063;  Loss pred: 0.9063; Loss self: 0.0000; time: 0.15s
Val loss: 0.5755 score: 0.9091 time: 0.05s
Test loss: 0.5757 score: 0.8140 time: 0.05s
Epoch 164/1000, LR 0.000254
Train loss: 0.9037;  Loss pred: 0.9037; Loss self: 0.0000; time: 0.15s
Val loss: 0.5729 score: 0.9091 time: 0.05s
Test loss: 0.5734 score: 0.8140 time: 0.05s
Epoch 165/1000, LR 0.000254
Train loss: 0.9024;  Loss pred: 0.9024; Loss self: 0.0000; time: 0.15s
Val loss: 0.5703 score: 0.9091 time: 0.05s
Test loss: 0.5710 score: 0.8140 time: 0.04s
Epoch 166/1000, LR 0.000254
Train loss: 0.9009;  Loss pred: 0.9009; Loss self: 0.0000; time: 0.15s
Val loss: 0.5676 score: 0.9318 time: 0.05s
Test loss: 0.5686 score: 0.8140 time: 0.04s
Epoch 167/1000, LR 0.000254
Train loss: 0.8955;  Loss pred: 0.8955; Loss self: 0.0000; time: 0.16s
Val loss: 0.5650 score: 0.9545 time: 0.05s
Test loss: 0.5662 score: 0.8140 time: 0.05s
Epoch 168/1000, LR 0.000254
Train loss: 0.8917;  Loss pred: 0.8917; Loss self: 0.0000; time: 0.15s
Val loss: 0.5623 score: 0.9545 time: 0.05s
Test loss: 0.5637 score: 0.8140 time: 0.04s
Epoch 169/1000, LR 0.000253
Train loss: 0.8921;  Loss pred: 0.8921; Loss self: 0.0000; time: 0.15s
Val loss: 0.5596 score: 0.9545 time: 0.05s
Test loss: 0.5612 score: 0.8140 time: 0.04s
Epoch 170/1000, LR 0.000253
Train loss: 0.8853;  Loss pred: 0.8853; Loss self: 0.0000; time: 0.15s
Val loss: 0.5569 score: 0.9545 time: 0.05s
Test loss: 0.5588 score: 0.8140 time: 0.05s
Epoch 171/1000, LR 0.000253
Train loss: 0.8837;  Loss pred: 0.8837; Loss self: 0.0000; time: 0.15s
Val loss: 0.5542 score: 0.9545 time: 0.05s
Test loss: 0.5564 score: 0.8140 time: 0.05s
Epoch 172/1000, LR 0.000253
Train loss: 0.8821;  Loss pred: 0.8821; Loss self: 0.0000; time: 0.15s
Val loss: 0.5514 score: 0.9545 time: 2.26s
Test loss: 0.5540 score: 0.8140 time: 1.68s
Epoch 173/1000, LR 0.000253
Train loss: 0.8809;  Loss pred: 0.8809; Loss self: 0.0000; time: 6.05s
Val loss: 0.5486 score: 0.9545 time: 0.16s
Test loss: 0.5516 score: 0.8140 time: 2.09s
Epoch 174/1000, LR 0.000252
Train loss: 0.8760;  Loss pred: 0.8760; Loss self: 0.0000; time: 3.54s
Val loss: 0.5459 score: 0.9545 time: 0.05s
Test loss: 0.5492 score: 0.8140 time: 0.05s
Epoch 175/1000, LR 0.000252
Train loss: 0.8753;  Loss pred: 0.8753; Loss self: 0.0000; time: 0.16s
Val loss: 0.5431 score: 0.9545 time: 0.05s
Test loss: 0.5468 score: 0.8140 time: 0.05s
Epoch 176/1000, LR 0.000252
Train loss: 0.8712;  Loss pred: 0.8712; Loss self: 0.0000; time: 0.16s
Val loss: 0.5403 score: 0.9545 time: 0.05s
Test loss: 0.5444 score: 0.8140 time: 0.05s
Epoch 177/1000, LR 0.000252
Train loss: 0.8690;  Loss pred: 0.8690; Loss self: 0.0000; time: 0.16s
Val loss: 0.5374 score: 0.9545 time: 0.05s
Test loss: 0.5420 score: 0.8140 time: 0.05s
Epoch 178/1000, LR 0.000251
Train loss: 0.8654;  Loss pred: 0.8654; Loss self: 0.0000; time: 4.93s
Val loss: 0.5346 score: 0.9545 time: 3.13s
Test loss: 0.5397 score: 0.8140 time: 2.43s
Epoch 179/1000, LR 0.000251
Train loss: 0.8615;  Loss pred: 0.8615; Loss self: 0.0000; time: 4.36s
Val loss: 0.5318 score: 0.9545 time: 1.00s
Test loss: 0.5373 score: 0.8140 time: 0.24s
Epoch 180/1000, LR 0.000251
Train loss: 0.8629;  Loss pred: 0.8629; Loss self: 0.0000; time: 0.21s
Val loss: 0.5290 score: 0.9545 time: 0.06s
Test loss: 0.5348 score: 0.8140 time: 0.23s
Epoch 181/1000, LR 0.000251
Train loss: 0.8569;  Loss pred: 0.8569; Loss self: 0.0000; time: 0.17s
Val loss: 0.5262 score: 0.9545 time: 0.09s
Test loss: 0.5323 score: 0.8140 time: 0.23s
Epoch 182/1000, LR 0.000251
Train loss: 0.8563;  Loss pred: 0.8563; Loss self: 0.0000; time: 0.32s
Val loss: 0.5234 score: 0.9545 time: 0.19s
Test loss: 0.5298 score: 0.8140 time: 0.24s
Epoch 183/1000, LR 0.000250
Train loss: 0.8509;  Loss pred: 0.8509; Loss self: 0.0000; time: 0.55s
Val loss: 0.5206 score: 0.9545 time: 0.22s
Test loss: 0.5274 score: 0.8140 time: 2.69s
Epoch 184/1000, LR 0.000250
Train loss: 0.8503;  Loss pred: 0.8503; Loss self: 0.0000; time: 8.55s
Val loss: 0.5178 score: 0.9545 time: 2.45s
Test loss: 0.5251 score: 0.8140 time: 2.34s
Epoch 185/1000, LR 0.000250
Train loss: 0.8508;  Loss pred: 0.8508; Loss self: 0.0000; time: 2.01s
Val loss: 0.5150 score: 0.9545 time: 1.25s
Test loss: 0.5227 score: 0.8140 time: 0.25s
Epoch 186/1000, LR 0.000250
Train loss: 0.8449;  Loss pred: 0.8449; Loss self: 0.0000; time: 0.37s
Val loss: 0.5122 score: 0.9545 time: 0.20s
Test loss: 0.5204 score: 0.8140 time: 0.36s
Epoch 187/1000, LR 0.000249
Train loss: 0.8404;  Loss pred: 0.8404; Loss self: 0.0000; time: 0.24s
Val loss: 0.5094 score: 0.9545 time: 0.20s
Test loss: 0.5180 score: 0.8140 time: 0.24s
Epoch 188/1000, LR 0.000249
Train loss: 0.8395;  Loss pred: 0.8395; Loss self: 0.0000; time: 0.54s
Val loss: 0.5066 score: 0.9545 time: 3.04s
Test loss: 0.5157 score: 0.8140 time: 3.05s
Epoch 189/1000, LR 0.000249
Train loss: 0.8362;  Loss pred: 0.8362; Loss self: 0.0000; time: 6.65s
Val loss: 0.5038 score: 0.9545 time: 2.55s
Test loss: 0.5135 score: 0.8140 time: 0.89s
Epoch 190/1000, LR 0.000249
Train loss: 0.8327;  Loss pred: 0.8327; Loss self: 0.0000; time: 0.46s
Val loss: 0.5009 score: 0.9545 time: 0.23s
Test loss: 0.5114 score: 0.8140 time: 0.17s
Epoch 191/1000, LR 0.000249
Train loss: 0.8284;  Loss pred: 0.8284; Loss self: 0.0000; time: 0.18s
Val loss: 0.4981 score: 0.9545 time: 0.05s
Test loss: 0.5092 score: 0.8140 time: 0.05s
Epoch 192/1000, LR 0.000248
Train loss: 0.8298;  Loss pred: 0.8298; Loss self: 0.0000; time: 0.24s
Val loss: 0.4953 score: 0.9545 time: 0.05s
Test loss: 0.5070 score: 0.8140 time: 0.23s
Epoch 193/1000, LR 0.000248
Train loss: 0.8263;  Loss pred: 0.8263; Loss self: 0.0000; time: 0.16s
Val loss: 0.4925 score: 0.9545 time: 0.05s
Test loss: 0.5048 score: 0.8140 time: 0.05s
Epoch 194/1000, LR 0.000248
Train loss: 0.8223;  Loss pred: 0.8223; Loss self: 0.0000; time: 0.16s
Val loss: 0.4897 score: 0.9545 time: 0.06s
Test loss: 0.5026 score: 0.8140 time: 0.05s
Epoch 195/1000, LR 0.000248
Train loss: 0.8195;  Loss pred: 0.8195; Loss self: 0.0000; time: 0.16s
Val loss: 0.4870 score: 0.9545 time: 0.05s
Test loss: 0.5004 score: 0.8140 time: 0.05s
Epoch 196/1000, LR 0.000247
Train loss: 0.8177;  Loss pred: 0.8177; Loss self: 0.0000; time: 0.16s
Val loss: 0.4842 score: 0.9318 time: 0.05s
Test loss: 0.4982 score: 0.8140 time: 0.05s
Epoch 197/1000, LR 0.000247
Train loss: 0.8154;  Loss pred: 0.8154; Loss self: 0.0000; time: 0.16s
Val loss: 0.4815 score: 0.9318 time: 0.05s
Test loss: 0.4959 score: 0.8140 time: 0.05s
Epoch 198/1000, LR 0.000247
Train loss: 0.8132;  Loss pred: 0.8132; Loss self: 0.0000; time: 0.17s
Val loss: 0.4788 score: 0.9318 time: 0.05s
Test loss: 0.4938 score: 0.8140 time: 0.05s
Epoch 199/1000, LR 0.000247
Train loss: 0.8068;  Loss pred: 0.8068; Loss self: 0.0000; time: 0.16s
Val loss: 0.4761 score: 0.9318 time: 0.08s
Test loss: 0.4916 score: 0.8140 time: 0.08s
Epoch 200/1000, LR 0.000246
Train loss: 0.8051;  Loss pred: 0.8051; Loss self: 0.0000; time: 0.18s
Val loss: 0.4733 score: 0.9318 time: 0.06s
Test loss: 0.4896 score: 0.8140 time: 0.05s
Epoch 201/1000, LR 0.000246
Train loss: 0.8027;  Loss pred: 0.8027; Loss self: 0.0000; time: 0.18s
Val loss: 0.4706 score: 0.9318 time: 0.05s
Test loss: 0.4875 score: 0.8140 time: 0.05s
Epoch 202/1000, LR 0.000246
Train loss: 0.8004;  Loss pred: 0.8004; Loss self: 0.0000; time: 0.17s
Val loss: 0.4679 score: 0.9318 time: 0.06s
Test loss: 0.4855 score: 0.8140 time: 0.05s
Epoch 203/1000, LR 0.000246
Train loss: 0.7986;  Loss pred: 0.7986; Loss self: 0.0000; time: 0.22s
Val loss: 0.4652 score: 0.9318 time: 0.06s
Test loss: 0.4835 score: 0.8140 time: 0.05s
Epoch 204/1000, LR 0.000245
Train loss: 0.7972;  Loss pred: 0.7972; Loss self: 0.0000; time: 0.17s
Val loss: 0.4625 score: 0.9318 time: 0.09s
Test loss: 0.4815 score: 0.8140 time: 0.07s
Epoch 205/1000, LR 0.000245
Train loss: 0.7949;  Loss pred: 0.7949; Loss self: 0.0000; time: 0.19s
Val loss: 0.4598 score: 0.9318 time: 0.06s
Test loss: 0.4797 score: 0.8140 time: 0.06s
Epoch 206/1000, LR 0.000245
Train loss: 0.7913;  Loss pred: 0.7913; Loss self: 0.0000; time: 0.20s
Val loss: 0.4570 score: 0.9318 time: 0.06s
Test loss: 0.4779 score: 0.8140 time: 0.06s
Epoch 207/1000, LR 0.000245
Train loss: 0.7862;  Loss pred: 0.7862; Loss self: 0.0000; time: 0.19s
Val loss: 0.4543 score: 0.9545 time: 0.05s
Test loss: 0.4761 score: 0.8140 time: 0.06s
Epoch 208/1000, LR 0.000244
Train loss: 0.7872;  Loss pred: 0.7872; Loss self: 0.0000; time: 0.17s
Val loss: 0.4515 score: 0.9545 time: 0.06s
Test loss: 0.4743 score: 0.8140 time: 0.05s
Epoch 209/1000, LR 0.000244
Train loss: 0.7819;  Loss pred: 0.7819; Loss self: 0.0000; time: 0.18s
Val loss: 0.4489 score: 0.9545 time: 0.06s
Test loss: 0.4725 score: 0.8140 time: 0.06s
Epoch 210/1000, LR 0.000244
Train loss: 0.7822;  Loss pred: 0.7822; Loss self: 0.0000; time: 0.16s
Val loss: 0.4462 score: 0.9545 time: 0.08s
Test loss: 0.4707 score: 0.8140 time: 0.09s
Epoch 211/1000, LR 0.000244
Train loss: 0.7788;  Loss pred: 0.7788; Loss self: 0.0000; time: 0.17s
Val loss: 0.4436 score: 0.9545 time: 0.06s
Test loss: 0.4690 score: 0.8140 time: 0.05s
Epoch 212/1000, LR 0.000243
Train loss: 0.7776;  Loss pred: 0.7776; Loss self: 0.0000; time: 0.16s
Val loss: 0.4410 score: 0.9545 time: 0.05s
Test loss: 0.4672 score: 0.8140 time: 0.10s
Epoch 213/1000, LR 0.000243
Train loss: 0.7721;  Loss pred: 0.7721; Loss self: 0.0000; time: 0.16s
Val loss: 0.4384 score: 0.9545 time: 0.06s
Test loss: 0.4655 score: 0.8140 time: 0.05s
Epoch 214/1000, LR 0.000243
Train loss: 0.7730;  Loss pred: 0.7730; Loss self: 0.0000; time: 0.17s
Val loss: 0.4358 score: 0.9545 time: 0.07s
Test loss: 0.4638 score: 0.8140 time: 0.09s
Epoch 215/1000, LR 0.000243
Train loss: 0.7702;  Loss pred: 0.7702; Loss self: 0.0000; time: 0.16s
Val loss: 0.4331 score: 0.9545 time: 0.06s
Test loss: 0.4622 score: 0.8140 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.7654;  Loss pred: 0.7654; Loss self: 0.0000; time: 0.16s
Val loss: 0.4305 score: 0.9545 time: 0.07s
Test loss: 0.4606 score: 0.8140 time: 0.05s
Epoch 217/1000, LR 0.000242
Train loss: 0.7626;  Loss pred: 0.7626; Loss self: 0.0000; time: 0.16s
Val loss: 0.4280 score: 0.9545 time: 0.05s
Test loss: 0.4590 score: 0.8140 time: 0.21s
Epoch 218/1000, LR 0.000242
Train loss: 0.7609;  Loss pred: 0.7609; Loss self: 0.0000; time: 0.32s
Val loss: 0.4254 score: 0.9545 time: 0.19s
Test loss: 0.4574 score: 0.8140 time: 0.05s
Epoch 219/1000, LR 0.000242
Train loss: 0.7605;  Loss pred: 0.7605; Loss self: 0.0000; time: 0.50s
Val loss: 0.4229 score: 0.9545 time: 0.05s
Test loss: 0.4559 score: 0.8140 time: 0.20s
Epoch 220/1000, LR 0.000241
Train loss: 0.7553;  Loss pred: 0.7553; Loss self: 0.0000; time: 0.27s
Val loss: 0.4203 score: 0.9545 time: 0.55s
Test loss: 0.4545 score: 0.8140 time: 3.70s
Epoch 221/1000, LR 0.000241
Train loss: 0.7544;  Loss pred: 0.7544; Loss self: 0.0000; time: 6.54s
Val loss: 0.4178 score: 0.9545 time: 0.43s
Test loss: 0.4529 score: 0.8140 time: 0.06s
Epoch 222/1000, LR 0.000241
Train loss: 0.7539;  Loss pred: 0.7539; Loss self: 0.0000; time: 0.30s
Val loss: 0.4153 score: 0.9545 time: 2.74s
Test loss: 0.4515 score: 0.8140 time: 3.25s
Epoch 223/1000, LR 0.000241
Train loss: 0.7504;  Loss pred: 0.7504; Loss self: 0.0000; time: 8.85s
Val loss: 0.4129 score: 0.9545 time: 2.38s
Test loss: 0.4500 score: 0.8140 time: 0.07s
Epoch 224/1000, LR 0.000240
Train loss: 0.7491;  Loss pred: 0.7491; Loss self: 0.0000; time: 0.18s
Val loss: 0.4104 score: 0.9545 time: 0.06s
Test loss: 0.4487 score: 0.8140 time: 0.05s
Epoch 225/1000, LR 0.000240
Train loss: 0.7473;  Loss pred: 0.7473; Loss self: 0.0000; time: 0.17s
Val loss: 0.4079 score: 0.9545 time: 0.11s
Test loss: 0.4473 score: 0.8140 time: 0.05s
Epoch 226/1000, LR 0.000240
Train loss: 0.7430;  Loss pred: 0.7430; Loss self: 0.0000; time: 0.16s
Val loss: 0.4055 score: 0.9545 time: 0.05s
Test loss: 0.4460 score: 0.8140 time: 0.05s
Epoch 227/1000, LR 0.000240
Train loss: 0.7431;  Loss pred: 0.7431; Loss self: 0.0000; time: 0.16s
Val loss: 0.4031 score: 0.9545 time: 0.05s
Test loss: 0.4446 score: 0.8140 time: 0.05s
Epoch 228/1000, LR 0.000239
Train loss: 0.7393;  Loss pred: 0.7393; Loss self: 0.0000; time: 0.16s
Val loss: 0.4007 score: 0.9545 time: 0.05s
Test loss: 0.4433 score: 0.8140 time: 0.05s
Epoch 229/1000, LR 0.000239
Train loss: 0.7373;  Loss pred: 0.7373; Loss self: 0.0000; time: 0.16s
Val loss: 0.3984 score: 0.9545 time: 0.05s
Test loss: 0.4419 score: 0.8140 time: 0.05s
Epoch 230/1000, LR 0.000239
Train loss: 0.7333;  Loss pred: 0.7333; Loss self: 0.0000; time: 0.15s
Val loss: 0.3961 score: 0.9545 time: 0.05s
Test loss: 0.4405 score: 0.8140 time: 0.05s
Epoch 231/1000, LR 0.000238
Train loss: 0.7319;  Loss pred: 0.7319; Loss self: 0.0000; time: 0.17s
Val loss: 0.3939 score: 0.9773 time: 0.08s
Test loss: 0.4391 score: 0.8140 time: 0.04s
Epoch 232/1000, LR 0.000238
Train loss: 0.7342;  Loss pred: 0.7342; Loss self: 0.0000; time: 0.15s
Val loss: 0.3916 score: 0.9773 time: 0.05s
Test loss: 0.4379 score: 0.8140 time: 0.04s
Epoch 233/1000, LR 0.000238
Train loss: 0.7303;  Loss pred: 0.7303; Loss self: 0.0000; time: 0.15s
Val loss: 0.3893 score: 0.9773 time: 0.05s
Test loss: 0.4367 score: 0.8140 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.7254;  Loss pred: 0.7254; Loss self: 0.0000; time: 0.17s
Val loss: 0.3870 score: 0.9773 time: 0.06s
Test loss: 0.4356 score: 0.8140 time: 0.05s
Epoch 235/1000, LR 0.000237
Train loss: 0.7247;  Loss pred: 0.7247; Loss self: 0.0000; time: 0.20s
Val loss: 0.3848 score: 0.9773 time: 0.07s
Test loss: 0.4344 score: 0.8140 time: 0.06s
Epoch 236/1000, LR 0.000237
Train loss: 0.7230;  Loss pred: 0.7230; Loss self: 0.0000; time: 0.37s
Val loss: 0.3825 score: 0.9773 time: 0.26s
Test loss: 0.4333 score: 0.8140 time: 0.24s
Epoch 237/1000, LR 0.000237
Train loss: 0.7192;  Loss pred: 0.7192; Loss self: 0.0000; time: 0.54s
Val loss: 0.3802 score: 0.9773 time: 0.23s
Test loss: 0.4324 score: 0.8140 time: 0.24s
Epoch 238/1000, LR 0.000236
Train loss: 0.7161;  Loss pred: 0.7161; Loss self: 0.0000; time: 0.58s
Val loss: 0.3779 score: 0.9773 time: 2.80s
Test loss: 0.4314 score: 0.8140 time: 3.07s
Epoch 239/1000, LR 0.000236
Train loss: 0.7163;  Loss pred: 0.7163; Loss self: 0.0000; time: 8.39s
Val loss: 0.3757 score: 0.9773 time: 2.99s
Test loss: 0.4306 score: 0.8140 time: 0.28s
Epoch 240/1000, LR 0.000236
Train loss: 0.7130;  Loss pred: 0.7130; Loss self: 0.0000; time: 0.54s
Val loss: 0.3735 score: 0.9773 time: 0.22s
Test loss: 0.4296 score: 0.8140 time: 0.24s
Epoch 241/1000, LR 0.000236
Train loss: 0.7121;  Loss pred: 0.7121; Loss self: 0.0000; time: 0.19s
Val loss: 0.3713 score: 0.9773 time: 0.06s
Test loss: 0.4287 score: 0.8140 time: 0.07s
Epoch 242/1000, LR 0.000235
Train loss: 0.7126;  Loss pred: 0.7126; Loss self: 0.0000; time: 0.20s
Val loss: 0.3692 score: 0.9773 time: 0.06s
Test loss: 0.4278 score: 0.8140 time: 0.06s
Epoch 243/1000, LR 0.000235
Train loss: 0.7110;  Loss pred: 0.7110; Loss self: 0.0000; time: 0.20s
Val loss: 0.3671 score: 0.9773 time: 0.05s
Test loss: 0.4269 score: 0.8140 time: 0.05s
Epoch 244/1000, LR 0.000235
Train loss: 0.7098;  Loss pred: 0.7098; Loss self: 0.0000; time: 0.17s
Val loss: 0.3650 score: 0.9773 time: 0.05s
Test loss: 0.4260 score: 0.8140 time: 0.05s
Epoch 245/1000, LR 0.000234
Train loss: 0.7037;  Loss pred: 0.7037; Loss self: 0.0000; time: 0.27s
Val loss: 0.3630 score: 0.9773 time: 0.23s
Test loss: 0.4251 score: 0.8140 time: 0.05s
Epoch 246/1000, LR 0.000234
Train loss: 0.7014;  Loss pred: 0.7014; Loss self: 0.0000; time: 0.27s
Val loss: 0.3609 score: 0.9773 time: 0.19s
Test loss: 0.4242 score: 0.8140 time: 0.24s
Epoch 247/1000, LR 0.000234
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.36s
Val loss: 0.3590 score: 0.9773 time: 3.65s
Test loss: 0.4233 score: 0.8140 time: 3.08s
Epoch 248/1000, LR 0.000234
Train loss: 0.7039;  Loss pred: 0.7039; Loss self: 0.0000; time: 2.00s
Val loss: 0.3570 score: 0.9773 time: 1.95s
Test loss: 0.4224 score: 0.8140 time: 3.20s
Epoch 249/1000, LR 0.000233
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 3.63s
Val loss: 0.3552 score: 0.9773 time: 0.23s
Test loss: 0.4214 score: 0.8140 time: 0.23s
Epoch 250/1000, LR 0.000233
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.24s
Val loss: 0.3534 score: 0.9773 time: 0.23s
Test loss: 0.4204 score: 0.8140 time: 0.04s
Epoch 251/1000, LR 0.000233
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.34s
Val loss: 0.3516 score: 0.9545 time: 0.15s
Test loss: 0.4194 score: 0.8140 time: 0.05s
Epoch 252/1000, LR 0.000232
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.31s
Val loss: 0.3498 score: 0.9545 time: 0.19s
Test loss: 0.4185 score: 0.8140 time: 0.05s
Epoch 253/1000, LR 0.000232
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 6.89s
Val loss: 0.3479 score: 0.9545 time: 0.72s
Test loss: 0.4177 score: 0.8140 time: 1.80s
Epoch 254/1000, LR 0.000232
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 2.67s
Val loss: 0.3460 score: 0.9545 time: 2.04s
Test loss: 0.4171 score: 0.8140 time: 0.55s
Epoch 255/1000, LR 0.000232
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.87s
Val loss: 0.3441 score: 0.9545 time: 0.06s
Test loss: 0.4165 score: 0.8140 time: 0.55s
Epoch 256/1000, LR 0.000231
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.55s
Val loss: 0.3421 score: 0.9545 time: 0.24s
Test loss: 0.4159 score: 0.8140 time: 0.05s
Epoch 257/1000, LR 0.000231
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.32s
Val loss: 0.3402 score: 0.9545 time: 0.08s
Test loss: 0.4155 score: 0.8140 time: 0.05s
Epoch 258/1000, LR 0.000231
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.22s
Val loss: 0.3382 score: 0.9545 time: 0.05s
Test loss: 0.4151 score: 0.8140 time: 0.18s
Epoch 259/1000, LR 0.000230
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.20s
Val loss: 0.3364 score: 0.9545 time: 0.09s
Test loss: 0.4145 score: 0.8140 time: 0.24s
Epoch 260/1000, LR 0.000230
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.16s
Val loss: 0.3347 score: 0.9545 time: 0.07s
Test loss: 0.4138 score: 0.8140 time: 0.05s
Epoch 261/1000, LR 0.000230
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.44s
Val loss: 0.3329 score: 0.9545 time: 0.06s
Test loss: 0.4131 score: 0.8140 time: 0.09s
Epoch 262/1000, LR 0.000229
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 0.21s
Val loss: 0.3312 score: 0.9545 time: 0.17s
Test loss: 0.4126 score: 0.8140 time: 0.26s
Epoch 263/1000, LR 0.000229
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.55s
Val loss: 0.3295 score: 0.9545 time: 0.22s
Test loss: 0.4119 score: 0.8140 time: 0.24s
Epoch 264/1000, LR 0.000229
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.55s
Val loss: 0.3278 score: 0.9545 time: 0.25s
Test loss: 0.4114 score: 0.8140 time: 1.69s
Epoch 265/1000, LR 0.000228
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 10.12s
Val loss: 0.3260 score: 0.9545 time: 4.20s
Test loss: 0.4109 score: 0.8372 time: 2.88s
Epoch 266/1000, LR 0.000228
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 8.23s
Val loss: 0.3243 score: 0.9545 time: 1.55s
Test loss: 0.4106 score: 0.8140 time: 0.05s
Epoch 267/1000, LR 0.000228
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.16s
Val loss: 0.3225 score: 0.9545 time: 0.05s
Test loss: 0.4102 score: 0.8140 time: 0.05s
Epoch 268/1000, LR 0.000228
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.15s
Val loss: 0.3209 score: 0.9545 time: 0.05s
Test loss: 0.4097 score: 0.8372 time: 0.05s
Epoch 269/1000, LR 0.000227
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.15s
Val loss: 0.3193 score: 0.9545 time: 0.05s
Test loss: 0.4092 score: 0.8372 time: 0.05s
Epoch 270/1000, LR 0.000227
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.15s
Val loss: 0.3179 score: 0.9545 time: 0.05s
Test loss: 0.4085 score: 0.8372 time: 0.05s
Epoch 271/1000, LR 0.000227
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.15s
Val loss: 0.3165 score: 0.9545 time: 0.05s
Test loss: 0.4078 score: 0.8372 time: 0.05s
Epoch 272/1000, LR 0.000226
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.16s
Val loss: 0.3149 score: 0.9545 time: 0.05s
Test loss: 0.4074 score: 0.8372 time: 0.05s
Epoch 273/1000, LR 0.000226
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.16s
Val loss: 0.3134 score: 0.9545 time: 0.05s
Test loss: 0.4070 score: 0.8372 time: 0.05s
Epoch 274/1000, LR 0.000226
Train loss: 0.6579;  Loss pred: 0.6579; Loss self: 0.0000; time: 0.16s
Val loss: 0.3118 score: 0.9545 time: 0.05s
Test loss: 0.4067 score: 0.8372 time: 0.05s
Epoch 275/1000, LR 0.000225
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 0.16s
Val loss: 0.3102 score: 0.9545 time: 0.05s
Test loss: 0.4063 score: 0.8372 time: 0.05s
Epoch 276/1000, LR 0.000225
Train loss: 0.6561;  Loss pred: 0.6561; Loss self: 0.0000; time: 0.16s
Val loss: 0.3087 score: 0.9545 time: 0.05s
Test loss: 0.4059 score: 0.8372 time: 0.05s
Epoch 277/1000, LR 0.000225
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.16s
Val loss: 0.3073 score: 0.9545 time: 0.05s
Test loss: 0.4054 score: 0.8372 time: 0.05s
Epoch 278/1000, LR 0.000224
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.16s
Val loss: 0.3059 score: 0.9545 time: 0.05s
Test loss: 0.4050 score: 0.8372 time: 0.05s
Epoch 279/1000, LR 0.000224
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 0.16s
Val loss: 0.3045 score: 0.9545 time: 0.05s
Test loss: 0.4045 score: 0.8372 time: 0.05s
Epoch 280/1000, LR 0.000224
Train loss: 0.6504;  Loss pred: 0.6504; Loss self: 0.0000; time: 0.16s
Val loss: 0.3032 score: 0.9318 time: 0.05s
Test loss: 0.4041 score: 0.8372 time: 0.05s
Epoch 281/1000, LR 0.000223
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.15s
Val loss: 0.3018 score: 0.9318 time: 0.05s
Test loss: 0.4037 score: 0.8372 time: 0.05s
Epoch 282/1000, LR 0.000223
Train loss: 0.6508;  Loss pred: 0.6508; Loss self: 0.0000; time: 0.16s
Val loss: 0.3003 score: 0.9318 time: 0.05s
Test loss: 0.4035 score: 0.8372 time: 0.05s
Epoch 283/1000, LR 0.000223
Train loss: 0.6505;  Loss pred: 0.6505; Loss self: 0.0000; time: 0.16s
Val loss: 0.2990 score: 0.9318 time: 0.05s
Test loss: 0.4032 score: 0.8372 time: 0.05s
Epoch 284/1000, LR 0.000222
Train loss: 0.6480;  Loss pred: 0.6480; Loss self: 0.0000; time: 0.16s
Val loss: 0.2975 score: 0.9318 time: 0.05s
Test loss: 0.4029 score: 0.8372 time: 0.04s
Epoch 285/1000, LR 0.000222
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.16s
Val loss: 0.2961 score: 0.9318 time: 0.05s
Test loss: 0.4028 score: 0.8372 time: 0.22s
Epoch 286/1000, LR 0.000222
Train loss: 0.6459;  Loss pred: 0.6459; Loss self: 0.0000; time: 0.36s
Val loss: 0.2946 score: 0.9318 time: 0.22s
Test loss: 0.4026 score: 0.8372 time: 0.23s
Epoch 287/1000, LR 0.000221
Train loss: 0.6446;  Loss pred: 0.6446; Loss self: 0.0000; time: 0.24s
Val loss: 0.2932 score: 0.9545 time: 0.24s
Test loss: 0.4025 score: 0.8372 time: 0.06s
Epoch 288/1000, LR 0.000221
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 2.73s
Val loss: 0.2919 score: 0.9318 time: 2.82s
Test loss: 0.4022 score: 0.8372 time: 2.69s
Epoch 289/1000, LR 0.000221
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 7.23s
Val loss: 0.2906 score: 0.9318 time: 3.63s
Test loss: 0.4021 score: 0.8372 time: 0.28s
Epoch 290/1000, LR 0.000220
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.59s
Val loss: 0.2893 score: 0.9318 time: 0.21s
Test loss: 0.4019 score: 0.8372 time: 0.05s
Epoch 291/1000, LR 0.000220
Train loss: 0.6396;  Loss pred: 0.6396; Loss self: 0.0000; time: 0.23s
Val loss: 0.2880 score: 0.9318 time: 0.22s
Test loss: 0.4016 score: 0.8372 time: 0.14s
Epoch 292/1000, LR 0.000220
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.36s
Val loss: 0.2866 score: 0.9545 time: 0.10s
Test loss: 0.4016 score: 0.8372 time: 0.10s
Epoch 293/1000, LR 0.000219
Train loss: 0.6391;  Loss pred: 0.6391; Loss self: 0.0000; time: 0.25s
Val loss: 0.2852 score: 0.9545 time: 0.11s
Test loss: 0.4016 score: 0.8372 time: 0.09s
Epoch 294/1000, LR 0.000219
Train loss: 0.6381;  Loss pred: 0.6381; Loss self: 0.0000; time: 0.76s
Val loss: 0.2840 score: 0.9545 time: 0.06s
Test loss: 0.4014 score: 0.8372 time: 0.07s
Epoch 295/1000, LR 0.000219
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 0.15s
Val loss: 0.2828 score: 0.9545 time: 0.16s
Test loss: 0.4012 score: 0.8372 time: 0.21s
Epoch 296/1000, LR 0.000218
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.28s
Val loss: 0.2817 score: 0.9545 time: 0.21s
Test loss: 0.4009 score: 0.8372 time: 0.05s
Epoch 297/1000, LR 0.000218
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.20s
Val loss: 0.2806 score: 0.9318 time: 0.05s
Test loss: 0.4006 score: 0.8372 time: 0.10s
Epoch 298/1000, LR 0.000218
Train loss: 0.6331;  Loss pred: 0.6331; Loss self: 0.0000; time: 0.47s
Val loss: 0.2794 score: 0.9318 time: 0.12s
Test loss: 0.4004 score: 0.8372 time: 0.22s
Epoch 299/1000, LR 0.000217
Train loss: 0.6339;  Loss pred: 0.6339; Loss self: 0.0000; time: 0.56s
Val loss: 0.2783 score: 0.9318 time: 0.20s
Test loss: 0.4001 score: 0.8372 time: 0.24s
Epoch 300/1000, LR 0.000217
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.29s
Val loss: 0.2773 score: 0.9318 time: 2.85s
Test loss: 0.3998 score: 0.8372 time: 1.99s
Epoch 301/1000, LR 0.000217
Train loss: 0.6291;  Loss pred: 0.6291; Loss self: 0.0000; time: 2.27s
Val loss: 0.2764 score: 0.9318 time: 0.19s
Test loss: 0.3994 score: 0.8372 time: 0.22s
Epoch 302/1000, LR 0.000216
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.54s
Val loss: 0.2755 score: 0.9318 time: 0.20s
Test loss: 0.3990 score: 0.8372 time: 0.23s
Epoch 303/1000, LR 0.000216
Train loss: 0.6296;  Loss pred: 0.6296; Loss self: 0.0000; time: 0.20s
Val loss: 0.2746 score: 0.9318 time: 0.05s
Test loss: 0.3986 score: 0.8372 time: 0.05s
Epoch 304/1000, LR 0.000216
Train loss: 0.6297;  Loss pred: 0.6297; Loss self: 0.0000; time: 0.34s
Val loss: 0.2736 score: 0.9318 time: 0.10s
Test loss: 0.3984 score: 0.8372 time: 0.05s
Epoch 305/1000, LR 0.000215
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.25s
Val loss: 0.2725 score: 0.9318 time: 0.24s
Test loss: 0.3983 score: 0.8372 time: 0.24s
Epoch 306/1000, LR 0.000215
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.21s
Val loss: 0.2712 score: 0.9318 time: 0.05s
Test loss: 0.3984 score: 0.8372 time: 0.23s
Epoch 307/1000, LR 0.000215
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.21s
Val loss: 0.2698 score: 0.9318 time: 0.05s
Test loss: 0.3986 score: 0.8372 time: 1.76s
Epoch 308/1000, LR 0.000214
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 7.66s
Val loss: 0.2685 score: 0.9318 time: 1.04s
Test loss: 0.3989 score: 0.8372 time: 0.53s
Epoch 309/1000, LR 0.000214
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.57s
Val loss: 0.2673 score: 0.9318 time: 0.20s
Test loss: 0.3990 score: 0.8372 time: 0.22s
Epoch 310/1000, LR 0.000214
Train loss: 0.6222;  Loss pred: 0.6222; Loss self: 0.0000; time: 0.19s
Val loss: 0.2661 score: 0.9318 time: 0.06s
Test loss: 0.3991 score: 0.8372 time: 0.05s
Epoch 311/1000, LR 0.000213
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.18s
Val loss: 0.2649 score: 0.9318 time: 0.06s
Test loss: 0.3992 score: 0.8372 time: 0.10s
Epoch 312/1000, LR 0.000213
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.16s
Val loss: 0.2638 score: 0.9318 time: 0.09s
Test loss: 0.3994 score: 0.8372 time: 0.05s
Epoch 313/1000, LR 0.000213
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.17s
Val loss: 0.2627 score: 0.9318 time: 0.08s
Test loss: 0.3994 score: 0.8372 time: 0.05s
Epoch 314/1000, LR 0.000212
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 0.16s
Val loss: 0.2617 score: 0.9318 time: 0.05s
Test loss: 0.3993 score: 0.8372 time: 0.05s
Epoch 315/1000, LR 0.000212
Train loss: 0.6192;  Loss pred: 0.6192; Loss self: 0.0000; time: 0.21s
Val loss: 0.2607 score: 0.9318 time: 0.11s
Test loss: 0.3992 score: 0.8372 time: 0.05s
Epoch 316/1000, LR 0.000212
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.16s
Val loss: 0.2599 score: 0.9318 time: 0.05s
Test loss: 0.3989 score: 0.8372 time: 0.05s
Epoch 317/1000, LR 0.000211
Train loss: 0.6175;  Loss pred: 0.6175; Loss self: 0.0000; time: 0.16s
Val loss: 0.2591 score: 0.9318 time: 0.05s
Test loss: 0.3986 score: 0.8372 time: 0.05s
Epoch 318/1000, LR 0.000211
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.22s
Val loss: 0.2583 score: 0.9318 time: 0.05s
Test loss: 0.3983 score: 0.8372 time: 0.05s
Epoch 319/1000, LR 0.000210
Train loss: 0.6150;  Loss pred: 0.6150; Loss self: 0.0000; time: 0.51s
Val loss: 0.2576 score: 0.9318 time: 0.21s
Test loss: 0.3980 score: 0.8372 time: 0.23s
Epoch 320/1000, LR 0.000210
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.30s
Val loss: 0.2568 score: 0.9318 time: 0.21s
Test loss: 0.3978 score: 0.8372 time: 0.23s
Epoch 321/1000, LR 0.000210
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 6.96s
Val loss: 0.2560 score: 0.9318 time: 0.94s
Test loss: 0.3976 score: 0.8372 time: 0.31s
Epoch 322/1000, LR 0.000209
Train loss: 0.6175;  Loss pred: 0.6175; Loss self: 0.0000; time: 0.33s
Val loss: 0.2550 score: 0.9318 time: 0.24s
Test loss: 0.3976 score: 0.8372 time: 0.24s
Epoch 323/1000, LR 0.000209
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 0.17s
Val loss: 0.2541 score: 0.9318 time: 0.07s
Test loss: 0.3976 score: 0.8372 time: 0.05s
Epoch 324/1000, LR 0.000209
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.23s
Val loss: 0.2531 score: 0.9318 time: 0.19s
Test loss: 0.3978 score: 0.8372 time: 0.05s
Epoch 325/1000, LR 0.000208
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.20s
Val loss: 0.2520 score: 0.9318 time: 0.06s
Test loss: 0.3979 score: 0.8372 time: 0.08s
Epoch 326/1000, LR 0.000208
Train loss: 0.6138;  Loss pred: 0.6138; Loss self: 0.0000; time: 0.45s
Val loss: 0.2508 score: 0.9318 time: 0.20s
Test loss: 0.3982 score: 0.8372 time: 0.24s
Epoch 327/1000, LR 0.000208
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.54s
Val loss: 0.2497 score: 0.9318 time: 0.94s
Test loss: 0.3985 score: 0.8372 time: 1.24s
Epoch 328/1000, LR 0.000207
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 5.70s
Val loss: 0.2487 score: 0.9318 time: 3.43s
Test loss: 0.3986 score: 0.8372 time: 3.00s
Epoch 329/1000, LR 0.000207
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 1.66s
Val loss: 0.2478 score: 0.9318 time: 0.28s
Test loss: 0.3987 score: 0.8372 time: 0.10s
Epoch 330/1000, LR 0.000207
Train loss: 0.6081;  Loss pred: 0.6081; Loss self: 0.0000; time: 3.19s
Val loss: 0.2470 score: 0.9318 time: 3.33s
Test loss: 0.3986 score: 0.8372 time: 3.07s
Epoch 331/1000, LR 0.000206
Train loss: 0.6079;  Loss pred: 0.6079; Loss self: 0.0000; time: 4.55s
Val loss: 0.2463 score: 0.9318 time: 1.21s
Test loss: 0.3985 score: 0.8372 time: 2.56s
Epoch 332/1000, LR 0.000206
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 4.25s
Val loss: 0.2457 score: 0.9318 time: 1.11s
Test loss: 0.3983 score: 0.8372 time: 0.25s
Epoch 333/1000, LR 0.000205
Train loss: 0.6048;  Loss pred: 0.6048; Loss self: 0.0000; time: 0.55s
Val loss: 0.2452 score: 0.9318 time: 0.22s
Test loss: 0.3980 score: 0.8372 time: 0.22s
Epoch 334/1000, LR 0.000205
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.17s
Val loss: 0.2446 score: 0.9318 time: 0.08s
Test loss: 0.3979 score: 0.8372 time: 0.05s
Epoch 335/1000, LR 0.000205
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.21s
Val loss: 0.2439 score: 0.9318 time: 0.05s
Test loss: 0.3978 score: 0.8372 time: 0.05s
Epoch 336/1000, LR 0.000204
Train loss: 0.6033;  Loss pred: 0.6033; Loss self: 0.0000; time: 0.16s
Val loss: 0.2431 score: 0.9318 time: 0.06s
Test loss: 0.3978 score: 0.8372 time: 0.18s
Epoch 337/1000, LR 0.000204
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.16s
Val loss: 0.2423 score: 0.9318 time: 0.07s
Test loss: 0.3979 score: 0.8372 time: 0.05s
Epoch 338/1000, LR 0.000204
Train loss: 0.6040;  Loss pred: 0.6040; Loss self: 0.0000; time: 0.17s
Val loss: 0.2413 score: 0.9318 time: 0.05s
Test loss: 0.3981 score: 0.8372 time: 0.05s
Epoch 339/1000, LR 0.000203
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.16s
Val loss: 0.2400 score: 0.9318 time: 0.05s
Test loss: 0.3986 score: 0.8372 time: 0.05s
Epoch 340/1000, LR 0.000203
Train loss: 0.6020;  Loss pred: 0.6020; Loss self: 0.0000; time: 0.21s
Val loss: 0.2388 score: 0.9318 time: 0.05s
Test loss: 0.3991 score: 0.8372 time: 0.05s
Epoch 341/1000, LR 0.000203
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.20s
Val loss: 0.2377 score: 0.9318 time: 0.05s
Test loss: 0.3996 score: 0.8372 time: 0.06s
Epoch 342/1000, LR 0.000202
Train loss: 0.5970;  Loss pred: 0.5970; Loss self: 0.0000; time: 0.16s
Val loss: 0.2368 score: 0.9318 time: 0.05s
Test loss: 0.3998 score: 0.8372 time: 0.06s
Epoch 343/1000, LR 0.000202
Train loss: 0.5989;  Loss pred: 0.5989; Loss self: 0.0000; time: 0.16s
Val loss: 0.2360 score: 0.9318 time: 0.06s
Test loss: 0.4000 score: 0.8372 time: 0.05s
Epoch 344/1000, LR 0.000201
Train loss: 0.5993;  Loss pred: 0.5993; Loss self: 0.0000; time: 0.21s
Val loss: 0.2353 score: 0.9318 time: 0.05s
Test loss: 0.4000 score: 0.8372 time: 0.05s
Epoch 345/1000, LR 0.000201
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.16s
Val loss: 0.2348 score: 0.9318 time: 0.05s
Test loss: 0.3997 score: 0.8372 time: 0.05s
Epoch 346/1000, LR 0.000201
Train loss: 0.6007;  Loss pred: 0.6007; Loss self: 0.0000; time: 0.16s
Val loss: 0.2343 score: 0.9318 time: 0.06s
Test loss: 0.3996 score: 0.8372 time: 0.16s
Epoch 347/1000, LR 0.000200
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.19s
Val loss: 0.2338 score: 0.9318 time: 0.06s
Test loss: 0.3994 score: 0.8372 time: 0.05s
Epoch 348/1000, LR 0.000200
Train loss: 0.5968;  Loss pred: 0.5968; Loss self: 0.0000; time: 0.23s
Val loss: 0.2333 score: 0.9318 time: 0.05s
Test loss: 0.3993 score: 0.8372 time: 0.06s
Epoch 349/1000, LR 0.000200
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.17s
Val loss: 0.2329 score: 0.9318 time: 0.06s
Test loss: 0.3991 score: 0.8372 time: 0.05s
Epoch 350/1000, LR 0.000199
Train loss: 0.5952;  Loss pred: 0.5952; Loss self: 0.0000; time: 0.23s
Val loss: 0.2324 score: 0.9318 time: 0.06s
Test loss: 0.3991 score: 0.8372 time: 0.06s
Epoch 351/1000, LR 0.000199
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.20s
Val loss: 0.2319 score: 0.9318 time: 0.08s
Test loss: 0.3991 score: 0.8372 time: 0.07s
Epoch 352/1000, LR 0.000198
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.30s
Val loss: 0.2312 score: 0.9318 time: 0.10s
Test loss: 0.3992 score: 0.8372 time: 0.09s
Epoch 353/1000, LR 0.000198
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.17s
Val loss: 0.2304 score: 0.9318 time: 0.17s
Test loss: 0.3994 score: 0.8372 time: 0.05s
Epoch 354/1000, LR 0.000198
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.36s
Val loss: 0.2296 score: 0.9318 time: 0.20s
Test loss: 0.3996 score: 0.8372 time: 0.24s
Epoch 355/1000, LR 0.000197
Train loss: 0.5929;  Loss pred: 0.5929; Loss self: 0.0000; time: 0.18s
Val loss: 0.2287 score: 0.9318 time: 3.02s
Test loss: 0.3999 score: 0.8372 time: 3.16s
Epoch 356/1000, LR 0.000197
Train loss: 0.5936;  Loss pred: 0.5936; Loss self: 0.0000; time: 6.69s
Val loss: 0.2279 score: 0.9318 time: 0.22s
Test loss: 0.4001 score: 0.8372 time: 0.29s
Epoch 357/1000, LR 0.000196
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.79s
Val loss: 0.2271 score: 0.9318 time: 0.25s
Test loss: 0.4005 score: 0.8372 time: 0.14s
Epoch 358/1000, LR 0.000196
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.18s
Val loss: 0.2262 score: 0.9318 time: 0.05s
Test loss: 0.4008 score: 0.8372 time: 0.05s
Epoch 359/1000, LR 0.000196
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.16s
Val loss: 0.2254 score: 0.9545 time: 0.06s
Test loss: 0.4012 score: 0.8372 time: 0.05s
Epoch 360/1000, LR 0.000195
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 0.20s
Val loss: 0.2246 score: 0.9545 time: 0.06s
Test loss: 0.4014 score: 0.8140 time: 0.09s
Epoch 361/1000, LR 0.000195
Train loss: 0.5892;  Loss pred: 0.5892; Loss self: 0.0000; time: 0.19s
Val loss: 0.2240 score: 0.9545 time: 0.07s
Test loss: 0.4015 score: 0.8140 time: 0.25s
Epoch 362/1000, LR 0.000195
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 0.53s
Val loss: 0.2235 score: 0.9545 time: 0.41s
Test loss: 0.4016 score: 0.8140 time: 0.25s
Epoch 363/1000, LR 0.000194
Train loss: 0.5886;  Loss pred: 0.5886; Loss self: 0.0000; time: 0.35s
Val loss: 0.2229 score: 0.9545 time: 0.19s
Test loss: 0.4016 score: 0.8140 time: 0.24s
Epoch 364/1000, LR 0.000194
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 7.74s
Val loss: 0.2224 score: 0.9545 time: 2.31s
Test loss: 0.4017 score: 0.8140 time: 2.15s
Epoch 365/1000, LR 0.000193
Train loss: 0.5896;  Loss pred: 0.5896; Loss self: 0.0000; time: 5.94s
Val loss: 0.2219 score: 0.9545 time: 0.85s
Test loss: 0.4018 score: 0.8140 time: 0.62s
Epoch 366/1000, LR 0.000193
Train loss: 0.5896;  Loss pred: 0.5896; Loss self: 0.0000; time: 2.31s
Val loss: 0.2215 score: 0.9545 time: 1.06s
Test loss: 0.4018 score: 0.8372 time: 0.24s
Epoch 367/1000, LR 0.000193
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.28s
Val loss: 0.2211 score: 0.9545 time: 0.23s
Test loss: 0.4017 score: 0.8372 time: 0.24s
Epoch 368/1000, LR 0.000192
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.55s
Val loss: 0.2205 score: 0.9545 time: 0.16s
Test loss: 0.4019 score: 0.8372 time: 0.05s
Epoch 369/1000, LR 0.000192
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 0.55s
Val loss: 0.2198 score: 0.9545 time: 0.17s
Test loss: 0.4021 score: 0.8372 time: 0.24s
Epoch 370/1000, LR 0.000191
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 0.37s
Val loss: 0.2191 score: 0.9545 time: 0.14s
Test loss: 0.4023 score: 0.8140 time: 2.36s
Epoch 371/1000, LR 0.000191
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 4.72s
Val loss: 0.2184 score: 0.9545 time: 0.65s
Test loss: 0.4025 score: 0.8140 time: 1.11s
Epoch 372/1000, LR 0.000191
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 8.16s
Val loss: 0.2178 score: 0.9545 time: 0.54s
Test loss: 0.4027 score: 0.8140 time: 0.93s
Epoch 373/1000, LR 0.000190
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 0.16s
Val loss: 0.2172 score: 0.9545 time: 0.05s
Test loss: 0.4028 score: 0.8140 time: 0.06s
Epoch 374/1000, LR 0.000190
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.17s
Val loss: 0.2166 score: 0.9545 time: 0.06s
Test loss: 0.4030 score: 0.8140 time: 0.06s
Epoch 375/1000, LR 0.000190
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.17s
Val loss: 0.2160 score: 0.9545 time: 0.05s
Test loss: 0.4031 score: 0.8140 time: 2.67s
Epoch 376/1000, LR 0.000189
Train loss: 0.5836;  Loss pred: 0.5836; Loss self: 0.0000; time: 7.30s
Val loss: 0.2154 score: 0.9545 time: 0.35s
Test loss: 0.4032 score: 0.8140 time: 0.58s
Epoch 377/1000, LR 0.000189
Train loss: 0.5840;  Loss pred: 0.5840; Loss self: 0.0000; time: 1.56s
Val loss: 0.2150 score: 0.9545 time: 1.62s
Test loss: 0.4032 score: 0.8140 time: 1.41s
Epoch 378/1000, LR 0.000188
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 1.95s
Val loss: 0.2145 score: 0.9545 time: 0.57s
Test loss: 0.4032 score: 0.8140 time: 0.25s
Epoch 379/1000, LR 0.000188
Train loss: 0.5830;  Loss pred: 0.5830; Loss self: 0.0000; time: 0.38s
Val loss: 0.2140 score: 0.9545 time: 0.17s
Test loss: 0.4033 score: 0.8140 time: 0.22s
Epoch 380/1000, LR 0.000188
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 0.35s
Val loss: 0.2134 score: 0.9545 time: 0.21s
Test loss: 0.4035 score: 0.8140 time: 0.23s
Epoch 381/1000, LR 0.000187
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.36s
Val loss: 0.2126 score: 0.9545 time: 0.10s
Test loss: 0.4037 score: 0.8140 time: 0.04s
Epoch 382/1000, LR 0.000187
Train loss: 0.5824;  Loss pred: 0.5824; Loss self: 0.0000; time: 0.56s
Val loss: 0.2119 score: 0.9545 time: 0.14s
Test loss: 0.4040 score: 0.8140 time: 0.09s
Epoch 383/1000, LR 0.000186
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.56s
Val loss: 0.2112 score: 0.9545 time: 0.24s
Test loss: 0.4042 score: 0.8140 time: 0.05s
Epoch 384/1000, LR 0.000186
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 5.59s
Val loss: 0.2105 score: 0.9545 time: 3.57s
Test loss: 0.4044 score: 0.8140 time: 1.20s
Epoch 385/1000, LR 0.000186
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 1.64s
Val loss: 0.2099 score: 0.9545 time: 0.20s
Test loss: 0.4046 score: 0.8140 time: 0.24s
Epoch 386/1000, LR 0.000185
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.17s
Val loss: 0.2093 score: 0.9545 time: 0.09s
Test loss: 0.4047 score: 0.8140 time: 0.13s
Epoch 387/1000, LR 0.000185
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.17s
Val loss: 0.2088 score: 0.9545 time: 0.06s
Test loss: 0.4048 score: 0.8140 time: 0.05s
Epoch 388/1000, LR 0.000184
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.18s
Val loss: 0.2083 score: 0.9545 time: 0.05s
Test loss: 0.4049 score: 0.8140 time: 0.05s
Epoch 389/1000, LR 0.000184
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.17s
Val loss: 0.2078 score: 0.9545 time: 0.07s
Test loss: 0.4049 score: 0.8140 time: 0.05s
Epoch 390/1000, LR 0.000184
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 0.18s
Val loss: 0.2074 score: 0.9545 time: 0.05s
Test loss: 0.4050 score: 0.8140 time: 0.09s
Epoch 391/1000, LR 0.000183
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.17s
Val loss: 0.2071 score: 0.9545 time: 0.10s
Test loss: 0.4050 score: 0.8140 time: 0.05s
Epoch 392/1000, LR 0.000183
Train loss: 0.5772;  Loss pred: 0.5772; Loss self: 0.0000; time: 0.17s
Val loss: 0.2067 score: 0.9545 time: 0.05s
Test loss: 0.4051 score: 0.8140 time: 0.10s
Epoch 393/1000, LR 0.000182
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.17s
Val loss: 0.2062 score: 0.9545 time: 0.05s
Test loss: 0.4053 score: 0.8140 time: 0.05s
Epoch 394/1000, LR 0.000182
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.18s
Val loss: 0.2057 score: 0.9545 time: 0.05s
Test loss: 0.4054 score: 0.8140 time: 0.05s
Epoch 395/1000, LR 0.000182
Train loss: 0.5776;  Loss pred: 0.5776; Loss self: 0.0000; time: 0.17s
Val loss: 0.2052 score: 0.9545 time: 0.05s
Test loss: 0.4057 score: 0.8140 time: 0.05s
Epoch 396/1000, LR 0.000181
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 0.16s
Val loss: 0.2046 score: 0.9545 time: 0.06s
Test loss: 0.4059 score: 0.8140 time: 0.07s
Epoch 397/1000, LR 0.000181
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.15s
Val loss: 0.2040 score: 0.9545 time: 0.06s
Test loss: 0.4062 score: 0.8140 time: 0.14s
Epoch 398/1000, LR 0.000180
Train loss: 0.5790;  Loss pred: 0.5790; Loss self: 0.0000; time: 0.19s
Val loss: 0.2033 score: 0.9545 time: 0.06s
Test loss: 0.4065 score: 0.8140 time: 0.05s
Epoch 399/1000, LR 0.000180
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.16s
Val loss: 0.2028 score: 0.9545 time: 0.08s
Test loss: 0.4068 score: 0.8140 time: 0.06s
Epoch 400/1000, LR 0.000180
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.17s
Val loss: 0.2023 score: 0.9545 time: 0.06s
Test loss: 0.4071 score: 0.8140 time: 0.05s
Epoch 401/1000, LR 0.000179
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 0.26s
Val loss: 0.2018 score: 0.9545 time: 0.25s
Test loss: 0.4074 score: 0.8140 time: 0.06s
Epoch 402/1000, LR 0.000179
Train loss: 0.5729;  Loss pred: 0.5729; Loss self: 0.0000; time: 0.33s
Val loss: 0.2014 score: 0.9545 time: 0.16s
Test loss: 0.4076 score: 0.8140 time: 0.24s
Epoch 403/1000, LR 0.000178
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.37s
Val loss: 0.2010 score: 0.9545 time: 2.74s
Test loss: 0.4078 score: 0.8140 time: 2.48s
Epoch 404/1000, LR 0.000178
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 1.94s
Val loss: 0.2006 score: 0.9545 time: 2.95s
Test loss: 0.4080 score: 0.8140 time: 0.68s
Epoch 405/1000, LR 0.000178
Train loss: 0.5729;  Loss pred: 0.5729; Loss self: 0.0000; time: 2.14s
Val loss: 0.2003 score: 0.9545 time: 1.01s
Test loss: 0.4082 score: 0.8140 time: 2.38s
Epoch 406/1000, LR 0.000177
Train loss: 0.5695;  Loss pred: 0.5695; Loss self: 0.0000; time: 1.51s
Val loss: 0.2000 score: 0.9545 time: 0.18s
Test loss: 0.4083 score: 0.8140 time: 0.16s
Epoch 407/1000, LR 0.000177
Train loss: 0.5716;  Loss pred: 0.5716; Loss self: 0.0000; time: 0.41s
Val loss: 0.1997 score: 0.9545 time: 0.15s
Test loss: 0.4085 score: 0.8140 time: 0.05s
Epoch 408/1000, LR 0.000176
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 0.17s
Val loss: 0.1996 score: 0.9545 time: 0.05s
Test loss: 0.4086 score: 0.8140 time: 0.05s
Epoch 409/1000, LR 0.000176
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 0.22s
Val loss: 0.1996 score: 0.9545 time: 0.06s
Test loss: 0.4087 score: 0.8140 time: 0.05s
Epoch 410/1000, LR 0.000175
Train loss: 0.5727;  Loss pred: 0.5727; Loss self: 0.0000; time: 0.17s
Val loss: 0.1995 score: 0.9545 time: 0.08s
Test loss: 0.4088 score: 0.8140 time: 0.10s
Epoch 411/1000, LR 0.000175
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 0.36s
Val loss: 0.1993 score: 0.9545 time: 0.14s
Test loss: 0.4090 score: 0.8140 time: 0.24s
Epoch 412/1000, LR 0.000175
Train loss: 0.5719;  Loss pred: 0.5719; Loss self: 0.0000; time: 0.18s
Val loss: 0.1990 score: 0.9545 time: 0.06s
Test loss: 0.4093 score: 0.8140 time: 0.08s
Epoch 413/1000, LR 0.000174
Train loss: 0.5699;  Loss pred: 0.5699; Loss self: 0.0000; time: 0.28s
Val loss: 0.1987 score: 0.9545 time: 0.19s
Test loss: 0.4095 score: 0.8140 time: 0.24s
Epoch 414/1000, LR 0.000174
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 9.41s
Val loss: 0.1981 score: 0.9545 time: 3.47s
Test loss: 0.4099 score: 0.8140 time: 2.62s
Epoch 415/1000, LR 0.000173
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 1.71s
Val loss: 0.1975 score: 0.9545 time: 0.22s
Test loss: 0.4102 score: 0.8140 time: 0.24s
Epoch 416/1000, LR 0.000173
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.19s
Val loss: 0.1970 score: 0.9545 time: 0.06s
Test loss: 0.4106 score: 0.8140 time: 0.07s
Epoch 417/1000, LR 0.000173
Train loss: 0.5668;  Loss pred: 0.5668; Loss self: 0.0000; time: 0.18s
Val loss: 0.1964 score: 0.9545 time: 0.05s
Test loss: 0.4110 score: 0.8140 time: 0.07s
Epoch 418/1000, LR 0.000172
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.16s
Val loss: 0.1956 score: 0.9545 time: 0.05s
Test loss: 0.4114 score: 0.8140 time: 0.05s
Epoch 419/1000, LR 0.000172
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.18s
Val loss: 0.1950 score: 0.9545 time: 0.09s
Test loss: 0.4119 score: 0.8140 time: 0.04s
Epoch 420/1000, LR 0.000171
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.15s
Val loss: 0.1942 score: 0.9545 time: 0.05s
Test loss: 0.4125 score: 0.8140 time: 0.05s
Epoch 421/1000, LR 0.000171
Train loss: 0.5712;  Loss pred: 0.5712; Loss self: 0.0000; time: 0.15s
Val loss: 0.1935 score: 0.9545 time: 0.05s
Test loss: 0.4130 score: 0.8140 time: 0.05s
Epoch 422/1000, LR 0.000171
Train loss: 0.5676;  Loss pred: 0.5676; Loss self: 0.0000; time: 0.16s
Val loss: 0.1929 score: 0.9545 time: 0.05s
Test loss: 0.4135 score: 0.8140 time: 0.05s
Epoch 423/1000, LR 0.000170
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.18s
Val loss: 0.1925 score: 0.9318 time: 3.15s
Test loss: 0.4138 score: 0.8140 time: 3.31s
Epoch 424/1000, LR 0.000170
Train loss: 0.5662;  Loss pred: 0.5662; Loss self: 0.0000; time: 8.19s
Val loss: 0.1921 score: 0.9318 time: 2.90s
Test loss: 0.4142 score: 0.8140 time: 2.76s
Epoch 425/1000, LR 0.000169
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 1.54s
Val loss: 0.1919 score: 0.9545 time: 0.45s
Test loss: 0.4143 score: 0.8140 time: 0.05s
Epoch 426/1000, LR 0.000169
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 0.15s
Val loss: 0.1917 score: 0.9545 time: 0.05s
Test loss: 0.4145 score: 0.8140 time: 0.05s
Epoch 427/1000, LR 0.000168
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.15s
Val loss: 0.1915 score: 0.9545 time: 0.05s
Test loss: 0.4146 score: 0.8140 time: 0.04s
Epoch 428/1000, LR 0.000168
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.15s
Val loss: 0.1914 score: 0.9545 time: 0.05s
Test loss: 0.4147 score: 0.8140 time: 0.04s
Epoch 429/1000, LR 0.000168
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.14s
Val loss: 0.1915 score: 0.9545 time: 0.05s
Test loss: 0.4148 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 430/1000, LR 0.000167
Train loss: 0.5667;  Loss pred: 0.5667; Loss self: 0.0000; time: 0.14s
Val loss: 0.1916 score: 0.9545 time: 0.05s
Test loss: 0.4148 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 427,   Train_Loss: 0.5646,   Val_Loss: 0.1914,   Val_Precision: 1.0000,   Val_Recall: 0.9091,   Val_accuracy: 0.9524,   Val_Score: 0.9545,   Val_Loss: 0.1914,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4147


[0.05545937595888972, 0.057529293932020664, 0.05470380000770092, 0.05550090002361685, 0.057805703952908516, 0.060653330059722066, 0.055280742002651095, 0.05565055401530117, 0.7109664009185508, 0.31898007704876363, 0.1344758050981909, 0.06007817306090146, 0.05518483591731638, 0.05276909493841231, 0.0538266699295491, 0.05469510005787015, 0.05622987204696983, 0.05546813702676445, 1.297919764998369, 0.513117947964929, 0.27114238403737545, 0.05265923507977277, 0.05199010600335896, 0.05150219995994121, 0.05466046102810651, 0.053594280034303665, 0.054290933068841696, 0.05432291503529996, 0.0538062599953264, 1.8271140879951417, 0.8477141369367018, 0.08036299899686128, 0.05348086101002991, 0.049798041000030935, 0.05032224499154836, 0.04971343604847789, 0.04985185002442449, 0.04927924496587366, 0.049995991052128375, 0.04923918505664915, 0.050050979014486074, 0.049178126035258174, 0.049004415050148964, 0.04907885298598558, 0.049217629013583064, 0.04972954001277685, 0.1419258660171181, 0.9357607350684702, 0.4322917010867968, 0.0567030580714345, 0.056750652962364256, 0.05634796107187867, 0.10557014402002096, 0.05621543596498668, 0.05699615797493607, 0.05795564502477646, 0.057973846909590065, 0.05665381799917668, 3.5709817359456792, 0.09258924203459173, 0.052572960034012794, 0.05078897706698626, 0.05072854505851865, 0.05473598896060139, 0.05362162506207824, 0.05336401204112917, 0.050551208085380495, 0.05139677203260362, 0.050800247001461685, 0.05001360806636512, 0.05054433294571936, 0.050314499996602535, 0.05068053596187383, 0.049981547985225916, 0.050519780023023486, 0.05011103698052466, 0.05023378902114928, 1.2415582379326224, 0.054319899063557386, 0.05017784901428968, 0.04938702890649438, 0.04996224900241941, 0.04960649798158556, 0.04930786299519241, 0.04952123400289565, 0.04878442897461355, 0.04974715306889266, 0.05205671594012529, 0.049344186089001596, 0.04915229103062302, 1.7849164409562945, 0.387693680007942, 0.4082161580445245, 0.05592207401059568, 0.054509787005372345, 0.05587615899275988, 0.056411084020510316, 0.050510814995504916, 0.05051064502913505, 0.050930417026393116, 0.052005036966875196, 0.05417391995433718, 0.05350572895258665, 1.798725588945672, 0.26537680393084884, 0.08149278908967972, 0.0540065459208563, 0.05263013101648539, 0.05185056396294385, 0.05126343294978142, 0.05158588697668165, 0.05166915990412235, 0.05204587196931243, 0.05076477595139295, 0.05160653591156006, 0.05141747300513089, 0.051453712047077715, 1.4993561550509185, 0.21717809094116092, 0.0546054890146479, 0.05589847301598638, 0.05318957392591983, 0.06447987398132682, 0.05154588504228741, 0.05086784600280225, 0.10351475607603788, 0.05790265090763569, 0.05859625490847975, 0.05191781197208911, 0.05200030992273241, 1.541631220956333, 0.32009176607243717, 0.1973802630091086, 0.05458328896202147, 0.05388558504637331, 0.05621476692613214, 0.05402219796087593, 0.054226765991188586, 0.05298422195482999, 0.053424868965521455, 0.05351552402134985, 0.05355909699574113, 0.05387668404728174, 0.05284880299586803, 0.052429168950766325, 0.38165334006771445, 1.565964404027909, 1.0109545269515365, 0.048854355001822114, 0.048071552999317646, 0.04897273704409599, 0.04854643705766648, 0.048556186025962234, 0.04824847693089396, 0.048016497050412, 0.04766917589586228, 0.04780828708317131, 0.055876895086839795, 0.04822872101794928, 0.04871944501064718, 0.06286621594335884, 0.04880620399489999, 0.05628752999473363, 0.05238977400586009, 0.05259281606413424, 0.05225759407039732, 0.05211399798281491, 2.2372530420543626, 0.3573656240478158, 0.05237980803940445, 0.05212225508876145, 0.05183890892658383, 0.05174215289298445, 0.05201577104162425, 0.051447126083076, 0.05204572097864002, 0.05216061498504132, 0.0516660480061546, 0.05151507700793445, 0.052921624039299786, 0.052174380980432034, 0.05238828994333744, 1.9781540100229904, 0.054647520068101585, 0.05410671699792147, 0.05652543995529413, 0.05793883197475225, 0.054068783996626735, 0.05434737307950854, 0.05399795901030302, 0.04855033697094768, 0.04940653906669468, 0.04922254499979317, 0.0499601480551064, 0.04901986406184733, 0.04888689611107111, 0.04996061394922435, 0.05389843904413283, 0.05272371903993189, 0.04872848896775395, 0.04920637304894626, 0.0495099889812991, 0.14230923098511994, 0.7482448990922421, 0.05387521593365818, 0.054153622942976654, 0.05547530797775835, 0.04959046398289502, 0.050351866986602545, 0.05301779496949166, 0.05130091204773635, 0.05010366998612881, 0.04933218297082931, 0.04856491705868393, 2.0116327210562304, 0.04985905601643026, 0.049996405956335366, 0.04966721800155938, 0.05112981202546507, 0.050206927000544965, 0.05025013000704348, 0.05015570204705, 0.048190874978899956, 0.048838049988262355, 0.04907591105438769, 0.048574788961559534, 1.956354875001125, 0.053682955098338425, 0.05301607493311167, 0.05270198010839522, 0.052885398035869, 0.05326252500526607, 0.05263467796612531, 0.053776626009494066, 0.05263316503260285, 0.053022531094029546, 0.052324921009130776, 2.0086210949812084, 0.05765336798503995, 0.04933463898487389, 0.04908384697046131, 0.04924994194880128, 0.048615487990900874, 0.052271928056143224, 0.053431935026310384, 0.05321253300644457, 0.05333104496821761, 0.05302306998055428, 0.05280563700944185, 0.05298119189683348, 0.05416607309598476, 0.05366159102413803, 0.05355061206500977, 1.850088482024148, 0.9061089319875464, 0.05083067202940583, 0.052040801965631545, 0.05175290408078581, 0.05112950806505978, 0.051120463060215116, 0.05227689107414335, 0.051971185952425, 0.048650128999724984, 0.04871485393960029, 0.049198834924027324, 0.04945102892816067, 0.04904836101923138, 0.048837580950930715, 0.049392578075639904, 0.05253667791839689, 2.384068739018403, 0.051824813010171056, 0.05332548497244716, 0.0522444179514423, 0.0510998519603163, 0.051166116958484054, 0.051263703033328056, 0.05331534903962165, 0.05252336000557989, 0.052072918973863125, 0.055333961034193635, 0.05263735610060394, 0.05228745401836932, 0.05288647406268865, 0.05228657694533467, 0.051777904969640076, 1.7425204470055178, 1.4346817249897867, 0.05153802002314478, 0.05119078909046948, 0.05042924406006932, 0.05115442001260817, 0.0506798499263823, 0.05122986703645438, 0.05153047200292349, 0.05170172604266554, 0.05072360299527645, 0.051213309983722866, 0.04906692006625235, 0.04810817597899586, 1.9581554090837017, 1.0322526330128312, 0.05263629008550197, 0.051759189926087856, 0.052253557019867, 0.053675664006732404, 0.05401281698141247, 0.05236483609769493, 0.0524191630538553, 0.05264303297735751, 0.05308309104293585, 0.052821284043602645, 1.343839078093879, 0.051012675976380706, 0.05029693292453885, 0.04996269498951733, 0.05008294607978314, 0.05127258598804474, 0.05084754095878452, 0.05140564602334052, 0.05072511802427471, 0.0509243420092389, 0.051158690941520035, 0.05040607193950564, 0.051086653023958206, 0.0517140559386462, 0.05118823901284486, 0.05121310905087739, 0.05626375006977469, 0.04968244198244065, 0.05044862790964544, 0.04978307196870446, 0.05142125894781202, 0.05027606594376266, 0.09632291994057596, 0.05642897298093885, 0.05625422403682023, 0.05464953696355224, 0.052983022993430495, 0.05011435807682574, 1.755774887977168, 0.0770078090718016, 0.0500655040377751, 0.048775761970318854, 0.04821814794559032, 0.048547586891800165, 0.04884694400243461, 0.048847015015780926, 0.048819413990713656, 0.04862278199288994, 0.04920842102728784, 0.048961310996674, 0.05019584996625781, 0.04999544098973274, 0.049443705938756466, 0.0499740649247542, 0.04518215206917375, 0.04663477907888591, 0.045910976943559945, 0.0460202390095219, 0.04590909997932613, 0.0461677429266274, 0.04592353303451091, 0.046598118962720037, 0.046545303077436984, 0.048491018009372056, 0.047851874958723783, 0.04699575202539563, 0.046233452972956, 0.0464578130049631, 0.047509194002486765, 0.0469088020035997, 0.20602687902282923, 0.7732695699669421, 0.048897241009399295, 0.04871757188811898, 0.048839297029189765, 0.048433163086883724, 0.04815973003860563, 0.04846484202425927, 0.0485783169278875, 0.048113722004927695, 0.04782442911528051, 0.04732542100828141, 0.04752296698279679, 0.04786555399186909, 0.0496188880642876, 0.04768913902807981, 1.5199367110617459, 0.11430055601522326, 0.04782999900635332, 0.04803597100544721, 0.04748573596589267, 0.06605039106216282, 0.04962508997414261, 0.04986781196203083, 0.04977571789640933, 0.05011964694131166, 0.04991974192671478, 0.04937136999797076, 0.04908443393651396, 0.04875304293818772, 0.04849113302771002, 0.04982769605703652, 0.15227053698617965, 0.053770557045936584, 0.04890860803425312, 0.04909280699212104, 0.049282882013358176, 0.048870894010178745, 0.04866016306914389, 0.04848804697394371, 0.04892890900373459, 0.048829674953594804, 0.04827384790405631, 0.0484481219900772, 0.048943017958663404, 0.048541476018726826, 0.04868455999530852, 0.04872175201307982, 0.04886131198145449, 0.04869390302337706, 2.0386035289848223, 0.05069138400722295, 0.05039292701985687, 0.04990556801203638, 0.0606567069189623, 0.04809085000306368, 0.04696538404095918, 0.04686940705869347, 0.04684811201877892, 0.047199176042340696, 0.04736789804883301, 0.04741481295786798, 0.04777987999841571, 0.05816807202063501, 0.0563531779916957, 0.047567720990628004, 0.048637976055033505, 1.5010524360695854, 0.050085972994565964, 0.050376159022562206, 0.04990229499526322, 0.05008022498805076, 0.050090156961232424, 0.047302796971052885, 0.04718807095196098, 0.04691277595702559, 0.046715447097085416, 0.04680804302915931, 0.0468076909892261, 0.04728872899431735, 0.04762320395093411, 0.0471589679364115, 0.049283024040050805, 0.04896511195693165, 0.04849643004126847, 0.04873420705553144, 0.04834821599069983, 0.048124692984856665, 0.048444316955283284, 0.04846175608690828, 0.04830183507874608, 0.04798794805537909, 0.04832625901326537, 0.04897072503808886, 0.051274831988848746, 0.048594609019346535, 0.04767705500125885, 0.047838396043516695, 0.0480003310367465, 0.04821112402714789, 0.048083421075716615, 0.04807150596752763, 0.04798443603795022, 0.048360191052779555, 0.048242246033623815, 0.04864404199179262, 0.04875876801088452, 0.04844267398584634, 0.04810804291628301, 0.048817597911693156, 0.050410643918439746, 0.05340377299580723, 0.1322092650225386, 0.046778319985605776, 0.046841360977850854, 0.046688377042301, 0.049051506095565856, 0.04973994300235063, 0.04642375896219164, 0.04649531899485737, 0.04602562601212412, 0.04649706301279366, 0.04593431996181607, 0.048919009976089, 0.04631171596702188, 0.04607943003065884, 0.04601433000061661, 0.0457252049818635, 0.045904992963187397, 0.04578310495708138, 0.0462790789315477, 0.05107511195819825, 0.04888900206424296, 0.049470582976937294, 0.05046574503649026, 0.04934993397910148, 0.0499208279652521, 0.04936456400901079, 0.048836247995495796, 0.09615159407258034, 0.04929576499853283, 0.0499151679687202, 0.04990072699729353, 0.049816195969469845, 0.04875864100176841, 0.04866779700387269, 0.05081911198794842, 0.04832957801409066, 0.049066183040849864, 0.05037433304823935, 0.04977728193625808, 1.680243045091629, 2.093855029088445, 0.05300849198829383, 0.05159078899305314, 0.05203734594397247, 0.04979041393380612, 2.432495735003613, 0.24743938900064677, 0.23463093698956072, 0.236086759949103, 0.24276866798754781, 2.70300792704802, 2.345691093010828, 0.2584444429958239, 0.3662865379592404, 0.24051169608719647, 3.058565536979586, 0.8921304219402373, 0.1694706151029095, 0.0581900350516662, 0.23758560908026993, 0.05055809300392866, 0.051329457899555564, 0.05102452798746526, 0.05088865105062723, 0.051136343041434884, 0.05158790701534599, 0.08367786300368607, 0.05169786501210183, 0.05742436309810728, 0.05192972801160067, 0.052635088097304106, 0.07414023100864142, 0.06226054800208658, 0.06984889495652169, 0.06261722289491445, 0.05128581402823329, 0.061017261003144085, 0.08978993399068713, 0.05427257192786783, 0.10892973793670535, 0.05326961900573224, 0.09050150506664068, 0.050994465011172, 0.051645257975906134, 0.21535163698717952, 0.05383607291150838, 0.20528147602453828, 3.708591348025948, 0.06747221294790506, 3.2515786420553923, 0.0720171369612217, 0.04959176201373339, 0.05002364097163081, 0.05012274906039238, 0.050145584042184055, 0.05037390801589936, 0.04991536296438426, 0.05029489507433027, 0.048207450890913606, 0.04928362893406302, 0.048935856902971864, 0.05908457306213677, 0.061328917974606156, 0.24438752606511116, 0.24773223709780723, 3.0705982310464606, 0.286076272954233, 0.2451222490053624, 0.07906607107724994, 0.06321908498648554, 0.05263808602467179, 0.05114498594775796, 0.05792767007369548, 0.24372381810098886, 3.085189817007631, 3.2042236709967256, 0.23582289402838796, 0.049059637007303536, 0.0533195500029251, 0.053066298947669566, 1.8076291279867291, 0.5594078130088747, 0.5535196099663153, 0.05458021303638816, 0.053748125093989074, 0.18635789991822094, 0.24842857604380697, 0.05280099902302027, 0.08995602990034968, 0.262594896950759, 0.24738088506273925, 1.6938465810380876, 2.88252308499068, 0.05819007498212159, 0.050383636029437184, 0.05070764000993222, 0.05058145604562014, 0.05054929293692112, 0.05005832901224494, 0.04987663892097771, 0.0504953539930284, 0.05051745497621596, 0.05075148993637413, 0.05058675794862211, 0.05062996200285852, 0.05067927809432149, 0.05045429605524987, 0.050248612998984754, 0.05036247102543712, 0.05042547301854938, 0.05150360497646034, 0.04756819293834269, 0.22829963499680161, 0.23583263403270394, 0.06630104698706418, 2.694706332986243, 0.28378600301221013, 0.05106994998641312, 0.14529714500531554, 0.10903623909689486, 0.09863367700017989, 0.07276218791957945, 0.2159068320179358, 0.05784924398176372, 0.10571054392494261, 0.22481909103225917, 0.24136766197625548, 1.9996830049203709, 0.2281579339178279, 0.2374159439932555, 0.056176198995672166, 0.05508131405804306, 0.2446328109363094, 0.22941993700806051, 1.7610586889786646, 0.5381728060310706, 0.2204324440099299, 0.05463472590781748, 0.0993351599900052, 0.050736923003569245, 0.051724717020988464, 0.05586168693844229, 0.05257992004044354, 0.051609807065688074, 0.051111418986693025, 0.050942722940817475, 0.2383339770603925, 0.2351992130279541, 0.3164727210532874, 0.24456914607435465, 0.05858401907607913, 0.05315938196144998, 0.08406178199220449, 0.24566263100132346, 1.2491268940502778, 3.004772101994604, 0.10418926808051765, 3.073923705960624, 2.565369123010896, 0.2526922250399366, 0.22614463791251183, 0.052320110029540956, 0.05281059199478477, 0.18149904895108193, 0.052292327978648245, 0.05760351603385061, 0.05124969105236232, 0.05124614806845784, 0.0667208710219711, 0.06069694203324616, 0.05108126695267856, 0.05336788296699524, 0.052669367054477334, 0.16920302307698876, 0.05104449996724725, 0.0632087750127539, 0.05874540202785283, 0.06000380904879421, 0.07913443096913397, 0.09636736498214304, 0.051246659946627915, 0.24824476102367043, 3.1651455219835043, 0.29452221805695444, 0.14200926397461444, 0.0496014510281384, 0.05058071203529835, 0.09072097798343748, 0.2498164779972285, 0.25514523894526064, 0.245224520098418, 2.1516365099232644, 0.6228301390074193, 0.24891232000663877, 0.2464705010643229, 0.05520644597709179, 0.24854715005494654, 2.367516377940774, 1.1163427350111306, 0.9379589939489961, 0.05939677101559937, 0.06312397704459727, 2.671495308051817, 0.5806826699990779, 1.4162244310136884, 0.25575000199023634, 0.22633959201630205, 0.23926565900910646, 0.04840214503929019, 0.09527869103476405, 0.05358644400257617, 1.2008583639981225, 0.24240771506447345, 0.13855406595394015, 0.05317482503596693, 0.052562950062565506, 0.052629456971772015, 0.0909804409602657, 0.052778844023123384, 0.09946291008964181, 0.05310221598483622, 0.05303074594121426, 0.0510773150017485, 0.07328788400627673, 0.141786323976703, 0.05035960301756859, 0.06078625901136547, 0.05675951892044395, 0.061518535017967224, 0.24147367500700057, 2.4890672409674153, 0.6822039830731228, 2.3799915469717234, 0.16847384406719357, 0.05113628297112882, 0.05640459095593542, 0.057707200990989804, 0.10086762602441013, 0.24298563809134066, 0.0892515319865197, 0.24262586794793606, 2.6245757050346583, 0.2451887100469321, 0.07178402098361403, 0.0697554029757157, 0.052750263013876975, 0.04807256790809333, 0.049458789988420904, 0.050963080022484064, 0.053560519008897245, 3.3169041220098734, 2.767250534030609, 0.051023224950768054, 0.049645385006442666, 0.04771824099589139, 0.04813905793707818, 0.04755589098203927, 0.051145917968824506]
[0.0012604403627020392, 0.0013074839530004697, 0.0012432681819932027, 0.0012613840914458376, 0.001313765998929739, 0.0013784847740845923, 0.0012563805000602522, 0.001264785318529572, 0.016158327293603426, 0.007249547205653719, 0.003056268297686157, 0.001365413024111397, 0.0012542008163026449, 0.0011992976122366433, 0.0012233334074897523, 0.0012430704558606851, 0.0013076714429527867, 0.0012899566750410338, 0.030184180581357415, 0.011932975534068118, 0.006305636838078499, 0.001224633373948204, 0.001209072232636255, 0.001197725580463749, 0.0012711735122815468, 0.0012463786054489224, 0.001262579838810272, 0.001263323605472092, 0.0012513083719843348, 0.0424910253022126, 0.019714282254341905, 0.0018689069534153786, 0.0012437409537216259, 0.0011580939767449055, 0.0011702847672453106, 0.001156126419732044, 0.0011593453494052207, 0.0011460289526947361, 0.0011626974663285668, 0.0011450973268988173, 0.001163976256150839, 0.0011436773496571668, 0.0011396375593057898, 0.001141368674092688, 0.0011445960235716992, 0.0011565009305296943, 0.003300601535281816, 0.021761877559731867, 0.010053295374111555, 0.0013186757691031278, 0.001319782627031727, 0.0013104176993460157, 0.0024551196283725805, 0.0013073357201159694, 0.0013254920459287458, 0.0013478056982506154, 0.0013482289978974434, 0.0013175306511436437, 0.08304608688245765, 0.0021532381868509705, 0.0012226269775351814, 0.0011811390015578202, 0.0011797336060120616, 0.0012729299758279392, 0.001247014536327401, 0.0012410235358402133, 0.0011756094903576859, 0.0011952737682000842, 0.0011814010930572484, 0.0011631071643340726, 0.0011754496033888223, 0.00117010465108378, 0.0011786171153924147, 0.0011623615810517655, 0.0011748786051865928, 0.0011653729530354572, 0.0011682276516546343, 0.028873447393781917, 0.0012632534665943578, 0.0011669267212625505, 0.0011485355559649856, 0.0011619127674981258, 0.0011536394879438502, 0.0011466944882602885, 0.0011516566047185036, 0.0011345216040607802, 0.0011569105364858758, 0.0012106213009331463, 0.0011475392113721302, 0.001143076535595884, 0.0415096846734022, 0.009016132093207953, 0.009493399024291267, 0.0013005133490836204, 0.0012676694652412174, 0.0012994455579711601, 0.0013118856748955887, 0.001174670116174533, 0.001174666163468257, 0.0011844283029393747, 0.0012094194643459347, 0.0012598586035892367, 0.0012443192779671313, 0.041830827649899344, 0.0061715535797871824, 0.0018951811416204585, 0.0012559661842059605, 0.0012239565352671021, 0.001205827068905671, 0.0011921728592972423, 0.0011996717901553871, 0.0012016083698633106, 0.0012103691155654053, 0.001180576184916115, 0.001200151997943257, 0.0011957551861658347, 0.0011965979545832026, 0.03486874779188182, 0.005050653277701417, 0.0012698950933639048, 0.0012999644887438694, 0.0012369668354865076, 0.001499531953054112, 0.001198741512611335, 0.0011829731628558663, 0.0024073199087450673, 0.0013465732769217603, 0.001362703602522785, 0.0012073909760950956, 0.0012093095330868004, 0.03585188885944961, 0.00744399455982412, 0.004590238674630432, 0.0012693788130702668, 0.0012531531406133327, 0.0013073201610728404, 0.0012563301851366494, 0.0012610875811904323, 0.0012321912082518603, 0.0012424388131516617, 0.00124454707026395, 0.0012455603952497938, 0.0012529461406344592, 0.0012290419301364658, 0.0012192829988550307, 0.008875659071342197, 0.03641777683785835, 0.023510570394221778, 0.001136147790740049, 0.001117943093007387, 0.0011389008614906045, 0.001128986908317825, 0.0011292136285107495, 0.0011220576030440456, 0.0011166627221026045, 0.0011085854859502857, 0.0011118206298411932, 0.0012994626764381348, 0.001121598163208123, 0.0011330103490848182, 0.0014620050219385776, 0.0011350279998813952, 0.0013090123254589218, 0.0012183668373455835, 0.0012230887456775405, 0.0012152928853580772, 0.001211953441460812, 0.052029140512892154, 0.008310828466228274, 0.0012181350706838244, 0.001212145467180499, 0.0012055560215484611, 0.0012033058812321966, 0.0012096690939912616, 0.0011964447926296744, 0.0012103656041544191, 0.0012130375577916587, 0.00120153600014313, 0.0011980250466961501, 0.0012307354427744137, 0.0012133576972193497, 0.0012183323242636614, 0.04600358162844164, 0.0012708725597232926, 0.0012582957441377085, 0.0013145451152393984, 0.0013474146970872615, 0.0012574135813169008, 0.001263892397197873, 0.0012557664886116982, 0.0011290776039755276, 0.0011489892806208065, 0.0011447103488323993, 0.0011618639082582884, 0.0011399968386476124, 0.001136904560722584, 0.0011618747430052175, 0.0012534520707937868, 0.0012261330009286488, 0.0011332206736686965, 0.0011443342569522387, 0.0011513950925883512, 0.0033095169996539523, 0.017401044164935865, 0.001252911998457167, 0.0012593865800692246, 0.0012901234413432174, 0.0011532666042533724, 0.001170973650851222, 0.0012329719760346897, 0.0011930444662264267, 0.001165201627584391, 0.0011472600690890537, 0.0011294166757833473, 0.04678215630363326, 0.001159512930614657, 0.0011627071152636131, 0.0011550515814316134, 0.0011890653959410482, 0.0011676029535010457, 0.0011686076745824065, 0.0011664116755127907, 0.0011207180227651152, 0.0011357686043781943, 0.0011413002570787834, 0.0011296462549199892, 0.045496625000026165, 0.0012484408162404285, 0.0012329319751886434, 0.0012256274443812841, 0.0012298929775783488, 0.00123866337221549, 0.001224062278281984, 0.0012506192095231177, 0.0012240270937814615, 0.0012330821184658033, 0.0012168586281193204, 0.04671211848793508, 0.0013407759996520918, 0.0011473171856947416, 0.0011414848132665421, 0.001145347487181425, 0.0011305927439744388, 0.001215626233863796, 0.001242603140146753, 0.0012375007675917342, 0.001240256859725991, 0.0012330946507105648, 0.0012280380699870197, 0.001232120741786825, 0.0012596761185112734, 0.0012479439773055356, 0.0012453630712792971, 0.0430253135354453, 0.02107230074389643, 0.0011821086518466473, 0.001210251208503059, 0.001203555908855484, 0.0011890583270944136, 0.0011888479781445376, 0.0012157416528870547, 0.0012086322314517443, 0.0011313983488308137, 0.0011329035799907043, 0.0011441589517215656, 0.001150023928561876, 0.0011406595585867763, 0.0011357576965332725, 0.0011486646064102304, 0.001221783207404579, 0.055443459046939605, 0.0012052282095388618, 0.001240127557498771, 0.0012149864639870304, 0.001188368650239914, 0.0011899096967089315, 0.0011921791403099549, 0.0012398918381307362, 0.001221473488501858, 0.0012109981156712354, 0.0012868363031207821, 0.0012241245604791614, 0.0012159873027527749, 0.0012299180014578755, 0.0012159669057054574, 0.0012041373248753507, 0.040523731325709716, 0.03336469127883225, 0.0011985586051894135, 0.0011904834672202204, 0.0011727731176760308, 0.001189637674711818, 0.0011786011610786583, 0.0011913922566617297, 0.0011983830698354299, 0.0012023657219224545, 0.0011796186743087548, 0.0011910072089237876, 0.00114109116433145, 0.001118794790209206, 0.04553849788566748, 0.02400587518634491, 0.0012240997694302784, 0.0012037020913043687, 0.0012151990004620234, 0.001248271255970521, 0.0012561120228235458, 0.0012177868859929053, 0.0012190503035780303, 0.0012242565808687792, 0.001234490489370601, 0.0012284019545023871, 0.03125207158357858, 0.0011863413017762954, 0.0011696961145241593, 0.0011619231392911008, 0.0011647196762740266, 0.0011923857206522033, 0.0011825009525298726, 0.0011954801400776865, 0.0011796539075412723, 0.001184287023470672, 0.0011897369986400008, 0.001172234231151294, 0.0011880616982315863, 0.0012026524636894464, 0.0011904241630894153, 0.0011910025360669162, 0.0013084593039482486, 0.0011554056274986199, 0.0011732239048754753, 0.001157745859737313, 0.0011958432313444655, 0.0011692108359014572, 0.00224006790559479, 0.001312301697231136, 0.001308237768298145, 0.0012709194642686566, 0.0012321633254286162, 0.0011654501878331567, 0.040831974139003906, 0.0017908792807395722, 0.0011643140473901186, 0.0011343200458213687, 0.0011213522778044259, 0.0011290136486465155, 0.0011359754419170839, 0.0011359770933902541, 0.001135335209086364, 0.0011307623719276731, 0.0011443818843555312, 0.001138635139457535, 0.0011673453480525071, 0.0011626846741798313, 0.0011498536264827085, 0.0011621875563896326, 0.0010507477225389244, 0.0010845297460206026, 0.0010676971382223243, 0.0010702381165005094, 0.0010676534878913053, 0.0010736684401541254, 0.001067989140337463, 0.0010836771851795357, 0.0010824489087776043, 0.0011276980932412107, 0.0011128343013656694, 0.001092924465706875, 0.0010751965807664187, 0.0010804142559293745, 0.0011048649768020179, 0.0010909023721767373, 0.004791322767972773, 0.017983013255045165, 0.001137145139753472, 0.0011329667880957902, 0.0011357976053299945, 0.0011263526299275285, 0.001119993721828038, 0.0011270893494013784, 0.0011297283006485464, 0.001118923767556458, 0.001112196025936756, 0.0011005911862391026, 0.0011051852786696929, 0.0011131524184155603, 0.0011539276294020373, 0.0011090497448390654, 0.03534736537352898, 0.0026581524654703086, 0.0011123255582872866, 0.001117115604777842, 0.0011043194410672716, 0.0015360556060968096, 0.0011540718598637817, 0.001159716557256531, 0.001157574834800217, 0.0011655731846816665, 0.001160924230853832, 0.0011481713953016455, 0.0011414984636398595, 0.0011337916962369237, 0.0011277007680862796, 0.0011587836292334074, 0.003541175278748364, 0.0012504780708357346, 0.0011374094891686772, 0.0011416931858632801, 0.0011461135351943763, 0.001136532418841366, 0.0011316316992824162, 0.0011276289993940398, 0.0011378816047380137, 0.0011355738361301117, 0.001122647625675728, 0.001126700511397144, 0.0011382097199689164, 0.0011288715353192284, 0.0011321990696583377, 0.0011330640003041818, 0.001136309580964058, 0.001132416349380862, 0.04740938439499587, 0.0011788693955168128, 0.0011719285353455086, 0.0011605946049310786, 0.0014106210911386582, 0.0011183918605363647, 0.0010922182335106785, 0.00108998621066729, 0.001089490977180905, 0.001097655256798621, 0.0011015790243914655, 0.0011026700687876275, 0.001111159999963156, 0.0013527458609450002, 0.0013105390230626908, 0.0011062260695494884, 0.0011311157222100815, 0.034908196187664775, 0.001164790069641069, 0.0011715385819200514, 0.0011605184882619354, 0.001164656395070948, 0.0011648873711914518, 0.0011000650458384393, 0.0010973969988828135, 0.0010909947896982695, 0.001086405746443847, 0.0010885591402130072, 0.0010885509532378163, 0.0010997378835887757, 0.001107516370951956, 0.0010967201845677092, 0.0011461168381407163, 0.0011387235338821313, 0.001127823954448104, 0.0011333536524542195, 0.0011243771160627867, 0.0011191789066245736, 0.0011266120222158903, 0.0011270175834164717, 0.0011232984902033972, 0.0011159987919855602, 0.00112386648868059, 0.0011388540706532293, 0.0011924379532290406, 0.0011301071864964311, 0.0011087687209595082, 0.0011125208382213184, 0.0011162867682964304, 0.0011211889308639044, 0.0011182190947841074, 0.0011179419992448285, 0.001115917117161633, 0.0011246556058785943, 0.0011219126984563677, 0.0011312567905068052, 0.0011339248374624307, 0.0011265738136243336, 0.001118791695727512, 0.0011352929746905386, 0.0011723405562427847, 0.0012419482092048194, 0.0030746340702915956, 0.0010878679066419947, 0.0010893339762290896, 0.0010857762102860697, 0.0011407326998968805, 0.001156742860519782, 0.0010796223014463173, 0.001081286488252497, 0.0010703633956307935, 0.0010813270468091548, 0.0010682399991120016, 0.0011376513947927674, 0.0010770166503958577, 0.001071614651875787, 0.0010701006976887584, 0.001063376860043337, 0.001067557975888079, 0.0010647233710949157, 0.0010762576495708769, 0.0011877933013534478, 0.0011369535363777432, 0.0011504786738822626, 0.0011736219775927968, 0.0011476728832349181, 0.0011609494875640023, 0.001148013116488623, 0.0011357266975696697, 0.002236083583083264, 0.0011464131395007636, 0.001160817859737679, 0.0011604820231928728, 0.0011585161853365081, 0.0011339218837620561, 0.0011318092326482022, 0.001181839813673219, 0.0011239436747462943, 0.0011410740242058107, 0.0011714961174009151, 0.0011576112078199553, 0.0390754196532937, 0.04869430300205686, 0.00123275562763474, 0.0011997857905361195, 0.0012101708359063365, 0.0011579166031117703, 0.05656966825589797, 0.005754404395363878, 0.005456533418361878, 0.005490389766258209, 0.005645782976454601, 0.06286064946623303, 0.0545509556514146, 0.0060103358836238125, 0.008518291580447451, 0.005593295257841778, 0.0711294310925485, 0.020747219114889238, 0.0039411770954165, 0.0013532566291085163, 0.005525246722796975, 0.001175769604742527, 0.0011937083232454783, 0.0011866169299410527, 0.0011834570011773774, 0.0011892172800333693, 0.0011997187677987439, 0.001945996814039211, 0.001202275930513996, 0.0013354503046071461, 0.001207668093293039, 0.0012240718162163746, 0.0017241914188056145, 0.0014479197209787576, 0.0016243929059656208, 0.001456214485928243, 0.0011926933494937975, 0.0014190060698405602, 0.0020881379997834217, 0.00126215283553181, 0.002533249719458264, 0.0012388283489705172, 0.0021046861643404806, 0.0011859177909574884, 0.0012010525110675845, 0.005008177604353012, 0.001252001695616474, 0.004773987814524146, 0.08624631041920809, 0.0015691212313466293, 0.07561810795477657, 0.0016748171386330626, 0.0011532967910170555, 0.0011633404877123444, 0.0011656453269858692, 0.0011661763730740479, 0.001171486232927892, 0.0011608223945205642, 0.0011696487226588435, 0.0011211035090910142, 0.001146130905443326, 0.0011380431837900433, 0.0013740598386543434, 0.0014262539063861898, 0.005683430838723516, 0.005761214816228075, 0.071409261187127, 0.006652936580331, 0.005700517418729358, 0.0018387458390058127, 0.0014702112787554776, 0.0012241415354574835, 0.0011894182778548362, 0.0013471551179929182, 0.005667995769790438, 0.0717486003955263, 0.07451682955806338, 0.005484253349497394, 0.001140921790867524, 0.0012399895349517465, 0.0012340999755271992, 0.042037886697365794, 0.013009484023462201, 0.012872549068984078, 0.0012693072799160037, 0.0012499563975346295, 0.004333904649260952, 0.005777408745204813, 0.0012279302098376806, 0.002092000695356969, 0.006106858068622302, 0.005753043838668355, 0.03939178095437413, 0.0670354205811786, 0.0013532575577237579, 0.0011717124658008648, 0.001179247442091447, 0.0011763129312934916, 0.0011755649520214214, 0.0011641471863312776, 0.0011599218353715746, 0.0011743105579774048, 0.0011748245343306037, 0.001180267207822654, 0.001176436231363305, 0.0011774409768106633, 0.0011785878626586392, 0.0011733557222151133, 0.0011685723953252269, 0.0011712202564055144, 0.0011726854190360321, 0.0011977582552665194, 0.001106237045077737, 0.005309293837134921, 0.005484479861225673, 0.0015418848136526553, 0.06266758913921495, 0.0065996744886560495, 0.0011876732554979795, 0.0033790033722166405, 0.002535726490625462, 0.0022938064418646484, 0.0016921439051064988, 0.005021089116696182, 0.001345331255389854, 0.002458384742440526, 0.005228350954238585, 0.005613201441308267, 0.04650425592838072, 0.0053059984632053, 0.005521301023098965, 0.0013064232324574922, 0.0012809607920475131, 0.005689135138053707, 0.005335347372280477, 0.040954853232061966, 0.012515646651885363, 0.005126335907207672, 0.0012705750211120346, 0.002310119999767563, 0.0011799284419434708, 0.001202900395836941, 0.0012991089985684254, 0.0012227888381498497, 0.0012002280712950714, 0.001188637650853326, 0.0011847144869957553, 0.005542650629311453, 0.005469749140184979, 0.007359830722169474, 0.0056876545598687125, 0.00136241904828091, 0.0012362646967779066, 0.0019549251626094066, 0.005713084441891243, 0.02904946265233204, 0.0698784209766187, 0.002423006234430643, 0.07148659781303776, 0.05965974704676503, 0.0058765633730217814, 0.005259177625872368, 0.0012167467448730455, 0.001228153302204297, 0.00422090811514144, 0.0012161006506662382, 0.0013396166519500142, 0.0011918532802874958, 0.001191770885312973, 0.0015516481633016535, 0.001411556791470841, 0.0011879364407599666, 0.0012411135573719824, 0.0012248690012669147, 0.00393495402504625, 0.0011870813945871453, 0.0014699715119245093, 0.001366172140182624, 0.0013954374197394002, 0.0018403356039333482, 0.0022411015112126288, 0.001191782789456463, 0.005773133977294662, 0.07360803539496522, 0.0068493539083012665, 0.003302541022665452, 0.0011535221169334511, 0.0011762956287278685, 0.002109790185661337, 0.005809685534819267, 0.0059336102080293175, 0.005702895816242279, 0.050038058370308475, 0.014484421837381845, 0.005788658604805552, 0.005731872117774951, 0.0012838708366765532, 0.005780166280347594, 0.0550585204172273, 0.025961458953747223, 0.021812999859278977, 0.0013813202561767295, 0.0014679994661534249, 0.062127797861670164, 0.01350424813951344, 0.03293545188403926, 0.005947674464889217, 0.0052637114422395825, 0.005564317651374569, 0.0011256312799834928, 0.002215783512436373, 0.0012461963721529343, 0.027926938697630757, 0.005637388722429615, 0.00322218758032419, 0.0012366238380457427, 0.0012223941875015233, 0.0012239408598086515, 0.002115824208378272, 0.0012274149772819392, 0.0023130909323172515, 0.0012349352554613074, 0.0012332731614235876, 0.0011878445349243838, 0.0017043693954948075, 0.0032973563715512325, 0.0011711535585481067, 0.0014136339304968715, 0.0013199888121033477, 0.0014306636050690053, 0.0056156668606279206, 0.057885284673660824, 0.015865208908677274, 0.05534864062724938, 0.0039179963736556645, 0.0011892158830495074, 0.001311734673393847, 0.0013420279300230186, 0.002345758744753724, 0.005650828792821876, 0.0020756170229423183, 0.005642462045300839, 0.06103664430313159, 0.005702063024347258, 0.0016693958368282332, 0.0016222186738538535, 0.0012267503026483018, 0.0011179666955370544, 0.0011502044183353698, 0.0011851879074996294, 0.0012455934653231918, 0.07713730516302031, 0.06435466358210719, 0.0011865866267620478, 0.0011545438373591317, 0.001109726534788172, 0.0011195129752808878, 0.0011059509530706808, 0.0011894399527633606]
[793.3735142028248, 764.8277424018532, 804.3316916522418, 792.7799365645789, 761.170559151821, 725.434200507632, 795.9372180259428, 790.6480138167567, 61.8875940454473, 137.93964941977728, 327.1964050921449, 732.3791280303587, 797.320482494962, 833.821388283296, 817.4386425463305, 804.4596308160293, 764.7180837274772, 775.2198343934224, 33.12993696498184, 83.80139531377101, 158.58826406893542, 816.570919324215, 827.0804448296734, 834.9157906544908, 786.6746674143367, 802.3244266454803, 792.0291210592268, 791.5628233878441, 799.1635174742673, 23.534381504979308, 50.72464658355789, 535.0721169786041, 804.0259484965227, 863.4877825811118, 854.4928789886434, 864.9573117027899, 862.5557522726341, 872.5783041070922, 860.0689594324873, 873.2882144683948, 859.124054048067, 874.37247952822, 877.4719574959867, 876.1410950716108, 873.670692022423, 864.6772117528565, 302.9750756977142, 45.95191739569374, 99.46987159803545, 758.3365247396113, 757.7005330408555, 763.1154558573694, 407.3121278668069, 764.9144627604097, 754.4368169326282, 741.9467073762562, 741.7137604661339, 758.9956249837371, 12.04150656027161, 464.4168053987851, 817.9109559777605, 846.6404027646929, 847.6489903346671, 785.5891675027763, 801.915271128365, 805.786490844404, 850.6225989173873, 836.6284165224012, 846.4525772633104, 859.7660049429252, 850.7383022776978, 854.6244125034245, 848.4519586049411, 860.3174918213898, 851.1517663062571, 858.0943957857364, 855.9975434442397, 34.63389689363371, 791.6067728639838, 856.9518392020836, 870.6739593793526, 860.6498077762219, 866.8219235303012, 872.0718641607438, 868.3143880761465, 881.4287858606759, 864.3710714551078, 826.0221418780593, 871.429917243773, 874.8320596736783, 24.090763586088173, 110.912305816074, 105.33634975641986, 768.9271322779033, 788.8491656693142, 769.5589814176686, 762.2615439257606, 851.3028349240985, 851.3056995252618, 844.2891794448999, 826.8429849860315, 793.7398666414467, 803.6522600804831, 23.90581435226291, 162.0337548838851, 527.6540474357815, 796.1997803565182, 817.0224768494508, 829.3063124777369, 838.8045342598022, 833.5613191925395, 832.2179048351298, 826.19424697801, 847.044022043401, 833.2277925743866, 836.2915850747679, 835.702581781797, 28.678976542794604, 197.9941890715385, 787.4666224207839, 769.2517823823639, 808.429111688102, 666.874752460786, 834.2082004164541, 845.3277144393174, 415.3997133356899, 742.6257576460897, 733.8352948863505, 828.2321301043406, 826.9181484474606, 27.892533191774262, 134.33647646615518, 217.85359561515864, 787.7868999414635, 797.9870676544516, 764.923566373641, 795.9690946144315, 792.966337085032, 811.5623559907755, 804.8686095561732, 803.5051657691942, 802.8514745761909, 798.1189035736414, 813.6418908742729, 820.1541405391949, 112.66768945968289, 27.459117135355807, 42.53405949886147, 880.1671826062637, 894.4999135062344, 878.039550072155, 885.7498635568646, 885.5720253029876, 891.2198422675326, 895.525551454842, 902.0504171068003, 899.4256565853027, 769.5488436351471, 891.5849123180499, 882.6044711839955, 683.9921785453432, 881.0355340172182, 763.9347472526003, 820.7708625578383, 817.6021597239385, 822.8469137341797, 825.1142047129, 19.219998449757455, 120.32494763471308, 820.9270253081459, 824.9834917306105, 829.4927669272165, 831.0438896683448, 826.6723560742834, 835.8095636005845, 826.1966438633359, 824.376783370587, 832.2680301554655, 834.7070896036331, 812.5223059683136, 824.1592749538729, 820.7941134652094, 21.737437925523427, 786.860958126069, 794.7257269675383, 760.7194218038572, 742.1620100788004, 795.2832821741044, 791.2065949736397, 796.3263943327087, 885.6787137385064, 870.3301387282686, 873.5834362116116, 860.68600022103, 877.1954150208945, 879.5813074796962, 860.6779741278095, 797.7967592863121, 815.5722089223763, 882.4406607078506, 873.8705443139917, 868.511605127643, 302.1588951211193, 57.46781575412925, 798.1406525210052, 794.0373637656462, 775.1196265055427, 867.1021915590821, 853.9901809686877, 811.0484418437949, 838.1917257141118, 858.220565717133, 871.6419467070086, 885.4128165819885, 21.375671388672963, 862.4310894660748, 860.0618219948507, 865.7622015118694, 840.9966377068619, 856.4555245441184, 855.7191791140194, 857.3302385372377, 892.2851062328139, 880.4610341799998, 876.1936167083246, 885.2328732509526, 21.979652336836523, 800.9991238602831, 811.0747552370001, 815.9086226278292, 813.0788761547312, 807.3218458146433, 816.9518967642206, 799.6039021192685, 816.9753799408468, 810.9759966710056, 821.7881493312992, 21.407721001955466, 745.8367395146413, 871.5985539730796, 876.0519530157737, 873.0974758244685, 884.4917901070561, 822.6212730055679, 804.7621703916656, 808.0803068478673, 806.2845951288907, 810.9677545221325, 814.3070027222933, 811.6087702166243, 793.854853088612, 801.3180224316823, 802.978683937329, 23.242131615756282, 47.45566287011394, 845.9459275912043, 826.2747378181795, 830.8712479762951, 841.0016373575239, 841.1504400762182, 822.5431757028912, 827.3815425217095, 883.8619934644588, 882.6876511487457, 874.0044366172584, 869.5471243372446, 876.6857669951524, 880.4694901494815, 870.5761406936424, 818.4758097341094, 18.036392699693913, 829.7183820337352, 806.3686626050893, 823.0544369345952, 841.4897176882905, 840.399908300448, 838.8001150062145, 806.5219636477343, 818.683343857511, 825.7651164434035, 777.0996183235127, 816.9103310929144, 822.3770081613363, 813.0623332731583, 822.3908029962693, 830.470062958573, 24.67689838239461, 29.971804373758307, 834.3355057235316, 839.9948655607965, 852.6798448293237, 840.5920737524083, 848.4634438038377, 839.3541207007597, 834.4577165441153, 831.6937033110913, 847.7315778219521, 839.6254804398836, 876.354169814194, 893.8189637198861, 21.959441932201592, 41.65646918671071, 816.9268755482414, 830.7703436124873, 822.9104859531617, 801.1079284386052, 796.1073390191382, 821.1617414361167, 820.3106935496456, 816.822237778262, 810.0507931088599, 814.0657838704674, 31.997878839028726, 842.9277464273656, 854.9229048322582, 860.6421252701006, 858.5756902459399, 838.6547932266639, 845.6652807429665, 836.4839920594719, 847.7062582569483, 844.3898988856599, 840.5218978170042, 853.0718293543293, 841.7071280797003, 831.495407187079, 840.0367121285389, 839.6287746811441, 764.2576249658823, 865.4969096566864, 852.3522201042596, 863.7474205495282, 836.2300122531258, 855.2777388767558, 446.415038357722, 762.0198938322867, 764.3870435730317, 786.8319182407395, 811.5807209666327, 858.0375295654917, 24.49061112244315, 558.3849289869689, 858.8748046470464, 881.5854076491202, 891.780415301755, 885.7288848534474, 880.3007205088779, 880.2994407356941, 880.7971355039081, 884.3591057025047, 873.8341751741018, 878.2444571984814, 856.6445239776807, 860.0784221271425, 869.6759108886787, 860.4463147983853, 951.7032286148548, 922.0586191104833, 936.5951862200947, 934.3715053523083, 936.6334783161474, 931.3862293059947, 936.3391089201714, 922.7840298532512, 923.8311313272857, 886.7621626687486, 898.6063772232765, 914.9763148116791, 930.0624814926241, 925.5709044118435, 905.0879709251489, 916.6723123029288, 208.71063136978015, 55.608033304398994, 879.3952196962259, 882.6384060919638, 880.4385528788468, 887.8214277036329, 892.8621478054486, 887.241105180456, 885.1685838319949, 893.7159340031112, 899.1220762165033, 908.6025878665821, 904.8256607287568, 898.349573208834, 866.605473792319, 901.6728101272954, 28.290651635068752, 376.20114458824673, 899.0173718023339, 895.1625021824553, 905.5350859652966, 651.0180985837144, 866.4971695246368, 862.2796611317145, 863.8750341981886, 857.9469853479113, 861.3826582502494, 870.9501073550798, 876.0414769296597, 881.9962285127145, 886.760059316986, 862.97387603029, 282.39212162168155, 799.6941516388752, 879.1908363019649, 875.8920631061312, 872.5139083453928, 879.8693142598136, 883.6797348767399, 886.816497746489, 878.8260534629527, 880.6120466881047, 890.7514496350484, 887.5473028409018, 878.5727115626013, 885.8403890193001, 883.2369031196686, 882.5626793645729, 880.0418624927801, 883.0674341170902, 21.092870383390835, 848.2703883932817, 853.294351865218, 861.6273035832219, 708.9075913311325, 894.1409851824336, 915.5679417525702, 917.4427990128422, 917.8598271529829, 911.0328528071387, 907.7878008365494, 906.8895840253359, 899.9604017721643, 739.2371537558582, 763.0448101141084, 903.9743570744548, 884.0828399468313, 28.64656754602983, 858.5238027554192, 853.5783758492065, 861.6838164273128, 858.6223406596093, 858.4520913615844, 909.0371553782328, 911.2472523781577, 916.5946615350607, 920.4664125473558, 918.6455407506127, 918.6524498697761, 909.3075858555487, 902.9211903571761, 911.8096065626501, 872.5113938839309, 878.1762826933103, 886.663203114307, 882.3371220753124, 889.3813167433397, 893.5121936992045, 887.6170148026097, 887.2976027300061, 890.2353281173988, 896.0583176087692, 889.7854060707796, 878.0756251118419, 838.6180574780166, 884.871817424867, 901.9013443440381, 898.8595679688884, 895.8271551727734, 891.9103395263407, 894.2791306859845, 894.5007886594309, 896.1239007996656, 889.1610860898063, 891.3349509065129, 883.9725943673657, 881.892667804917, 887.647118996021, 893.8214359463352, 880.8299023188996, 852.9944602487213, 805.1865549532607, 325.24195632332953, 919.229250072076, 917.9921142840564, 921.0001016107451, 876.6295558025098, 864.4963665914914, 926.249854843077, 924.8242818756879, 934.2621431954644, 924.7895934452582, 936.119224922557, 879.0038886931248, 928.4907523318694, 933.1712647354807, 934.4914942676289, 940.4003769268081, 936.7172767999987, 939.2110919586962, 929.1455446553323, 841.897322421785, 879.5434184460449, 869.2034217596786, 852.0631166528502, 871.3284199774102, 861.3639186820095, 871.0701869493057, 880.493522024172, 447.2104744050454, 872.2858850304847, 861.4615907322269, 861.7108925553769, 863.1730938739844, 881.8949650061094, 883.5411226149874, 846.1383585411195, 889.7243006645579, 876.3673335706695, 853.6093164513452, 863.8478905912003, 25.591535775501498, 20.536283268245153, 811.1907807053998, 833.4821164644349, 826.3296142408417, 863.6200545985893, 17.677317736360237, 173.77993121332676, 183.26654000411366, 182.13643157824777, 177.1233510339381, 15.908203438737488, 18.331484536954548, 166.38005252329924, 117.39443180078038, 178.7854840307248, 14.058878085203181, 48.199230675804166, 253.7313030573981, 738.958138826018, 180.98739299261243, 850.5067625208618, 837.7255821431988, 842.7319506133094, 844.9821151128745, 840.8892275530508, 833.5286792543973, 513.875455902905, 831.7558179614232, 748.8110913226183, 828.0420800662417, 816.9455310971997, 579.982007271978, 690.6460251290899, 615.6146067416797, 686.7120260533354, 838.4384807917472, 704.7186204864933, 478.8955519720049, 792.2970751625721, 394.74987101305203, 807.2143334716334, 475.13021986028855, 843.2287698396179, 832.6030633840696, 199.67342993803157, 798.7209629996621, 209.4684860647631, 11.594698893661752, 637.2993877227663, 13.224345689765874, 597.0801091850368, 867.0794957455245, 859.5935674571543, 857.893886629996, 857.5032242884446, 853.6165188221657, 861.4582254101102, 854.9575446265625, 891.9782980706177, 872.5006849136468, 878.7012779863802, 727.7703429417811, 701.1374310860102, 175.95006051390564, 173.57450327719422, 14.003785830797405, 150.30956449463818, 175.42267246030087, 543.8489533391346, 680.1743493945252, 816.8990031257147, 840.7471270775662, 742.3050149487365, 176.4292071864007, 13.937554105408758, 13.419787260551681, 182.3402268773096, 876.484267374391, 806.4584190534434, 810.3071224621058, 23.788065446750032, 76.86699935189827, 77.68469124809648, 787.8312965054254, 800.0279065512726, 230.7388096714419, 173.08797838304054, 814.378530626907, 478.01131338981924, 163.75032606998158, 173.82102901400242, 25.38600631330324, 14.917486775353623, 738.957631747534, 853.4517035427293, 847.9984474051148, 850.1139224070118, 850.6548262437292, 858.9979100077755, 862.1270584837774, 851.5634924737186, 851.1909402452036, 847.2657660673218, 850.0248235649433, 849.2994720709501, 848.4730173143106, 852.256464997806, 855.745013317458, 853.8103695960733, 852.7436120268447, 834.8930141813005, 903.965388294991, 188.34896516852695, 182.332696135841, 648.5568773655959, 15.957211913458446, 151.522624595935, 841.982418456253, 295.9452506683933, 394.3642990271164, 435.95657495280824, 590.9662866037761, 199.15997839488426, 743.3113562133194, 406.77115454567314, 191.2648957104357, 178.1514542914623, 21.50340823730324, 188.4659422603583, 181.1167324180281, 765.4487268409301, 780.6640189209696, 175.77364146461585, 187.42922067182516, 24.417130598264208, 79.89998661789956, 195.07110304535283, 787.0452223472633, 432.8779457779756, 847.5090221173845, 831.3240260464216, 769.7583506095074, 817.8026890669511, 833.1749805859638, 841.2992801314155, 844.085229797298, 180.4190931161445, 182.82374097437702, 135.87269024921645, 175.81939786847443, 733.9885634025687, 808.8882604237698, 511.5285327164207, 175.0367967025817, 34.42404467057231, 14.31056950091359, 412.7104527384675, 13.988636060361225, 16.7617204145391, 170.1674833612474, 190.14379645223042, 821.8637150365579, 814.2305998812965, 236.91584197551066, 822.3003576654219, 746.4822108207965, 839.029448120311, 839.087455754877, 644.4759989095488, 708.4376668670914, 841.7958787090186, 805.7280448353715, 816.4138360638348, 254.13257528167597, 842.4022182133431, 680.2852925297747, 731.9721802161214, 716.6211725831112, 543.379152075687, 446.2091498295915, 839.0790745149714, 173.21614290140005, 13.585473306470016, 145.99917209534493, 302.79714714729226, 866.910122762468, 850.1264270457866, 473.98078102564455, 172.1263558942539, 168.53146144430036, 175.34951228670954, 19.98478823057968, 69.03969044999566, 172.75159380963203, 174.46306886347438, 778.8945518761175, 173.0054035642491, 18.16249315132539, 38.51863648270284, 45.84422163165294, 723.9450775650231, 681.199157803704, 16.09585458391004, 74.05077199921996, 30.36241930187715, 168.13294101808685, 189.98001903662941, 179.7165551382508, 888.3903794985733, 451.30762747685856, 802.4417518343403, 35.80771995194866, 177.38709342878482, 310.3481641187967, 808.653342458865, 818.0667171233206, 817.0329407552731, 472.6290568187022, 814.720382681381, 432.3219576146111, 809.759050588002, 810.8503706070142, 841.861010088882, 586.7272685389207, 303.2732550923978, 853.8589945794232, 707.3967159577974, 757.5821785993331, 698.9763327010523, 178.07324131192928, 17.27554776032783, 63.03100108899686, 18.067291060219418, 255.23249759084274, 840.8902153540861, 762.3492923402742, 745.1409748102991, 426.30129898758526, 176.96519159636867, 481.7844472013612, 177.22759886933065, 16.383600563517437, 175.37512225489206, 599.0191049595217, 616.4397045339958, 815.1618123437228, 894.4810288106258, 869.410675232188, 843.7480619505162, 802.830159148701, 12.963895975969367, 15.538889403471833, 842.7534723940008, 866.1429455007698, 901.1229060958547, 893.2455648842285, 904.1992298333781, 840.7318063234338]
Elapsed: 0.24348114415645347~0.5871454243834539
Time per graph: 0.0056610723630242344~0.013654317172038093
Speed: 682.4113183847829~296.7936497153866
Total Time: 0.0530
best val loss: 0.19141912460327148 test_score: 0.8140

Testing...
Test loss: 0.4391 score: 0.8140 time: 0.05s
test Score 0.8140
Epoch Time List: [0.480402200948447, 0.23165051499381661, 0.23635498306248337, 0.24537496489938349, 0.2494306720327586, 0.2522010870743543, 0.23866619099862874, 0.2368397390237078, 0.8923679920844734, 3.482520340010524, 0.6687864249106497, 0.2591526519972831, 0.24343741813208908, 0.22690718597732484, 0.23128005699254572, 0.2292608447605744, 0.28828608291223645, 0.3012420309241861, 4.807711353059858, 2.84745625697542, 1.284412236069329, 0.31760633597150445, 0.2498023509979248, 0.25499244104139507, 0.25318737095221877, 0.26072893710806966, 0.26677056087646633, 0.2661114679649472, 0.2589731839252636, 3.775720320059918, 4.195870227995329, 1.18387481989339, 0.31894658599048853, 0.23825160600245, 0.24877576704602689, 0.2390887641813606, 0.23418310389388353, 0.23681667598430067, 0.24010133708361536, 0.23607592401094735, 0.24074556794948876, 0.2354166880249977, 0.2333479248918593, 0.23109959112480283, 0.23545167688280344, 0.23509613098576665, 0.32459252688568085, 6.943038871977478, 3.2115896781906486, 1.3041555748786777, 0.3591682950500399, 0.2739344679284841, 0.32167562504764646, 0.27764329011552036, 0.2815652199788019, 0.27800144208595157, 0.2762110590701923, 0.27105708490125835, 8.558935633976944, 2.5695596809964627, 0.24475416890345514, 0.24042713386006653, 0.2421369361691177, 0.252992843859829, 0.25830665009561926, 0.2556055859895423, 0.24427121388725936, 0.24270237109158188, 0.24242074484936893, 0.23750618495978415, 0.2422159321140498, 0.24431246402673423, 0.24438434198964387, 0.23672560404520482, 0.2396555079612881, 0.24055274995043874, 0.24067654891405255, 6.5129730919143185, 0.9573900459799916, 0.25578214693814516, 0.2396165980026126, 0.23728616803418845, 0.24125806393567473, 0.239739004871808, 0.24093338486272842, 0.23408401396591216, 0.2398862399859354, 0.23462435218971223, 0.23656323098111898, 0.2345487530110404, 4.750608935020864, 3.1640275300014764, 1.6501905767945573, 0.2698421620298177, 0.26027152698952705, 0.2635450870729983, 0.2640966789331287, 0.24664243508595973, 0.24299498193431646, 0.24151444295421243, 0.24228266708087176, 0.24994208093266934, 0.2569977610837668, 5.9562670171726495, 2.5639751709531993, 2.252431231085211, 0.2516482979990542, 0.2437203839654103, 0.24449919606558979, 0.248160659102723, 0.24592142400797457, 0.25396329921204597, 0.24572805094067007, 0.24028735794126987, 0.2427210749592632, 0.24705357605125755, 0.24498226307332516, 2.1862613251432776, 4.368229487910867, 0.4297018718207255, 0.36224929604213685, 0.2564236130565405, 0.2658683229237795, 0.25815837597474456, 0.24034255696460605, 0.3001924640266225, 0.282580483937636, 0.2842074119253084, 0.248626826913096, 0.24542293592821807, 1.7346668120007962, 6.556002522818744, 1.1186421690508723, 0.2581495000049472, 0.25687140389345586, 0.26136599807068706, 0.25921133894007653, 0.25813911703880876, 0.2554723139619455, 0.25584166415501386, 0.255838253069669, 0.254921811982058, 0.25782274501398206, 0.25792451889719814, 0.2519736341200769, 0.5783722980413586, 5.864421446924098, 3.186518831877038, 2.779937923885882, 0.2347439369186759, 0.23399049893487245, 0.23652323288843036, 0.23714383493643254, 0.23261753492988646, 0.23073062603361905, 0.23104176297783852, 0.23484687402378768, 0.24074353813193738, 0.2345844869269058, 0.23310343897901475, 0.24611599498894066, 0.24307057401165366, 0.24101394903846085, 0.24686526495497674, 0.2570637691533193, 0.2525096479803324, 0.2507795401616022, 2.428092908114195, 7.505616341019049, 0.25318944407626987, 0.2494652319001034, 0.24726579699199647, 0.25000528106465936, 0.24785440508276224, 0.24810787907335907, 0.24881537887267768, 0.24890788504853845, 0.24752156110480428, 0.24664850602857769, 0.24764904798939824, 0.24894000787753612, 0.25034175999462605, 4.084463177016005, 4.062065158970654, 0.26752617093734443, 0.2624818111071363, 0.2820732139516622, 0.26189229311421514, 0.2602439569309354, 0.2602414949797094, 0.25114548997953534, 0.24213765701279044, 0.2377538449363783, 0.2562938790069893, 0.23618142597842962, 0.23400303395465016, 0.23578378383535892, 0.2421328390482813, 0.2521880710264668, 0.2556494759628549, 0.23713864001911134, 0.23745526594575495, 0.3263405760517344, 5.851404786924832, 0.8721186158945784, 0.26002403697930276, 0.26536631491035223, 0.3308978680288419, 0.24206786206923425, 0.24487479985691607, 0.2539507100591436, 0.24011229304596782, 0.24012731190305203, 0.23376119195017964, 3.0736863389611244, 5.454608349013142, 0.23844668397214264, 0.23313227505423129, 0.24111499590799212, 0.23444232600741088, 0.23559300310444087, 0.23887486895546317, 0.2308670450001955, 0.22800163482315838, 0.24415974714793265, 0.22985930694267154, 3.3599332800367847, 1.6888335380936041, 0.254239339963533, 0.2528514019213617, 0.251472377916798, 0.2518444269662723, 0.2540884871268645, 0.2598153919680044, 0.2573032259242609, 0.25390682904981077, 0.2515582471387461, 5.494140371098183, 0.8143191379494965, 0.23501959291752428, 0.23714191908948123, 0.23680471908301115, 0.23260392004158348, 0.2397349098464474, 0.25440652307588607, 0.2551693341229111, 0.2555427150800824, 0.2549610409187153, 0.262247726903297, 0.2565293119987473, 0.25978270289488137, 0.2597151338122785, 0.25979775190353394, 8.79942558100447, 4.2771238358691335, 0.47419394890312105, 0.25046966201625764, 0.2515510180965066, 0.24729631701484323, 0.24770437099505216, 0.25385651097167283, 0.2618143099825829, 0.256894304882735, 0.23773664701730013, 0.23643074498977512, 0.23371615109499544, 0.2343672189163044, 0.23391605110373348, 0.23348442290443927, 0.2394326999783516, 11.960624307044782, 3.6245032830629498, 0.25286101503297687, 0.24906757893040776, 0.24719527608249336, 0.24825661210343242, 0.24868838500697166, 0.2512796079972759, 0.26462450390681624, 0.25361922895535827, 0.26542040205094963, 0.2572052931645885, 0.25116379698738456, 0.2548625549534336, 0.2520213848911226, 0.25340388203039765, 9.786080386955291, 4.146915941033512, 0.25376143713947386, 0.24564530095085502, 0.24323290202300996, 0.24353831203188747, 0.24109426012728363, 0.24308621499221772, 0.2491076331352815, 0.24439024704042822, 0.2435111029772088, 0.24600026512052864, 0.2461735469987616, 0.2333271490642801, 4.722994087031111, 6.916646301862784, 0.386709550046362, 0.251659682020545, 0.2491323041031137, 0.25265534094069153, 0.25679113902151585, 0.25177766697015613, 0.25277117686346173, 0.25195907603483647, 0.25955879408866167, 0.255572073161602, 2.8071263069286942, 6.486098140012473, 0.24200535705313087, 0.24196343612857163, 0.2475049908971414, 0.24572852801065892, 0.24593147088307887, 0.24534385208971798, 0.2438277080655098, 0.24526742496527731, 0.24513256398495287, 0.24540515698026866, 0.2430530219571665, 0.2439451339887455, 0.24621825909707695, 0.24848339997697622, 4.855698894942179, 0.30135459813755006, 0.25282949989195913, 0.23958103300537914, 0.2444021450355649, 0.24284454516600817, 0.29054352606181055, 0.2758578910725191, 0.2796572250081226, 0.29920319211669266, 0.25604023691266775, 0.2569624480092898, 7.001272014924325, 6.603762212092988, 0.24224706320092082, 0.23581031209323555, 0.23293076606933028, 0.23368761804886162, 0.23432971618603915, 0.23606191703584045, 0.23691315506584942, 0.2350940831238404, 0.23594288108870387, 0.23629320797044784, 0.2730976858874783, 0.2591687219683081, 0.25063322810456157, 0.2515114889247343, 0.24267398891970515, 0.23219005903229117, 0.23227505409158766, 0.2312120129354298, 0.23136886896099895, 0.2329401649767533, 0.2307528150267899, 0.235892343102023, 0.23553732107393444, 0.23743924114387482, 0.23504060204140842, 0.23591194383334368, 0.23387577303219587, 0.238008251064457, 0.23312712006736547, 0.23650049499701709, 0.3932669840287417, 6.201583506073803, 0.44339079095516354, 0.2493024319410324, 0.24438658717554063, 0.24056231998838484, 0.2538900920189917, 0.24453339411411434, 0.24386399379000068, 0.24457804311532527, 0.2425723411142826, 0.24063294602092355, 0.24133963603526354, 0.24125325796194375, 0.24367206182796508, 0.2403740402078256, 4.3462954689748585, 7.818593163974583, 0.34533663489855826, 0.24051346199121326, 0.24058857513591647, 0.29885232693050057, 0.24685087811667472, 0.25116393400821835, 0.24932713306043297, 0.2504882839275524, 0.24772664008196443, 0.249970608856529, 0.32755679602269083, 0.24211333587300032, 0.2432404118590057, 0.24661697202827781, 0.35412112693302333, 6.660802643862553, 0.24699507607147098, 0.2479627428110689, 0.24578392505645752, 0.2479024069616571, 0.24449870886746794, 0.24335894791875035, 0.24226005596574396, 0.24491413997020572, 0.2428194689564407, 0.24193848110735416, 0.24357561999931931, 0.24369073112029582, 0.24610294203739613, 0.2432292849989608, 0.24477622483391315, 0.2463309558806941, 2.3586120378458872, 4.344944540178403, 0.25944862712640315, 0.2550948590505868, 0.2653950930107385, 0.2539260581834242, 0.23930602392647415, 0.23729586484842002, 0.2390366441104561, 0.2376052400795743, 0.23980297206435353, 0.24035533994901925, 0.2466475780820474, 0.26128390594385564, 0.2923608439741656, 0.2551501019624993, 0.23995779908727854, 5.791421435074881, 4.1203217200236395, 0.25462310400325805, 0.2543116550659761, 0.2539524179883301, 0.2532627460313961, 0.24641727400012314, 0.23639344901312143, 0.23735561582725495, 0.23718517902307212, 0.23764725006185472, 0.2360865519149229, 0.23703055107034743, 0.23781008610967547, 0.2382508759619668, 5.6508835250278935, 0.2468563880538568, 0.24401930498424917, 0.24338334880303591, 0.2433473049895838, 0.24359956197440624, 0.24295712891034782, 0.24388303013984114, 0.2423991389805451, 0.24423419800586998, 0.242636013077572, 0.2504274579696357, 0.2566701788455248, 0.2502403490943834, 0.24103881197515875, 0.2414093598490581, 0.24134868080727756, 0.24151771795004606, 0.24038723600097, 0.2421752898953855, 0.241672926931642, 0.24333071883302182, 0.24329869193024933, 0.2440480530494824, 0.24445461900904775, 0.25060763000510633, 0.24362129194196314, 0.24531407293397933, 0.25368392292875797, 0.25830489792861044, 5.940537646994926, 0.32787362404633313, 0.23519998602569103, 0.23302434710785747, 0.23839554621372372, 0.24672456493135542, 0.2418173699406907, 0.23091156012378633, 0.22958684712648392, 0.2328994859708473, 0.23259517212864012, 0.23640571406576782, 0.23613254097290337, 0.23226602794602513, 0.2978987470269203, 0.23131633491721004, 0.23152344301342964, 0.2314202570123598, 0.2328611999982968, 0.23957707011140883, 0.32956075691618025, 0.24537296791095287, 0.24711605219636112, 0.24723585997708142, 0.24691595102194697, 0.24816266004927456, 0.24546707118861377, 0.2971016060328111, 0.2460471959784627, 0.24535949307028204, 0.24575488886330277, 0.24581111199222505, 0.2465588670456782, 0.2431120730470866, 0.2554132388904691, 0.24078595300670713, 0.24981719115749002, 0.25456283090170473, 0.24795426591299474, 4.091811235994101, 8.302291542990133, 3.638208154006861, 0.25986504717729986, 0.25516919407527894, 0.25667800195515156, 10.483953597955406, 5.597976327990182, 0.4979495200095698, 0.4867775719612837, 0.7456607919884846, 3.4627063480438665, 13.343575674924068, 3.5188740799203515, 0.9248007700080052, 0.6748986579477787, 6.633395142969675, 10.085445508011617, 0.8601750178495422, 0.2874900458846241, 0.5268305151257664, 0.25823591102380306, 0.2646168661303818, 0.25668118707835674, 0.2548418560763821, 0.255879900068976, 0.2751457799458876, 0.3165975649608299, 0.2815637409221381, 0.28195634903386235, 0.2732701350469142, 0.3222180779557675, 0.3308337260968983, 0.30337806488387287, 0.3197460968513042, 0.29688343906309456, 0.2743920700158924, 0.29825618490576744, 0.32436863612383604, 0.27584662195295095, 0.31826071708928794, 0.26655599602963775, 0.32577731006313115, 0.2614811861421913, 0.2843505331547931, 0.42054617893882096, 0.5579459508880973, 0.7555120120523497, 4.526920282049105, 7.0334277810761705, 6.28189651411958, 11.296925690956414, 0.2770325050223619, 0.32273214985616505, 0.2555128709645942, 0.2564204179216176, 0.2597523699514568, 0.25253334804438055, 0.25154855207074434, 0.29164097597822547, 0.24970039899926633, 0.2479412789689377, 0.28211318782996386, 0.3183574171271175, 0.8758314379956573, 1.0197229880141094, 6.445097122108564, 11.65927407995332, 1.002192143118009, 0.32536931813228875, 0.32549402804579586, 0.30625542893540114, 0.26779497996903956, 0.5544963920256123, 0.6964277479564771, 7.0895796000259, 7.142474296037108, 4.094979275949299, 0.5148265819298103, 0.5424974928610027, 0.5472653979668394, 9.409702838980593, 5.259708486846648, 1.4792313339421526, 0.844351094099693, 0.44424496591091156, 0.452267152024433, 0.5362753489753231, 0.2790412639733404, 0.5814851450268179, 0.6378620920004323, 1.0160387101350352, 2.4867206050548702, 17.196718681021594, 9.827882907935418, 0.2559632280608639, 0.2510065510869026, 0.25111342198215425, 0.2524487090995535, 0.25084030407015234, 0.2544885780662298, 0.25530043395701796, 0.25496153498534113, 0.25541797606274486, 0.2560168970376253, 0.2574685920262709, 0.25452442304231226, 0.2546183590311557, 0.25391168100759387, 0.25043043692130595, 0.2544663930311799, 0.25516336713917553, 0.25125758594367653, 0.4382887239335105, 0.8031452859286219, 0.5421460248762742, 8.246158139081672, 11.134069422958419, 0.8456597049953416, 0.5941606190754101, 0.5615572129609063, 0.4532339700963348, 0.89467031089589, 0.5192105270689353, 0.5431131310760975, 0.35469386412296444, 0.8138603720581159, 0.9956711569102481, 5.132983723888174, 2.685916274902411, 0.9722826980287209, 0.30525626009330153, 0.49169334897305816, 0.7306925531011075, 0.49066878797020763, 2.0190030449302867, 9.234405496972613, 0.9853973760036752, 0.3017115400871262, 0.3316616148222238, 0.2932562700007111, 0.2957830410450697, 0.25996004207991064, 0.36192604596726596, 0.2617807339411229, 0.2622871260391548, 0.3172575121279806, 0.9464843370951712, 0.7381432829424739, 8.210084498976357, 0.8041698841843754, 0.2908640750683844, 0.47193729504942894, 0.335060304030776, 0.8935746641363949, 2.7282630221452564, 12.133415987016633, 2.039289348060265, 9.594160304986872, 8.318989349179901, 5.607507123844698, 0.9901643070625141, 0.2903073320630938, 0.31193712493404746, 0.39223595906514674, 0.2769206080120057, 0.2739860210567713, 0.25683707708958536, 0.3126059900969267, 0.31364592001773417, 0.267756185028702, 0.26624695502687246, 0.31463447597343475, 0.2627381390193477, 0.3851384820882231, 0.29732687992509454, 0.3359800629550591, 0.27908750891219825, 0.34310481091961265, 0.3603457420831546, 0.48757400910835713, 0.3800669079646468, 0.8024382089497522, 6.353394043864682, 7.1989248908357695, 1.1691459398716688, 0.27702436700928956, 0.25836186297237873, 0.35347896406892687, 0.5069940539542586, 1.1975248489761725, 0.7833748949924484, 12.202292431844398, 7.4072894571581855, 3.6185345428530127, 0.7510046029929072, 0.7573980109300464, 0.9664890019921586, 2.8766878199530765, 6.47486969595775, 9.630847267108038, 0.2627694179536775, 0.2916360429953784, 2.8866869090124965, 8.219970389152877, 4.589588272036053, 2.768940200097859, 0.7636275100521743, 0.7962957309791818, 0.502920906059444, 0.7863017549971119, 0.8466707330662757, 10.348912594956346, 2.0729071359382942, 0.3888796699466184, 0.27475850109476596, 0.2859370890073478, 0.28715146495960653, 0.31760113209020346, 0.32135507406201214, 0.32400885899551213, 0.2698627948993817, 0.2869337670272216, 0.26905973500106484, 0.29235236695967615, 0.34815485996659845, 0.295205197064206, 0.2925257950555533, 0.27950477495323867, 0.5684523071395233, 0.7251453018980101, 5.591694243019447, 5.568288777954876, 5.518511277856305, 1.854852291988209, 0.6116897069150582, 0.2700112001039088, 0.33657455421052873, 0.3517789088655263, 0.7427019400056452, 0.32777984195854515, 0.7024590260116383, 15.494243092020042, 2.166209144052118, 0.3223674619803205, 0.30474994191899896, 0.2585319390054792, 0.3040707301115617, 0.24324868712574244, 0.2520150279160589, 0.2637809378793463, 6.640947736916132, 13.846126860007644, 2.0402371141826734, 0.2494232360040769, 0.23932452301960438, 0.23962690704502165, 0.23535135004203767, 0.24186815705616027]
Total Epoch List: [16, 336, 430]
Total Time List: [0.05510198697447777, 0.05002069193869829, 0.05303841095883399]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351dbaa200>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.1464;  Loss pred: 2.1464; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 2.1515;  Loss pred: 2.1515; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 2.1310;  Loss pred: 2.1310; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 2.1520;  Loss pred: 2.1520; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 2.1351;  Loss pred: 2.1351; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 2.0772;  Loss pred: 2.0772; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 2.0816;  Loss pred: 2.0816; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 2.0663;  Loss pred: 2.0663; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 2.0103;  Loss pred: 2.0103; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 2.0231;  Loss pred: 2.0231; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 1 of 2
Epoch 11/1000, LR 0.000270
Train loss: 1.9816;  Loss pred: 1.9816; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 2.0103,   Val_Loss: 0.6930,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.6930,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6935


[0.08459676499478519, 0.06626931903883815, 0.06665601104032248, 0.089698052033782, 0.06427923694718629, 0.06718824605923146, 0.09642257506493479, 0.06766697100829333, 0.06542453705333173, 0.07118417404126376, 0.10903747600968927]
[0.0019226537498814816, 0.0015061208872463214, 0.0015149093418255109, 0.0020385920916768637, 0.0014608917487996885, 0.0015270055922552604, 0.0021914221605667, 0.0015378857047339393, 0.0014869212966666303, 0.0016178221373014492, 0.002478124454765665]
[520.1144512170447, 663.9573280391356, 660.1055075645452, 490.53462145898953, 684.5134150573643, 654.8764490921628, 456.3246726232799, 650.2433808453952, 672.5305517123152, 618.114919398998, 403.53098411861697]
Elapsed: 0.07712939666287805~0.014639790206840378
Time per graph: 0.001752940833247228~0.0003327225047009177
Speed: 588.6223891934407~96.36741999045645
Total Time: 0.1094
best val loss: 0.6929962038993835 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.06s
test Score 0.5000
Epoch Time List: [0.6273739230819046, 0.28242024790961295, 0.29188285302370787, 0.3171775030205026, 0.30749962106347084, 0.2928375790361315, 0.4048267911421135, 0.28354904404841363, 0.3434214540757239, 0.3817248650593683, 0.31397711706813425]
Total Epoch List: [11]
Total Time List: [0.10935878101736307]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351b1a9ea0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2607;  Loss pred: 2.2607; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7017 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.3139;  Loss pred: 2.3139; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7016 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.2717;  Loss pred: 2.2717; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7014 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 2.2816;  Loss pred: 2.2816; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7012 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 2.2865;  Loss pred: 2.2865; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7009 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5116 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 2.2309;  Loss pred: 2.2309; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7005 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 2.2124;  Loss pred: 2.2124; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7001 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 2.1963;  Loss pred: 2.1963; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 2.1705;  Loss pred: 2.1705; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 1.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5116 time: 1.48s
Epoch 10/1000, LR 0.000240
Train loss: 2.1340;  Loss pred: 2.1340; Loss self: 0.0000; time: 5.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.5000 time: 2.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5116 time: 0.65s
Epoch 11/1000, LR 0.000270
Train loss: 2.1295;  Loss pred: 2.1295; Loss self: 0.0000; time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5116 time: 0.26s
Epoch 12/1000, LR 0.000270
Train loss: 2.0743;  Loss pred: 2.0743; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 2.0449;  Loss pred: 2.0449; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5116 time: 0.20s
Epoch 14/1000, LR 0.000270
Train loss: 2.0495;  Loss pred: 2.0495; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5116 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 2.0283;  Loss pred: 2.0283; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5116 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 1.9772;  Loss pred: 1.9772; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 1.9648;  Loss pred: 1.9648; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 1.9065;  Loss pred: 1.9065; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 1.9328;  Loss pred: 1.9328; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 1.8619;  Loss pred: 1.8619; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 1.8281;  Loss pred: 1.8281; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 1.8242;  Loss pred: 1.8242; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 2.09s
Epoch 23/1000, LR 0.000270
Train loss: 1.7793;  Loss pred: 1.7793; Loss self: 0.0000; time: 3.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 1.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.76s
Epoch 24/1000, LR 0.000270
Train loss: 1.7774;  Loss pred: 1.7774; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 1.7626;  Loss pred: 1.7626; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 1.7182;  Loss pred: 1.7182; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 1.6890;  Loss pred: 1.6890; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 1.6923;  Loss pred: 1.6923; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 1.6695;  Loss pred: 1.6695; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.12s
Epoch 30/1000, LR 0.000270
Train loss: 1.6392;  Loss pred: 1.6392; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 1.6223;  Loss pred: 1.6223; Loss self: 0.0000; time: 4.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 2.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.33s
Epoch 32/1000, LR 0.000270
Train loss: 1.6139;  Loss pred: 1.6139; Loss self: 0.0000; time: 4.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 1.5789;  Loss pred: 1.5789; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 1.5658;  Loss pred: 1.5658; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 1.5540;  Loss pred: 1.5540; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.10s
Epoch 36/1000, LR 0.000270
Train loss: 1.5263;  Loss pred: 1.5263; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 1.5208;  Loss pred: 1.5208; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 1.5083;  Loss pred: 1.5083; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 1.5026;  Loss pred: 1.5026; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 1.4905;  Loss pred: 1.4905; Loss self: 0.0000; time: 5.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 2.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 2.31s
Epoch 41/1000, LR 0.000269
Train loss: 1.4708;  Loss pred: 1.4708; Loss self: 0.0000; time: 6.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 2.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 2.83s
Epoch 42/1000, LR 0.000269
Train loss: 1.4523;  Loss pred: 1.4523; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.07s
Epoch 43/1000, LR 0.000269
Train loss: 1.4461;  Loss pred: 1.4461; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.11s
Epoch 44/1000, LR 0.000269
Train loss: 1.4294;  Loss pred: 1.4294; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 1.4082;  Loss pred: 1.4082; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 1.3862;  Loss pred: 1.3862; Loss self: 0.0000; time: 3.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 4.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 3.93s
Epoch 47/1000, LR 0.000269
Train loss: 1.3770;  Loss pred: 1.3770; Loss self: 0.0000; time: 8.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 2.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 1.83s
Epoch 48/1000, LR 0.000269
Train loss: 1.3747;  Loss pred: 1.3747; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 1.3517;  Loss pred: 1.3517; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 1.3575;  Loss pred: 1.3575; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 1.3412;  Loss pred: 1.3412; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 1.3363;  Loss pred: 1.3363; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 1.3104;  Loss pred: 1.3104; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 1.3225;  Loss pred: 1.3225; Loss self: 0.0000; time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 1.2964;  Loss pred: 1.2964; Loss self: 0.0000; time: 0.28s
Val loss: 0.6916 score: 0.5227 time: 2.77s
Test loss: 0.6914 score: 0.5349 time: 2.85s
Epoch 56/1000, LR 0.000269
Train loss: 1.2895;  Loss pred: 1.2895; Loss self: 0.0000; time: 1.68s
Val loss: 0.6915 score: 0.5227 time: 1.38s
Test loss: 0.6913 score: 0.5349 time: 2.14s
Epoch 57/1000, LR 0.000269
Train loss: 1.2723;  Loss pred: 1.2723; Loss self: 0.0000; time: 1.47s
Val loss: 0.6914 score: 0.5227 time: 1.08s
Test loss: 0.6912 score: 0.5349 time: 0.99s
Epoch 58/1000, LR 0.000269
Train loss: 1.2696;  Loss pred: 1.2696; Loss self: 0.0000; time: 0.52s
Val loss: 0.6913 score: 0.5227 time: 0.14s
Test loss: 0.6912 score: 0.5349 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 1.2497;  Loss pred: 1.2497; Loss self: 0.0000; time: 0.72s
Val loss: 0.6913 score: 0.5227 time: 0.34s
Test loss: 0.6911 score: 0.5349 time: 0.12s
Epoch 60/1000, LR 0.000268
Train loss: 1.2452;  Loss pred: 1.2452; Loss self: 0.0000; time: 0.56s
Val loss: 0.6912 score: 0.5455 time: 0.18s
Test loss: 0.6910 score: 0.5349 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 1.2477;  Loss pred: 1.2477; Loss self: 0.0000; time: 3.91s
Val loss: 0.6911 score: 0.5909 time: 2.74s
Test loss: 0.6910 score: 0.5581 time: 2.60s
Epoch 62/1000, LR 0.000268
Train loss: 1.2257;  Loss pred: 1.2257; Loss self: 0.0000; time: 6.56s
Val loss: 0.6910 score: 0.5909 time: 2.37s
Test loss: 0.6909 score: 0.5814 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 1.2344;  Loss pred: 1.2344; Loss self: 0.0000; time: 0.33s
Val loss: 0.6909 score: 0.5909 time: 0.16s
Test loss: 0.6908 score: 0.6047 time: 0.29s
Epoch 64/1000, LR 0.000268
Train loss: 1.2138;  Loss pred: 1.2138; Loss self: 0.0000; time: 0.67s
Val loss: 0.6908 score: 0.5909 time: 0.24s
Test loss: 0.6908 score: 0.6279 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 1.2249;  Loss pred: 1.2249; Loss self: 0.0000; time: 0.25s
Val loss: 0.6907 score: 0.6136 time: 0.50s
Test loss: 0.6907 score: 0.6512 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 1.2052;  Loss pred: 1.2052; Loss self: 0.0000; time: 0.36s
Val loss: 0.6905 score: 0.6364 time: 1.52s
Test loss: 0.6906 score: 0.6744 time: 2.09s
Epoch 67/1000, LR 0.000268
Train loss: 1.2116;  Loss pred: 1.2116; Loss self: 0.0000; time: 5.04s
Val loss: 0.6904 score: 0.6591 time: 0.33s
Test loss: 0.6905 score: 0.6977 time: 0.82s
Epoch 68/1000, LR 0.000268
Train loss: 1.1958;  Loss pred: 1.1958; Loss self: 0.0000; time: 1.31s
Val loss: 0.6903 score: 0.7500 time: 0.22s
Test loss: 0.6904 score: 0.7674 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 1.1917;  Loss pred: 1.1917; Loss self: 0.0000; time: 0.55s
Val loss: 0.6902 score: 0.7955 time: 0.19s
Test loss: 0.6903 score: 0.7674 time: 0.07s
Epoch 70/1000, LR 0.000268
Train loss: 1.1790;  Loss pred: 1.1790; Loss self: 0.0000; time: 0.17s
Val loss: 0.6900 score: 0.7727 time: 0.08s
Test loss: 0.6902 score: 0.7907 time: 0.15s
Epoch 71/1000, LR 0.000268
Train loss: 1.1818;  Loss pred: 1.1818; Loss self: 0.0000; time: 0.21s
Val loss: 0.6899 score: 0.7500 time: 0.06s
Test loss: 0.6901 score: 0.8140 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 1.1712;  Loss pred: 1.1712; Loss self: 0.0000; time: 0.16s
Val loss: 0.6898 score: 0.7273 time: 0.05s
Test loss: 0.6900 score: 0.7907 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 1.1737;  Loss pred: 1.1737; Loss self: 0.0000; time: 0.16s
Val loss: 0.6896 score: 0.7273 time: 0.05s
Test loss: 0.6899 score: 0.7674 time: 0.06s
Epoch 74/1000, LR 0.000267
Train loss: 1.1627;  Loss pred: 1.1627; Loss self: 0.0000; time: 0.17s
Val loss: 0.6895 score: 0.7500 time: 0.05s
Test loss: 0.6898 score: 0.7674 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 1.1636;  Loss pred: 1.1636; Loss self: 0.0000; time: 3.44s
Val loss: 0.6893 score: 0.7500 time: 3.06s
Test loss: 0.6897 score: 0.7209 time: 2.69s
Epoch 76/1000, LR 0.000267
Train loss: 1.1504;  Loss pred: 1.1504; Loss self: 0.0000; time: 6.66s
Val loss: 0.6892 score: 0.7273 time: 0.26s
Test loss: 0.6896 score: 0.6977 time: 0.07s
Epoch 77/1000, LR 0.000267
Train loss: 1.1479;  Loss pred: 1.1479; Loss self: 0.0000; time: 0.26s
Val loss: 0.6890 score: 0.7273 time: 0.05s
Test loss: 0.6894 score: 0.6977 time: 0.06s
Epoch 78/1000, LR 0.000267
Train loss: 1.1453;  Loss pred: 1.1453; Loss self: 0.0000; time: 0.27s
Val loss: 0.6888 score: 0.7273 time: 0.09s
Test loss: 0.6893 score: 0.6977 time: 0.05s
Epoch 79/1000, LR 0.000267
Train loss: 1.1389;  Loss pred: 1.1389; Loss self: 0.0000; time: 0.18s
Val loss: 0.6887 score: 0.7045 time: 0.05s
Test loss: 0.6892 score: 0.6977 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 1.1301;  Loss pred: 1.1301; Loss self: 0.0000; time: 0.18s
Val loss: 0.6885 score: 0.7045 time: 2.40s
Test loss: 0.6890 score: 0.6512 time: 3.19s
Epoch 81/1000, LR 0.000267
Train loss: 1.1311;  Loss pred: 1.1311; Loss self: 0.0000; time: 8.43s
Val loss: 0.6883 score: 0.6818 time: 1.40s
Test loss: 0.6889 score: 0.6512 time: 0.25s
Epoch 82/1000, LR 0.000267
Train loss: 1.1230;  Loss pred: 1.1230; Loss self: 0.0000; time: 0.57s
Val loss: 0.6881 score: 0.6818 time: 0.19s
Test loss: 0.6887 score: 0.6279 time: 0.22s
Epoch 83/1000, LR 0.000266
Train loss: 1.1292;  Loss pred: 1.1292; Loss self: 0.0000; time: 0.57s
Val loss: 0.6879 score: 0.6818 time: 0.18s
Test loss: 0.6886 score: 0.6047 time: 0.05s
Epoch 84/1000, LR 0.000266
Train loss: 1.1236;  Loss pred: 1.1236; Loss self: 0.0000; time: 0.28s
Val loss: 0.6877 score: 0.6818 time: 1.21s
Test loss: 0.6884 score: 0.6047 time: 3.02s
Epoch 85/1000, LR 0.000266
Train loss: 1.1138;  Loss pred: 1.1138; Loss self: 0.0000; time: 8.28s
Val loss: 0.6875 score: 0.6818 time: 1.99s
Test loss: 0.6883 score: 0.6047 time: 1.42s
Epoch 86/1000, LR 0.000266
Train loss: 1.1144;  Loss pred: 1.1144; Loss self: 0.0000; time: 1.26s
Val loss: 0.6873 score: 0.6818 time: 1.10s
Test loss: 0.6881 score: 0.6047 time: 0.52s
Epoch 87/1000, LR 0.000266
Train loss: 1.1055;  Loss pred: 1.1055; Loss self: 0.0000; time: 0.83s
Val loss: 0.6871 score: 0.6818 time: 0.22s
Test loss: 0.6879 score: 0.6047 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 1.1059;  Loss pred: 1.1059; Loss self: 0.0000; time: 0.75s
Val loss: 0.6868 score: 0.6818 time: 0.21s
Test loss: 0.6877 score: 0.6047 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 1.1022;  Loss pred: 1.1022; Loss self: 0.0000; time: 0.57s
Val loss: 0.6866 score: 0.6818 time: 0.16s
Test loss: 0.6876 score: 0.6047 time: 0.05s
Epoch 90/1000, LR 0.000266
Train loss: 1.0970;  Loss pred: 1.0970; Loss self: 0.0000; time: 0.17s
Val loss: 0.6864 score: 0.6818 time: 0.05s
Test loss: 0.6874 score: 0.6047 time: 0.14s
Epoch 91/1000, LR 0.000266
Train loss: 1.0913;  Loss pred: 1.0913; Loss self: 0.0000; time: 0.20s
Val loss: 0.6861 score: 0.6818 time: 0.12s
Test loss: 0.6872 score: 0.6047 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 1.0891;  Loss pred: 1.0891; Loss self: 0.0000; time: 0.71s
Val loss: 0.6859 score: 0.6818 time: 0.37s
Test loss: 0.6870 score: 0.6047 time: 0.06s
Epoch 93/1000, LR 0.000265
Train loss: 1.0898;  Loss pred: 1.0898; Loss self: 0.0000; time: 0.73s
Val loss: 0.6856 score: 0.6818 time: 0.08s
Test loss: 0.6868 score: 0.6047 time: 0.16s
Epoch 94/1000, LR 0.000265
Train loss: 1.0832;  Loss pred: 1.0832; Loss self: 0.0000; time: 0.69s
Val loss: 0.6853 score: 0.6818 time: 0.05s
Test loss: 0.6865 score: 0.6047 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 1.0806;  Loss pred: 1.0806; Loss self: 0.0000; time: 0.60s
Val loss: 0.6850 score: 0.6818 time: 0.13s
Test loss: 0.6863 score: 0.6047 time: 0.24s
Epoch 96/1000, LR 0.000265
Train loss: 1.0773;  Loss pred: 1.0773; Loss self: 0.0000; time: 0.44s
Val loss: 0.6847 score: 0.6818 time: 0.05s
Test loss: 0.6861 score: 0.6047 time: 0.24s
Epoch 97/1000, LR 0.000265
Train loss: 1.0750;  Loss pred: 1.0750; Loss self: 0.0000; time: 0.19s
Val loss: 0.6844 score: 0.6818 time: 0.05s
Test loss: 0.6858 score: 0.6047 time: 0.05s
Epoch 98/1000, LR 0.000265
Train loss: 1.0708;  Loss pred: 1.0708; Loss self: 0.0000; time: 0.36s
Val loss: 0.6841 score: 0.6818 time: 0.12s
Test loss: 0.6856 score: 0.6047 time: 0.05s
Epoch 99/1000, LR 0.000265
Train loss: 1.0728;  Loss pred: 1.0728; Loss self: 0.0000; time: 1.15s
Val loss: 0.6838 score: 0.6818 time: 2.53s
Test loss: 0.6853 score: 0.6047 time: 0.94s
Epoch 100/1000, LR 0.000265
Train loss: 1.0678;  Loss pred: 1.0678; Loss self: 0.0000; time: 1.37s
Val loss: 0.6834 score: 0.6818 time: 0.76s
Test loss: 0.6850 score: 0.6047 time: 2.41s
Epoch 101/1000, LR 0.000265
Train loss: 1.0675;  Loss pred: 1.0675; Loss self: 0.0000; time: 0.60s
Val loss: 0.6830 score: 0.6818 time: 0.21s
Test loss: 0.6847 score: 0.6047 time: 0.23s
Epoch 102/1000, LR 0.000264
Train loss: 1.0656;  Loss pred: 1.0656; Loss self: 0.0000; time: 0.56s
Val loss: 0.6827 score: 0.6818 time: 0.16s
Test loss: 0.6844 score: 0.6279 time: 0.05s
Epoch 103/1000, LR 0.000264
Train loss: 1.0597;  Loss pred: 1.0597; Loss self: 0.0000; time: 0.36s
Val loss: 0.6823 score: 0.6818 time: 0.10s
Test loss: 0.6841 score: 0.6279 time: 0.05s
Epoch 104/1000, LR 0.000264
Train loss: 1.0568;  Loss pred: 1.0568; Loss self: 0.0000; time: 0.47s
Val loss: 0.6819 score: 0.6818 time: 0.05s
Test loss: 0.6838 score: 0.6279 time: 0.24s
Epoch 105/1000, LR 0.000264
Train loss: 1.0566;  Loss pred: 1.0566; Loss self: 0.0000; time: 0.23s
Val loss: 0.6815 score: 0.6818 time: 0.06s
Test loss: 0.6835 score: 0.6512 time: 0.24s
Epoch 106/1000, LR 0.000264
Train loss: 1.0524;  Loss pred: 1.0524; Loss self: 0.0000; time: 0.31s
Val loss: 0.6811 score: 0.6818 time: 0.20s
Test loss: 0.6831 score: 0.6512 time: 0.25s
Epoch 107/1000, LR 0.000264
Train loss: 1.0499;  Loss pred: 1.0499; Loss self: 0.0000; time: 0.17s
Val loss: 0.6807 score: 0.6818 time: 3.17s
Test loss: 0.6828 score: 0.6512 time: 3.21s
Epoch 108/1000, LR 0.000264
Train loss: 1.0451;  Loss pred: 1.0451; Loss self: 0.0000; time: 5.87s
Val loss: 0.6802 score: 0.7045 time: 2.22s
Test loss: 0.6825 score: 0.6512 time: 1.17s
Epoch 109/1000, LR 0.000264
Train loss: 1.0435;  Loss pred: 1.0435; Loss self: 0.0000; time: 5.79s
Val loss: 0.6798 score: 0.7045 time: 0.29s
Test loss: 0.6821 score: 0.6512 time: 0.05s
Epoch 110/1000, LR 0.000263
Train loss: 1.0411;  Loss pred: 1.0411; Loss self: 0.0000; time: 0.36s
Val loss: 0.6793 score: 0.7045 time: 2.30s
Test loss: 0.6817 score: 0.6512 time: 1.11s
Epoch 111/1000, LR 0.000263
Train loss: 1.0352;  Loss pred: 1.0352; Loss self: 0.0000; time: 0.17s
Val loss: 0.6788 score: 0.7045 time: 0.05s
Test loss: 0.6813 score: 0.6512 time: 0.06s
Epoch 112/1000, LR 0.000263
Train loss: 1.0395;  Loss pred: 1.0395; Loss self: 0.0000; time: 0.18s
Val loss: 0.6783 score: 0.7045 time: 0.06s
Test loss: 0.6808 score: 0.6512 time: 1.12s
Epoch 113/1000, LR 0.000263
Train loss: 1.0365;  Loss pred: 1.0365; Loss self: 0.0000; time: 9.34s
Val loss: 0.6778 score: 0.7045 time: 1.48s
Test loss: 0.6804 score: 0.6512 time: 1.43s
Epoch 114/1000, LR 0.000263
Train loss: 1.0334;  Loss pred: 1.0334; Loss self: 0.0000; time: 8.28s
Val loss: 0.6772 score: 0.7045 time: 1.96s
Test loss: 0.6800 score: 0.6512 time: 2.35s
Epoch 115/1000, LR 0.000263
Train loss: 1.0309;  Loss pred: 1.0309; Loss self: 0.0000; time: 0.36s
Val loss: 0.6766 score: 0.7045 time: 0.05s
Test loss: 0.6795 score: 0.6512 time: 0.05s
Epoch 116/1000, LR 0.000263
Train loss: 1.0303;  Loss pred: 1.0303; Loss self: 0.0000; time: 0.16s
Val loss: 0.6761 score: 0.7273 time: 0.05s
Test loss: 0.6790 score: 0.6744 time: 0.05s
Epoch 117/1000, LR 0.000262
Train loss: 1.0257;  Loss pred: 1.0257; Loss self: 0.0000; time: 0.15s
Val loss: 0.6755 score: 0.7273 time: 0.05s
Test loss: 0.6786 score: 0.6744 time: 0.05s
Epoch 118/1000, LR 0.000262
Train loss: 1.0259;  Loss pred: 1.0259; Loss self: 0.0000; time: 0.16s
Val loss: 0.6749 score: 0.7273 time: 0.05s
Test loss: 0.6781 score: 0.6744 time: 0.05s
Epoch 119/1000, LR 0.000262
Train loss: 1.0259;  Loss pred: 1.0259; Loss self: 0.0000; time: 0.15s
Val loss: 0.6743 score: 0.7500 time: 0.05s
Test loss: 0.6776 score: 0.6744 time: 0.05s
Epoch 120/1000, LR 0.000262
Train loss: 1.0237;  Loss pred: 1.0237; Loss self: 0.0000; time: 0.15s
Val loss: 0.6736 score: 0.7500 time: 0.05s
Test loss: 0.6771 score: 0.6744 time: 0.05s
Epoch 121/1000, LR 0.000262
Train loss: 1.0195;  Loss pred: 1.0195; Loss self: 0.0000; time: 0.15s
Val loss: 0.6730 score: 0.7500 time: 0.05s
Test loss: 0.6765 score: 0.6744 time: 0.05s
Epoch 122/1000, LR 0.000262
Train loss: 1.0190;  Loss pred: 1.0190; Loss self: 0.0000; time: 0.16s
Val loss: 0.6723 score: 0.7500 time: 0.05s
Test loss: 0.6760 score: 0.6744 time: 0.05s
Epoch 123/1000, LR 0.000262
Train loss: 1.0139;  Loss pred: 1.0139; Loss self: 0.0000; time: 0.15s
Val loss: 0.6716 score: 0.7727 time: 0.05s
Test loss: 0.6754 score: 0.6744 time: 0.05s
Epoch 124/1000, LR 0.000261
Train loss: 1.0110;  Loss pred: 1.0110; Loss self: 0.0000; time: 0.15s
Val loss: 0.6709 score: 0.7727 time: 0.05s
Test loss: 0.6748 score: 0.6744 time: 0.05s
Epoch 125/1000, LR 0.000261
Train loss: 1.0148;  Loss pred: 1.0148; Loss self: 0.0000; time: 0.15s
Val loss: 0.6702 score: 0.7727 time: 0.05s
Test loss: 0.6743 score: 0.6744 time: 0.05s
Epoch 126/1000, LR 0.000261
Train loss: 1.0121;  Loss pred: 1.0121; Loss self: 0.0000; time: 0.15s
Val loss: 0.6694 score: 0.7727 time: 0.05s
Test loss: 0.6736 score: 0.6744 time: 0.05s
Epoch 127/1000, LR 0.000261
Train loss: 1.0080;  Loss pred: 1.0080; Loss self: 0.0000; time: 1.07s
Val loss: 0.6686 score: 0.7727 time: 0.38s
Test loss: 0.6730 score: 0.6744 time: 0.46s
Epoch 128/1000, LR 0.000261
Train loss: 1.0078;  Loss pred: 1.0078; Loss self: 0.0000; time: 1.45s
Val loss: 0.6678 score: 0.7727 time: 0.65s
Test loss: 0.6723 score: 0.6744 time: 1.35s
Epoch 129/1000, LR 0.000261
Train loss: 1.0033;  Loss pred: 1.0033; Loss self: 0.0000; time: 0.90s
Val loss: 0.6669 score: 0.7955 time: 0.08s
Test loss: 0.6716 score: 0.6977 time: 0.14s
Epoch 130/1000, LR 0.000260
Train loss: 1.0052;  Loss pred: 1.0052; Loss self: 0.0000; time: 0.34s
Val loss: 0.6660 score: 0.7955 time: 0.08s
Test loss: 0.6708 score: 0.6977 time: 0.11s
Epoch 131/1000, LR 0.000260
Train loss: 1.0009;  Loss pred: 1.0009; Loss self: 0.0000; time: 0.63s
Val loss: 0.6651 score: 0.8182 time: 0.10s
Test loss: 0.6701 score: 0.6977 time: 0.09s
Epoch 132/1000, LR 0.000260
Train loss: 0.9978;  Loss pred: 0.9978; Loss self: 0.0000; time: 0.31s
Val loss: 0.6642 score: 0.8182 time: 0.05s
Test loss: 0.6694 score: 0.6977 time: 0.05s
Epoch 133/1000, LR 0.000260
Train loss: 0.9974;  Loss pred: 0.9974; Loss self: 0.0000; time: 0.16s
Val loss: 0.6632 score: 0.8182 time: 0.04s
Test loss: 0.6686 score: 0.7209 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 0.9961;  Loss pred: 0.9961; Loss self: 0.0000; time: 0.14s
Val loss: 0.6623 score: 0.8182 time: 0.04s
Test loss: 0.6678 score: 0.7209 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9963;  Loss pred: 0.9963; Loss self: 0.0000; time: 0.14s
Val loss: 0.6613 score: 0.8182 time: 0.04s
Test loss: 0.6670 score: 0.7209 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9895;  Loss pred: 0.9895; Loss self: 0.0000; time: 0.14s
Val loss: 0.6602 score: 0.8182 time: 0.04s
Test loss: 0.6661 score: 0.7209 time: 0.05s
Epoch 137/1000, LR 0.000259
Train loss: 0.9925;  Loss pred: 0.9925; Loss self: 0.0000; time: 0.15s
Val loss: 0.6592 score: 0.8182 time: 0.05s
Test loss: 0.6653 score: 0.7209 time: 0.05s
Epoch 138/1000, LR 0.000259
Train loss: 0.9882;  Loss pred: 0.9882; Loss self: 0.0000; time: 0.15s
Val loss: 0.6581 score: 0.8182 time: 0.05s
Test loss: 0.6644 score: 0.7209 time: 0.05s
Epoch 139/1000, LR 0.000259
Train loss: 0.9830;  Loss pred: 0.9830; Loss self: 0.0000; time: 0.16s
Val loss: 0.6571 score: 0.8182 time: 0.05s
Test loss: 0.6635 score: 0.7209 time: 0.05s
Epoch 140/1000, LR 0.000259
Train loss: 0.9863;  Loss pred: 0.9863; Loss self: 0.0000; time: 0.16s
Val loss: 0.6560 score: 0.8182 time: 0.05s
Test loss: 0.6626 score: 0.7209 time: 0.05s
Epoch 141/1000, LR 0.000259
Train loss: 0.9801;  Loss pred: 0.9801; Loss self: 0.0000; time: 0.15s
Val loss: 0.6548 score: 0.8182 time: 0.05s
Test loss: 0.6617 score: 0.7209 time: 0.05s
Epoch 142/1000, LR 0.000259
Train loss: 0.9784;  Loss pred: 0.9784; Loss self: 0.0000; time: 0.16s
Val loss: 0.6537 score: 0.8182 time: 0.05s
Test loss: 0.6607 score: 0.7209 time: 0.05s
Epoch 143/1000, LR 0.000258
Train loss: 0.9781;  Loss pred: 0.9781; Loss self: 0.0000; time: 0.15s
Val loss: 0.6525 score: 0.8182 time: 0.04s
Test loss: 0.6597 score: 0.7209 time: 0.05s
Epoch 144/1000, LR 0.000258
Train loss: 0.9735;  Loss pred: 0.9735; Loss self: 0.0000; time: 0.15s
Val loss: 0.6513 score: 0.8182 time: 0.04s
Test loss: 0.6587 score: 0.7442 time: 0.05s
Epoch 145/1000, LR 0.000258
Train loss: 0.9757;  Loss pred: 0.9757; Loss self: 0.0000; time: 0.15s
Val loss: 0.6501 score: 0.8182 time: 0.04s
Test loss: 0.6577 score: 0.7674 time: 0.05s
Epoch 146/1000, LR 0.000258
Train loss: 0.9706;  Loss pred: 0.9706; Loss self: 0.0000; time: 0.15s
Val loss: 0.6488 score: 0.8182 time: 0.05s
Test loss: 0.6566 score: 0.7674 time: 0.05s
Epoch 147/1000, LR 0.000258
Train loss: 0.9704;  Loss pred: 0.9704; Loss self: 0.0000; time: 0.16s
Val loss: 0.6475 score: 0.8182 time: 0.05s
Test loss: 0.6555 score: 0.7674 time: 0.05s
Epoch 148/1000, LR 0.000257
Train loss: 0.9644;  Loss pred: 0.9644; Loss self: 0.0000; time: 0.16s
Val loss: 0.6462 score: 0.8182 time: 0.04s
Test loss: 0.6544 score: 0.7907 time: 0.04s
Epoch 149/1000, LR 0.000257
Train loss: 0.9668;  Loss pred: 0.9668; Loss self: 0.0000; time: 0.15s
Val loss: 0.6448 score: 0.8182 time: 0.04s
Test loss: 0.6533 score: 0.7907 time: 0.05s
Epoch 150/1000, LR 0.000257
Train loss: 0.9646;  Loss pred: 0.9646; Loss self: 0.0000; time: 0.15s
Val loss: 0.6434 score: 0.8409 time: 0.05s
Test loss: 0.6521 score: 0.7907 time: 0.05s
Epoch 151/1000, LR 0.000257
Train loss: 0.9607;  Loss pred: 0.9607; Loss self: 0.0000; time: 0.15s
Val loss: 0.6420 score: 0.8409 time: 0.05s
Test loss: 0.6510 score: 0.7907 time: 0.05s
Epoch 152/1000, LR 0.000257
Train loss: 0.9582;  Loss pred: 0.9582; Loss self: 0.0000; time: 0.15s
Val loss: 0.6406 score: 0.8409 time: 0.05s
Test loss: 0.6498 score: 0.7907 time: 0.05s
Epoch 153/1000, LR 0.000257
Train loss: 0.9560;  Loss pred: 0.9560; Loss self: 0.0000; time: 0.15s
Val loss: 0.6391 score: 0.8409 time: 0.05s
Test loss: 0.6486 score: 0.7907 time: 0.05s
Epoch 154/1000, LR 0.000256
Train loss: 0.9540;  Loss pred: 0.9540; Loss self: 0.0000; time: 0.15s
Val loss: 0.6377 score: 0.8409 time: 0.05s
Test loss: 0.6474 score: 0.7907 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9517;  Loss pred: 0.9517; Loss self: 0.0000; time: 0.16s
Val loss: 0.6362 score: 0.8409 time: 0.05s
Test loss: 0.6461 score: 0.7907 time: 0.97s
Epoch 156/1000, LR 0.000256
Train loss: 0.9516;  Loss pred: 0.9516; Loss self: 0.0000; time: 3.86s
Val loss: 0.6346 score: 0.8409 time: 0.14s
Test loss: 0.6448 score: 0.7907 time: 0.06s
Epoch 157/1000, LR 0.000256
Train loss: 0.9479;  Loss pred: 0.9479; Loss self: 0.0000; time: 0.25s
Val loss: 0.6330 score: 0.8409 time: 0.05s
Test loss: 0.6435 score: 0.7907 time: 0.05s
Epoch 158/1000, LR 0.000256
Train loss: 0.9453;  Loss pred: 0.9453; Loss self: 0.0000; time: 0.16s
Val loss: 0.6314 score: 0.8409 time: 0.05s
Test loss: 0.6421 score: 0.7907 time: 0.05s
Epoch 159/1000, LR 0.000255
Train loss: 0.9438;  Loss pred: 0.9438; Loss self: 0.0000; time: 0.16s
Val loss: 0.6298 score: 0.8409 time: 0.05s
Test loss: 0.6407 score: 0.7907 time: 0.05s
Epoch 160/1000, LR 0.000255
Train loss: 0.9418;  Loss pred: 0.9418; Loss self: 0.0000; time: 0.16s
Val loss: 0.6281 score: 0.8409 time: 0.05s
Test loss: 0.6393 score: 0.7907 time: 0.05s
Epoch 161/1000, LR 0.000255
Train loss: 0.9408;  Loss pred: 0.9408; Loss self: 0.0000; time: 0.16s
Val loss: 0.6264 score: 0.8409 time: 0.05s
Test loss: 0.6379 score: 0.7907 time: 0.05s
Epoch 162/1000, LR 0.000255
Train loss: 0.9372;  Loss pred: 0.9372; Loss self: 0.0000; time: 0.16s
Val loss: 0.6247 score: 0.8409 time: 0.05s
Test loss: 0.6365 score: 0.7907 time: 0.05s
Epoch 163/1000, LR 0.000255
Train loss: 0.9345;  Loss pred: 0.9345; Loss self: 0.0000; time: 0.16s
Val loss: 0.6230 score: 0.8409 time: 0.05s
Test loss: 0.6350 score: 0.7907 time: 0.05s
Epoch 164/1000, LR 0.000254
Train loss: 0.9344;  Loss pred: 0.9344; Loss self: 0.0000; time: 0.16s
Val loss: 0.6212 score: 0.8409 time: 0.05s
Test loss: 0.6335 score: 0.7907 time: 0.05s
Epoch 165/1000, LR 0.000254
Train loss: 0.9304;  Loss pred: 0.9304; Loss self: 0.0000; time: 0.16s
Val loss: 0.6194 score: 0.8409 time: 0.05s
Test loss: 0.6320 score: 0.7907 time: 0.05s
Epoch 166/1000, LR 0.000254
Train loss: 0.9296;  Loss pred: 0.9296; Loss self: 0.0000; time: 0.16s
Val loss: 0.6175 score: 0.8636 time: 0.05s
Test loss: 0.6304 score: 0.7907 time: 0.05s
Epoch 167/1000, LR 0.000254
Train loss: 0.9254;  Loss pred: 0.9254; Loss self: 0.0000; time: 0.16s
Val loss: 0.6157 score: 0.8636 time: 0.05s
Test loss: 0.6288 score: 0.7907 time: 0.05s
Epoch 168/1000, LR 0.000254
Train loss: 0.9236;  Loss pred: 0.9236; Loss self: 0.0000; time: 0.16s
Val loss: 0.6138 score: 0.8636 time: 0.05s
Test loss: 0.6271 score: 0.7907 time: 0.05s
Epoch 169/1000, LR 0.000253
Train loss: 0.9203;  Loss pred: 0.9203; Loss self: 0.0000; time: 0.16s
Val loss: 0.6118 score: 0.8636 time: 0.05s
Test loss: 0.6255 score: 0.7907 time: 0.05s
Epoch 170/1000, LR 0.000253
Train loss: 0.9202;  Loss pred: 0.9202; Loss self: 0.0000; time: 0.16s
Val loss: 0.6099 score: 0.8636 time: 0.05s
Test loss: 0.6238 score: 0.7907 time: 0.05s
Epoch 171/1000, LR 0.000253
Train loss: 0.9182;  Loss pred: 0.9182; Loss self: 0.0000; time: 0.16s
Val loss: 0.6080 score: 0.8636 time: 0.05s
Test loss: 0.6222 score: 0.7907 time: 0.05s
Epoch 172/1000, LR 0.000253
Train loss: 0.9124;  Loss pred: 0.9124; Loss self: 0.0000; time: 0.16s
Val loss: 0.6060 score: 0.8636 time: 0.05s
Test loss: 0.6205 score: 0.7907 time: 0.05s
Epoch 173/1000, LR 0.000253
Train loss: 0.9126;  Loss pred: 0.9126; Loss self: 0.0000; time: 0.16s
Val loss: 0.6040 score: 0.8636 time: 0.05s
Test loss: 0.6188 score: 0.7907 time: 0.05s
Epoch 174/1000, LR 0.000252
Train loss: 0.9077;  Loss pred: 0.9077; Loss self: 0.0000; time: 0.16s
Val loss: 0.6020 score: 0.8636 time: 0.05s
Test loss: 0.6170 score: 0.7907 time: 0.05s
Epoch 175/1000, LR 0.000252
Train loss: 0.9062;  Loss pred: 0.9062; Loss self: 0.0000; time: 0.16s
Val loss: 0.5999 score: 0.8636 time: 0.05s
Test loss: 0.6153 score: 0.7907 time: 0.05s
Epoch 176/1000, LR 0.000252
Train loss: 0.9046;  Loss pred: 0.9046; Loss self: 0.0000; time: 0.16s
Val loss: 0.5978 score: 0.8636 time: 0.05s
Test loss: 0.6134 score: 0.7907 time: 0.05s
Epoch 177/1000, LR 0.000252
Train loss: 0.9020;  Loss pred: 0.9020; Loss self: 0.0000; time: 0.16s
Val loss: 0.5957 score: 0.8636 time: 0.05s
Test loss: 0.6116 score: 0.7907 time: 0.05s
Epoch 178/1000, LR 0.000251
Train loss: 0.8992;  Loss pred: 0.8992; Loss self: 0.0000; time: 0.16s
Val loss: 0.5935 score: 0.8636 time: 0.05s
Test loss: 0.6097 score: 0.7907 time: 0.05s
Epoch 179/1000, LR 0.000251
Train loss: 0.8971;  Loss pred: 0.8971; Loss self: 0.0000; time: 0.16s
Val loss: 0.5914 score: 0.8636 time: 0.05s
Test loss: 0.6079 score: 0.7907 time: 0.05s
Epoch 180/1000, LR 0.000251
Train loss: 0.8951;  Loss pred: 0.8951; Loss self: 0.0000; time: 0.15s
Val loss: 0.5892 score: 0.8636 time: 0.05s
Test loss: 0.6060 score: 0.7907 time: 0.05s
Epoch 181/1000, LR 0.000251
Train loss: 0.8922;  Loss pred: 0.8922; Loss self: 0.0000; time: 0.15s
Val loss: 0.5871 score: 0.8636 time: 0.05s
Test loss: 0.6041 score: 0.7907 time: 0.05s
Epoch 182/1000, LR 0.000251
Train loss: 0.8885;  Loss pred: 0.8885; Loss self: 0.0000; time: 0.16s
Val loss: 0.5849 score: 0.8636 time: 0.05s
Test loss: 0.6023 score: 0.7907 time: 0.05s
Epoch 183/1000, LR 0.000250
Train loss: 0.8862;  Loss pred: 0.8862; Loss self: 0.0000; time: 0.15s
Val loss: 0.5827 score: 0.8636 time: 0.05s
Test loss: 0.6004 score: 0.7907 time: 0.05s
Epoch 184/1000, LR 0.000250
Train loss: 0.8855;  Loss pred: 0.8855; Loss self: 0.0000; time: 0.15s
Val loss: 0.5805 score: 0.8636 time: 0.05s
Test loss: 0.5985 score: 0.7907 time: 0.05s
Epoch 185/1000, LR 0.000250
Train loss: 0.8816;  Loss pred: 0.8816; Loss self: 0.0000; time: 0.15s
Val loss: 0.5783 score: 0.8636 time: 0.05s
Test loss: 0.5966 score: 0.7907 time: 0.05s
Epoch 186/1000, LR 0.000250
Train loss: 0.8797;  Loss pred: 0.8797; Loss self: 0.0000; time: 0.16s
Val loss: 0.5761 score: 0.8636 time: 0.05s
Test loss: 0.5947 score: 0.7907 time: 0.05s
Epoch 187/1000, LR 0.000249
Train loss: 0.8763;  Loss pred: 0.8763; Loss self: 0.0000; time: 0.16s
Val loss: 0.5738 score: 0.8636 time: 0.05s
Test loss: 0.5926 score: 0.7907 time: 0.05s
Epoch 188/1000, LR 0.000249
Train loss: 0.8729;  Loss pred: 0.8729; Loss self: 0.0000; time: 0.17s
Val loss: 0.5715 score: 0.8636 time: 0.10s
Test loss: 0.5906 score: 0.7907 time: 0.05s
Epoch 189/1000, LR 0.000249
Train loss: 0.8702;  Loss pred: 0.8702; Loss self: 0.0000; time: 0.15s
Val loss: 0.5692 score: 0.8636 time: 0.05s
Test loss: 0.5886 score: 0.7907 time: 0.05s
Epoch 190/1000, LR 0.000249
Train loss: 0.8661;  Loss pred: 0.8661; Loss self: 0.0000; time: 0.15s
Val loss: 0.5669 score: 0.8636 time: 0.05s
Test loss: 0.5866 score: 0.7907 time: 0.05s
Epoch 191/1000, LR 0.000249
Train loss: 0.8645;  Loss pred: 0.8645; Loss self: 0.0000; time: 0.15s
Val loss: 0.5645 score: 0.8636 time: 0.05s
Test loss: 0.5845 score: 0.7907 time: 0.05s
Epoch 192/1000, LR 0.000248
Train loss: 0.8636;  Loss pred: 0.8636; Loss self: 0.0000; time: 0.16s
Val loss: 0.5622 score: 0.8636 time: 0.05s
Test loss: 0.5824 score: 0.8140 time: 0.05s
Epoch 193/1000, LR 0.000248
Train loss: 0.8607;  Loss pred: 0.8607; Loss self: 0.0000; time: 0.17s
Val loss: 0.5598 score: 0.8636 time: 0.05s
Test loss: 0.5803 score: 0.8140 time: 0.05s
Epoch 194/1000, LR 0.000248
Train loss: 0.8577;  Loss pred: 0.8577; Loss self: 0.0000; time: 0.16s
Val loss: 0.5574 score: 0.8636 time: 0.05s
Test loss: 0.5782 score: 0.8140 time: 0.05s
Epoch 195/1000, LR 0.000248
Train loss: 0.8569;  Loss pred: 0.8569; Loss self: 0.0000; time: 0.18s
Val loss: 0.5550 score: 0.8636 time: 0.10s
Test loss: 0.5760 score: 0.8140 time: 0.05s
Epoch 196/1000, LR 0.000247
Train loss: 0.8542;  Loss pred: 0.8542; Loss self: 0.0000; time: 0.15s
Val loss: 0.5526 score: 0.8636 time: 0.05s
Test loss: 0.5738 score: 0.8140 time: 0.05s
Epoch 197/1000, LR 0.000247
Train loss: 0.8513;  Loss pred: 0.8513; Loss self: 0.0000; time: 2.16s
Val loss: 0.5502 score: 0.8636 time: 0.05s
Test loss: 0.5717 score: 0.8140 time: 0.05s
Epoch 198/1000, LR 0.000247
Train loss: 0.8466;  Loss pred: 0.8466; Loss self: 0.0000; time: 0.16s
Val loss: 0.5479 score: 0.8636 time: 0.05s
Test loss: 0.5696 score: 0.8140 time: 0.05s
Epoch 199/1000, LR 0.000247
Train loss: 0.8433;  Loss pred: 0.8433; Loss self: 0.0000; time: 0.15s
Val loss: 0.5455 score: 0.8636 time: 0.05s
Test loss: 0.5675 score: 0.8140 time: 0.05s
Epoch 200/1000, LR 0.000246
Train loss: 0.8395;  Loss pred: 0.8395; Loss self: 0.0000; time: 0.15s
Val loss: 0.5431 score: 0.8636 time: 0.05s
Test loss: 0.5654 score: 0.8140 time: 0.05s
Epoch 201/1000, LR 0.000246
Train loss: 0.8384;  Loss pred: 0.8384; Loss self: 0.0000; time: 0.37s
Val loss: 0.5407 score: 0.8636 time: 2.51s
Test loss: 0.5633 score: 0.8140 time: 2.60s
Epoch 202/1000, LR 0.000246
Train loss: 0.8345;  Loss pred: 0.8345; Loss self: 0.0000; time: 8.04s
Val loss: 0.5384 score: 0.8636 time: 2.10s
Test loss: 0.5611 score: 0.8140 time: 0.92s
Epoch 203/1000, LR 0.000246
Train loss: 0.8326;  Loss pred: 0.8326; Loss self: 0.0000; time: 2.00s
Val loss: 0.5359 score: 0.8636 time: 0.21s
Test loss: 0.5589 score: 0.8140 time: 0.24s
Epoch 204/1000, LR 0.000245
Train loss: 0.8314;  Loss pred: 0.8314; Loss self: 0.0000; time: 0.55s
Val loss: 0.5335 score: 0.8636 time: 0.20s
Test loss: 0.5567 score: 0.8140 time: 0.23s
Epoch 205/1000, LR 0.000245
Train loss: 0.8279;  Loss pred: 0.8279; Loss self: 0.0000; time: 0.74s
Val loss: 0.5311 score: 0.8636 time: 0.23s
Test loss: 0.5545 score: 0.8140 time: 0.24s
Epoch 206/1000, LR 0.000245
Train loss: 0.8255;  Loss pred: 0.8255; Loss self: 0.0000; time: 0.55s
Val loss: 0.5287 score: 0.8636 time: 0.45s
Test loss: 0.5523 score: 0.8140 time: 0.32s
Epoch 207/1000, LR 0.000245
Train loss: 0.8224;  Loss pred: 0.8224; Loss self: 0.0000; time: 3.99s
Val loss: 0.5263 score: 0.8636 time: 1.25s
Test loss: 0.5501 score: 0.8372 time: 2.59s
Epoch 208/1000, LR 0.000244
Train loss: 0.8202;  Loss pred: 0.8202; Loss self: 0.0000; time: 6.75s
Val loss: 0.5239 score: 0.8636 time: 0.24s
Test loss: 0.5479 score: 0.8372 time: 0.24s
Epoch 209/1000, LR 0.000244
Train loss: 0.8176;  Loss pred: 0.8176; Loss self: 0.0000; time: 0.36s
Val loss: 0.5215 score: 0.8636 time: 0.13s
Test loss: 0.5457 score: 0.8372 time: 0.08s
Epoch 210/1000, LR 0.000244
Train loss: 0.8144;  Loss pred: 0.8144; Loss self: 0.0000; time: 0.23s
Val loss: 0.5192 score: 0.8636 time: 0.20s
Test loss: 0.5436 score: 0.8372 time: 0.24s
Epoch 211/1000, LR 0.000244
Train loss: 0.8098;  Loss pred: 0.8098; Loss self: 0.0000; time: 0.55s
Val loss: 0.5169 score: 0.8636 time: 0.23s
Test loss: 0.5415 score: 0.8372 time: 0.05s
Epoch 212/1000, LR 0.000243
Train loss: 0.8071;  Loss pred: 0.8071; Loss self: 0.0000; time: 6.35s
Val loss: 0.5146 score: 0.8636 time: 0.26s
Test loss: 0.5394 score: 0.8372 time: 0.24s
Epoch 213/1000, LR 0.000243
Train loss: 0.8040;  Loss pred: 0.8040; Loss self: 0.0000; time: 0.55s
Val loss: 0.5123 score: 0.8636 time: 0.21s
Test loss: 0.5374 score: 0.8372 time: 0.24s
Epoch 214/1000, LR 0.000243
Train loss: 0.8044;  Loss pred: 0.8044; Loss self: 0.0000; time: 0.73s
Val loss: 0.5101 score: 0.8636 time: 0.08s
Test loss: 0.5354 score: 0.8372 time: 0.14s
Epoch 215/1000, LR 0.000243
Train loss: 0.8010;  Loss pred: 0.8010; Loss self: 0.0000; time: 0.16s
Val loss: 0.5079 score: 0.8636 time: 0.05s
Test loss: 0.5334 score: 0.8372 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.7978;  Loss pred: 0.7978; Loss self: 0.0000; time: 0.16s
Val loss: 0.5056 score: 0.8636 time: 0.05s
Test loss: 0.5313 score: 0.8372 time: 0.05s
Epoch 217/1000, LR 0.000242
Train loss: 0.7937;  Loss pred: 0.7937; Loss self: 0.0000; time: 0.15s
Val loss: 0.5034 score: 0.8636 time: 0.05s
Test loss: 0.5292 score: 0.8372 time: 0.05s
Epoch 218/1000, LR 0.000242
Train loss: 0.7923;  Loss pred: 0.7923; Loss self: 0.0000; time: 0.16s
Val loss: 0.5011 score: 0.8636 time: 0.05s
Test loss: 0.5271 score: 0.8372 time: 0.05s
Epoch 219/1000, LR 0.000242
Train loss: 0.7896;  Loss pred: 0.7896; Loss self: 0.0000; time: 0.23s
Val loss: 0.4987 score: 0.8636 time: 0.05s
Test loss: 0.5248 score: 0.8372 time: 0.07s
Epoch 220/1000, LR 0.000241
Train loss: 0.7856;  Loss pred: 0.7856; Loss self: 0.0000; time: 0.17s
Val loss: 0.4963 score: 0.8636 time: 0.05s
Test loss: 0.5225 score: 0.8372 time: 0.05s
Epoch 221/1000, LR 0.000241
Train loss: 0.7832;  Loss pred: 0.7832; Loss self: 0.0000; time: 0.17s
Val loss: 0.4940 score: 0.8636 time: 0.05s
Test loss: 0.5202 score: 0.8372 time: 0.05s
Epoch 222/1000, LR 0.000241
Train loss: 0.7797;  Loss pred: 0.7797; Loss self: 0.0000; time: 0.18s
Val loss: 0.4916 score: 0.8864 time: 0.11s
Test loss: 0.5178 score: 0.8372 time: 0.05s
Epoch 223/1000, LR 0.000241
Train loss: 0.7760;  Loss pred: 0.7760; Loss self: 0.0000; time: 0.30s
Val loss: 0.4892 score: 0.8864 time: 0.08s
Test loss: 0.5155 score: 0.8372 time: 0.05s
Epoch 224/1000, LR 0.000240
Train loss: 0.7765;  Loss pred: 0.7765; Loss self: 0.0000; time: 0.36s
Val loss: 0.4869 score: 0.8864 time: 0.17s
Test loss: 0.5132 score: 0.8372 time: 0.06s
Epoch 225/1000, LR 0.000240
Train loss: 0.7710;  Loss pred: 0.7710; Loss self: 0.0000; time: 0.18s
Val loss: 0.4847 score: 0.8864 time: 0.15s
Test loss: 0.5111 score: 0.8372 time: 0.05s
Epoch 226/1000, LR 0.000240
Train loss: 0.7696;  Loss pred: 0.7696; Loss self: 0.0000; time: 0.17s
Val loss: 0.4826 score: 0.8864 time: 0.05s
Test loss: 0.5090 score: 0.8372 time: 0.05s
Epoch 227/1000, LR 0.000240
Train loss: 0.7682;  Loss pred: 0.7682; Loss self: 0.0000; time: 0.16s
Val loss: 0.4805 score: 0.8864 time: 0.05s
Test loss: 0.5071 score: 0.8372 time: 0.05s
Epoch 228/1000, LR 0.000239
Train loss: 0.7653;  Loss pred: 0.7653; Loss self: 0.0000; time: 0.26s
Val loss: 0.4785 score: 0.8864 time: 0.05s
Test loss: 0.5052 score: 0.8372 time: 0.05s
Epoch 229/1000, LR 0.000239
Train loss: 0.7647;  Loss pred: 0.7647; Loss self: 0.0000; time: 0.24s
Val loss: 0.4765 score: 0.8864 time: 0.05s
Test loss: 0.5032 score: 0.8372 time: 0.05s
Epoch 230/1000, LR 0.000239
Train loss: 0.7596;  Loss pred: 0.7596; Loss self: 0.0000; time: 0.16s
Val loss: 0.4745 score: 0.8864 time: 0.07s
Test loss: 0.5013 score: 0.8372 time: 0.05s
Epoch 231/1000, LR 0.000238
Train loss: 0.7581;  Loss pred: 0.7581; Loss self: 0.0000; time: 0.26s
Val loss: 0.4725 score: 0.8864 time: 0.05s
Test loss: 0.4993 score: 0.8372 time: 0.09s
Epoch 232/1000, LR 0.000238
Train loss: 0.7540;  Loss pred: 0.7540; Loss self: 0.0000; time: 0.23s
Val loss: 0.4705 score: 0.8864 time: 0.06s
Test loss: 0.4973 score: 0.8372 time: 0.08s
Epoch 233/1000, LR 0.000238
Train loss: 0.7526;  Loss pred: 0.7526; Loss self: 0.0000; time: 0.53s
Val loss: 0.4684 score: 0.8864 time: 0.21s
Test loss: 0.4953 score: 0.8372 time: 0.08s
Epoch 234/1000, LR 0.000238
Train loss: 0.7502;  Loss pred: 0.7502; Loss self: 0.0000; time: 0.66s
Val loss: 0.4664 score: 0.8864 time: 0.15s
Test loss: 0.4932 score: 0.8372 time: 0.05s
Epoch 235/1000, LR 0.000237
Train loss: 0.7478;  Loss pred: 0.7478; Loss self: 0.0000; time: 0.16s
Val loss: 0.4643 score: 0.8864 time: 0.06s
Test loss: 0.4911 score: 0.8372 time: 0.05s
Epoch 236/1000, LR 0.000237
Train loss: 0.7451;  Loss pred: 0.7451; Loss self: 0.0000; time: 0.30s
Val loss: 0.4624 score: 0.8864 time: 0.05s
Test loss: 0.4892 score: 0.8372 time: 0.05s
Epoch 237/1000, LR 0.000237
Train loss: 0.7435;  Loss pred: 0.7435; Loss self: 0.0000; time: 0.16s
Val loss: 0.4605 score: 0.8864 time: 0.06s
Test loss: 0.4873 score: 0.8372 time: 0.16s
Epoch 238/1000, LR 0.000236
Train loss: 0.7400;  Loss pred: 0.7400; Loss self: 0.0000; time: 0.16s
Val loss: 0.4587 score: 0.8864 time: 0.10s
Test loss: 0.4854 score: 0.8372 time: 0.05s
Epoch 239/1000, LR 0.000236
Train loss: 0.7395;  Loss pred: 0.7395; Loss self: 0.0000; time: 0.17s
Val loss: 0.4569 score: 0.8864 time: 0.06s
Test loss: 0.4836 score: 0.8372 time: 0.12s
Epoch 240/1000, LR 0.000236
Train loss: 0.7334;  Loss pred: 0.7334; Loss self: 0.0000; time: 0.31s
Val loss: 0.4551 score: 0.8864 time: 0.06s
Test loss: 0.4817 score: 0.8372 time: 0.21s
Epoch 241/1000, LR 0.000236
Train loss: 0.7350;  Loss pred: 0.7350; Loss self: 0.0000; time: 0.20s
Val loss: 0.4533 score: 0.8864 time: 0.06s
Test loss: 0.4799 score: 0.8372 time: 0.25s
Epoch 242/1000, LR 0.000235
Train loss: 0.7301;  Loss pred: 0.7301; Loss self: 0.0000; time: 0.55s
Val loss: 0.4516 score: 0.8864 time: 0.06s
Test loss: 0.4782 score: 0.8372 time: 0.07s
Epoch 243/1000, LR 0.000235
Train loss: 0.7270;  Loss pred: 0.7270; Loss self: 0.0000; time: 0.17s
Val loss: 0.4500 score: 0.8864 time: 0.06s
Test loss: 0.4765 score: 0.8372 time: 0.06s
Epoch 244/1000, LR 0.000235
Train loss: 0.7251;  Loss pred: 0.7251; Loss self: 0.0000; time: 0.51s
Val loss: 0.4483 score: 0.8864 time: 0.05s
Test loss: 0.4748 score: 0.8372 time: 0.09s
Epoch 245/1000, LR 0.000234
Train loss: 0.7259;  Loss pred: 0.7259; Loss self: 0.0000; time: 0.17s
Val loss: 0.4467 score: 0.8864 time: 0.05s
Test loss: 0.4730 score: 0.8372 time: 0.05s
Epoch 246/1000, LR 0.000234
Train loss: 0.7202;  Loss pred: 0.7202; Loss self: 0.0000; time: 0.21s
Val loss: 0.4449 score: 0.8864 time: 0.21s
Test loss: 0.4711 score: 0.8372 time: 0.24s
Epoch 247/1000, LR 0.000234
Train loss: 0.7197;  Loss pred: 0.7197; Loss self: 0.0000; time: 0.18s
Val loss: 0.4432 score: 0.8864 time: 0.05s
Test loss: 0.4692 score: 0.8372 time: 0.05s
Epoch 248/1000, LR 0.000234
Train loss: 0.7177;  Loss pred: 0.7177; Loss self: 0.0000; time: 0.55s
Val loss: 0.4416 score: 0.8864 time: 0.19s
Test loss: 0.4674 score: 0.8372 time: 0.24s
Epoch 249/1000, LR 0.000233
Train loss: 0.7149;  Loss pred: 0.7149; Loss self: 0.0000; time: 0.54s
Val loss: 0.4399 score: 0.8864 time: 0.10s
Test loss: 0.4655 score: 0.8372 time: 0.11s
Epoch 250/1000, LR 0.000233
Train loss: 0.7137;  Loss pred: 0.7137; Loss self: 0.0000; time: 0.54s
Val loss: 0.4383 score: 0.8864 time: 0.22s
Test loss: 0.4637 score: 0.8372 time: 0.25s
Epoch 251/1000, LR 0.000233
Train loss: 0.7082;  Loss pred: 0.7082; Loss self: 0.0000; time: 2.95s
Val loss: 0.4367 score: 0.8864 time: 2.82s
Test loss: 0.4619 score: 0.8372 time: 2.23s
Epoch 252/1000, LR 0.000232
Train loss: 0.7088;  Loss pred: 0.7088; Loss self: 0.0000; time: 8.37s
Val loss: 0.4352 score: 0.8864 time: 2.96s
Test loss: 0.4603 score: 0.8372 time: 2.82s
Epoch 253/1000, LR 0.000232
Train loss: 0.7080;  Loss pred: 0.7080; Loss self: 0.0000; time: 4.85s
Val loss: 0.4339 score: 0.8864 time: 0.07s
Test loss: 0.4588 score: 0.8372 time: 1.34s
Epoch 254/1000, LR 0.000232
Train loss: 0.7044;  Loss pred: 0.7044; Loss self: 0.0000; time: 1.07s
Val loss: 0.4326 score: 0.8864 time: 0.05s
Test loss: 0.4574 score: 0.8372 time: 0.05s
Epoch 255/1000, LR 0.000232
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.18s
Val loss: 0.4313 score: 0.8864 time: 0.05s
Test loss: 0.4561 score: 0.8372 time: 0.06s
Epoch 256/1000, LR 0.000231
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.19s
Val loss: 0.4301 score: 0.8864 time: 0.06s
Test loss: 0.4548 score: 0.8372 time: 0.06s
Epoch 257/1000, LR 0.000231
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.23s
Val loss: 0.4289 score: 0.8864 time: 0.17s
Test loss: 0.4535 score: 0.8372 time: 0.05s
Epoch 258/1000, LR 0.000231
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.36s
Val loss: 0.4278 score: 0.8864 time: 0.18s
Test loss: 0.4523 score: 0.8372 time: 0.05s
Epoch 259/1000, LR 0.000230
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.32s
Val loss: 0.4266 score: 0.8864 time: 0.18s
Test loss: 0.4509 score: 0.8372 time: 2.20s
Epoch 260/1000, LR 0.000230
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 5.89s
Val loss: 0.4254 score: 0.8864 time: 1.79s
Test loss: 0.4495 score: 0.8372 time: 1.46s
Epoch 261/1000, LR 0.000230
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 1.27s
Val loss: 0.4241 score: 0.8864 time: 0.22s
Test loss: 0.4480 score: 0.8372 time: 0.23s
Epoch 262/1000, LR 0.000229
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.58s
Val loss: 0.4228 score: 0.8864 time: 0.17s
Test loss: 0.4465 score: 0.8372 time: 0.05s
Epoch 263/1000, LR 0.000229
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.16s
Val loss: 0.4215 score: 0.8864 time: 0.05s
Test loss: 0.4449 score: 0.8372 time: 0.05s
Epoch 264/1000, LR 0.000229
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.16s
Val loss: 0.4203 score: 0.8864 time: 0.06s
Test loss: 0.4434 score: 0.8372 time: 0.14s
Epoch 265/1000, LR 0.000228
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.55s
Val loss: 0.4192 score: 0.8864 time: 0.19s
Test loss: 0.4419 score: 0.8372 time: 0.14s
Epoch 266/1000, LR 0.000228
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.45s
Val loss: 0.4179 score: 0.8864 time: 3.16s
Test loss: 0.4403 score: 0.8372 time: 2.45s
Epoch 267/1000, LR 0.000228
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 5.94s
Val loss: 0.4166 score: 0.8864 time: 0.22s
Test loss: 0.4386 score: 0.8372 time: 0.23s
Epoch 268/1000, LR 0.000228
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.62s
Val loss: 0.4153 score: 0.8864 time: 0.09s
Test loss: 0.4369 score: 0.8372 time: 0.23s
Epoch 269/1000, LR 0.000227
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 0.17s
Val loss: 0.4140 score: 0.8864 time: 0.05s
Test loss: 0.4351 score: 0.8372 time: 0.05s
Epoch 270/1000, LR 0.000227
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.16s
Val loss: 0.4128 score: 0.8864 time: 0.06s
Test loss: 0.4337 score: 0.8372 time: 0.08s
Epoch 271/1000, LR 0.000227
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.16s
Val loss: 0.4118 score: 0.8864 time: 0.05s
Test loss: 0.4324 score: 0.8372 time: 0.08s
Epoch 272/1000, LR 0.000226
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.26s
Val loss: 0.4108 score: 0.8864 time: 0.05s
Test loss: 0.4310 score: 0.8372 time: 0.05s
Epoch 273/1000, LR 0.000226
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.17s
Val loss: 0.4099 score: 0.8864 time: 0.21s
Test loss: 0.4299 score: 0.8372 time: 0.08s
Epoch 274/1000, LR 0.000226
Train loss: 0.6686;  Loss pred: 0.6686; Loss self: 0.0000; time: 0.65s
Val loss: 0.4091 score: 0.8864 time: 0.10s
Test loss: 0.4288 score: 0.8372 time: 0.05s
Epoch 275/1000, LR 0.000225
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.24s
Val loss: 0.4083 score: 0.8864 time: 0.05s
Test loss: 0.4277 score: 0.8372 time: 0.09s
Epoch 276/1000, LR 0.000225
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.17s
Val loss: 0.4076 score: 0.8864 time: 0.05s
Test loss: 0.4269 score: 0.8372 time: 0.05s
Epoch 277/1000, LR 0.000225
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.17s
Val loss: 0.4070 score: 0.8864 time: 0.05s
Test loss: 0.4261 score: 0.8372 time: 0.05s
Epoch 278/1000, LR 0.000224
Train loss: 0.6605;  Loss pred: 0.6605; Loss self: 0.0000; time: 0.15s
Val loss: 0.4063 score: 0.8864 time: 0.05s
Test loss: 0.4254 score: 0.8372 time: 0.05s
Epoch 279/1000, LR 0.000224
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.17s
Val loss: 0.4057 score: 0.8864 time: 0.13s
Test loss: 0.4245 score: 0.8372 time: 0.24s
Epoch 280/1000, LR 0.000224
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.54s
Val loss: 0.4051 score: 0.8864 time: 0.23s
Test loss: 0.4239 score: 0.8372 time: 0.05s
Epoch 281/1000, LR 0.000223
Train loss: 0.6586;  Loss pred: 0.6586; Loss self: 0.0000; time: 2.09s
Val loss: 0.4046 score: 0.8864 time: 3.11s
Test loss: 0.4232 score: 0.8372 time: 3.10s
Epoch 282/1000, LR 0.000223
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 3.16s
Val loss: 0.4040 score: 0.8864 time: 0.05s
Test loss: 0.4224 score: 0.8372 time: 0.25s
Epoch 283/1000, LR 0.000223
Train loss: 0.6548;  Loss pred: 0.6548; Loss self: 0.0000; time: 0.55s
Val loss: 0.4033 score: 0.8864 time: 0.18s
Test loss: 0.4214 score: 0.8372 time: 0.24s
Epoch 284/1000, LR 0.000222
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.56s
Val loss: 0.4025 score: 0.8864 time: 0.19s
Test loss: 0.4203 score: 0.8372 time: 0.23s
Epoch 285/1000, LR 0.000222
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 0.57s
Val loss: 0.4018 score: 0.8864 time: 0.18s
Test loss: 0.4192 score: 0.8372 time: 0.24s
Epoch 286/1000, LR 0.000222
Train loss: 0.6528;  Loss pred: 0.6528; Loss self: 0.0000; time: 6.47s
Val loss: 0.4011 score: 0.8864 time: 1.84s
Test loss: 0.4183 score: 0.8372 time: 2.47s
Epoch 287/1000, LR 0.000221
Train loss: 0.6492;  Loss pred: 0.6492; Loss self: 0.0000; time: 6.30s
Val loss: 0.4004 score: 0.8864 time: 0.05s
Test loss: 0.4172 score: 0.8372 time: 0.24s
Epoch 288/1000, LR 0.000221
Train loss: 0.6494;  Loss pred: 0.6494; Loss self: 0.0000; time: 0.27s
Val loss: 0.3997 score: 0.8864 time: 0.11s
Test loss: 0.4162 score: 0.8372 time: 0.06s
Epoch 289/1000, LR 0.000221
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.22s
Val loss: 0.3991 score: 0.8864 time: 0.10s
Test loss: 0.4152 score: 0.8372 time: 0.06s
Epoch 290/1000, LR 0.000220
Train loss: 0.6465;  Loss pred: 0.6465; Loss self: 0.0000; time: 0.18s
Val loss: 0.3985 score: 0.8864 time: 0.05s
Test loss: 0.4143 score: 0.8372 time: 0.05s
Epoch 291/1000, LR 0.000220
Train loss: 0.6434;  Loss pred: 0.6434; Loss self: 0.0000; time: 0.18s
Val loss: 0.3979 score: 0.8864 time: 0.05s
Test loss: 0.4135 score: 0.8372 time: 0.06s
Epoch 292/1000, LR 0.000220
Train loss: 0.6448;  Loss pred: 0.6448; Loss self: 0.0000; time: 0.18s
Val loss: 0.3974 score: 0.8864 time: 0.05s
Test loss: 0.4127 score: 0.8372 time: 0.05s
Epoch 293/1000, LR 0.000219
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 8.54s
Val loss: 0.3968 score: 0.8864 time: 1.95s
Test loss: 0.4117 score: 0.8372 time: 2.33s
Epoch 294/1000, LR 0.000219
Train loss: 0.6411;  Loss pred: 0.6411; Loss self: 0.0000; time: 7.44s
Val loss: 0.3960 score: 0.8864 time: 0.04s
Test loss: 0.4104 score: 0.8372 time: 0.05s
Epoch 295/1000, LR 0.000219
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 0.15s
Val loss: 0.3952 score: 0.8864 time: 0.04s
Test loss: 0.4091 score: 0.8372 time: 0.05s
Epoch 296/1000, LR 0.000218
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 0.15s
Val loss: 0.3945 score: 0.8864 time: 0.04s
Test loss: 0.4079 score: 0.8372 time: 0.05s
Epoch 297/1000, LR 0.000218
Train loss: 0.6369;  Loss pred: 0.6369; Loss self: 0.0000; time: 0.15s
Val loss: 0.3939 score: 0.8864 time: 0.04s
Test loss: 0.4069 score: 0.8372 time: 0.04s
Epoch 298/1000, LR 0.000218
Train loss: 0.6348;  Loss pred: 0.6348; Loss self: 0.0000; time: 0.15s
Val loss: 0.3934 score: 0.8864 time: 0.04s
Test loss: 0.4060 score: 0.8372 time: 0.04s
Epoch 299/1000, LR 0.000217
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 0.15s
Val loss: 0.3930 score: 0.8864 time: 0.05s
Test loss: 0.4054 score: 0.8372 time: 0.05s
Epoch 300/1000, LR 0.000217
Train loss: 0.6329;  Loss pred: 0.6329; Loss self: 0.0000; time: 0.15s
Val loss: 0.3927 score: 0.8864 time: 0.04s
Test loss: 0.4049 score: 0.8372 time: 0.05s
Epoch 301/1000, LR 0.000217
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.15s
Val loss: 0.3925 score: 0.8864 time: 0.04s
Test loss: 0.4045 score: 0.8372 time: 0.05s
Epoch 302/1000, LR 0.000216
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.15s
Val loss: 0.3923 score: 0.8864 time: 0.04s
Test loss: 0.4041 score: 0.8372 time: 0.05s
Epoch 303/1000, LR 0.000216
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.15s
Val loss: 0.3921 score: 0.8864 time: 0.04s
Test loss: 0.4037 score: 0.8372 time: 0.05s
Epoch 304/1000, LR 0.000216
Train loss: 0.6280;  Loss pred: 0.6280; Loss self: 0.0000; time: 0.15s
Val loss: 0.3920 score: 0.8864 time: 0.05s
Test loss: 0.4034 score: 0.8372 time: 0.05s
Epoch 305/1000, LR 0.000215
Train loss: 0.6291;  Loss pred: 0.6291; Loss self: 0.0000; time: 0.15s
Val loss: 0.3919 score: 0.8864 time: 0.05s
Test loss: 0.4032 score: 0.8372 time: 0.05s
Epoch 306/1000, LR 0.000215
Train loss: 0.6253;  Loss pred: 0.6253; Loss self: 0.0000; time: 0.15s
Val loss: 0.3916 score: 0.8864 time: 0.04s
Test loss: 0.4026 score: 0.8372 time: 0.05s
Epoch 307/1000, LR 0.000215
Train loss: 0.6257;  Loss pred: 0.6257; Loss self: 0.0000; time: 0.15s
Val loss: 0.3911 score: 0.8864 time: 0.04s
Test loss: 0.4017 score: 0.8372 time: 0.05s
Epoch 308/1000, LR 0.000214
Train loss: 0.6240;  Loss pred: 0.6240; Loss self: 0.0000; time: 0.15s
Val loss: 0.3905 score: 0.8864 time: 0.05s
Test loss: 0.4005 score: 0.8372 time: 0.05s
Epoch 309/1000, LR 0.000214
Train loss: 0.6238;  Loss pred: 0.6238; Loss self: 0.0000; time: 0.15s
Val loss: 0.3900 score: 0.8864 time: 0.05s
Test loss: 0.3995 score: 0.8372 time: 0.05s
Epoch 310/1000, LR 0.000214
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.15s
Val loss: 0.3894 score: 0.8864 time: 0.05s
Test loss: 0.3985 score: 0.8372 time: 0.05s
Epoch 311/1000, LR 0.000213
Train loss: 0.6202;  Loss pred: 0.6202; Loss self: 0.0000; time: 0.15s
Val loss: 0.3889 score: 0.8864 time: 0.04s
Test loss: 0.3975 score: 0.8372 time: 0.05s
Epoch 312/1000, LR 0.000213
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.15s
Val loss: 0.3883 score: 0.8864 time: 0.22s
Test loss: 0.3964 score: 0.8372 time: 2.81s
Epoch 313/1000, LR 0.000213
Train loss: 0.6197;  Loss pred: 0.6197; Loss self: 0.0000; time: 7.67s
Val loss: 0.3878 score: 0.8864 time: 0.99s
Test loss: 0.3954 score: 0.8372 time: 0.50s
Epoch 314/1000, LR 0.000212
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 0.55s
Val loss: 0.3871 score: 0.8864 time: 0.08s
Test loss: 0.3941 score: 0.8372 time: 0.13s
Epoch 315/1000, LR 0.000212
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.25s
Val loss: 0.3865 score: 0.8864 time: 0.07s
Test loss: 0.3930 score: 0.8372 time: 0.05s
Epoch 316/1000, LR 0.000212
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.17s
Val loss: 0.3861 score: 0.8864 time: 0.08s
Test loss: 0.3922 score: 0.8372 time: 0.05s
Epoch 317/1000, LR 0.000211
Train loss: 0.6167;  Loss pred: 0.6167; Loss self: 0.0000; time: 0.16s
Val loss: 0.3859 score: 0.8864 time: 0.07s
Test loss: 0.3917 score: 0.8372 time: 0.05s
Epoch 318/1000, LR 0.000211
Train loss: 0.6157;  Loss pred: 0.6157; Loss self: 0.0000; time: 0.17s
Val loss: 0.3858 score: 0.8864 time: 0.06s
Test loss: 0.3914 score: 0.8372 time: 0.24s
Epoch 319/1000, LR 0.000210
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 0.30s
Val loss: 0.3858 score: 0.8864 time: 0.19s
Test loss: 0.3913 score: 0.8372 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 320/1000, LR 0.000210
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 2.48s
Val loss: 0.3859 score: 0.8864 time: 2.56s
Test loss: 0.3912 score: 0.8372 time: 1.53s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 317,   Train_Loss: 0.6157,   Val_Loss: 0.3858,   Val_Precision: 1.0000,   Val_Recall: 0.7727,   Val_accuracy: 0.8718,   Val_Score: 0.8864,   Val_Loss: 0.3858,   Test_Precision: 1.0000,   Test_Recall: 0.6818,   Test_accuracy: 0.8108,   Test_Score: 0.8372,   Test_loss: 0.3914


[0.08459676499478519, 0.06626931903883815, 0.06665601104032248, 0.089698052033782, 0.06427923694718629, 0.06718824605923146, 0.09642257506493479, 0.06766697100829333, 0.06542453705333173, 0.07118417404126376, 0.10903747600968927, 0.05468032497446984, 0.0548763950355351, 0.05448356398846954, 0.054790173075161874, 0.054713204968720675, 0.05653753399383277, 0.05680288397707045, 0.055585638037882745, 1.4804800390265882, 0.6557771749794483, 0.2656160199549049, 0.06327604805119336, 0.2021953599760309, 0.056601344025693834, 0.07482093095313758, 0.2504513809690252, 0.05772252904716879, 0.058897563023492694, 0.05719587206840515, 0.24992314097471535, 0.05229868204332888, 2.0954069229774177, 0.7661122210556641, 0.07166752405464649, 0.07931198505684733, 0.06522167497314513, 0.05442927195690572, 0.05522781505715102, 0.12725600705016404, 0.25028498307801783, 0.3354708789847791, 0.053588483948260546, 0.2500298290979117, 0.055462119984440506, 0.10419729701243341, 0.05981402297038585, 0.06189750996418297, 0.060559379984624684, 0.24353447696194053, 2.3137422299478203, 2.8373248040443286, 0.07770050689578056, 0.11080820602364838, 0.05474730802234262, 0.09818487707525492, 3.9333824950736016, 1.841174609027803, 0.2466274710604921, 0.2415607359725982, 0.053474657936021686, 0.055495138047263026, 0.23009767895564437, 0.24827055202331394, 0.05764050397556275, 2.8589647270273417, 2.146260942914523, 0.9960931029636413, 0.2506180090131238, 0.12033728102687746, 0.23185732495039701, 2.6031034500338137, 0.2575325210345909, 0.2998981639975682, 0.2497449170332402, 0.05365002702455968, 2.0942675719270483, 0.830333348014392, 0.25357821793295443, 0.07493854500353336, 0.1577497790567577, 0.0793314470211044, 0.052735768957063556, 0.06640536594204605, 0.05601161194499582, 2.696211951901205, 0.07413948909379542, 0.061128705041483045, 0.06019063398707658, 0.060051303007639945, 3.1960599499288946, 0.2587771910475567, 0.22809617407619953, 0.05499097506981343, 3.0277279829606414, 1.4244586590211838, 0.5287767639383674, 0.22805398399941623, 0.24303356104064733, 0.05624497891403735, 0.14082806406076998, 0.2553049139678478, 0.06472760101314634, 0.1630173809826374, 0.2357342189643532, 0.24767416098620743, 0.24913921998813748, 0.05632512201555073, 0.05782794894184917, 0.9498419190058485, 2.4180220529669896, 0.23439472203608602, 0.05823535693343729, 0.051316914963535964, 0.24453643499873579, 0.24819667602423579, 0.25720047194045037, 3.2120706470450386, 1.176621043938212, 0.05663292994722724, 1.1116661409614608, 0.06503781606443226, 1.1300636349478737, 1.4325072600040585, 2.355186625965871, 0.05213836708571762, 0.05130533408373594, 0.051297910045832396, 0.05118882097303867, 0.05041386897210032, 0.05064943397883326, 0.050211466033943, 0.05268438602797687, 0.05172203900292516, 0.052055774023756385, 0.05190583097282797, 0.05226895504165441, 0.46333046501968056, 1.3566809330368415, 0.14648569491691887, 0.11865499999839813, 0.09611038805451244, 0.053623382933437824, 0.04938574507832527, 0.0481765519361943, 0.04842587606981397, 0.05177068093325943, 0.05271221394650638, 0.053150240099057555, 0.05268782691564411, 0.05268899002112448, 0.052331010112538934, 0.05353300110436976, 0.04955265193711966, 0.04979407007340342, 0.04981141095049679, 0.05303098901640624, 0.0536107401130721, 0.04920347000006586, 0.04986086394637823, 0.05295018898323178, 0.0500788219505921, 0.0500689169857651, 0.049807026982307434, 0.052565157995559275, 0.9742379010422155, 0.0680344719439745, 0.05275011993944645, 0.05261184403207153, 0.05255502602085471, 0.05264316301327199, 0.053719934076070786, 0.05334303597919643, 0.05369243095628917, 0.05455213296227157, 0.053605518070980906, 0.05329342000186443, 0.05327548098284751, 0.053368504042737186, 0.0534480819478631, 0.053236634004861116, 0.053247499046847224, 0.0533535199938342, 0.05325449991505593, 0.05279919400345534, 0.05224345298483968, 0.053287462098523974, 0.050735307042486966, 0.05192630097735673, 0.05131093191448599, 0.051174922031350434, 0.05105923500377685, 0.05128834000788629, 0.051460409071296453, 0.0515485709765926, 0.051489903940819204, 0.05281276907771826, 0.05146248498931527, 0.05183888599276543, 0.05205286806449294, 0.05230519094038755, 0.05298671801574528, 0.055469596991315484, 0.05490928504150361, 0.05474885005969554, 0.052013479988090694, 0.05834611598402262, 0.054617374087683856, 0.05451101006474346, 0.05204040801618248, 0.05261968600098044, 2.606530265067704, 0.9211624809540808, 0.2484977780841291, 0.23734895896632224, 0.24179827701300383, 0.32587404700461775, 2.5938762900186703, 0.24440678791143, 0.07989601790904999, 0.24014651600737125, 0.05556968308519572, 0.24587829504162073, 0.24564621993340552, 0.1466228679055348, 0.053799414075911045, 0.050965606002137065, 0.05742147902492434, 0.052994532976299524, 0.07095098996069282, 0.0545356769580394, 0.05446126591414213, 0.0515277850208804, 0.05121218308340758, 0.06443883501924574, 0.05337820702698082, 0.052160780993290246, 0.05164974695071578, 0.052352600963786244, 0.052403679001145065, 0.051989570958539844, 0.09171751502435654, 0.08698624605312943, 0.08565741509664804, 0.057043020031414926, 0.05833322799298912, 0.05468091904185712, 0.16641600092407316, 0.05420237104408443, 0.12143539893440902, 0.21360827004536986, 0.24978595494758338, 0.07857739005703479, 0.06254101602826267, 0.08969656191766262, 0.055554667953401804, 0.24694293399807066, 0.053027615998871624, 0.24723792902659625, 0.1111475300276652, 0.25210351904388517, 2.2372030599508435, 2.8249222539598122, 1.3480530380038545, 0.05558339401613921, 0.06699277402367443, 0.06870051799342036, 0.05123939597979188, 0.055796772008761764, 2.2022682760143653, 1.4604845480062068, 0.23252561304252595, 0.05867771490011364, 0.05447226495016366, 0.14277716598007828, 0.1399853409966454, 2.454710590071045, 0.2387701200786978, 0.23501511197537184, 0.054216001997701824, 0.08784024498891085, 0.08320450503379107, 0.05230379197746515, 0.08525432192254812, 0.05361292709130794, 0.09847789199557155, 0.05222330195829272, 0.05170190893113613, 0.05212552298326045, 0.2453287070384249, 0.05565779795870185, 3.10152285406366, 0.2536554510006681, 0.24641203100327402, 0.2375793500104919, 0.24733240099158138, 2.4730275260517374, 0.24900592293124646, 0.06218485103454441, 0.06124537403229624, 0.05501338001340628, 0.061959358979947865, 0.058469027979299426, 2.3359245469328016, 0.04944346193224192, 0.05017430998850614, 0.0494336080737412, 0.04935004492290318, 0.049257938051596284, 0.05018594500143081, 0.0494404430501163, 0.049474978004582226, 0.050372795085422695, 0.0495896190404892, 0.04997448995709419, 0.05023470800369978, 0.049742815899662673, 0.05048439698293805, 0.049808130017481744, 0.05007164995186031, 0.05022765893954784, 0.049679468967951834, 2.812340941047296, 0.5075800759950653, 0.13433663500472903, 0.05711126502137631, 0.052672052988782525, 0.052740031969733536, 0.2408177200704813, 0.24699899996630847, 1.5402842609910294]
[0.0019226537498814816, 0.0015061208872463214, 0.0015149093418255109, 0.0020385920916768637, 0.0014608917487996885, 0.0015270055922552604, 0.0021914221605667, 0.0015378857047339393, 0.0014869212966666303, 0.0016178221373014492, 0.002478124454765665, 0.0012716354645225544, 0.001276195233384537, 0.0012670596276388266, 0.0012741900715153925, 0.0012724001155516437, 0.0013148263719495992, 0.001320997301792336, 0.0012926892566949475, 0.03442976834945554, 0.01525063197626624, 0.006177116743137323, 0.0014715360011905432, 0.004702217673861184, 0.0013163103261789263, 0.0017400216500729668, 0.005824450720209888, 0.0013423843964457858, 0.0013697107679882022, 0.0013301365597303523, 0.005812166069179427, 0.0012162484196122996, 0.04873039355761437, 0.01781656328036428, 0.0016666866059220115, 0.0018444647687638914, 0.0015167831389103518, 0.0012657970222536214, 0.001284367792026768, 0.0029594420244224194, 0.005820581001814368, 0.007801648348483236, 0.0012462438127502453, 0.005814647188323529, 0.0012898167438241978, 0.002423192953777521, 0.0013910237900089733, 0.001439476975911232, 0.0014083576740610392, 0.005663592487486989, 0.053807958835995824, 0.06598429776847275, 0.0018069885324600132, 0.002576935023805776, 0.0012731932098219215, 0.0022833692343082537, 0.09147401151333957, 0.04281801416343728, 0.0057355225828021415, 0.005617691534246469, 0.001243596696186551, 0.001290584605750303, 0.005351108812921962, 0.005773733767984045, 0.0013404768366409943, 0.06648755179133353, 0.04991304518405868, 0.023164955882875378, 0.005828325791002879, 0.0027985414192297085, 0.005392030812799931, 0.060537289535670086, 0.005989128396153277, 0.0069743759069201905, 0.0058080213263544235, 0.0012476750470827832, 0.048703897021559264, 0.019310077860799812, 0.005897167858905917, 0.0017427568605472875, 0.0036685995129478533, 0.0018449173725838231, 0.0012264132315596176, 0.001544310835861536, 0.0013025956266278097, 0.06270260353258617, 0.0017241741649719866, 0.0014215977916623964, 0.001399782185745967, 0.0013965419304102313, 0.07432697557974173, 0.006018074210408295, 0.005304562187818594, 0.0012788598853444985, 0.07041227867350329, 0.03312694555863218, 0.01229713404507831, 0.005303581023242238, 0.005651943280015054, 0.001308022765442729, 0.0032750712572272088, 0.005937323580647624, 0.0015052930468173568, 0.003791101883317149, 0.005482191138705889, 0.005759864208981568, 0.005793935348561337, 0.0013098865585011799, 0.0013448360219034691, 0.022089346953624384, 0.05623307099923232, 0.0054510400473508375, 0.0013543106263590066, 0.001193416627058976, 0.005686893837179902, 0.005772015721493855, 0.00598140632419652, 0.07469931737314044, 0.02736328009158633, 0.0013170448824936568, 0.025852700952592112, 0.001512507350335634, 0.02628054964995055, 0.03331412232567578, 0.0547717819992063, 0.0012125201647841307, 0.001193147304272929, 0.0011929746522286603, 0.001190437697047411, 0.001172415557490705, 0.0011778938134612386, 0.001167708512417279, 0.0012252182797203924, 0.0012028381163470966, 0.0012105993959013112, 0.0012071123482053016, 0.001215557093991963, 0.010775127093480942, 0.0315507193729498, 0.0034066440678353228, 0.0027594186046139097, 0.0022351253035933125, 0.0012470554170566936, 0.0011485056994959366, 0.0011203849287487048, 0.0011261831644142782, 0.001203969324029289, 0.0012258654406164275, 0.0012360520953269198, 0.0012252983003638166, 0.0012253253493284763, 0.001217000235175324, 0.0012449535140551108, 0.00115238725435162, 0.001158001629614033, 0.0011584049058255069, 0.0012332788143350289, 0.001246761397978421, 0.0011442667441875782, 0.0011595549754971682, 0.001231399743796088, 0.0011646237662928396, 0.001164393418273607, 0.001158302953076917, 0.0012224455347804482, 0.02265669537307478, 0.0015821970219528953, 0.001226746975335964, 0.001223531256559803, 0.0012222099074617374, 0.0012242596049598137, 0.0012493007924667624, 0.0012405357204464285, 0.0012486611850299808, 0.0012686542549365481, 0.0012466399551390908, 0.0012393818605084751, 0.0012389646740197096, 0.0012411280009938881, 0.0012429786499503048, 0.0012380612559270026, 0.0012383139313220284, 0.0012407795347403302, 0.001238476742210603, 0.0012278882326384963, 0.0012149640229032483, 0.0012392433046168366, 0.0011798908614531853, 0.0012075883948222497, 0.0011932774863833951, 0.001190114465845359, 0.0011874240698552755, 0.001192752093206658, 0.0011967536993324757, 0.0011988039761998278, 0.0011974396265306791, 0.0012282039320399595, 0.001196801976495704, 0.0012055554882038472, 0.0012105318154533242, 0.0012163997893113382, 0.0012322492561801228, 0.0012899906277050112, 0.00127696011724427, 0.0012732290711557103, 0.0012096158136765277, 0.001356886418233084, 0.0012701714904112525, 0.001267697908482406, 0.0012102420468879647, 0.0012237136279297776, 0.060616982908551255, 0.021422383278001878, 0.005779018094979747, 0.005519743231774936, 0.005623215744488461, 0.007578466209409715, 0.06032270441903884, 0.005683878788637907, 0.0018580469281174415, 0.0055848026978458426, 0.0012923182112836214, 0.005718099884688854, 0.005712702789148966, 0.0034098341373380186, 0.0012511491645560708, 0.0011852466512124898, 0.0013353832331377752, 0.001232430999448826, 0.0016500230223416936, 0.001268271557163707, 0.0012665410677707473, 0.0011983205818809396, 0.0011909810019397112, 0.0014985775585871102, 0.0012413536517902515, 0.0012130414184486104, 0.0012011569058305996, 0.001217502347995029, 0.001218690209328955, 0.0012090597897334847, 0.0021329654656827104, 0.0020229359547239405, 0.001992032909224373, 0.001326581861195696, 0.001356586697511375, 0.0012716492800431889, 0.0038701395563737946, 0.0012605202568391728, 0.002824079044986256, 0.004967634187101624, 0.005808975696455427, 0.001827381164117088, 0.001454442233215411, 0.002085966556224712, 0.001291969022172135, 0.00574285893018769, 0.001233200372066782, 0.005749719279688285, 0.0025848262797131443, 0.005862872535904306, 0.05202797813839171, 0.06569586637115843, 0.03135007065125243, 0.0012926370701427724, 0.001557971488922661, 0.0015976864649632641, 0.00119161385999516, 0.0012975993490409712, 0.05121554130265966, 0.0339647569303769, 0.005407572396337813, 0.0013645980209328753, 0.0012667968593061316, 0.0033203992088390298, 0.003255473046433614, 0.057086292792349884, 0.005552793490202274, 0.0054654677203574845, 0.0012608372557605075, 0.00204279639509095, 0.0019349884891579317, 0.0012163672552898872, 0.001982658649361584, 0.0012468122579373938, 0.002290183534780734, 0.0012144953943789005, 0.0012023699751427008, 0.0012122214647269872, 0.005705318768335463, 0.0012943673943884152, 0.07212843846659674, 0.005898963976759724, 0.00573051234891335, 0.005525101163034695, 0.0057519163021298, 0.057512268047714823, 0.005790835417005731, 0.0014461593263847537, 0.0014243110240068893, 0.001279380930544332, 0.0014409153251150667, 0.0013597448367278935, 0.05432382667285585, 0.0011498479519126026, 0.001166844418337352, 0.0011496187924125859, 0.0011476754633233297, 0.0011455334430603788, 0.0011671150000332746, 0.0011497777453515418, 0.0011505808838274936, 0.0011714603508237837, 0.0011532469544299814, 0.0011621974408626556, 0.0011682490233418553, 0.0011568096720851785, 0.001174055743789257, 0.001158328605057715, 0.0011644569756246584, 0.0011680850916173915, 0.001155336487626787, 0.06540327769877433, 0.011804187813838728, 0.003124107790807652, 0.0013281689539854956, 0.0012249314648554075, 0.001226512371389152, 0.005600412094662356, 0.00574416278991415, 0.03582056420909371]
[520.1144512170447, 663.9573280391356, 660.1055075645452, 490.53462145898953, 684.5134150573643, 654.8764490921628, 456.3246726232799, 650.2433808453952, 672.5305517123152, 618.114919398998, 403.53098411861697, 786.3888888750503, 783.5791686417346, 789.2288398956458, 784.8122680870534, 785.91630712518, 760.5566950389193, 757.0038172244522, 773.5811176745804, 29.044633407061934, 65.57105315741981, 161.8878259199138, 679.5620353093312, 212.6656121342122, 759.699274640553, 574.7054928644511, 171.69000958840016, 744.9431047080757, 730.0811407570195, 751.80250680631, 172.05289527131183, 822.2004517126259, 20.521073748720937, 56.12754739866721, 599.9928219539509, 542.1627004945028, 659.2900292380585, 790.016078738756, 778.593177287615, 337.9015340552803, 171.80415489248995, 128.17804075909368, 802.4112053910001, 171.97948002900563, 775.3039373912021, 412.6786512981138, 718.894965839189, 694.6967660715588, 710.0468996036154, 176.5663758841013, 18.584611303468208, 15.155120745678362, 553.4069431191197, 388.05790241584685, 785.4267461415913, 437.9493184784675, 10.93206675268823, 23.354656201078793, 174.35202905459417, 178.00906189024764, 804.119215712351, 774.842653123569, 186.87715667175007, 173.19814875169757, 746.0031927935652, 15.040409415862223, 20.034842520876325, 43.16865549220609, 171.57585829256297, 357.32899757304557, 185.45888084061747, 16.51874419337481, 166.9692038397915, 143.3820048339765, 172.1756763292878, 801.4907425920894, 20.532237893763206, 51.78643023651591, 169.57292448269004, 573.8035079006744, 272.5835830459628, 542.0296945870758, 815.3858538596406, 647.538032356111, 767.6979559564649, 15.948301085780372, 579.9878111595806, 703.4338445550159, 714.396861299591, 716.0544042571297, 13.454065528700943, 166.16611311812906, 188.5169717298068, 781.9464911362206, 14.20206842952674, 30.186906252195083, 81.31976087552128, 188.5518474437617, 176.93029644086175, 764.5126877142143, 305.3368679515802, 168.42605703004708, 664.3224733644398, 263.7755541206973, 182.40881696730796, 173.61520405996086, 172.5942627662052, 763.4248885981673, 743.5850793055117, 45.27069098509142, 17.783129788761702, 183.45122973110279, 738.3830419232903, 837.9303399386795, 175.8429168243264, 173.24970136103337, 167.18476321441506, 13.38700319046783, 36.54532631515475, 759.275567060879, 38.6806779621893, 661.1538117669936, 38.050954539372874, 30.01729987733408, 18.257576501974885, 824.728552187034, 838.1194814913256, 838.2407774816054, 840.0271618416108, 852.9398928655278, 848.9729622244148, 856.3781023826701, 816.1810973210509, 831.367069607757, 826.0370882272607, 828.4233041661531, 822.6680630162253, 92.80632992301406, 31.694998398589796, 293.54402164926734, 362.3951793062282, 447.40220979662485, 801.8889828971714, 870.6965933550755, 892.5503854437279, 887.9550250780871, 830.5859460383335, 815.7502176561476, 809.0273895256113, 816.127794923962, 816.1097789644498, 821.692528149707, 803.2428429739207, 867.7638495426094, 863.5566431226046, 863.256012617951, 810.8466539573127, 802.0780893773775, 873.9221034602324, 862.3998181468216, 812.0839760103066, 858.6463963234581, 858.8162594414647, 863.3319956092653, 818.0323552653006, 44.13706339488503, 632.0325383786317, 815.1640233114366, 817.3064600014349, 818.190062030164, 816.8202201140379, 800.4477432736479, 806.1033499624923, 800.8577602866623, 788.2368234756088, 802.1562247204145, 806.8538292062261, 807.1255145278597, 805.7186681786293, 804.5190478854812, 807.714477141316, 807.5496646738007, 805.944949929626, 807.443503714945, 814.406371377297, 823.0696392230813, 806.9440409921693, 847.5360159738615, 828.0967292230351, 838.028045790771, 840.2553104752682, 842.1591118006232, 838.3971872239998, 835.5938239904997, 834.1647340626693, 835.1151722757685, 814.197035128418, 835.5601174122806, 829.4931338995408, 826.0832034600566, 822.0981364738212, 811.5241254840944, 775.1994305408823, 783.109814077858, 785.4046240809596, 826.7087687623579, 736.9813615661244, 787.2952648907454, 788.8314663208097, 826.2809927744748, 817.1846559327397, 16.49702693894601, 46.68014697631124, 173.03977657877618, 181.16784749033312, 177.83418695612738, 131.95282163538073, 16.577506092123805, 175.93619378354856, 538.1995389175622, 179.0573551301495, 773.8032252959815, 174.88326894702612, 175.0484905147625, 293.26939661079166, 799.2652101996305, 843.7062437400812, 748.8487013950902, 811.4044522145458, 606.0521498547406, 788.4746719672128, 789.5519738338298, 834.5012304056012, 839.6439560088139, 667.2994629272445, 805.5722062426152, 824.3741596877422, 832.5307003155432, 821.3536521278915, 820.5530760361389, 827.0889566349997, 468.83084423494137, 494.33102301870196, 501.9997387439571, 753.8170310113121, 737.1441883032434, 786.380345346507, 258.3886150444069, 793.323228702058, 354.0977373757846, 201.3030674836087, 172.1473891877683, 547.2312069513735, 687.5487916692628, 479.394071307572, 774.01236627078, 174.12929904014163, 810.8982308560694, 173.92153448823922, 386.8731944767201, 170.56485432285749, 19.220427850185764, 15.221657849070038, 31.897854748855252, 773.6123488161682, 641.8602696584012, 625.90503326508, 839.1980267870179, 770.6539007892377, 19.525323262532215, 29.442283424841314, 184.925864455783, 732.8165398601219, 789.3925475531527, 301.1686056718607, 307.17502056897837, 17.517339996791836, 180.0895354319349, 182.96695748019022, 793.1237718676257, 489.52504635464544, 516.7989399436583, 822.1201250289148, 504.37325674896175, 802.0453710122354, 436.64622717486344, 823.3872311318277, 831.690761307739, 824.931771213288, 175.27504432355352, 772.5781755129093, 13.864157068409405, 169.5212928812113, 174.5044664618209, 180.99216113732547, 173.8551027993442, 17.38759457669717, 172.68665537675912, 691.4867412983428, 702.0938426684277, 781.6280328443968, 694.0033064886331, 735.432099456551, 18.408128831242166, 869.6802027925929, 857.0122839726223, 869.8535606758859, 871.3264611445946, 872.9557448173881, 856.8135958937121, 869.7333063219555, 869.1262075147858, 853.6353785228746, 867.1169658490647, 860.4389967144804, 855.9818840159886, 864.4464375868114, 851.7483137321118, 863.312876530554, 858.7693842991172, 856.1020144648434, 865.5487043901223, 15.289753590113119, 84.71569715517764, 320.09138831329426, 752.916258883522, 816.3722042343335, 815.3199456662608, 178.55828876469297, 174.08977366655472, 27.916924874849727]
Elapsed: 0.34558912765465427~0.7055916372862143
Time per graph: 0.008035601693566~0.01640961964130724
Speed: 546.2463549591024~315.1754683082197
Total Time: 1.5431
best val loss: 0.3858189582824707 test_score: 0.8372

Testing...
Test loss: 0.5178 score: 0.8372 time: 2.16s
test Score 0.8372
Epoch Time List: [0.6273739230819046, 0.28242024790961295, 0.29188285302370787, 0.3171775030205026, 0.30749962106347084, 0.2928375790361315, 0.4048267911421135, 0.28354904404841363, 0.3434214540757239, 0.3817248650593683, 0.31397711706813425, 0.3308772360906005, 0.26345525309443474, 0.2610588950337842, 0.35844664997421205, 0.2897656738059595, 0.2690707731526345, 0.7012239169562235, 0.7781725730746984, 3.3545097650494426, 8.502680602134205, 1.9824120560660958, 0.6292500160634518, 0.45426276698708534, 0.305009420029819, 0.43178983009420335, 0.6242573501076549, 0.3452781628584489, 0.5385527510661632, 0.5484014640096575, 0.7015934339724481, 0.8073326209560037, 3.1065023510018364, 5.2320361798629165, 0.8236266110325232, 0.28907948499545455, 0.3250760938972235, 0.276082884054631, 0.27269734803121537, 0.41918843088205904, 1.0323780310573056, 7.078759087016806, 4.683296793024056, 0.7396085800137371, 0.3039977248990908, 0.3129833649145439, 0.2646810448495671, 0.419016528991051, 0.5546847431687638, 0.7409440730698407, 10.114863075083122, 11.606216792948544, 0.8519249049713835, 0.35785679006949067, 0.26860425306949764, 0.3174889509100467, 11.269803594914265, 12.733366462751292, 1.3126564208650962, 0.9707220048876479, 0.7111622251104563, 0.3003629300510511, 0.6442550910869613, 0.4877127088839188, 0.8572243720991537, 5.907176528009586, 5.207922377972864, 3.5405913398135453, 0.9088622179115191, 1.1699371819850057, 0.9721661730436608, 9.247156074969098, 9.183080699061975, 0.7813996498007327, 1.1499148270813748, 0.7964258930878714, 3.968752152984962, 6.189746639109217, 1.7797678840579465, 0.8121473939390853, 0.40535975713282824, 0.3503764070337638, 0.2560773299774155, 0.267909261980094, 0.267229849123396, 9.186403407831676, 6.994171160040423, 0.37457812402863055, 0.4118984378874302, 0.2928269359981641, 5.771025559050031, 10.087341368896887, 0.9880453641526401, 0.7937377709895372, 4.516462257015519, 11.689222458051518, 2.877689504995942, 1.2760230309795588, 1.1980382819892839, 0.7845205818302929, 0.3575154399732128, 0.5693851679097861, 1.145193472970277, 0.966559075168334, 0.972661197069101, 0.9761343859136105, 0.732827533967793, 0.29302245809230953, 0.5332234450615942, 4.629004194983281, 4.543425768963061, 1.039451921125874, 0.7715998260537162, 0.5069630218204111, 0.7555627500405535, 0.539278898970224, 0.7648417529417202, 6.546058761887252, 9.2672432619147, 6.127344141015783, 3.761330191977322, 0.27465940883848816, 1.3644524540286511, 12.24933265009895, 12.592166770016775, 0.4562857720302418, 0.2520481119863689, 0.24762389296665788, 0.24923287390265614, 0.24560985807329416, 0.24487597588449717, 0.23977535997983068, 0.25139853300061077, 0.24796154897194356, 0.24833988584578037, 0.24681403301656246, 0.24741142406128347, 1.9027179450495169, 3.456496559898369, 1.1191298860358074, 0.5310655101202428, 0.8237200060393661, 0.4055156089598313, 0.24659488792531192, 0.23239249584730715, 0.2299458709312603, 0.23364062304608524, 0.24995635694358498, 0.252101902035065, 0.25088858196977526, 0.2512731129536405, 0.25091867288574576, 0.26235188893042505, 0.2363582740072161, 0.23861550691071898, 0.2376888960134238, 0.24385588697623461, 0.25986924511380494, 0.24602892098482698, 0.2374490649672225, 0.24045027210377157, 0.23923188308253884, 0.2415611050091684, 0.2392266399692744, 0.24299006292130798, 1.175215742085129, 4.064770563156344, 0.343210291932337, 0.25779384293127805, 0.2516986469272524, 0.2511188789503649, 0.2569526460720226, 0.2564568258821964, 0.25507944100536406, 0.2576593061676249, 0.25589306093752384, 0.2561425620224327, 0.25543346104677767, 0.2552934919949621, 0.25837078399490565, 0.25564540293999016, 0.259177676984109, 0.25865718396380544, 0.2552183329826221, 0.25828207982704043, 0.25514954898972064, 0.25893274403642863, 0.25373429094906896, 0.2557644569315016, 0.25094569695647806, 0.24740886501967907, 0.24648107797838748, 0.2478249000851065, 0.2465595561079681, 0.24715339089743793, 0.24645857291761786, 0.2513543780660257, 0.24892057292163372, 0.31027740298304707, 0.2476877380395308, 0.24710890802089125, 0.24818829901050776, 0.25807276007253677, 0.2672375339316204, 0.26232403505127877, 0.3214286221191287, 0.25323131296318024, 2.2593817898305133, 0.2637132831150666, 0.24580584012437612, 0.24611397401895374, 5.488480814034119, 11.05813836993184, 2.4461172689916566, 0.9814561330713332, 1.2054284259211272, 1.3204096589470282, 7.824173953034915, 7.224987281952053, 0.5716367108980194, 0.658100995933637, 0.8306864638580009, 6.847434633877128, 0.997626302880235, 0.9513561221538112, 0.2560890691820532, 0.25499752711039037, 0.24823785410262644, 0.25577145896386355, 0.34587272501084954, 0.2661615351680666, 0.26602274598553777, 0.33515883795917034, 0.4203808900201693, 0.5874592080945149, 0.38535234390292317, 0.26613897806964815, 0.24990107596386224, 0.3553966430481523, 0.3373570990515873, 0.2746634219074622, 0.399646777077578, 0.37029502203222364, 0.8213337928755209, 0.8563234428875148, 0.27763853897340596, 0.3978428039699793, 0.3868177430704236, 0.31839665106963366, 0.34444998402614146, 0.5836167879169807, 0.5130420229397714, 0.6851019220193848, 0.2824799990048632, 0.6443031689850613, 0.2681776729878038, 0.661328217945993, 0.27882910007610917, 0.9832760880235583, 0.7547123051481321, 1.0122823399724439, 7.999136421829462, 14.15175046690274, 6.265366062987596, 1.1753663930576295, 0.295343813020736, 0.30482868291437626, 0.44482639094348997, 0.5917794859269634, 2.7019400239223614, 9.137120847124606, 1.7220185599289834, 0.8018212190363556, 0.25949935405515134, 0.35743554204236716, 0.8754133590264246, 6.059197047143243, 6.3931684219278395, 0.9406423439504579, 0.27287619304843247, 0.29586248903069645, 0.2898216029861942, 0.3517750420141965, 0.45972332602832466, 0.799112272914499, 0.383204170037061, 0.2637669040123001, 0.26240019185934216, 0.24562082497868687, 0.5481750210747123, 0.8176149479113519, 8.294025685056113, 3.4618036050815135, 0.9753484519897029, 0.9765192810446024, 0.9851221090648323, 10.77595985494554, 6.592725892085582, 0.4398658679565415, 0.3731469390913844, 0.2785982668865472, 0.2940515710506588, 0.27749750891234726, 12.818504371098243, 7.530364376027137, 0.23514550703112036, 0.23897964786738157, 0.23905174899846315, 0.23848134407307953, 0.24206190183758736, 0.2381957300240174, 0.2386631880654022, 0.24143662303686142, 0.24041684693656862, 0.24293789092916995, 0.24090340198017657, 0.2398495809175074, 0.24078708898741752, 0.24174471606966108, 0.2395887691527605, 0.24208700889721513, 0.24053795891813934, 3.1801951830275357, 9.162430218886584, 0.7665183661738411, 0.3708680820418522, 0.2982014500303194, 0.27547464589588344, 0.470342198968865, 0.7392057289835066, 6.570879652048461]
Total Epoch List: [11, 320]
Total Time List: [0.10935878101736307, 1.5431462450651452]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351db65e70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.5415;  Loss pred: 2.5415; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 2.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4884 time: 2.81s
Epoch 2/1000, LR 0.000000
Train loss: 2.4920;  Loss pred: 2.4920; Loss self: 0.0000; time: 3.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 2.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4884 time: 2.67s
Epoch 3/1000, LR 0.000030
Train loss: 2.5302;  Loss pred: 2.5302; Loss self: 0.0000; time: 1.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 1.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.47s
Epoch 4/1000, LR 0.000060
Train loss: 2.4749;  Loss pred: 2.4749; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4884 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 2.4383;  Loss pred: 2.4383; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 2.4925;  Loss pred: 2.4925; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4884 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 2.4278;  Loss pred: 2.4278; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.4884 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 2.4003;  Loss pred: 2.4003; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4884 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 2.3677;  Loss pred: 2.3677; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 2.3603;  Loss pred: 2.3603; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 2.3310;  Loss pred: 2.3310; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 2.2647;  Loss pred: 2.2647; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 2.77s
Epoch 13/1000, LR 0.000270
Train loss: 2.2534;  Loss pred: 2.2534; Loss self: 0.0000; time: 4.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 1.37s
Epoch 14/1000, LR 0.000270
Train loss: 2.2230;  Loss pred: 2.2230; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 1.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.77s
Epoch 15/1000, LR 0.000270
Train loss: 2.1897;  Loss pred: 2.1897; Loss self: 0.0000; time: 2.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.07s
Epoch 16/1000, LR 0.000270
Train loss: 2.1655;  Loss pred: 2.1655; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 2.0881;  Loss pred: 2.0881; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 2.0876;  Loss pred: 2.0876; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 2.0662;  Loss pred: 2.0662; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 2.0020;  Loss pred: 2.0020; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 3.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 3.87s
Epoch 21/1000, LR 0.000270
Train loss: 1.9924;  Loss pred: 1.9924; Loss self: 0.0000; time: 7.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 2.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 1.22s
Epoch 22/1000, LR 0.000270
Train loss: 1.9536;  Loss pred: 1.9536; Loss self: 0.0000; time: 2.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 1.9622;  Loss pred: 1.9622; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 1.9113;  Loss pred: 1.9113; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.08s
Test loss: 0.6932 score: 0.5116 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 1.8757;  Loss pred: 1.8757; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
Test loss: 0.6931 score: 0.5116 time: 0.12s
Epoch 26/1000, LR 0.000270
Train loss: 1.8752;  Loss pred: 1.8752; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.06s
Test loss: 0.6931 score: 0.5116 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 1.8455;  Loss pred: 1.8455; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.07s
Test loss: 0.6931 score: 0.5116 time: 0.09s
Epoch 28/1000, LR 0.000270
Train loss: 1.8165;  Loss pred: 1.8165; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.09s
Test loss: 0.6930 score: 0.5116 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 1.7526;  Loss pred: 1.7526; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.24s
Test loss: 0.6930 score: 0.5116 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 1.7549;  Loss pred: 1.7549; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 2.99s
Test loss: 0.6930 score: 0.5116 time: 1.59s
Epoch 31/1000, LR 0.000270
Train loss: 1.7271;  Loss pred: 1.7271; Loss self: 0.0000; time: 5.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 2.51s
Test loss: 0.6929 score: 0.5116 time: 2.43s
Epoch 32/1000, LR 0.000270
Train loss: 1.7309;  Loss pred: 1.7309; Loss self: 0.0000; time: 4.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.06s
Test loss: 0.6929 score: 0.5116 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 1.6949;  Loss pred: 1.6949; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.06s
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 1.6933;  Loss pred: 1.6933; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
Test loss: 0.6929 score: 0.5116 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 1.6290;  Loss pred: 1.6290; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 1.6254;  Loss pred: 1.6254; Loss self: 0.0000; time: 3.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 2.85s
Test loss: 0.6928 score: 0.5116 time: 2.89s
Epoch 37/1000, LR 0.000270
Train loss: 1.6054;  Loss pred: 1.6054; Loss self: 0.0000; time: 6.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.10s
Test loss: 0.6928 score: 0.5116 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 1.5858;  Loss pred: 1.5858; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.06s
Test loss: 0.6928 score: 0.5116 time: 0.13s
Epoch 39/1000, LR 0.000269
Train loss: 1.5818;  Loss pred: 1.5818; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.06s
Test loss: 0.6928 score: 0.5116 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 1.5575;  Loss pred: 1.5575; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.20s
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 1.5449;  Loss pred: 1.5449; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.21s
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 1.5221;  Loss pred: 1.5221; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 2.70s
Test loss: 0.6927 score: 0.5116 time: 2.81s
Epoch 43/1000, LR 0.000269
Train loss: 1.5214;  Loss pred: 1.5214; Loss self: 0.0000; time: 6.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.97s
Test loss: 0.6927 score: 0.5116 time: 0.14s
Epoch 44/1000, LR 0.000269
Train loss: 1.4839;  Loss pred: 1.4839; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
Test loss: 0.6927 score: 0.5116 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 1.4707;  Loss pred: 1.4707; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
Test loss: 0.6926 score: 0.5116 time: 0.18s
Epoch 46/1000, LR 0.000269
Train loss: 1.4407;  Loss pred: 1.4407; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.14s
Test loss: 0.6926 score: 0.5116 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 1.4456;  Loss pred: 1.4456; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.11s
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 1.4329;  Loss pred: 1.4329; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.07s
Test loss: 0.6925 score: 0.5116 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 1.4090;  Loss pred: 1.4090; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.09s
Test loss: 0.6924 score: 0.5116 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 1.4149;  Loss pred: 1.4149; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.23s
Test loss: 0.6923 score: 0.5116 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 1.4020;  Loss pred: 1.4020; Loss self: 0.0000; time: 9.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 2.79s
Test loss: 0.6923 score: 0.5116 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 1.3911;  Loss pred: 1.3911; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.06s
Test loss: 0.6922 score: 0.5116 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 1.3565;  Loss pred: 1.3565; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 1.92s
Test loss: 0.6922 score: 0.5116 time: 3.15s
Epoch 54/1000, LR 0.000269
Train loss: 1.3470;  Loss pred: 1.3470; Loss self: 0.0000; time: 7.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 2.99s
Test loss: 0.6921 score: 0.5116 time: 1.12s
Epoch 55/1000, LR 0.000269
Train loss: 1.3429;  Loss pred: 1.3429; Loss self: 0.0000; time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.22s
Test loss: 0.6920 score: 0.5116 time: 0.18s
Epoch 56/1000, LR 0.000269
Train loss: 1.3398;  Loss pred: 1.3398; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
Test loss: 0.6919 score: 0.5116 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 1.3192;  Loss pred: 1.3192; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.05s
Test loss: 0.6918 score: 0.5116 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 1.3208;  Loss pred: 1.3208; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.05s
Test loss: 0.6917 score: 0.5116 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 1.3083;  Loss pred: 1.3083; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.06s
Test loss: 0.6916 score: 0.5116 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 1.2949;  Loss pred: 1.2949; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.05s
Test loss: 0.6915 score: 0.5116 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 1.2834;  Loss pred: 1.2834; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.14s
Test loss: 0.6914 score: 0.5116 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 1.2745;  Loss pred: 1.2745; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.08s
Test loss: 0.6913 score: 0.5116 time: 0.17s
Epoch 63/1000, LR 0.000268
Train loss: 1.2567;  Loss pred: 1.2567; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.22s
Test loss: 0.6911 score: 0.5116 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 1.2525;  Loss pred: 1.2525; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.05s
Test loss: 0.6910 score: 0.5116 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 1.2563;  Loss pred: 1.2563; Loss self: 0.0000; time: 0.30s
Val loss: 0.6906 score: 0.5227 time: 0.17s
Test loss: 0.6908 score: 0.5116 time: 1.49s
Epoch 66/1000, LR 0.000268
Train loss: 1.2421;  Loss pred: 1.2421; Loss self: 0.0000; time: 6.54s
Val loss: 0.6905 score: 0.5227 time: 0.65s
Test loss: 0.6907 score: 0.5116 time: 0.28s
Epoch 67/1000, LR 0.000268
Train loss: 1.2353;  Loss pred: 1.2353; Loss self: 0.0000; time: 0.30s
Val loss: 0.6903 score: 0.5227 time: 0.10s
Test loss: 0.6905 score: 0.5116 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 1.2226;  Loss pred: 1.2226; Loss self: 0.0000; time: 0.17s
Val loss: 0.6902 score: 0.5227 time: 0.05s
Test loss: 0.6903 score: 0.5349 time: 0.23s
Epoch 69/1000, LR 0.000268
Train loss: 1.2256;  Loss pred: 1.2256; Loss self: 0.0000; time: 0.55s
Val loss: 0.6900 score: 0.5227 time: 0.20s
Test loss: 0.6902 score: 0.5349 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 1.2198;  Loss pred: 1.2198; Loss self: 0.0000; time: 0.23s
Val loss: 0.6898 score: 0.5227 time: 0.29s
Test loss: 0.6900 score: 0.5349 time: 3.27s
Epoch 71/1000, LR 0.000268
Train loss: 1.2011;  Loss pred: 1.2011; Loss self: 0.0000; time: 3.09s
Val loss: 0.6897 score: 0.5227 time: 0.41s
Test loss: 0.6898 score: 0.5349 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 1.1961;  Loss pred: 1.1961; Loss self: 0.0000; time: 0.55s
Val loss: 0.6895 score: 0.5227 time: 0.15s
Test loss: 0.6896 score: 0.5116 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 1.1925;  Loss pred: 1.1925; Loss self: 0.0000; time: 0.17s
Val loss: 0.6893 score: 0.5227 time: 0.10s
Test loss: 0.6895 score: 0.5116 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 1.1859;  Loss pred: 1.1859; Loss self: 0.0000; time: 0.42s
Val loss: 0.6892 score: 0.5000 time: 0.24s
Test loss: 0.6893 score: 0.5116 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 1.1723;  Loss pred: 1.1723; Loss self: 0.0000; time: 0.74s
Val loss: 0.6890 score: 0.5000 time: 0.24s
Test loss: 0.6891 score: 0.5116 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 1.1746;  Loss pred: 1.1746; Loss self: 0.0000; time: 0.70s
Val loss: 0.6888 score: 0.5000 time: 0.17s
Test loss: 0.6889 score: 0.5116 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 1.1712;  Loss pred: 1.1712; Loss self: 0.0000; time: 0.43s
Val loss: 0.6886 score: 0.5000 time: 0.09s
Test loss: 0.6887 score: 0.5116 time: 0.05s
Epoch 78/1000, LR 0.000267
Train loss: 1.1721;  Loss pred: 1.1721; Loss self: 0.0000; time: 0.16s
Val loss: 0.6884 score: 0.5000 time: 0.05s
Test loss: 0.6885 score: 0.5116 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 1.1583;  Loss pred: 1.1583; Loss self: 0.0000; time: 0.29s
Val loss: 0.6882 score: 0.5227 time: 0.06s
Test loss: 0.6882 score: 0.5116 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 1.1593;  Loss pred: 1.1593; Loss self: 0.0000; time: 0.15s
Val loss: 0.6880 score: 0.5227 time: 0.05s
Test loss: 0.6880 score: 0.5116 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 1.1410;  Loss pred: 1.1410; Loss self: 0.0000; time: 0.15s
Val loss: 0.6878 score: 0.5455 time: 0.06s
Test loss: 0.6878 score: 0.5116 time: 0.08s
Epoch 82/1000, LR 0.000267
Train loss: 1.1429;  Loss pred: 1.1429; Loss self: 0.0000; time: 0.15s
Val loss: 0.6875 score: 0.5455 time: 0.05s
Test loss: 0.6875 score: 0.5116 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 1.1451;  Loss pred: 1.1451; Loss self: 0.0000; time: 0.15s
Val loss: 0.6873 score: 0.5455 time: 0.06s
Test loss: 0.6873 score: 0.5116 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.1304;  Loss pred: 1.1304; Loss self: 0.0000; time: 0.15s
Val loss: 0.6870 score: 0.5455 time: 0.05s
Test loss: 0.6870 score: 0.5116 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 1.1344;  Loss pred: 1.1344; Loss self: 0.0000; time: 0.27s
Val loss: 0.6868 score: 0.5455 time: 0.17s
Test loss: 0.6867 score: 0.5116 time: 0.09s
Epoch 86/1000, LR 0.000266
Train loss: 1.1222;  Loss pred: 1.1222; Loss self: 0.0000; time: 0.26s
Val loss: 0.6865 score: 0.5455 time: 0.08s
Test loss: 0.6864 score: 0.5116 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 1.1225;  Loss pred: 1.1225; Loss self: 0.0000; time: 0.33s
Val loss: 0.6862 score: 0.5455 time: 0.12s
Test loss: 0.6861 score: 0.5116 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 1.1186;  Loss pred: 1.1186; Loss self: 0.0000; time: 0.55s
Val loss: 0.6859 score: 0.5455 time: 0.21s
Test loss: 0.6858 score: 0.5116 time: 0.25s
Epoch 89/1000, LR 0.000266
Train loss: 1.1135;  Loss pred: 1.1135; Loss self: 0.0000; time: 3.19s
Val loss: 0.6856 score: 0.5455 time: 1.46s
Test loss: 0.6855 score: 0.5116 time: 3.13s
Epoch 90/1000, LR 0.000266
Train loss: 1.1022;  Loss pred: 1.1022; Loss self: 0.0000; time: 2.09s
Val loss: 0.6853 score: 0.5455 time: 0.82s
Test loss: 0.6852 score: 0.5116 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 1.0981;  Loss pred: 1.0981; Loss self: 0.0000; time: 0.53s
Val loss: 0.6850 score: 0.5455 time: 0.22s
Test loss: 0.6848 score: 0.5116 time: 0.05s
Epoch 92/1000, LR 0.000266
Train loss: 1.0987;  Loss pred: 1.0987; Loss self: 0.0000; time: 0.55s
Val loss: 0.6847 score: 0.5455 time: 0.17s
Test loss: 0.6845 score: 0.5116 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 1.0968;  Loss pred: 1.0968; Loss self: 0.0000; time: 0.54s
Val loss: 0.6843 score: 0.5455 time: 0.22s
Test loss: 0.6841 score: 0.5116 time: 0.20s
Epoch 94/1000, LR 0.000265
Train loss: 1.0937;  Loss pred: 1.0937; Loss self: 0.0000; time: 0.35s
Val loss: 0.6839 score: 0.5455 time: 0.25s
Test loss: 0.6837 score: 0.5116 time: 0.05s
Epoch 95/1000, LR 0.000265
Train loss: 1.0919;  Loss pred: 1.0919; Loss self: 0.0000; time: 0.53s
Val loss: 0.6836 score: 0.5455 time: 0.22s
Test loss: 0.6833 score: 0.5349 time: 3.14s
Epoch 96/1000, LR 0.000265
Train loss: 1.0868;  Loss pred: 1.0868; Loss self: 0.0000; time: 6.86s
Val loss: 0.6832 score: 0.5455 time: 0.98s
Test loss: 0.6829 score: 0.5349 time: 0.29s
Epoch 97/1000, LR 0.000265
Train loss: 1.0836;  Loss pred: 1.0836; Loss self: 0.0000; time: 0.55s
Val loss: 0.6828 score: 0.5455 time: 0.05s
Test loss: 0.6825 score: 0.5349 time: 0.05s
Epoch 98/1000, LR 0.000265
Train loss: 1.0800;  Loss pred: 1.0800; Loss self: 0.0000; time: 0.14s
Val loss: 0.6824 score: 0.5455 time: 0.05s
Test loss: 0.6820 score: 0.5349 time: 0.05s
Epoch 99/1000, LR 0.000265
Train loss: 1.0767;  Loss pred: 1.0767; Loss self: 0.0000; time: 0.17s
Val loss: 0.6819 score: 0.5455 time: 2.45s
Test loss: 0.6816 score: 0.5349 time: 2.81s
Epoch 100/1000, LR 0.000265
Train loss: 1.0804;  Loss pred: 1.0804; Loss self: 0.0000; time: 7.34s
Val loss: 0.6815 score: 0.5455 time: 2.74s
Test loss: 0.6811 score: 0.5349 time: 2.48s
Epoch 101/1000, LR 0.000265
Train loss: 1.0716;  Loss pred: 1.0716; Loss self: 0.0000; time: 1.84s
Val loss: 0.6810 score: 0.5455 time: 0.05s
Test loss: 0.6806 score: 0.5349 time: 0.04s
Epoch 102/1000, LR 0.000264
Train loss: 1.0685;  Loss pred: 1.0685; Loss self: 0.0000; time: 0.15s
Val loss: 0.6805 score: 0.5455 time: 0.05s
Test loss: 0.6801 score: 0.5349 time: 0.04s
Epoch 103/1000, LR 0.000264
Train loss: 1.0694;  Loss pred: 1.0694; Loss self: 0.0000; time: 0.14s
Val loss: 0.6800 score: 0.5455 time: 0.05s
Test loss: 0.6796 score: 0.5349 time: 0.04s
Epoch 104/1000, LR 0.000264
Train loss: 1.0598;  Loss pred: 1.0598; Loss self: 0.0000; time: 0.14s
Val loss: 0.6795 score: 0.5682 time: 0.05s
Test loss: 0.6791 score: 0.5349 time: 0.04s
Epoch 105/1000, LR 0.000264
Train loss: 1.0531;  Loss pred: 1.0531; Loss self: 0.0000; time: 0.14s
Val loss: 0.6790 score: 0.5682 time: 0.05s
Test loss: 0.6785 score: 0.5349 time: 0.04s
Epoch 106/1000, LR 0.000264
Train loss: 1.0519;  Loss pred: 1.0519; Loss self: 0.0000; time: 0.14s
Val loss: 0.6785 score: 0.5682 time: 0.05s
Test loss: 0.6780 score: 0.5349 time: 0.04s
Epoch 107/1000, LR 0.000264
Train loss: 1.0547;  Loss pred: 1.0547; Loss self: 0.0000; time: 0.14s
Val loss: 0.6779 score: 0.5682 time: 0.05s
Test loss: 0.6774 score: 0.5349 time: 0.04s
Epoch 108/1000, LR 0.000264
Train loss: 1.0526;  Loss pred: 1.0526; Loss self: 0.0000; time: 0.16s
Val loss: 0.6773 score: 0.5682 time: 0.05s
Test loss: 0.6768 score: 0.5349 time: 0.08s
Epoch 109/1000, LR 0.000264
Train loss: 1.0416;  Loss pred: 1.0416; Loss self: 0.0000; time: 0.17s
Val loss: 0.6767 score: 0.5682 time: 2.54s
Test loss: 0.6761 score: 0.5349 time: 2.04s
Epoch 110/1000, LR 0.000263
Train loss: 1.0434;  Loss pred: 1.0434; Loss self: 0.0000; time: 3.82s
Val loss: 0.6761 score: 0.5682 time: 0.23s
Test loss: 0.6755 score: 0.5349 time: 2.77s
Epoch 111/1000, LR 0.000263
Train loss: 1.0419;  Loss pred: 1.0419; Loss self: 0.0000; time: 0.37s
Val loss: 0.6755 score: 0.5682 time: 0.06s
Test loss: 0.6748 score: 0.5581 time: 0.05s
Epoch 112/1000, LR 0.000263
Train loss: 1.0430;  Loss pred: 1.0430; Loss self: 0.0000; time: 0.16s
Val loss: 0.6748 score: 0.5682 time: 0.05s
Test loss: 0.6741 score: 0.5349 time: 0.05s
Epoch 113/1000, LR 0.000263
Train loss: 1.0390;  Loss pred: 1.0390; Loss self: 0.0000; time: 0.16s
Val loss: 0.6741 score: 0.5682 time: 0.05s
Test loss: 0.6734 score: 0.5349 time: 0.05s
Epoch 114/1000, LR 0.000263
Train loss: 1.0329;  Loss pred: 1.0329; Loss self: 0.0000; time: 0.16s
Val loss: 0.6734 score: 0.5682 time: 0.05s
Test loss: 0.6727 score: 0.5349 time: 0.05s
Epoch 115/1000, LR 0.000263
Train loss: 1.0326;  Loss pred: 1.0326; Loss self: 0.0000; time: 0.15s
Val loss: 0.6727 score: 0.5682 time: 0.06s
Test loss: 0.6720 score: 0.5349 time: 0.05s
Epoch 116/1000, LR 0.000263
Train loss: 1.0314;  Loss pred: 1.0314; Loss self: 0.0000; time: 0.16s
Val loss: 0.6720 score: 0.5682 time: 0.06s
Test loss: 0.6712 score: 0.5349 time: 0.05s
Epoch 117/1000, LR 0.000262
Train loss: 1.0266;  Loss pred: 1.0266; Loss self: 0.0000; time: 0.16s
Val loss: 0.6712 score: 0.5909 time: 0.05s
Test loss: 0.6704 score: 0.5581 time: 0.05s
Epoch 118/1000, LR 0.000262
Train loss: 1.0249;  Loss pred: 1.0249; Loss self: 0.0000; time: 0.16s
Val loss: 0.6704 score: 0.6136 time: 0.88s
Test loss: 0.6696 score: 0.5581 time: 0.04s
Epoch 119/1000, LR 0.000262
Train loss: 1.0204;  Loss pred: 1.0204; Loss self: 0.0000; time: 0.15s
Val loss: 0.6695 score: 0.6591 time: 0.05s
Test loss: 0.6687 score: 0.5814 time: 0.05s
Epoch 120/1000, LR 0.000262
Train loss: 1.0217;  Loss pred: 1.0217; Loss self: 0.0000; time: 0.15s
Val loss: 0.6686 score: 0.6818 time: 0.05s
Test loss: 0.6678 score: 0.5814 time: 0.05s
Epoch 121/1000, LR 0.000262
Train loss: 1.0172;  Loss pred: 1.0172; Loss self: 0.0000; time: 0.15s
Val loss: 0.6677 score: 0.6818 time: 0.05s
Test loss: 0.6668 score: 0.5814 time: 0.05s
Epoch 122/1000, LR 0.000262
Train loss: 1.0158;  Loss pred: 1.0158; Loss self: 0.0000; time: 5.06s
Val loss: 0.6667 score: 0.6818 time: 0.19s
Test loss: 0.6658 score: 0.5814 time: 0.06s
Epoch 123/1000, LR 0.000262
Train loss: 1.0152;  Loss pred: 1.0152; Loss self: 0.0000; time: 0.39s
Val loss: 0.6657 score: 0.6818 time: 0.07s
Test loss: 0.6649 score: 0.5814 time: 0.05s
Epoch 124/1000, LR 0.000261
Train loss: 1.0137;  Loss pred: 1.0137; Loss self: 0.0000; time: 0.15s
Val loss: 0.6647 score: 0.6818 time: 0.06s
Test loss: 0.6639 score: 0.5814 time: 0.04s
Epoch 125/1000, LR 0.000261
Train loss: 1.0082;  Loss pred: 1.0082; Loss self: 0.0000; time: 0.15s
Val loss: 0.6636 score: 0.6818 time: 0.05s
Test loss: 0.6629 score: 0.6047 time: 0.04s
Epoch 126/1000, LR 0.000261
Train loss: 1.0091;  Loss pred: 1.0091; Loss self: 0.0000; time: 0.15s
Val loss: 0.6626 score: 0.7045 time: 0.05s
Test loss: 0.6618 score: 0.6047 time: 0.05s
Epoch 127/1000, LR 0.000261
Train loss: 1.0079;  Loss pred: 1.0079; Loss self: 0.0000; time: 0.15s
Val loss: 0.6616 score: 0.7045 time: 0.05s
Test loss: 0.6608 score: 0.6047 time: 0.04s
Epoch 128/1000, LR 0.000261
Train loss: 1.0026;  Loss pred: 1.0026; Loss self: 0.0000; time: 0.15s
Val loss: 0.6605 score: 0.7045 time: 0.05s
Test loss: 0.6597 score: 0.6047 time: 0.14s
Epoch 129/1000, LR 0.000261
Train loss: 1.0016;  Loss pred: 1.0016; Loss self: 0.0000; time: 0.15s
Val loss: 0.6594 score: 0.7045 time: 0.05s
Test loss: 0.6586 score: 0.6047 time: 0.05s
Epoch 130/1000, LR 0.000260
Train loss: 0.9956;  Loss pred: 0.9956; Loss self: 0.0000; time: 0.15s
Val loss: 0.6583 score: 0.7045 time: 0.05s
Test loss: 0.6575 score: 0.6047 time: 0.05s
Epoch 131/1000, LR 0.000260
Train loss: 0.9963;  Loss pred: 0.9963; Loss self: 0.0000; time: 0.15s
Val loss: 0.6572 score: 0.7273 time: 0.05s
Test loss: 0.6563 score: 0.6047 time: 0.04s
Epoch 132/1000, LR 0.000260
Train loss: 0.9963;  Loss pred: 0.9963; Loss self: 0.0000; time: 0.15s
Val loss: 0.6560 score: 0.7273 time: 0.05s
Test loss: 0.6551 score: 0.6047 time: 0.05s
Epoch 133/1000, LR 0.000260
Train loss: 0.9913;  Loss pred: 0.9913; Loss self: 0.0000; time: 0.15s
Val loss: 0.6548 score: 0.7500 time: 0.05s
Test loss: 0.6539 score: 0.6047 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 0.9860;  Loss pred: 0.9860; Loss self: 0.0000; time: 0.15s
Val loss: 0.6536 score: 0.7500 time: 0.05s
Test loss: 0.6527 score: 0.6279 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9890;  Loss pred: 0.9890; Loss self: 0.0000; time: 0.15s
Val loss: 0.6523 score: 0.7500 time: 0.06s
Test loss: 0.6514 score: 0.6512 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9891;  Loss pred: 0.9891; Loss self: 0.0000; time: 0.14s
Val loss: 0.6510 score: 0.7500 time: 0.05s
Test loss: 0.6501 score: 0.6512 time: 0.04s
Epoch 137/1000, LR 0.000259
Train loss: 0.9832;  Loss pred: 0.9832; Loss self: 0.0000; time: 0.15s
Val loss: 0.6497 score: 0.7727 time: 0.05s
Test loss: 0.6488 score: 0.6512 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9807;  Loss pred: 0.9807; Loss self: 0.0000; time: 0.15s
Val loss: 0.6484 score: 0.7727 time: 0.05s
Test loss: 0.6474 score: 0.6512 time: 0.04s
Epoch 139/1000, LR 0.000259
Train loss: 0.9799;  Loss pred: 0.9799; Loss self: 0.0000; time: 0.15s
Val loss: 0.6470 score: 0.7727 time: 0.05s
Test loss: 0.6461 score: 0.6744 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9751;  Loss pred: 0.9751; Loss self: 0.0000; time: 0.15s
Val loss: 0.6457 score: 0.7955 time: 0.05s
Test loss: 0.6447 score: 0.6744 time: 0.04s
Epoch 141/1000, LR 0.000259
Train loss: 0.9774;  Loss pred: 0.9774; Loss self: 0.0000; time: 0.14s
Val loss: 0.6442 score: 0.7955 time: 0.05s
Test loss: 0.6433 score: 0.6977 time: 0.05s
Epoch 142/1000, LR 0.000259
Train loss: 0.9737;  Loss pred: 0.9737; Loss self: 0.0000; time: 0.15s
Val loss: 0.6428 score: 0.7955 time: 0.05s
Test loss: 0.6418 score: 0.6744 time: 0.05s
Epoch 143/1000, LR 0.000258
Train loss: 0.9672;  Loss pred: 0.9672; Loss self: 0.0000; time: 0.15s
Val loss: 0.6413 score: 0.7955 time: 0.05s
Test loss: 0.6403 score: 0.6744 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9679;  Loss pred: 0.9679; Loss self: 0.0000; time: 0.53s
Val loss: 0.6398 score: 0.7955 time: 1.53s
Test loss: 0.6389 score: 0.6744 time: 1.59s
Epoch 145/1000, LR 0.000258
Train loss: 0.9662;  Loss pred: 0.9662; Loss self: 0.0000; time: 4.74s
Val loss: 0.6383 score: 0.7955 time: 0.11s
Test loss: 0.6374 score: 0.6744 time: 0.32s
Epoch 146/1000, LR 0.000258
Train loss: 0.9642;  Loss pred: 0.9642; Loss self: 0.0000; time: 0.84s
Val loss: 0.6368 score: 0.7955 time: 0.06s
Test loss: 0.6358 score: 0.6744 time: 0.05s
Epoch 147/1000, LR 0.000258
Train loss: 0.9631;  Loss pred: 0.9631; Loss self: 0.0000; time: 0.16s
Val loss: 0.6352 score: 0.7955 time: 0.05s
Test loss: 0.6343 score: 0.6744 time: 0.05s
Epoch 148/1000, LR 0.000257
Train loss: 0.9612;  Loss pred: 0.9612; Loss self: 0.0000; time: 0.16s
Val loss: 0.6336 score: 0.8182 time: 0.06s
Test loss: 0.6327 score: 0.6744 time: 0.05s
Epoch 149/1000, LR 0.000257
Train loss: 0.9597;  Loss pred: 0.9597; Loss self: 0.0000; time: 0.16s
Val loss: 0.6320 score: 0.8182 time: 0.05s
Test loss: 0.6311 score: 0.6744 time: 0.05s
Epoch 150/1000, LR 0.000257
Train loss: 0.9556;  Loss pred: 0.9556; Loss self: 0.0000; time: 0.16s
Val loss: 0.6303 score: 0.8182 time: 0.06s
Test loss: 0.6294 score: 0.6744 time: 0.05s
Epoch 151/1000, LR 0.000257
Train loss: 0.9546;  Loss pred: 0.9546; Loss self: 0.0000; time: 0.16s
Val loss: 0.6286 score: 0.8182 time: 0.05s
Test loss: 0.6277 score: 0.7442 time: 0.05s
Epoch 152/1000, LR 0.000257
Train loss: 0.9486;  Loss pred: 0.9486; Loss self: 0.0000; time: 0.16s
Val loss: 0.6269 score: 0.8182 time: 0.05s
Test loss: 0.6261 score: 0.7442 time: 0.05s
Epoch 153/1000, LR 0.000257
Train loss: 0.9523;  Loss pred: 0.9523; Loss self: 0.0000; time: 0.16s
Val loss: 0.6251 score: 0.8182 time: 0.06s
Test loss: 0.6244 score: 0.7674 time: 0.05s
Epoch 154/1000, LR 0.000256
Train loss: 0.9482;  Loss pred: 0.9482; Loss self: 0.0000; time: 0.16s
Val loss: 0.6233 score: 0.8182 time: 0.05s
Test loss: 0.6227 score: 0.7674 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9440;  Loss pred: 0.9440; Loss self: 0.0000; time: 0.16s
Val loss: 0.6215 score: 0.8182 time: 0.05s
Test loss: 0.6209 score: 0.7674 time: 0.05s
Epoch 156/1000, LR 0.000256
Train loss: 0.9437;  Loss pred: 0.9437; Loss self: 0.0000; time: 0.16s
Val loss: 0.6197 score: 0.8182 time: 0.06s
Test loss: 0.6191 score: 0.7674 time: 0.05s
Epoch 157/1000, LR 0.000256
Train loss: 0.9424;  Loss pred: 0.9424; Loss self: 0.0000; time: 0.40s
Val loss: 0.6178 score: 0.8182 time: 0.27s
Test loss: 0.6173 score: 0.7674 time: 0.48s
Epoch 158/1000, LR 0.000256
Train loss: 0.9375;  Loss pred: 0.9375; Loss self: 0.0000; time: 1.38s
Val loss: 0.6159 score: 0.8409 time: 1.59s
Test loss: 0.6154 score: 0.7674 time: 1.34s
Epoch 159/1000, LR 0.000255
Train loss: 0.9379;  Loss pred: 0.9379; Loss self: 0.0000; time: 0.65s
Val loss: 0.6139 score: 0.8409 time: 0.06s
Test loss: 0.6135 score: 0.7674 time: 0.04s
Epoch 160/1000, LR 0.000255
Train loss: 0.9337;  Loss pred: 0.9337; Loss self: 0.0000; time: 0.15s
Val loss: 0.6119 score: 0.8409 time: 0.05s
Test loss: 0.6116 score: 0.7674 time: 0.04s
Epoch 161/1000, LR 0.000255
Train loss: 0.9344;  Loss pred: 0.9344; Loss self: 0.0000; time: 0.15s
Val loss: 0.6099 score: 0.8636 time: 0.05s
Test loss: 0.6097 score: 0.7674 time: 0.04s
Epoch 162/1000, LR 0.000255
Train loss: 0.9299;  Loss pred: 0.9299; Loss self: 0.0000; time: 0.15s
Val loss: 0.6079 score: 0.8636 time: 0.05s
Test loss: 0.6077 score: 0.7907 time: 0.04s
Epoch 163/1000, LR 0.000255
Train loss: 0.9268;  Loss pred: 0.9268; Loss self: 0.0000; time: 0.15s
Val loss: 0.6059 score: 0.8864 time: 0.05s
Test loss: 0.6056 score: 0.7907 time: 0.04s
Epoch 164/1000, LR 0.000254
Train loss: 0.9233;  Loss pred: 0.9233; Loss self: 0.0000; time: 0.15s
Val loss: 0.6038 score: 0.8864 time: 0.05s
Test loss: 0.6036 score: 0.7907 time: 0.05s
Epoch 165/1000, LR 0.000254
Train loss: 0.9216;  Loss pred: 0.9216; Loss self: 0.0000; time: 0.15s
Val loss: 0.6017 score: 0.8864 time: 0.05s
Test loss: 0.6016 score: 0.7907 time: 0.04s
Epoch 166/1000, LR 0.000254
Train loss: 0.9199;  Loss pred: 0.9199; Loss self: 0.0000; time: 0.15s
Val loss: 0.5996 score: 0.8864 time: 0.05s
Test loss: 0.5995 score: 0.7907 time: 0.04s
Epoch 167/1000, LR 0.000254
Train loss: 0.9154;  Loss pred: 0.9154; Loss self: 0.0000; time: 0.15s
Val loss: 0.5975 score: 0.8864 time: 0.05s
Test loss: 0.5975 score: 0.7674 time: 0.04s
Epoch 168/1000, LR 0.000254
Train loss: 0.9140;  Loss pred: 0.9140; Loss self: 0.0000; time: 0.15s
Val loss: 0.5953 score: 0.9091 time: 0.05s
Test loss: 0.5954 score: 0.7674 time: 0.05s
Epoch 169/1000, LR 0.000253
Train loss: 0.9128;  Loss pred: 0.9128; Loss self: 0.0000; time: 0.15s
Val loss: 0.5931 score: 0.9091 time: 0.05s
Test loss: 0.5933 score: 0.7674 time: 0.05s
Epoch 170/1000, LR 0.000253
Train loss: 0.9104;  Loss pred: 0.9104; Loss self: 0.0000; time: 0.15s
Val loss: 0.5908 score: 0.9091 time: 0.05s
Test loss: 0.5912 score: 0.7907 time: 0.04s
Epoch 171/1000, LR 0.000253
Train loss: 0.9092;  Loss pred: 0.9092; Loss self: 0.0000; time: 0.14s
Val loss: 0.5885 score: 0.9091 time: 0.05s
Test loss: 0.5891 score: 0.7907 time: 0.04s
Epoch 172/1000, LR 0.000253
Train loss: 0.9038;  Loss pred: 0.9038; Loss self: 0.0000; time: 0.14s
Val loss: 0.5862 score: 0.9091 time: 0.05s
Test loss: 0.5870 score: 0.7907 time: 0.04s
Epoch 173/1000, LR 0.000253
Train loss: 0.9030;  Loss pred: 0.9030; Loss self: 0.0000; time: 0.14s
Val loss: 0.5838 score: 0.9091 time: 0.05s
Test loss: 0.5849 score: 0.7907 time: 0.04s
Epoch 174/1000, LR 0.000252
Train loss: 0.8984;  Loss pred: 0.8984; Loss self: 0.0000; time: 0.14s
Val loss: 0.5815 score: 0.9091 time: 0.05s
Test loss: 0.5826 score: 0.7907 time: 0.04s
Epoch 175/1000, LR 0.000252
Train loss: 0.8954;  Loss pred: 0.8954; Loss self: 0.0000; time: 0.14s
Val loss: 0.5791 score: 0.9091 time: 0.05s
Test loss: 0.5804 score: 0.8140 time: 0.04s
Epoch 176/1000, LR 0.000252
Train loss: 0.8951;  Loss pred: 0.8951; Loss self: 0.0000; time: 0.15s
Val loss: 0.5767 score: 0.9091 time: 0.05s
Test loss: 0.5782 score: 0.8140 time: 0.04s
Epoch 177/1000, LR 0.000252
Train loss: 0.8930;  Loss pred: 0.8930; Loss self: 0.0000; time: 0.15s
Val loss: 0.5742 score: 0.9091 time: 0.05s
Test loss: 0.5760 score: 0.8140 time: 0.05s
Epoch 178/1000, LR 0.000251
Train loss: 0.8908;  Loss pred: 0.8908; Loss self: 0.0000; time: 0.15s
Val loss: 0.5718 score: 0.9318 time: 0.05s
Test loss: 0.5738 score: 0.8140 time: 0.05s
Epoch 179/1000, LR 0.000251
Train loss: 0.8892;  Loss pred: 0.8892; Loss self: 0.0000; time: 0.15s
Val loss: 0.5693 score: 0.9318 time: 0.05s
Test loss: 0.5715 score: 0.8140 time: 0.04s
Epoch 180/1000, LR 0.000251
Train loss: 0.8845;  Loss pred: 0.8845; Loss self: 0.0000; time: 0.14s
Val loss: 0.5669 score: 0.9318 time: 0.05s
Test loss: 0.5693 score: 0.8140 time: 0.04s
Epoch 181/1000, LR 0.000251
Train loss: 0.8838;  Loss pred: 0.8838; Loss self: 0.0000; time: 0.14s
Val loss: 0.5644 score: 0.9318 time: 0.05s
Test loss: 0.5671 score: 0.8140 time: 0.04s
Epoch 182/1000, LR 0.000251
Train loss: 0.8781;  Loss pred: 0.8781; Loss self: 0.0000; time: 0.16s
Val loss: 0.5619 score: 0.9318 time: 0.05s
Test loss: 0.5648 score: 0.8140 time: 0.04s
Epoch 183/1000, LR 0.000250
Train loss: 0.8758;  Loss pred: 0.8758; Loss self: 0.0000; time: 0.15s
Val loss: 0.5594 score: 0.9318 time: 0.06s
Test loss: 0.5625 score: 0.8140 time: 0.04s
Epoch 184/1000, LR 0.000250
Train loss: 0.8762;  Loss pred: 0.8762; Loss self: 0.0000; time: 0.16s
Val loss: 0.5569 score: 0.9318 time: 0.05s
Test loss: 0.5602 score: 0.8140 time: 0.04s
Epoch 185/1000, LR 0.000250
Train loss: 0.8711;  Loss pred: 0.8711; Loss self: 0.0000; time: 0.16s
Val loss: 0.5544 score: 0.9318 time: 0.05s
Test loss: 0.5579 score: 0.8140 time: 0.05s
Epoch 186/1000, LR 0.000250
Train loss: 0.8712;  Loss pred: 0.8712; Loss self: 0.0000; time: 0.15s
Val loss: 0.5518 score: 0.9318 time: 0.05s
Test loss: 0.5556 score: 0.8140 time: 0.06s
Epoch 187/1000, LR 0.000249
Train loss: 0.8678;  Loss pred: 0.8678; Loss self: 0.0000; time: 0.15s
Val loss: 0.5493 score: 0.9318 time: 0.05s
Test loss: 0.5533 score: 0.8140 time: 0.05s
Epoch 188/1000, LR 0.000249
Train loss: 0.8625;  Loss pred: 0.8625; Loss self: 0.0000; time: 0.15s
Val loss: 0.5468 score: 0.9318 time: 0.08s
Test loss: 0.5509 score: 0.8140 time: 0.49s
Epoch 189/1000, LR 0.000249
Train loss: 0.8635;  Loss pred: 0.8635; Loss self: 0.0000; time: 4.08s
Val loss: 0.5443 score: 0.9318 time: 0.20s
Test loss: 0.5485 score: 0.8140 time: 0.27s
Epoch 190/1000, LR 0.000249
Train loss: 0.8602;  Loss pred: 0.8602; Loss self: 0.0000; time: 0.36s
Val loss: 0.5417 score: 0.9318 time: 0.23s
Test loss: 0.5461 score: 0.8140 time: 0.22s
Epoch 191/1000, LR 0.000249
Train loss: 0.8585;  Loss pred: 0.8585; Loss self: 0.0000; time: 0.60s
Val loss: 0.5392 score: 0.9318 time: 0.27s
Test loss: 0.5438 score: 0.8140 time: 0.43s
Epoch 192/1000, LR 0.000248
Train loss: 0.8551;  Loss pred: 0.8551; Loss self: 0.0000; time: 1.21s
Val loss: 0.5366 score: 0.9318 time: 0.16s
Test loss: 0.5414 score: 0.8140 time: 0.14s
Epoch 193/1000, LR 0.000248
Train loss: 0.8500;  Loss pred: 0.8500; Loss self: 0.0000; time: 0.15s
Val loss: 0.5341 score: 0.9318 time: 0.05s
Test loss: 0.5391 score: 0.8140 time: 0.05s
Epoch 194/1000, LR 0.000248
Train loss: 0.8491;  Loss pred: 0.8491; Loss self: 0.0000; time: 0.15s
Val loss: 0.5315 score: 0.9318 time: 0.05s
Test loss: 0.5368 score: 0.8140 time: 0.05s
Epoch 195/1000, LR 0.000248
Train loss: 0.8473;  Loss pred: 0.8473; Loss self: 0.0000; time: 0.15s
Val loss: 0.5288 score: 0.9318 time: 0.05s
Test loss: 0.5346 score: 0.8140 time: 0.05s
Epoch 196/1000, LR 0.000247
Train loss: 0.8441;  Loss pred: 0.8441; Loss self: 0.0000; time: 0.15s
Val loss: 0.5262 score: 0.9318 time: 0.05s
Test loss: 0.5325 score: 0.8140 time: 0.05s
Epoch 197/1000, LR 0.000247
Train loss: 0.8420;  Loss pred: 0.8420; Loss self: 0.0000; time: 0.15s
Val loss: 0.5235 score: 0.9318 time: 0.05s
Test loss: 0.5303 score: 0.8140 time: 0.05s
Epoch 198/1000, LR 0.000247
Train loss: 0.8399;  Loss pred: 0.8399; Loss self: 0.0000; time: 0.15s
Val loss: 0.5209 score: 0.9318 time: 0.05s
Test loss: 0.5282 score: 0.8140 time: 0.05s
Epoch 199/1000, LR 0.000247
Train loss: 0.8364;  Loss pred: 0.8364; Loss self: 0.0000; time: 0.15s
Val loss: 0.5182 score: 0.9318 time: 0.06s
Test loss: 0.5261 score: 0.8140 time: 0.05s
Epoch 200/1000, LR 0.000246
Train loss: 0.8329;  Loss pred: 0.8329; Loss self: 0.0000; time: 0.15s
Val loss: 0.5156 score: 0.9318 time: 0.05s
Test loss: 0.5240 score: 0.8140 time: 0.05s
Epoch 201/1000, LR 0.000246
Train loss: 0.8309;  Loss pred: 0.8309; Loss self: 0.0000; time: 0.15s
Val loss: 0.5129 score: 0.9318 time: 0.05s
Test loss: 0.5219 score: 0.8140 time: 0.05s
Epoch 202/1000, LR 0.000246
Train loss: 0.8275;  Loss pred: 0.8275; Loss self: 0.0000; time: 0.15s
Val loss: 0.5103 score: 0.9318 time: 0.05s
Test loss: 0.5198 score: 0.8140 time: 0.25s
Epoch 203/1000, LR 0.000246
Train loss: 0.8269;  Loss pred: 0.8269; Loss self: 0.0000; time: 3.84s
Val loss: 0.5076 score: 0.9318 time: 0.79s
Test loss: 0.5176 score: 0.8140 time: 0.92s
Epoch 204/1000, LR 0.000245
Train loss: 0.8243;  Loss pred: 0.8243; Loss self: 0.0000; time: 0.77s
Val loss: 0.5050 score: 0.9318 time: 0.06s
Test loss: 0.5154 score: 0.8140 time: 0.05s
Epoch 205/1000, LR 0.000245
Train loss: 0.8225;  Loss pred: 0.8225; Loss self: 0.0000; time: 0.16s
Val loss: 0.5024 score: 0.9318 time: 0.05s
Test loss: 0.5132 score: 0.8140 time: 0.05s
Epoch 206/1000, LR 0.000245
Train loss: 0.8203;  Loss pred: 0.8203; Loss self: 0.0000; time: 0.15s
Val loss: 0.4998 score: 0.9318 time: 0.05s
Test loss: 0.5110 score: 0.8140 time: 0.04s
Epoch 207/1000, LR 0.000245
Train loss: 0.8158;  Loss pred: 0.8158; Loss self: 0.0000; time: 0.14s
Val loss: 0.4971 score: 0.9318 time: 0.05s
Test loss: 0.5088 score: 0.8140 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.8121;  Loss pred: 0.8121; Loss self: 0.0000; time: 0.14s
Val loss: 0.4945 score: 0.9318 time: 0.05s
Test loss: 0.5066 score: 0.8140 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.8114;  Loss pred: 0.8114; Loss self: 0.0000; time: 0.14s
Val loss: 0.4919 score: 0.9318 time: 0.05s
Test loss: 0.5044 score: 0.8140 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.8087;  Loss pred: 0.8087; Loss self: 0.0000; time: 0.14s
Val loss: 0.4892 score: 0.9318 time: 0.05s
Test loss: 0.5024 score: 0.8140 time: 0.05s
Epoch 211/1000, LR 0.000244
Train loss: 0.8062;  Loss pred: 0.8062; Loss self: 0.0000; time: 0.14s
Val loss: 0.4866 score: 0.9318 time: 0.05s
Test loss: 0.5003 score: 0.8140 time: 0.04s
Epoch 212/1000, LR 0.000243
Train loss: 0.8025;  Loss pred: 0.8025; Loss self: 0.0000; time: 0.14s
Val loss: 0.4840 score: 0.9318 time: 0.05s
Test loss: 0.4983 score: 0.8140 time: 0.04s
Epoch 213/1000, LR 0.000243
Train loss: 0.8023;  Loss pred: 0.8023; Loss self: 0.0000; time: 0.14s
Val loss: 0.4813 score: 0.9318 time: 0.05s
Test loss: 0.4964 score: 0.8140 time: 0.04s
Epoch 214/1000, LR 0.000243
Train loss: 0.7992;  Loss pred: 0.7992; Loss self: 0.0000; time: 0.15s
Val loss: 0.4787 score: 0.9318 time: 0.05s
Test loss: 0.4945 score: 0.8140 time: 0.05s
Epoch 215/1000, LR 0.000243
Train loss: 0.7984;  Loss pred: 0.7984; Loss self: 0.0000; time: 0.15s
Val loss: 0.4760 score: 0.9318 time: 0.05s
Test loss: 0.4926 score: 0.8140 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.7949;  Loss pred: 0.7949; Loss self: 0.0000; time: 0.15s
Val loss: 0.4734 score: 0.9318 time: 0.05s
Test loss: 0.4907 score: 0.8140 time: 0.05s
Epoch 217/1000, LR 0.000242
Train loss: 0.7913;  Loss pred: 0.7913; Loss self: 0.0000; time: 0.15s
Val loss: 0.4708 score: 0.9318 time: 0.06s
Test loss: 0.4889 score: 0.8140 time: 0.05s
Epoch 218/1000, LR 0.000242
Train loss: 0.7881;  Loss pred: 0.7881; Loss self: 0.0000; time: 0.15s
Val loss: 0.4682 score: 0.9318 time: 0.05s
Test loss: 0.4870 score: 0.8140 time: 0.05s
Epoch 219/1000, LR 0.000242
Train loss: 0.7860;  Loss pred: 0.7860; Loss self: 0.0000; time: 0.15s
Val loss: 0.4656 score: 0.9091 time: 0.05s
Test loss: 0.4852 score: 0.8140 time: 0.05s
Epoch 220/1000, LR 0.000241
Train loss: 0.7814;  Loss pred: 0.7814; Loss self: 0.0000; time: 0.15s
Val loss: 0.4630 score: 0.9091 time: 0.05s
Test loss: 0.4833 score: 0.8140 time: 0.05s
Epoch 221/1000, LR 0.000241
Train loss: 0.7811;  Loss pred: 0.7811; Loss self: 0.0000; time: 0.15s
Val loss: 0.4605 score: 0.9091 time: 0.05s
Test loss: 0.4815 score: 0.8140 time: 0.05s
Epoch 222/1000, LR 0.000241
Train loss: 0.7829;  Loss pred: 0.7829; Loss self: 0.0000; time: 0.16s
Val loss: 0.4579 score: 0.9091 time: 0.05s
Test loss: 0.4797 score: 0.8140 time: 0.04s
Epoch 223/1000, LR 0.000241
Train loss: 0.7792;  Loss pred: 0.7792; Loss self: 0.0000; time: 0.14s
Val loss: 0.4554 score: 0.9091 time: 0.05s
Test loss: 0.4779 score: 0.8140 time: 0.04s
Epoch 224/1000, LR 0.000240
Train loss: 0.7736;  Loss pred: 0.7736; Loss self: 0.0000; time: 0.14s
Val loss: 0.4529 score: 0.9091 time: 0.05s
Test loss: 0.4761 score: 0.8140 time: 0.05s
Epoch 225/1000, LR 0.000240
Train loss: 0.7743;  Loss pred: 0.7743; Loss self: 0.0000; time: 0.14s
Val loss: 0.4505 score: 0.9091 time: 0.05s
Test loss: 0.4744 score: 0.8140 time: 0.05s
Epoch 226/1000, LR 0.000240
Train loss: 0.7754;  Loss pred: 0.7754; Loss self: 0.0000; time: 0.14s
Val loss: 0.4480 score: 0.9091 time: 0.05s
Test loss: 0.4727 score: 0.8140 time: 0.04s
Epoch 227/1000, LR 0.000240
Train loss: 0.7693;  Loss pred: 0.7693; Loss self: 0.0000; time: 0.14s
Val loss: 0.4455 score: 0.9091 time: 0.05s
Test loss: 0.4711 score: 0.8140 time: 0.04s
Epoch 228/1000, LR 0.000239
Train loss: 0.7642;  Loss pred: 0.7642; Loss self: 0.0000; time: 0.15s
Val loss: 0.4430 score: 0.9091 time: 0.05s
Test loss: 0.4695 score: 0.8140 time: 0.04s
Epoch 229/1000, LR 0.000239
Train loss: 0.7648;  Loss pred: 0.7648; Loss self: 0.0000; time: 0.16s
Val loss: 0.4406 score: 0.9091 time: 0.12s
Test loss: 0.4679 score: 0.8140 time: 0.05s
Epoch 230/1000, LR 0.000239
Train loss: 0.7608;  Loss pred: 0.7608; Loss self: 0.0000; time: 0.15s
Val loss: 0.4381 score: 0.9091 time: 0.05s
Test loss: 0.4663 score: 0.8140 time: 0.04s
Epoch 231/1000, LR 0.000238
Train loss: 0.7593;  Loss pred: 0.7593; Loss self: 0.0000; time: 0.14s
Val loss: 0.4357 score: 0.9091 time: 0.05s
Test loss: 0.4648 score: 0.8140 time: 0.04s
Epoch 232/1000, LR 0.000238
Train loss: 0.7571;  Loss pred: 0.7571; Loss self: 0.0000; time: 0.14s
Val loss: 0.4333 score: 0.9091 time: 0.05s
Test loss: 0.4632 score: 0.8140 time: 0.04s
Epoch 233/1000, LR 0.000238
Train loss: 0.7530;  Loss pred: 0.7530; Loss self: 0.0000; time: 0.14s
Val loss: 0.4310 score: 0.9091 time: 0.05s
Test loss: 0.4616 score: 0.8140 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.7532;  Loss pred: 0.7532; Loss self: 0.0000; time: 0.14s
Val loss: 0.4287 score: 0.9091 time: 0.05s
Test loss: 0.4599 score: 0.8140 time: 0.04s
Epoch 235/1000, LR 0.000237
Train loss: 0.7491;  Loss pred: 0.7491; Loss self: 0.0000; time: 0.15s
Val loss: 0.4265 score: 0.9318 time: 0.05s
Test loss: 0.4581 score: 0.8140 time: 0.05s
Epoch 236/1000, LR 0.000237
Train loss: 0.7515;  Loss pred: 0.7515; Loss self: 0.0000; time: 0.15s
Val loss: 0.4242 score: 0.9318 time: 0.05s
Test loss: 0.4566 score: 0.8140 time: 0.15s
Epoch 237/1000, LR 0.000237
Train loss: 0.7457;  Loss pred: 0.7457; Loss self: 0.0000; time: 0.16s
Val loss: 0.4219 score: 0.9318 time: 0.05s
Test loss: 0.4552 score: 0.8140 time: 0.05s
Epoch 238/1000, LR 0.000236
Train loss: 0.7414;  Loss pred: 0.7414; Loss self: 0.0000; time: 0.57s
Val loss: 0.4195 score: 0.9318 time: 0.61s
Test loss: 0.4539 score: 0.8140 time: 0.77s
Epoch 239/1000, LR 0.000236
Train loss: 0.7430;  Loss pred: 0.7430; Loss self: 0.0000; time: 5.95s
Val loss: 0.4171 score: 0.9318 time: 1.47s
Test loss: 0.4527 score: 0.8140 time: 0.12s
Epoch 240/1000, LR 0.000236
Train loss: 0.7381;  Loss pred: 0.7381; Loss self: 0.0000; time: 0.16s
Val loss: 0.4147 score: 0.9318 time: 0.05s
Test loss: 0.4516 score: 0.8140 time: 0.05s
Epoch 241/1000, LR 0.000236
Train loss: 0.7394;  Loss pred: 0.7394; Loss self: 0.0000; time: 0.16s
Val loss: 0.4123 score: 0.9091 time: 0.06s
Test loss: 0.4506 score: 0.8140 time: 0.05s
Epoch 242/1000, LR 0.000235
Train loss: 0.7348;  Loss pred: 0.7348; Loss self: 0.0000; time: 0.16s
Val loss: 0.4100 score: 0.9091 time: 0.05s
Test loss: 0.4494 score: 0.8140 time: 0.05s
Epoch 243/1000, LR 0.000235
Train loss: 0.7333;  Loss pred: 0.7333; Loss self: 0.0000; time: 0.15s
Val loss: 0.4078 score: 0.9091 time: 0.05s
Test loss: 0.4482 score: 0.8140 time: 0.05s
Epoch 244/1000, LR 0.000235
Train loss: 0.7312;  Loss pred: 0.7312; Loss self: 0.0000; time: 0.15s
Val loss: 0.4056 score: 0.9091 time: 0.05s
Test loss: 0.4470 score: 0.8140 time: 0.04s
Epoch 245/1000, LR 0.000234
Train loss: 0.7295;  Loss pred: 0.7295; Loss self: 0.0000; time: 0.15s
Val loss: 0.4035 score: 0.9318 time: 0.05s
Test loss: 0.4455 score: 0.8140 time: 0.04s
Epoch 246/1000, LR 0.000234
Train loss: 0.7290;  Loss pred: 0.7290; Loss self: 0.0000; time: 0.15s
Val loss: 0.4013 score: 0.9318 time: 0.05s
Test loss: 0.4443 score: 0.8140 time: 0.05s
Epoch 247/1000, LR 0.000234
Train loss: 0.7243;  Loss pred: 0.7243; Loss self: 0.0000; time: 0.15s
Val loss: 0.3992 score: 0.9318 time: 0.05s
Test loss: 0.4431 score: 0.8140 time: 0.05s
Epoch 248/1000, LR 0.000234
Train loss: 0.7237;  Loss pred: 0.7237; Loss self: 0.0000; time: 0.15s
Val loss: 0.3970 score: 0.9318 time: 0.05s
Test loss: 0.4421 score: 0.8140 time: 0.04s
Epoch 249/1000, LR 0.000233
Train loss: 0.7245;  Loss pred: 0.7245; Loss self: 0.0000; time: 0.15s
Val loss: 0.3948 score: 0.9318 time: 0.05s
Test loss: 0.4410 score: 0.8140 time: 0.05s
Epoch 250/1000, LR 0.000233
Train loss: 0.7179;  Loss pred: 0.7179; Loss self: 0.0000; time: 3.99s
Val loss: 0.3928 score: 0.9318 time: 0.14s
Test loss: 0.4398 score: 0.8140 time: 0.38s
Epoch 251/1000, LR 0.000233
Train loss: 0.7181;  Loss pred: 0.7181; Loss self: 0.0000; time: 0.21s
Val loss: 0.3908 score: 0.9318 time: 0.06s
Test loss: 0.4384 score: 0.8140 time: 0.05s
Epoch 252/1000, LR 0.000232
Train loss: 0.7137;  Loss pred: 0.7137; Loss self: 0.0000; time: 0.16s
Val loss: 0.3889 score: 0.9318 time: 0.05s
Test loss: 0.4371 score: 0.8140 time: 0.05s
Epoch 253/1000, LR 0.000232
Train loss: 0.7127;  Loss pred: 0.7127; Loss self: 0.0000; time: 0.15s
Val loss: 0.3869 score: 0.9318 time: 0.05s
Test loss: 0.4360 score: 0.8140 time: 0.05s
Epoch 254/1000, LR 0.000232
Train loss: 0.7130;  Loss pred: 0.7130; Loss self: 0.0000; time: 0.15s
Val loss: 0.3848 score: 0.9318 time: 0.05s
Test loss: 0.4350 score: 0.8140 time: 0.05s
Epoch 255/1000, LR 0.000232
Train loss: 0.7094;  Loss pred: 0.7094; Loss self: 0.0000; time: 0.15s
Val loss: 0.3828 score: 0.9318 time: 0.05s
Test loss: 0.4341 score: 0.8140 time: 0.04s
Epoch 256/1000, LR 0.000231
Train loss: 0.7071;  Loss pred: 0.7071; Loss self: 0.0000; time: 0.15s
Val loss: 0.3808 score: 0.9318 time: 0.05s
Test loss: 0.4332 score: 0.8140 time: 0.04s
Epoch 257/1000, LR 0.000231
Train loss: 0.7085;  Loss pred: 0.7085; Loss self: 0.0000; time: 0.15s
Val loss: 0.3786 score: 0.9318 time: 0.05s
Test loss: 0.4325 score: 0.8140 time: 0.04s
Epoch 258/1000, LR 0.000231
Train loss: 0.7077;  Loss pred: 0.7077; Loss self: 0.0000; time: 0.15s
Val loss: 0.3765 score: 0.9318 time: 0.05s
Test loss: 0.4319 score: 0.8140 time: 0.04s
Epoch 259/1000, LR 0.000230
Train loss: 0.7022;  Loss pred: 0.7022; Loss self: 0.0000; time: 0.15s
Val loss: 0.3743 score: 0.9318 time: 0.05s
Test loss: 0.4315 score: 0.8140 time: 0.04s
Epoch 260/1000, LR 0.000230
Train loss: 0.7031;  Loss pred: 0.7031; Loss self: 0.0000; time: 0.14s
Val loss: 0.3722 score: 0.9091 time: 0.05s
Test loss: 0.4310 score: 0.8140 time: 0.05s
Epoch 261/1000, LR 0.000230
Train loss: 0.7011;  Loss pred: 0.7011; Loss self: 0.0000; time: 0.15s
Val loss: 0.3701 score: 0.9091 time: 0.05s
Test loss: 0.4305 score: 0.8140 time: 0.04s
Epoch 262/1000, LR 0.000229
Train loss: 0.7000;  Loss pred: 0.7000; Loss self: 0.0000; time: 0.14s
Val loss: 0.3681 score: 0.9091 time: 0.05s
Test loss: 0.4298 score: 0.8140 time: 0.04s
Epoch 263/1000, LR 0.000229
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.14s
Val loss: 0.3663 score: 0.9091 time: 0.05s
Test loss: 0.4290 score: 0.8140 time: 0.05s
Epoch 264/1000, LR 0.000229
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.15s
Val loss: 0.3645 score: 0.9091 time: 0.05s
Test loss: 0.4279 score: 0.8140 time: 0.04s
Epoch 265/1000, LR 0.000228
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.14s
Val loss: 0.3628 score: 0.9318 time: 0.05s
Test loss: 0.4268 score: 0.8140 time: 0.05s
Epoch 266/1000, LR 0.000228
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.15s
Val loss: 0.3611 score: 0.9318 time: 0.05s
Test loss: 0.4258 score: 0.8140 time: 0.04s
Epoch 267/1000, LR 0.000228
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.14s
Val loss: 0.3595 score: 0.9318 time: 0.05s
Test loss: 0.4247 score: 0.8140 time: 0.05s
Epoch 268/1000, LR 0.000228
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.15s
Val loss: 0.3579 score: 0.9318 time: 0.05s
Test loss: 0.4237 score: 0.8140 time: 0.04s
Epoch 269/1000, LR 0.000227
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.14s
Val loss: 0.3563 score: 0.9318 time: 0.05s
Test loss: 0.4226 score: 0.8140 time: 0.04s
Epoch 270/1000, LR 0.000227
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.15s
Val loss: 0.3547 score: 0.9318 time: 0.05s
Test loss: 0.4216 score: 0.8140 time: 0.04s
Epoch 271/1000, LR 0.000227
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.15s
Val loss: 0.3531 score: 0.9318 time: 0.05s
Test loss: 0.4208 score: 0.8140 time: 0.04s
Epoch 272/1000, LR 0.000226
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.15s
Val loss: 0.3512 score: 0.9318 time: 0.05s
Test loss: 0.4204 score: 0.8140 time: 2.06s
Epoch 273/1000, LR 0.000226
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 2.17s
Val loss: 0.3493 score: 0.9318 time: 0.12s
Test loss: 0.4201 score: 0.8140 time: 0.35s
Epoch 274/1000, LR 0.000226
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.52s
Val loss: 0.3475 score: 0.9318 time: 0.07s
Test loss: 0.4198 score: 0.8140 time: 0.05s
Epoch 275/1000, LR 0.000225
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.16s
Val loss: 0.3456 score: 0.9318 time: 0.05s
Test loss: 0.4195 score: 0.8140 time: 0.05s
Epoch 276/1000, LR 0.000225
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.16s
Val loss: 0.3439 score: 0.9318 time: 0.05s
Test loss: 0.4191 score: 0.8140 time: 0.05s
Epoch 277/1000, LR 0.000225
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.16s
Val loss: 0.3421 score: 0.9318 time: 0.05s
Test loss: 0.4187 score: 0.8140 time: 0.05s
Epoch 278/1000, LR 0.000224
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.16s
Val loss: 0.3405 score: 0.9318 time: 0.05s
Test loss: 0.4182 score: 0.8140 time: 0.05s
Epoch 279/1000, LR 0.000224
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.16s
Val loss: 0.3390 score: 0.9318 time: 0.05s
Test loss: 0.4174 score: 0.8140 time: 0.05s
Epoch 280/1000, LR 0.000224
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.16s
Val loss: 0.3376 score: 0.9318 time: 0.05s
Test loss: 0.4164 score: 0.8140 time: 0.05s
Epoch 281/1000, LR 0.000223
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.16s
Val loss: 0.3362 score: 0.9318 time: 0.05s
Test loss: 0.4156 score: 0.8140 time: 0.05s
Epoch 282/1000, LR 0.000223
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.16s
Val loss: 0.3346 score: 0.9318 time: 0.05s
Test loss: 0.4150 score: 0.8140 time: 0.05s
Epoch 283/1000, LR 0.000223
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.16s
Val loss: 0.3330 score: 0.9318 time: 0.05s
Test loss: 0.4147 score: 0.8140 time: 0.05s
Epoch 284/1000, LR 0.000222
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 0.16s
Val loss: 0.3314 score: 0.9318 time: 0.05s
Test loss: 0.4143 score: 0.8140 time: 0.05s
Epoch 285/1000, LR 0.000222
Train loss: 0.6625;  Loss pred: 0.6625; Loss self: 0.0000; time: 0.16s
Val loss: 0.3298 score: 0.9318 time: 0.05s
Test loss: 0.4139 score: 0.8140 time: 0.05s
Epoch 286/1000, LR 0.000222
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.16s
Val loss: 0.3283 score: 0.9318 time: 0.06s
Test loss: 0.4135 score: 0.8140 time: 0.05s
Epoch 287/1000, LR 0.000221
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.15s
Val loss: 0.3267 score: 0.9318 time: 0.05s
Test loss: 0.4132 score: 0.8140 time: 0.05s
Epoch 288/1000, LR 0.000221
Train loss: 0.6613;  Loss pred: 0.6613; Loss self: 0.0000; time: 0.15s
Val loss: 0.3253 score: 0.9318 time: 0.05s
Test loss: 0.4128 score: 0.8140 time: 0.04s
Epoch 289/1000, LR 0.000221
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.15s
Val loss: 0.3238 score: 0.9318 time: 0.05s
Test loss: 0.4123 score: 0.8140 time: 0.04s
Epoch 290/1000, LR 0.000220
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.15s
Val loss: 0.3224 score: 0.9318 time: 0.05s
Test loss: 0.4119 score: 0.8140 time: 0.04s
Epoch 291/1000, LR 0.000220
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.15s
Val loss: 0.3207 score: 0.9318 time: 0.05s
Test loss: 0.4119 score: 0.8140 time: 0.05s
Epoch 292/1000, LR 0.000220
Train loss: 0.6577;  Loss pred: 0.6577; Loss self: 0.0000; time: 0.15s
Val loss: 0.3191 score: 0.9318 time: 0.05s
Test loss: 0.4118 score: 0.8140 time: 0.05s
Epoch 293/1000, LR 0.000219
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 0.15s
Val loss: 0.3176 score: 0.9318 time: 0.05s
Test loss: 0.4115 score: 0.8140 time: 0.05s
Epoch 294/1000, LR 0.000219
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.15s
Val loss: 0.3163 score: 0.9318 time: 0.05s
Test loss: 0.4110 score: 0.8140 time: 0.05s
Epoch 295/1000, LR 0.000219
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.15s
Val loss: 0.3149 score: 0.9318 time: 0.05s
Test loss: 0.4105 score: 0.8140 time: 0.05s
Epoch 296/1000, LR 0.000218
Train loss: 0.6463;  Loss pred: 0.6463; Loss self: 0.0000; time: 0.15s
Val loss: 0.3137 score: 0.9318 time: 0.05s
Test loss: 0.4099 score: 0.8140 time: 0.05s
Epoch 297/1000, LR 0.000218
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.15s
Val loss: 0.3123 score: 0.9318 time: 0.05s
Test loss: 0.4095 score: 0.8372 time: 0.04s
Epoch 298/1000, LR 0.000218
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 0.16s
Val loss: 0.3110 score: 0.9318 time: 0.06s
Test loss: 0.4091 score: 0.8372 time: 0.05s
Epoch 299/1000, LR 0.000217
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.16s
Val loss: 0.3097 score: 0.9318 time: 0.05s
Test loss: 0.4086 score: 0.8372 time: 0.05s
Epoch 300/1000, LR 0.000217
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.16s
Val loss: 0.3083 score: 0.9318 time: 0.05s
Test loss: 0.4083 score: 0.8372 time: 0.05s
Epoch 301/1000, LR 0.000217
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.16s
Val loss: 0.3070 score: 0.9318 time: 0.05s
Test loss: 0.4081 score: 0.8372 time: 0.05s
Epoch 302/1000, LR 0.000216
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.16s
Val loss: 0.3056 score: 0.9318 time: 0.05s
Test loss: 0.4078 score: 0.8372 time: 0.05s
Epoch 303/1000, LR 0.000216
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.16s
Val loss: 0.3043 score: 0.9318 time: 0.05s
Test loss: 0.4075 score: 0.8372 time: 0.05s
Epoch 304/1000, LR 0.000216
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.15s
Val loss: 0.3030 score: 0.9318 time: 0.05s
Test loss: 0.4073 score: 0.8372 time: 0.05s
Epoch 305/1000, LR 0.000215
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.16s
Val loss: 0.3016 score: 0.9318 time: 0.05s
Test loss: 0.4073 score: 0.8372 time: 0.05s
Epoch 306/1000, LR 0.000215
Train loss: 0.6415;  Loss pred: 0.6415; Loss self: 0.0000; time: 0.16s
Val loss: 0.3003 score: 0.9318 time: 0.06s
Test loss: 0.4070 score: 0.8372 time: 0.05s
Epoch 307/1000, LR 0.000215
Train loss: 0.6393;  Loss pred: 0.6393; Loss self: 0.0000; time: 0.16s
Val loss: 0.2991 score: 0.9318 time: 0.05s
Test loss: 0.4068 score: 0.8372 time: 0.05s
Epoch 308/1000, LR 0.000214
Train loss: 0.6363;  Loss pred: 0.6363; Loss self: 0.0000; time: 0.16s
Val loss: 0.2979 score: 0.9318 time: 0.05s
Test loss: 0.4066 score: 0.8372 time: 0.05s
Epoch 309/1000, LR 0.000214
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.16s
Val loss: 0.2966 score: 0.9318 time: 0.05s
Test loss: 0.4064 score: 0.8372 time: 0.05s
Epoch 310/1000, LR 0.000214
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 0.16s
Val loss: 0.2954 score: 0.9318 time: 0.05s
Test loss: 0.4063 score: 0.8372 time: 0.05s
Epoch 311/1000, LR 0.000213
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.16s
Val loss: 0.2940 score: 0.9318 time: 0.05s
Test loss: 0.4063 score: 0.8372 time: 0.05s
Epoch 312/1000, LR 0.000213
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 1.92s
Val loss: 0.2928 score: 0.9318 time: 1.97s
Test loss: 0.4061 score: 0.8372 time: 1.48s
Epoch 313/1000, LR 0.000213
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 5.20s
Val loss: 0.2916 score: 0.9318 time: 0.83s
Test loss: 0.4059 score: 0.8372 time: 0.88s
Epoch 314/1000, LR 0.000212
Train loss: 0.6287;  Loss pred: 0.6287; Loss self: 0.0000; time: 1.32s
Val loss: 0.2905 score: 0.9318 time: 0.14s
Test loss: 0.4057 score: 0.8372 time: 0.06s
Epoch 315/1000, LR 0.000212
Train loss: 0.6291;  Loss pred: 0.6291; Loss self: 0.0000; time: 0.18s
Val loss: 0.2895 score: 0.9318 time: 0.05s
Test loss: 0.4053 score: 0.8372 time: 0.05s
Epoch 316/1000, LR 0.000212
Train loss: 0.6276;  Loss pred: 0.6276; Loss self: 0.0000; time: 0.15s
Val loss: 0.2884 score: 0.9318 time: 0.05s
Test loss: 0.4049 score: 0.8372 time: 0.05s
Epoch 317/1000, LR 0.000211
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.14s
Val loss: 0.2874 score: 0.9318 time: 0.05s
Test loss: 0.4047 score: 0.8372 time: 0.04s
Epoch 318/1000, LR 0.000211
Train loss: 0.6263;  Loss pred: 0.6263; Loss self: 0.0000; time: 0.14s
Val loss: 0.2862 score: 0.9318 time: 0.05s
Test loss: 0.4046 score: 0.8372 time: 0.05s
Epoch 319/1000, LR 0.000210
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.15s
Val loss: 0.2849 score: 0.9318 time: 0.05s
Test loss: 0.4048 score: 0.8372 time: 0.05s
Epoch 320/1000, LR 0.000210
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.15s
Val loss: 0.2836 score: 0.9318 time: 0.05s
Test loss: 0.4050 score: 0.8372 time: 0.05s
Epoch 321/1000, LR 0.000210
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.16s
Val loss: 0.2824 score: 0.9318 time: 0.05s
Test loss: 0.4050 score: 0.8372 time: 0.05s
Epoch 322/1000, LR 0.000209
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.16s
Val loss: 0.2813 score: 0.9318 time: 0.05s
Test loss: 0.4049 score: 0.8372 time: 0.05s
Epoch 323/1000, LR 0.000209
Train loss: 0.6199;  Loss pred: 0.6199; Loss self: 0.0000; time: 0.16s
Val loss: 0.2802 score: 0.9318 time: 0.05s
Test loss: 0.4048 score: 0.8372 time: 0.05s
Epoch 324/1000, LR 0.000209
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.15s
Val loss: 0.2793 score: 0.9318 time: 0.05s
Test loss: 0.4044 score: 0.8372 time: 0.05s
Epoch 325/1000, LR 0.000208
Train loss: 0.6184;  Loss pred: 0.6184; Loss self: 0.0000; time: 0.15s
Val loss: 0.2784 score: 0.9318 time: 0.05s
Test loss: 0.4039 score: 0.8372 time: 0.05s
Epoch 326/1000, LR 0.000208
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 0.16s
Val loss: 0.2775 score: 0.9318 time: 0.05s
Test loss: 0.4035 score: 0.8372 time: 0.05s
Epoch 327/1000, LR 0.000208
Train loss: 0.6204;  Loss pred: 0.6204; Loss self: 0.0000; time: 0.15s
Val loss: 0.2766 score: 0.9318 time: 0.05s
Test loss: 0.4033 score: 0.8372 time: 0.05s
Epoch 328/1000, LR 0.000207
Train loss: 0.6183;  Loss pred: 0.6183; Loss self: 0.0000; time: 0.15s
Val loss: 0.2756 score: 0.9318 time: 0.05s
Test loss: 0.4032 score: 0.8372 time: 0.05s
Epoch 329/1000, LR 0.000207
Train loss: 0.6198;  Loss pred: 0.6198; Loss self: 0.0000; time: 0.15s
Val loss: 0.2744 score: 0.9318 time: 0.06s
Test loss: 0.4034 score: 0.8372 time: 0.05s
Epoch 330/1000, LR 0.000207
Train loss: 0.6153;  Loss pred: 0.6153; Loss self: 0.0000; time: 0.16s
Val loss: 0.2732 score: 0.9318 time: 0.05s
Test loss: 0.4037 score: 0.8372 time: 0.05s
Epoch 331/1000, LR 0.000206
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.16s
Val loss: 0.2720 score: 0.9318 time: 0.05s
Test loss: 0.4041 score: 0.8372 time: 0.05s
Epoch 332/1000, LR 0.000206
Train loss: 0.6150;  Loss pred: 0.6150; Loss self: 0.0000; time: 0.16s
Val loss: 0.2708 score: 0.9318 time: 0.05s
Test loss: 0.4043 score: 0.8372 time: 0.05s
Epoch 333/1000, LR 0.000205
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.16s
Val loss: 0.2698 score: 0.9318 time: 0.05s
Test loss: 0.4044 score: 0.8372 time: 0.05s
Epoch 334/1000, LR 0.000205
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.16s
Val loss: 0.2689 score: 0.9318 time: 2.18s
Test loss: 0.4042 score: 0.8372 time: 0.89s
Epoch 335/1000, LR 0.000205
Train loss: 0.6121;  Loss pred: 0.6121; Loss self: 0.0000; time: 1.73s
Val loss: 0.2681 score: 0.9318 time: 0.07s
Test loss: 0.4038 score: 0.8372 time: 0.07s
Epoch 336/1000, LR 0.000204
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.16s
Val loss: 0.2673 score: 0.9318 time: 0.05s
Test loss: 0.4035 score: 0.8372 time: 0.04s
Epoch 337/1000, LR 0.000204
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.14s
Val loss: 0.2665 score: 0.9318 time: 0.05s
Test loss: 0.4031 score: 0.8372 time: 0.04s
Epoch 338/1000, LR 0.000204
Train loss: 0.6075;  Loss pred: 0.6075; Loss self: 0.0000; time: 0.14s
Val loss: 0.2659 score: 0.9318 time: 0.05s
Test loss: 0.4027 score: 0.8372 time: 0.04s
Epoch 339/1000, LR 0.000203
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.16s
Val loss: 0.2652 score: 0.9318 time: 0.10s
Test loss: 0.4023 score: 0.8372 time: 0.04s
Epoch 340/1000, LR 0.000203
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 0.14s
Val loss: 0.2644 score: 0.9091 time: 0.05s
Test loss: 0.4021 score: 0.8372 time: 0.04s
Epoch 341/1000, LR 0.000203
Train loss: 0.6075;  Loss pred: 0.6075; Loss self: 0.0000; time: 0.14s
Val loss: 0.2634 score: 0.9091 time: 0.05s
Test loss: 0.4022 score: 0.8372 time: 0.05s
Epoch 342/1000, LR 0.000202
Train loss: 0.6060;  Loss pred: 0.6060; Loss self: 0.0000; time: 0.14s
Val loss: 0.2625 score: 0.9091 time: 0.05s
Test loss: 0.4022 score: 0.8372 time: 0.05s
Epoch 343/1000, LR 0.000202
Train loss: 0.6030;  Loss pred: 0.6030; Loss self: 0.0000; time: 0.14s
Val loss: 0.2615 score: 0.9318 time: 0.05s
Test loss: 0.4024 score: 0.8372 time: 0.05s
Epoch 344/1000, LR 0.000201
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.15s
Val loss: 0.2603 score: 0.9318 time: 0.05s
Test loss: 0.4028 score: 0.8372 time: 0.05s
Epoch 345/1000, LR 0.000201
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 0.15s
Val loss: 0.2591 score: 0.9318 time: 0.05s
Test loss: 0.4033 score: 0.8372 time: 0.14s
Epoch 346/1000, LR 0.000201
Train loss: 0.6052;  Loss pred: 0.6052; Loss self: 0.0000; time: 0.15s
Val loss: 0.2579 score: 0.9318 time: 0.05s
Test loss: 0.4037 score: 0.8372 time: 0.05s
Epoch 347/1000, LR 0.000200
Train loss: 0.6032;  Loss pred: 0.6032; Loss self: 0.0000; time: 0.15s
Val loss: 0.2569 score: 0.9318 time: 0.05s
Test loss: 0.4041 score: 0.8372 time: 0.04s
Epoch 348/1000, LR 0.000200
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.15s
Val loss: 0.2559 score: 0.9318 time: 0.05s
Test loss: 0.4043 score: 0.8372 time: 0.04s
Epoch 349/1000, LR 0.000200
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.15s
Val loss: 0.2550 score: 0.9318 time: 0.05s
Test loss: 0.4043 score: 0.8372 time: 0.04s
Epoch 350/1000, LR 0.000199
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.15s
Val loss: 0.2544 score: 0.9318 time: 0.05s
Test loss: 0.4040 score: 0.8372 time: 0.05s
Epoch 351/1000, LR 0.000199
Train loss: 0.5995;  Loss pred: 0.5995; Loss self: 0.0000; time: 0.15s
Val loss: 0.2538 score: 0.9318 time: 0.05s
Test loss: 0.4036 score: 0.8372 time: 0.04s
Epoch 352/1000, LR 0.000198
Train loss: 0.6001;  Loss pred: 0.6001; Loss self: 0.0000; time: 0.15s
Val loss: 0.2532 score: 0.9318 time: 0.05s
Test loss: 0.4033 score: 0.8372 time: 0.05s
Epoch 353/1000, LR 0.000198
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.18s
Val loss: 0.2524 score: 0.9318 time: 0.06s
Test loss: 0.4032 score: 0.8372 time: 0.05s
Epoch 354/1000, LR 0.000198
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.15s
Val loss: 0.2518 score: 0.9318 time: 0.05s
Test loss: 0.4031 score: 0.8372 time: 0.04s
Epoch 355/1000, LR 0.000197
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.15s
Val loss: 0.2511 score: 0.9091 time: 0.05s
Test loss: 0.4029 score: 0.8372 time: 0.04s
Epoch 356/1000, LR 0.000197
Train loss: 0.5979;  Loss pred: 0.5979; Loss self: 0.0000; time: 0.15s
Val loss: 0.2505 score: 0.9091 time: 0.05s
Test loss: 0.4028 score: 0.8372 time: 0.04s
Epoch 357/1000, LR 0.000196
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.15s
Val loss: 0.2497 score: 0.9091 time: 0.05s
Test loss: 0.4029 score: 0.8372 time: 0.04s
Epoch 358/1000, LR 0.000196
Train loss: 0.5972;  Loss pred: 0.5972; Loss self: 0.0000; time: 0.15s
Val loss: 0.2488 score: 0.9091 time: 0.05s
Test loss: 0.4031 score: 0.8372 time: 0.05s
Epoch 359/1000, LR 0.000196
Train loss: 0.5960;  Loss pred: 0.5960; Loss self: 0.0000; time: 0.15s
Val loss: 0.2480 score: 0.9091 time: 0.05s
Test loss: 0.4033 score: 0.8372 time: 0.05s
Epoch 360/1000, LR 0.000195
Train loss: 0.5948;  Loss pred: 0.5948; Loss self: 0.0000; time: 0.15s
Val loss: 0.2470 score: 0.9318 time: 0.05s
Test loss: 0.4036 score: 0.8372 time: 0.04s
Epoch 361/1000, LR 0.000195
Train loss: 0.5931;  Loss pred: 0.5931; Loss self: 0.0000; time: 0.15s
Val loss: 0.2459 score: 0.9318 time: 0.05s
Test loss: 0.4042 score: 0.8372 time: 0.05s
Epoch 362/1000, LR 0.000195
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 0.15s
Val loss: 0.2448 score: 0.9318 time: 0.05s
Test loss: 0.4048 score: 0.8372 time: 0.04s
Epoch 363/1000, LR 0.000194
Train loss: 0.5924;  Loss pred: 0.5924; Loss self: 0.0000; time: 0.15s
Val loss: 0.2438 score: 0.9091 time: 0.05s
Test loss: 0.4054 score: 0.8372 time: 0.04s
Epoch 364/1000, LR 0.000194
Train loss: 0.5922;  Loss pred: 0.5922; Loss self: 0.0000; time: 0.15s
Val loss: 0.2428 score: 0.9091 time: 0.05s
Test loss: 0.4059 score: 0.8372 time: 0.05s
Epoch 365/1000, LR 0.000193
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.15s
Val loss: 0.2421 score: 0.9091 time: 0.05s
Test loss: 0.4061 score: 0.8372 time: 0.05s
Epoch 366/1000, LR 0.000193
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 0.15s
Val loss: 0.2413 score: 0.9091 time: 0.05s
Test loss: 0.4062 score: 0.8372 time: 0.04s
Epoch 367/1000, LR 0.000193
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.15s
Val loss: 0.2407 score: 0.9091 time: 0.05s
Test loss: 0.4062 score: 0.8372 time: 0.04s
Epoch 368/1000, LR 0.000192
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 0.15s
Val loss: 0.2400 score: 0.9091 time: 0.05s
Test loss: 0.4062 score: 0.8372 time: 0.04s
Epoch 369/1000, LR 0.000192
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.15s
Val loss: 0.2393 score: 0.9091 time: 0.05s
Test loss: 0.4063 score: 0.8372 time: 0.04s
Epoch 370/1000, LR 0.000191
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 0.15s
Val loss: 0.2388 score: 0.9318 time: 0.05s
Test loss: 0.4061 score: 0.8372 time: 0.04s
Epoch 371/1000, LR 0.000191
Train loss: 0.5894;  Loss pred: 0.5894; Loss self: 0.0000; time: 0.15s
Val loss: 0.2385 score: 0.9318 time: 0.05s
Test loss: 0.4057 score: 0.8372 time: 0.05s
Epoch 372/1000, LR 0.000191
Train loss: 0.5919;  Loss pred: 0.5919; Loss self: 0.0000; time: 0.15s
Val loss: 0.2381 score: 0.9318 time: 0.05s
Test loss: 0.4054 score: 0.8372 time: 0.04s
Epoch 373/1000, LR 0.000190
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 0.15s
Val loss: 0.2374 score: 0.9318 time: 0.05s
Test loss: 0.4055 score: 0.8372 time: 0.04s
Epoch 374/1000, LR 0.000190
Train loss: 0.5875;  Loss pred: 0.5875; Loss self: 0.0000; time: 0.14s
Val loss: 0.2369 score: 0.9318 time: 0.05s
Test loss: 0.4054 score: 0.8372 time: 0.05s
Epoch 375/1000, LR 0.000190
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 0.14s
Val loss: 0.2362 score: 0.9318 time: 0.05s
Test loss: 0.4056 score: 0.8372 time: 0.04s
Epoch 376/1000, LR 0.000189
Train loss: 0.5834;  Loss pred: 0.5834; Loss self: 0.0000; time: 0.14s
Val loss: 0.2354 score: 0.9318 time: 0.05s
Test loss: 0.4058 score: 0.8372 time: 0.04s
Epoch 377/1000, LR 0.000189
Train loss: 0.5853;  Loss pred: 0.5853; Loss self: 0.0000; time: 0.14s
Val loss: 0.2346 score: 0.9318 time: 0.05s
Test loss: 0.4062 score: 0.8372 time: 0.04s
Epoch 378/1000, LR 0.000188
Train loss: 0.5844;  Loss pred: 0.5844; Loss self: 0.0000; time: 0.15s
Val loss: 0.2339 score: 0.9318 time: 0.05s
Test loss: 0.4064 score: 0.8372 time: 0.05s
Epoch 379/1000, LR 0.000188
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.15s
Val loss: 0.2332 score: 0.9318 time: 0.05s
Test loss: 0.4066 score: 0.8372 time: 0.05s
Epoch 380/1000, LR 0.000188
Train loss: 0.5853;  Loss pred: 0.5853; Loss self: 0.0000; time: 0.15s
Val loss: 0.2326 score: 0.9318 time: 0.05s
Test loss: 0.4068 score: 0.8372 time: 0.04s
Epoch 381/1000, LR 0.000187
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 0.15s
Val loss: 0.2320 score: 0.9318 time: 0.05s
Test loss: 0.4068 score: 0.8372 time: 0.04s
Epoch 382/1000, LR 0.000187
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 0.15s
Val loss: 0.2315 score: 0.9318 time: 0.05s
Test loss: 0.4068 score: 0.8372 time: 0.04s
Epoch 383/1000, LR 0.000186
Train loss: 0.5864;  Loss pred: 0.5864; Loss self: 0.0000; time: 0.15s
Val loss: 0.2309 score: 0.9318 time: 0.05s
Test loss: 0.4070 score: 0.8372 time: 0.05s
Epoch 384/1000, LR 0.000186
Train loss: 0.5800;  Loss pred: 0.5800; Loss self: 0.0000; time: 0.15s
Val loss: 0.2303 score: 0.9318 time: 0.05s
Test loss: 0.4071 score: 0.8372 time: 0.05s
Epoch 385/1000, LR 0.000186
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.15s
Val loss: 0.2297 score: 0.9318 time: 2.01s
Test loss: 0.4073 score: 0.8372 time: 1.67s
Epoch 386/1000, LR 0.000185
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 3.33s
Val loss: 0.2291 score: 0.9318 time: 0.06s
Test loss: 0.4075 score: 0.8372 time: 0.06s
Epoch 387/1000, LR 0.000185
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 0.36s
Val loss: 0.2284 score: 0.9318 time: 0.06s
Test loss: 0.4078 score: 0.8372 time: 0.05s
Epoch 388/1000, LR 0.000184
Train loss: 0.5802;  Loss pred: 0.5802; Loss self: 0.0000; time: 0.15s
Val loss: 0.2277 score: 0.9318 time: 0.05s
Test loss: 0.4081 score: 0.8372 time: 0.04s
Epoch 389/1000, LR 0.000184
Train loss: 0.5820;  Loss pred: 0.5820; Loss self: 0.0000; time: 0.15s
Val loss: 0.2269 score: 0.9318 time: 0.05s
Test loss: 0.4085 score: 0.8372 time: 0.04s
Epoch 390/1000, LR 0.000184
Train loss: 0.5814;  Loss pred: 0.5814; Loss self: 0.0000; time: 0.15s
Val loss: 0.2263 score: 0.9091 time: 0.05s
Test loss: 0.4088 score: 0.8372 time: 0.04s
Epoch 391/1000, LR 0.000183
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.15s
Val loss: 0.2257 score: 0.9091 time: 0.05s
Test loss: 0.4091 score: 0.8372 time: 0.05s
Epoch 392/1000, LR 0.000183
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.15s
Val loss: 0.2252 score: 0.9091 time: 0.05s
Test loss: 0.4091 score: 0.8372 time: 0.04s
Epoch 393/1000, LR 0.000182
Train loss: 0.5799;  Loss pred: 0.5799; Loss self: 0.0000; time: 0.15s
Val loss: 0.2249 score: 0.9318 time: 0.05s
Test loss: 0.4090 score: 0.8372 time: 0.05s
Epoch 394/1000, LR 0.000182
Train loss: 0.5791;  Loss pred: 0.5791; Loss self: 0.0000; time: 0.15s
Val loss: 0.2246 score: 0.9318 time: 0.05s
Test loss: 0.4088 score: 0.8372 time: 0.05s
Epoch 395/1000, LR 0.000182
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.15s
Val loss: 0.2243 score: 0.9091 time: 0.05s
Test loss: 0.4087 score: 0.8372 time: 0.04s
Epoch 396/1000, LR 0.000181
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.15s
Val loss: 0.2239 score: 0.9091 time: 0.05s
Test loss: 0.4088 score: 0.8372 time: 0.05s
Epoch 397/1000, LR 0.000181
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.15s
Val loss: 0.2233 score: 0.9091 time: 0.05s
Test loss: 0.4090 score: 0.8372 time: 0.04s
Epoch 398/1000, LR 0.000180
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.15s
Val loss: 0.2227 score: 0.9091 time: 0.05s
Test loss: 0.4093 score: 0.8372 time: 0.04s
Epoch 399/1000, LR 0.000180
Train loss: 0.5761;  Loss pred: 0.5761; Loss self: 0.0000; time: 0.15s
Val loss: 0.2220 score: 0.9318 time: 0.69s
Test loss: 0.4097 score: 0.8372 time: 1.48s
Epoch 400/1000, LR 0.000180
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 3.79s
Val loss: 0.2213 score: 0.9318 time: 0.10s
Test loss: 0.4101 score: 0.8372 time: 0.68s
Epoch 401/1000, LR 0.000179
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 1.64s
Val loss: 0.2207 score: 0.9318 time: 0.10s
Test loss: 0.4104 score: 0.8372 time: 1.29s
Epoch 402/1000, LR 0.000179
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.61s
Val loss: 0.2202 score: 0.9318 time: 0.38s
Test loss: 0.4107 score: 0.8372 time: 0.61s
Epoch 403/1000, LR 0.000178
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.14s
Val loss: 0.2196 score: 0.9091 time: 0.05s
Test loss: 0.4110 score: 0.8372 time: 0.04s
Epoch 404/1000, LR 0.000178
Train loss: 0.5733;  Loss pred: 0.5733; Loss self: 0.0000; time: 0.14s
Val loss: 0.2192 score: 0.9318 time: 0.05s
Test loss: 0.4111 score: 0.8372 time: 0.04s
Epoch 405/1000, LR 0.000178
Train loss: 0.5751;  Loss pred: 0.5751; Loss self: 0.0000; time: 0.14s
Val loss: 0.2189 score: 0.9318 time: 0.05s
Test loss: 0.4111 score: 0.8372 time: 0.04s
Epoch 406/1000, LR 0.000177
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.14s
Val loss: 0.2185 score: 0.9318 time: 0.05s
Test loss: 0.4112 score: 0.8372 time: 0.04s
Epoch 407/1000, LR 0.000177
Train loss: 0.5731;  Loss pred: 0.5731; Loss self: 0.0000; time: 0.14s
Val loss: 0.2179 score: 0.9318 time: 0.05s
Test loss: 0.4115 score: 0.8372 time: 0.04s
Epoch 408/1000, LR 0.000176
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.14s
Val loss: 0.2175 score: 0.9318 time: 0.05s
Test loss: 0.4117 score: 0.8372 time: 0.04s
Epoch 409/1000, LR 0.000176
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.14s
Val loss: 0.2171 score: 0.9091 time: 0.05s
Test loss: 0.4118 score: 0.8372 time: 0.04s
Epoch 410/1000, LR 0.000175
Train loss: 0.5712;  Loss pred: 0.5712; Loss self: 0.0000; time: 0.14s
Val loss: 0.2168 score: 0.9091 time: 0.05s
Test loss: 0.4119 score: 0.8372 time: 0.04s
Epoch 411/1000, LR 0.000175
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 0.14s
Val loss: 0.2165 score: 0.9091 time: 0.05s
Test loss: 0.4120 score: 0.8372 time: 0.04s
Epoch 412/1000, LR 0.000175
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.14s
Val loss: 0.2161 score: 0.9091 time: 0.05s
Test loss: 0.4122 score: 0.8372 time: 0.04s
Epoch 413/1000, LR 0.000174
Train loss: 0.5720;  Loss pred: 0.5720; Loss self: 0.0000; time: 0.14s
Val loss: 0.2156 score: 0.9091 time: 0.05s
Test loss: 0.4124 score: 0.8372 time: 0.04s
Epoch 414/1000, LR 0.000174
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.14s
Val loss: 0.2153 score: 0.9091 time: 0.05s
Test loss: 0.4126 score: 0.8372 time: 0.04s
Epoch 415/1000, LR 0.000173
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.14s
Val loss: 0.2146 score: 0.9091 time: 0.05s
Test loss: 0.4131 score: 0.8372 time: 0.04s
Epoch 416/1000, LR 0.000173
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.14s
Val loss: 0.2139 score: 0.9091 time: 0.05s
Test loss: 0.4137 score: 0.8372 time: 0.04s
Epoch 417/1000, LR 0.000173
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.14s
Val loss: 0.2133 score: 0.9318 time: 0.05s
Test loss: 0.4141 score: 0.8372 time: 0.04s
Epoch 418/1000, LR 0.000172
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.14s
Val loss: 0.2127 score: 0.9318 time: 0.05s
Test loss: 0.4145 score: 0.8372 time: 0.04s
Epoch 419/1000, LR 0.000172
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.14s
Val loss: 0.2123 score: 0.9318 time: 0.05s
Test loss: 0.4148 score: 0.8372 time: 0.04s
Epoch 420/1000, LR 0.000171
Train loss: 0.5682;  Loss pred: 0.5682; Loss self: 0.0000; time: 0.14s
Val loss: 0.2119 score: 0.9318 time: 0.05s
Test loss: 0.4150 score: 0.8372 time: 0.04s
Epoch 421/1000, LR 0.000171
Train loss: 0.5683;  Loss pred: 0.5683; Loss self: 0.0000; time: 2.99s
Val loss: 0.2117 score: 0.9318 time: 0.14s
Test loss: 0.4151 score: 0.8372 time: 1.53s
Epoch 422/1000, LR 0.000171
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.69s
Val loss: 0.2116 score: 0.9091 time: 0.16s
Test loss: 0.4149 score: 0.8372 time: 0.48s
Epoch 423/1000, LR 0.000170
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 0.16s
Val loss: 0.2113 score: 0.9318 time: 0.06s
Test loss: 0.4151 score: 0.8372 time: 0.05s
Epoch 424/1000, LR 0.000170
Train loss: 0.5680;  Loss pred: 0.5680; Loss self: 0.0000; time: 0.15s
Val loss: 0.2109 score: 0.9318 time: 0.05s
Test loss: 0.4154 score: 0.8372 time: 0.05s
Epoch 425/1000, LR 0.000169
Train loss: 0.5675;  Loss pred: 0.5675; Loss self: 0.0000; time: 0.15s
Val loss: 0.2105 score: 0.9318 time: 0.05s
Test loss: 0.4157 score: 0.8372 time: 0.05s
Epoch 426/1000, LR 0.000169
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.15s
Val loss: 0.2101 score: 0.9318 time: 0.05s
Test loss: 0.4159 score: 0.8372 time: 0.05s
Epoch 427/1000, LR 0.000168
Train loss: 0.5694;  Loss pred: 0.5694; Loss self: 0.0000; time: 0.15s
Val loss: 0.2095 score: 0.9318 time: 0.05s
Test loss: 0.4164 score: 0.8372 time: 0.05s
Epoch 428/1000, LR 0.000168
Train loss: 0.5647;  Loss pred: 0.5647; Loss self: 0.0000; time: 0.15s
Val loss: 0.2088 score: 0.9091 time: 0.05s
Test loss: 0.4169 score: 0.8372 time: 0.05s
Epoch 429/1000, LR 0.000168
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.16s
Val loss: 0.2082 score: 0.9318 time: 0.05s
Test loss: 0.4174 score: 0.8372 time: 0.05s
Epoch 430/1000, LR 0.000167
Train loss: 0.5694;  Loss pred: 0.5694; Loss self: 0.0000; time: 0.16s
Val loss: 0.2077 score: 0.9318 time: 0.06s
Test loss: 0.4178 score: 0.8372 time: 0.05s
Epoch 431/1000, LR 0.000167
Train loss: 0.5645;  Loss pred: 0.5645; Loss self: 0.0000; time: 0.15s
Val loss: 0.2072 score: 0.9318 time: 0.05s
Test loss: 0.4182 score: 0.8372 time: 0.05s
Epoch 432/1000, LR 0.000166
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.15s
Val loss: 0.2070 score: 0.9318 time: 0.05s
Test loss: 0.4183 score: 0.8372 time: 0.05s
Epoch 433/1000, LR 0.000166
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 0.15s
Val loss: 0.2067 score: 0.9318 time: 0.05s
Test loss: 0.4184 score: 0.8372 time: 0.05s
Epoch 434/1000, LR 0.000166
Train loss: 0.5657;  Loss pred: 0.5657; Loss self: 0.0000; time: 0.15s
Val loss: 0.2065 score: 0.9318 time: 0.05s
Test loss: 0.4186 score: 0.8372 time: 0.05s
Epoch 435/1000, LR 0.000165
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 0.15s
Val loss: 0.2062 score: 0.9091 time: 0.05s
Test loss: 0.4188 score: 0.8372 time: 0.05s
Epoch 436/1000, LR 0.000165
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.16s
Val loss: 0.2059 score: 0.9091 time: 0.06s
Test loss: 0.4190 score: 0.8372 time: 0.05s
Epoch 437/1000, LR 0.000164
Train loss: 0.5636;  Loss pred: 0.5636; Loss self: 0.0000; time: 0.16s
Val loss: 0.2056 score: 0.9318 time: 0.05s
Test loss: 0.4191 score: 0.8372 time: 0.05s
Epoch 438/1000, LR 0.000164
Train loss: 0.5636;  Loss pred: 0.5636; Loss self: 0.0000; time: 0.15s
Val loss: 0.2054 score: 0.9318 time: 0.05s
Test loss: 0.4193 score: 0.8372 time: 0.05s
Epoch 439/1000, LR 0.000163
Train loss: 0.5641;  Loss pred: 0.5641; Loss self: 0.0000; time: 0.16s
Val loss: 0.2052 score: 0.9318 time: 0.05s
Test loss: 0.4194 score: 0.8372 time: 0.05s
Epoch 440/1000, LR 0.000163
Train loss: 0.5617;  Loss pred: 0.5617; Loss self: 0.0000; time: 0.15s
Val loss: 0.2050 score: 0.9318 time: 0.05s
Test loss: 0.4195 score: 0.8372 time: 0.05s
Epoch 441/1000, LR 0.000163
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 0.16s
Val loss: 0.2048 score: 0.9318 time: 0.05s
Test loss: 0.4197 score: 0.8372 time: 0.05s
Epoch 442/1000, LR 0.000162
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.15s
Val loss: 0.2044 score: 0.9318 time: 0.06s
Test loss: 0.4200 score: 0.8372 time: 0.05s
Epoch 443/1000, LR 0.000162
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.15s
Val loss: 0.2039 score: 0.9318 time: 0.05s
Test loss: 0.4204 score: 0.8372 time: 0.05s
Epoch 444/1000, LR 0.000161
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.16s
Val loss: 0.2034 score: 0.9318 time: 0.05s
Test loss: 0.4208 score: 0.8372 time: 0.05s
Epoch 445/1000, LR 0.000161
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.16s
Val loss: 0.2029 score: 0.9318 time: 0.05s
Test loss: 0.4212 score: 0.8372 time: 0.05s
Epoch 446/1000, LR 0.000161
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.31s
Val loss: 0.2027 score: 0.9318 time: 0.38s
Test loss: 0.4214 score: 0.8372 time: 1.59s
Epoch 447/1000, LR 0.000160
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 3.16s
Val loss: 0.2023 score: 0.9318 time: 0.10s
Test loss: 0.4217 score: 0.8372 time: 0.71s
Epoch 448/1000, LR 0.000160
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.95s
Val loss: 0.2020 score: 0.9318 time: 0.06s
Test loss: 0.4220 score: 0.8372 time: 0.05s
Epoch 449/1000, LR 0.000159
Train loss: 0.5586;  Loss pred: 0.5586; Loss self: 0.0000; time: 0.15s
Val loss: 0.2016 score: 0.9318 time: 0.05s
Test loss: 0.4223 score: 0.8372 time: 0.14s
Epoch 450/1000, LR 0.000159
Train loss: 0.5614;  Loss pred: 0.5614; Loss self: 0.0000; time: 0.14s
Val loss: 0.2013 score: 0.9318 time: 0.05s
Test loss: 0.4226 score: 0.8372 time: 0.04s
Epoch 451/1000, LR 0.000158
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 0.14s
Val loss: 0.2008 score: 0.8864 time: 0.05s
Test loss: 0.4230 score: 0.8140 time: 0.04s
Epoch 452/1000, LR 0.000158
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.14s
Val loss: 0.2004 score: 0.8864 time: 0.05s
Test loss: 0.4234 score: 0.8140 time: 0.04s
Epoch 453/1000, LR 0.000158
Train loss: 0.5615;  Loss pred: 0.5615; Loss self: 0.0000; time: 0.14s
Val loss: 0.1999 score: 0.8864 time: 0.05s
Test loss: 0.4238 score: 0.8140 time: 0.04s
Epoch 454/1000, LR 0.000157
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.14s
Val loss: 0.1996 score: 0.8864 time: 0.05s
Test loss: 0.4240 score: 0.8140 time: 0.04s
Epoch 455/1000, LR 0.000157
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.14s
Val loss: 0.1993 score: 0.8864 time: 0.05s
Test loss: 0.4243 score: 0.8140 time: 0.04s
Epoch 456/1000, LR 0.000156
Train loss: 0.5561;  Loss pred: 0.5561; Loss self: 0.0000; time: 0.15s
Val loss: 0.1991 score: 0.8864 time: 0.12s
Test loss: 0.4244 score: 0.8140 time: 0.04s
Epoch 457/1000, LR 0.000156
Train loss: 0.5578;  Loss pred: 0.5578; Loss self: 0.0000; time: 0.15s
Val loss: 0.1990 score: 0.8864 time: 0.05s
Test loss: 0.4244 score: 0.8140 time: 0.05s
Epoch 458/1000, LR 0.000155
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.15s
Val loss: 0.1991 score: 0.9318 time: 0.05s
Test loss: 0.4243 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 459/1000, LR 0.000155
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.15s
Val loss: 0.1993 score: 0.9318 time: 0.05s
Test loss: 0.4242 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 456,   Train_Loss: 0.5578,   Val_Loss: 0.1990,   Val_Precision: 0.8696,   Val_Recall: 0.9091,   Val_accuracy: 0.8889,   Val_Score: 0.8864,   Val_Loss: 0.1990,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4244


[0.08459676499478519, 0.06626931903883815, 0.06665601104032248, 0.089698052033782, 0.06427923694718629, 0.06718824605923146, 0.09642257506493479, 0.06766697100829333, 0.06542453705333173, 0.07118417404126376, 0.10903747600968927, 0.05468032497446984, 0.0548763950355351, 0.05448356398846954, 0.054790173075161874, 0.054713204968720675, 0.05653753399383277, 0.05680288397707045, 0.055585638037882745, 1.4804800390265882, 0.6557771749794483, 0.2656160199549049, 0.06327604805119336, 0.2021953599760309, 0.056601344025693834, 0.07482093095313758, 0.2504513809690252, 0.05772252904716879, 0.058897563023492694, 0.05719587206840515, 0.24992314097471535, 0.05229868204332888, 2.0954069229774177, 0.7661122210556641, 0.07166752405464649, 0.07931198505684733, 0.06522167497314513, 0.05442927195690572, 0.05522781505715102, 0.12725600705016404, 0.25028498307801783, 0.3354708789847791, 0.053588483948260546, 0.2500298290979117, 0.055462119984440506, 0.10419729701243341, 0.05981402297038585, 0.06189750996418297, 0.060559379984624684, 0.24353447696194053, 2.3137422299478203, 2.8373248040443286, 0.07770050689578056, 0.11080820602364838, 0.05474730802234262, 0.09818487707525492, 3.9333824950736016, 1.841174609027803, 0.2466274710604921, 0.2415607359725982, 0.053474657936021686, 0.055495138047263026, 0.23009767895564437, 0.24827055202331394, 0.05764050397556275, 2.8589647270273417, 2.146260942914523, 0.9960931029636413, 0.2506180090131238, 0.12033728102687746, 0.23185732495039701, 2.6031034500338137, 0.2575325210345909, 0.2998981639975682, 0.2497449170332402, 0.05365002702455968, 2.0942675719270483, 0.830333348014392, 0.25357821793295443, 0.07493854500353336, 0.1577497790567577, 0.0793314470211044, 0.052735768957063556, 0.06640536594204605, 0.05601161194499582, 2.696211951901205, 0.07413948909379542, 0.061128705041483045, 0.06019063398707658, 0.060051303007639945, 3.1960599499288946, 0.2587771910475567, 0.22809617407619953, 0.05499097506981343, 3.0277279829606414, 1.4244586590211838, 0.5287767639383674, 0.22805398399941623, 0.24303356104064733, 0.05624497891403735, 0.14082806406076998, 0.2553049139678478, 0.06472760101314634, 0.1630173809826374, 0.2357342189643532, 0.24767416098620743, 0.24913921998813748, 0.05632512201555073, 0.05782794894184917, 0.9498419190058485, 2.4180220529669896, 0.23439472203608602, 0.05823535693343729, 0.051316914963535964, 0.24453643499873579, 0.24819667602423579, 0.25720047194045037, 3.2120706470450386, 1.176621043938212, 0.05663292994722724, 1.1116661409614608, 0.06503781606443226, 1.1300636349478737, 1.4325072600040585, 2.355186625965871, 0.05213836708571762, 0.05130533408373594, 0.051297910045832396, 0.05118882097303867, 0.05041386897210032, 0.05064943397883326, 0.050211466033943, 0.05268438602797687, 0.05172203900292516, 0.052055774023756385, 0.05190583097282797, 0.05226895504165441, 0.46333046501968056, 1.3566809330368415, 0.14648569491691887, 0.11865499999839813, 0.09611038805451244, 0.053623382933437824, 0.04938574507832527, 0.0481765519361943, 0.04842587606981397, 0.05177068093325943, 0.05271221394650638, 0.053150240099057555, 0.05268782691564411, 0.05268899002112448, 0.052331010112538934, 0.05353300110436976, 0.04955265193711966, 0.04979407007340342, 0.04981141095049679, 0.05303098901640624, 0.0536107401130721, 0.04920347000006586, 0.04986086394637823, 0.05295018898323178, 0.0500788219505921, 0.0500689169857651, 0.049807026982307434, 0.052565157995559275, 0.9742379010422155, 0.0680344719439745, 0.05275011993944645, 0.05261184403207153, 0.05255502602085471, 0.05264316301327199, 0.053719934076070786, 0.05334303597919643, 0.05369243095628917, 0.05455213296227157, 0.053605518070980906, 0.05329342000186443, 0.05327548098284751, 0.053368504042737186, 0.0534480819478631, 0.053236634004861116, 0.053247499046847224, 0.0533535199938342, 0.05325449991505593, 0.05279919400345534, 0.05224345298483968, 0.053287462098523974, 0.050735307042486966, 0.05192630097735673, 0.05131093191448599, 0.051174922031350434, 0.05105923500377685, 0.05128834000788629, 0.051460409071296453, 0.0515485709765926, 0.051489903940819204, 0.05281276907771826, 0.05146248498931527, 0.05183888599276543, 0.05205286806449294, 0.05230519094038755, 0.05298671801574528, 0.055469596991315484, 0.05490928504150361, 0.05474885005969554, 0.052013479988090694, 0.05834611598402262, 0.054617374087683856, 0.05451101006474346, 0.05204040801618248, 0.05261968600098044, 2.606530265067704, 0.9211624809540808, 0.2484977780841291, 0.23734895896632224, 0.24179827701300383, 0.32587404700461775, 2.5938762900186703, 0.24440678791143, 0.07989601790904999, 0.24014651600737125, 0.05556968308519572, 0.24587829504162073, 0.24564621993340552, 0.1466228679055348, 0.053799414075911045, 0.050965606002137065, 0.05742147902492434, 0.052994532976299524, 0.07095098996069282, 0.0545356769580394, 0.05446126591414213, 0.0515277850208804, 0.05121218308340758, 0.06443883501924574, 0.05337820702698082, 0.052160780993290246, 0.05164974695071578, 0.052352600963786244, 0.052403679001145065, 0.051989570958539844, 0.09171751502435654, 0.08698624605312943, 0.08565741509664804, 0.057043020031414926, 0.05833322799298912, 0.05468091904185712, 0.16641600092407316, 0.05420237104408443, 0.12143539893440902, 0.21360827004536986, 0.24978595494758338, 0.07857739005703479, 0.06254101602826267, 0.08969656191766262, 0.055554667953401804, 0.24694293399807066, 0.053027615998871624, 0.24723792902659625, 0.1111475300276652, 0.25210351904388517, 2.2372030599508435, 2.8249222539598122, 1.3480530380038545, 0.05558339401613921, 0.06699277402367443, 0.06870051799342036, 0.05123939597979188, 0.055796772008761764, 2.2022682760143653, 1.4604845480062068, 0.23252561304252595, 0.05867771490011364, 0.05447226495016366, 0.14277716598007828, 0.1399853409966454, 2.454710590071045, 0.2387701200786978, 0.23501511197537184, 0.054216001997701824, 0.08784024498891085, 0.08320450503379107, 0.05230379197746515, 0.08525432192254812, 0.05361292709130794, 0.09847789199557155, 0.05222330195829272, 0.05170190893113613, 0.05212552298326045, 0.2453287070384249, 0.05565779795870185, 3.10152285406366, 0.2536554510006681, 0.24641203100327402, 0.2375793500104919, 0.24733240099158138, 2.4730275260517374, 0.24900592293124646, 0.06218485103454441, 0.06124537403229624, 0.05501338001340628, 0.061959358979947865, 0.058469027979299426, 2.3359245469328016, 0.04944346193224192, 0.05017430998850614, 0.0494336080737412, 0.04935004492290318, 0.049257938051596284, 0.05018594500143081, 0.0494404430501163, 0.049474978004582226, 0.050372795085422695, 0.0495896190404892, 0.04997448995709419, 0.05023470800369978, 0.049742815899662673, 0.05048439698293805, 0.049808130017481744, 0.05007164995186031, 0.05022765893954784, 0.049679468967951834, 2.812340941047296, 0.5075800759950653, 0.13433663500472903, 0.05711126502137631, 0.052672052988782525, 0.052740031969733536, 0.2408177200704813, 0.24699899996630847, 1.5402842609910294, 2.8151212240336463, 2.672238891944289, 0.4744423439260572, 0.051515544997528195, 0.05196398904081434, 0.08046992297749966, 0.07211169297806919, 0.050759447971358895, 0.05036586499772966, 0.24091502698138356, 0.05225209007039666, 2.778734190040268, 1.380030931904912, 0.7744984499877319, 0.07275736203882843, 0.058869717991910875, 0.057914559030905366, 0.08430228801444173, 0.0773160009412095, 3.880209008930251, 1.2256643370492384, 0.2431661169975996, 0.049872992909513414, 0.23424911499023438, 0.12028067791834474, 0.05121146596502513, 0.09096211695577949, 0.04986458597704768, 0.05343920399900526, 1.5996504529612139, 2.433322911034338, 0.23497205902822316, 0.0543774280231446, 0.08919988106936216, 0.24242650798987597, 2.8954306290252134, 0.23801004502456635, 0.1310518200043589, 0.05801335698924959, 0.24396602099295706, 0.240371446008794, 2.8159471339313313, 0.1479107600171119, 0.051787703996524215, 0.18529160099569708, 0.06595625402405858, 0.04971874400507659, 0.22562802000902593, 0.2447222089394927, 0.23690260795410722, 0.053044412983581424, 0.04773282399401069, 3.159081368939951, 1.1281052510021254, 0.18888215499464422, 0.06361547799315304, 0.051269318093545735, 0.05338644306175411, 0.05191844596993178, 0.054513902054168284, 0.24598884594161063, 0.17896988696884364, 0.05423133703880012, 0.05411772395018488, 1.4909199939575046, 0.28538446594029665, 0.058233347022905946, 0.23302480205893517, 0.05519809597171843, 3.2789746549678966, 0.053949831053614616, 0.05113312101457268, 0.24701657600235194, 0.24083348095882684, 0.23608808510471135, 0.23756848298944533, 0.05224914907012135, 0.06554252898786217, 0.05223998497240245, 0.04885788797400892, 0.0891070649959147, 0.04829049902036786, 0.0483347550034523, 0.05323534004855901, 0.09113864600658417, 0.058497846010141075, 0.22989736194722354, 0.2550101469969377, 3.139166402979754, 0.24722550401929766, 0.05128787795547396, 0.22015577694401145, 0.2056221819948405, 0.051905813976190984, 3.1465106169926003, 0.2942919520428404, 0.04955989809241146, 0.05815014895051718, 2.8180349631002173, 2.4861585879698396, 0.04849507799372077, 0.0470155329676345, 0.04713625798467547, 0.04639587504789233, 0.04769292299170047, 0.04659958905540407, 0.047167232958599925, 0.08669182797893882, 2.0460680670803413, 2.772752225981094, 0.05154373298864812, 0.055337524972856045, 0.050786255043931305, 0.050700071034953, 0.05087506293784827, 0.051077060983516276, 0.05032158095855266, 0.04854928492568433, 0.04964459710754454, 0.04949348501395434, 0.05010436801239848, 0.0680356890661642, 0.053059083060361445, 0.04894226393662393, 0.048938922001980245, 0.04979719896800816, 0.04907580593135208, 0.1427160850726068, 0.053467349964194, 0.04927297600079328, 0.049121610005386174, 0.049700311035849154, 0.049021653016097844, 0.04889634298160672, 0.0485775179695338, 0.04788806103169918, 0.04723011504393071, 0.0477241970365867, 0.04658093897160143, 0.04689992195926607, 0.04995782405603677, 0.050142246996983886, 0.04676647495944053, 1.5989008760079741, 0.3276873509166762, 0.05201916501391679, 0.05187789793126285, 0.052862404962070286, 0.05127344699576497, 0.05168498994316906, 0.051611325005069375, 0.052401997963897884, 0.05184208007995039, 0.05142341693863273, 0.05214235000312328, 0.05153977801091969, 0.48986000299919397, 1.342280660988763, 0.049110481049865484, 0.04860620992258191, 0.048961870023049414, 0.04856127907987684, 0.04861448996234685, 0.05310215207282454, 0.04902819590643048, 0.04897971998434514, 0.04874254192691296, 0.049717011977918446, 0.04976283188443631, 0.04634138790424913, 0.04595602198969573, 0.04624368797522038, 0.04618165292777121, 0.04640879202634096, 0.04705826099961996, 0.04722207202576101, 0.051108932006172836, 0.04997157596517354, 0.04642272798810154, 0.046301794005557895, 0.04654466500505805, 0.04665375710465014, 0.05032111797481775, 0.046422946033999324, 0.054055197979323566, 0.06141661503352225, 0.04988073103595525, 0.4908980840118602, 0.27872072206810117, 0.2207755820127204, 0.436075157020241, 0.14341423998121172, 0.05436667101457715, 0.05334727093577385, 0.04933225503191352, 0.049443333060480654, 0.04983798100147396, 0.05392590397968888, 0.04998671100474894, 0.05427786405198276, 0.05379238503519446, 0.25509347300976515, 0.9269476649351418, 0.050866703037172556, 0.054810622008517385, 0.04644490289501846, 0.046109932009130716, 0.04612396005541086, 0.04609286307822913, 0.04934136208612472, 0.04566326399799436, 0.04594554309733212, 0.04577594995498657, 0.051335091004148126, 0.05092029192019254, 0.050705461064353585, 0.05087807204108685, 0.05047558690421283, 0.05480239901226014, 0.05093577201478183, 0.05538132297806442, 0.04601990606170148, 0.04654716805089265, 0.05017821700312197, 0.04933204397093505, 0.04670860501937568, 0.04870802699588239, 0.04912246100138873, 0.049604842090047896, 0.04822078696452081, 0.04530011804308742, 0.046266802004538476, 0.04656867100857198, 0.049004461034201086, 0.053582434891723096, 0.15234806295484304, 0.050468642031773925, 0.7766167079098523, 0.12793406401760876, 0.05647693097125739, 0.05130211403593421, 0.05122180702164769, 0.05113288201391697, 0.04784299398306757, 0.04820688802283257, 0.051507399999536574, 0.050730676972307265, 0.047240487998351455, 0.051190893980674446, 0.38474957703147084, 0.050239244010299444, 0.049968356965109706, 0.050000441959127784, 0.054129634983837605, 0.04850710101891309, 0.046981382998637855, 0.047422599978744984, 0.04673397494480014, 0.04665484291035682, 0.050651223049499094, 0.047308290959335864, 0.04655661294236779, 0.05027287907432765, 0.046904995921067894, 0.050111535005271435, 0.04704771493561566, 0.05230177706107497, 0.04677932197228074, 0.04672304401174188, 0.046823172946460545, 0.04696346295531839, 2.0685945600271225, 0.3560233030002564, 0.05802934104576707, 0.056213862961158156, 0.05094904196448624, 0.05062748200725764, 0.050879737944342196, 0.0506133419694379, 0.051451778039336205, 0.05563035700470209, 0.05109541304409504, 0.0555479780305177, 0.05164470395538956, 0.0509339130949229, 0.05086977896280587, 0.051634174888022244, 0.04833751090336591, 0.048167280037887394, 0.04871841403655708, 0.05468394700437784, 0.05197280703578144, 0.05125990207307041, 0.05341607006266713, 0.05170342803467065, 0.05158112198114395, 0.048337521962821484, 0.05117596499621868, 0.05313680099789053, 0.04972457606345415, 0.05307262891437858, 0.05098106199875474, 0.05046307004522532, 0.05016995896585286, 0.05005251907277852, 0.05039040697738528, 0.05040782305877656, 0.053821110050193965, 0.05356744502205402, 0.05274900293443352, 0.04991413501556963, 1.4808099130168557, 0.8842611199943349, 0.06683781603351235, 0.050112675060518086, 0.05023844807874411, 0.046972047071903944, 0.05037390801589936, 0.051050585927441716, 0.05115981900598854, 0.05515956797171384, 0.050559079041704535, 0.05500976799521595, 0.055283720954321325, 0.05047907994594425, 0.05096407700330019, 0.0504403110826388, 0.05501230398658663, 0.05381165293511003, 0.055188624071888626, 0.050589088001288474, 0.05136169900652021, 0.05583458999171853, 0.8976613921113312, 0.07452907296828926, 0.047812720062211156, 0.046913124970160425, 0.04688608890864998, 0.04703614301979542, 0.04659564699977636, 0.05075712699908763, 0.049441106035374105, 0.04947231092955917, 0.05385122890584171, 0.1438461660873145, 0.04970788396894932, 0.04901395400520414, 0.048712355084717274, 0.0489392289891839, 0.052904805983416736, 0.04845569096505642, 0.054244723985902965, 0.0539086899952963, 0.04819257208146155, 0.04895532096270472, 0.0486194749828428, 0.04856144601944834, 0.05251207295805216, 0.05334873090032488, 0.0488005040679127, 0.053249571938067675, 0.0481853709788993, 0.04873949196189642, 0.052859659073874354, 0.049267764086835086, 0.04841052403207868, 0.0486684130737558, 0.04854820598848164, 0.048883704002946615, 0.0485916449688375, 0.05308733496349305, 0.049284214968793094, 0.04484289092943072, 0.04951117595192045, 0.04577757406514138, 0.048848912003450096, 0.048777604941278696, 0.05311203096061945, 0.05301627598237246, 0.048747821943834424, 0.048297428060323, 0.048447418957948685, 0.05257030203938484, 0.05269515095278621, 1.6798435470554978, 0.06415148603264242, 0.05309663701336831, 0.04899701999966055, 0.04871490004006773, 0.0485765659250319, 0.052989209070801735, 0.04873852105811238, 0.04948393499944359, 0.05038352811243385, 0.048874628031626344, 0.05305991997011006, 0.049020513077266514, 0.04867755190934986, 1.4802586400182918, 0.6842142009409145, 1.2977699580369517, 0.6131615430349484, 0.045614531030878425, 0.04908050398807973, 0.048658128012903035, 0.045578246004879475, 0.04540728603024036, 0.04511926893610507, 0.045045471051707864, 0.045035377028398216, 0.048281245050020516, 0.04586059495341033, 0.04846472595818341, 0.04834303294774145, 0.04559108801186085, 0.04836304404307157, 0.04496834706515074, 0.045584643026813865, 0.04534925892949104, 0.045567909022793174, 1.533659546985291, 0.4853590449783951, 0.051000652951188385, 0.049687394057400525, 0.049700189963914454, 0.049728938029147685, 0.04944783903192729, 0.04979907302185893, 0.055611152900382876, 0.051444459008052945, 0.05091031291522086, 0.05103384901303798, 0.05084997497033328, 0.0512425119522959, 0.0559373659780249, 0.05182012601289898, 0.05162410601042211, 0.05538381391670555, 0.05121807800605893, 0.05596971791237593, 0.0526238780003041, 0.051381932105869055, 0.051490479963831604, 0.05114167800638825, 0.05119338491931558, 1.5991632109507918, 0.7144866980379447, 0.052783838007599115, 0.1459043100476265, 0.047789899981580675, 0.048099542036652565, 0.04805142607074231, 0.04522172990255058, 0.048940471024252474, 0.044944817083887756, 0.048473843024112284, 0.05308772507123649, 0.05359423696063459, 0.04984675906598568]
[0.0019226537498814816, 0.0015061208872463214, 0.0015149093418255109, 0.0020385920916768637, 0.0014608917487996885, 0.0015270055922552604, 0.0021914221605667, 0.0015378857047339393, 0.0014869212966666303, 0.0016178221373014492, 0.002478124454765665, 0.0012716354645225544, 0.001276195233384537, 0.0012670596276388266, 0.0012741900715153925, 0.0012724001155516437, 0.0013148263719495992, 0.001320997301792336, 0.0012926892566949475, 0.03442976834945554, 0.01525063197626624, 0.006177116743137323, 0.0014715360011905432, 0.004702217673861184, 0.0013163103261789263, 0.0017400216500729668, 0.005824450720209888, 0.0013423843964457858, 0.0013697107679882022, 0.0013301365597303523, 0.005812166069179427, 0.0012162484196122996, 0.04873039355761437, 0.01781656328036428, 0.0016666866059220115, 0.0018444647687638914, 0.0015167831389103518, 0.0012657970222536214, 0.001284367792026768, 0.0029594420244224194, 0.005820581001814368, 0.007801648348483236, 0.0012462438127502453, 0.005814647188323529, 0.0012898167438241978, 0.002423192953777521, 0.0013910237900089733, 0.001439476975911232, 0.0014083576740610392, 0.005663592487486989, 0.053807958835995824, 0.06598429776847275, 0.0018069885324600132, 0.002576935023805776, 0.0012731932098219215, 0.0022833692343082537, 0.09147401151333957, 0.04281801416343728, 0.0057355225828021415, 0.005617691534246469, 0.001243596696186551, 0.001290584605750303, 0.005351108812921962, 0.005773733767984045, 0.0013404768366409943, 0.06648755179133353, 0.04991304518405868, 0.023164955882875378, 0.005828325791002879, 0.0027985414192297085, 0.005392030812799931, 0.060537289535670086, 0.005989128396153277, 0.0069743759069201905, 0.0058080213263544235, 0.0012476750470827832, 0.048703897021559264, 0.019310077860799812, 0.005897167858905917, 0.0017427568605472875, 0.0036685995129478533, 0.0018449173725838231, 0.0012264132315596176, 0.001544310835861536, 0.0013025956266278097, 0.06270260353258617, 0.0017241741649719866, 0.0014215977916623964, 0.001399782185745967, 0.0013965419304102313, 0.07432697557974173, 0.006018074210408295, 0.005304562187818594, 0.0012788598853444985, 0.07041227867350329, 0.03312694555863218, 0.01229713404507831, 0.005303581023242238, 0.005651943280015054, 0.001308022765442729, 0.0032750712572272088, 0.005937323580647624, 0.0015052930468173568, 0.003791101883317149, 0.005482191138705889, 0.005759864208981568, 0.005793935348561337, 0.0013098865585011799, 0.0013448360219034691, 0.022089346953624384, 0.05623307099923232, 0.0054510400473508375, 0.0013543106263590066, 0.001193416627058976, 0.005686893837179902, 0.005772015721493855, 0.00598140632419652, 0.07469931737314044, 0.02736328009158633, 0.0013170448824936568, 0.025852700952592112, 0.001512507350335634, 0.02628054964995055, 0.03331412232567578, 0.0547717819992063, 0.0012125201647841307, 0.001193147304272929, 0.0011929746522286603, 0.001190437697047411, 0.001172415557490705, 0.0011778938134612386, 0.001167708512417279, 0.0012252182797203924, 0.0012028381163470966, 0.0012105993959013112, 0.0012071123482053016, 0.001215557093991963, 0.010775127093480942, 0.0315507193729498, 0.0034066440678353228, 0.0027594186046139097, 0.0022351253035933125, 0.0012470554170566936, 0.0011485056994959366, 0.0011203849287487048, 0.0011261831644142782, 0.001203969324029289, 0.0012258654406164275, 0.0012360520953269198, 0.0012252983003638166, 0.0012253253493284763, 0.001217000235175324, 0.0012449535140551108, 0.00115238725435162, 0.001158001629614033, 0.0011584049058255069, 0.0012332788143350289, 0.001246761397978421, 0.0011442667441875782, 0.0011595549754971682, 0.001231399743796088, 0.0011646237662928396, 0.001164393418273607, 0.001158302953076917, 0.0012224455347804482, 0.02265669537307478, 0.0015821970219528953, 0.001226746975335964, 0.001223531256559803, 0.0012222099074617374, 0.0012242596049598137, 0.0012493007924667624, 0.0012405357204464285, 0.0012486611850299808, 0.0012686542549365481, 0.0012466399551390908, 0.0012393818605084751, 0.0012389646740197096, 0.0012411280009938881, 0.0012429786499503048, 0.0012380612559270026, 0.0012383139313220284, 0.0012407795347403302, 0.001238476742210603, 0.0012278882326384963, 0.0012149640229032483, 0.0012392433046168366, 0.0011798908614531853, 0.0012075883948222497, 0.0011932774863833951, 0.001190114465845359, 0.0011874240698552755, 0.001192752093206658, 0.0011967536993324757, 0.0011988039761998278, 0.0011974396265306791, 0.0012282039320399595, 0.001196801976495704, 0.0012055554882038472, 0.0012105318154533242, 0.0012163997893113382, 0.0012322492561801228, 0.0012899906277050112, 0.00127696011724427, 0.0012732290711557103, 0.0012096158136765277, 0.001356886418233084, 0.0012701714904112525, 0.001267697908482406, 0.0012102420468879647, 0.0012237136279297776, 0.060616982908551255, 0.021422383278001878, 0.005779018094979747, 0.005519743231774936, 0.005623215744488461, 0.007578466209409715, 0.06032270441903884, 0.005683878788637907, 0.0018580469281174415, 0.0055848026978458426, 0.0012923182112836214, 0.005718099884688854, 0.005712702789148966, 0.0034098341373380186, 0.0012511491645560708, 0.0011852466512124898, 0.0013353832331377752, 0.001232430999448826, 0.0016500230223416936, 0.001268271557163707, 0.0012665410677707473, 0.0011983205818809396, 0.0011909810019397112, 0.0014985775585871102, 0.0012413536517902515, 0.0012130414184486104, 0.0012011569058305996, 0.001217502347995029, 0.001218690209328955, 0.0012090597897334847, 0.0021329654656827104, 0.0020229359547239405, 0.001992032909224373, 0.001326581861195696, 0.001356586697511375, 0.0012716492800431889, 0.0038701395563737946, 0.0012605202568391728, 0.002824079044986256, 0.004967634187101624, 0.005808975696455427, 0.001827381164117088, 0.001454442233215411, 0.002085966556224712, 0.001291969022172135, 0.00574285893018769, 0.001233200372066782, 0.005749719279688285, 0.0025848262797131443, 0.005862872535904306, 0.05202797813839171, 0.06569586637115843, 0.03135007065125243, 0.0012926370701427724, 0.001557971488922661, 0.0015976864649632641, 0.00119161385999516, 0.0012975993490409712, 0.05121554130265966, 0.0339647569303769, 0.005407572396337813, 0.0013645980209328753, 0.0012667968593061316, 0.0033203992088390298, 0.003255473046433614, 0.057086292792349884, 0.005552793490202274, 0.0054654677203574845, 0.0012608372557605075, 0.00204279639509095, 0.0019349884891579317, 0.0012163672552898872, 0.001982658649361584, 0.0012468122579373938, 0.002290183534780734, 0.0012144953943789005, 0.0012023699751427008, 0.0012122214647269872, 0.005705318768335463, 0.0012943673943884152, 0.07212843846659674, 0.005898963976759724, 0.00573051234891335, 0.005525101163034695, 0.0057519163021298, 0.057512268047714823, 0.005790835417005731, 0.0014461593263847537, 0.0014243110240068893, 0.001279380930544332, 0.0014409153251150667, 0.0013597448367278935, 0.05432382667285585, 0.0011498479519126026, 0.001166844418337352, 0.0011496187924125859, 0.0011476754633233297, 0.0011455334430603788, 0.0011671150000332746, 0.0011497777453515418, 0.0011505808838274936, 0.0011714603508237837, 0.0011532469544299814, 0.0011621974408626556, 0.0011682490233418553, 0.0011568096720851785, 0.001174055743789257, 0.001158328605057715, 0.0011644569756246584, 0.0011680850916173915, 0.001155336487626787, 0.06540327769877433, 0.011804187813838728, 0.003124107790807652, 0.0013281689539854956, 0.0012249314648554075, 0.001226512371389152, 0.005600412094662356, 0.00574416278991415, 0.03582056420909371, 0.06546793544264294, 0.062145090510332304, 0.01103354288200133, 0.0011980359301750744, 0.001208464861414287, 0.0018713935576162714, 0.0016770161157690507, 0.0011804522784036953, 0.001171299185993713, 0.005602675046078687, 0.001215164885358062, 0.06462172534977367, 0.032093742602439815, 0.01801159186017981, 0.0016920316753215915, 0.0013690632091142064, 0.001346850210021055, 0.0019605183259172494, 0.0017980465335164998, 0.09023741881233142, 0.028503821791842753, 0.005655025976688363, 0.0011598370444072888, 0.005447653836982195, 0.002797225067868482, 0.0011909643247680261, 0.002115398068739058, 0.0011596415343499461, 0.0012427721860233782, 0.03720117332467939, 0.05658890490777531, 0.005464466489028446, 0.0012645913493754559, 0.0020744158388223757, 0.005637825767206418, 0.06733559602384218, 0.0055351173261527055, 0.0030477167442874158, 0.0013491478369592926, 0.005673628395185048, 0.0055900336281114885, 0.06548714264956584, 0.003439785116677021, 0.0012043652092214934, 0.004309106999899932, 0.0015338663726525252, 0.0011562498605831764, 0.005247163256023859, 0.0056912141613835515, 0.005509362975676912, 0.0012335909996181726, 0.0011100656742793183, 0.07346700857999887, 0.02623500583725873, 0.0043926082556894, 0.0014794297207710008, 0.0011923097231057147, 0.0012415451874826537, 0.0012074057202309716, 0.0012677651640504253, 0.00572067083585141, 0.004162090394624271, 0.0012611938846232586, 0.0012585517197717415, 0.034672557999011734, 0.006636848045123178, 0.0013542638842536266, 0.00541918144323105, 0.0012836766505050798, 0.07625522453413713, 0.001254647233804991, 0.0011891423491761088, 0.005744571534938417, 0.005600778626949462, 0.005490420583830496, 0.005524848441615007, 0.001215096490002822, 0.001524244860182841, 0.0012148833714512198, 0.0011362299528839283, 0.0020722573254863884, 0.0011230348609387875, 0.001124064069847728, 0.0012380311639199769, 0.0021195033955019576, 0.0013604150234916529, 0.005346450277842408, 0.0059304685348125045, 0.07300386983673847, 0.005749430326030178, 0.00119274134780172, 0.005119901789395615, 0.004781911209182337, 0.001207111952934674, 0.07317466551145582, 0.006843998884717219, 0.001152555769590964, 0.0013523290453608646, 0.06553569681628413, 0.05781764158069394, 0.0011277925114818784, 0.0010933844876194069, 0.0010961920461552434, 0.0010789738383230774, 0.0011091377439930341, 0.00108371137338149, 0.0010969123943860447, 0.002016089022766019, 0.04758297830419398, 0.06448260990653706, 0.0011986914648522819, 0.0012869191854152568, 0.0011810756986960769, 0.0011790714194175116, 0.001183140998554611, 0.0011878386275236343, 0.0011702693246175037, 0.0011290531378066123, 0.0011545255141289428, 0.001151011279394287, 0.001165217860753453, 0.0015822253271200975, 0.0012339321641944522, 0.0011381921845726496, 0.0011381144651623314, 0.001158074394604841, 0.001141297812357025, 0.0033189787226187628, 0.0012434267433533489, 0.001145883162809146, 0.0011423630233810737, 0.001155821186880213, 0.0011400384422348335, 0.0011371242553862028, 0.0011297097202217163, 0.0011136758379464925, 0.001098374768463505, 0.0011098650473624814, 0.0010832776505023587, 0.0010906958595178155, 0.0011618098617682969, 0.0011660987673717182, 0.0010875924409172216, 0.03718374130251103, 0.0076206360678296794, 0.0012097480235794603, 0.0012064627425875081, 0.0012293582549318671, 0.0011924057440875575, 0.001201976510306257, 0.0012002633722109158, 0.0012186511154394857, 0.001205629769301172, 0.0011958934171775052, 0.0012126127907703089, 0.0011985994886260392, 0.011392093093004511, 0.03121582932532007, 0.001142104210461988, 0.0011303769749437655, 0.0011386481400709166, 0.0011293320716250427, 0.0011305695340080664, 0.0012349337691354545, 0.0011401906024751274, 0.001139063255449887, 0.0011335474866723946, 0.0011562095808818243, 0.00115727516010317, 0.0010777066954476542, 0.0010687446974347843, 0.0010754346040748926, 0.0010739919285528189, 0.00107927423317072, 0.0010943781627818596, 0.0010981877215293257, 0.0011885798140970428, 0.001162129673608687, 0.001079598325304687, 0.0010767859071059975, 0.001082434069885071, 0.0010849710954569799, 0.0011702585575539012, 0.0010796033961395191, 0.0012570976274261295, 0.0014282933728726104, 0.0011600170008361685, 0.011416234511903725, 0.006481877257397702, 0.0051343158607609395, 0.010141282721400954, 0.0033352148832839937, 0.001264341186385515, 0.001240634207808694, 0.0011472617449282214, 0.001149844954894899, 0.0011590228139877665, 0.001254090790225323, 0.0011624816512732312, 0.0012622759081856456, 0.0012509856984928944, 0.0059324063490643055, 0.021556922440352136, 0.001182946582259827, 0.0012746656281050555, 0.0010801140208143828, 0.0010723240002123422, 0.0010726502338467643, 0.0010719270483309099, 0.0011474735368866213, 0.0010619363720463806, 0.0010685010022635376, 0.0010645569756973622, 0.0011938393256778633, 0.0011841928353533149, 0.0011791967689384554, 0.0011832109776996943, 0.0011738508582375077, 0.0012744743956339566, 0.001184552837553066, 0.0012879377436759167, 0.0010702303735279414, 0.0010824922802533175, 0.0011669352791423713, 0.001147256836533373, 0.001086246628357574, 0.00113274481385773, 0.0011423828139857843, 0.0011536009788383232, 0.0011214136503376933, 0.001053491117281103, 0.0010759721396404298, 0.0010829923490365578, 0.001139638628702351, 0.0012461031370168162, 0.003542978208252164, 0.0011736893495761377, 0.018060853672322144, 0.0029752107911071805, 0.0013134169993315671, 0.0011930724194403304, 0.001191204814456923, 0.001189136791021325, 0.001112627767048083, 0.0011210904191356413, 0.0011978465116171295, 0.0011797831854024946, 0.0010986159999616617, 0.0011904859065273127, 0.00894766458212723, 0.001168354511867429, 0.0011620548131420863, 0.0011628009757936695, 0.001258828720554363, 0.001128072116718909, 0.0010925903022939035, 0.001102851162296395, 0.001086836626623259, 0.0010849963467524841, 0.0011779354197557928, 0.0011001928130078109, 0.0010827119288922742, 0.0011691367226587825, 0.0010908138586294858, 0.0011653845350063125, 0.0010941329054794339, 0.0012163203967691853, 0.0010878912086576917, 0.001086582418877718, 0.0010889109987548965, 0.0010921735571004277, 0.0481068502331889, 0.008279611697680381, 0.0013495195592038852, 0.001307299138631585, 0.0011848614410345638, 0.0011773833024943637, 0.001183249719635865, 0.0011770544644055326, 0.0011965529776589816, 0.0012937292326674905, 0.001188265419630117, 0.001291813442570179, 0.0012010396268695247, 0.001184509606858672, 0.0011830181154140898, 0.0012007947648377266, 0.0011241281605433933, 0.0011201693032066836, 0.001132986372943188, 0.0012717196977762289, 0.0012086699310646847, 0.0011920907458853583, 0.0012422341875038868, 0.0012024053031318756, 0.0011995609763056734, 0.0011241284177400345, 0.001190138720842295, 0.0012357395580904775, 0.0011563854898477709, 0.0012342471840553159, 0.0011856060929942964, 0.001173559768493612, 0.00116674323176402, 0.0011640120714599656, 0.0011718699297066345, 0.0011722749548552687, 0.001251653722097534, 0.001245754535396605, 0.001226720998475198, 0.0011607938375713866, 0.034437439837601294, 0.02056421209289151, 0.0015543678147328455, 0.0011654110479190252, 0.0011683360018312585, 0.0010923731877186963, 0.001171486232927892, 0.0011872229285451563, 0.001189763232697408, 0.001282780650504973, 0.0011757925358535939, 0.0012792969301213012, 0.0012856679291702634, 0.0011739320917661454, 0.0011852110931000045, 0.0011730304902939255, 0.0012793559066648053, 0.0012514337891886055, 0.0012834563737648518, 0.0011764904186346156, 0.0011944581164307026, 0.0012984788370167101, 0.020875846328170494, 0.0017332342550764944, 0.0011119237223770036, 0.0010910029062828005, 0.0010903741606662785, 0.001093863791158033, 0.0010836196976692178, 0.0011803983023043634, 0.0011497931636133512, 0.0011505188588269574, 0.00125235416060097, 0.003345259676449174, 0.0011559973016034724, 0.0011398593954698637, 0.0011328454670864482, 0.0011381216043996256, 0.0012303443251957381, 0.0011268765340710794, 0.0012615052089744875, 0.0012536904650068906, 0.0011207574902665476, 0.0011384958363419703, 0.0011306854647172744, 0.001129335953940659, 0.0012212109990244688, 0.0012406681604726717, 0.0011348954434398301, 0.0012383621380945972, 0.0011205900227651, 0.0011334765572534051, 0.0012292943970668455, 0.0011457619555077928, 0.0011258261402808997, 0.0011318235598547861, 0.001129028046243759, 0.0011368303256499212, 0.001130038255089244, 0.0012345891851975128, 0.001146144534157979, 0.0010428579285914122, 0.0011514226965562895, 0.0010645947457009622, 0.0011360212093825604, 0.0011343629056111324, 0.0012351635107120803, 0.0012329366507528479, 0.0011336702777635912, 0.0011231960014028604, 0.0011266841618127602, 0.0012225651637066243, 0.0012254686268089816, 0.039066129001290646, 0.00149189502401494, 0.0012348055119387978, 0.0011394655813874548, 0.0011329046520945985, 0.0011296875796519047, 0.0012323071876930635, 0.0011334539780956368, 0.001150789186033572, 0.0011717099561031129, 0.0011366192565494498, 0.001233951627211862, 0.0011400119320294539, 0.001132036090915113, 0.03442461953530911, 0.015911958161416617, 0.030180696698533762, 0.014259570768254614, 0.001060803047229731, 0.0011414070694902262, 0.0011315843723930937, 0.0010599592094158018, 0.0010559833960521014, 0.0010492853240954669, 0.0010475690942257643, 0.001047334349497633, 0.0011228196523260586, 0.0010665254640327983, 0.0011270866501903118, 0.0011242565801800337, 0.0010602578607409499, 0.0011247219544900365, 0.0010457755131430405, 0.0010601079773677642, 0.001054633928592815, 0.0010597188144835623, 0.035666501092681185, 0.011287419650660352, 0.0011860616965392648, 0.0011555207920325703, 0.0011558183712538244, 0.0011564869309104112, 0.0011499497449285415, 0.0011581179772525332, 0.0012932826255902994, 0.0011963827676291382, 0.0011839607654702525, 0.0011868336979776275, 0.001182557557449611, 0.0011916863244719976, 0.0013008689762331372, 0.0012051192096023018, 0.0012005606048935374, 0.0012879956724815244, 0.0011911180931641612, 0.00130162134679944, 0.001223811116286142, 0.0011949286536248617, 0.0011974530224146884, 0.0011893413489857732, 0.0011905438353329203, 0.037189842115134694, 0.01661596972181267, 0.0012275311164557934, 0.003393123489479686, 0.0011113930228274576, 0.0011185940008523853, 0.001117475024900984, 0.001051668137268618, 0.0011381504889361042, 0.0010452283042764595, 0.0011272986749793555, 0.0012345982574706162, 0.001246377603735688, 0.0011592269550229227]
[520.1144512170447, 663.9573280391356, 660.1055075645452, 490.53462145898953, 684.5134150573643, 654.8764490921628, 456.3246726232799, 650.2433808453952, 672.5305517123152, 618.114919398998, 403.53098411861697, 786.3888888750503, 783.5791686417346, 789.2288398956458, 784.8122680870534, 785.91630712518, 760.5566950389193, 757.0038172244522, 773.5811176745804, 29.044633407061934, 65.57105315741981, 161.8878259199138, 679.5620353093312, 212.6656121342122, 759.699274640553, 574.7054928644511, 171.69000958840016, 744.9431047080757, 730.0811407570195, 751.80250680631, 172.05289527131183, 822.2004517126259, 20.521073748720937, 56.12754739866721, 599.9928219539509, 542.1627004945028, 659.2900292380585, 790.016078738756, 778.593177287615, 337.9015340552803, 171.80415489248995, 128.17804075909368, 802.4112053910001, 171.97948002900563, 775.3039373912021, 412.6786512981138, 718.894965839189, 694.6967660715588, 710.0468996036154, 176.5663758841013, 18.584611303468208, 15.155120745678362, 553.4069431191197, 388.05790241584685, 785.4267461415913, 437.9493184784675, 10.93206675268823, 23.354656201078793, 174.35202905459417, 178.00906189024764, 804.119215712351, 774.842653123569, 186.87715667175007, 173.19814875169757, 746.0031927935652, 15.040409415862223, 20.034842520876325, 43.16865549220609, 171.57585829256297, 357.32899757304557, 185.45888084061747, 16.51874419337481, 166.9692038397915, 143.3820048339765, 172.1756763292878, 801.4907425920894, 20.532237893763206, 51.78643023651591, 169.57292448269004, 573.8035079006744, 272.5835830459628, 542.0296945870758, 815.3858538596406, 647.538032356111, 767.6979559564649, 15.948301085780372, 579.9878111595806, 703.4338445550159, 714.396861299591, 716.0544042571297, 13.454065528700943, 166.16611311812906, 188.5169717298068, 781.9464911362206, 14.20206842952674, 30.186906252195083, 81.31976087552128, 188.5518474437617, 176.93029644086175, 764.5126877142143, 305.3368679515802, 168.42605703004708, 664.3224733644398, 263.7755541206973, 182.40881696730796, 173.61520405996086, 172.5942627662052, 763.4248885981673, 743.5850793055117, 45.27069098509142, 17.783129788761702, 183.45122973110279, 738.3830419232903, 837.9303399386795, 175.8429168243264, 173.24970136103337, 167.18476321441506, 13.38700319046783, 36.54532631515475, 759.275567060879, 38.6806779621893, 661.1538117669936, 38.050954539372874, 30.01729987733408, 18.257576501974885, 824.728552187034, 838.1194814913256, 838.2407774816054, 840.0271618416108, 852.9398928655278, 848.9729622244148, 856.3781023826701, 816.1810973210509, 831.367069607757, 826.0370882272607, 828.4233041661531, 822.6680630162253, 92.80632992301406, 31.694998398589796, 293.54402164926734, 362.3951793062282, 447.40220979662485, 801.8889828971714, 870.6965933550755, 892.5503854437279, 887.9550250780871, 830.5859460383335, 815.7502176561476, 809.0273895256113, 816.127794923962, 816.1097789644498, 821.692528149707, 803.2428429739207, 867.7638495426094, 863.5566431226046, 863.256012617951, 810.8466539573127, 802.0780893773775, 873.9221034602324, 862.3998181468216, 812.0839760103066, 858.6463963234581, 858.8162594414647, 863.3319956092653, 818.0323552653006, 44.13706339488503, 632.0325383786317, 815.1640233114366, 817.3064600014349, 818.190062030164, 816.8202201140379, 800.4477432736479, 806.1033499624923, 800.8577602866623, 788.2368234756088, 802.1562247204145, 806.8538292062261, 807.1255145278597, 805.7186681786293, 804.5190478854812, 807.714477141316, 807.5496646738007, 805.944949929626, 807.443503714945, 814.406371377297, 823.0696392230813, 806.9440409921693, 847.5360159738615, 828.0967292230351, 838.028045790771, 840.2553104752682, 842.1591118006232, 838.3971872239998, 835.5938239904997, 834.1647340626693, 835.1151722757685, 814.197035128418, 835.5601174122806, 829.4931338995408, 826.0832034600566, 822.0981364738212, 811.5241254840944, 775.1994305408823, 783.109814077858, 785.4046240809596, 826.7087687623579, 736.9813615661244, 787.2952648907454, 788.8314663208097, 826.2809927744748, 817.1846559327397, 16.49702693894601, 46.68014697631124, 173.03977657877618, 181.16784749033312, 177.83418695612738, 131.95282163538073, 16.577506092123805, 175.93619378354856, 538.1995389175622, 179.0573551301495, 773.8032252959815, 174.88326894702612, 175.0484905147625, 293.26939661079166, 799.2652101996305, 843.7062437400812, 748.8487013950902, 811.4044522145458, 606.0521498547406, 788.4746719672128, 789.5519738338298, 834.5012304056012, 839.6439560088139, 667.2994629272445, 805.5722062426152, 824.3741596877422, 832.5307003155432, 821.3536521278915, 820.5530760361389, 827.0889566349997, 468.83084423494137, 494.33102301870196, 501.9997387439571, 753.8170310113121, 737.1441883032434, 786.380345346507, 258.3886150444069, 793.323228702058, 354.0977373757846, 201.3030674836087, 172.1473891877683, 547.2312069513735, 687.5487916692628, 479.394071307572, 774.01236627078, 174.12929904014163, 810.8982308560694, 173.92153448823922, 386.8731944767201, 170.56485432285749, 19.220427850185764, 15.221657849070038, 31.897854748855252, 773.6123488161682, 641.8602696584012, 625.90503326508, 839.1980267870179, 770.6539007892377, 19.525323262532215, 29.442283424841314, 184.925864455783, 732.8165398601219, 789.3925475531527, 301.1686056718607, 307.17502056897837, 17.517339996791836, 180.0895354319349, 182.96695748019022, 793.1237718676257, 489.52504635464544, 516.7989399436583, 822.1201250289148, 504.37325674896175, 802.0453710122354, 436.64622717486344, 823.3872311318277, 831.690761307739, 824.931771213288, 175.27504432355352, 772.5781755129093, 13.864157068409405, 169.5212928812113, 174.5044664618209, 180.99216113732547, 173.8551027993442, 17.38759457669717, 172.68665537675912, 691.4867412983428, 702.0938426684277, 781.6280328443968, 694.0033064886331, 735.432099456551, 18.408128831242166, 869.6802027925929, 857.0122839726223, 869.8535606758859, 871.3264611445946, 872.9557448173881, 856.8135958937121, 869.7333063219555, 869.1262075147858, 853.6353785228746, 867.1169658490647, 860.4389967144804, 855.9818840159886, 864.4464375868114, 851.7483137321118, 863.312876530554, 858.7693842991172, 856.1020144648434, 865.5487043901223, 15.289753590113119, 84.71569715517764, 320.09138831329426, 752.916258883522, 816.3722042343335, 815.3199456662608, 178.55828876469297, 174.08977366655472, 27.916924874849727, 15.274653053266194, 16.091375711066654, 90.63271976141665, 834.6995067617593, 827.4961332592518, 534.361142759181, 596.2971915397587, 847.1329322624396, 853.7528344234396, 178.4861680849936, 822.9335887247423, 15.474671940239404, 31.15872188505614, 55.51980123482639, 591.0054844628944, 730.4264648576797, 742.473062378902, 510.0691928151905, 556.1591323469623, 11.081877265125682, 35.083014737559886, 176.83384729306044, 862.1900850830556, 183.56526128942954, 357.49715369239544, 839.655713612394, 472.7242663108215, 862.3354462381899, 804.6527040485188, 26.880872580881647, 17.671308565340343, 183.0004817501946, 790.7692872435592, 482.0634230057217, 177.37334236483633, 14.850986091307787, 180.66464377821418, 328.1144817261583, 741.2086152499036, 176.25405302339766, 178.8898004067706, 15.270173037647867, 290.7158342978246, 830.3129253014574, 232.06664397593804, 651.947273784152, 864.8649691474394, 190.57916653383674, 175.70943064931103, 181.509187979602, 810.6414527258428, 900.847601336042, 13.61155189694557, 38.11701076810162, 227.65517473696374, 675.9361299561109, 838.7082488895683, 805.4479289856469, 828.2220162156464, 788.7896184219692, 174.8046739086972, 240.26388309383995, 792.8994995870266, 794.5640884598419, 28.8412524979698, 150.67393334925154, 738.4085270435521, 184.5297136616587, 779.012377927523, 13.113855556904571, 797.0367869598549, 840.9422141031685, 174.07738661064826, 178.5466033576591, 182.13540925171364, 181.00044020532135, 822.9799100133007, 656.0625698157478, 823.1242796627183, 880.1035366668908, 482.5655519230874, 890.4443083485958, 889.6290049867581, 807.734109724428, 471.80863315539636, 735.069800562325, 187.03998878365255, 168.620741199433, 13.697903991067061, 173.9302754000799, 838.4047403429489, 195.31624650910464, 209.12140695539824, 828.4235754346122, 13.665931958970766, 146.113407796284, 867.6369737447886, 739.4650018281263, 15.258859653286281, 17.295759091182177, 886.6879233716814, 914.5913549379778, 912.2489106789043, 926.8065308739856, 901.6012712721102, 922.754918479552, 911.6498319446123, 496.0098431705299, 21.015918625502675, 15.508057156021266, 834.2430302723753, 777.0495702706653, 846.685780686211, 848.1250444472846, 845.2077995958674, 841.8651968616024, 854.5041546969068, 885.6979060725864, 866.1566918722208, 868.8012167233011, 858.2086094641393, 632.0212316536226, 810.4173219707178, 878.5862471683236, 878.64624394996, 863.5023834899837, 876.1954935625306, 301.2975025073295, 804.2291235454203, 872.6893216132859, 875.378473858757, 865.1857323183315, 877.163403402157, 879.4113706249004, 885.1831422710433, 897.9273554537202, 910.4360630924548, 901.0104448071698, 923.1243712415376, 916.8458752947764, 860.7260386634873, 857.5602924732611, 919.4620727196757, 26.893474539434518, 131.22264219144049, 826.6184201245085, 828.8693589122311, 813.4325335908055, 838.6407101428478, 831.9630137740423, 833.1504760975706, 820.5793990836886, 829.4420272814243, 836.1949197447342, 824.6655549169598, 834.3070470906883, 87.78018155540401, 32.035029073819, 875.5768438989415, 884.6606239920524, 878.2344297666166, 885.4791474761347, 884.5099482336353, 809.760025187484, 877.0463445578296, 877.9143697379983, 882.1862442971558, 864.8950990678649, 864.0987333650633, 927.8962487883805, 935.6771569489081, 929.8566330401998, 931.1056940134351, 926.54857242554, 913.7609228770112, 910.5911315484474, 841.3402180817737, 860.4891714835608, 926.2704253619303, 928.6897176130678, 923.8437959608723, 921.6835399461118, 854.5120166352133, 926.2660747232107, 795.4831654940522, 700.1362738166188, 862.0563313116752, 87.5945565902048, 154.27629377873672, 194.7679159442663, 98.60685551046903, 299.8307560367318, 790.9257491316795, 806.0393576977688, 871.6406734738333, 869.6824695738258, 862.795786184201, 797.3904344041389, 860.2286314839723, 792.219825725239, 799.369650032558, 168.56566141288113, 46.38881096162931, 845.3467088003778, 784.5194676557027, 925.8281817747551, 932.5539667133995, 932.270341669321, 932.8993064939381, 871.47979265234, 941.676004630082, 935.8905587187812, 939.3578951891488, 837.6336568006744, 844.4570598179989, 848.0348881045754, 845.1578111150739, 851.896979060408, 784.637183317107, 844.2004174889344, 776.4350450246837, 934.3782654042683, 923.794116819001, 856.945554628309, 871.6444026794086, 920.6012464333439, 882.8113691329577, 875.3633088290173, 866.8508594774257, 891.7316100966564, 949.2248995709094, 929.3920940501134, 923.3675573882044, 877.4711341073527, 802.5017916205638, 282.24841961230237, 852.0142066221667, 55.36836841397357, 336.11063894664915, 761.3728164847315, 838.1720872142023, 839.4861973890742, 840.9461447586033, 898.7731832840229, 891.9887128916847, 834.8314999473257, 847.6133686028422, 910.2361516989529, 839.9931444102799, 111.76100655331659, 855.9045990259061, 860.5446048591242, 859.9923983701943, 794.3892474582404, 886.4681478951741, 915.2561558531964, 906.7406683579725, 920.1014904208229, 921.6620894559804, 848.9429753350297, 908.9315874243041, 923.6067076707125, 855.3319561512518, 916.7466952211391, 858.0858677643113, 913.9657485776956, 822.1517970562855, 919.2095607003413, 920.3167496791039, 918.348699887722, 915.605393940194, 20.7870603698369, 120.77861094381518, 741.0044509395066, 764.9358669713113, 843.9805409878545, 849.3410751464153, 845.1301389767001, 849.5783587253516, 835.7339947926658, 772.9592674799027, 841.5628221439614, 774.1055844800702, 832.6119951649483, 844.2312280201822, 845.2955935082799, 832.781778604055, 889.5782839535033, 892.7221957764084, 882.6231487694546, 786.3368018507797, 827.3557356714641, 838.8623126650533, 805.0011906445548, 831.6663253191953, 833.6383224800562, 889.5780804211104, 840.2381860933586, 809.2320047965825, 864.7635315206542, 810.2104772192719, 843.4504561919545, 852.1082835717908, 857.0866089260121, 859.0976197916461, 853.3370254242633, 853.0421944597988, 798.9430162235205, 802.7263570681159, 815.1812851031244, 861.4794183368516, 29.038163252429914, 48.62816992369345, 643.348370007181, 858.066346449705, 855.9181591875904, 915.4380675420936, 853.6165188221657, 842.301791817159, 840.5033644659027, 779.5565045406204, 850.4901753556607, 781.679355632614, 777.8058216365201, 851.8380296559831, 843.7315561942881, 852.4927597998161, 781.6433212919871, 799.0834262581099, 779.1460780755879, 849.9856727779876, 837.1997194746475, 770.1319201301169, 47.902249531822335, 576.9560560386375, 899.3422659085464, 916.5878424716024, 917.1163771791372, 914.1906040617151, 922.8329848109283, 847.1716691288088, 869.7216435496887, 869.1730625081426, 798.4961694223364, 298.93045584474623, 865.0539223689448, 877.3011864220218, 882.7329314137506, 878.6407323561118, 812.7806009434862, 887.4086643612045, 792.7038215029866, 797.6450550690786, 892.2536843917696, 878.3519166947834, 884.4192582329232, 885.4761034665022, 818.859313254485, 806.0172992744639, 881.1384394751233, 807.5182285035359, 892.387027980546, 882.2414487540526, 813.4747887780561, 872.7816412413588, 888.2366150696188, 883.5299382955951, 885.7175898570179, 879.6387441796154, 884.9257938803369, 809.9860358326542, 872.4903100765168, 958.9033871092103, 868.4907836112931, 939.3245683751416, 880.2652553850739, 881.5520985863469, 809.6094090599335, 811.0716794649477, 882.0906921655521, 890.3165598444173, 887.5601822529095, 817.9523101804718, 816.0143622802624, 25.597621918643707, 670.2884478485839, 809.844133615727, 877.6043930895777, 882.6868158331997, 885.2004908367092, 811.4859752397018, 882.2590235910077, 868.9688885995728, 853.4535315598172, 879.8020922466167, 810.40453932503, 877.1838012430238, 883.3640623521305, 29.048977548591537, 62.84581632603864, 33.133761290824744, 70.12833810020784, 942.6820582873353, 876.1116228644166, 883.7166935110486, 943.4325312868895, 946.9845868207769, 953.0296260095383, 954.5909721010607, 954.8049297530083, 890.6149780406653, 937.6241203081556, 887.2432299958012, 889.4766707434919, 943.1667870881529, 889.1086334785854, 956.228165062439, 943.3001367304001, 948.1963104811996, 943.6465469260681, 28.0375133350325, 88.59420761781429, 843.1264603838379, 865.4106502410848, 865.1878399503256, 864.6876789284412, 869.6032191060145, 863.4698879058546, 773.2261921817476, 835.8528951246027, 844.6225830826533, 842.5780306912474, 845.6248016854859, 839.1469965412849, 768.7169255858889, 829.7934279298454, 832.9442061683153, 776.4001241350013, 839.5473175489568, 768.2725874609405, 817.1195592949556, 836.8700482379946, 835.1058298583436, 840.8015082068436, 839.952272501048, 26.889062795806876, 60.183065854245434, 814.6433003566249, 294.7137064420086, 899.7717094317667, 893.9794056091711, 894.8745857551554, 950.8703026766503, 878.6184337844098, 956.7287796442062, 887.0763553574773, 809.9800837632401, 802.325071473335, 862.6438469766482]
Elapsed: 0.2731657783461063~0.6296862729125117
Time per graph: 0.006352124890911341~0.01464404182992756
Speed: 643.2956927200837~312.8722059025725
Total Time: 0.0518
best val loss: 0.1990262120962143 test_score: 0.8140

Testing...
Test loss: 0.5738 score: 0.8140 time: 0.05s
test Score 0.8140
Epoch Time List: [0.6273739230819046, 0.28242024790961295, 0.29188285302370787, 0.3171775030205026, 0.30749962106347084, 0.2928375790361315, 0.4048267911421135, 0.28354904404841363, 0.3434214540757239, 0.3817248650593683, 0.31397711706813425, 0.3308772360906005, 0.26345525309443474, 0.2610588950337842, 0.35844664997421205, 0.2897656738059595, 0.2690707731526345, 0.7012239169562235, 0.7781725730746984, 3.3545097650494426, 8.502680602134205, 1.9824120560660958, 0.6292500160634518, 0.45426276698708534, 0.305009420029819, 0.43178983009420335, 0.6242573501076549, 0.3452781628584489, 0.5385527510661632, 0.5484014640096575, 0.7015934339724481, 0.8073326209560037, 3.1065023510018364, 5.2320361798629165, 0.8236266110325232, 0.28907948499545455, 0.3250760938972235, 0.276082884054631, 0.27269734803121537, 0.41918843088205904, 1.0323780310573056, 7.078759087016806, 4.683296793024056, 0.7396085800137371, 0.3039977248990908, 0.3129833649145439, 0.2646810448495671, 0.419016528991051, 0.5546847431687638, 0.7409440730698407, 10.114863075083122, 11.606216792948544, 0.8519249049713835, 0.35785679006949067, 0.26860425306949764, 0.3174889509100467, 11.269803594914265, 12.733366462751292, 1.3126564208650962, 0.9707220048876479, 0.7111622251104563, 0.3003629300510511, 0.6442550910869613, 0.4877127088839188, 0.8572243720991537, 5.907176528009586, 5.207922377972864, 3.5405913398135453, 0.9088622179115191, 1.1699371819850057, 0.9721661730436608, 9.247156074969098, 9.183080699061975, 0.7813996498007327, 1.1499148270813748, 0.7964258930878714, 3.968752152984962, 6.189746639109217, 1.7797678840579465, 0.8121473939390853, 0.40535975713282824, 0.3503764070337638, 0.2560773299774155, 0.267909261980094, 0.267229849123396, 9.186403407831676, 6.994171160040423, 0.37457812402863055, 0.4118984378874302, 0.2928269359981641, 5.771025559050031, 10.087341368896887, 0.9880453641526401, 0.7937377709895372, 4.516462257015519, 11.689222458051518, 2.877689504995942, 1.2760230309795588, 1.1980382819892839, 0.7845205818302929, 0.3575154399732128, 0.5693851679097861, 1.145193472970277, 0.966559075168334, 0.972661197069101, 0.9761343859136105, 0.732827533967793, 0.29302245809230953, 0.5332234450615942, 4.629004194983281, 4.543425768963061, 1.039451921125874, 0.7715998260537162, 0.5069630218204111, 0.7555627500405535, 0.539278898970224, 0.7648417529417202, 6.546058761887252, 9.2672432619147, 6.127344141015783, 3.761330191977322, 0.27465940883848816, 1.3644524540286511, 12.24933265009895, 12.592166770016775, 0.4562857720302418, 0.2520481119863689, 0.24762389296665788, 0.24923287390265614, 0.24560985807329416, 0.24487597588449717, 0.23977535997983068, 0.25139853300061077, 0.24796154897194356, 0.24833988584578037, 0.24681403301656246, 0.24741142406128347, 1.9027179450495169, 3.456496559898369, 1.1191298860358074, 0.5310655101202428, 0.8237200060393661, 0.4055156089598313, 0.24659488792531192, 0.23239249584730715, 0.2299458709312603, 0.23364062304608524, 0.24995635694358498, 0.252101902035065, 0.25088858196977526, 0.2512731129536405, 0.25091867288574576, 0.26235188893042505, 0.2363582740072161, 0.23861550691071898, 0.2376888960134238, 0.24385588697623461, 0.25986924511380494, 0.24602892098482698, 0.2374490649672225, 0.24045027210377157, 0.23923188308253884, 0.2415611050091684, 0.2392266399692744, 0.24299006292130798, 1.175215742085129, 4.064770563156344, 0.343210291932337, 0.25779384293127805, 0.2516986469272524, 0.2511188789503649, 0.2569526460720226, 0.2564568258821964, 0.25507944100536406, 0.2576593061676249, 0.25589306093752384, 0.2561425620224327, 0.25543346104677767, 0.2552934919949621, 0.25837078399490565, 0.25564540293999016, 0.259177676984109, 0.25865718396380544, 0.2552183329826221, 0.25828207982704043, 0.25514954898972064, 0.25893274403642863, 0.25373429094906896, 0.2557644569315016, 0.25094569695647806, 0.24740886501967907, 0.24648107797838748, 0.2478249000851065, 0.2465595561079681, 0.24715339089743793, 0.24645857291761786, 0.2513543780660257, 0.24892057292163372, 0.31027740298304707, 0.2476877380395308, 0.24710890802089125, 0.24818829901050776, 0.25807276007253677, 0.2672375339316204, 0.26232403505127877, 0.3214286221191287, 0.25323131296318024, 2.2593817898305133, 0.2637132831150666, 0.24580584012437612, 0.24611397401895374, 5.488480814034119, 11.05813836993184, 2.4461172689916566, 0.9814561330713332, 1.2054284259211272, 1.3204096589470282, 7.824173953034915, 7.224987281952053, 0.5716367108980194, 0.658100995933637, 0.8306864638580009, 6.847434633877128, 0.997626302880235, 0.9513561221538112, 0.2560890691820532, 0.25499752711039037, 0.24823785410262644, 0.25577145896386355, 0.34587272501084954, 0.2661615351680666, 0.26602274598553777, 0.33515883795917034, 0.4203808900201693, 0.5874592080945149, 0.38535234390292317, 0.26613897806964815, 0.24990107596386224, 0.3553966430481523, 0.3373570990515873, 0.2746634219074622, 0.399646777077578, 0.37029502203222364, 0.8213337928755209, 0.8563234428875148, 0.27763853897340596, 0.3978428039699793, 0.3868177430704236, 0.31839665106963366, 0.34444998402614146, 0.5836167879169807, 0.5130420229397714, 0.6851019220193848, 0.2824799990048632, 0.6443031689850613, 0.2681776729878038, 0.661328217945993, 0.27882910007610917, 0.9832760880235583, 0.7547123051481321, 1.0122823399724439, 7.999136421829462, 14.15175046690274, 6.265366062987596, 1.1753663930576295, 0.295343813020736, 0.30482868291437626, 0.44482639094348997, 0.5917794859269634, 2.7019400239223614, 9.137120847124606, 1.7220185599289834, 0.8018212190363556, 0.25949935405515134, 0.35743554204236716, 0.8754133590264246, 6.059197047143243, 6.3931684219278395, 0.9406423439504579, 0.27287619304843247, 0.29586248903069645, 0.2898216029861942, 0.3517750420141965, 0.45972332602832466, 0.799112272914499, 0.383204170037061, 0.2637669040123001, 0.26240019185934216, 0.24562082497868687, 0.5481750210747123, 0.8176149479113519, 8.294025685056113, 3.4618036050815135, 0.9753484519897029, 0.9765192810446024, 0.9851221090648323, 10.77595985494554, 6.592725892085582, 0.4398658679565415, 0.3731469390913844, 0.2785982668865472, 0.2940515710506588, 0.27749750891234726, 12.818504371098243, 7.530364376027137, 0.23514550703112036, 0.23897964786738157, 0.23905174899846315, 0.23848134407307953, 0.24206190183758736, 0.2381957300240174, 0.2386631880654022, 0.24143662303686142, 0.24041684693656862, 0.24293789092916995, 0.24090340198017657, 0.2398495809175074, 0.24078708898741752, 0.24174471606966108, 0.2395887691527605, 0.24208700889721513, 0.24053795891813934, 3.1801951830275357, 9.162430218886584, 0.7665183661738411, 0.3708680820418522, 0.2982014500303194, 0.27547464589588344, 0.470342198968865, 0.7392057289835066, 6.570879652048461, 5.668789776042104, 8.824478239985183, 3.3294207100989297, 0.8474947719369084, 0.5181838640710339, 0.34481039899401367, 0.4000353600131348, 0.28787610097788274, 0.2800041629234329, 0.7395415829960257, 0.27013777394313365, 3.5222537711961195, 6.832898594904691, 3.3020268068648875, 2.708349593100138, 0.2765918660443276, 0.362110237008892, 0.5554180049803108, 0.3680529519915581, 7.210860121995211, 11.169446500018239, 2.7314095600740984, 0.48624093900434673, 0.5122504478786141, 0.7142654260387644, 0.25203239487018436, 0.5059778169961646, 0.3002174539724365, 0.5583118259673938, 4.882083059055731, 10.7787213671254, 4.856119670090266, 0.27777959196828306, 0.4226209131302312, 1.1052147329319268, 9.211090137949213, 7.151122009032406, 0.45164866594132036, 0.26301153178792447, 0.8188022988615558, 0.9949660659767687, 6.0495733838761225, 8.015437382971868, 0.39324474800378084, 0.38794858113396913, 0.36252082500141114, 0.3883388029644266, 0.44564951001666486, 0.9583166280062869, 1.0077033849665895, 12.010703230160289, 0.2858324348926544, 5.250087955966592, 11.580653337878175, 1.23472515901085, 0.2681176968617365, 0.2657218559179455, 0.25249543611425906, 0.25443339510820806, 0.37914409302175045, 0.528883937979117, 0.4098149719648063, 0.8496511608827859, 0.5352866570465267, 1.955533431028016, 7.473850509966724, 0.4477466370444745, 0.4500718569615856, 0.7949226308846846, 3.795334210852161, 3.553940115030855, 0.7450749130221084, 0.5131311909062788, 0.8965026859659702, 1.2113751959986985, 1.0991050739539787, 0.5660013819579035, 0.2703976259799674, 0.396901109139435, 0.2491836390690878, 0.2957602139795199, 0.250781113980338, 0.26078922499436885, 0.24895443103741854, 0.5200514829484746, 0.3886752740945667, 0.6799960010685027, 1.008504530065693, 7.777123701875098, 3.1530102690448985, 0.797965359990485, 0.9328491479391232, 0.9582588670309633, 0.6446702380198985, 3.8887059229891747, 8.125001957989298, 0.6447040969505906, 0.2511120259296149, 5.4310006950981915, 12.561067359056324, 1.9359577490249649, 0.2416092228377238, 0.23856848303694278, 0.23443122499156743, 0.23894082789774984, 0.23856944299768656, 0.23502432403620332, 0.2986478890525177, 4.749269701074809, 6.816364585887641, 0.4740970510756597, 0.26060833607334644, 0.2561209298437461, 0.2537881010212004, 0.2562864888459444, 0.2596966669661924, 0.25454484194051474, 1.0838314198190346, 0.24997622694354504, 0.24968924699351192, 0.2506291129393503, 5.307506374898367, 0.509657529881224, 0.24886861792765558, 0.2489193129586056, 0.2518174289725721, 0.24725438910536468, 0.3402158261742443, 0.2532954268390313, 0.25081912195309997, 0.2508993148803711, 0.24917545495554805, 0.24722879996988922, 0.2460904101608321, 0.24848547403234988, 0.23671152698807418, 0.24209485296159983, 0.2467492651194334, 0.2394628580659628, 0.24094829487148672, 0.24140341696329415, 0.2397649158956483, 0.23639315704349428, 3.652102171909064, 5.169021739973687, 0.9415523660136387, 0.2628145660273731, 0.26482908497564495, 0.2593913950258866, 0.2637595258420333, 0.25828753609675914, 0.25670561904553324, 0.26250861899461597, 0.2592916270950809, 0.2587915159529075, 0.2606126479804516, 1.1609618470538408, 4.3101827329955995, 0.7514072860358283, 0.24654932296834886, 0.2445811639772728, 0.24205213296227157, 0.24197838304098696, 0.25019255105871707, 0.2523540408583358, 0.2505587110063061, 0.24388902401551604, 0.24670150107704103, 0.24766771192662418, 0.2385317940497771, 0.23118459910620004, 0.22885233513079584, 0.2328123050974682, 0.2289583720266819, 0.2363271559588611, 0.23998047003988177, 0.2450643969932571, 0.2523872611345723, 0.24391698196996003, 0.2334396420046687, 0.2341840370791033, 0.24740584183018655, 0.2518581800395623, 0.24704725795891136, 0.25138870999217033, 0.25558536301832646, 0.2394710968947038, 0.7110260581830516, 4.553387455875054, 0.8030698928050697, 1.3078488850733265, 1.5095766820013523, 0.25623905297834426, 0.24906329298391938, 0.24571741407271475, 0.24729916697833687, 0.24917527299840003, 0.25469266320578754, 0.2541895848698914, 0.2571553500602022, 0.2554009520681575, 0.4552640749607235, 5.5517277620965615, 0.877231667865999, 0.2599526181584224, 0.23835525999311358, 0.22849908890202641, 0.22838369314558804, 0.22834644315298647, 0.23296432802453637, 0.22750013100448996, 0.2288941330043599, 0.22737454692833126, 0.2456344481324777, 0.25289839087054133, 0.2518721451051533, 0.2562515091849491, 0.2525890119140968, 0.25629476399626583, 0.25213696621358395, 0.25710749707650393, 0.24595230992417783, 0.22776470705866814, 0.23948800307698548, 0.2338346210308373, 0.2318956529488787, 0.23652013100218028, 0.24550589302089065, 0.32202857092488557, 0.24100333102978766, 0.22995654097758234, 0.2299417608883232, 0.23185587697662413, 0.23696264997124672, 0.2521460020216182, 0.35137064999435097, 0.25519159506075084, 1.9485441269353032, 7.5473770450335, 0.26818014588207006, 0.26287085411604494, 0.2593652099603787, 0.24509223003406078, 0.24131890095304698, 0.25003236101474613, 0.2450993040110916, 0.2446697698906064, 0.24116422596853226, 0.24484288098756224, 4.516854716115631, 0.31440753291826695, 0.25318853685166687, 0.2507338161813095, 0.2547612569760531, 0.24690408702008426, 0.23705801100004464, 0.24088509392458946, 0.24008486198727041, 0.236402704147622, 0.23953236010856926, 0.2365633308654651, 0.2386545578483492, 0.2393813340459019, 0.23648262105416507, 0.23828656203113496, 0.24040284287184477, 0.2423182650236413, 0.2368183081271127, 0.23607453994918615, 0.2367569199996069, 0.2357264329912141, 2.2587879459606484, 2.641386840841733, 0.646306615206413, 0.2671332749305293, 0.25858425779733807, 0.25558114307932556, 0.25658660498447716, 0.2557510599726811, 0.25764781690668315, 0.26115426211617887, 0.25606544502079487, 0.2618172891670838, 0.25966609094757587, 0.25612332695163786, 0.2616529590450227, 0.24744053708855063, 0.243715162971057, 0.24704166885931045, 0.24867054598871619, 0.25373433309141546, 0.24903856788296252, 0.2482726201415062, 0.25048877589870244, 0.24546448898036033, 0.24600232089869678, 0.2448474350385368, 0.2597698679892346, 0.25584262202028185, 0.2522326309699565, 0.25577445107046515, 0.25359588803257793, 0.25732513109687716, 0.25175800488796085, 0.25280325289350003, 0.2569303489290178, 0.2578823951771483, 0.2565739678684622, 0.2585072460351512, 0.25658667006064206, 0.2557605541078374, 5.360352326999418, 6.91185355593916, 1.5225715570850298, 0.276982928160578, 0.240373644977808, 0.23423127690330148, 0.24038191209547222, 0.2534644139232114, 0.25318324402906, 0.26139626291114837, 0.25477914104703814, 0.25833573495037854, 0.2586293750209734, 0.25401928008068353, 0.25828155188355595, 0.2527515438850969, 0.2566141129937023, 0.2603370960569009, 0.2617968649137765, 0.25429431116208434, 0.257631937158294, 0.2616120890015736, 3.23318883695174, 1.8689493569545448, 0.2585076860850677, 0.23318776697851717, 0.23405822680797428, 0.30128438386600465, 0.23433074005879462, 0.23418364208191633, 0.23428978316951543, 0.24207030714023858, 0.250093684066087, 0.33902516192756593, 0.24673409096430987, 0.24308230495080352, 0.24332826503086835, 0.24527042906265706, 0.24847566697280854, 0.24331824097316712, 0.24172301508951932, 0.290719592012465, 0.24264500895515084, 0.245293450076133, 0.24257298314478248, 0.24086917389649898, 0.24449224106501788, 0.24555706593673676, 0.2430059410398826, 0.2468762049684301, 0.24138411891181022, 0.24190081504639238, 0.2459800848737359, 0.24395793885923922, 0.24357823398895562, 0.24429172999225557, 0.24405179591849446, 0.24818715313449502, 0.24363074696157128, 0.24669443594757468, 0.2424898979952559, 0.235026124981232, 0.22934080206323415, 0.23064618394710124, 0.23281968606170267, 0.23384727106895298, 0.25146004487760365, 0.24596065701916814, 0.2419377400074154, 0.2417485989863053, 0.24450377118773758, 0.2472100859740749, 0.24683610687498003, 3.837764453026466, 3.447231473051943, 0.46821122092660517, 0.2464001470943913, 0.2461916480679065, 0.2421236268710345, 0.24919126403983682, 0.2491668350994587, 0.2464901237981394, 0.25219980091787875, 0.2490407869918272, 0.2513856200966984, 0.2493349889991805, 0.2466539138695225, 2.3122418430866674, 4.57503135188017, 3.031525473925285, 1.597139487043023, 0.23174782609567046, 0.23129848681855947, 0.2345301618333906, 0.22986581595614552, 0.22812926105689257, 0.23161140293814242, 0.2302873310400173, 0.2269389359280467, 0.23049362609162927, 0.2315416120691225, 0.2320942240767181, 0.23247498308774084, 0.22870883508585393, 0.2322409839835018, 0.227919518831186, 0.23239909403491765, 0.23052972997538745, 0.22834676899947226, 4.65833384601865, 1.3306056199362502, 0.25738699710927904, 0.24908077588770539, 0.2461412539705634, 0.2499452669871971, 0.24530721607152373, 0.24472472292836756, 0.26139405800495297, 0.2590932857710868, 0.2537343630101532, 0.2528614840703085, 0.2528482669731602, 0.2529671409865841, 0.2579013390932232, 0.25971468386705965, 0.25873691716697067, 0.2576029871124774, 0.25533171894494444, 0.25890741194598377, 0.2583008299116045, 0.2577652661129832, 0.25426871003583074, 0.2555857260012999, 0.2597310980781913, 2.2874419110594317, 3.969581139041111, 1.0635315730469301, 0.346740146051161, 0.22796882397960871, 0.22724769893102348, 0.22639941202942282, 0.2271918870974332, 0.23340632196050137, 0.22501000110059977, 0.3162978378823027, 0.2504232720239088, 0.24961859406903386, 0.24832458898890764]
Total Epoch List: [11, 320, 459]
Total Time List: [0.10935878101736307, 1.5431462450651452, 0.051788583979941905]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351db668f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.3640;  Loss pred: 3.3640; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 3.4144;  Loss pred: 3.4144; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 3.3569;  Loss pred: 3.3569; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 3.3655;  Loss pred: 3.3655; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 2.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 1.15s
Epoch 5/1000, LR 0.000090
Train loss: 3.2947;  Loss pred: 3.2947; Loss self: 0.0000; time: 1.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 3.3456;  Loss pred: 3.3456; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 3.2881;  Loss pred: 3.2881; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 3.2806;  Loss pred: 3.2806; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 3.2513;  Loss pred: 3.2513; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 3.1726;  Loss pred: 3.1726; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 3.0598;  Loss pred: 3.0598; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 3.0213;  Loss pred: 3.0213; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 3.0060;  Loss pred: 3.0060; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 2.9746;  Loss pred: 2.9746; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 2.8749;  Loss pred: 2.8749; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 2.8499;  Loss pred: 2.8499; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 2.7866;  Loss pred: 2.7866; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 18/1000, LR 0.000270
Train loss: 2.6985;  Loss pred: 2.6985; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 2.6882;  Loss pred: 2.6882; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 2.6836;  Loss pred: 2.6836; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 21/1000, LR 0.000270
Train loss: 2.6250;  Loss pred: 2.6250; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 018,   Train_Loss: 2.6882,   Val_Loss: 0.6931,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6931,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6923


[0.05763421696610749, 0.05268593202345073, 0.05269372195471078, 1.1601972279604524, 0.059166791033931077, 0.05830445501487702, 0.05859927192796022, 0.05849420605227351, 0.05895744392182678, 0.058564196922816336, 0.0599090059986338, 0.05998086207546294, 0.061176410992629826, 0.060593164060264826, 0.05962545203510672, 0.058340215939097106, 0.05812178598716855, 0.058072637068107724, 0.058020789991132915, 0.05795856099575758, 0.05787855095695704]
[0.0013098685674115338, 0.0011974075459875166, 0.0011975845898797904, 0.02636811881728301, 0.0013446997962257062, 0.0013251012503381141, 0.0013318016347263685, 0.001329413773915307, 0.001339941907314245, 0.0013310044755185531, 0.0013615683181507682, 0.001363201410805976, 0.0013903729771052233, 0.0013771173650060189, 0.001355123909888789, 0.0013259139986158434, 0.001320949681526558, 0.0013198326606388118, 0.0013186543179802936, 0.0013172400226308541, 0.0013154216126581146]
[763.4353742651651, 835.1375464026224, 835.0140845586336, 37.924586388944405, 743.6604086702421, 754.6593135768599, 750.8625713659374, 752.211252524383, 746.3010109179895, 751.3122745965259, 734.4471714487034, 733.5673159322528, 719.2314698765323, 726.1545206029912, 737.9398981175581, 754.1967284785638, 757.0311072291173, 757.6718093307301, 758.3488609294072, 759.1630855572948, 760.212535948279]
Elapsed: 0.1107130904704155~0.23468049815618433
Time per graph: 0.002516206601600352~0.0053336476853678245
Speed: 722.3087107961301~155.57026079003296
Total Time: 0.0583
best val loss: 0.6931474208831787 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.05s
test Score 0.5000
Epoch Time List: [0.24144823208916932, 0.23756754712667316, 0.2251020959811285, 3.434465694008395, 1.6451996190007776, 0.25691361108329147, 0.2523142059799284, 0.25170884712133557, 0.251293208100833, 0.2512686770642176, 0.2530690210405737, 0.26223690703045577, 0.261356764822267, 0.26592284301295877, 0.2648622450651601, 0.2626241809921339, 0.2538929469883442, 0.24912723200395703, 0.2498031910508871, 0.25082546996418387, 0.2470288008917123]
Total Epoch List: [21]
Total Time List: [0.05826871597673744]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351db2dfc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.3994;  Loss pred: 2.3994; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.3744;  Loss pred: 2.3744; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.3896;  Loss pred: 2.3896; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6992 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 2.4065;  Loss pred: 2.4065; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 2.3702;  Loss pred: 2.3702; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5116 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 2.3791;  Loss pred: 2.3791; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 2.3489;  Loss pred: 2.3489; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 2.3086;  Loss pred: 2.3086; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 2.3065;  Loss pred: 2.3065; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 2.2944;  Loss pred: 2.2944; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 2.2462;  Loss pred: 2.2462; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 2.1624;  Loss pred: 2.1624; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 2.1478;  Loss pred: 2.1478; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 2.1488;  Loss pred: 2.1488; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 2.1041;  Loss pred: 2.1041; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 2.0764;  Loss pred: 2.0764; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 2.0418;  Loss pred: 2.0418; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 2.0153;  Loss pred: 2.0153; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 1.9873;  Loss pred: 1.9873; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 1.9592;  Loss pred: 1.9592; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 1.9334;  Loss pred: 1.9334; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 1.8962;  Loss pred: 1.8962; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 1.8782;  Loss pred: 1.8782; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 1.8672;  Loss pred: 1.8672; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 1.7983;  Loss pred: 1.7983; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 1.8180;  Loss pred: 1.8180; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 1.7591;  Loss pred: 1.7591; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 1.7517;  Loss pred: 1.7517; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 1.7527;  Loss pred: 1.7527; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 1.7021;  Loss pred: 1.7021; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 1.7072;  Loss pred: 1.7072; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 1.6861;  Loss pred: 1.6861; Loss self: 0.0000; time: 0.16s
Val loss: 0.6925 score: 0.5227 time: 0.05s
Test loss: 0.6925 score: 0.5349 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 1.6503;  Loss pred: 1.6503; Loss self: 0.0000; time: 0.17s
Val loss: 0.6925 score: 0.5455 time: 0.05s
Test loss: 0.6925 score: 0.5581 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 1.6463;  Loss pred: 1.6463; Loss self: 0.0000; time: 0.16s
Val loss: 0.6925 score: 0.6136 time: 0.05s
Test loss: 0.6925 score: 0.6047 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 1.6203;  Loss pred: 1.6203; Loss self: 0.0000; time: 0.16s
Val loss: 0.6925 score: 0.6364 time: 0.05s
Test loss: 0.6926 score: 0.5814 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 1.5966;  Loss pred: 1.5966; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 1.5846;  Loss pred: 1.5846; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 1.5367;  Loss pred: 1.5367; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 1.5332;  Loss pred: 1.5332; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 1.5476;  Loss pred: 1.5476; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 1.5032;  Loss pred: 1.5032; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 1.4996;  Loss pred: 1.4996; Loss self: 0.0000; time: 1.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.56s
Epoch 43/1000, LR 0.000269
Train loss: 1.4903;  Loss pred: 1.4903; Loss self: 0.0000; time: 1.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 1.23s
Epoch 44/1000, LR 0.000269
Train loss: 1.4611;  Loss pred: 1.4611; Loss self: 0.0000; time: 3.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 1.4472;  Loss pred: 1.4472; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 1.4258;  Loss pred: 1.4258; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 1.4233;  Loss pred: 1.4233; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 1.4000;  Loss pred: 1.4000; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 1.3961;  Loss pred: 1.3961; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 1.3850;  Loss pred: 1.3850; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 1.3662;  Loss pred: 1.3662; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 1.3661;  Loss pred: 1.3661; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 1.3621;  Loss pred: 1.3621; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.58s
Epoch 54/1000, LR 0.000269
Train loss: 1.3484;  Loss pred: 1.3484; Loss self: 0.0000; time: 1.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.65s
Epoch 55/1000, LR 0.000269
Train loss: 1.3425;  Loss pred: 1.3425; Loss self: 0.0000; time: 5.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.40s
Epoch 56/1000, LR 0.000269
Train loss: 1.3267;  Loss pred: 1.3267; Loss self: 0.0000; time: 2.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.44s
Epoch 57/1000, LR 0.000269
Train loss: 1.3096;  Loss pred: 1.3096; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 1.3008;  Loss pred: 1.3008; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 1.2931;  Loss pred: 1.2931; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 1.2772;  Loss pred: 1.2772; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 1.2859;  Loss pred: 1.2859; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 1.2658;  Loss pred: 1.2658; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 1.2580;  Loss pred: 1.2580; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 1.2505;  Loss pred: 1.2505; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 1.2449;  Loss pred: 1.2449; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 1.2423;  Loss pred: 1.2423; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 1.2443;  Loss pred: 1.2443; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 1.2279;  Loss pred: 1.2279; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 1.2170;  Loss pred: 1.2170; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 1.2128;  Loss pred: 1.2128; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 1.2012;  Loss pred: 1.2012; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4884 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 1.2038;  Loss pred: 1.2038; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.38s
Epoch 73/1000, LR 0.000267
Train loss: 1.1928;  Loss pred: 1.1928; Loss self: 0.0000; time: 1.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.48s
Epoch 74/1000, LR 0.000267
Train loss: 1.1887;  Loss pred: 1.1887; Loss self: 0.0000; time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4884 time: 0.78s
Epoch 75/1000, LR 0.000267
Train loss: 1.1842;  Loss pred: 1.1842; Loss self: 0.0000; time: 1.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4884 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 1.1829;  Loss pred: 1.1829; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 1.1650;  Loss pred: 1.1650; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4884 time: 0.05s
Epoch 78/1000, LR 0.000267
Train loss: 1.1562;  Loss pred: 1.1562; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4884 time: 0.05s
Epoch 79/1000, LR 0.000267
Train loss: 1.1657;  Loss pred: 1.1657; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 1.1596;  Loss pred: 1.1596; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4884 time: 0.05s
Epoch 81/1000, LR 0.000267
Train loss: 1.1626;  Loss pred: 1.1626; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4884 time: 0.05s
Epoch 82/1000, LR 0.000267
Train loss: 1.1432;  Loss pred: 1.1432; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.05s
Epoch 83/1000, LR 0.000266
Train loss: 1.1490;  Loss pred: 1.1490; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.4884 time: 0.05s
Epoch 84/1000, LR 0.000266
Train loss: 1.1460;  Loss pred: 1.1460; Loss self: 0.0000; time: 0.16s
Val loss: 0.6869 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 1.1277;  Loss pred: 1.1277; Loss self: 0.0000; time: 0.16s
Val loss: 0.6866 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4884 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 1.1348;  Loss pred: 1.1348; Loss self: 0.0000; time: 0.17s
Val loss: 0.6864 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4884 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 1.1282;  Loss pred: 1.1282; Loss self: 0.0000; time: 0.18s
Val loss: 0.6861 score: 0.5227 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4884 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 1.1246;  Loss pred: 1.1246; Loss self: 0.0000; time: 0.16s
Val loss: 0.6859 score: 0.5227 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.4884 time: 0.05s
Epoch 89/1000, LR 0.000266
Train loss: 1.1223;  Loss pred: 1.1223; Loss self: 0.0000; time: 0.16s
Val loss: 0.6856 score: 0.5227 time: 0.05s
Test loss: 0.6873 score: 0.5116 time: 0.05s
Epoch 90/1000, LR 0.000266
Train loss: 1.1120;  Loss pred: 1.1120; Loss self: 0.0000; time: 0.17s
Val loss: 0.6853 score: 0.5227 time: 1.86s
Test loss: 0.6871 score: 0.5116 time: 1.31s
Epoch 91/1000, LR 0.000266
Train loss: 1.1167;  Loss pred: 1.1167; Loss self: 0.0000; time: 2.40s
Val loss: 0.6850 score: 0.5227 time: 0.05s
Test loss: 0.6869 score: 0.5116 time: 0.05s
Epoch 92/1000, LR 0.000266
Train loss: 1.1027;  Loss pred: 1.1027; Loss self: 0.0000; time: 0.16s
Val loss: 0.6848 score: 0.5455 time: 0.05s
Test loss: 0.6866 score: 0.5116 time: 0.05s
Epoch 93/1000, LR 0.000265
Train loss: 1.1103;  Loss pred: 1.1103; Loss self: 0.0000; time: 0.16s
Val loss: 0.6845 score: 0.5455 time: 0.05s
Test loss: 0.6864 score: 0.5116 time: 0.05s
Epoch 94/1000, LR 0.000265
Train loss: 1.1018;  Loss pred: 1.1018; Loss self: 0.0000; time: 0.15s
Val loss: 0.6842 score: 0.5455 time: 0.05s
Test loss: 0.6861 score: 0.5116 time: 0.05s
Epoch 95/1000, LR 0.000265
Train loss: 1.0985;  Loss pred: 1.0985; Loss self: 0.0000; time: 0.15s
Val loss: 0.6838 score: 0.5455 time: 0.05s
Test loss: 0.6859 score: 0.5349 time: 0.05s
Epoch 96/1000, LR 0.000265
Train loss: 1.0962;  Loss pred: 1.0962; Loss self: 0.0000; time: 0.15s
Val loss: 0.6835 score: 0.5455 time: 0.05s
Test loss: 0.6856 score: 0.5349 time: 0.05s
Epoch 97/1000, LR 0.000265
Train loss: 1.0975;  Loss pred: 1.0975; Loss self: 0.0000; time: 0.16s
Val loss: 0.6832 score: 0.5455 time: 0.05s
Test loss: 0.6853 score: 0.5349 time: 0.05s
Epoch 98/1000, LR 0.000265
Train loss: 1.0945;  Loss pred: 1.0945; Loss self: 0.0000; time: 0.15s
Val loss: 0.6829 score: 0.5455 time: 0.05s
Test loss: 0.6851 score: 0.5349 time: 0.05s
Epoch 99/1000, LR 0.000265
Train loss: 1.0846;  Loss pred: 1.0846; Loss self: 0.0000; time: 0.15s
Val loss: 0.6825 score: 0.5455 time: 0.05s
Test loss: 0.6848 score: 0.5349 time: 0.05s
Epoch 100/1000, LR 0.000265
Train loss: 1.0874;  Loss pred: 1.0874; Loss self: 0.0000; time: 0.15s
Val loss: 0.6822 score: 0.5455 time: 0.05s
Test loss: 0.6845 score: 0.5349 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 1.0809;  Loss pred: 1.0809; Loss self: 0.0000; time: 0.15s
Val loss: 0.6818 score: 0.5455 time: 0.05s
Test loss: 0.6842 score: 0.5349 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 1.0796;  Loss pred: 1.0796; Loss self: 0.0000; time: 0.15s
Val loss: 0.6814 score: 0.5455 time: 0.05s
Test loss: 0.6839 score: 0.5581 time: 0.05s
Epoch 103/1000, LR 0.000264
Train loss: 1.0796;  Loss pred: 1.0796; Loss self: 0.0000; time: 0.15s
Val loss: 0.6810 score: 0.5682 time: 0.05s
Test loss: 0.6836 score: 0.5581 time: 0.05s
Epoch 104/1000, LR 0.000264
Train loss: 1.0749;  Loss pred: 1.0749; Loss self: 0.0000; time: 0.15s
Val loss: 0.6806 score: 0.6136 time: 0.05s
Test loss: 0.6832 score: 0.5581 time: 0.05s
Epoch 105/1000, LR 0.000264
Train loss: 1.0670;  Loss pred: 1.0670; Loss self: 0.0000; time: 0.17s
Val loss: 0.6802 score: 0.6136 time: 0.05s
Test loss: 0.6829 score: 0.5581 time: 0.05s
Epoch 106/1000, LR 0.000264
Train loss: 1.0653;  Loss pred: 1.0653; Loss self: 0.0000; time: 0.16s
Val loss: 0.6798 score: 0.6364 time: 0.05s
Test loss: 0.6825 score: 0.5581 time: 0.05s
Epoch 107/1000, LR 0.000264
Train loss: 1.0643;  Loss pred: 1.0643; Loss self: 0.0000; time: 0.16s
Val loss: 0.6794 score: 0.6818 time: 0.05s
Test loss: 0.6822 score: 0.5581 time: 0.05s
Epoch 108/1000, LR 0.000264
Train loss: 1.0641;  Loss pred: 1.0641; Loss self: 0.0000; time: 0.16s
Val loss: 0.6789 score: 0.6818 time: 0.05s
Test loss: 0.6818 score: 0.5814 time: 0.05s
Epoch 109/1000, LR 0.000264
Train loss: 1.0578;  Loss pred: 1.0578; Loss self: 0.0000; time: 0.16s
Val loss: 0.6785 score: 0.6818 time: 0.05s
Test loss: 0.6814 score: 0.5814 time: 0.05s
Epoch 110/1000, LR 0.000263
Train loss: 1.0604;  Loss pred: 1.0604; Loss self: 0.0000; time: 0.16s
Val loss: 0.6780 score: 0.6818 time: 0.05s
Test loss: 0.6810 score: 0.6279 time: 0.05s
Epoch 111/1000, LR 0.000263
Train loss: 1.0524;  Loss pred: 1.0524; Loss self: 0.0000; time: 0.16s
Val loss: 0.6775 score: 0.6818 time: 0.05s
Test loss: 0.6807 score: 0.6279 time: 0.05s
Epoch 112/1000, LR 0.000263
Train loss: 1.0533;  Loss pred: 1.0533; Loss self: 0.0000; time: 0.16s
Val loss: 0.6770 score: 0.6818 time: 0.05s
Test loss: 0.6803 score: 0.6279 time: 0.05s
Epoch 113/1000, LR 0.000263
Train loss: 1.0492;  Loss pred: 1.0492; Loss self: 0.0000; time: 0.16s
Val loss: 0.6765 score: 0.6818 time: 0.05s
Test loss: 0.6799 score: 0.6279 time: 0.05s
Epoch 114/1000, LR 0.000263
Train loss: 1.0504;  Loss pred: 1.0504; Loss self: 0.0000; time: 0.16s
Val loss: 0.6760 score: 0.6818 time: 0.05s
Test loss: 0.6795 score: 0.6279 time: 0.05s
Epoch 115/1000, LR 0.000263
Train loss: 1.0500;  Loss pred: 1.0500; Loss self: 0.0000; time: 0.16s
Val loss: 0.6755 score: 0.6818 time: 0.05s
Test loss: 0.6790 score: 0.6279 time: 0.05s
Epoch 116/1000, LR 0.000263
Train loss: 1.0429;  Loss pred: 1.0429; Loss self: 0.0000; time: 0.16s
Val loss: 0.6750 score: 0.6818 time: 0.05s
Test loss: 0.6786 score: 0.6279 time: 0.05s
Epoch 117/1000, LR 0.000262
Train loss: 1.0397;  Loss pred: 1.0397; Loss self: 0.0000; time: 0.16s
Val loss: 0.6744 score: 0.6818 time: 0.05s
Test loss: 0.6781 score: 0.6279 time: 0.05s
Epoch 118/1000, LR 0.000262
Train loss: 1.0410;  Loss pred: 1.0410; Loss self: 0.0000; time: 0.16s
Val loss: 0.6738 score: 0.6818 time: 0.05s
Test loss: 0.6776 score: 0.6279 time: 0.05s
Epoch 119/1000, LR 0.000262
Train loss: 1.0375;  Loss pred: 1.0375; Loss self: 0.0000; time: 3.24s
Val loss: 0.6732 score: 0.6818 time: 0.54s
Test loss: 0.6772 score: 0.6279 time: 0.69s
Epoch 120/1000, LR 0.000262
Train loss: 1.0362;  Loss pred: 1.0362; Loss self: 0.0000; time: 1.50s
Val loss: 0.6726 score: 0.6818 time: 0.05s
Test loss: 0.6767 score: 0.6279 time: 0.05s
Epoch 121/1000, LR 0.000262
Train loss: 1.0351;  Loss pred: 1.0351; Loss self: 0.0000; time: 0.16s
Val loss: 0.6720 score: 0.7045 time: 0.05s
Test loss: 0.6761 score: 0.6279 time: 0.05s
Epoch 122/1000, LR 0.000262
Train loss: 1.0293;  Loss pred: 1.0293; Loss self: 0.0000; time: 0.15s
Val loss: 0.6713 score: 0.7273 time: 0.05s
Test loss: 0.6756 score: 0.6279 time: 0.05s
Epoch 123/1000, LR 0.000262
Train loss: 1.0306;  Loss pred: 1.0306; Loss self: 0.0000; time: 0.15s
Val loss: 0.6707 score: 0.7273 time: 0.05s
Test loss: 0.6751 score: 0.6279 time: 0.05s
Epoch 124/1000, LR 0.000261
Train loss: 1.0298;  Loss pred: 1.0298; Loss self: 0.0000; time: 0.16s
Val loss: 0.6700 score: 0.7273 time: 0.05s
Test loss: 0.6745 score: 0.6279 time: 0.05s
Epoch 125/1000, LR 0.000261
Train loss: 1.0258;  Loss pred: 1.0258; Loss self: 0.0000; time: 0.15s
Val loss: 0.6693 score: 0.7500 time: 0.05s
Test loss: 0.6739 score: 0.6279 time: 0.05s
Epoch 126/1000, LR 0.000261
Train loss: 1.0179;  Loss pred: 1.0179; Loss self: 0.0000; time: 0.16s
Val loss: 0.6686 score: 0.7500 time: 0.05s
Test loss: 0.6734 score: 0.6279 time: 0.05s
Epoch 127/1000, LR 0.000261
Train loss: 1.0206;  Loss pred: 1.0206; Loss self: 0.0000; time: 0.15s
Val loss: 0.6679 score: 0.7500 time: 0.05s
Test loss: 0.6728 score: 0.6279 time: 0.05s
Epoch 128/1000, LR 0.000261
Train loss: 1.0176;  Loss pred: 1.0176; Loss self: 0.0000; time: 0.16s
Val loss: 0.6671 score: 0.7500 time: 0.05s
Test loss: 0.6721 score: 0.6279 time: 0.05s
Epoch 129/1000, LR 0.000261
Train loss: 1.0161;  Loss pred: 1.0161; Loss self: 0.0000; time: 0.15s
Val loss: 0.6663 score: 0.7727 time: 0.05s
Test loss: 0.6715 score: 0.6279 time: 0.05s
Epoch 130/1000, LR 0.000260
Train loss: 1.0141;  Loss pred: 1.0141; Loss self: 0.0000; time: 0.15s
Val loss: 0.6655 score: 0.7955 time: 0.05s
Test loss: 0.6708 score: 0.6279 time: 0.05s
Epoch 131/1000, LR 0.000260
Train loss: 1.0157;  Loss pred: 1.0157; Loss self: 0.0000; time: 0.14s
Val loss: 0.6647 score: 0.7955 time: 0.04s
Test loss: 0.6701 score: 0.6279 time: 0.05s
Epoch 132/1000, LR 0.000260
Train loss: 1.0089;  Loss pred: 1.0089; Loss self: 0.0000; time: 0.15s
Val loss: 0.6639 score: 0.7955 time: 0.04s
Test loss: 0.6694 score: 0.6279 time: 0.05s
Epoch 133/1000, LR 0.000260
Train loss: 1.0098;  Loss pred: 1.0098; Loss self: 0.0000; time: 0.15s
Val loss: 0.6631 score: 0.7955 time: 0.04s
Test loss: 0.6687 score: 0.6279 time: 0.05s
Epoch 134/1000, LR 0.000260
Train loss: 1.0025;  Loss pred: 1.0025; Loss self: 0.0000; time: 0.15s
Val loss: 0.6622 score: 0.7955 time: 0.04s
Test loss: 0.6680 score: 0.6279 time: 0.05s
Epoch 135/1000, LR 0.000260
Train loss: 1.0078;  Loss pred: 1.0078; Loss self: 0.0000; time: 0.15s
Val loss: 0.6614 score: 0.7955 time: 0.04s
Test loss: 0.6673 score: 0.6279 time: 0.05s
Epoch 136/1000, LR 0.000260
Train loss: 1.0020;  Loss pred: 1.0020; Loss self: 0.0000; time: 0.15s
Val loss: 0.6605 score: 0.7955 time: 0.05s
Test loss: 0.6666 score: 0.6279 time: 0.05s
Epoch 137/1000, LR 0.000259
Train loss: 0.9993;  Loss pred: 0.9993; Loss self: 0.0000; time: 0.15s
Val loss: 0.6596 score: 0.7955 time: 0.05s
Test loss: 0.6659 score: 0.6279 time: 0.05s
Epoch 138/1000, LR 0.000259
Train loss: 1.0015;  Loss pred: 1.0015; Loss self: 0.0000; time: 0.16s
Val loss: 0.6586 score: 0.7955 time: 0.05s
Test loss: 0.6651 score: 0.6279 time: 0.05s
Epoch 139/1000, LR 0.000259
Train loss: 0.9978;  Loss pred: 0.9978; Loss self: 0.0000; time: 0.16s
Val loss: 0.6577 score: 0.7955 time: 0.05s
Test loss: 0.6643 score: 0.6279 time: 0.05s
Epoch 140/1000, LR 0.000259
Train loss: 0.9948;  Loss pred: 0.9948; Loss self: 0.0000; time: 0.16s
Val loss: 0.6567 score: 0.7955 time: 0.05s
Test loss: 0.6635 score: 0.6279 time: 0.05s
Epoch 141/1000, LR 0.000259
Train loss: 0.9976;  Loss pred: 0.9976; Loss self: 0.0000; time: 1.22s
Val loss: 0.6557 score: 0.7955 time: 0.17s
Test loss: 0.6626 score: 0.6279 time: 0.98s
Epoch 142/1000, LR 0.000259
Train loss: 0.9893;  Loss pred: 0.9893; Loss self: 0.0000; time: 4.92s
Val loss: 0.6546 score: 0.7955 time: 0.06s
Test loss: 0.6617 score: 0.6279 time: 1.08s
Epoch 143/1000, LR 0.000258
Train loss: 0.9859;  Loss pred: 0.9859; Loss self: 0.0000; time: 2.79s
Val loss: 0.6536 score: 0.7955 time: 0.28s
Test loss: 0.6608 score: 0.6744 time: 0.29s
Epoch 144/1000, LR 0.000258
Train loss: 0.9882;  Loss pred: 0.9882; Loss self: 0.0000; time: 0.39s
Val loss: 0.6525 score: 0.7955 time: 0.05s
Test loss: 0.6599 score: 0.6744 time: 0.05s
Epoch 145/1000, LR 0.000258
Train loss: 0.9865;  Loss pred: 0.9865; Loss self: 0.0000; time: 0.17s
Val loss: 0.6514 score: 0.7955 time: 0.05s
Test loss: 0.6590 score: 0.6744 time: 0.05s
Epoch 146/1000, LR 0.000258
Train loss: 0.9851;  Loss pred: 0.9851; Loss self: 0.0000; time: 0.16s
Val loss: 0.6503 score: 0.7955 time: 0.05s
Test loss: 0.6581 score: 0.6744 time: 0.05s
Epoch 147/1000, LR 0.000258
Train loss: 0.9808;  Loss pred: 0.9808; Loss self: 0.0000; time: 0.16s
Val loss: 0.6492 score: 0.7955 time: 0.05s
Test loss: 0.6572 score: 0.6744 time: 0.05s
Epoch 148/1000, LR 0.000257
Train loss: 0.9810;  Loss pred: 0.9810; Loss self: 0.0000; time: 0.16s
Val loss: 0.6480 score: 0.7955 time: 0.05s
Test loss: 0.6562 score: 0.6977 time: 0.05s
Epoch 149/1000, LR 0.000257
Train loss: 0.9790;  Loss pred: 0.9790; Loss self: 0.0000; time: 0.16s
Val loss: 0.6469 score: 0.7955 time: 0.05s
Test loss: 0.6553 score: 0.6977 time: 0.05s
Epoch 150/1000, LR 0.000257
Train loss: 0.9774;  Loss pred: 0.9774; Loss self: 0.0000; time: 0.16s
Val loss: 0.6457 score: 0.7955 time: 0.05s
Test loss: 0.6543 score: 0.6977 time: 0.05s
Epoch 151/1000, LR 0.000257
Train loss: 0.9738;  Loss pred: 0.9738; Loss self: 0.0000; time: 0.15s
Val loss: 0.6445 score: 0.7955 time: 0.05s
Test loss: 0.6533 score: 0.6977 time: 0.05s
Epoch 152/1000, LR 0.000257
Train loss: 0.9728;  Loss pred: 0.9728; Loss self: 0.0000; time: 0.15s
Val loss: 0.6433 score: 0.7955 time: 0.05s
Test loss: 0.6523 score: 0.6977 time: 0.05s
Epoch 153/1000, LR 0.000257
Train loss: 0.9677;  Loss pred: 0.9677; Loss self: 0.0000; time: 1.67s
Val loss: 0.6421 score: 0.7955 time: 0.05s
Test loss: 0.6513 score: 0.6977 time: 1.33s
Epoch 154/1000, LR 0.000256
Train loss: 0.9669;  Loss pred: 0.9669; Loss self: 0.0000; time: 5.58s
Val loss: 0.6408 score: 0.7955 time: 0.08s
Test loss: 0.6502 score: 0.6977 time: 0.11s
Epoch 155/1000, LR 0.000256
Train loss: 0.9649;  Loss pred: 0.9649; Loss self: 0.0000; time: 0.22s
Val loss: 0.6395 score: 0.7955 time: 0.05s
Test loss: 0.6491 score: 0.6977 time: 0.05s
Epoch 156/1000, LR 0.000256
Train loss: 0.9635;  Loss pred: 0.9635; Loss self: 0.0000; time: 0.16s
Val loss: 0.6382 score: 0.7955 time: 0.05s
Test loss: 0.6480 score: 0.6977 time: 0.05s
Epoch 157/1000, LR 0.000256
Train loss: 0.9644;  Loss pred: 0.9644; Loss self: 0.0000; time: 0.16s
Val loss: 0.6368 score: 0.7955 time: 0.05s
Test loss: 0.6469 score: 0.6977 time: 0.05s
Epoch 158/1000, LR 0.000256
Train loss: 0.9594;  Loss pred: 0.9594; Loss self: 0.0000; time: 0.16s
Val loss: 0.6354 score: 0.7955 time: 0.05s
Test loss: 0.6457 score: 0.6977 time: 0.05s
Epoch 159/1000, LR 0.000255
Train loss: 0.9566;  Loss pred: 0.9566; Loss self: 0.0000; time: 0.16s
Val loss: 0.6341 score: 0.7955 time: 0.05s
Test loss: 0.6445 score: 0.6977 time: 0.05s
Epoch 160/1000, LR 0.000255
Train loss: 0.9550;  Loss pred: 0.9550; Loss self: 0.0000; time: 0.17s
Val loss: 0.6326 score: 0.7955 time: 0.05s
Test loss: 0.6433 score: 0.7209 time: 0.05s
Epoch 161/1000, LR 0.000255
Train loss: 0.9554;  Loss pred: 0.9554; Loss self: 0.0000; time: 0.17s
Val loss: 0.6312 score: 0.7955 time: 0.05s
Test loss: 0.6420 score: 0.7209 time: 0.05s
Epoch 162/1000, LR 0.000255
Train loss: 0.9510;  Loss pred: 0.9510; Loss self: 0.0000; time: 0.17s
Val loss: 0.6297 score: 0.7955 time: 0.05s
Test loss: 0.6407 score: 0.7209 time: 0.05s
Epoch 163/1000, LR 0.000255
Train loss: 0.9486;  Loss pred: 0.9486; Loss self: 0.0000; time: 0.17s
Val loss: 0.6281 score: 0.7955 time: 0.05s
Test loss: 0.6394 score: 0.7209 time: 0.05s
Epoch 164/1000, LR 0.000254
Train loss: 0.9472;  Loss pred: 0.9472; Loss self: 0.0000; time: 0.16s
Val loss: 0.6266 score: 0.7955 time: 0.05s
Test loss: 0.6381 score: 0.7209 time: 0.05s
Epoch 165/1000, LR 0.000254
Train loss: 0.9448;  Loss pred: 0.9448; Loss self: 0.0000; time: 0.16s
Val loss: 0.6251 score: 0.7955 time: 2.04s
Test loss: 0.6368 score: 0.7442 time: 2.07s
Epoch 166/1000, LR 0.000254
Train loss: 0.9450;  Loss pred: 0.9450; Loss self: 0.0000; time: 3.90s
Val loss: 0.6236 score: 0.7955 time: 0.05s
Test loss: 0.6355 score: 0.7442 time: 0.05s
Epoch 167/1000, LR 0.000254
Train loss: 0.9381;  Loss pred: 0.9381; Loss self: 0.0000; time: 0.16s
Val loss: 0.6221 score: 0.7955 time: 0.05s
Test loss: 0.6342 score: 0.7442 time: 0.05s
Epoch 168/1000, LR 0.000254
Train loss: 0.9388;  Loss pred: 0.9388; Loss self: 0.0000; time: 0.16s
Val loss: 0.6205 score: 0.7955 time: 0.05s
Test loss: 0.6329 score: 0.7442 time: 0.05s
Epoch 169/1000, LR 0.000253
Train loss: 0.9363;  Loss pred: 0.9363; Loss self: 0.0000; time: 0.16s
Val loss: 0.6190 score: 0.7955 time: 0.05s
Test loss: 0.6316 score: 0.7442 time: 0.05s
Epoch 170/1000, LR 0.000253
Train loss: 0.9350;  Loss pred: 0.9350; Loss self: 0.0000; time: 0.16s
Val loss: 0.6174 score: 0.7955 time: 0.05s
Test loss: 0.6302 score: 0.7442 time: 0.05s
Epoch 171/1000, LR 0.000253
Train loss: 0.9316;  Loss pred: 0.9316; Loss self: 0.0000; time: 0.16s
Val loss: 0.6158 score: 0.7955 time: 0.05s
Test loss: 0.6289 score: 0.7442 time: 0.15s
Epoch 172/1000, LR 0.000253
Train loss: 0.9314;  Loss pred: 0.9314; Loss self: 0.0000; time: 0.16s
Val loss: 0.6141 score: 0.7955 time: 0.05s
Test loss: 0.6275 score: 0.7442 time: 0.05s
Epoch 173/1000, LR 0.000253
Train loss: 0.9272;  Loss pred: 0.9272; Loss self: 0.0000; time: 0.16s
Val loss: 0.6125 score: 0.7955 time: 0.05s
Test loss: 0.6260 score: 0.7442 time: 0.05s
Epoch 174/1000, LR 0.000252
Train loss: 0.9251;  Loss pred: 0.9251; Loss self: 0.0000; time: 0.17s
Val loss: 0.6107 score: 0.7955 time: 0.06s
Test loss: 0.6245 score: 0.7674 time: 0.05s
Epoch 175/1000, LR 0.000252
Train loss: 0.9227;  Loss pred: 0.9227; Loss self: 0.0000; time: 0.16s
Val loss: 0.6090 score: 0.7955 time: 0.05s
Test loss: 0.6230 score: 0.7674 time: 0.05s
Epoch 176/1000, LR 0.000252
Train loss: 0.9208;  Loss pred: 0.9208; Loss self: 0.0000; time: 0.16s
Val loss: 0.6072 score: 0.8182 time: 0.05s
Test loss: 0.6215 score: 0.7674 time: 0.05s
Epoch 177/1000, LR 0.000252
Train loss: 0.9188;  Loss pred: 0.9188; Loss self: 0.0000; time: 0.16s
Val loss: 0.6054 score: 0.8182 time: 0.05s
Test loss: 0.6199 score: 0.7674 time: 0.05s
Epoch 178/1000, LR 0.000251
Train loss: 0.9166;  Loss pred: 0.9166; Loss self: 0.0000; time: 0.17s
Val loss: 0.6036 score: 0.8182 time: 0.05s
Test loss: 0.6183 score: 0.7674 time: 0.05s
Epoch 179/1000, LR 0.000251
Train loss: 0.9144;  Loss pred: 0.9144; Loss self: 0.0000; time: 0.16s
Val loss: 0.6018 score: 0.8182 time: 0.05s
Test loss: 0.6167 score: 0.7907 time: 0.05s
Epoch 180/1000, LR 0.000251
Train loss: 0.9106;  Loss pred: 0.9106; Loss self: 0.0000; time: 0.16s
Val loss: 0.5999 score: 0.8182 time: 0.05s
Test loss: 0.6151 score: 0.7907 time: 0.05s
Epoch 181/1000, LR 0.000251
Train loss: 0.9063;  Loss pred: 0.9063; Loss self: 0.0000; time: 0.16s
Val loss: 0.5981 score: 0.8182 time: 0.05s
Test loss: 0.6135 score: 0.7907 time: 0.05s
Epoch 182/1000, LR 0.000251
Train loss: 0.9050;  Loss pred: 0.9050; Loss self: 0.0000; time: 0.16s
Val loss: 0.5963 score: 0.8182 time: 0.05s
Test loss: 0.6120 score: 0.7907 time: 0.05s
Epoch 183/1000, LR 0.000250
Train loss: 0.9036;  Loss pred: 0.9036; Loss self: 0.0000; time: 0.16s
Val loss: 0.5944 score: 0.8182 time: 0.05s
Test loss: 0.6104 score: 0.7907 time: 0.05s
Epoch 184/1000, LR 0.000250
Train loss: 0.9000;  Loss pred: 0.9000; Loss self: 0.0000; time: 0.16s
Val loss: 0.5925 score: 0.8182 time: 0.75s
Test loss: 0.6088 score: 0.7907 time: 1.54s
Epoch 185/1000, LR 0.000250
Train loss: 0.8978;  Loss pred: 0.8978; Loss self: 0.0000; time: 2.04s
Val loss: 0.5906 score: 0.8182 time: 0.60s
Test loss: 0.6071 score: 0.7907 time: 0.22s
Epoch 186/1000, LR 0.000250
Train loss: 0.8966;  Loss pred: 0.8966; Loss self: 0.0000; time: 1.55s
Val loss: 0.5887 score: 0.8182 time: 0.23s
Test loss: 0.6054 score: 0.7907 time: 0.81s
Epoch 187/1000, LR 0.000249
Train loss: 0.8946;  Loss pred: 0.8946; Loss self: 0.0000; time: 0.99s
Val loss: 0.5866 score: 0.8182 time: 0.35s
Test loss: 0.6036 score: 0.7907 time: 0.24s
Epoch 188/1000, LR 0.000249
Train loss: 0.8901;  Loss pred: 0.8901; Loss self: 0.0000; time: 0.20s
Val loss: 0.5846 score: 0.8182 time: 0.05s
Test loss: 0.6019 score: 0.7907 time: 0.05s
Epoch 189/1000, LR 0.000249
Train loss: 0.8879;  Loss pred: 0.8879; Loss self: 0.0000; time: 0.16s
Val loss: 0.5826 score: 0.8182 time: 0.05s
Test loss: 0.6001 score: 0.7907 time: 0.05s
Epoch 190/1000, LR 0.000249
Train loss: 0.8868;  Loss pred: 0.8868; Loss self: 0.0000; time: 0.16s
Val loss: 0.5806 score: 0.8182 time: 0.05s
Test loss: 0.5983 score: 0.7907 time: 0.05s
Epoch 191/1000, LR 0.000249
Train loss: 0.8856;  Loss pred: 0.8856; Loss self: 0.0000; time: 0.16s
Val loss: 0.5786 score: 0.8182 time: 0.05s
Test loss: 0.5965 score: 0.7907 time: 0.05s
Epoch 192/1000, LR 0.000248
Train loss: 0.8819;  Loss pred: 0.8819; Loss self: 0.0000; time: 0.16s
Val loss: 0.5765 score: 0.8182 time: 0.05s
Test loss: 0.5948 score: 0.7907 time: 0.05s
Epoch 193/1000, LR 0.000248
Train loss: 0.8793;  Loss pred: 0.8793; Loss self: 0.0000; time: 0.16s
Val loss: 0.5746 score: 0.8182 time: 0.05s
Test loss: 0.5930 score: 0.7907 time: 0.05s
Epoch 194/1000, LR 0.000248
Train loss: 0.8771;  Loss pred: 0.8771; Loss self: 0.0000; time: 0.16s
Val loss: 0.5726 score: 0.8182 time: 0.05s
Test loss: 0.5913 score: 0.7907 time: 0.05s
Epoch 195/1000, LR 0.000248
Train loss: 0.8750;  Loss pred: 0.8750; Loss self: 0.0000; time: 0.16s
Val loss: 0.5706 score: 0.8182 time: 0.05s
Test loss: 0.5896 score: 0.7907 time: 0.05s
Epoch 196/1000, LR 0.000247
Train loss: 0.8706;  Loss pred: 0.8706; Loss self: 0.0000; time: 0.16s
Val loss: 0.5686 score: 0.8182 time: 0.05s
Test loss: 0.5878 score: 0.7907 time: 0.05s
Epoch 197/1000, LR 0.000247
Train loss: 0.8671;  Loss pred: 0.8671; Loss self: 0.0000; time: 0.16s
Val loss: 0.5666 score: 0.8182 time: 0.05s
Test loss: 0.5861 score: 0.7907 time: 0.05s
Epoch 198/1000, LR 0.000247
Train loss: 0.8648;  Loss pred: 0.8648; Loss self: 0.0000; time: 2.05s
Val loss: 0.5646 score: 0.8182 time: 0.37s
Test loss: 0.5843 score: 0.7907 time: 1.42s
Epoch 199/1000, LR 0.000247
Train loss: 0.8632;  Loss pred: 0.8632; Loss self: 0.0000; time: 3.22s
Val loss: 0.5625 score: 0.8182 time: 0.19s
Test loss: 0.5825 score: 0.7907 time: 0.20s
Epoch 200/1000, LR 0.000246
Train loss: 0.8604;  Loss pred: 0.8604; Loss self: 0.0000; time: 0.22s
Val loss: 0.5604 score: 0.8182 time: 0.05s
Test loss: 0.5807 score: 0.7907 time: 0.05s
Epoch 201/1000, LR 0.000246
Train loss: 0.8600;  Loss pred: 0.8600; Loss self: 0.0000; time: 0.17s
Val loss: 0.5583 score: 0.8182 time: 0.05s
Test loss: 0.5787 score: 0.7907 time: 0.05s
Epoch 202/1000, LR 0.000246
Train loss: 0.8548;  Loss pred: 0.8548; Loss self: 0.0000; time: 0.16s
Val loss: 0.5561 score: 0.8182 time: 0.05s
Test loss: 0.5767 score: 0.7907 time: 0.05s
Epoch 203/1000, LR 0.000246
Train loss: 0.8531;  Loss pred: 0.8531; Loss self: 0.0000; time: 0.17s
Val loss: 0.5539 score: 0.8182 time: 0.05s
Test loss: 0.5747 score: 0.7907 time: 0.05s
Epoch 204/1000, LR 0.000245
Train loss: 0.8513;  Loss pred: 0.8513; Loss self: 0.0000; time: 0.17s
Val loss: 0.5517 score: 0.8182 time: 0.05s
Test loss: 0.5726 score: 0.7907 time: 0.05s
Epoch 205/1000, LR 0.000245
Train loss: 0.8472;  Loss pred: 0.8472; Loss self: 0.0000; time: 0.17s
Val loss: 0.5495 score: 0.8182 time: 0.05s
Test loss: 0.5706 score: 0.7907 time: 0.05s
Epoch 206/1000, LR 0.000245
Train loss: 0.8446;  Loss pred: 0.8446; Loss self: 0.0000; time: 0.17s
Val loss: 0.5473 score: 0.8182 time: 0.05s
Test loss: 0.5686 score: 0.7907 time: 0.05s
Epoch 207/1000, LR 0.000245
Train loss: 0.8421;  Loss pred: 0.8421; Loss self: 0.0000; time: 0.17s
Val loss: 0.5452 score: 0.8182 time: 0.05s
Test loss: 0.5667 score: 0.7907 time: 0.05s
Epoch 208/1000, LR 0.000244
Train loss: 0.8390;  Loss pred: 0.8390; Loss self: 0.0000; time: 0.17s
Val loss: 0.5432 score: 0.8182 time: 0.05s
Test loss: 0.5648 score: 0.7907 time: 0.05s
Epoch 209/1000, LR 0.000244
Train loss: 0.8366;  Loss pred: 0.8366; Loss self: 0.0000; time: 0.17s
Val loss: 0.5412 score: 0.8182 time: 0.05s
Test loss: 0.5631 score: 0.7907 time: 0.05s
Epoch 210/1000, LR 0.000244
Train loss: 0.8342;  Loss pred: 0.8342; Loss self: 0.0000; time: 0.16s
Val loss: 0.5393 score: 0.8182 time: 0.05s
Test loss: 0.5614 score: 0.7907 time: 0.05s
Epoch 211/1000, LR 0.000244
Train loss: 0.8334;  Loss pred: 0.8334; Loss self: 0.0000; time: 0.17s
Val loss: 0.5374 score: 0.8182 time: 0.05s
Test loss: 0.5598 score: 0.7907 time: 0.05s
Epoch 212/1000, LR 0.000243
Train loss: 0.8295;  Loss pred: 0.8295; Loss self: 0.0000; time: 0.17s
Val loss: 0.5357 score: 0.8182 time: 0.05s
Test loss: 0.5583 score: 0.7907 time: 0.05s
Epoch 213/1000, LR 0.000243
Train loss: 0.8256;  Loss pred: 0.8256; Loss self: 0.0000; time: 0.17s
Val loss: 0.5339 score: 0.8182 time: 0.05s
Test loss: 0.5567 score: 0.7907 time: 0.05s
Epoch 214/1000, LR 0.000243
Train loss: 0.8244;  Loss pred: 0.8244; Loss self: 0.0000; time: 0.16s
Val loss: 0.5320 score: 0.8182 time: 0.05s
Test loss: 0.5551 score: 0.7907 time: 0.05s
Epoch 215/1000, LR 0.000243
Train loss: 0.8233;  Loss pred: 0.8233; Loss self: 0.0000; time: 0.16s
Val loss: 0.5301 score: 0.8182 time: 0.05s
Test loss: 0.5533 score: 0.7907 time: 0.05s
Epoch 216/1000, LR 0.000242
Train loss: 0.8170;  Loss pred: 0.8170; Loss self: 0.0000; time: 1.41s
Val loss: 0.5281 score: 0.8182 time: 0.22s
Test loss: 0.5514 score: 0.7907 time: 0.52s
Epoch 217/1000, LR 0.000242
Train loss: 0.8176;  Loss pred: 0.8176; Loss self: 0.0000; time: 0.80s
Val loss: 0.5260 score: 0.8182 time: 0.26s
Test loss: 0.5495 score: 0.7907 time: 0.44s
Epoch 218/1000, LR 0.000242
Train loss: 0.8148;  Loss pred: 0.8148; Loss self: 0.0000; time: 0.79s
Val loss: 0.5239 score: 0.8182 time: 0.31s
Test loss: 0.5475 score: 0.7907 time: 0.34s
Epoch 219/1000, LR 0.000242
Train loss: 0.8123;  Loss pred: 0.8123; Loss self: 0.0000; time: 0.74s
Val loss: 0.5219 score: 0.8182 time: 0.23s
Test loss: 0.5455 score: 0.7907 time: 0.41s
Epoch 220/1000, LR 0.000241
Train loss: 0.8088;  Loss pred: 0.8088; Loss self: 0.0000; time: 0.83s
Val loss: 0.5199 score: 0.8182 time: 1.13s
Test loss: 0.5436 score: 0.7907 time: 1.13s
Epoch 221/1000, LR 0.000241
Train loss: 0.8069;  Loss pred: 0.8069; Loss self: 0.0000; time: 0.70s
Val loss: 0.5179 score: 0.8182 time: 0.28s
Test loss: 0.5417 score: 0.7907 time: 0.24s
Epoch 222/1000, LR 0.000241
Train loss: 0.8056;  Loss pred: 0.8056; Loss self: 0.0000; time: 0.17s
Val loss: 0.5160 score: 0.8182 time: 0.05s
Test loss: 0.5399 score: 0.7907 time: 0.05s
Epoch 223/1000, LR 0.000241
Train loss: 0.8019;  Loss pred: 0.8019; Loss self: 0.0000; time: 0.16s
Val loss: 0.5140 score: 0.8182 time: 0.05s
Test loss: 0.5379 score: 0.8140 time: 0.05s
Epoch 224/1000, LR 0.000240
Train loss: 0.7997;  Loss pred: 0.7997; Loss self: 0.0000; time: 0.16s
Val loss: 0.5120 score: 0.8182 time: 0.05s
Test loss: 0.5360 score: 0.8140 time: 0.05s
Epoch 225/1000, LR 0.000240
Train loss: 0.7948;  Loss pred: 0.7948; Loss self: 0.0000; time: 0.16s
Val loss: 0.5101 score: 0.8182 time: 0.05s
Test loss: 0.5342 score: 0.8140 time: 0.05s
Epoch 226/1000, LR 0.000240
Train loss: 0.7930;  Loss pred: 0.7930; Loss self: 0.0000; time: 0.16s
Val loss: 0.5082 score: 0.8182 time: 0.05s
Test loss: 0.5324 score: 0.8140 time: 0.05s
Epoch 227/1000, LR 0.000240
Train loss: 0.7920;  Loss pred: 0.7920; Loss self: 0.0000; time: 0.16s
Val loss: 0.5064 score: 0.8182 time: 0.05s
Test loss: 0.5306 score: 0.8140 time: 0.05s
Epoch 228/1000, LR 0.000239
Train loss: 0.7886;  Loss pred: 0.7886; Loss self: 0.0000; time: 0.16s
Val loss: 0.5047 score: 0.8182 time: 0.05s
Test loss: 0.5290 score: 0.8140 time: 0.05s
Epoch 229/1000, LR 0.000239
Train loss: 0.7882;  Loss pred: 0.7882; Loss self: 0.0000; time: 0.15s
Val loss: 0.5031 score: 0.8182 time: 0.05s
Test loss: 0.5275 score: 0.8140 time: 0.05s
Epoch 230/1000, LR 0.000239
Train loss: 0.7839;  Loss pred: 0.7839; Loss self: 0.0000; time: 0.15s
Val loss: 0.5015 score: 0.8182 time: 0.05s
Test loss: 0.5261 score: 0.8140 time: 0.05s
Epoch 231/1000, LR 0.000238
Train loss: 0.7822;  Loss pred: 0.7822; Loss self: 0.0000; time: 0.15s
Val loss: 0.4999 score: 0.8182 time: 0.05s
Test loss: 0.5246 score: 0.8140 time: 0.05s
Epoch 232/1000, LR 0.000238
Train loss: 0.7805;  Loss pred: 0.7805; Loss self: 0.0000; time: 0.15s
Val loss: 0.4982 score: 0.8182 time: 0.05s
Test loss: 0.5231 score: 0.8140 time: 0.05s
Epoch 233/1000, LR 0.000238
Train loss: 0.7758;  Loss pred: 0.7758; Loss self: 0.0000; time: 0.16s
Val loss: 0.4966 score: 0.8182 time: 0.05s
Test loss: 0.5216 score: 0.8140 time: 0.05s
Epoch 234/1000, LR 0.000238
Train loss: 0.7750;  Loss pred: 0.7750; Loss self: 0.0000; time: 0.16s
Val loss: 0.4950 score: 0.8182 time: 0.05s
Test loss: 0.5199 score: 0.8140 time: 0.05s
Epoch 235/1000, LR 0.000237
Train loss: 0.7735;  Loss pred: 0.7735; Loss self: 0.0000; time: 0.17s
Val loss: 0.4934 score: 0.8182 time: 0.05s
Test loss: 0.5184 score: 0.8140 time: 0.05s
Epoch 236/1000, LR 0.000237
Train loss: 0.7686;  Loss pred: 0.7686; Loss self: 0.0000; time: 0.17s
Val loss: 0.4918 score: 0.8182 time: 2.02s
Test loss: 0.5168 score: 0.8140 time: 1.20s
Epoch 237/1000, LR 0.000237
Train loss: 0.7692;  Loss pred: 0.7692; Loss self: 0.0000; time: 2.21s
Val loss: 0.4901 score: 0.8182 time: 0.07s
Test loss: 0.5152 score: 0.8140 time: 0.72s
Epoch 238/1000, LR 0.000236
Train loss: 0.7646;  Loss pred: 0.7646; Loss self: 0.0000; time: 0.42s
Val loss: 0.4884 score: 0.8182 time: 0.05s
Test loss: 0.5134 score: 0.8140 time: 0.05s
Epoch 239/1000, LR 0.000236
Train loss: 0.7610;  Loss pred: 0.7610; Loss self: 0.0000; time: 0.16s
Val loss: 0.4867 score: 0.8182 time: 0.05s
Test loss: 0.5116 score: 0.8140 time: 0.05s
Epoch 240/1000, LR 0.000236
Train loss: 0.7603;  Loss pred: 0.7603; Loss self: 0.0000; time: 0.16s
Val loss: 0.4851 score: 0.8182 time: 0.05s
Test loss: 0.5099 score: 0.8140 time: 0.05s
Epoch 241/1000, LR 0.000236
Train loss: 0.7590;  Loss pred: 0.7590; Loss self: 0.0000; time: 0.15s
Val loss: 0.4835 score: 0.8182 time: 0.05s
Test loss: 0.5083 score: 0.8140 time: 0.05s
Epoch 242/1000, LR 0.000235
Train loss: 0.7567;  Loss pred: 0.7567; Loss self: 0.0000; time: 0.15s
Val loss: 0.4820 score: 0.8182 time: 0.04s
Test loss: 0.5067 score: 0.8140 time: 0.05s
Epoch 243/1000, LR 0.000235
Train loss: 0.7544;  Loss pred: 0.7544; Loss self: 0.0000; time: 0.15s
Val loss: 0.4807 score: 0.8182 time: 0.05s
Test loss: 0.5053 score: 0.8140 time: 0.05s
Epoch 244/1000, LR 0.000235
Train loss: 0.7526;  Loss pred: 0.7526; Loss self: 0.0000; time: 0.15s
Val loss: 0.4792 score: 0.8182 time: 0.04s
Test loss: 0.5038 score: 0.8140 time: 0.05s
Epoch 245/1000, LR 0.000234
Train loss: 0.7468;  Loss pred: 0.7468; Loss self: 0.0000; time: 0.15s
Val loss: 0.4779 score: 0.8182 time: 0.04s
Test loss: 0.5025 score: 0.8140 time: 0.05s
Epoch 246/1000, LR 0.000234
Train loss: 0.7467;  Loss pred: 0.7467; Loss self: 0.0000; time: 0.15s
Val loss: 0.4765 score: 0.8182 time: 0.05s
Test loss: 0.5010 score: 0.8140 time: 0.05s
Epoch 247/1000, LR 0.000234
Train loss: 0.7463;  Loss pred: 0.7463; Loss self: 0.0000; time: 0.15s
Val loss: 0.4751 score: 0.8182 time: 0.05s
Test loss: 0.4996 score: 0.8140 time: 0.05s
Epoch 248/1000, LR 0.000234
Train loss: 0.7427;  Loss pred: 0.7427; Loss self: 0.0000; time: 0.15s
Val loss: 0.4738 score: 0.8182 time: 0.05s
Test loss: 0.4982 score: 0.8140 time: 0.05s
Epoch 249/1000, LR 0.000233
Train loss: 0.7407;  Loss pred: 0.7407; Loss self: 0.0000; time: 0.15s
Val loss: 0.4726 score: 0.8182 time: 0.05s
Test loss: 0.4969 score: 0.8140 time: 0.05s
Epoch 250/1000, LR 0.000233
Train loss: 0.7383;  Loss pred: 0.7383; Loss self: 0.0000; time: 0.15s
Val loss: 0.4714 score: 0.8182 time: 0.05s
Test loss: 0.4956 score: 0.8140 time: 0.05s
Epoch 251/1000, LR 0.000233
Train loss: 0.7352;  Loss pred: 0.7352; Loss self: 0.0000; time: 0.15s
Val loss: 0.4702 score: 0.8182 time: 0.05s
Test loss: 0.4943 score: 0.8140 time: 0.05s
Epoch 252/1000, LR 0.000232
Train loss: 0.7346;  Loss pred: 0.7346; Loss self: 0.0000; time: 0.16s
Val loss: 0.4690 score: 0.8182 time: 0.05s
Test loss: 0.4931 score: 0.8140 time: 0.05s
Epoch 253/1000, LR 0.000232
Train loss: 0.7302;  Loss pred: 0.7302; Loss self: 0.0000; time: 0.16s
Val loss: 0.4678 score: 0.8182 time: 0.05s
Test loss: 0.4919 score: 0.8140 time: 0.05s
Epoch 254/1000, LR 0.000232
Train loss: 0.7321;  Loss pred: 0.7321; Loss self: 0.0000; time: 0.16s
Val loss: 0.4665 score: 0.8182 time: 0.05s
Test loss: 0.4904 score: 0.8140 time: 0.05s
Epoch 255/1000, LR 0.000232
Train loss: 0.7263;  Loss pred: 0.7263; Loss self: 0.0000; time: 0.16s
Val loss: 0.4653 score: 0.8182 time: 0.05s
Test loss: 0.4890 score: 0.8140 time: 0.05s
Epoch 256/1000, LR 0.000231
Train loss: 0.7263;  Loss pred: 0.7263; Loss self: 0.0000; time: 0.16s
Val loss: 0.4639 score: 0.8182 time: 0.05s
Test loss: 0.4874 score: 0.8140 time: 0.05s
Epoch 257/1000, LR 0.000231
Train loss: 0.7242;  Loss pred: 0.7242; Loss self: 0.0000; time: 0.24s
Val loss: 0.4626 score: 0.8182 time: 0.05s
Test loss: 0.4858 score: 0.8140 time: 0.05s
Epoch 258/1000, LR 0.000231
Train loss: 0.7233;  Loss pred: 0.7233; Loss self: 0.0000; time: 0.16s
Val loss: 0.4612 score: 0.8409 time: 0.05s
Test loss: 0.4841 score: 0.8140 time: 0.05s
Epoch 259/1000, LR 0.000230
Train loss: 0.7180;  Loss pred: 0.7180; Loss self: 0.0000; time: 0.16s
Val loss: 0.4600 score: 0.8409 time: 0.05s
Test loss: 0.4826 score: 0.8140 time: 0.05s
Epoch 260/1000, LR 0.000230
Train loss: 0.7186;  Loss pred: 0.7186; Loss self: 0.0000; time: 0.16s
Val loss: 0.4588 score: 0.8409 time: 0.05s
Test loss: 0.4812 score: 0.8140 time: 0.05s
Epoch 261/1000, LR 0.000230
Train loss: 0.7160;  Loss pred: 0.7160; Loss self: 0.0000; time: 0.16s
Val loss: 0.4577 score: 0.8409 time: 0.05s
Test loss: 0.4798 score: 0.8140 time: 0.05s
Epoch 262/1000, LR 0.000229
Train loss: 0.7144;  Loss pred: 0.7144; Loss self: 0.0000; time: 0.16s
Val loss: 0.4567 score: 0.8409 time: 0.05s
Test loss: 0.4786 score: 0.8140 time: 0.05s
Epoch 263/1000, LR 0.000229
Train loss: 0.7133;  Loss pred: 0.7133; Loss self: 0.0000; time: 0.16s
Val loss: 0.4557 score: 0.8409 time: 0.05s
Test loss: 0.4775 score: 0.8140 time: 0.15s
Epoch 264/1000, LR 0.000229
Train loss: 0.7109;  Loss pred: 0.7109; Loss self: 0.0000; time: 0.16s
Val loss: 0.4547 score: 0.8409 time: 0.05s
Test loss: 0.4764 score: 0.8140 time: 0.05s
Epoch 265/1000, LR 0.000228
Train loss: 0.7091;  Loss pred: 0.7091; Loss self: 0.0000; time: 0.16s
Val loss: 0.4539 score: 0.8409 time: 0.05s
Test loss: 0.4754 score: 0.8140 time: 0.05s
Epoch 266/1000, LR 0.000228
Train loss: 0.7076;  Loss pred: 0.7076; Loss self: 0.0000; time: 0.16s
Val loss: 0.4530 score: 0.8409 time: 0.05s
Test loss: 0.4744 score: 0.8140 time: 0.05s
Epoch 267/1000, LR 0.000228
Train loss: 0.7066;  Loss pred: 0.7066; Loss self: 0.0000; time: 0.16s
Val loss: 0.4522 score: 0.8409 time: 0.05s
Test loss: 0.4736 score: 0.8140 time: 0.05s
Epoch 268/1000, LR 0.000228
Train loss: 0.7029;  Loss pred: 0.7029; Loss self: 0.0000; time: 0.16s
Val loss: 0.4513 score: 0.8409 time: 0.05s
Test loss: 0.4725 score: 0.8140 time: 0.05s
Epoch 269/1000, LR 0.000227
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.16s
Val loss: 0.4503 score: 0.8409 time: 0.05s
Test loss: 0.4712 score: 0.8140 time: 0.05s
Epoch 270/1000, LR 0.000227
Train loss: 0.7014;  Loss pred: 0.7014; Loss self: 0.0000; time: 0.17s
Val loss: 0.4494 score: 0.8409 time: 0.13s
Test loss: 0.4700 score: 0.8140 time: 0.05s
Epoch 271/1000, LR 0.000227
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.15s
Val loss: 0.4485 score: 0.8409 time: 0.05s
Test loss: 0.4690 score: 0.8140 time: 0.05s
Epoch 272/1000, LR 0.000226
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.15s
Val loss: 0.4478 score: 0.8409 time: 0.05s
Test loss: 0.4682 score: 0.8140 time: 0.05s
Epoch 273/1000, LR 0.000226
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.15s
Val loss: 0.4471 score: 0.8409 time: 0.05s
Test loss: 0.4673 score: 0.8140 time: 0.05s
Epoch 274/1000, LR 0.000226
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.15s
Val loss: 0.4463 score: 0.8409 time: 0.96s
Test loss: 0.4663 score: 0.8140 time: 1.89s
Epoch 275/1000, LR 0.000225
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 4.79s
Val loss: 0.4454 score: 0.8409 time: 0.29s
Test loss: 0.4651 score: 0.8140 time: 0.25s
Epoch 276/1000, LR 0.000225
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.17s
Val loss: 0.4442 score: 0.8409 time: 0.05s
Test loss: 0.4635 score: 0.8140 time: 0.05s
Epoch 277/1000, LR 0.000225
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.16s
Val loss: 0.4433 score: 0.8409 time: 0.05s
Test loss: 0.4622 score: 0.8140 time: 0.05s
Epoch 278/1000, LR 0.000224
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.16s
Val loss: 0.4423 score: 0.8409 time: 0.05s
Test loss: 0.4609 score: 0.8140 time: 0.05s
Epoch 279/1000, LR 0.000224
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.16s
Val loss: 0.4411 score: 0.8409 time: 0.05s
Test loss: 0.4592 score: 0.8140 time: 0.05s
Epoch 280/1000, LR 0.000224
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.16s
Val loss: 0.4402 score: 0.8409 time: 0.05s
Test loss: 0.4579 score: 0.8140 time: 0.05s
Epoch 281/1000, LR 0.000223
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.16s
Val loss: 0.4392 score: 0.8409 time: 0.05s
Test loss: 0.4565 score: 0.8140 time: 0.05s
Epoch 282/1000, LR 0.000223
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.16s
Val loss: 0.4381 score: 0.8409 time: 0.05s
Test loss: 0.4550 score: 0.8140 time: 0.05s
Epoch 283/1000, LR 0.000223
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.16s
Val loss: 0.4373 score: 0.8409 time: 0.05s
Test loss: 0.4540 score: 0.8140 time: 0.05s
Epoch 284/1000, LR 0.000222
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.16s
Val loss: 0.4367 score: 0.8409 time: 0.05s
Test loss: 0.4530 score: 0.8140 time: 0.05s
Epoch 285/1000, LR 0.000222
Train loss: 0.6760;  Loss pred: 0.6760; Loss self: 0.0000; time: 0.16s
Val loss: 0.4362 score: 0.8409 time: 0.05s
Test loss: 0.4524 score: 0.8140 time: 0.05s
Epoch 286/1000, LR 0.000222
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.16s
Val loss: 0.4358 score: 0.8409 time: 0.05s
Test loss: 0.4518 score: 0.8140 time: 0.05s
Epoch 287/1000, LR 0.000221
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.16s
Val loss: 0.4355 score: 0.8409 time: 0.05s
Test loss: 0.4514 score: 0.8140 time: 0.05s
Epoch 288/1000, LR 0.000221
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.16s
Val loss: 0.4352 score: 0.8409 time: 0.05s
Test loss: 0.4510 score: 0.8140 time: 0.05s
Epoch 289/1000, LR 0.000221
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.16s
Val loss: 0.4349 score: 0.8409 time: 0.05s
Test loss: 0.4506 score: 0.8140 time: 0.05s
Epoch 290/1000, LR 0.000220
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.16s
Val loss: 0.4346 score: 0.8409 time: 0.05s
Test loss: 0.4502 score: 0.8140 time: 0.05s
Epoch 291/1000, LR 0.000220
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.16s
Val loss: 0.4341 score: 0.8409 time: 0.05s
Test loss: 0.4493 score: 0.8140 time: 0.05s
Epoch 292/1000, LR 0.000220
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.16s
Val loss: 0.4333 score: 0.8409 time: 0.05s
Test loss: 0.4481 score: 0.8140 time: 0.05s
Epoch 293/1000, LR 0.000219
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 4.43s
Val loss: 0.4324 score: 0.8409 time: 0.24s
Test loss: 0.4468 score: 0.8140 time: 0.79s
Epoch 294/1000, LR 0.000219
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.72s
Val loss: 0.4315 score: 0.8409 time: 0.26s
Test loss: 0.4455 score: 0.8140 time: 0.66s
Epoch 295/1000, LR 0.000219
Train loss: 0.6608;  Loss pred: 0.6608; Loss self: 0.0000; time: 0.15s
Val loss: 0.4307 score: 0.8409 time: 0.05s
Test loss: 0.4442 score: 0.8140 time: 0.05s
Epoch 296/1000, LR 0.000218
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.15s
Val loss: 0.4301 score: 0.8409 time: 0.05s
Test loss: 0.4433 score: 0.8140 time: 0.05s
Epoch 297/1000, LR 0.000218
Train loss: 0.6589;  Loss pred: 0.6589; Loss self: 0.0000; time: 0.15s
Val loss: 0.4296 score: 0.8409 time: 0.05s
Test loss: 0.4424 score: 0.8140 time: 0.05s
Epoch 298/1000, LR 0.000218
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.15s
Val loss: 0.4290 score: 0.8409 time: 0.05s
Test loss: 0.4414 score: 0.8140 time: 0.05s
Epoch 299/1000, LR 0.000217
Train loss: 0.6585;  Loss pred: 0.6585; Loss self: 0.0000; time: 0.16s
Val loss: 0.4286 score: 0.8409 time: 0.05s
Test loss: 0.4407 score: 0.8140 time: 0.05s
Epoch 300/1000, LR 0.000217
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.16s
Val loss: 0.4283 score: 0.8409 time: 0.05s
Test loss: 0.4402 score: 0.8140 time: 0.05s
Epoch 301/1000, LR 0.000217
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.16s
Val loss: 0.4279 score: 0.8409 time: 0.05s
Test loss: 0.4395 score: 0.8140 time: 0.05s
Epoch 302/1000, LR 0.000216
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 0.16s
Val loss: 0.4275 score: 0.8409 time: 0.05s
Test loss: 0.4389 score: 0.8140 time: 0.05s
Epoch 303/1000, LR 0.000216
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.16s
Val loss: 0.4272 score: 0.8409 time: 0.05s
Test loss: 0.4384 score: 0.8140 time: 0.05s
Epoch 304/1000, LR 0.000216
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.16s
Val loss: 0.4270 score: 0.8409 time: 0.05s
Test loss: 0.4379 score: 0.8140 time: 0.05s
Epoch 305/1000, LR 0.000215
Train loss: 0.6489;  Loss pred: 0.6489; Loss self: 0.0000; time: 0.16s
Val loss: 0.4265 score: 0.8409 time: 0.05s
Test loss: 0.4371 score: 0.8140 time: 0.05s
Epoch 306/1000, LR 0.000215
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 0.16s
Val loss: 0.4263 score: 0.8409 time: 0.05s
Test loss: 0.4366 score: 0.8140 time: 0.05s
Epoch 307/1000, LR 0.000215
Train loss: 0.6457;  Loss pred: 0.6457; Loss self: 0.0000; time: 0.16s
Val loss: 0.4259 score: 0.8409 time: 0.05s
Test loss: 0.4360 score: 0.8140 time: 0.05s
Epoch 308/1000, LR 0.000214
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.16s
Val loss: 0.4257 score: 0.8409 time: 0.05s
Test loss: 0.4356 score: 0.8140 time: 0.05s
Epoch 309/1000, LR 0.000214
Train loss: 0.6425;  Loss pred: 0.6425; Loss self: 0.0000; time: 0.16s
Val loss: 0.4255 score: 0.8409 time: 0.05s
Test loss: 0.4351 score: 0.8140 time: 0.05s
Epoch 310/1000, LR 0.000214
Train loss: 0.6417;  Loss pred: 0.6417; Loss self: 0.0000; time: 0.16s
Val loss: 0.4255 score: 0.8409 time: 0.05s
Test loss: 0.4350 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 311/1000, LR 0.000213
Train loss: 0.6419;  Loss pred: 0.6419; Loss self: 0.0000; time: 0.16s
Val loss: 0.4257 score: 0.8409 time: 0.05s
Test loss: 0.4351 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 308,   Train_Loss: 0.6425,   Val_Loss: 0.4255,   Val_Precision: 1.0000,   Val_Recall: 0.6818,   Val_accuracy: 0.8108,   Val_Score: 0.8409,   Val_Loss: 0.4255,   Test_Precision: 1.0000,   Test_Recall: 0.6364,   Test_accuracy: 0.7778,   Test_Score: 0.8140,   Test_loss: 0.4351


[0.05763421696610749, 0.05268593202345073, 0.05269372195471078, 1.1601972279604524, 0.059166791033931077, 0.05830445501487702, 0.05859927192796022, 0.05849420605227351, 0.05895744392182678, 0.058564196922816336, 0.0599090059986338, 0.05998086207546294, 0.061176410992629826, 0.060593164060264826, 0.05962545203510672, 0.058340215939097106, 0.05812178598716855, 0.058072637068107724, 0.058020789991132915, 0.05795856099575758, 0.05787855095695704, 0.05352840106934309, 0.05296762497164309, 0.053289561066776514, 0.053268742049112916, 0.05457125999964774, 0.054946555057540536, 0.053562697023153305, 0.052397975930944085, 0.052394132944755256, 0.052626013988628983, 0.05519146891310811, 0.05464325693901628, 0.054621079005301, 0.05552978802006692, 0.05465748498681933, 0.05457829497754574, 0.05465638905297965, 0.05519902508240193, 0.0544215269619599, 0.05448368494398892, 0.054182297084480524, 0.05560273106675595, 0.05541726399678737, 0.05542143899947405, 0.05566166201606393, 0.05540756811387837, 0.05505481094587594, 0.0559681779704988, 0.06392720201984048, 0.05482185597065836, 0.054550349013879895, 0.054153111996129155, 0.055255790008232, 0.05460315907839686, 0.05433158506639302, 0.054757784004323184, 0.054470456088893116, 0.054400785942561924, 0.05608016601763666, 0.05037003802135587, 0.05015494907274842, 0.5701379539677873, 1.2335574108874425, 0.052464334992691875, 0.05192666198126972, 0.05187992402352393, 0.0546306410105899, 0.055154573055915534, 0.05545258696656674, 0.05451761791482568, 0.055550928926095366, 0.05491606204304844, 0.588153372053057, 0.6576820210320875, 0.40218735998496413, 0.44886324100662023, 0.05170228099450469, 0.052230999106541276, 0.05107728892471641, 0.05148036894388497, 0.05108586302958429, 0.05081109004095197, 0.05206674197688699, 0.05370534700341523, 0.051107955048792064, 0.051168230012990534, 0.05223509494680911, 0.05166409502271563, 0.05165037210099399, 0.05125761800445616, 0.050627719960175455, 0.3816498469095677, 0.48453565896488726, 0.7806677459739149, 0.052978447056375444, 0.051469959085807204, 0.05058939103037119, 0.052096238010562956, 0.0511767219286412, 0.050995578058063984, 0.053793956991285086, 0.054179753991775215, 0.05381704296451062, 0.05417779099661857, 0.05457418505102396, 0.05466982000507414, 0.05523789604194462, 0.05510926805436611, 0.05496985500212759, 1.3151743459748104, 0.05198754998855293, 0.05160756304394454, 0.05155254201963544, 0.05161270999815315, 0.05062383599579334, 0.0509343120502308, 0.0505036620888859, 0.05137361399829388, 0.05296753405127674, 0.05139159900136292, 0.05063124292064458, 0.05037611105944961, 0.05022014502901584, 0.05323060404043645, 0.05450174806173891, 0.05489103903528303, 0.05418990191537887, 0.05403273901902139, 0.053859314997680485, 0.05362193600740284, 0.05373410903848708, 0.05366889096330851, 0.05362409306690097, 0.0539609850384295, 0.05361418495886028, 0.05443102598655969, 0.05412667500786483, 0.05404133489355445, 0.6915804579621181, 0.05068362201564014, 0.05015087500214577, 0.049937327043153346, 0.052883188938722014, 0.053594469907693565, 0.0528554969932884, 0.053270229953341186, 0.05251947802025825, 0.0527195279719308, 0.05297481594607234, 0.05238435894716531, 0.051489362958818674, 0.04984612308908254, 0.049814528902061284, 0.049694673041813076, 0.04974870092701167, 0.04985016491264105, 0.053719524992629886, 0.05477410601451993, 0.05551306193228811, 0.05351108894683421, 0.9897231999784708, 1.0900711450958624, 0.29747660306748, 0.055281636072322726, 0.05171135696582496, 0.05174568097572774, 0.05451555491890758, 0.05396787298377603, 0.053355176001787186, 0.052671323996037245, 0.05248758208472282, 0.05224353703670204, 1.3374955740291625, 0.11737902590539306, 0.05284926004242152, 0.05263712408486754, 0.05232911801431328, 0.05240735597908497, 0.05612248193938285, 0.05593690602108836, 0.0563539449358359, 0.05614356498699635, 0.05645320098847151, 0.05303482909221202, 2.080103399930522, 0.05473142198752612, 0.05440908798482269, 0.05455616000108421, 0.05387571488972753, 0.053360051941126585, 0.15310905606020242, 0.05406891996972263, 0.0541867510182783, 0.05731162498705089, 0.05453988804947585, 0.053914683987386525, 0.05417414498515427, 0.053582357009872794, 0.0558270598994568, 0.05396798602305353, 0.054688454954884946, 0.05391339503694326, 0.053995816968381405, 1.5418021070072427, 0.2232163370354101, 0.8116051730467007, 0.24862624204251915, 0.05606607790105045, 0.05481934698764235, 0.05401837907265872, 0.053947333013638854, 0.05403671006206423, 0.055105841951444745, 0.054619934991933405, 0.0547183669405058, 0.05354120105039328, 0.055200132075697184, 1.4256981450598687, 0.20181289699394256, 0.05634512391407043, 0.055526994983665645, 0.05527004704345018, 0.055433816043660045, 0.057609040988609195, 0.05579250201117247, 0.056296739960089326, 0.055598297039978206, 0.05577625194564462, 0.05534030601847917, 0.05543215200304985, 0.05544470401946455, 0.05546769197098911, 0.05624093010555953, 0.05570043297484517, 0.05747039197012782, 0.5231488730059937, 0.4414531980874017, 0.341775661916472, 0.4158753240481019, 1.1393005939899012, 0.24596348905470222, 0.054595483001321554, 0.05454713408835232, 0.05367841094266623, 0.053522338974289596, 0.05418379895854741, 0.053964629070833325, 0.0507647308986634, 0.05091581505257636, 0.05097946396563202, 0.05135638592764735, 0.05480120005086064, 0.05509372602682561, 0.05464161199051887, 0.05517157504800707, 1.2048839249182492, 0.7301392969675362, 0.05512386199552566, 0.053800476947799325, 0.05336923094000667, 0.04928156302776188, 0.04965828103013337, 0.049799878033809364, 0.049869056907482445, 0.04975008394103497, 0.049789655953645706, 0.04953203990589827, 0.05718074296601117, 0.049940762924961746, 0.05099865107331425, 0.05387447797693312, 0.05384689103811979, 0.05409504997078329, 0.054225399042479694, 0.054174419958144426, 0.05429816897958517, 0.055137115996330976, 0.05392370605841279, 0.05389309593010694, 0.054483768064528704, 0.05435055401176214, 0.053559878026135266, 0.1499981730012223, 0.05400856595952064, 0.05461190000642091, 0.05399235291406512, 0.054040772025473416, 0.05392203002702445, 0.053781462949700654, 0.05047470494173467, 0.05028231698088348, 0.05055725097190589, 0.05031184700783342, 1.8951348130358383, 0.2507386460201815, 0.053883332991972566, 0.05334901995956898, 0.05381393909920007, 0.0538290060358122, 0.05344948801212013, 0.05366028402931988, 0.05365041899494827, 0.053666137042455375, 0.054198793950490654, 0.05408130295109004, 0.05345139198470861, 0.053746737889014184, 0.05362317606341094, 0.05374307697638869, 0.05384611408226192, 0.05416076199617237, 0.053633983014151454, 0.7958932230249047, 0.6636332330526784, 0.05099504499230534, 0.05035563698038459, 0.04974505491554737, 0.053380792029201984, 0.05349877697881311, 0.05328975594602525, 0.053496432956308126, 0.053255781065672636, 0.053090818924829364, 0.05346703506074846, 0.0539597780443728, 0.0535577479749918, 0.05340889189392328, 0.056004868005402386, 0.05311242106836289, 0.05314341001212597, 0.053044450003653765]
[0.0013098685674115338, 0.0011974075459875166, 0.0011975845898797904, 0.02636811881728301, 0.0013446997962257062, 0.0013251012503381141, 0.0013318016347263685, 0.001329413773915307, 0.001339941907314245, 0.0013310044755185531, 0.0013615683181507682, 0.001363201410805976, 0.0013903729771052233, 0.0013771173650060189, 0.001355123909888789, 0.0013259139986158434, 0.001320949681526558, 0.0013198326606388118, 0.0013186543179802936, 0.0013172400226308541, 0.0013154216126581146, 0.001244846536496351, 0.0012318052318986766, 0.0012392921178320119, 0.001238807954630533, 0.0012690990697592497, 0.0012778268618032682, 0.0012456441168175187, 0.0012185575797893974, 0.001218468208017564, 0.0012238607904332321, 0.0012835225328629793, 0.0012707734171864252, 0.0012702576512860697, 0.0012913904190713235, 0.0012711043020190542, 0.0012692626738964126, 0.0012710788151855733, 0.0012836982577302774, 0.0012656169060920907, 0.0012670624405578818, 0.0012600534205693145, 0.0012930867689943245, 0.0012887735813206366, 0.0012888706744063733, 0.0012944572561875332, 0.00128854809567159, 0.0012803444406017661, 0.0013015855341976465, 0.0014866791167404763, 0.0012749268830385665, 0.0012686127677646486, 0.001259374697584399, 0.0012850183722844652, 0.001269840908799927, 0.0012635252341021632, 0.0012734368373098414, 0.0012667547927649563, 0.0012651345568037656, 0.001304189907386899, 0.0011713962330547876, 0.0011663941644825214, 0.01325902218529738, 0.028687381648545174, 0.0012201008137835319, 0.0012075967902620865, 0.0012065098610121844, 0.0012704800235020907, 0.0012826644896724543, 0.0012895950457341102, 0.0012678515794145506, 0.0012918820680487295, 0.001277117721931359, 0.01367798539658272, 0.015294930721676453, 0.00935319441825498, 0.010438680023409773, 0.001202378627779179, 0.0012146743978265412, 0.001187843928481777, 0.0011972178824159296, 0.001188043326269402, 0.0011816532567663247, 0.001210854464578767, 0.0012489615582189587, 0.001188557094157955, 0.0011899588375114077, 0.0012147696499257934, 0.0012014905819236193, 0.0012011714442091625, 0.0011920376280106085, 0.00117738883628315, 0.008875577835106226, 0.01126827113871831, 0.018155063859858486, 0.0012320569082878011, 0.001196975792693191, 0.0011764974658225859, 0.0012115404188503013, 0.0011901563239218884, 0.0011859436757689298, 0.0012510222556112811, 0.0012599942788784934, 0.0012515591387095492, 0.001259948627828339, 0.0012691670942098596, 0.001271391162908701, 0.0012846022335335958, 0.0012816108849852583, 0.0012783687209797113, 0.03058544990639094, 0.0012090127904314635, 0.0012001758847428963, 0.0011988963260380335, 0.0012002955813523988, 0.0011772985115300776, 0.0011845188848890883, 0.0011745037695089745, 0.0011947352092626484, 0.001231803117471552, 0.0011951534651479748, 0.0011774707655963857, 0.0011715374664988281, 0.0011679103495119963, 0.0012379210241961964, 0.0012674825130636956, 0.00127653579151821, 0.001260230277101834, 0.0012565753260237534, 0.0012525422092483833, 0.0012470217676140195, 0.0012496304427555135, 0.0012481137433327562, 0.0012470719317883947, 0.001254906628800686, 0.0012468415106711692, 0.001265837813640923, 0.0012587598839038333, 0.0012567752300826616, 0.016083266464235305, 0.0011786888840846544, 0.0011662994186545526, 0.0011613331870500778, 0.0012298416032260934, 0.0012463830211091526, 0.001229197604495079, 0.0012388425570544462, 0.0012213832097734477, 0.0012260355342309489, 0.0012319724638621475, 0.0012182409057480304, 0.0011974270455539227, 0.0011592121648623846, 0.0011584774163270065, 0.001155690070739839, 0.0011569465331863179, 0.0011593061607590941, 0.0012492912788983695, 0.0012738164189423239, 0.00129100144028577, 0.0012444439289961444, 0.023016818604150485, 0.025350491746415404, 0.006918060536453024, 0.001285619443542389, 0.0012025896968796503, 0.0012033879296680869, 0.0012678036027652926, 0.0012550668135761869, 0.0012408180465531904, 0.0012249145115357499, 0.0012206414438307631, 0.0012149659775977219, 0.03110454823323634, 0.002729744788497513, 0.0012290525591260818, 0.0012241191647643613, 0.0012169562328910066, 0.0012187757204438364, 0.001305173998590299, 0.0013008582795601944, 0.0013105568589729278, 0.001305664302023171, 0.0013128651392667792, 0.0012333681184235354, 0.04837449767280284, 0.0012728237671517702, 0.001265327627554016, 0.0012687479070019583, 0.001252923602086687, 0.001240931440491316, 0.003560675722330289, 0.0012574167434819216, 0.0012601570004250767, 0.001332828488070951, 0.0012683694895226942, 0.0012538298601717796, 0.0012598638368640528, 0.0012461013258109953, 0.0012983037185920186, 0.0012550694423965937, 0.0012718245338345336, 0.0012537998845800757, 0.0012557166736832884, 0.03585586295365681, 0.005191077605474654, 0.018874538908062805, 0.005782005628895795, 0.001303862276768615, 0.0012748685345963337, 0.0012562413737827608, 0.0012545891398520663, 0.0012566676758619588, 0.0012815312081731337, 0.0012702310463240326, 0.0012725201614071117, 0.0012451442104742625, 0.0012837240017603995, 0.033155770815345785, 0.004693323185905641, 0.0013103517189318704, 0.0012913254647364104, 0.0012853499312430274, 0.001289158512643257, 0.0013397451392699812, 0.0012975000467714529, 0.0013092265106997517, 0.0012929836520925164, 0.0012971221382708051, 0.0012869838608948643, 0.0012891198140244152, 0.0012894117213828966, 0.0012899463249067234, 0.0013079286071060355, 0.0012953589063917483, 0.0013365207434913448, 0.012166252860604503, 0.010266353443893062, 0.007948271207359815, 0.009671519163909346, 0.026495362650927935, 0.0057200811408070285, 0.001269662395379571, 0.0012685380020547052, 0.0012483351382015402, 0.0012447055575416186, 0.0012600883478731957, 0.00125499137374031, 0.0011805751371782186, 0.0011840887221529386, 0.0011855689294333028, 0.001194334556456915, 0.0012744465128107126, 0.0012812494424843165, 0.0012707351625702063, 0.0012830598848373737, 0.02802055639344766, 0.01697998365040782, 0.001281950278965713, 0.0012511738825069611, 0.0012411449055815505, 0.0011460828611107414, 0.0011548437448868224, 0.0011581366984606829, 0.001159745509476336, 0.001156978696303139, 0.0011578989756661792, 0.001151907904788332, 0.0013297847201397947, 0.0011614130912781801, 0.0011860151412398663, 0.0012528948366728632, 0.0012522532799562744, 0.0012580244179251929, 0.0012610557916855744, 0.0012598702315847542, 0.0012627481158043062, 0.0012822585115425807, 0.0012540396757770417, 0.0012533278123280684, 0.0012670643735936907, 0.0012639663723665615, 0.0012455785587473318, 0.0034883296046795886, 0.0012560131618493172, 0.0012700441861958351, 0.0012556361142805842, 0.0012567621401272887, 0.0012540006983028941, 0.0012507316965046664, 0.0011738303474822017, 0.0011693562088577554, 0.0011757500226024626, 0.0011700429536705447, 0.044072902628740425, 0.005831131302794918, 0.0012531007672551759, 0.0012406748827806739, 0.0012514869557953504, 0.0012518373496700512, 0.0012430113491190727, 0.0012479135820772065, 0.0012476841626732156, 0.001248049698661753, 0.0012604370686160617, 0.0012577047197927917, 0.001243055627551363, 0.0012499241369538182, 0.0012470506061258357, 0.0012498389994509, 0.0012522352112153935, 0.0012595526045621481, 0.0012473019305616617, 0.018509144721509413, 0.015433331001225079, 0.0011859312788908218, 0.001171061325125223, 0.0011568617422220318, 0.0012414137681209764, 0.0012441576041584445, 0.001239296649907564, 0.0012441030920071657, 0.0012385065364109915, 0.0012346702075541713, 0.001243419420017406, 0.0012548785591714603, 0.001245529022674228, 0.001242067253347053, 0.0013024387908233113, 0.0012351725829851834, 0.0012358932560959527, 0.0012335918605500876]
[763.4353742651651, 835.1375464026224, 835.0140845586336, 37.924586388944405, 743.6604086702421, 754.6593135768599, 750.8625713659374, 752.211252524383, 746.3010109179895, 751.3122745965259, 734.4471714487034, 733.5673159322528, 719.2314698765323, 726.1545206029912, 737.9398981175581, 754.1967284785638, 757.0311072291173, 757.6718093307301, 758.3488609294072, 759.1630855572948, 760.212535948279, 803.3118707262687, 811.8166525877007, 806.9122570951038, 807.2276225399635, 787.9605492025944, 782.5786340011672, 802.7975137512695, 820.642386199616, 820.7025783848643, 817.086393989313, 779.1059170339892, 786.9223470334034, 787.2418630878169, 774.3591598884008, 786.717501004107, 787.8589834601977, 786.733275744198, 778.999265581393, 790.1285098092997, 789.2270877824336, 793.6171464446184, 773.3433084136592, 775.9314859443941, 775.8730335458823, 772.524542791953, 776.0672677714844, 781.0398266969447, 768.2937261717826, 672.6401068930641, 784.3587058237206, 788.2626010157882, 794.0448556875849, 778.1989904332897, 787.500223902109, 791.4365087536861, 785.2764822733715, 789.4187617931105, 790.4297567575727, 766.7594990085609, 853.6821032727606, 857.343109602796, 75.4203429200742, 34.85853160986245, 819.6044037533263, 828.0909721389443, 828.8369886683431, 787.1040720841011, 779.6271028407152, 775.4372221791094, 788.7358553922888, 774.0644635701223, 783.0131732004473, 73.1101818729707, 65.38113955513174, 106.91534413614482, 95.79755273247203, 831.6847762397633, 823.2658906694127, 841.861439893146, 835.2698491122159, 841.7201442813702, 846.2719450683601, 825.8630820243791, 800.6651553199265, 841.3563007744781, 840.3652029605715, 823.2013370280423, 832.299491186167, 832.5206237802201, 838.8996928468608, 849.3370831991735, 112.66872068257082, 88.74475841852555, 55.08105108961015, 811.6508200824162, 835.4387833942773, 849.9805813868184, 825.3954919217273, 840.2257584992939, 843.2103652406852, 799.3462910149226, 793.6543972962231, 799.0033943031046, 793.6831533549197, 787.9183163211194, 786.5399958516231, 778.4510830634853, 780.2680296457551, 782.2469242157488, 32.695284949561795, 827.1211089860575, 833.2112090505982, 834.1004791504184, 833.1281190532073, 849.4022460797547, 844.2246153750722, 851.4234061743971, 837.0055492188662, 811.8180460954179, 836.7126307718044, 849.2779856776341, 853.5791885414706, 856.2301039783092, 807.8059750615491, 788.9655199919483, 783.3701229878401, 793.5057728494759, 795.8138117866376, 798.3762883328881, 801.9106209455695, 800.2365865822998, 801.2090286977898, 801.8783636369116, 796.8720357750439, 802.0265538494178, 789.9906206180592, 794.4326894964806, 795.6872287610468, 62.17642431179766, 848.4002975701086, 857.4127569690497, 861.0793277509937, 813.1128410169424, 802.3215841869403, 813.5388454574583, 807.205075661645, 818.7438569631953, 815.6370448326904, 811.706453945464, 820.8557070130353, 835.1239465594382, 862.6548532802143, 863.2019803808823, 865.2838899618038, 864.3441778125431, 862.5849097060062, 800.453838821163, 785.0424795358821, 774.5924743342229, 803.5717614104719, 43.44649089860212, 39.44696655209465, 144.5491832184389, 777.8351556698655, 831.5388054584967, 830.9872280967747, 788.7657029991333, 796.77033061738, 805.9199354634248, 816.3835031607546, 819.2413956236651, 823.0683150298901, 32.14963909784305, 366.33461274979226, 813.6348544044774, 816.9139318985309, 821.7222386251276, 820.4955048134977, 766.181368216105, 768.7232465769359, 763.0344255217522, 765.8936515691408, 761.6928579263587, 810.7879432445347, 20.67204928439435, 785.6547196928332, 790.3091485744949, 788.1786401232317, 798.1332607467414, 805.8462920433984, 280.84556920716966, 795.2812821872349, 793.5519142953453, 750.2840830235665, 788.4137928738056, 797.5563764791788, 793.7365695717689, 802.5029580553363, 770.2357974330365, 796.7686617327479, 786.2719843790194, 797.5754442942234, 796.3579850116856, 27.889441715361468, 192.6382296703429, 52.98142671834071, 172.95036777592566, 766.9521680451694, 784.3946045124046, 796.0253665176035, 797.0736938771158, 795.7553291200012, 780.3165413548797, 787.2583518517642, 785.8421660637835, 803.1198246660203, 778.9836433911631, 30.16066209316301, 213.06864249260863, 763.1538811695132, 774.3981105523411, 777.9982522214218, 775.6997996698065, 746.4106199667899, 770.7128816590665, 763.8097699881763, 773.4049834130828, 770.9374240833644, 777.0105207882568, 775.7230857216974, 775.547471313118, 775.2260545199897, 764.5677253077534, 771.9868177581168, 748.2113576387421, 82.19457637923145, 97.40556912102512, 125.81352270340697, 103.39637269516486, 37.74245377105557, 174.8226948855682, 787.610945743609, 788.3090600204781, 801.0669325872591, 803.4028561542464, 793.5951488543019, 796.8182259449743, 847.0447737788, 844.5313102735881, 843.4768955002861, 837.2863320362901, 784.654349906425, 780.4881444951269, 786.9460367944697, 779.3868484375137, 35.688085060075416, 58.89287178294676, 780.0614551188424, 799.2494200696651, 805.7076941643976, 872.537260552729, 865.9180122225127, 863.4559299684852, 862.2581349347355, 864.3201497099917, 863.6332020456841, 868.1249567288579, 752.0014216247531, 861.020086229148, 843.1595560867755, 798.151585216489, 798.5604957129101, 794.897130573394, 792.9863266900845, 793.7325408047217, 791.9235732638974, 779.8739419533908, 797.4229359054124, 797.8758551144656, 789.2258837360932, 791.1602886457102, 802.8397670924039, 286.67015830685887, 796.1700007407798, 787.3741802600595, 796.4090779381169, 795.695516335905, 797.4477218021914, 799.5319881910973, 851.9118645594249, 855.1714117777807, 850.5209277279461, 854.669477614388, 22.689678699489352, 171.49330860045808, 798.0204195313244, 806.0129320573823, 799.0494789971467, 798.8258221114521, 804.4978838758826, 801.3375400045381, 801.4848868943388, 801.2501433815262, 793.3755876427713, 795.0991868462983, 804.4692271494343, 800.0485552963985, 801.8920764624473, 800.1030536247763, 798.5720182947266, 793.9326998951544, 801.7304996471053, 54.02734783514354, 64.79482620573752, 843.2191795592746, 853.9262449753165, 864.4075290096973, 805.5331958446183, 803.7566918030501, 806.9093062380081, 803.791909548795, 807.4240794060336, 809.9328823856187, 804.2338601933702, 796.8898605298148, 802.8716969219538, 805.1093830106672, 767.7903998604568, 809.6034625243908, 809.1313671852917, 810.6408869738136]
Elapsed: 0.1320895048185162~0.27482381706274184
Time per graph: 0.0030681476086518466~0.006384928569374207
Speed: 721.1205554603819~227.09332457160357
Total Time: 0.0541
best val loss: 0.4254922866821289 test_score: 0.8140

Testing...
Test loss: 0.4841 score: 0.8140 time: 0.05s
test Score 0.8140
Epoch Time List: [0.24144823208916932, 0.23756754712667316, 0.2251020959811285, 3.434465694008395, 1.6451996190007776, 0.25691361108329147, 0.2523142059799284, 0.25170884712133557, 0.251293208100833, 0.2512686770642176, 0.2530690210405737, 0.26223690703045577, 0.261356764822267, 0.26592284301295877, 0.2648622450651601, 0.2626241809921339, 0.2538929469883442, 0.24912723200395703, 0.2498031910508871, 0.25082546996418387, 0.2470288008917123, 0.25776567799039185, 0.2526191349606961, 0.25245179084595293, 0.25272855698131025, 0.26590626290999353, 0.26796004083007574, 0.2600723971845582, 0.25251302192918956, 0.2506266220007092, 0.24844657094217837, 0.2528777208644897, 0.2635207660496235, 0.26019829290453345, 0.2631485660094768, 0.26152968604583293, 0.2597069739131257, 0.2599470920395106, 0.26122286485042423, 0.2599713410018012, 0.2653896501287818, 0.2641383018344641, 0.2662282780511305, 0.2638321410631761, 0.2627378748729825, 0.26397049706429243, 0.26450446294620633, 0.26228272588923573, 0.2659017861587927, 0.2715796501142904, 0.265584489912726, 0.25845784216653556, 0.26021045504603535, 0.26669755997136235, 0.2603706809459254, 0.2644298579543829, 0.26080000295769423, 0.2604537371080369, 0.25846574001479894, 0.2596326519269496, 0.24854881584178656, 0.23899513599462807, 1.9581867039669305, 3.1391427750932053, 3.4883547039935365, 0.24904928705655038, 0.2474038590444252, 0.25255732994992286, 0.26067903405055404, 0.26632207294460386, 0.2616200930206105, 0.2649546639295295, 0.2609765339875594, 0.7939839760074392, 2.4275098480284214, 5.755670748883858, 2.701906479895115, 0.3010525449644774, 0.2512785710860044, 0.24872503394726664, 0.24662521993741393, 0.24279799091164023, 0.24181347305420786, 0.24964770104270428, 0.2501157639781013, 0.24735936906654388, 0.24596022500190884, 0.2490916799288243, 0.2449302668683231, 0.2444682710338384, 0.24664300493896008, 0.24764859513379633, 1.3025275859981775, 2.1610240009613335, 2.1570494670886546, 1.7740393390413374, 0.2503080510068685, 0.24548037885688245, 0.24824339000042528, 0.24840199889149517, 0.24544558010529727, 0.3180918210418895, 0.2611153709003702, 0.2574448661180213, 0.25997040001675487, 0.2622772880131379, 0.26480276708025485, 0.2842469110619277, 0.26568808197043836, 0.2645054808817804, 3.347185779013671, 2.502586298971437, 0.25404888298362494, 0.25447623105719686, 0.24737765302415937, 0.24382041883654892, 0.24417945183813572, 0.24755468999501318, 0.24311322788707912, 0.24442813696805388, 0.24355079501401633, 0.2417708410648629, 0.24075558001641184, 0.240832116920501, 0.24702632694970816, 0.26539046689867973, 0.2623932759743184, 0.25936991802882403, 0.261416569002904, 0.256028707139194, 0.2566215409897268, 0.2558378350222483, 0.25638968497514725, 0.2554748399415985, 0.25748954294249415, 0.2556817269651219, 0.25942857994232327, 0.257382347015664, 0.25635741103906184, 4.465570452040993, 1.5911003779619932, 0.24718992598354816, 0.2382779368199408, 0.24209945497568697, 0.25323997414670885, 0.25154286390170455, 0.2534849400399253, 0.25043864315375686, 0.25157003209460527, 0.2504100180231035, 0.25005865388084203, 0.23619436903391033, 0.23979893291834742, 0.23611750302370638, 0.23745848308317363, 0.23546343808993697, 0.23705232003703713, 0.24430227605625987, 0.25794378796126693, 0.25866281404159963, 0.25708598096389323, 2.3806474449811503, 6.067264645011164, 3.358824785100296, 0.4957698849029839, 0.26219195791054517, 0.25090303795877844, 0.2538472500164062, 0.2584673019591719, 0.25363046990241855, 0.25285993493162096, 0.24942437722347677, 0.2489267559722066, 3.0539317200891674, 5.775578713044524, 0.31391030305530876, 0.2599875519517809, 0.2515543730696663, 0.2525356390979141, 0.26002352603245527, 0.27101008500903845, 0.2720266639953479, 0.2712421319447458, 0.27060803095810115, 0.2550904940580949, 4.2771947619039565, 4.006524771917611, 0.2608454949222505, 0.26391874090768397, 0.2595991159323603, 0.25781219894997776, 0.35653668199665844, 0.2579843749990687, 0.2587218901608139, 0.2800623500952497, 0.2626038360176608, 0.26074382197111845, 0.2606038759695366, 0.26542503596283495, 0.2599996138596907, 0.26394898793660104, 0.26355971908196807, 0.2639581629773602, 0.26003415999002755, 2.452528733992949, 2.859118298976682, 2.5834390679374337, 1.574736577924341, 0.3049175860360265, 0.264102196902968, 0.2631729490822181, 0.2634993268875405, 0.26312594895716757, 0.2640883921412751, 0.26546069304458797, 0.26387318887282163, 0.26370415510609746, 0.26473420299589634, 3.8437440230045468, 3.6070859290193766, 0.32400754804257303, 0.2691879639169201, 0.26599602785427123, 0.2669118168996647, 0.26957473205402493, 0.2701822029193863, 0.26808342104777694, 0.26912118191830814, 0.26740589598193765, 0.26711467292625457, 0.2668000749545172, 0.2705757759977132, 0.26702574209775776, 0.26814069494139403, 0.26639214798342437, 0.2691509020514786, 2.149030990083702, 1.4975806178990752, 1.435706065967679, 1.3780350411543623, 3.0908358050510287, 1.2245018670801073, 0.2679587749298662, 0.2604079790180549, 0.2588339280337095, 0.2593271580990404, 0.2610766489524394, 0.25997482088860124, 0.25360023009125143, 0.24446779012214392, 0.24345930013805628, 0.24469235097058117, 0.25166427704971284, 0.2647271470632404, 0.26533044199459255, 0.26576099009253085, 3.391458256985061, 3.0077646559802815, 0.5218576799379662, 0.25897559605073184, 0.25610956514719874, 0.24255320394877344, 0.23647571506444365, 0.2401096059475094, 0.23889471986331046, 0.24073167005553842, 0.24119389394763857, 0.23966768092941493, 0.25374725996516645, 0.24202676489949226, 0.24098622298333794, 0.2538932180032134, 0.25771768402773887, 0.2594714199658483, 0.25907591090071946, 0.26082258799578995, 0.25956381601281464, 0.33850096410606056, 0.2598399978596717, 0.2592586138052866, 0.26082146691624075, 0.26087734499014914, 0.25646436016540974, 0.3526121359318495, 0.25709177704993635, 0.25887411611620337, 0.262018981971778, 0.25991674314718693, 0.25845829804893583, 0.2598094381392002, 0.3414038419723511, 0.2411382399732247, 0.2427003460470587, 0.24152252590283751, 2.999134410987608, 5.326432249043137, 0.2728878650814295, 0.26142277906183153, 0.25740472704637796, 0.25847212492953986, 0.2584949410520494, 0.2579081740695983, 0.25750014593359083, 0.25872597796842456, 0.26238492294214666, 0.2587981621036306, 0.2591223461786285, 0.2594444890273735, 0.25948577909730375, 0.25844080303795636, 0.2592461189487949, 0.25876610877458006, 0.2552806939929724, 5.460049442015588, 1.6423721270402893, 0.24805308494251221, 0.23911558603867888, 0.2384888199158013, 0.24611739092506468, 0.255798430996947, 0.2548640980385244, 0.25610364985186607, 0.2569985290756449, 0.2568116660695523, 0.25507447798736393, 0.2568639989476651, 0.2562175679486245, 0.25477869098540395, 0.2587261430453509, 0.2540747349848971, 0.25231188302859664, 0.25265625899191946]
Total Epoch List: [21, 311]
Total Time List: [0.05826871597673744, 0.054139356943778694]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7f351dbaa4d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.4582;  Loss pred: 2.4582; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.4703;  Loss pred: 2.4703; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.4393;  Loss pred: 2.4393; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.4271;  Loss pred: 2.4271; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.4111;  Loss pred: 2.4111; Loss self: 0.0000; time: 0.14s
Val loss: 0.6932 score: 0.4773 time: 0.05s
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.4423;  Loss pred: 2.4423; Loss self: 0.0000; time: 4.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.62s
Epoch 7/1000, LR 0.000150
Train loss: 2.3672;  Loss pred: 2.3672; Loss self: 0.0000; time: 4.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.66s
Epoch 8/1000, LR 0.000180
Train loss: 2.3368;  Loss pred: 2.3368; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 9/1000, LR 0.000210
Train loss: 2.3244;  Loss pred: 2.3244; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 006,   Train_Loss: 2.3672,   Val_Loss: 0.6932,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6932,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6932


[0.05763421696610749, 0.05268593202345073, 0.05269372195471078, 1.1601972279604524, 0.059166791033931077, 0.05830445501487702, 0.05859927192796022, 0.05849420605227351, 0.05895744392182678, 0.058564196922816336, 0.0599090059986338, 0.05998086207546294, 0.061176410992629826, 0.060593164060264826, 0.05962545203510672, 0.058340215939097106, 0.05812178598716855, 0.058072637068107724, 0.058020789991132915, 0.05795856099575758, 0.05787855095695704, 0.05352840106934309, 0.05296762497164309, 0.053289561066776514, 0.053268742049112916, 0.05457125999964774, 0.054946555057540536, 0.053562697023153305, 0.052397975930944085, 0.052394132944755256, 0.052626013988628983, 0.05519146891310811, 0.05464325693901628, 0.054621079005301, 0.05552978802006692, 0.05465748498681933, 0.05457829497754574, 0.05465638905297965, 0.05519902508240193, 0.0544215269619599, 0.05448368494398892, 0.054182297084480524, 0.05560273106675595, 0.05541726399678737, 0.05542143899947405, 0.05566166201606393, 0.05540756811387837, 0.05505481094587594, 0.0559681779704988, 0.06392720201984048, 0.05482185597065836, 0.054550349013879895, 0.054153111996129155, 0.055255790008232, 0.05460315907839686, 0.05433158506639302, 0.054757784004323184, 0.054470456088893116, 0.054400785942561924, 0.05608016601763666, 0.05037003802135587, 0.05015494907274842, 0.5701379539677873, 1.2335574108874425, 0.052464334992691875, 0.05192666198126972, 0.05187992402352393, 0.0546306410105899, 0.055154573055915534, 0.05545258696656674, 0.05451761791482568, 0.055550928926095366, 0.05491606204304844, 0.588153372053057, 0.6576820210320875, 0.40218735998496413, 0.44886324100662023, 0.05170228099450469, 0.052230999106541276, 0.05107728892471641, 0.05148036894388497, 0.05108586302958429, 0.05081109004095197, 0.05206674197688699, 0.05370534700341523, 0.051107955048792064, 0.051168230012990534, 0.05223509494680911, 0.05166409502271563, 0.05165037210099399, 0.05125761800445616, 0.050627719960175455, 0.3816498469095677, 0.48453565896488726, 0.7806677459739149, 0.052978447056375444, 0.051469959085807204, 0.05058939103037119, 0.052096238010562956, 0.0511767219286412, 0.050995578058063984, 0.053793956991285086, 0.054179753991775215, 0.05381704296451062, 0.05417779099661857, 0.05457418505102396, 0.05466982000507414, 0.05523789604194462, 0.05510926805436611, 0.05496985500212759, 1.3151743459748104, 0.05198754998855293, 0.05160756304394454, 0.05155254201963544, 0.05161270999815315, 0.05062383599579334, 0.0509343120502308, 0.0505036620888859, 0.05137361399829388, 0.05296753405127674, 0.05139159900136292, 0.05063124292064458, 0.05037611105944961, 0.05022014502901584, 0.05323060404043645, 0.05450174806173891, 0.05489103903528303, 0.05418990191537887, 0.05403273901902139, 0.053859314997680485, 0.05362193600740284, 0.05373410903848708, 0.05366889096330851, 0.05362409306690097, 0.0539609850384295, 0.05361418495886028, 0.05443102598655969, 0.05412667500786483, 0.05404133489355445, 0.6915804579621181, 0.05068362201564014, 0.05015087500214577, 0.049937327043153346, 0.052883188938722014, 0.053594469907693565, 0.0528554969932884, 0.053270229953341186, 0.05251947802025825, 0.0527195279719308, 0.05297481594607234, 0.05238435894716531, 0.051489362958818674, 0.04984612308908254, 0.049814528902061284, 0.049694673041813076, 0.04974870092701167, 0.04985016491264105, 0.053719524992629886, 0.05477410601451993, 0.05551306193228811, 0.05351108894683421, 0.9897231999784708, 1.0900711450958624, 0.29747660306748, 0.055281636072322726, 0.05171135696582496, 0.05174568097572774, 0.05451555491890758, 0.05396787298377603, 0.053355176001787186, 0.052671323996037245, 0.05248758208472282, 0.05224353703670204, 1.3374955740291625, 0.11737902590539306, 0.05284926004242152, 0.05263712408486754, 0.05232911801431328, 0.05240735597908497, 0.05612248193938285, 0.05593690602108836, 0.0563539449358359, 0.05614356498699635, 0.05645320098847151, 0.05303482909221202, 2.080103399930522, 0.05473142198752612, 0.05440908798482269, 0.05455616000108421, 0.05387571488972753, 0.053360051941126585, 0.15310905606020242, 0.05406891996972263, 0.0541867510182783, 0.05731162498705089, 0.05453988804947585, 0.053914683987386525, 0.05417414498515427, 0.053582357009872794, 0.0558270598994568, 0.05396798602305353, 0.054688454954884946, 0.05391339503694326, 0.053995816968381405, 1.5418021070072427, 0.2232163370354101, 0.8116051730467007, 0.24862624204251915, 0.05606607790105045, 0.05481934698764235, 0.05401837907265872, 0.053947333013638854, 0.05403671006206423, 0.055105841951444745, 0.054619934991933405, 0.0547183669405058, 0.05354120105039328, 0.055200132075697184, 1.4256981450598687, 0.20181289699394256, 0.05634512391407043, 0.055526994983665645, 0.05527004704345018, 0.055433816043660045, 0.057609040988609195, 0.05579250201117247, 0.056296739960089326, 0.055598297039978206, 0.05577625194564462, 0.05534030601847917, 0.05543215200304985, 0.05544470401946455, 0.05546769197098911, 0.05624093010555953, 0.05570043297484517, 0.05747039197012782, 0.5231488730059937, 0.4414531980874017, 0.341775661916472, 0.4158753240481019, 1.1393005939899012, 0.24596348905470222, 0.054595483001321554, 0.05454713408835232, 0.05367841094266623, 0.053522338974289596, 0.05418379895854741, 0.053964629070833325, 0.0507647308986634, 0.05091581505257636, 0.05097946396563202, 0.05135638592764735, 0.05480120005086064, 0.05509372602682561, 0.05464161199051887, 0.05517157504800707, 1.2048839249182492, 0.7301392969675362, 0.05512386199552566, 0.053800476947799325, 0.05336923094000667, 0.04928156302776188, 0.04965828103013337, 0.049799878033809364, 0.049869056907482445, 0.04975008394103497, 0.049789655953645706, 0.04953203990589827, 0.05718074296601117, 0.049940762924961746, 0.05099865107331425, 0.05387447797693312, 0.05384689103811979, 0.05409504997078329, 0.054225399042479694, 0.054174419958144426, 0.05429816897958517, 0.055137115996330976, 0.05392370605841279, 0.05389309593010694, 0.054483768064528704, 0.05435055401176214, 0.053559878026135266, 0.1499981730012223, 0.05400856595952064, 0.05461190000642091, 0.05399235291406512, 0.054040772025473416, 0.05392203002702445, 0.053781462949700654, 0.05047470494173467, 0.05028231698088348, 0.05055725097190589, 0.05031184700783342, 1.8951348130358383, 0.2507386460201815, 0.053883332991972566, 0.05334901995956898, 0.05381393909920007, 0.0538290060358122, 0.05344948801212013, 0.05366028402931988, 0.05365041899494827, 0.053666137042455375, 0.054198793950490654, 0.05408130295109004, 0.05345139198470861, 0.053746737889014184, 0.05362317606341094, 0.05374307697638869, 0.05384611408226192, 0.05416076199617237, 0.053633983014151454, 0.7958932230249047, 0.6636332330526784, 0.05099504499230534, 0.05035563698038459, 0.04974505491554737, 0.053380792029201984, 0.05349877697881311, 0.05328975594602525, 0.053496432956308126, 0.053255781065672636, 0.053090818924829364, 0.05346703506074846, 0.0539597780443728, 0.0535577479749918, 0.05340889189392328, 0.056004868005402386, 0.05311242106836289, 0.05314341001212597, 0.053044450003653765, 0.047089489991776645, 0.05058561090845615, 0.048233295092359185, 0.04755429795477539, 0.04736225598026067, 0.624283917946741, 0.664006479899399, 0.055163727956824005, 0.052321690949611366]
[0.0013098685674115338, 0.0011974075459875166, 0.0011975845898797904, 0.02636811881728301, 0.0013446997962257062, 0.0013251012503381141, 0.0013318016347263685, 0.001329413773915307, 0.001339941907314245, 0.0013310044755185531, 0.0013615683181507682, 0.001363201410805976, 0.0013903729771052233, 0.0013771173650060189, 0.001355123909888789, 0.0013259139986158434, 0.001320949681526558, 0.0013198326606388118, 0.0013186543179802936, 0.0013172400226308541, 0.0013154216126581146, 0.001244846536496351, 0.0012318052318986766, 0.0012392921178320119, 0.001238807954630533, 0.0012690990697592497, 0.0012778268618032682, 0.0012456441168175187, 0.0012185575797893974, 0.001218468208017564, 0.0012238607904332321, 0.0012835225328629793, 0.0012707734171864252, 0.0012702576512860697, 0.0012913904190713235, 0.0012711043020190542, 0.0012692626738964126, 0.0012710788151855733, 0.0012836982577302774, 0.0012656169060920907, 0.0012670624405578818, 0.0012600534205693145, 0.0012930867689943245, 0.0012887735813206366, 0.0012888706744063733, 0.0012944572561875332, 0.00128854809567159, 0.0012803444406017661, 0.0013015855341976465, 0.0014866791167404763, 0.0012749268830385665, 0.0012686127677646486, 0.001259374697584399, 0.0012850183722844652, 0.001269840908799927, 0.0012635252341021632, 0.0012734368373098414, 0.0012667547927649563, 0.0012651345568037656, 0.001304189907386899, 0.0011713962330547876, 0.0011663941644825214, 0.01325902218529738, 0.028687381648545174, 0.0012201008137835319, 0.0012075967902620865, 0.0012065098610121844, 0.0012704800235020907, 0.0012826644896724543, 0.0012895950457341102, 0.0012678515794145506, 0.0012918820680487295, 0.001277117721931359, 0.01367798539658272, 0.015294930721676453, 0.00935319441825498, 0.010438680023409773, 0.001202378627779179, 0.0012146743978265412, 0.001187843928481777, 0.0011972178824159296, 0.001188043326269402, 0.0011816532567663247, 0.001210854464578767, 0.0012489615582189587, 0.001188557094157955, 0.0011899588375114077, 0.0012147696499257934, 0.0012014905819236193, 0.0012011714442091625, 0.0011920376280106085, 0.00117738883628315, 0.008875577835106226, 0.01126827113871831, 0.018155063859858486, 0.0012320569082878011, 0.001196975792693191, 0.0011764974658225859, 0.0012115404188503013, 0.0011901563239218884, 0.0011859436757689298, 0.0012510222556112811, 0.0012599942788784934, 0.0012515591387095492, 0.001259948627828339, 0.0012691670942098596, 0.001271391162908701, 0.0012846022335335958, 0.0012816108849852583, 0.0012783687209797113, 0.03058544990639094, 0.0012090127904314635, 0.0012001758847428963, 0.0011988963260380335, 0.0012002955813523988, 0.0011772985115300776, 0.0011845188848890883, 0.0011745037695089745, 0.0011947352092626484, 0.001231803117471552, 0.0011951534651479748, 0.0011774707655963857, 0.0011715374664988281, 0.0011679103495119963, 0.0012379210241961964, 0.0012674825130636956, 0.00127653579151821, 0.001260230277101834, 0.0012565753260237534, 0.0012525422092483833, 0.0012470217676140195, 0.0012496304427555135, 0.0012481137433327562, 0.0012470719317883947, 0.001254906628800686, 0.0012468415106711692, 0.001265837813640923, 0.0012587598839038333, 0.0012567752300826616, 0.016083266464235305, 0.0011786888840846544, 0.0011662994186545526, 0.0011613331870500778, 0.0012298416032260934, 0.0012463830211091526, 0.001229197604495079, 0.0012388425570544462, 0.0012213832097734477, 0.0012260355342309489, 0.0012319724638621475, 0.0012182409057480304, 0.0011974270455539227, 0.0011592121648623846, 0.0011584774163270065, 0.001155690070739839, 0.0011569465331863179, 0.0011593061607590941, 0.0012492912788983695, 0.0012738164189423239, 0.00129100144028577, 0.0012444439289961444, 0.023016818604150485, 0.025350491746415404, 0.006918060536453024, 0.001285619443542389, 0.0012025896968796503, 0.0012033879296680869, 0.0012678036027652926, 0.0012550668135761869, 0.0012408180465531904, 0.0012249145115357499, 0.0012206414438307631, 0.0012149659775977219, 0.03110454823323634, 0.002729744788497513, 0.0012290525591260818, 0.0012241191647643613, 0.0012169562328910066, 0.0012187757204438364, 0.001305173998590299, 0.0013008582795601944, 0.0013105568589729278, 0.001305664302023171, 0.0013128651392667792, 0.0012333681184235354, 0.04837449767280284, 0.0012728237671517702, 0.001265327627554016, 0.0012687479070019583, 0.001252923602086687, 0.001240931440491316, 0.003560675722330289, 0.0012574167434819216, 0.0012601570004250767, 0.001332828488070951, 0.0012683694895226942, 0.0012538298601717796, 0.0012598638368640528, 0.0012461013258109953, 0.0012983037185920186, 0.0012550694423965937, 0.0012718245338345336, 0.0012537998845800757, 0.0012557166736832884, 0.03585586295365681, 0.005191077605474654, 0.018874538908062805, 0.005782005628895795, 0.001303862276768615, 0.0012748685345963337, 0.0012562413737827608, 0.0012545891398520663, 0.0012566676758619588, 0.0012815312081731337, 0.0012702310463240326, 0.0012725201614071117, 0.0012451442104742625, 0.0012837240017603995, 0.033155770815345785, 0.004693323185905641, 0.0013103517189318704, 0.0012913254647364104, 0.0012853499312430274, 0.001289158512643257, 0.0013397451392699812, 0.0012975000467714529, 0.0013092265106997517, 0.0012929836520925164, 0.0012971221382708051, 0.0012869838608948643, 0.0012891198140244152, 0.0012894117213828966, 0.0012899463249067234, 0.0013079286071060355, 0.0012953589063917483, 0.0013365207434913448, 0.012166252860604503, 0.010266353443893062, 0.007948271207359815, 0.009671519163909346, 0.026495362650927935, 0.0057200811408070285, 0.001269662395379571, 0.0012685380020547052, 0.0012483351382015402, 0.0012447055575416186, 0.0012600883478731957, 0.00125499137374031, 0.0011805751371782186, 0.0011840887221529386, 0.0011855689294333028, 0.001194334556456915, 0.0012744465128107126, 0.0012812494424843165, 0.0012707351625702063, 0.0012830598848373737, 0.02802055639344766, 0.01697998365040782, 0.001281950278965713, 0.0012511738825069611, 0.0012411449055815505, 0.0011460828611107414, 0.0011548437448868224, 0.0011581366984606829, 0.001159745509476336, 0.001156978696303139, 0.0011578989756661792, 0.001151907904788332, 0.0013297847201397947, 0.0011614130912781801, 0.0011860151412398663, 0.0012528948366728632, 0.0012522532799562744, 0.0012580244179251929, 0.0012610557916855744, 0.0012598702315847542, 0.0012627481158043062, 0.0012822585115425807, 0.0012540396757770417, 0.0012533278123280684, 0.0012670643735936907, 0.0012639663723665615, 0.0012455785587473318, 0.0034883296046795886, 0.0012560131618493172, 0.0012700441861958351, 0.0012556361142805842, 0.0012567621401272887, 0.0012540006983028941, 0.0012507316965046664, 0.0011738303474822017, 0.0011693562088577554, 0.0011757500226024626, 0.0011700429536705447, 0.044072902628740425, 0.005831131302794918, 0.0012531007672551759, 0.0012406748827806739, 0.0012514869557953504, 0.0012518373496700512, 0.0012430113491190727, 0.0012479135820772065, 0.0012476841626732156, 0.001248049698661753, 0.0012604370686160617, 0.0012577047197927917, 0.001243055627551363, 0.0012499241369538182, 0.0012470506061258357, 0.0012498389994509, 0.0012522352112153935, 0.0012595526045621481, 0.0012473019305616617, 0.018509144721509413, 0.015433331001225079, 0.0011859312788908218, 0.001171061325125223, 0.0011568617422220318, 0.0012414137681209764, 0.0012441576041584445, 0.001239296649907564, 0.0012441030920071657, 0.0012385065364109915, 0.0012346702075541713, 0.001243419420017406, 0.0012548785591714603, 0.001245529022674228, 0.001242067253347053, 0.0013024387908233113, 0.0012351725829851834, 0.0012358932560959527, 0.0012335918605500876, 0.0010951044184134104, 0.001176409556010608, 0.001121704537031609, 0.001105913905925009, 0.0011014478134944341, 0.014518230649924208, 0.015442011160451139, 0.0012828773943447443, 0.0012167835104560784]
[763.4353742651651, 835.1375464026224, 835.0140845586336, 37.924586388944405, 743.6604086702421, 754.6593135768599, 750.8625713659374, 752.211252524383, 746.3010109179895, 751.3122745965259, 734.4471714487034, 733.5673159322528, 719.2314698765323, 726.1545206029912, 737.9398981175581, 754.1967284785638, 757.0311072291173, 757.6718093307301, 758.3488609294072, 759.1630855572948, 760.212535948279, 803.3118707262687, 811.8166525877007, 806.9122570951038, 807.2276225399635, 787.9605492025944, 782.5786340011672, 802.7975137512695, 820.642386199616, 820.7025783848643, 817.086393989313, 779.1059170339892, 786.9223470334034, 787.2418630878169, 774.3591598884008, 786.717501004107, 787.8589834601977, 786.733275744198, 778.999265581393, 790.1285098092997, 789.2270877824336, 793.6171464446184, 773.3433084136592, 775.9314859443941, 775.8730335458823, 772.524542791953, 776.0672677714844, 781.0398266969447, 768.2937261717826, 672.6401068930641, 784.3587058237206, 788.2626010157882, 794.0448556875849, 778.1989904332897, 787.500223902109, 791.4365087536861, 785.2764822733715, 789.4187617931105, 790.4297567575727, 766.7594990085609, 853.6821032727606, 857.343109602796, 75.4203429200742, 34.85853160986245, 819.6044037533263, 828.0909721389443, 828.8369886683431, 787.1040720841011, 779.6271028407152, 775.4372221791094, 788.7358553922888, 774.0644635701223, 783.0131732004473, 73.1101818729707, 65.38113955513174, 106.91534413614482, 95.79755273247203, 831.6847762397633, 823.2658906694127, 841.861439893146, 835.2698491122159, 841.7201442813702, 846.2719450683601, 825.8630820243791, 800.6651553199265, 841.3563007744781, 840.3652029605715, 823.2013370280423, 832.299491186167, 832.5206237802201, 838.8996928468608, 849.3370831991735, 112.66872068257082, 88.74475841852555, 55.08105108961015, 811.6508200824162, 835.4387833942773, 849.9805813868184, 825.3954919217273, 840.2257584992939, 843.2103652406852, 799.3462910149226, 793.6543972962231, 799.0033943031046, 793.6831533549197, 787.9183163211194, 786.5399958516231, 778.4510830634853, 780.2680296457551, 782.2469242157488, 32.695284949561795, 827.1211089860575, 833.2112090505982, 834.1004791504184, 833.1281190532073, 849.4022460797547, 844.2246153750722, 851.4234061743971, 837.0055492188662, 811.8180460954179, 836.7126307718044, 849.2779856776341, 853.5791885414706, 856.2301039783092, 807.8059750615491, 788.9655199919483, 783.3701229878401, 793.5057728494759, 795.8138117866376, 798.3762883328881, 801.9106209455695, 800.2365865822998, 801.2090286977898, 801.8783636369116, 796.8720357750439, 802.0265538494178, 789.9906206180592, 794.4326894964806, 795.6872287610468, 62.17642431179766, 848.4002975701086, 857.4127569690497, 861.0793277509937, 813.1128410169424, 802.3215841869403, 813.5388454574583, 807.205075661645, 818.7438569631953, 815.6370448326904, 811.706453945464, 820.8557070130353, 835.1239465594382, 862.6548532802143, 863.2019803808823, 865.2838899618038, 864.3441778125431, 862.5849097060062, 800.453838821163, 785.0424795358821, 774.5924743342229, 803.5717614104719, 43.44649089860212, 39.44696655209465, 144.5491832184389, 777.8351556698655, 831.5388054584967, 830.9872280967747, 788.7657029991333, 796.77033061738, 805.9199354634248, 816.3835031607546, 819.2413956236651, 823.0683150298901, 32.14963909784305, 366.33461274979226, 813.6348544044774, 816.9139318985309, 821.7222386251276, 820.4955048134977, 766.181368216105, 768.7232465769359, 763.0344255217522, 765.8936515691408, 761.6928579263587, 810.7879432445347, 20.67204928439435, 785.6547196928332, 790.3091485744949, 788.1786401232317, 798.1332607467414, 805.8462920433984, 280.84556920716966, 795.2812821872349, 793.5519142953453, 750.2840830235665, 788.4137928738056, 797.5563764791788, 793.7365695717689, 802.5029580553363, 770.2357974330365, 796.7686617327479, 786.2719843790194, 797.5754442942234, 796.3579850116856, 27.889441715361468, 192.6382296703429, 52.98142671834071, 172.95036777592566, 766.9521680451694, 784.3946045124046, 796.0253665176035, 797.0736938771158, 795.7553291200012, 780.3165413548797, 787.2583518517642, 785.8421660637835, 803.1198246660203, 778.9836433911631, 30.16066209316301, 213.06864249260863, 763.1538811695132, 774.3981105523411, 777.9982522214218, 775.6997996698065, 746.4106199667899, 770.7128816590665, 763.8097699881763, 773.4049834130828, 770.9374240833644, 777.0105207882568, 775.7230857216974, 775.547471313118, 775.2260545199897, 764.5677253077534, 771.9868177581168, 748.2113576387421, 82.19457637923145, 97.40556912102512, 125.81352270340697, 103.39637269516486, 37.74245377105557, 174.8226948855682, 787.610945743609, 788.3090600204781, 801.0669325872591, 803.4028561542464, 793.5951488543019, 796.8182259449743, 847.0447737788, 844.5313102735881, 843.4768955002861, 837.2863320362901, 784.654349906425, 780.4881444951269, 786.9460367944697, 779.3868484375137, 35.688085060075416, 58.89287178294676, 780.0614551188424, 799.2494200696651, 805.7076941643976, 872.537260552729, 865.9180122225127, 863.4559299684852, 862.2581349347355, 864.3201497099917, 863.6332020456841, 868.1249567288579, 752.0014216247531, 861.020086229148, 843.1595560867755, 798.151585216489, 798.5604957129101, 794.897130573394, 792.9863266900845, 793.7325408047217, 791.9235732638974, 779.8739419533908, 797.4229359054124, 797.8758551144656, 789.2258837360932, 791.1602886457102, 802.8397670924039, 286.67015830685887, 796.1700007407798, 787.3741802600595, 796.4090779381169, 795.695516335905, 797.4477218021914, 799.5319881910973, 851.9118645594249, 855.1714117777807, 850.5209277279461, 854.669477614388, 22.689678699489352, 171.49330860045808, 798.0204195313244, 806.0129320573823, 799.0494789971467, 798.8258221114521, 804.4978838758826, 801.3375400045381, 801.4848868943388, 801.2501433815262, 793.3755876427713, 795.0991868462983, 804.4692271494343, 800.0485552963985, 801.8920764624473, 800.1030536247763, 798.5720182947266, 793.9326998951544, 801.7304996471053, 54.02734783514354, 64.79482620573752, 843.2191795592746, 853.9262449753165, 864.4075290096973, 805.5331958446183, 803.7566918030501, 806.9093062380081, 803.791909548795, 807.4240794060336, 809.9328823856187, 804.2338601933702, 796.8898605298148, 802.8716969219538, 805.1093830106672, 767.7903998604568, 809.6034625243908, 809.1313671852917, 810.6408869738136, 913.1549313341299, 850.0440980700284, 891.5003612683262, 904.2295197143574, 907.8959418217167, 68.878916729789, 64.7584041747827, 779.4977169355848, 821.8388821074483]
Elapsed: 0.13340268729157648~0.2742488938017542
Time per graph: 0.00309878442527409~0.006371729605787921
Speed: 720.2751413049938~230.6574690445588
Total Time: 0.0533
best val loss: 0.6931506991386414 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.05s
test Score 0.5116
Epoch Time List: [0.24144823208916932, 0.23756754712667316, 0.2251020959811285, 3.434465694008395, 1.6451996190007776, 0.25691361108329147, 0.2523142059799284, 0.25170884712133557, 0.251293208100833, 0.2512686770642176, 0.2530690210405737, 0.26223690703045577, 0.261356764822267, 0.26592284301295877, 0.2648622450651601, 0.2626241809921339, 0.2538929469883442, 0.24912723200395703, 0.2498031910508871, 0.25082546996418387, 0.2470288008917123, 0.25776567799039185, 0.2526191349606961, 0.25245179084595293, 0.25272855698131025, 0.26590626290999353, 0.26796004083007574, 0.2600723971845582, 0.25251302192918956, 0.2506266220007092, 0.24844657094217837, 0.2528777208644897, 0.2635207660496235, 0.26019829290453345, 0.2631485660094768, 0.26152968604583293, 0.2597069739131257, 0.2599470920395106, 0.26122286485042423, 0.2599713410018012, 0.2653896501287818, 0.2641383018344641, 0.2662282780511305, 0.2638321410631761, 0.2627378748729825, 0.26397049706429243, 0.26450446294620633, 0.26228272588923573, 0.2659017861587927, 0.2715796501142904, 0.265584489912726, 0.25845784216653556, 0.26021045504603535, 0.26669755997136235, 0.2603706809459254, 0.2644298579543829, 0.26080000295769423, 0.2604537371080369, 0.25846574001479894, 0.2596326519269496, 0.24854881584178656, 0.23899513599462807, 1.9581867039669305, 3.1391427750932053, 3.4883547039935365, 0.24904928705655038, 0.2474038590444252, 0.25255732994992286, 0.26067903405055404, 0.26632207294460386, 0.2616200930206105, 0.2649546639295295, 0.2609765339875594, 0.7939839760074392, 2.4275098480284214, 5.755670748883858, 2.701906479895115, 0.3010525449644774, 0.2512785710860044, 0.24872503394726664, 0.24662521993741393, 0.24279799091164023, 0.24181347305420786, 0.24964770104270428, 0.2501157639781013, 0.24735936906654388, 0.24596022500190884, 0.2490916799288243, 0.2449302668683231, 0.2444682710338384, 0.24664300493896008, 0.24764859513379633, 1.3025275859981775, 2.1610240009613335, 2.1570494670886546, 1.7740393390413374, 0.2503080510068685, 0.24548037885688245, 0.24824339000042528, 0.24840199889149517, 0.24544558010529727, 0.3180918210418895, 0.2611153709003702, 0.2574448661180213, 0.25997040001675487, 0.2622772880131379, 0.26480276708025485, 0.2842469110619277, 0.26568808197043836, 0.2645054808817804, 3.347185779013671, 2.502586298971437, 0.25404888298362494, 0.25447623105719686, 0.24737765302415937, 0.24382041883654892, 0.24417945183813572, 0.24755468999501318, 0.24311322788707912, 0.24442813696805388, 0.24355079501401633, 0.2417708410648629, 0.24075558001641184, 0.240832116920501, 0.24702632694970816, 0.26539046689867973, 0.2623932759743184, 0.25936991802882403, 0.261416569002904, 0.256028707139194, 0.2566215409897268, 0.2558378350222483, 0.25638968497514725, 0.2554748399415985, 0.25748954294249415, 0.2556817269651219, 0.25942857994232327, 0.257382347015664, 0.25635741103906184, 4.465570452040993, 1.5911003779619932, 0.24718992598354816, 0.2382779368199408, 0.24209945497568697, 0.25323997414670885, 0.25154286390170455, 0.2534849400399253, 0.25043864315375686, 0.25157003209460527, 0.2504100180231035, 0.25005865388084203, 0.23619436903391033, 0.23979893291834742, 0.23611750302370638, 0.23745848308317363, 0.23546343808993697, 0.23705232003703713, 0.24430227605625987, 0.25794378796126693, 0.25866281404159963, 0.25708598096389323, 2.3806474449811503, 6.067264645011164, 3.358824785100296, 0.4957698849029839, 0.26219195791054517, 0.25090303795877844, 0.2538472500164062, 0.2584673019591719, 0.25363046990241855, 0.25285993493162096, 0.24942437722347677, 0.2489267559722066, 3.0539317200891674, 5.775578713044524, 0.31391030305530876, 0.2599875519517809, 0.2515543730696663, 0.2525356390979141, 0.26002352603245527, 0.27101008500903845, 0.2720266639953479, 0.2712421319447458, 0.27060803095810115, 0.2550904940580949, 4.2771947619039565, 4.006524771917611, 0.2608454949222505, 0.26391874090768397, 0.2595991159323603, 0.25781219894997776, 0.35653668199665844, 0.2579843749990687, 0.2587218901608139, 0.2800623500952497, 0.2626038360176608, 0.26074382197111845, 0.2606038759695366, 0.26542503596283495, 0.2599996138596907, 0.26394898793660104, 0.26355971908196807, 0.2639581629773602, 0.26003415999002755, 2.452528733992949, 2.859118298976682, 2.5834390679374337, 1.574736577924341, 0.3049175860360265, 0.264102196902968, 0.2631729490822181, 0.2634993268875405, 0.26312594895716757, 0.2640883921412751, 0.26546069304458797, 0.26387318887282163, 0.26370415510609746, 0.26473420299589634, 3.8437440230045468, 3.6070859290193766, 0.32400754804257303, 0.2691879639169201, 0.26599602785427123, 0.2669118168996647, 0.26957473205402493, 0.2701822029193863, 0.26808342104777694, 0.26912118191830814, 0.26740589598193765, 0.26711467292625457, 0.2668000749545172, 0.2705757759977132, 0.26702574209775776, 0.26814069494139403, 0.26639214798342437, 0.2691509020514786, 2.149030990083702, 1.4975806178990752, 1.435706065967679, 1.3780350411543623, 3.0908358050510287, 1.2245018670801073, 0.2679587749298662, 0.2604079790180549, 0.2588339280337095, 0.2593271580990404, 0.2610766489524394, 0.25997482088860124, 0.25360023009125143, 0.24446779012214392, 0.24345930013805628, 0.24469235097058117, 0.25166427704971284, 0.2647271470632404, 0.26533044199459255, 0.26576099009253085, 3.391458256985061, 3.0077646559802815, 0.5218576799379662, 0.25897559605073184, 0.25610956514719874, 0.24255320394877344, 0.23647571506444365, 0.2401096059475094, 0.23889471986331046, 0.24073167005553842, 0.24119389394763857, 0.23966768092941493, 0.25374725996516645, 0.24202676489949226, 0.24098622298333794, 0.2538932180032134, 0.25771768402773887, 0.2594714199658483, 0.25907591090071946, 0.26082258799578995, 0.25956381601281464, 0.33850096410606056, 0.2598399978596717, 0.2592586138052866, 0.26082146691624075, 0.26087734499014914, 0.25646436016540974, 0.3526121359318495, 0.25709177704993635, 0.25887411611620337, 0.262018981971778, 0.25991674314718693, 0.25845829804893583, 0.2598094381392002, 0.3414038419723511, 0.2411382399732247, 0.2427003460470587, 0.24152252590283751, 2.999134410987608, 5.326432249043137, 0.2728878650814295, 0.26142277906183153, 0.25740472704637796, 0.25847212492953986, 0.2584949410520494, 0.2579081740695983, 0.25750014593359083, 0.25872597796842456, 0.26238492294214666, 0.2587981621036306, 0.2591223461786285, 0.2594444890273735, 0.25948577909730375, 0.25844080303795636, 0.2592461189487949, 0.25876610877458006, 0.2552806939929724, 5.460049442015588, 1.6423721270402893, 0.24805308494251221, 0.23911558603867888, 0.2384888199158013, 0.24611739092506468, 0.255798430996947, 0.2548640980385244, 0.25610364985186607, 0.2569985290756449, 0.2568116660695523, 0.25507447798736393, 0.2568639989476651, 0.2562175679486245, 0.25477869098540395, 0.2587261430453509, 0.2540747349848971, 0.25231188302859664, 0.25265625899191946, 0.2442600860958919, 0.2508105100132525, 0.23506220895797014, 0.23654110392089933, 0.23657151591032743, 4.954526419984177, 5.332010749145411, 0.6372562369797379, 0.2633447441039607]
Total Epoch List: [21, 311, 9]
Total Time List: [0.05826871597673744, 0.054139356943778694, 0.053295415942557156]
T-times Epoch Time: 0.9954852363360082 ~ 0.2685643083152793
T-times Total Epoch: 212.55555555555554 ~ 69.93347809186021
T-times Total Time: 0.22535090986639258 ~ 0.24236087313137744
T-times Inference Elapsed: 0.21668320326471205 ~ 0.0601222511904267
T-times Time Per Graph: 0.005037327226403222 ~ 0.001399487892750291
T-times Speed: 681.9940508032868 ~ 31.42811332003168
T-times cross validation test micro f1 score:0.5898145168978502 ~ 0.05308175344747838
T-times cross validation test precision:0.6230466346745417 ~ 0.11262461227639416
T-times cross validation test recall:0.6192881192881192 ~ 0.15154530648105488
T-times cross validation test f1_score:0.5898145168978502 ~ 0.12375984000698209
