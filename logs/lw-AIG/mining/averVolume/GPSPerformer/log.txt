Namespace(seed=60, model='GPSPerformer', dataset='mining/averVolume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/averVolume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 436], edge_attr=[436, 2], x=[115, 14887], y=[1, 1], num_nodes=115)
Data(edge_index=[2, 390], edge_attr=[390, 2], x=[103, 14887], y=[1, 1], num_nodes=115)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d829b8b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7632;  Loss pred: 0.7632; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9733 score: 0.5116 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9004 score: 0.5000 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7672;  Loss pred: 0.7672; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8787 score: 0.5116 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8192 score: 0.5000 time: 0.12s
Epoch 3/1000, LR 0.000030
Train loss: 0.7224;  Loss pred: 0.7224; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8229 score: 0.5116 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7726 score: 0.5000 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.7375;  Loss pred: 0.7375; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7856 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7424 score: 0.5000 time: 0.10s
Epoch 5/1000, LR 0.000090
Train loss: 0.7226;  Loss pred: 0.7226; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7646 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7253 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7538 score: 0.5116 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7172 score: 0.5000 time: 0.11s
Epoch 7/1000, LR 0.000150
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7462 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7123 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7410 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7081 score: 0.5000 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7385 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7058 score: 0.5000 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7372 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7039 score: 0.5000 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6308;  Loss pred: 0.6308; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7348 score: 0.5116 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7017 score: 0.5000 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7341 score: 0.5116 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7008 score: 0.5000 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.6239;  Loss pred: 0.6239; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7323 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 0.19s
Epoch 14/1000, LR 0.000270
Train loss: 0.6255;  Loss pred: 0.6255; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7311 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.5000 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7303 score: 0.5116 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.5998;  Loss pred: 0.5998; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7284 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7248 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5000 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7203 score: 0.5116 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6843 score: 0.5000 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7156 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.5000 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7111 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6746 score: 0.5000 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7067 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6701 score: 0.5000 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7007 score: 0.5116 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6641 score: 0.5000 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.4972;  Loss pred: 0.4972; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5116 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6591 score: 0.5000 time: 0.15s
Epoch 24/1000, LR 0.000270
Train loss: 0.4978;  Loss pred: 0.4978; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5116 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6548 score: 0.5000 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.4816;  Loss pred: 0.4816; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6520 score: 0.5000 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 0.4433;  Loss pred: 0.4433; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6828 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6497 score: 0.5000 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.4632;  Loss pred: 0.4632; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6806 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6480 score: 0.5000 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.4351;  Loss pred: 0.4351; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6475 score: 0.5000 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.4045;  Loss pred: 0.4045; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6752 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6467 score: 0.5000 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.4128;  Loss pred: 0.4128; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6441 score: 0.5000 time: 0.19s
Epoch 31/1000, LR 0.000270
Train loss: 0.3860;  Loss pred: 0.3860; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6651 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6403 score: 0.5000 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.3799;  Loss pred: 0.3799; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6592 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6362 score: 0.5000 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.3477;  Loss pred: 0.3477; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6561 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6338 score: 0.5000 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.3388;  Loss pred: 0.3388; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6531 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6303 score: 0.5000 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.3155;  Loss pred: 0.3155; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6494 score: 0.5116 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6261 score: 0.5000 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.3012;  Loss pred: 0.3012; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6454 score: 0.5116 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6220 score: 0.5000 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.2708;  Loss pred: 0.2708; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6439 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6204 score: 0.5000 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.2621;  Loss pred: 0.2621; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6430 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6212 score: 0.5000 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.2353;  Loss pred: 0.2353; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6391 score: 0.5116 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6204 score: 0.5000 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.2340;  Loss pred: 0.2340; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6330 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6181 score: 0.5000 time: 0.11s
Epoch 41/1000, LR 0.000269
Train loss: 0.2135;  Loss pred: 0.2135; Loss self: 0.0000; time: 0.19s
Val loss: 0.6216 score: 0.5349 time: 0.08s
Test loss: 0.6130 score: 0.5227 time: 0.13s
Epoch 42/1000, LR 0.000269
Train loss: 0.1930;  Loss pred: 0.1930; Loss self: 0.0000; time: 0.14s
Val loss: 0.6107 score: 0.5581 time: 0.15s
Test loss: 0.6095 score: 0.5682 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.1785;  Loss pred: 0.1785; Loss self: 0.0000; time: 0.17s
Val loss: 0.6041 score: 0.6047 time: 0.06s
Test loss: 0.6102 score: 0.5682 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.1652;  Loss pred: 0.1652; Loss self: 0.0000; time: 0.35s
Val loss: 0.6001 score: 0.6047 time: 0.09s
Test loss: 0.6142 score: 0.5682 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.1533;  Loss pred: 0.1533; Loss self: 0.0000; time: 0.16s
Val loss: 0.5978 score: 0.6047 time: 0.09s
Test loss: 0.6193 score: 0.5909 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.1455;  Loss pred: 0.1455; Loss self: 0.0000; time: 0.17s
Val loss: 0.5982 score: 0.6047 time: 0.13s
Test loss: 0.6257 score: 0.5682 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.1391;  Loss pred: 0.1391; Loss self: 0.0000; time: 0.15s
Val loss: 0.6046 score: 0.6047 time: 0.08s
Test loss: 0.6394 score: 0.5682 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 0.22s
Val loss: 0.6159 score: 0.6047 time: 0.06s
Test loss: 0.6581 score: 0.5455 time: 0.20s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.15s
Val loss: 0.6290 score: 0.6047 time: 0.12s
Test loss: 0.6776 score: 0.5455 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 0.18s
Val loss: 0.6443 score: 0.5814 time: 0.07s
Test loss: 0.6970 score: 0.5455 time: 0.22s
     INFO: Early stopping counter 5 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.30s
Val loss: 0.6564 score: 0.5814 time: 0.17s
Test loss: 0.7091 score: 0.5455 time: 0.13s
     INFO: Early stopping counter 6 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0926;  Loss pred: 0.0926; Loss self: 0.0000; time: 0.39s
Val loss: 0.6656 score: 0.5581 time: 0.09s
Test loss: 0.7163 score: 0.5455 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.21s
Val loss: 0.6644 score: 0.5814 time: 0.06s
Test loss: 0.7164 score: 0.5455 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0822;  Loss pred: 0.0822; Loss self: 0.0000; time: 0.21s
Val loss: 0.6544 score: 0.6047 time: 0.18s
Test loss: 0.7089 score: 0.5455 time: 0.19s
     INFO: Early stopping counter 9 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0793;  Loss pred: 0.0793; Loss self: 0.0000; time: 0.41s
Val loss: 0.6450 score: 0.6047 time: 0.10s
Test loss: 0.7013 score: 0.5909 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.22s
Val loss: 0.6359 score: 0.6047 time: 0.12s
Test loss: 0.6950 score: 0.5909 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0653;  Loss pred: 0.0653; Loss self: 0.0000; time: 0.23s
Val loss: 0.6350 score: 0.6047 time: 0.07s
Test loss: 0.6983 score: 0.5909 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.16s
Val loss: 0.6461 score: 0.6047 time: 0.05s
Test loss: 0.7123 score: 0.5909 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0609;  Loss pred: 0.0609; Loss self: 0.0000; time: 0.15s
Val loss: 0.6576 score: 0.6047 time: 0.06s
Test loss: 0.7271 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.21s
Val loss: 0.6669 score: 0.6047 time: 0.23s
Test loss: 0.7389 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.19s
Val loss: 0.6783 score: 0.6047 time: 0.22s
Test loss: 0.7507 score: 0.6136 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.61s
Val loss: 0.6893 score: 0.6047 time: 0.08s
Test loss: 0.7582 score: 0.6364 time: 0.22s
     INFO: Early stopping counter 17 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.26s
Val loss: 0.6949 score: 0.6047 time: 0.14s
Test loss: 0.7651 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0449;  Loss pred: 0.0449; Loss self: 0.0000; time: 0.18s
Val loss: 0.6962 score: 0.6047 time: 0.13s
Test loss: 0.7677 score: 0.6364 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.27s
Val loss: 0.6944 score: 0.6279 time: 0.18s
Test loss: 0.7674 score: 0.6818 time: 0.11s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 044,   Train_Loss: 0.1533,   Val_Loss: 0.5978,   Val_Precision: 0.5676,   Val_Recall: 0.9545,   Val_accuracy: 0.7119,   Val_Score: 0.6047,   Val_Loss: 0.5978,   Test_Precision: 0.5526,   Test_Recall: 0.9545,   Test_accuracy: 0.7000,   Test_Score: 0.5909,   Test_loss: 0.6193


[0.08374932792503387, 0.12289128405973315, 0.07516246393788606, 0.10959600692149252, 0.08232551102992147, 0.11518683494068682, 0.07347436796408147, 0.07498858298640698, 0.06376529601402581, 0.07348674000240862, 0.05954030796419829, 0.059361143968999386, 0.19897365500219166, 0.0847056150669232, 0.06056350201833993, 0.05874769005458802, 0.05871189699973911, 0.06089370895642787, 0.06068921601399779, 0.06527896004263312, 0.06414733501151204, 0.0643916999688372, 0.15840244095306844, 0.062041984987445176, 0.06342304800637066, 0.06466882501263171, 0.061913358978927135, 0.05814945593010634, 0.0690739779965952, 0.1920453489292413, 0.06166033598128706, 0.0684415289433673, 0.0628115120343864, 0.2285462860018015, 0.083039614954032, 0.05744171992409974, 0.09478490694891661, 0.09751461795531213, 0.09935775795020163, 0.1153019149787724, 0.13896151504013687, 0.06413777207490057, 0.09266008296981454, 0.0956406919285655, 0.08460183802526444, 0.06042038695886731, 0.08774950308725238, 0.20773301599547267, 0.07257838908117265, 0.22554882208351046, 0.13148974406067282, 0.09904248407110572, 0.15475070499815047, 0.19875972997397184, 0.06862824503332376, 0.19707918004132807, 0.16248734993860126, 0.18967065901961178, 0.05773005401715636, 0.05796188104432076, 0.07607808301690966, 0.22162520501296967, 0.05916735401842743, 0.07579440507106483, 0.11852810997515917]
[0.0019033938164780425, 0.002792983728630299, 0.0017082378167701377, 0.00249081833912483, 0.0018710343415891243, 0.002617882612288337, 0.0016698719991836697, 0.001704285976963795, 0.001449211273046041, 0.0016701531818729234, 0.001353188817368143, 0.0013491169083863497, 0.004522128522777083, 0.0019251276151573454, 0.0013764432276895438, 0.0013351747739679095, 0.0013343612954486161, 0.0013839479308279062, 0.0013793003639544952, 0.0014836127282416617, 0.0014578939775343645, 0.0014634477265644819, 0.0036000554762061006, 0.0014100451133510267, 0.0014414329092356968, 0.0014697460230143572, 0.0014071217949756167, 0.0013215785438660532, 0.0015698631362862545, 0.00436466702111912, 0.0014013712723019787, 0.0015554892941674386, 0.0014275343644178727, 0.005194233772768216, 0.001887263976228, 0.0013054936346386305, 0.0021542024306571957, 0.0022162413171661847, 0.0022581308625045826, 0.0026204980676993728, 0.0031582162509122018, 0.001457676638065922, 0.0021059109765866942, 0.0021736520892855797, 0.0019227690460287374, 0.0013731906127015297, 0.001994306888346645, 0.004721204908988016, 0.0016495088427539238, 0.005126109592807056, 0.0029884032741062006, 0.0022509655470705843, 0.0035170614772306926, 0.004517266590317542, 0.0015597328416664493, 0.0044790722736665475, 0.003692894316786392, 0.004310696795900268, 0.0013120466822080991, 0.0013173154782800173, 0.0017290473412934014, 0.005036936477567492, 0.0013447125913278962, 0.0017226001152514734, 0.0026938206812536173]
[525.3773503637607, 358.04003788106843, 585.398584542963, 401.47448101388176, 534.4637336536916, 381.9880980552763, 598.8482952518864, 586.7559866810096, 690.0305142521688, 598.7474746948611, 738.995169901662, 741.2256074946677, 221.13480299447355, 519.4460835357492, 726.5101675704924, 748.9656182075491, 749.422216764611, 722.5705373191167, 725.0052462343808, 674.0303456314865, 685.9209348619651, 683.3178813619472, 277.7734972722822, 709.197167190957, 693.754106481611, 680.3896621193521, 710.6705358204821, 756.6708801693088, 636.9982050573205, 229.1125520369239, 713.5867701621559, 642.8845275564823, 700.5085306004421, 192.52117708731078, 529.8675821697506, 765.9937769645325, 464.2089275216925, 451.21440172348116, 442.84413122579633, 381.6068450216165, 316.6344292323762, 686.0232056176892, 474.85388087051126, 460.05522453626554, 520.0832632839536, 728.2310196052551, 501.42734091894823, 211.8103364029478, 606.2410664804065, 195.0797153075302, 334.6268586521641, 444.2538009084164, 284.32826849173887, 221.37281030600076, 641.1354388945099, 223.26051889790222, 270.7903108557447, 231.9810572042692, 762.1680032886159, 759.1196008003129, 578.3531636860546, 198.53337528745922, 743.6533326519279, 580.5177830572798, 371.2199579426468]
Elapsed: 0.09950884599775936~0.050208193336421486
Time per graph: 0.0022615646817672583~0.0011410953031004885
Speed: 531.1265570404784~187.31961682821918
Total Time: 0.1226
best val loss: 0.5977551937103271 test_score: 0.5909

Testing...
Test loss: 0.7674 score: 0.6818 time: 0.08s
test Score 0.6818
Epoch Time List: [0.5052011228399351, 0.4362354901386425, 0.48141768004279584, 0.3196370010264218, 0.3090726350201294, 0.49785935296677053, 0.28845445008482784, 0.29510697606019676, 0.25798914197366685, 0.28149793518241495, 0.2858224200317636, 0.3412361659575254, 0.39260433497838676, 0.2829891029978171, 0.2709295379463583, 0.2756802769144997, 0.2569202349986881, 0.34737623017281294, 0.24736407201271504, 0.27122120989952236, 0.30052394105587155, 0.30600703903473914, 0.39681613305583596, 0.28494060004595667, 0.25840707099996507, 0.2530015171505511, 0.25654663296882063, 0.2668465729802847, 0.2592045699711889, 0.4806343438103795, 0.2749604048440233, 0.2692889879690483, 0.29564462997950613, 0.4160056409891695, 0.352469025994651, 0.3543463450623676, 0.49004953005351126, 0.3540811110287905, 0.4888184469891712, 0.3577835289761424, 0.3991441720863804, 0.3487090590642765, 0.3190409380476922, 0.5267637631623074, 0.3314663580385968, 0.3546273229876533, 0.3073983429931104, 0.4868952879915014, 0.34204149805009365, 0.4701924021355808, 0.5928165569202974, 0.5792281380854547, 0.42831076704896986, 0.5864273669430986, 0.5793449149932712, 0.5405858890153468, 0.45487733907066286, 0.3956619640812278, 0.25864077685400844, 0.4954711690079421, 0.4864566719625145, 0.9133714289637282, 0.4570394289912656, 0.38510313897859305, 0.5722765199607238]
Total Epoch List: [65]
Total Time List: [0.12260988901834935]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d829b400>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7768;  Loss pred: 0.7768; Loss self: 0.0000; time: 0.33s
Val loss: 0.9330 score: 0.5227 time: 0.08s
Test loss: 0.9136 score: 0.5349 time: 0.13s
Epoch 2/1000, LR 0.000000
Train loss: 0.7304;  Loss pred: 0.7304; Loss self: 0.0000; time: 0.24s
Val loss: 0.8380 score: 0.5682 time: 0.06s
Test loss: 0.8290 score: 0.5814 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.7477;  Loss pred: 0.7477; Loss self: 0.0000; time: 0.30s
Val loss: 0.7592 score: 0.5455 time: 0.19s
Test loss: 0.7620 score: 0.5814 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.18s
Val loss: 0.7175 score: 0.5227 time: 0.10s
Test loss: 0.7218 score: 0.5581 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.21s
Val loss: 0.7009 score: 0.5227 time: 0.22s
Test loss: 0.7036 score: 0.5581 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.5867;  Loss pred: 0.5867; Loss self: 0.0000; time: 0.24s
Val loss: 0.6900 score: 0.5455 time: 0.07s
Test loss: 0.6968 score: 0.5581 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.17s
Val loss: 0.6854 score: 0.5455 time: 0.05s
Test loss: 0.6901 score: 0.5581 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6084;  Loss pred: 0.6084; Loss self: 0.0000; time: 0.20s
Val loss: 0.6805 score: 0.5455 time: 0.05s
Test loss: 0.6810 score: 0.5581 time: 0.11s
Epoch 9/1000, LR 0.000210
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.24s
Val loss: 0.6778 score: 0.5455 time: 0.17s
Test loss: 0.6721 score: 0.5581 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.5506;  Loss pred: 0.5506; Loss self: 0.0000; time: 0.23s
Val loss: 0.6773 score: 0.5227 time: 0.06s
Test loss: 0.6674 score: 0.5581 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.4796;  Loss pred: 0.4796; Loss self: 0.0000; time: 0.21s
Val loss: 0.6775 score: 0.5227 time: 0.17s
Test loss: 0.6614 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4685;  Loss pred: 0.4685; Loss self: 0.0000; time: 0.33s
Val loss: 0.6765 score: 0.5227 time: 0.06s
Test loss: 0.6582 score: 0.5581 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.4564;  Loss pred: 0.4564; Loss self: 0.0000; time: 0.19s
Val loss: 0.6801 score: 0.5227 time: 0.06s
Test loss: 0.6598 score: 0.5581 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4128;  Loss pred: 0.4128; Loss self: 0.0000; time: 0.24s
Val loss: 0.6852 score: 0.5455 time: 0.16s
Test loss: 0.6653 score: 0.5814 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3518;  Loss pred: 0.3518; Loss self: 0.0000; time: 0.25s
Val loss: 0.6885 score: 0.5455 time: 0.05s
Test loss: 0.6703 score: 0.5814 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2702;  Loss pred: 0.2702; Loss self: 0.0000; time: 0.19s
Val loss: 0.6886 score: 0.5455 time: 0.12s
Test loss: 0.6731 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3312;  Loss pred: 0.3312; Loss self: 0.0000; time: 0.26s
Val loss: 0.6862 score: 0.5455 time: 0.09s
Test loss: 0.6734 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2901;  Loss pred: 0.2901; Loss self: 0.0000; time: 0.37s
Val loss: 0.6846 score: 0.5455 time: 0.07s
Test loss: 0.6728 score: 0.5581 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2537;  Loss pred: 0.2537; Loss self: 0.0000; time: 0.21s
Val loss: 0.6806 score: 0.5455 time: 0.05s
Test loss: 0.6697 score: 0.5581 time: 0.17s
     INFO: Early stopping counter 7 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2443;  Loss pred: 0.2443; Loss self: 0.0000; time: 0.18s
Val loss: 0.6798 score: 0.5455 time: 0.18s
Test loss: 0.6682 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2215;  Loss pred: 0.2215; Loss self: 0.0000; time: 0.16s
Val loss: 0.6777 score: 0.5455 time: 0.06s
Test loss: 0.6660 score: 0.5581 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1963;  Loss pred: 0.1963; Loss self: 0.0000; time: 0.22s
Val loss: 0.6745 score: 0.5455 time: 0.07s
Test loss: 0.6624 score: 0.5581 time: 0.11s
Epoch 23/1000, LR 0.000270
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.25s
Val loss: 0.6702 score: 0.5455 time: 0.15s
Test loss: 0.6585 score: 0.5581 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.1466;  Loss pred: 0.1466; Loss self: 0.0000; time: 0.23s
Val loss: 0.6648 score: 0.5455 time: 0.11s
Test loss: 0.6542 score: 0.5581 time: 0.09s
Epoch 25/1000, LR 0.000270
Train loss: 0.1465;  Loss pred: 0.1465; Loss self: 0.0000; time: 0.18s
Val loss: 0.6586 score: 0.5455 time: 0.07s
Test loss: 0.6489 score: 0.5814 time: 0.12s
Epoch 26/1000, LR 0.000270
Train loss: 0.1426;  Loss pred: 0.1426; Loss self: 0.0000; time: 0.17s
Val loss: 0.6525 score: 0.5455 time: 0.08s
Test loss: 0.6440 score: 0.5814 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.1311;  Loss pred: 0.1311; Loss self: 0.0000; time: 0.20s
Val loss: 0.6465 score: 0.5455 time: 0.09s
Test loss: 0.6400 score: 0.5814 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.23s
Val loss: 0.6411 score: 0.5455 time: 0.06s
Test loss: 0.6363 score: 0.5814 time: 0.19s
Epoch 29/1000, LR 0.000270
Train loss: 0.1106;  Loss pred: 0.1106; Loss self: 0.0000; time: 0.18s
Val loss: 0.6384 score: 0.5455 time: 0.10s
Test loss: 0.6326 score: 0.5814 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.0944;  Loss pred: 0.0944; Loss self: 0.0000; time: 0.20s
Val loss: 0.6372 score: 0.5455 time: 0.05s
Test loss: 0.6306 score: 0.5814 time: 0.16s
Epoch 31/1000, LR 0.000270
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.30s
Val loss: 0.6368 score: 0.5455 time: 0.07s
Test loss: 0.6294 score: 0.5814 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.0951;  Loss pred: 0.0951; Loss self: 0.0000; time: 0.27s
Val loss: 0.6356 score: 0.5455 time: 0.15s
Test loss: 0.6282 score: 0.5814 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.16s
Val loss: 0.6336 score: 0.5682 time: 0.07s
Test loss: 0.6274 score: 0.5814 time: 0.08s
Epoch 34/1000, LR 0.000270
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.22s
Val loss: 0.6315 score: 0.5682 time: 0.07s
Test loss: 0.6273 score: 0.5814 time: 0.32s
Epoch 35/1000, LR 0.000270
Train loss: 0.0650;  Loss pred: 0.0650; Loss self: 0.0000; time: 0.17s
Val loss: 0.6288 score: 0.5909 time: 0.08s
Test loss: 0.6266 score: 0.5814 time: 0.11s
Epoch 36/1000, LR 0.000270
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.33s
Val loss: 0.6253 score: 0.5909 time: 0.06s
Test loss: 0.6265 score: 0.5814 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.0630;  Loss pred: 0.0630; Loss self: 0.0000; time: 0.34s
Val loss: 0.6195 score: 0.5909 time: 0.08s
Test loss: 0.6255 score: 0.6047 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.21s
Val loss: 0.6138 score: 0.6136 time: 0.10s
Test loss: 0.6242 score: 0.6047 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.0482;  Loss pred: 0.0482; Loss self: 0.0000; time: 0.28s
Val loss: 0.6105 score: 0.6136 time: 0.21s
Test loss: 0.6236 score: 0.6047 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.0435;  Loss pred: 0.0435; Loss self: 0.0000; time: 0.30s
Val loss: 0.6050 score: 0.6364 time: 0.08s
Test loss: 0.6218 score: 0.6047 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.0346;  Loss pred: 0.0346; Loss self: 0.0000; time: 0.21s
Val loss: 0.6021 score: 0.6591 time: 0.05s
Test loss: 0.6203 score: 0.6047 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.36s
Val loss: 0.5982 score: 0.6591 time: 0.05s
Test loss: 0.6180 score: 0.6279 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.25s
Val loss: 0.5996 score: 0.6591 time: 0.05s
Test loss: 0.6191 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.32s
Val loss: 0.6001 score: 0.6591 time: 0.07s
Test loss: 0.6198 score: 0.6279 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.31s
Val loss: 0.5952 score: 0.6591 time: 0.08s
Test loss: 0.6171 score: 0.6512 time: 0.11s
Epoch 46/1000, LR 0.000269
Train loss: 0.0262;  Loss pred: 0.0262; Loss self: 0.0000; time: 0.16s
Val loss: 0.5901 score: 0.6818 time: 0.07s
Test loss: 0.6141 score: 0.6279 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.17s
Val loss: 0.5797 score: 0.6818 time: 0.05s
Test loss: 0.6095 score: 0.6512 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.21s
Val loss: 0.5667 score: 0.6818 time: 0.10s
Test loss: 0.6035 score: 0.6279 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.34s
Val loss: 0.5515 score: 0.7273 time: 0.06s
Test loss: 0.5967 score: 0.6279 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.23s
Val loss: 0.5335 score: 0.7273 time: 0.08s
Test loss: 0.5889 score: 0.6279 time: 0.15s
Epoch 51/1000, LR 0.000269
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.23s
Val loss: 0.5169 score: 0.7500 time: 0.13s
Test loss: 0.5833 score: 0.6512 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.23s
Val loss: 0.4983 score: 0.7500 time: 0.06s
Test loss: 0.5769 score: 0.6512 time: 0.09s
Epoch 53/1000, LR 0.000269
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.20s
Val loss: 0.4825 score: 0.7500 time: 0.07s
Test loss: 0.5700 score: 0.6744 time: 0.14s
Epoch 54/1000, LR 0.000269
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.16s
Val loss: 0.4648 score: 0.7727 time: 0.25s
Test loss: 0.5645 score: 0.6977 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.16s
Val loss: 0.4558 score: 0.7727 time: 0.08s
Test loss: 0.5618 score: 0.6977 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.22s
Val loss: 0.4453 score: 0.7727 time: 0.06s
Test loss: 0.5594 score: 0.6977 time: 0.26s
Epoch 57/1000, LR 0.000269
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.16s
Val loss: 0.4390 score: 0.7727 time: 0.15s
Test loss: 0.5590 score: 0.6977 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.27s
Val loss: 0.4350 score: 0.7727 time: 0.08s
Test loss: 0.5606 score: 0.6977 time: 0.08s
Epoch 59/1000, LR 0.000268
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.20s
Val loss: 0.4321 score: 0.7727 time: 0.10s
Test loss: 0.5610 score: 0.6977 time: 0.10s
Epoch 60/1000, LR 0.000268
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.18s
Val loss: 0.4299 score: 0.7727 time: 0.13s
Test loss: 0.5611 score: 0.6977 time: 0.12s
Epoch 61/1000, LR 0.000268
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.17s
Val loss: 0.4215 score: 0.7955 time: 0.06s
Test loss: 0.5609 score: 0.6977 time: 0.09s
Epoch 62/1000, LR 0.000268
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.18s
Val loss: 0.4143 score: 0.7955 time: 0.20s
Test loss: 0.5610 score: 0.6977 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.16s
Val loss: 0.4037 score: 0.7955 time: 0.05s
Test loss: 0.5594 score: 0.6977 time: 0.14s
Epoch 64/1000, LR 0.000268
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.17s
Val loss: 0.3950 score: 0.7955 time: 0.06s
Test loss: 0.5595 score: 0.6977 time: 0.09s
Epoch 65/1000, LR 0.000268
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.22s
Val loss: 0.3840 score: 0.7955 time: 0.06s
Test loss: 0.5584 score: 0.7209 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.25s
Val loss: 0.3740 score: 0.8409 time: 0.16s
Test loss: 0.5580 score: 0.7209 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.25s
Val loss: 0.3636 score: 0.8182 time: 0.06s
Test loss: 0.5561 score: 0.7442 time: 0.13s
Epoch 68/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.23s
Val loss: 0.3521 score: 0.8182 time: 0.17s
Test loss: 0.5547 score: 0.7674 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.27s
Val loss: 0.3412 score: 0.8409 time: 0.05s
Test loss: 0.5534 score: 0.7674 time: 0.11s
Epoch 70/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.19s
Val loss: 0.3296 score: 0.8409 time: 0.12s
Test loss: 0.5566 score: 0.7209 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.31s
Val loss: 0.3187 score: 0.8409 time: 0.16s
Test loss: 0.5625 score: 0.7442 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.30s
Val loss: 0.3107 score: 0.8864 time: 0.08s
Test loss: 0.5754 score: 0.7674 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.18s
Val loss: 0.3033 score: 0.8864 time: 0.08s
Test loss: 0.5888 score: 0.7674 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.26s
Val loss: 0.2976 score: 0.8864 time: 0.05s
Test loss: 0.6033 score: 0.7674 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.20s
Val loss: 0.2970 score: 0.8864 time: 0.11s
Test loss: 0.6222 score: 0.7674 time: 0.11s
Epoch 76/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.20s
Val loss: 0.2998 score: 0.9091 time: 0.17s
Test loss: 0.6393 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.32s
Val loss: 0.3040 score: 0.9091 time: 0.08s
Test loss: 0.6595 score: 0.7442 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.22s
Val loss: 0.3065 score: 0.8864 time: 0.06s
Test loss: 0.6764 score: 0.7442 time: 0.30s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.3084 score: 0.8864 time: 0.15s
Test loss: 0.6945 score: 0.7442 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.21s
Val loss: 0.3136 score: 0.8864 time: 0.06s
Test loss: 0.7128 score: 0.7442 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.20s
Val loss: 0.3220 score: 0.8864 time: 0.16s
Test loss: 0.7303 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.29s
Val loss: 0.3338 score: 0.9091 time: 0.08s
Test loss: 0.7408 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.3440 score: 0.9091 time: 0.05s
Test loss: 0.7439 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.26s
Val loss: 0.3547 score: 0.9091 time: 0.15s
Test loss: 0.7486 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.22s
Val loss: 0.3652 score: 0.8864 time: 0.06s
Test loss: 0.7532 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.19s
Val loss: 0.3790 score: 0.8864 time: 0.14s
Test loss: 0.7659 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.16s
Val loss: 0.3894 score: 0.8864 time: 0.06s
Test loss: 0.7682 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.16s
Val loss: 0.3972 score: 0.8864 time: 0.08s
Test loss: 0.7594 score: 0.7907 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.21s
Val loss: 0.4070 score: 0.8864 time: 0.05s
Test loss: 0.7489 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.21s
Val loss: 0.4098 score: 0.8864 time: 0.10s
Test loss: 0.7287 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.39s
Val loss: 0.4164 score: 0.8864 time: 0.06s
Test loss: 0.7145 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.22s
Val loss: 0.4241 score: 0.8864 time: 0.23s
Test loss: 0.7013 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.30s
Val loss: 0.4310 score: 0.8864 time: 0.07s
Test loss: 0.6919 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.31s
Val loss: 0.4409 score: 0.8864 time: 0.14s
Test loss: 0.6903 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.18s
Val loss: 0.4519 score: 0.8864 time: 0.13s
Test loss: 0.6914 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0039,   Val_Loss: 0.2970,   Val_Precision: 0.9048,   Val_Recall: 0.8636,   Val_accuracy: 0.8837,   Val_Score: 0.8864,   Val_Loss: 0.2970,   Test_Precision: 0.8750,   Test_Recall: 0.6364,   Test_accuracy: 0.7368,   Test_Score: 0.7674,   Test_loss: 0.6222


[0.08374932792503387, 0.12289128405973315, 0.07516246393788606, 0.10959600692149252, 0.08232551102992147, 0.11518683494068682, 0.07347436796408147, 0.07498858298640698, 0.06376529601402581, 0.07348674000240862, 0.05954030796419829, 0.059361143968999386, 0.19897365500219166, 0.0847056150669232, 0.06056350201833993, 0.05874769005458802, 0.05871189699973911, 0.06089370895642787, 0.06068921601399779, 0.06527896004263312, 0.06414733501151204, 0.0643916999688372, 0.15840244095306844, 0.062041984987445176, 0.06342304800637066, 0.06466882501263171, 0.061913358978927135, 0.05814945593010634, 0.0690739779965952, 0.1920453489292413, 0.06166033598128706, 0.0684415289433673, 0.0628115120343864, 0.2285462860018015, 0.083039614954032, 0.05744171992409974, 0.09478490694891661, 0.09751461795531213, 0.09935775795020163, 0.1153019149787724, 0.13896151504013687, 0.06413777207490057, 0.09266008296981454, 0.0956406919285655, 0.08460183802526444, 0.06042038695886731, 0.08774950308725238, 0.20773301599547267, 0.07257838908117265, 0.22554882208351046, 0.13148974406067282, 0.09904248407110572, 0.15475070499815047, 0.19875972997397184, 0.06862824503332376, 0.19707918004132807, 0.16248734993860126, 0.18967065901961178, 0.05773005401715636, 0.05796188104432076, 0.07607808301690966, 0.22162520501296967, 0.05916735401842743, 0.07579440507106483, 0.11852810997515917, 0.13329688692465425, 0.1087285369867459, 0.05314465402625501, 0.05948480998631567, 0.23557127302046865, 0.05491444398649037, 0.09229667508043349, 0.11212576902471483, 0.06325464707333595, 0.09129202598705888, 0.05454373999964446, 0.0955028380267322, 0.10944388492498547, 0.0655963160097599, 0.08155848702881485, 0.05495364801026881, 0.05528234597295523, 0.07695881091058254, 0.16987517301458865, 0.0588038609130308, 0.09132100595161319, 0.11799857299774885, 0.062021246063522995, 0.09052809095010161, 0.12053457298316061, 0.061124118976294994, 0.08304336096625775, 0.19193756696768105, 0.05321028199978173, 0.1594991859747097, 0.0821198959602043, 0.05732642801012844, 0.08944544300902635, 0.3261265819892287, 0.11812122003175318, 0.09059448400512338, 0.0739748680498451, 0.0898832370294258, 0.060836819000542164, 0.09160137199796736, 0.09929384698625654, 0.05403605289757252, 0.05326700792647898, 0.09576565003953874, 0.1176717970520258, 0.08675254310946912, 0.06214999093208462, 0.05761576001532376, 0.07673117599915713, 0.15387706295587122, 0.05445864994544536, 0.0951883269008249, 0.14282860199455172, 0.07474058191291988, 0.09773175290320069, 0.26231901103165, 0.05944013700354844, 0.0881722189951688, 0.10751691390760243, 0.1251139820087701, 0.09316424001008272, 0.05592761002480984, 0.14038938097655773, 0.09474401897750795, 0.09724188398104161, 0.07111072598490864, 0.1339404210448265, 0.054872150998562574, 0.11634491803124547, 0.08816802804358304, 0.054681145935319364, 0.09303271095268428, 0.05948506307322532, 0.05586546799167991, 0.11718122905585915, 0.05476806696970016, 0.08346834895201027, 0.3088197379838675, 0.07489148306194693, 0.11036044999491423, 0.054591792984865606, 0.08041682897601277, 0.09423529403284192, 0.06562154891435057, 0.09845123905688524, 0.06417848693672568, 0.11950023903045803, 0.0871005670633167, 0.12471880798693746, 0.10269605298526585, 0.0977598400786519, 0.0670643460471183, 0.08082922198809683, 0.059180247015319765, 0.05887846404220909]
[0.0019033938164780425, 0.002792983728630299, 0.0017082378167701377, 0.00249081833912483, 0.0018710343415891243, 0.002617882612288337, 0.0016698719991836697, 0.001704285976963795, 0.001449211273046041, 0.0016701531818729234, 0.001353188817368143, 0.0013491169083863497, 0.004522128522777083, 0.0019251276151573454, 0.0013764432276895438, 0.0013351747739679095, 0.0013343612954486161, 0.0013839479308279062, 0.0013793003639544952, 0.0014836127282416617, 0.0014578939775343645, 0.0014634477265644819, 0.0036000554762061006, 0.0014100451133510267, 0.0014414329092356968, 0.0014697460230143572, 0.0014071217949756167, 0.0013215785438660532, 0.0015698631362862545, 0.00436466702111912, 0.0014013712723019787, 0.0015554892941674386, 0.0014275343644178727, 0.005194233772768216, 0.001887263976228, 0.0013054936346386305, 0.0021542024306571957, 0.0022162413171661847, 0.0022581308625045826, 0.0026204980676993728, 0.0031582162509122018, 0.001457676638065922, 0.0021059109765866942, 0.0021736520892855797, 0.0019227690460287374, 0.0013731906127015297, 0.001994306888346645, 0.004721204908988016, 0.0016495088427539238, 0.005126109592807056, 0.0029884032741062006, 0.0022509655470705843, 0.0035170614772306926, 0.004517266590317542, 0.0015597328416664493, 0.0044790722736665475, 0.003692894316786392, 0.004310696795900268, 0.0013120466822080991, 0.0013173154782800173, 0.0017290473412934014, 0.005036936477567492, 0.0013447125913278962, 0.0017226001152514734, 0.0026938206812536173, 0.003099927602898936, 0.0025285706275987416, 0.0012359221866570932, 0.0013833676741003644, 0.005478401698150434, 0.0012770800927090784, 0.0021464343041961275, 0.002607576023830577, 0.0014710383040310686, 0.002123070371792067, 0.0012684590697591735, 0.0022209962331798186, 0.0025452066261624526, 0.001525495721157207, 0.001896709000670113, 0.0012779918141922978, 0.0012856359528594238, 0.0017897397886181986, 0.0039505854189439225, 0.0013675316491402513, 0.0021237443244561207, 0.002744152860412764, 0.001442354559616814, 0.0021053044407000373, 0.002803129604259549, 0.0014214911389836046, 0.0019312409527036686, 0.0044636643480856055, 0.001237448418599575, 0.0037092833947606906, 0.0019097650223303326, 0.0013331727444215915, 0.002080126581605264, 0.007584339116028575, 0.0027470051170175157, 0.0021068484652354276, 0.0017203457686010488, 0.0020903078378936234, 0.001414809744198655, 0.0021302644650690082, 0.0023091592322385243, 0.0012566523929668028, 0.0012387676261971857, 0.0022271081404543892, 0.0027365534198145534, 0.0020175010025457935, 0.0014453486263275492, 0.0013399013957052037, 0.0017844459534687705, 0.0035785363478109586, 0.001266480231289427, 0.0022136820209494165, 0.003321595395222133, 0.0017381530677423227, 0.0022728314628651324, 0.0061004421170151165, 0.0013823287675243823, 0.0020505167208178794, 0.0025003933466884285, 0.002909627488576049, 0.0021666102327926214, 0.0013006420936002288, 0.0032648693250362263, 0.0022033492785466964, 0.0022614391623498047, 0.0016537378136025265, 0.003114893512670384, 0.0012760965348502925, 0.0027056957681684994, 0.0020504192568275123, 0.001271654556635334, 0.0021635514175042856, 0.0013833735598424493, 0.0012991969300390675, 0.002725144861764166, 0.0012736759760395386, 0.001941124394232797, 0.00718185437171785, 0.0017416623967894634, 0.0025665220929049822, 0.0012695765810433863, 0.0018701588133956458, 0.0021915184658800445, 0.0015260825328918736, 0.002289563698997331, 0.0014925229520168762, 0.0027790753262897216, 0.0020255945828678303, 0.0029004373950450575, 0.002388280301982927, 0.0022734846529919046, 0.0015596359545841467, 0.0018797493485603915, 0.001376284814309762, 0.0013692666056327695]
[525.3773503637607, 358.04003788106843, 585.398584542963, 401.47448101388176, 534.4637336536916, 381.9880980552763, 598.8482952518864, 586.7559866810096, 690.0305142521688, 598.7474746948611, 738.995169901662, 741.2256074946677, 221.13480299447355, 519.4460835357492, 726.5101675704924, 748.9656182075491, 749.422216764611, 722.5705373191167, 725.0052462343808, 674.0303456314865, 685.9209348619651, 683.3178813619472, 277.7734972722822, 709.197167190957, 693.754106481611, 680.3896621193521, 710.6705358204821, 756.6708801693088, 636.9982050573205, 229.1125520369239, 713.5867701621559, 642.8845275564823, 700.5085306004421, 192.52117708731078, 529.8675821697506, 765.9937769645325, 464.2089275216925, 451.21440172348116, 442.84413122579633, 381.6068450216165, 316.6344292323762, 686.0232056176892, 474.85388087051126, 460.05522453626554, 520.0832632839536, 728.2310196052551, 501.42734091894823, 211.8103364029478, 606.2410664804065, 195.0797153075302, 334.6268586521641, 444.2538009084164, 284.32826849173887, 221.37281030600076, 641.1354388945099, 223.26051889790222, 270.7903108557447, 231.9810572042692, 762.1680032886159, 759.1196008003129, 578.3531636860546, 198.53337528745922, 743.6533326519279, 580.5177830572798, 371.2199579426468, 322.5881788545118, 395.4803512645603, 809.1124269763191, 722.8736211797943, 182.5349901482417, 783.036244718758, 465.8889387134144, 383.49792714038745, 679.7919518884804, 471.01594619113257, 788.3581140618573, 450.24839982204367, 392.89540963821656, 655.5246180837678, 527.2290054229178, 782.4776253610113, 777.825167206835, 558.7404416884916, 253.12704167964094, 731.2445021866123, 470.8664731834396, 364.4111865727422, 693.31080442916, 474.9906857497003, 356.7441186024474, 703.4866223049555, 517.8017784886115, 224.03118201055688, 808.1144918603586, 269.59385239005616, 523.62462832196, 750.0903421438146, 480.7399745972601, 131.8506444268324, 364.03281297332353, 474.64258417287556, 581.2784954347754, 478.3984358053632, 706.808815885275, 469.42528329110803, 433.05805248890914, 795.7650067725747, 807.2539020654235, 449.01277213955734, 365.4231606660054, 495.6627028874559, 691.8745981313002, 746.3235751565808, 560.3980317006003, 279.44385715453586, 789.5899006507835, 451.73606260356866, 301.0601476141331, 575.3233236810923, 439.97982971399017, 163.92254541860808, 723.416905944093, 487.68195345470536, 399.9370744304772, 343.6866072809179, 461.55048326854075, 768.8510197543741, 306.29097230067674, 453.8545067442001, 442.19628661640627, 604.6907749068067, 321.0382621211036, 783.6397738650053, 369.5907026076718, 487.70513477679606, 786.3770823468727, 462.2030204179417, 722.8705456202941, 769.7062522845781, 366.95296974144486, 785.1290428743685, 515.1653356019135, 139.2398046858206, 574.1640870488877, 389.63233660230253, 787.6641826349396, 534.7139466644016, 456.30461963660963, 655.2725546927232, 436.76443701388615, 670.0064469016573, 359.83191622771756, 493.68220494754837, 344.7755851266927, 418.7113209323571, 439.8534200281495, 641.1752672543606, 531.9858207509694, 726.5937904731752, 730.3179642929194]
Elapsed: 0.09740189558360726~0.04973356306433405
Time per graph: 0.002243793835619519~0.001145161335325155
Speed: 528.3750762763883~183.07531831234937
Total Time: 0.0596
best val loss: 0.2969529926776886 test_score: 0.7674

Testing...
Test loss: 0.6393 score: 0.7674 time: 0.12s
test Score 0.7674
Epoch Time List: [0.5052011228399351, 0.4362354901386425, 0.48141768004279584, 0.3196370010264218, 0.3090726350201294, 0.49785935296677053, 0.28845445008482784, 0.29510697606019676, 0.25798914197366685, 0.28149793518241495, 0.2858224200317636, 0.3412361659575254, 0.39260433497838676, 0.2829891029978171, 0.2709295379463583, 0.2756802769144997, 0.2569202349986881, 0.34737623017281294, 0.24736407201271504, 0.27122120989952236, 0.30052394105587155, 0.30600703903473914, 0.39681613305583596, 0.28494060004595667, 0.25840707099996507, 0.2530015171505511, 0.25654663296882063, 0.2668465729802847, 0.2592045699711889, 0.4806343438103795, 0.2749604048440233, 0.2692889879690483, 0.29564462997950613, 0.4160056409891695, 0.352469025994651, 0.3543463450623676, 0.49004953005351126, 0.3540811110287905, 0.4888184469891712, 0.3577835289761424, 0.3991441720863804, 0.3487090590642765, 0.3190409380476922, 0.5267637631623074, 0.3314663580385968, 0.3546273229876533, 0.3073983429931104, 0.4868952879915014, 0.34204149805009365, 0.4701924021355808, 0.5928165569202974, 0.5792281380854547, 0.42831076704896986, 0.5864273669430986, 0.5793449149932712, 0.5405858890153468, 0.45487733907066286, 0.3956619640812278, 0.25864077685400844, 0.4954711690079421, 0.4864566719625145, 0.9133714289637282, 0.4570394289912656, 0.38510313897859305, 0.5722765199607238, 0.5382651450345293, 0.40595252404455096, 0.5421426059911028, 0.33437401696573943, 0.6658152739983052, 0.3561661639250815, 0.303375513991341, 0.3633673151489347, 0.4735041089588776, 0.372730182018131, 0.4324080809019506, 0.4738884859252721, 0.35092323389835656, 0.45493361388798803, 0.3816128239268437, 0.36080420610960573, 0.3991781381191686, 0.5086690030293539, 0.42911247513256967, 0.41455614718142897, 0.30717616993933916, 0.3971787019399926, 0.45455540099646896, 0.4175047359894961, 0.3732549491105601, 0.30442297901026905, 0.3688860770780593, 0.4778283240739256, 0.3232527830405161, 0.4103575429180637, 0.4402691189898178, 0.46981179900467396, 0.30938490596599877, 0.6055462978547439, 0.3649530881084502, 0.48414977302309126, 0.487241946044378, 0.40372989885509014, 0.5453108559595421, 0.47644064703490585, 0.3571210609516129, 0.4615682860603556, 0.35122145095374435, 0.48000291909556836, 0.49702881090343, 0.31462480302434415, 0.27905118197668344, 0.36738704110030085, 0.46782337699551135, 0.4538434719434008, 0.40487696998752654, 0.37942006485536695, 0.4020701370900497, 0.479138873051852, 0.3334447320085019, 0.5305394359165803, 0.36192117305472493, 0.4307202970376238, 0.3987008520634845, 0.4305742080323398, 0.320365410996601, 0.43019407405517995, 0.34224363905377686, 0.32544718391727656, 0.37477697303984314, 0.4811468069674447, 0.4402979100123048, 0.4455852350220084, 0.43354872008785605, 0.38553798699285835, 0.5192393880570307, 0.47015014605131, 0.3088569201063365, 0.3594515809090808, 0.42402804421726614, 0.42445767391473055, 0.47107753914315253, 0.5751094999723136, 0.41341264790389687, 0.3673327799187973, 0.4106603229884058, 0.4376733241369948, 0.3529545289929956, 0.4672544860513881, 0.37569731299299747, 0.3966485959244892, 0.3430644019972533, 0.32871355686802417, 0.3802572649437934, 0.4138372589368373, 0.5452143209986389, 0.5061887999763712, 0.44770110596437007, 0.4975182939087972, 0.366916537983343]
Total Epoch List: [65, 95]
Total Time List: [0.12260988901834935, 0.059565117000602186]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d83f72b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8184;  Loss pred: 0.8184; Loss self: 0.0000; time: 0.17s
Val loss: 0.9664 score: 0.3409 time: 0.06s
Test loss: 0.8190 score: 0.3721 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7424;  Loss pred: 0.7424; Loss self: 0.0000; time: 0.19s
Val loss: 0.8785 score: 0.4091 time: 0.06s
Test loss: 0.7630 score: 0.4651 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7366;  Loss pred: 0.7366; Loss self: 0.0000; time: 0.18s
Val loss: 0.8401 score: 0.4091 time: 0.06s
Test loss: 0.7371 score: 0.4651 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7199;  Loss pred: 0.7199; Loss self: 0.0000; time: 0.18s
Val loss: 0.8253 score: 0.4091 time: 0.06s
Test loss: 0.7326 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.7827;  Loss pred: 0.7827; Loss self: 0.0000; time: 0.18s
Val loss: 0.8249 score: 0.4091 time: 0.06s
Test loss: 0.7301 score: 0.5581 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.22s
Val loss: 0.8153 score: 0.4091 time: 0.06s
Test loss: 0.7308 score: 0.5116 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.8076;  Loss pred: 0.8076; Loss self: 0.0000; time: 0.19s
Val loss: 0.8061 score: 0.4091 time: 0.06s
Test loss: 0.7312 score: 0.5116 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.7517;  Loss pred: 0.7517; Loss self: 0.0000; time: 0.19s
Val loss: 0.8007 score: 0.4091 time: 0.07s
Test loss: 0.7321 score: 0.5116 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.17s
Val loss: 0.7979 score: 0.4091 time: 0.05s
Test loss: 0.7356 score: 0.5116 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.15s
Val loss: 0.7948 score: 0.4318 time: 0.05s
Test loss: 0.7384 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.17s
Val loss: 0.7858 score: 0.4318 time: 0.05s
Test loss: 0.7416 score: 0.4884 time: 0.14s
Epoch 12/1000, LR 0.000270
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.16s
Val loss: 0.7815 score: 0.4318 time: 0.06s
Test loss: 0.7444 score: 0.4884 time: 0.07s
Epoch 13/1000, LR 0.000270
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.17s
Val loss: 0.7769 score: 0.4091 time: 0.06s
Test loss: 0.7441 score: 0.4651 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.6669;  Loss pred: 0.6669; Loss self: 0.0000; time: 0.16s
Val loss: 0.7693 score: 0.4091 time: 0.05s
Test loss: 0.7434 score: 0.4651 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.22s
Val loss: 0.7616 score: 0.4318 time: 0.08s
Test loss: 0.7435 score: 0.4651 time: 0.10s
Epoch 16/1000, LR 0.000270
Train loss: 0.5694;  Loss pred: 0.5694; Loss self: 0.0000; time: 0.39s
Val loss: 0.7590 score: 0.4318 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7424 score: 0.4884 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.39s
Val loss: 0.7567 score: 0.4318 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7410 score: 0.4884 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 0.16s
Val loss: 0.7564 score: 0.4318 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7405 score: 0.4884 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.5540;  Loss pred: 0.5540; Loss self: 0.0000; time: 0.16s
Val loss: 0.7555 score: 0.4318 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7417 score: 0.4884 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 0.22s
Val loss: 0.7590 score: 0.4318 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7429 score: 0.4884 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.5062;  Loss pred: 0.5062; Loss self: 0.0000; time: 0.19s
Val loss: 0.7658 score: 0.4318 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7454 score: 0.4884 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.5038;  Loss pred: 0.5038; Loss self: 0.0000; time: 0.17s
Val loss: 0.7723 score: 0.4318 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7446 score: 0.4884 time: 0.23s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.23s
Val loss: 0.7827 score: 0.4318 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7425 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.4677;  Loss pred: 0.4677; Loss self: 0.0000; time: 0.16s
Val loss: 0.7937 score: 0.4318 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7397 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.4641;  Loss pred: 0.4641; Loss self: 0.0000; time: 0.16s
Val loss: 0.8058 score: 0.4091 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7389 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.4344;  Loss pred: 0.4344; Loss self: 0.0000; time: 0.18s
Val loss: 0.8203 score: 0.4091 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7406 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.4605;  Loss pred: 0.4605; Loss self: 0.0000; time: 0.17s
Val loss: 0.8331 score: 0.4091 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7430 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.4180;  Loss pred: 0.4180; Loss self: 0.0000; time: 0.31s
Val loss: 0.8441 score: 0.4091 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7459 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.4121;  Loss pred: 0.4121; Loss self: 0.0000; time: 0.35s
Val loss: 0.8540 score: 0.4091 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7488 score: 0.4884 time: 0.12s
     INFO: Early stopping counter 10 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.3855;  Loss pred: 0.3855; Loss self: 0.0000; time: 0.25s
Val loss: 0.8717 score: 0.4318 time: 0.14s
Test loss: 0.7498 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.3944;  Loss pred: 0.3944; Loss self: 0.0000; time: 0.20s
Val loss: 0.8823 score: 0.4318 time: 0.07s
Test loss: 0.7556 score: 0.4884 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.3417;  Loss pred: 0.3417; Loss self: 0.0000; time: 0.21s
Val loss: 0.8961 score: 0.4091 time: 0.20s
Test loss: 0.7626 score: 0.4884 time: 0.17s
     INFO: Early stopping counter 13 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.3460;  Loss pred: 0.3460; Loss self: 0.0000; time: 0.23s
Val loss: 0.9138 score: 0.4545 time: 0.16s
Test loss: 0.7650 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.3519;  Loss pred: 0.3519; Loss self: 0.0000; time: 0.30s
Val loss: 0.9317 score: 0.4545 time: 0.09s
Test loss: 0.7695 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.3178;  Loss pred: 0.3178; Loss self: 0.0000; time: 0.36s
Val loss: 0.9468 score: 0.4545 time: 0.16s
Test loss: 0.7737 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.3137;  Loss pred: 0.3137; Loss self: 0.0000; time: 0.21s
Val loss: 0.9631 score: 0.4545 time: 0.14s
Test loss: 0.7748 score: 0.4884 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.2933;  Loss pred: 0.2933; Loss self: 0.0000; time: 0.20s
Val loss: 0.9757 score: 0.4545 time: 0.06s
Test loss: 0.7764 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.2893;  Loss pred: 0.2893; Loss self: 0.0000; time: 0.16s
Val loss: 0.9815 score: 0.4545 time: 0.09s
Test loss: 0.7774 score: 0.4884 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.2827;  Loss pred: 0.2827; Loss self: 0.0000; time: 0.35s
Val loss: 0.9818 score: 0.4545 time: 0.06s
Test loss: 0.7769 score: 0.4884 time: 0.25s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 018,   Train_Loss: 0.5540,   Val_Loss: 0.7555,   Val_Precision: 0.4634,   Val_Recall: 0.8636,   Val_accuracy: 0.6032,   Val_Score: 0.4318,   Val_Loss: 0.7555,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.7417


[0.08374932792503387, 0.12289128405973315, 0.07516246393788606, 0.10959600692149252, 0.08232551102992147, 0.11518683494068682, 0.07347436796408147, 0.07498858298640698, 0.06376529601402581, 0.07348674000240862, 0.05954030796419829, 0.059361143968999386, 0.19897365500219166, 0.0847056150669232, 0.06056350201833993, 0.05874769005458802, 0.05871189699973911, 0.06089370895642787, 0.06068921601399779, 0.06527896004263312, 0.06414733501151204, 0.0643916999688372, 0.15840244095306844, 0.062041984987445176, 0.06342304800637066, 0.06466882501263171, 0.061913358978927135, 0.05814945593010634, 0.0690739779965952, 0.1920453489292413, 0.06166033598128706, 0.0684415289433673, 0.0628115120343864, 0.2285462860018015, 0.083039614954032, 0.05744171992409974, 0.09478490694891661, 0.09751461795531213, 0.09935775795020163, 0.1153019149787724, 0.13896151504013687, 0.06413777207490057, 0.09266008296981454, 0.0956406919285655, 0.08460183802526444, 0.06042038695886731, 0.08774950308725238, 0.20773301599547267, 0.07257838908117265, 0.22554882208351046, 0.13148974406067282, 0.09904248407110572, 0.15475070499815047, 0.19875972997397184, 0.06862824503332376, 0.19707918004132807, 0.16248734993860126, 0.18967065901961178, 0.05773005401715636, 0.05796188104432076, 0.07607808301690966, 0.22162520501296967, 0.05916735401842743, 0.07579440507106483, 0.11852810997515917, 0.13329688692465425, 0.1087285369867459, 0.05314465402625501, 0.05948480998631567, 0.23557127302046865, 0.05491444398649037, 0.09229667508043349, 0.11212576902471483, 0.06325464707333595, 0.09129202598705888, 0.05454373999964446, 0.0955028380267322, 0.10944388492498547, 0.0655963160097599, 0.08155848702881485, 0.05495364801026881, 0.05528234597295523, 0.07695881091058254, 0.16987517301458865, 0.0588038609130308, 0.09132100595161319, 0.11799857299774885, 0.062021246063522995, 0.09052809095010161, 0.12053457298316061, 0.061124118976294994, 0.08304336096625775, 0.19193756696768105, 0.05321028199978173, 0.1594991859747097, 0.0821198959602043, 0.05732642801012844, 0.08944544300902635, 0.3261265819892287, 0.11812122003175318, 0.09059448400512338, 0.0739748680498451, 0.0898832370294258, 0.060836819000542164, 0.09160137199796736, 0.09929384698625654, 0.05403605289757252, 0.05326700792647898, 0.09576565003953874, 0.1176717970520258, 0.08675254310946912, 0.06214999093208462, 0.05761576001532376, 0.07673117599915713, 0.15387706295587122, 0.05445864994544536, 0.0951883269008249, 0.14282860199455172, 0.07474058191291988, 0.09773175290320069, 0.26231901103165, 0.05944013700354844, 0.0881722189951688, 0.10751691390760243, 0.1251139820087701, 0.09316424001008272, 0.05592761002480984, 0.14038938097655773, 0.09474401897750795, 0.09724188398104161, 0.07111072598490864, 0.1339404210448265, 0.054872150998562574, 0.11634491803124547, 0.08816802804358304, 0.054681145935319364, 0.09303271095268428, 0.05948506307322532, 0.05586546799167991, 0.11718122905585915, 0.05476806696970016, 0.08346834895201027, 0.3088197379838675, 0.07489148306194693, 0.11036044999491423, 0.054591792984865606, 0.08041682897601277, 0.09423529403284192, 0.06562154891435057, 0.09845123905688524, 0.06417848693672568, 0.11950023903045803, 0.0871005670633167, 0.12471880798693746, 0.10269605298526585, 0.0977598400786519, 0.0670643460471183, 0.08082922198809683, 0.059180247015319765, 0.05887846404220909, 0.05814406811259687, 0.05819180607795715, 0.06507877097465098, 0.06021966889966279, 0.07604256202466786, 0.06294243200682104, 0.06397659005597234, 0.08557534997817129, 0.06610800500493497, 0.05257903703022748, 0.14286376198288053, 0.07079096604138613, 0.06359292194247246, 0.06188474898226559, 0.10072116495575756, 0.25087880494538695, 0.07895247195847332, 0.07316774502396584, 0.05797421804163605, 0.0900378639344126, 0.07448822597507387, 0.23784299206454307, 0.05115171999204904, 0.054360260954126716, 0.06922610697802156, 0.06617284705862403, 0.05174609995447099, 0.058076072949916124, 0.12497881299350411, 0.11060829600319266, 0.0901679260423407, 0.1771157670300454, 0.06620198604650795, 0.10423047898802906, 0.05205103906337172, 0.1146670589223504, 0.10594289703294635, 0.08757507195696235, 0.2543767530005425]
[0.0019033938164780425, 0.002792983728630299, 0.0017082378167701377, 0.00249081833912483, 0.0018710343415891243, 0.002617882612288337, 0.0016698719991836697, 0.001704285976963795, 0.001449211273046041, 0.0016701531818729234, 0.001353188817368143, 0.0013491169083863497, 0.004522128522777083, 0.0019251276151573454, 0.0013764432276895438, 0.0013351747739679095, 0.0013343612954486161, 0.0013839479308279062, 0.0013793003639544952, 0.0014836127282416617, 0.0014578939775343645, 0.0014634477265644819, 0.0036000554762061006, 0.0014100451133510267, 0.0014414329092356968, 0.0014697460230143572, 0.0014071217949756167, 0.0013215785438660532, 0.0015698631362862545, 0.00436466702111912, 0.0014013712723019787, 0.0015554892941674386, 0.0014275343644178727, 0.005194233772768216, 0.001887263976228, 0.0013054936346386305, 0.0021542024306571957, 0.0022162413171661847, 0.0022581308625045826, 0.0026204980676993728, 0.0031582162509122018, 0.001457676638065922, 0.0021059109765866942, 0.0021736520892855797, 0.0019227690460287374, 0.0013731906127015297, 0.001994306888346645, 0.004721204908988016, 0.0016495088427539238, 0.005126109592807056, 0.0029884032741062006, 0.0022509655470705843, 0.0035170614772306926, 0.004517266590317542, 0.0015597328416664493, 0.0044790722736665475, 0.003692894316786392, 0.004310696795900268, 0.0013120466822080991, 0.0013173154782800173, 0.0017290473412934014, 0.005036936477567492, 0.0013447125913278962, 0.0017226001152514734, 0.0026938206812536173, 0.003099927602898936, 0.0025285706275987416, 0.0012359221866570932, 0.0013833676741003644, 0.005478401698150434, 0.0012770800927090784, 0.0021464343041961275, 0.002607576023830577, 0.0014710383040310686, 0.002123070371792067, 0.0012684590697591735, 0.0022209962331798186, 0.0025452066261624526, 0.001525495721157207, 0.001896709000670113, 0.0012779918141922978, 0.0012856359528594238, 0.0017897397886181986, 0.0039505854189439225, 0.0013675316491402513, 0.0021237443244561207, 0.002744152860412764, 0.001442354559616814, 0.0021053044407000373, 0.002803129604259549, 0.0014214911389836046, 0.0019312409527036686, 0.0044636643480856055, 0.001237448418599575, 0.0037092833947606906, 0.0019097650223303326, 0.0013331727444215915, 0.002080126581605264, 0.007584339116028575, 0.0027470051170175157, 0.0021068484652354276, 0.0017203457686010488, 0.0020903078378936234, 0.001414809744198655, 0.0021302644650690082, 0.0023091592322385243, 0.0012566523929668028, 0.0012387676261971857, 0.0022271081404543892, 0.0027365534198145534, 0.0020175010025457935, 0.0014453486263275492, 0.0013399013957052037, 0.0017844459534687705, 0.0035785363478109586, 0.001266480231289427, 0.0022136820209494165, 0.003321595395222133, 0.0017381530677423227, 0.0022728314628651324, 0.0061004421170151165, 0.0013823287675243823, 0.0020505167208178794, 0.0025003933466884285, 0.002909627488576049, 0.0021666102327926214, 0.0013006420936002288, 0.0032648693250362263, 0.0022033492785466964, 0.0022614391623498047, 0.0016537378136025265, 0.003114893512670384, 0.0012760965348502925, 0.0027056957681684994, 0.0020504192568275123, 0.001271654556635334, 0.0021635514175042856, 0.0013833735598424493, 0.0012991969300390675, 0.002725144861764166, 0.0012736759760395386, 0.001941124394232797, 0.00718185437171785, 0.0017416623967894634, 0.0025665220929049822, 0.0012695765810433863, 0.0018701588133956458, 0.0021915184658800445, 0.0015260825328918736, 0.002289563698997331, 0.0014925229520168762, 0.0027790753262897216, 0.0020255945828678303, 0.0029004373950450575, 0.002388280301982927, 0.0022734846529919046, 0.0015596359545841467, 0.0018797493485603915, 0.001376284814309762, 0.0013692666056327695, 0.0013521876305255087, 0.0013532978157664453, 0.0015134597901081623, 0.0014004574162712277, 0.0017684316749922758, 0.0014637774885307218, 0.001487827675720287, 0.001990124418097007, 0.0015373954652310457, 0.001222768303028546, 0.0033224130693693147, 0.001646301535846189, 0.0014789051614528478, 0.0014391802088898975, 0.0023423526733897105, 0.005834390812683417, 0.0018361039990342635, 0.001701575465673624, 0.001348237628875257, 0.0020939038124282, 0.001732284325001718, 0.005531232373594025, 0.0011895748835360242, 0.0012641921152122492, 0.0016099094646051526, 0.0015389034199680006, 0.0012033976733597906, 0.001350606347672468, 0.002906484023104747, 0.00257228595356262, 0.0020969285126125744, 0.004118971326280125, 0.0015395810708490221, 0.002423964627628583, 0.0012104892805435282, 0.00266667578889187, 0.0024637883030917754, 0.0020366295803944732, 0.005915738441873082]
[525.3773503637607, 358.04003788106843, 585.398584542963, 401.47448101388176, 534.4637336536916, 381.9880980552763, 598.8482952518864, 586.7559866810096, 690.0305142521688, 598.7474746948611, 738.995169901662, 741.2256074946677, 221.13480299447355, 519.4460835357492, 726.5101675704924, 748.9656182075491, 749.422216764611, 722.5705373191167, 725.0052462343808, 674.0303456314865, 685.9209348619651, 683.3178813619472, 277.7734972722822, 709.197167190957, 693.754106481611, 680.3896621193521, 710.6705358204821, 756.6708801693088, 636.9982050573205, 229.1125520369239, 713.5867701621559, 642.8845275564823, 700.5085306004421, 192.52117708731078, 529.8675821697506, 765.9937769645325, 464.2089275216925, 451.21440172348116, 442.84413122579633, 381.6068450216165, 316.6344292323762, 686.0232056176892, 474.85388087051126, 460.05522453626554, 520.0832632839536, 728.2310196052551, 501.42734091894823, 211.8103364029478, 606.2410664804065, 195.0797153075302, 334.6268586521641, 444.2538009084164, 284.32826849173887, 221.37281030600076, 641.1354388945099, 223.26051889790222, 270.7903108557447, 231.9810572042692, 762.1680032886159, 759.1196008003129, 578.3531636860546, 198.53337528745922, 743.6533326519279, 580.5177830572798, 371.2199579426468, 322.5881788545118, 395.4803512645603, 809.1124269763191, 722.8736211797943, 182.5349901482417, 783.036244718758, 465.8889387134144, 383.49792714038745, 679.7919518884804, 471.01594619113257, 788.3581140618573, 450.24839982204367, 392.89540963821656, 655.5246180837678, 527.2290054229178, 782.4776253610113, 777.825167206835, 558.7404416884916, 253.12704167964094, 731.2445021866123, 470.8664731834396, 364.4111865727422, 693.31080442916, 474.9906857497003, 356.7441186024474, 703.4866223049555, 517.8017784886115, 224.03118201055688, 808.1144918603586, 269.59385239005616, 523.62462832196, 750.0903421438146, 480.7399745972601, 131.8506444268324, 364.03281297332353, 474.64258417287556, 581.2784954347754, 478.3984358053632, 706.808815885275, 469.42528329110803, 433.05805248890914, 795.7650067725747, 807.2539020654235, 449.01277213955734, 365.4231606660054, 495.6627028874559, 691.8745981313002, 746.3235751565808, 560.3980317006003, 279.44385715453586, 789.5899006507835, 451.73606260356866, 301.0601476141331, 575.3233236810923, 439.97982971399017, 163.92254541860808, 723.416905944093, 487.68195345470536, 399.9370744304772, 343.6866072809179, 461.55048326854075, 768.8510197543741, 306.29097230067674, 453.8545067442001, 442.19628661640627, 604.6907749068067, 321.0382621211036, 783.6397738650053, 369.5907026076718, 487.70513477679606, 786.3770823468727, 462.2030204179417, 722.8705456202941, 769.7062522845781, 366.95296974144486, 785.1290428743685, 515.1653356019135, 139.2398046858206, 574.1640870488877, 389.63233660230253, 787.6641826349396, 534.7139466644016, 456.30461963660963, 655.2725546927232, 436.76443701388615, 670.0064469016573, 359.83191622771756, 493.68220494754837, 344.7755851266927, 418.7113209323571, 439.8534200281495, 641.1752672543606, 531.9858207509694, 726.5937904731752, 730.3179642929194, 739.5423367475742, 738.9356491598608, 660.7377391430618, 714.0524148620947, 565.472793855249, 683.163942494947, 672.1208486163428, 502.4811468602643, 650.4507282709567, 817.8164232121534, 300.9860541482361, 607.4221387918502, 676.1758806884002, 694.8400164364015, 426.92119396045547, 171.3975001170806, 544.6314590709298, 587.6906550272323, 741.7090122564166, 477.57685623598337, 577.2724405383095, 180.79153657943877, 840.6364440273733, 791.0190136189126, 621.1529418179181, 649.8133586711982, 830.9804997445937, 740.4081890502912, 344.0583165262977, 388.7592662919138, 476.8879787676189, 242.77906321409324, 649.5273415179994, 412.5472742472821, 826.1122308749274, 374.99871719146904, 405.87902732759676, 491.007304237578, 169.04060411490624]
Elapsed: 0.09635681740883457~0.05029439374515192
Time per graph: 0.002223677101793059~0.001160227947996023
Speed: 535.3156208167705~184.98260228570197
Total Time: 0.2551
best val loss: 0.7555288076400757 test_score: 0.4884

Testing...
Test loss: 0.7650 score: 0.4884 time: 0.05s
test Score 0.4884
Epoch Time List: [0.5052011228399351, 0.4362354901386425, 0.48141768004279584, 0.3196370010264218, 0.3090726350201294, 0.49785935296677053, 0.28845445008482784, 0.29510697606019676, 0.25798914197366685, 0.28149793518241495, 0.2858224200317636, 0.3412361659575254, 0.39260433497838676, 0.2829891029978171, 0.2709295379463583, 0.2756802769144997, 0.2569202349986881, 0.34737623017281294, 0.24736407201271504, 0.27122120989952236, 0.30052394105587155, 0.30600703903473914, 0.39681613305583596, 0.28494060004595667, 0.25840707099996507, 0.2530015171505511, 0.25654663296882063, 0.2668465729802847, 0.2592045699711889, 0.4806343438103795, 0.2749604048440233, 0.2692889879690483, 0.29564462997950613, 0.4160056409891695, 0.352469025994651, 0.3543463450623676, 0.49004953005351126, 0.3540811110287905, 0.4888184469891712, 0.3577835289761424, 0.3991441720863804, 0.3487090590642765, 0.3190409380476922, 0.5267637631623074, 0.3314663580385968, 0.3546273229876533, 0.3073983429931104, 0.4868952879915014, 0.34204149805009365, 0.4701924021355808, 0.5928165569202974, 0.5792281380854547, 0.42831076704896986, 0.5864273669430986, 0.5793449149932712, 0.5405858890153468, 0.45487733907066286, 0.3956619640812278, 0.25864077685400844, 0.4954711690079421, 0.4864566719625145, 0.9133714289637282, 0.4570394289912656, 0.38510313897859305, 0.5722765199607238, 0.5382651450345293, 0.40595252404455096, 0.5421426059911028, 0.33437401696573943, 0.6658152739983052, 0.3561661639250815, 0.303375513991341, 0.3633673151489347, 0.4735041089588776, 0.372730182018131, 0.4324080809019506, 0.4738884859252721, 0.35092323389835656, 0.45493361388798803, 0.3816128239268437, 0.36080420610960573, 0.3991781381191686, 0.5086690030293539, 0.42911247513256967, 0.41455614718142897, 0.30717616993933916, 0.3971787019399926, 0.45455540099646896, 0.4175047359894961, 0.3732549491105601, 0.30442297901026905, 0.3688860770780593, 0.4778283240739256, 0.3232527830405161, 0.4103575429180637, 0.4402691189898178, 0.46981179900467396, 0.30938490596599877, 0.6055462978547439, 0.3649530881084502, 0.48414977302309126, 0.487241946044378, 0.40372989885509014, 0.5453108559595421, 0.47644064703490585, 0.3571210609516129, 0.4615682860603556, 0.35122145095374435, 0.48000291909556836, 0.49702881090343, 0.31462480302434415, 0.27905118197668344, 0.36738704110030085, 0.46782337699551135, 0.4538434719434008, 0.40487696998752654, 0.37942006485536695, 0.4020701370900497, 0.479138873051852, 0.3334447320085019, 0.5305394359165803, 0.36192117305472493, 0.4307202970376238, 0.3987008520634845, 0.4305742080323398, 0.320365410996601, 0.43019407405517995, 0.34224363905377686, 0.32544718391727656, 0.37477697303984314, 0.4811468069674447, 0.4402979100123048, 0.4455852350220084, 0.43354872008785605, 0.38553798699285835, 0.5192393880570307, 0.47015014605131, 0.3088569201063365, 0.3594515809090808, 0.42402804421726614, 0.42445767391473055, 0.47107753914315253, 0.5751094999723136, 0.41341264790389687, 0.3673327799187973, 0.4106603229884058, 0.4376733241369948, 0.3529545289929956, 0.4672544860513881, 0.37569731299299747, 0.3966485959244892, 0.3430644019972533, 0.32871355686802417, 0.3802572649437934, 0.4138372589368373, 0.5452143209986389, 0.5061887999763712, 0.44770110596437007, 0.4975182939087972, 0.366916537983343, 0.2887494119349867, 0.30756892799399793, 0.2994197958614677, 0.2968245280208066, 0.30878737301100045, 0.33463607111480087, 0.30568411317653954, 0.3462332588387653, 0.28801990288775414, 0.2543677529320121, 0.3628009270178154, 0.27762646425981075, 0.2867803890258074, 0.269960323930718, 0.3943828269839287, 0.753230772097595, 0.5228706249035895, 0.29305762297008187, 0.2804474119329825, 0.36126216989941895, 0.31214738893322647, 0.6548356520943344, 0.3330433389637619, 0.270325023913756, 0.2979332549730316, 0.2918045410187915, 0.2729980220319703, 0.42393365898169577, 0.6709191949339584, 0.4957195420283824, 0.35936973500065506, 0.583476371015422, 0.4444412050070241, 0.48621627502143383, 0.565674405079335, 0.4599786679027602, 0.3547988790087402, 0.33019344694912434, 0.6620148690417409]
Total Epoch List: [65, 95, 39]
Total Time List: [0.12260988901834935, 0.059565117000602186, 0.2550843849312514]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d83f7520>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.21s
Val loss: 0.6635 score: 0.5116 time: 0.12s
Test loss: 0.6583 score: 0.6136 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7606;  Loss pred: 0.7606; Loss self: 0.0000; time: 0.29s
Val loss: 0.6595 score: 0.5814 time: 0.07s
Test loss: 0.6250 score: 0.6136 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7431;  Loss pred: 0.7431; Loss self: 0.0000; time: 0.20s
Val loss: 0.6575 score: 0.6047 time: 0.07s
Test loss: 0.6123 score: 0.7273 time: 0.17s
Epoch 4/1000, LR 0.000060
Train loss: 0.7175;  Loss pred: 0.7175; Loss self: 0.0000; time: 0.17s
Val loss: 0.6603 score: 0.5814 time: 0.19s
Test loss: 0.6123 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.7127;  Loss pred: 0.7127; Loss self: 0.0000; time: 0.18s
Val loss: 0.6667 score: 0.5581 time: 0.05s
Test loss: 0.6151 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.24s
Val loss: 0.6731 score: 0.5814 time: 0.06s
Test loss: 0.6200 score: 0.6818 time: 0.24s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6504;  Loss pred: 0.6504; Loss self: 0.0000; time: 0.25s
Val loss: 0.6809 score: 0.5814 time: 0.08s
Test loss: 0.6259 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.22s
Val loss: 0.6847 score: 0.5581 time: 0.07s
Test loss: 0.6287 score: 0.6818 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6571;  Loss pred: 0.6571; Loss self: 0.0000; time: 0.16s
Val loss: 0.6851 score: 0.5581 time: 0.07s
Test loss: 0.6296 score: 0.6818 time: 0.19s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.15s
Val loss: 0.6874 score: 0.5581 time: 0.07s
Test loss: 0.6302 score: 0.6818 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.14s
Val loss: 0.6897 score: 0.5581 time: 0.07s
Test loss: 0.6323 score: 0.6818 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.14s
Val loss: 0.6926 score: 0.5581 time: 0.06s
Test loss: 0.6350 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.18s
Val loss: 0.6953 score: 0.5814 time: 0.07s
Test loss: 0.6374 score: 0.6818 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 0.18s
Val loss: 0.6976 score: 0.5814 time: 0.06s
Test loss: 0.6399 score: 0.6818 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5255;  Loss pred: 0.5255; Loss self: 0.0000; time: 0.15s
Val loss: 0.7001 score: 0.5581 time: 0.06s
Test loss: 0.6420 score: 0.6818 time: 0.13s
     INFO: Early stopping counter 12 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5273;  Loss pred: 0.5273; Loss self: 0.0000; time: 0.15s
Val loss: 0.7041 score: 0.5581 time: 0.07s
Test loss: 0.6457 score: 0.6591 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4923;  Loss pred: 0.4923; Loss self: 0.0000; time: 0.15s
Val loss: 0.7066 score: 0.5814 time: 0.07s
Test loss: 0.6481 score: 0.6364 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4809;  Loss pred: 0.4809; Loss self: 0.0000; time: 0.20s
Val loss: 0.7090 score: 0.6047 time: 0.12s
Test loss: 0.6507 score: 0.6364 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4629;  Loss pred: 0.4629; Loss self: 0.0000; time: 0.15s
Val loss: 0.7107 score: 0.5581 time: 0.17s
Test loss: 0.6521 score: 0.6136 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4544;  Loss pred: 0.4544; Loss self: 0.0000; time: 0.24s
Val loss: 0.7120 score: 0.5349 time: 0.06s
Test loss: 0.6536 score: 0.6136 time: 0.20s
     INFO: Early stopping counter 17 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4401;  Loss pred: 0.4401; Loss self: 0.0000; time: 0.28s
Val loss: 0.7103 score: 0.5349 time: 0.08s
Test loss: 0.6531 score: 0.5909 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.4216;  Loss pred: 0.4216; Loss self: 0.0000; time: 0.15s
Val loss: 0.7099 score: 0.5349 time: 0.16s
Test loss: 0.6542 score: 0.5455 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 0.18s
Val loss: 0.7095 score: 0.5349 time: 0.05s
Test loss: 0.6561 score: 0.5227 time: 0.11s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 002,   Train_Loss: 0.7431,   Val_Loss: 0.6575,   Val_Precision: 0.5926,   Val_Recall: 0.7273,   Val_accuracy: 0.6531,   Val_Score: 0.6047,   Val_Loss: 0.6575,   Test_Precision: 0.7083,   Test_Recall: 0.7727,   Test_accuracy: 0.7391,   Test_Score: 0.7273,   Test_loss: 0.6123


[0.06005267798900604, 0.09834341995883733, 0.17781393905170262, 0.06374445103574544, 0.06275410996749997, 0.24895757099147886, 0.06115344807039946, 0.09615509095601737, 0.19199550698976964, 0.15620928909629583, 0.11233847704716027, 0.06376605294644833, 0.08923440601211041, 0.09069704194553196, 0.13816232094541192, 0.08616836799774319, 0.09109511296264827, 0.06206856097560376, 0.06673204898834229, 0.2057109239976853, 0.09579508705064654, 0.06055415701121092, 0.11380421102512628]
[0.0013648335906592283, 0.002235077726337212, 0.004041225887538696, 0.0014487375235396692, 0.0014262297719886358, 0.0056581266134427015, 0.0013898510925090786, 0.002185342976273122, 0.004363534249767492, 0.0035502111158249054, 0.002553147205617279, 0.0014492284760556438, 0.0020280546820934183, 0.002061296407852999, 0.0031400527487593618, 0.001958371999948709, 0.0020703434764238245, 0.0014106491130819036, 0.0015166374770077791, 0.004675248272674667, 0.002177161069332876, 0.0013762308411638844, 0.0025864593414801425]
[732.6900560214011, 447.41173347862684, 247.44966696455785, 690.2561601060222, 701.149295604501, 176.7369428644771, 719.5015389704188, 457.59407601336665, 229.17202954308985, 281.673389940825, 391.67346003389895, 690.0223232720998, 493.0833516617857, 485.1315881550378, 318.4659876796978, 510.62821569456196, 483.0116410091211, 708.8935091840511, 659.3533492083625, 213.89238424934155, 459.31374306009394, 726.6222860943121, 386.62892702876746]
Elapsed: 0.10840462056575748~0.05231367202967234
Time per graph: 0.0024637413764944886~0.0011889470915834623
Speed: 487.40676764514865~182.66350834050291
Total Time: 0.1142
best val loss: 0.6575481295585632 test_score: 0.7273

Testing...
Test loss: 0.6123 score: 0.7273 time: 0.09s
test Score 0.7273
Epoch Time List: [0.38405931601300836, 0.4532472229329869, 0.43730750505346805, 0.4142737650545314, 0.2867722490336746, 0.5461238081334159, 0.39286150492262095, 0.38095744710881263, 0.41283306013792753, 0.36855892080347985, 0.32161380792967975, 0.2584205811144784, 0.3387497519142926, 0.331867488916032, 0.3350329709937796, 0.3029268509708345, 0.3042493970133364, 0.3793594790622592, 0.38424927892629057, 0.4982128470437601, 0.46056465804576874, 0.3626019798684865, 0.34691718802787364]
Total Epoch List: [23]
Total Time List: [0.1141808689571917]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d83f52a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8578;  Loss pred: 0.8578; Loss self: 0.0000; time: 0.15s
Val loss: 1.6355 score: 0.4545 time: 0.06s
Test loss: 1.5768 score: 0.4186 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.9122;  Loss pred: 0.9122; Loss self: 0.0000; time: 0.20s
Val loss: 1.3557 score: 0.4545 time: 0.14s
Test loss: 1.3157 score: 0.4419 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8299;  Loss pred: 0.8299; Loss self: 0.0000; time: 0.18s
Val loss: 1.1579 score: 0.4545 time: 0.06s
Test loss: 1.1262 score: 0.4419 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.7710;  Loss pred: 0.7710; Loss self: 0.0000; time: 0.17s
Val loss: 1.0135 score: 0.4318 time: 0.06s
Test loss: 0.9855 score: 0.4186 time: 0.10s
Epoch 5/1000, LR 0.000090
Train loss: 0.7573;  Loss pred: 0.7573; Loss self: 0.0000; time: 0.19s
Val loss: 0.8940 score: 0.3864 time: 0.07s
Test loss: 0.8724 score: 0.3488 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.8401;  Loss pred: 0.8401; Loss self: 0.0000; time: 0.29s
Val loss: 0.8093 score: 0.4545 time: 0.06s
Test loss: 0.7950 score: 0.3256 time: 0.09s
Epoch 7/1000, LR 0.000150
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.15s
Val loss: 0.7566 score: 0.3636 time: 0.06s
Test loss: 0.7474 score: 0.3721 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.15s
Val loss: 0.7171 score: 0.4773 time: 0.07s
Test loss: 0.7182 score: 0.5349 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.21s
Val loss: 0.6907 score: 0.5909 time: 0.06s
Test loss: 0.7069 score: 0.5814 time: 0.11s
Epoch 10/1000, LR 0.000240
Train loss: 0.6246;  Loss pred: 0.6246; Loss self: 0.0000; time: 0.15s
Val loss: 0.6737 score: 0.6364 time: 0.06s
Test loss: 0.7103 score: 0.6047 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 0.20s
Val loss: 0.6635 score: 0.5909 time: 0.05s
Test loss: 0.7192 score: 0.5814 time: 0.13s
Epoch 12/1000, LR 0.000270
Train loss: 0.5317;  Loss pred: 0.5317; Loss self: 0.0000; time: 0.23s
Val loss: 0.6585 score: 0.6136 time: 0.15s
Test loss: 0.7279 score: 0.5116 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.5229;  Loss pred: 0.5229; Loss self: 0.0000; time: 0.19s
Val loss: 0.6558 score: 0.6136 time: 0.05s
Test loss: 0.7377 score: 0.4884 time: 0.11s
Epoch 14/1000, LR 0.000270
Train loss: 0.4907;  Loss pred: 0.4907; Loss self: 0.0000; time: 0.18s
Val loss: 0.6539 score: 0.5909 time: 0.06s
Test loss: 0.7464 score: 0.4884 time: 0.13s
Epoch 15/1000, LR 0.000270
Train loss: 0.4685;  Loss pred: 0.4685; Loss self: 0.0000; time: 0.18s
Val loss: 0.6539 score: 0.5455 time: 0.15s
Test loss: 0.7530 score: 0.4651 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.23s
Val loss: 0.6554 score: 0.5455 time: 0.08s
Test loss: 0.7566 score: 0.4419 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4487;  Loss pred: 0.4487; Loss self: 0.0000; time: 0.37s
Val loss: 0.6571 score: 0.5682 time: 0.05s
Test loss: 0.7590 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4508;  Loss pred: 0.4508; Loss self: 0.0000; time: 0.30s
Val loss: 0.6584 score: 0.5682 time: 0.06s
Test loss: 0.7592 score: 0.4419 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4225;  Loss pred: 0.4225; Loss self: 0.0000; time: 0.21s
Val loss: 0.6587 score: 0.5909 time: 0.05s
Test loss: 0.7590 score: 0.4651 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4029;  Loss pred: 0.4029; Loss self: 0.0000; time: 0.20s
Val loss: 0.6595 score: 0.5682 time: 0.06s
Test loss: 0.7594 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3939;  Loss pred: 0.3939; Loss self: 0.0000; time: 0.31s
Val loss: 0.6608 score: 0.5682 time: 0.05s
Test loss: 0.7604 score: 0.4186 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3406;  Loss pred: 0.3406; Loss self: 0.0000; time: 0.23s
Val loss: 0.6617 score: 0.5455 time: 0.05s
Test loss: 0.7610 score: 0.4186 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3369;  Loss pred: 0.3369; Loss self: 0.0000; time: 0.24s
Val loss: 0.6617 score: 0.5455 time: 0.05s
Test loss: 0.7611 score: 0.4186 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3246;  Loss pred: 0.3246; Loss self: 0.0000; time: 0.22s
Val loss: 0.6611 score: 0.5455 time: 0.09s
Test loss: 0.7618 score: 0.4651 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3032;  Loss pred: 0.3032; Loss self: 0.0000; time: 0.20s
Val loss: 0.6613 score: 0.5455 time: 0.06s
Test loss: 0.7635 score: 0.4419 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3009;  Loss pred: 0.3009; Loss self: 0.0000; time: 0.20s
Val loss: 0.6621 score: 0.5455 time: 0.32s
Test loss: 0.7650 score: 0.4419 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2871;  Loss pred: 0.2871; Loss self: 0.0000; time: 0.16s
Val loss: 0.6628 score: 0.5455 time: 0.07s
Test loss: 0.7658 score: 0.4419 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2563;  Loss pred: 0.2563; Loss self: 0.0000; time: 0.17s
Val loss: 0.6618 score: 0.5909 time: 0.06s
Test loss: 0.7653 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2691;  Loss pred: 0.2691; Loss self: 0.0000; time: 0.16s
Val loss: 0.6599 score: 0.6136 time: 0.05s
Test loss: 0.7633 score: 0.4419 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2482;  Loss pred: 0.2482; Loss self: 0.0000; time: 0.33s
Val loss: 0.6578 score: 0.5909 time: 0.08s
Test loss: 0.7587 score: 0.4419 time: 0.11s
     INFO: Early stopping counter 16 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2372;  Loss pred: 0.2372; Loss self: 0.0000; time: 0.19s
Val loss: 0.6542 score: 0.6136 time: 0.13s
Test loss: 0.7527 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2125;  Loss pred: 0.2125; Loss self: 0.0000; time: 0.16s
Val loss: 0.6494 score: 0.6364 time: 0.05s
Test loss: 0.7450 score: 0.4419 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.17s
Val loss: 0.6447 score: 0.6591 time: 0.05s
Test loss: 0.7379 score: 0.4651 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.1796;  Loss pred: 0.1796; Loss self: 0.0000; time: 0.16s
Val loss: 0.6411 score: 0.6591 time: 0.05s
Test loss: 0.7290 score: 0.4651 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.1851;  Loss pred: 0.1851; Loss self: 0.0000; time: 0.16s
Val loss: 0.6393 score: 0.6818 time: 0.06s
Test loss: 0.7182 score: 0.4884 time: 0.08s
Epoch 36/1000, LR 0.000270
Train loss: 0.1655;  Loss pred: 0.1655; Loss self: 0.0000; time: 0.17s
Val loss: 0.6366 score: 0.7045 time: 0.05s
Test loss: 0.7078 score: 0.5116 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.1590;  Loss pred: 0.1590; Loss self: 0.0000; time: 0.17s
Val loss: 0.6326 score: 0.6818 time: 0.08s
Test loss: 0.6988 score: 0.5116 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.16s
Val loss: 0.6280 score: 0.6591 time: 0.05s
Test loss: 0.6849 score: 0.5116 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.1548;  Loss pred: 0.1548; Loss self: 0.0000; time: 0.16s
Val loss: 0.6225 score: 0.6818 time: 0.05s
Test loss: 0.6699 score: 0.4651 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.1271;  Loss pred: 0.1271; Loss self: 0.0000; time: 0.18s
Val loss: 0.6172 score: 0.6591 time: 0.05s
Test loss: 0.6549 score: 0.4884 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.1175;  Loss pred: 0.1175; Loss self: 0.0000; time: 0.16s
Val loss: 0.6124 score: 0.7045 time: 0.05s
Test loss: 0.6406 score: 0.5581 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.1085;  Loss pred: 0.1085; Loss self: 0.0000; time: 0.18s
Val loss: 0.6061 score: 0.7045 time: 0.05s
Test loss: 0.6269 score: 0.6279 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.1014;  Loss pred: 0.1014; Loss self: 0.0000; time: 0.16s
Val loss: 0.5992 score: 0.7500 time: 0.05s
Test loss: 0.6145 score: 0.6744 time: 0.14s
Epoch 44/1000, LR 0.000269
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.18s
Val loss: 0.5912 score: 0.7727 time: 0.05s
Test loss: 0.6040 score: 0.6512 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0957;  Loss pred: 0.0957; Loss self: 0.0000; time: 0.16s
Val loss: 0.5831 score: 0.7727 time: 0.05s
Test loss: 0.5945 score: 0.7209 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0839;  Loss pred: 0.0839; Loss self: 0.0000; time: 0.16s
Val loss: 0.5730 score: 0.7727 time: 0.06s
Test loss: 0.5884 score: 0.7209 time: 0.18s
Epoch 47/1000, LR 0.000269
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 0.17s
Val loss: 0.5630 score: 0.7955 time: 0.05s
Test loss: 0.5845 score: 0.6977 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0779;  Loss pred: 0.0779; Loss self: 0.0000; time: 0.19s
Val loss: 0.5535 score: 0.7955 time: 0.06s
Test loss: 0.5800 score: 0.6977 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0713;  Loss pred: 0.0713; Loss self: 0.0000; time: 0.17s
Val loss: 0.5448 score: 0.7955 time: 0.10s
Test loss: 0.5758 score: 0.6977 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.25s
Val loss: 0.5381 score: 0.7955 time: 0.06s
Test loss: 0.5666 score: 0.7442 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.0634;  Loss pred: 0.0634; Loss self: 0.0000; time: 0.16s
Val loss: 0.5344 score: 0.7955 time: 0.05s
Test loss: 0.5584 score: 0.7907 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 0.15s
Val loss: 0.5292 score: 0.7955 time: 0.05s
Test loss: 0.5519 score: 0.7907 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0508;  Loss pred: 0.0508; Loss self: 0.0000; time: 0.16s
Val loss: 0.5213 score: 0.7727 time: 0.05s
Test loss: 0.5473 score: 0.7907 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.16s
Val loss: 0.5081 score: 0.8182 time: 0.06s
Test loss: 0.5494 score: 0.7442 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.18s
Val loss: 0.4963 score: 0.8409 time: 0.24s
Test loss: 0.5542 score: 0.6977 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.25s
Val loss: 0.4853 score: 0.8409 time: 0.06s
Test loss: 0.5615 score: 0.6977 time: 0.06s
Epoch 57/1000, LR 0.000269
Train loss: 0.0385;  Loss pred: 0.0385; Loss self: 0.0000; time: 0.21s
Val loss: 0.4776 score: 0.7955 time: 0.05s
Test loss: 0.5730 score: 0.7209 time: 0.11s
Epoch 58/1000, LR 0.000269
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.21s
Val loss: 0.4717 score: 0.8182 time: 0.16s
Test loss: 0.5935 score: 0.7209 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.31s
Val loss: 0.4674 score: 0.8636 time: 0.05s
Test loss: 0.6134 score: 0.7209 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.29s
Val loss: 0.4643 score: 0.8636 time: 0.05s
Test loss: 0.6247 score: 0.7209 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.28s
Val loss: 0.4624 score: 0.8636 time: 0.14s
Test loss: 0.6292 score: 0.7209 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.19s
Val loss: 0.4576 score: 0.8409 time: 0.05s
Test loss: 0.6232 score: 0.7209 time: 0.11s
Epoch 63/1000, LR 0.000268
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.21s
Val loss: 0.4507 score: 0.8409 time: 0.10s
Test loss: 0.6064 score: 0.7209 time: 0.08s
Epoch 64/1000, LR 0.000268
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.21s
Val loss: 0.4401 score: 0.8636 time: 0.15s
Test loss: 0.5831 score: 0.7209 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0189;  Loss pred: 0.0189; Loss self: 0.0000; time: 0.20s
Val loss: 0.4325 score: 0.8182 time: 0.05s
Test loss: 0.5607 score: 0.7209 time: 0.12s
Epoch 66/1000, LR 0.000268
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.18s
Val loss: 0.4243 score: 0.8409 time: 0.23s
Test loss: 0.5351 score: 0.7209 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.30s
Val loss: 0.4193 score: 0.8409 time: 0.05s
Test loss: 0.5103 score: 0.6977 time: 0.10s
Epoch 68/1000, LR 0.000268
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.18s
Val loss: 0.4173 score: 0.8409 time: 0.05s
Test loss: 0.4895 score: 0.7442 time: 0.12s
Epoch 69/1000, LR 0.000268
Train loss: 0.0172;  Loss pred: 0.0172; Loss self: 0.0000; time: 0.21s
Val loss: 0.4145 score: 0.8409 time: 0.06s
Test loss: 0.4742 score: 0.7674 time: 0.10s
Epoch 70/1000, LR 0.000268
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.19s
Val loss: 0.4067 score: 0.8409 time: 0.07s
Test loss: 0.4656 score: 0.8140 time: 0.13s
Epoch 71/1000, LR 0.000268
Train loss: 0.0155;  Loss pred: 0.0155; Loss self: 0.0000; time: 0.16s
Val loss: 0.3948 score: 0.8636 time: 0.06s
Test loss: 0.4636 score: 0.8140 time: 0.09s
Epoch 72/1000, LR 0.000267
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.22s
Val loss: 0.3825 score: 0.8864 time: 0.06s
Test loss: 0.4644 score: 0.8140 time: 0.09s
Epoch 73/1000, LR 0.000267
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.28s
Val loss: 0.3726 score: 0.8864 time: 0.24s
Test loss: 0.4701 score: 0.7674 time: 0.12s
Epoch 74/1000, LR 0.000267
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.24s
Val loss: 0.3627 score: 0.8864 time: 0.12s
Test loss: 0.4841 score: 0.7442 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0119;  Loss pred: 0.0119; Loss self: 0.0000; time: 0.18s
Val loss: 0.3562 score: 0.8864 time: 0.17s
Test loss: 0.4959 score: 0.7442 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.17s
Val loss: 0.3540 score: 0.8864 time: 0.14s
Test loss: 0.5061 score: 0.7442 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.19s
Val loss: 0.3536 score: 0.8864 time: 0.05s
Test loss: 0.5140 score: 0.7674 time: 0.11s
Epoch 78/1000, LR 0.000267
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.20s
Val loss: 0.3527 score: 0.8864 time: 0.06s
Test loss: 0.5161 score: 0.7674 time: 0.17s
Epoch 79/1000, LR 0.000267
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.19s
Val loss: 0.3503 score: 0.8864 time: 0.13s
Test loss: 0.5077 score: 0.7674 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.20s
Val loss: 0.3503 score: 0.8864 time: 0.06s
Test loss: 0.5036 score: 0.7674 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.20s
Val loss: 0.3516 score: 0.8864 time: 0.11s
Test loss: 0.4999 score: 0.7674 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.17s
Val loss: 0.3516 score: 0.8636 time: 0.14s
Test loss: 0.4946 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.20s
Val loss: 0.3512 score: 0.8636 time: 0.05s
Test loss: 0.4868 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.20s
Val loss: 0.3520 score: 0.8636 time: 0.23s
Test loss: 0.4813 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.17s
Val loss: 0.3557 score: 0.8636 time: 0.18s
Test loss: 0.4873 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.38s
Val loss: 0.3651 score: 0.8636 time: 0.08s
Test loss: 0.5044 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 7 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.20s
Val loss: 0.3698 score: 0.8636 time: 0.08s
Test loss: 0.5072 score: 0.7907 time: 0.15s
     INFO: Early stopping counter 8 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.18s
Val loss: 0.3789 score: 0.8636 time: 0.12s
Test loss: 0.5169 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.18s
Val loss: 0.3893 score: 0.8636 time: 0.06s
Test loss: 0.5247 score: 0.7907 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.21s
Val loss: 0.4027 score: 0.8636 time: 0.12s
Test loss: 0.5391 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.15s
Val loss: 0.4158 score: 0.8409 time: 0.15s
Test loss: 0.5522 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.28s
Val loss: 0.4268 score: 0.8409 time: 0.05s
Test loss: 0.5621 score: 0.7674 time: 0.12s
     INFO: Early stopping counter 13 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.28s
Val loss: 0.4310 score: 0.8409 time: 0.25s
Test loss: 0.5556 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.20s
Val loss: 0.4333 score: 0.8409 time: 0.20s
Test loss: 0.5435 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.22s
Val loss: 0.4292 score: 0.8409 time: 0.05s
Test loss: 0.5200 score: 0.7674 time: 0.11s
     INFO: Early stopping counter 16 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.4246 score: 0.8409 time: 0.16s
Test loss: 0.4910 score: 0.7674 time: 0.14s
     INFO: Early stopping counter 17 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.20s
Val loss: 0.4199 score: 0.8409 time: 0.33s
Test loss: 0.4657 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.39s
Val loss: 0.4172 score: 0.8409 time: 0.05s
Test loss: 0.4527 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 19 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.23s
Val loss: 0.4252 score: 0.8409 time: 0.06s
Test loss: 0.4581 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 078,   Train_Loss: 0.0096,   Val_Loss: 0.3503,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.3503,   Test_Precision: 0.8750,   Test_Recall: 0.6364,   Test_accuracy: 0.7368,   Test_Score: 0.7674,   Test_loss: 0.5077


[0.06005267798900604, 0.09834341995883733, 0.17781393905170262, 0.06374445103574544, 0.06275410996749997, 0.24895757099147886, 0.06115344807039946, 0.09615509095601737, 0.19199550698976964, 0.15620928909629583, 0.11233847704716027, 0.06376605294644833, 0.08923440601211041, 0.09069704194553196, 0.13816232094541192, 0.08616836799774319, 0.09109511296264827, 0.06206856097560376, 0.06673204898834229, 0.2057109239976853, 0.09579508705064654, 0.06055415701121092, 0.11380421102512628, 0.09205881494563073, 0.07671797194052488, 0.25519783899653703, 0.10826397989876568, 0.05360011092852801, 0.09036490204744041, 0.07968860096298158, 0.09472390508744866, 0.11903483292553574, 0.09362223406787962, 0.13962827099021524, 0.0658606180222705, 0.11818586604204029, 0.13927448901813477, 0.07099492405541241, 0.09029254596680403, 0.05680081306491047, 0.08770260005258024, 0.11040270794183016, 0.05477833200711757, 0.08447113900911063, 0.12198886589612812, 0.13114146597217768, 0.08962076506577432, 0.09481532801873982, 0.07317019603215158, 0.06549749104306102, 0.054735807003453374, 0.06012811197433621, 0.11722048197407275, 0.05655938701238483, 0.06209383497480303, 0.05753482296131551, 0.06356523197609931, 0.08079032099340111, 0.05544796597678214, 0.05505048704799265, 0.06226368504576385, 0.05756773194298148, 0.07866542902775109, 0.05696414492558688, 0.054546685074456036, 0.14305237191729248, 0.054781348910182714, 0.055953043047338724, 0.18094022397417575, 0.05642286501824856, 0.05748243804555386, 0.06810866203159094, 0.06952822301536798, 0.05416638299357146, 0.0613812260562554, 0.056606662925332785, 0.07273776002693921, 0.054564406047575176, 0.0634526340290904, 0.11089260701555759, 0.05105238000396639, 0.08738046407233924, 0.09970407199580222, 0.07341677998192608, 0.11839745100587606, 0.084429448004812, 0.0646906370529905, 0.12264199904166162, 0.05834990704897791, 0.10480706603266299, 0.1260985389817506, 0.10522209189366549, 0.1350719059119001, 0.09694166295230389, 0.09966535307466984, 0.12071660405490547, 0.05666685092728585, 0.08763850294053555, 0.06233946303837001, 0.11587612598668784, 0.17301847599446774, 0.059876671992242336, 0.09616210695821792, 0.09543927607592195, 0.062015105970203876, 0.11620519298594445, 0.05454657506197691, 0.06513163796626031, 0.11425677000079304, 0.15002427902072668, 0.11583051295019686, 0.08318720106035471, 0.09176208905410022, 0.05441481899470091, 0.12735954800155014, 0.0705936640733853, 0.05635477602481842, 0.11640573502518237, 0.13953578390646726, 0.05645883595570922, 0.1181001040386036, 0.12431194901000708]
[0.0013648335906592283, 0.002235077726337212, 0.004041225887538696, 0.0014487375235396692, 0.0014262297719886358, 0.0056581266134427015, 0.0013898510925090786, 0.002185342976273122, 0.004363534249767492, 0.0035502111158249054, 0.002553147205617279, 0.0014492284760556438, 0.0020280546820934183, 0.002061296407852999, 0.0031400527487593618, 0.001958371999948709, 0.0020703434764238245, 0.0014106491130819036, 0.0015166374770077791, 0.004675248272674667, 0.002177161069332876, 0.0013762308411638844, 0.0025864593414801425, 0.002140902673154203, 0.001784138882337788, 0.005934833465035745, 0.0025177669743898998, 0.0012465142076401863, 0.0021015093499404747, 0.0018532232782088739, 0.0022028815136615966, 0.002768251928500831, 0.0021772612573925493, 0.0032471690927957032, 0.001531642279587686, 0.002748508512605588, 0.003238941605072902, 0.0016510447454747073, 0.0020998266503907915, 0.0013209491410444295, 0.0020395953500600057, 0.0025675048358565155, 0.0012739146978399435, 0.001964445093235131, 0.002836950369677398, 0.0030498015342366905, 0.002084203838738938, 0.0022050076283427864, 0.0017016324658639902, 0.001523197466117698, 0.0012729257442663576, 0.0013983281854496792, 0.0027260577203272736, 0.0013153345816833682, 0.0014440426738326286, 0.0013380191386352445, 0.0014782612087464956, 0.0018788446742651422, 0.0012894875808553987, 0.0012802438848370382, 0.0014479926754828803, 0.001338784463790267, 0.001829428582040723, 0.0013247475564089973, 0.0012685275598710707, 0.0033267993469137786, 0.0012739848583763422, 0.0013012335592404354, 0.004207912185445948, 0.0013121596515871757, 0.0013368008847803223, 0.0015839223728276962, 0.0016169354189620462, 0.0012596833254318945, 0.0014274703734012883, 0.001316434021519367, 0.0016915758145799817, 0.0012689396755250042, 0.0014756426518393118, 0.002578897837571107, 0.0011872646512550323, 0.0020321038156357963, 0.0023186993487395867, 0.0017073669763238623, 0.0027534290931599086, 0.0019634755349956278, 0.0015044334198369882, 0.002852139512596782, 0.00135697458253437, 0.002437373628666581, 0.0029325241623662933, 0.0024470253928759417, 0.003141207114230235, 0.0022544572779605557, 0.002317798908713252, 0.002807362884997802, 0.0013178337424950197, 0.002038104719547338, 0.0014497549543806979, 0.0026947936275973916, 0.004023685488243436, 0.0013924807440056357, 0.0022363280687957656, 0.0022195180482772548, 0.0014422117667489273, 0.002702446348510336, 0.0012685250014413236, 0.0015146892550293096, 0.0026571341860649545, 0.0034889367214122483, 0.0026937328593069037, 0.0019345860711710398, 0.0021340020710255863, 0.0012654609068535095, 0.0029618499535244217, 0.0016417131179857047, 0.001310576186623684, 0.0027071101168647063, 0.0032450182303829594, 0.0013129961850164935, 0.002746514047409386, 0.0028909755583722578]
[732.6900560214011, 447.41173347862684, 247.44966696455785, 690.2561601060222, 701.149295604501, 176.7369428644771, 719.5015389704188, 457.59407601336665, 229.17202954308985, 281.673389940825, 391.67346003389895, 690.0223232720998, 493.0833516617857, 485.1315881550378, 318.4659876796978, 510.62821569456196, 483.0116410091211, 708.8935091840511, 659.3533492083625, 213.89238424934155, 459.31374306009394, 726.6222860943121, 386.62892702876746, 467.09269531000905, 560.4944827443494, 168.4967246160086, 397.1773441195121, 802.2371456905655, 475.8484657840447, 539.6003880150331, 453.9508792453458, 361.2387982843592, 459.29260744646825, 307.960556233009, 652.8939644243807, 363.8336921329013, 308.7428308166402, 605.6771039918004, 476.2297877369513, 757.0314169774424, 490.29333194478, 389.48320020063437, 784.9819157402025, 509.0496056334962, 352.4911858481734, 327.89018851690025, 479.79951932391464, 453.5131702703306, 587.6709689434952, 656.5136971694053, 785.5917790211271, 715.1397006836536, 366.8300904061365, 760.2628364869703, 692.5003104969907, 747.3734650910764, 676.4704330217517, 532.241974920637, 775.5018465060647, 781.1011728654261, 690.6112281724888, 746.9462240164294, 546.618769279587, 754.8607998271816, 788.315549172331, 300.58921375215635, 784.9386854365537, 768.5015444757869, 237.6475448938157, 762.1023850187814, 748.0545617415048, 631.3440716256509, 618.4538901633601, 793.8503112733833, 700.5399331807228, 759.6278914501513, 591.1647538235228, 788.0595266171857, 677.670843109307, 387.7625493462098, 842.2721917501049, 492.10084263688276, 431.2762672502523, 585.6971663778477, 363.18349453203945, 509.30097277846994, 664.7020644545069, 350.6139848991931, 736.9334789840631, 410.27768095902144, 341.0031579051293, 408.65942908124845, 318.3489542825176, 443.56573521083874, 431.44381345625857, 356.2061767446865, 758.8210619851989, 490.6519230386255, 689.7717417542311, 371.08593020222264, 248.52837104734945, 718.142785316644, 447.1615832906338, 450.5482623924504, 693.379448882342, 370.0350982180379, 788.3171390897144, 660.2014219613975, 376.34531415251405, 286.62027426946884, 371.2320605753384, 516.9064405569104, 468.60310661245376, 790.2259126174339, 337.62682637250435, 609.119820658403, 763.0231727132226, 369.3976073489652, 308.1646785947287, 761.6168359144458, 364.09790109875354, 345.9039966989699]
Elapsed: 0.09259433836149449~0.039284077285384265
Time per graph: 0.002142554942516766~0.0009025818001913522
Speed: 535.6690929142667~179.25543879969044
Total Time: 0.1250
best val loss: 0.3502638041973114 test_score: 0.7674

Testing...
Test loss: 0.4644 score: 0.8140 time: 0.12s
test Score 0.8140
Epoch Time List: [0.38405931601300836, 0.4532472229329869, 0.43730750505346805, 0.4142737650545314, 0.2867722490336746, 0.5461238081334159, 0.39286150492262095, 0.38095744710881263, 0.41283306013792753, 0.36855892080347985, 0.32161380792967975, 0.2584205811144784, 0.3387497519142926, 0.331867488916032, 0.3350329709937796, 0.3029268509708345, 0.3042493970133364, 0.3793594790622592, 0.38424927892629057, 0.4982128470437601, 0.46056465804576874, 0.3626019798684865, 0.34691718802787364, 0.30504643300082535, 0.40601907507516444, 0.4929779340745881, 0.332930939970538, 0.31115156097803265, 0.4384310218738392, 0.287150812917389, 0.3169685780303553, 0.3914646990597248, 0.3027127201203257, 0.3823989639058709, 0.44508058892097324, 0.352671192958951, 0.37931668490637094, 0.3904419739264995, 0.3992563111241907, 0.47390871890820563, 0.4473526959773153, 0.36746215203311294, 0.3042053319513798, 0.43760637985542417, 0.3992423058953136, 0.4136696219211444, 0.38833095179870725, 0.34781899699009955, 0.5901568591361865, 0.29481745685916394, 0.28165476804133505, 0.2711167059605941, 0.5220932370284572, 0.3743679510662332, 0.2675560798961669, 0.27409036201424897, 0.26714549504686147, 0.29230794205795974, 0.2717406530864537, 0.29987047181930393, 0.26886543398723006, 0.2658355610910803, 0.30233274796046317, 0.26578368397895247, 0.2864411890041083, 0.3476528689498082, 0.28683401888702065, 0.262478073942475, 0.39379186392761767, 0.27532112691551447, 0.3028584960848093, 0.33258497586939484, 0.3731036368990317, 0.2608722730074078, 0.259700943948701, 0.26333679910749197, 0.28662291914224625, 0.471891111927107, 0.36317244707606733, 0.36153769807424396, 0.4203811491606757, 0.4430512059479952, 0.43481054494623095, 0.48803975293412805, 0.35846959811169654, 0.38586751697584987, 0.4194201290374622, 0.36922870005946606, 0.4593671830371022, 0.4546385460998863, 0.3514567611273378, 0.36703482107259333, 0.39201955404132605, 0.3153774128295481, 0.36837235011626035, 0.6329558949219063, 0.411482171039097, 0.439166750991717, 0.3647835870506242, 0.35477323015220463, 0.42557965498417616, 0.3701065859058872, 0.35135946585796773, 0.4030566430883482, 0.3676572209224105, 0.3614227749640122, 0.4822019509738311, 0.41260434896685183, 0.5685432779137045, 0.4258463999722153, 0.419287362950854, 0.3231190770165995, 0.4195492248982191, 0.3558101119706407, 0.4548873120220378, 0.5986276470357552, 0.4543629849795252, 0.3825905618723482, 0.48951610713265836, 0.5833981268806383, 0.5618587909266353, 0.4123559969011694]
Total Epoch List: [23, 99]
Total Time List: [0.1141808689571917, 0.12501757103018463]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d83f7730>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7096;  Loss pred: 0.7096; Loss self: 0.0000; time: 0.29s
Val loss: 0.7797 score: 0.4545 time: 0.14s
Test loss: 0.7240 score: 0.4419 time: 0.28s
Epoch 2/1000, LR 0.000000
Train loss: 0.7372;  Loss pred: 0.7372; Loss self: 0.0000; time: 0.33s
Val loss: 0.7625 score: 0.4773 time: 0.10s
Test loss: 0.7204 score: 0.4419 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7491;  Loss pred: 0.7491; Loss self: 0.0000; time: 0.24s
Val loss: 0.7501 score: 0.4773 time: 0.05s
Test loss: 0.7188 score: 0.4419 time: 0.13s
Epoch 4/1000, LR 0.000060
Train loss: 0.7366;  Loss pred: 0.7366; Loss self: 0.0000; time: 0.27s
Val loss: 0.7432 score: 0.5000 time: 0.05s
Test loss: 0.7178 score: 0.4419 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.32s
Val loss: 0.7397 score: 0.4773 time: 0.07s
Test loss: 0.7139 score: 0.4651 time: 0.13s
Epoch 6/1000, LR 0.000120
Train loss: 0.7161;  Loss pred: 0.7161; Loss self: 0.0000; time: 0.17s
Val loss: 0.7314 score: 0.4545 time: 0.08s
Test loss: 0.7083 score: 0.4419 time: 0.10s
Epoch 7/1000, LR 0.000150
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.47s
Val loss: 0.7206 score: 0.5000 time: 0.06s
Test loss: 0.7025 score: 0.4884 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.55s
Val loss: 0.7123 score: 0.5909 time: 0.10s
Test loss: 0.6965 score: 0.4884 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6013;  Loss pred: 0.6013; Loss self: 0.0000; time: 0.22s
Val loss: 0.7054 score: 0.5682 time: 0.11s
Test loss: 0.6910 score: 0.5349 time: 0.14s
Epoch 10/1000, LR 0.000240
Train loss: 0.6119;  Loss pred: 0.6119; Loss self: 0.0000; time: 0.27s
Val loss: 0.6970 score: 0.4773 time: 0.20s
Test loss: 0.6867 score: 0.5581 time: 0.13s
Epoch 11/1000, LR 0.000270
Train loss: 0.5844;  Loss pred: 0.5844; Loss self: 0.0000; time: 0.65s
Val loss: 0.6833 score: 0.5227 time: 0.08s
Test loss: 0.6827 score: 0.5581 time: 0.12s
Epoch 12/1000, LR 0.000270
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.33s
Val loss: 0.6726 score: 0.5909 time: 0.63s
Test loss: 0.6790 score: 0.5581 time: 0.15s
Epoch 13/1000, LR 0.000270
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.21s
Val loss: 0.6688 score: 0.5682 time: 0.17s
Test loss: 0.6754 score: 0.5814 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.25s
Val loss: 0.6670 score: 0.5909 time: 0.08s
Test loss: 0.6723 score: 0.5581 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 0.39s
Val loss: 0.6652 score: 0.5682 time: 0.07s
Test loss: 0.6675 score: 0.5581 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5281;  Loss pred: 0.5281; Loss self: 0.0000; time: 0.69s
Val loss: 0.6672 score: 0.5909 time: 0.30s
Test loss: 0.6626 score: 0.5814 time: 0.36s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.5180;  Loss pred: 0.5180; Loss self: 0.0000; time: 0.27s
Val loss: 0.6685 score: 0.5909 time: 0.89s
Test loss: 0.6567 score: 0.6744 time: 0.34s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4848;  Loss pred: 0.4848; Loss self: 0.0000; time: 0.55s
Val loss: 0.6707 score: 0.6136 time: 0.08s
Test loss: 0.6485 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4654;  Loss pred: 0.4654; Loss self: 0.0000; time: 0.25s
Val loss: 0.6796 score: 0.5909 time: 0.05s
Test loss: 0.6457 score: 0.6512 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4172;  Loss pred: 0.4172; Loss self: 0.0000; time: 0.40s
Val loss: 0.6930 score: 0.5909 time: 0.06s
Test loss: 0.6469 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4138;  Loss pred: 0.4138; Loss self: 0.0000; time: 0.18s
Val loss: 0.7110 score: 0.6136 time: 0.06s
Test loss: 0.6483 score: 0.6977 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3885;  Loss pred: 0.3885; Loss self: 0.0000; time: 0.17s
Val loss: 0.7272 score: 0.6136 time: 0.11s
Test loss: 0.6492 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3792;  Loss pred: 0.3792; Loss self: 0.0000; time: 0.16s
Val loss: 0.7418 score: 0.6364 time: 0.08s
Test loss: 0.6498 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3606;  Loss pred: 0.3606; Loss self: 0.0000; time: 0.24s
Val loss: 0.7458 score: 0.6364 time: 0.10s
Test loss: 0.6502 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3366;  Loss pred: 0.3366; Loss self: 0.0000; time: 0.19s
Val loss: 0.7452 score: 0.6364 time: 0.05s
Test loss: 0.6470 score: 0.6744 time: 0.18s
     INFO: Early stopping counter 10 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3100;  Loss pred: 0.3100; Loss self: 0.0000; time: 0.16s
Val loss: 0.7525 score: 0.6136 time: 0.06s
Test loss: 0.6454 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2877;  Loss pred: 0.2877; Loss self: 0.0000; time: 0.16s
Val loss: 0.7651 score: 0.6136 time: 0.06s
Test loss: 0.6463 score: 0.6744 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2726;  Loss pred: 0.2726; Loss self: 0.0000; time: 0.18s
Val loss: 0.7792 score: 0.6136 time: 0.17s
Test loss: 0.6483 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2546;  Loss pred: 0.2546; Loss self: 0.0000; time: 0.36s
Val loss: 0.7936 score: 0.6136 time: 0.06s
Test loss: 0.6510 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2358;  Loss pred: 0.2358; Loss self: 0.0000; time: 0.19s
Val loss: 0.8218 score: 0.6136 time: 0.05s
Test loss: 0.6590 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2193;  Loss pred: 0.2193; Loss self: 0.0000; time: 0.15s
Val loss: 0.8432 score: 0.6136 time: 0.08s
Test loss: 0.6653 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2212;  Loss pred: 0.2212; Loss self: 0.0000; time: 0.16s
Val loss: 0.8524 score: 0.6591 time: 0.08s
Test loss: 0.6691 score: 0.6977 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1841;  Loss pred: 0.1841; Loss self: 0.0000; time: 0.18s
Val loss: 0.8588 score: 0.6591 time: 0.10s
Test loss: 0.6706 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1964;  Loss pred: 0.1964; Loss self: 0.0000; time: 0.16s
Val loss: 0.8480 score: 0.6136 time: 0.12s
Test loss: 0.6647 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1751;  Loss pred: 0.1751; Loss self: 0.0000; time: 0.17s
Val loss: 0.8379 score: 0.6136 time: 0.07s
Test loss: 0.6560 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 014,   Train_Loss: 0.5692,   Val_Loss: 0.6652,   Val_Precision: 0.5556,   Val_Recall: 0.6818,   Val_accuracy: 0.6122,   Val_Score: 0.5682,   Val_Loss: 0.6652,   Test_Precision: 0.5333,   Test_Recall: 0.7619,   Test_accuracy: 0.6275,   Test_Score: 0.5581,   Test_loss: 0.6675


[0.06005267798900604, 0.09834341995883733, 0.17781393905170262, 0.06374445103574544, 0.06275410996749997, 0.24895757099147886, 0.06115344807039946, 0.09615509095601737, 0.19199550698976964, 0.15620928909629583, 0.11233847704716027, 0.06376605294644833, 0.08923440601211041, 0.09069704194553196, 0.13816232094541192, 0.08616836799774319, 0.09109511296264827, 0.06206856097560376, 0.06673204898834229, 0.2057109239976853, 0.09579508705064654, 0.06055415701121092, 0.11380421102512628, 0.09205881494563073, 0.07671797194052488, 0.25519783899653703, 0.10826397989876568, 0.05360011092852801, 0.09036490204744041, 0.07968860096298158, 0.09472390508744866, 0.11903483292553574, 0.09362223406787962, 0.13962827099021524, 0.0658606180222705, 0.11818586604204029, 0.13927448901813477, 0.07099492405541241, 0.09029254596680403, 0.05680081306491047, 0.08770260005258024, 0.11040270794183016, 0.05477833200711757, 0.08447113900911063, 0.12198886589612812, 0.13114146597217768, 0.08962076506577432, 0.09481532801873982, 0.07317019603215158, 0.06549749104306102, 0.054735807003453374, 0.06012811197433621, 0.11722048197407275, 0.05655938701238483, 0.06209383497480303, 0.05753482296131551, 0.06356523197609931, 0.08079032099340111, 0.05544796597678214, 0.05505048704799265, 0.06226368504576385, 0.05756773194298148, 0.07866542902775109, 0.05696414492558688, 0.054546685074456036, 0.14305237191729248, 0.054781348910182714, 0.055953043047338724, 0.18094022397417575, 0.05642286501824856, 0.05748243804555386, 0.06810866203159094, 0.06952822301536798, 0.05416638299357146, 0.0613812260562554, 0.056606662925332785, 0.07273776002693921, 0.054564406047575176, 0.0634526340290904, 0.11089260701555759, 0.05105238000396639, 0.08738046407233924, 0.09970407199580222, 0.07341677998192608, 0.11839745100587606, 0.084429448004812, 0.0646906370529905, 0.12264199904166162, 0.05834990704897791, 0.10480706603266299, 0.1260985389817506, 0.10522209189366549, 0.1350719059119001, 0.09694166295230389, 0.09966535307466984, 0.12071660405490547, 0.05666685092728585, 0.08763850294053555, 0.06233946303837001, 0.11587612598668784, 0.17301847599446774, 0.059876671992242336, 0.09616210695821792, 0.09543927607592195, 0.062015105970203876, 0.11620519298594445, 0.05454657506197691, 0.06513163796626031, 0.11425677000079304, 0.15002427902072668, 0.11583051295019686, 0.08318720106035471, 0.09176208905410022, 0.05441481899470091, 0.12735954800155014, 0.0705936640733853, 0.05635477602481842, 0.11640573502518237, 0.13953578390646726, 0.05645883595570922, 0.1181001040386036, 0.12431194901000708, 0.2808459079824388, 0.08359691593796015, 0.13312651694286615, 0.09226944600231946, 0.1305474720429629, 0.10735305701382458, 0.05934308795258403, 0.09254831704311073, 0.14721005293540657, 0.13227627100422978, 0.1257343270117417, 0.15499608300160617, 0.07919540896546096, 0.07655374798923731, 0.0907294430071488, 0.3636754920007661, 0.3514023849274963, 0.06272083392832428, 0.08518132905010134, 0.05807322298642248, 0.08215149003081024, 0.05429472296964377, 0.0554832270136103, 0.07570722699165344, 0.18095002009067684, 0.07821614993736148, 0.0996668019797653, 0.07117518305312842, 0.054179904982447624, 0.05919885903131217, 0.0534952130401507, 0.07525423704646528, 0.05703352694399655, 0.05687436100561172, 0.053836785024032]
[0.0013648335906592283, 0.002235077726337212, 0.004041225887538696, 0.0014487375235396692, 0.0014262297719886358, 0.0056581266134427015, 0.0013898510925090786, 0.002185342976273122, 0.004363534249767492, 0.0035502111158249054, 0.002553147205617279, 0.0014492284760556438, 0.0020280546820934183, 0.002061296407852999, 0.0031400527487593618, 0.001958371999948709, 0.0020703434764238245, 0.0014106491130819036, 0.0015166374770077791, 0.004675248272674667, 0.002177161069332876, 0.0013762308411638844, 0.0025864593414801425, 0.002140902673154203, 0.001784138882337788, 0.005934833465035745, 0.0025177669743898998, 0.0012465142076401863, 0.0021015093499404747, 0.0018532232782088739, 0.0022028815136615966, 0.002768251928500831, 0.0021772612573925493, 0.0032471690927957032, 0.001531642279587686, 0.002748508512605588, 0.003238941605072902, 0.0016510447454747073, 0.0020998266503907915, 0.0013209491410444295, 0.0020395953500600057, 0.0025675048358565155, 0.0012739146978399435, 0.001964445093235131, 0.002836950369677398, 0.0030498015342366905, 0.002084203838738938, 0.0022050076283427864, 0.0017016324658639902, 0.001523197466117698, 0.0012729257442663576, 0.0013983281854496792, 0.0027260577203272736, 0.0013153345816833682, 0.0014440426738326286, 0.0013380191386352445, 0.0014782612087464956, 0.0018788446742651422, 0.0012894875808553987, 0.0012802438848370382, 0.0014479926754828803, 0.001338784463790267, 0.001829428582040723, 0.0013247475564089973, 0.0012685275598710707, 0.0033267993469137786, 0.0012739848583763422, 0.0013012335592404354, 0.004207912185445948, 0.0013121596515871757, 0.0013368008847803223, 0.0015839223728276962, 0.0016169354189620462, 0.0012596833254318945, 0.0014274703734012883, 0.001316434021519367, 0.0016915758145799817, 0.0012689396755250042, 0.0014756426518393118, 0.002578897837571107, 0.0011872646512550323, 0.0020321038156357963, 0.0023186993487395867, 0.0017073669763238623, 0.0027534290931599086, 0.0019634755349956278, 0.0015044334198369882, 0.002852139512596782, 0.00135697458253437, 0.002437373628666581, 0.0029325241623662933, 0.0024470253928759417, 0.003141207114230235, 0.0022544572779605557, 0.002317798908713252, 0.002807362884997802, 0.0013178337424950197, 0.002038104719547338, 0.0014497549543806979, 0.0026947936275973916, 0.004023685488243436, 0.0013924807440056357, 0.0022363280687957656, 0.0022195180482772548, 0.0014422117667489273, 0.002702446348510336, 0.0012685250014413236, 0.0015146892550293096, 0.0026571341860649545, 0.0034889367214122483, 0.0026937328593069037, 0.0019345860711710398, 0.0021340020710255863, 0.0012654609068535095, 0.0029618499535244217, 0.0016417131179857047, 0.001310576186623684, 0.0027071101168647063, 0.0032450182303829594, 0.0013129961850164935, 0.002746514047409386, 0.0028909755583722578, 0.006531300185638112, 0.0019441143241386081, 0.0030959655102992126, 0.0021458010698213827, 0.00303598772192937, 0.0024965827212517343, 0.0013800718128507913, 0.0021522864428630403, 0.00342348960314899, 0.0030761923489355763, 0.0029240541165521327, 0.0036045600698047945, 0.0018417536968711851, 0.0017803197206799374, 0.0021099870466778792, 0.008457569581413166, 0.008172148486685961, 0.0014586240448447507, 0.0019809611407000313, 0.0013505400694516856, 0.0019104997681583776, 0.0012626679760382273, 0.0012903076049676815, 0.0017606331858524057, 0.004208140002108764, 0.0018189802311014296, 0.002317832604180588, 0.001655236815189033, 0.0012599977902894796, 0.0013767176518909806, 0.0012440747218639698, 0.0017500985359643088, 0.00132636109172085, 0.00132265955827004, 0.0012520182563728372]
[732.6900560214011, 447.41173347862684, 247.44966696455785, 690.2561601060222, 701.149295604501, 176.7369428644771, 719.5015389704188, 457.59407601336665, 229.17202954308985, 281.673389940825, 391.67346003389895, 690.0223232720998, 493.0833516617857, 485.1315881550378, 318.4659876796978, 510.62821569456196, 483.0116410091211, 708.8935091840511, 659.3533492083625, 213.89238424934155, 459.31374306009394, 726.6222860943121, 386.62892702876746, 467.09269531000905, 560.4944827443494, 168.4967246160086, 397.1773441195121, 802.2371456905655, 475.8484657840447, 539.6003880150331, 453.9508792453458, 361.2387982843592, 459.29260744646825, 307.960556233009, 652.8939644243807, 363.8336921329013, 308.7428308166402, 605.6771039918004, 476.2297877369513, 757.0314169774424, 490.29333194478, 389.48320020063437, 784.9819157402025, 509.0496056334962, 352.4911858481734, 327.89018851690025, 479.79951932391464, 453.5131702703306, 587.6709689434952, 656.5136971694053, 785.5917790211271, 715.1397006836536, 366.8300904061365, 760.2628364869703, 692.5003104969907, 747.3734650910764, 676.4704330217517, 532.241974920637, 775.5018465060647, 781.1011728654261, 690.6112281724888, 746.9462240164294, 546.618769279587, 754.8607998271816, 788.315549172331, 300.58921375215635, 784.9386854365537, 768.5015444757869, 237.6475448938157, 762.1023850187814, 748.0545617415048, 631.3440716256509, 618.4538901633601, 793.8503112733833, 700.5399331807228, 759.6278914501513, 591.1647538235228, 788.0595266171857, 677.670843109307, 387.7625493462098, 842.2721917501049, 492.10084263688276, 431.2762672502523, 585.6971663778477, 363.18349453203945, 509.30097277846994, 664.7020644545069, 350.6139848991931, 736.9334789840631, 410.27768095902144, 341.0031579051293, 408.65942908124845, 318.3489542825176, 443.56573521083874, 431.44381345625857, 356.2061767446865, 758.8210619851989, 490.6519230386255, 689.7717417542311, 371.08593020222264, 248.52837104734945, 718.142785316644, 447.1615832906338, 450.5482623924504, 693.379448882342, 370.0350982180379, 788.3171390897144, 660.2014219613975, 376.34531415251405, 286.62027426946884, 371.2320605753384, 516.9064405569104, 468.60310661245376, 790.2259126174339, 337.62682637250435, 609.119820658403, 763.0231727132226, 369.3976073489652, 308.1646785947287, 761.6168359144458, 364.09790109875354, 345.9039966989699, 153.10887136973622, 514.373042564293, 323.00101427917843, 466.02642438016886, 329.38209623736554, 400.5475130015403, 724.5999742102671, 464.6221711408301, 292.09961644988823, 325.07720147799597, 341.990934551902, 277.42636566857243, 542.9607670660978, 561.696861740138, 473.93655879284876, 118.23727731400008, 122.36684167317773, 685.5776192188272, 504.8054600640073, 740.4445248381238, 523.4232511652948, 791.9738355427531, 775.0089948706822, 567.977479940464, 237.63467933549848, 549.7585861032034, 431.4375413463152, 604.1431599536994, 793.6521855091935, 726.3653506777205, 803.8102393895778, 571.3963982313701, 753.942501964211, 756.0524503432619, 798.7103981191558]
Elapsed: 0.09625099558578983~0.050407363483826906
Time per graph: 0.002230001519080081~0.0011662440472376908
Speed: 531.2050797711585~185.6056408685331
Total Time: 0.0545
best val loss: 0.6651803851127625 test_score: 0.5581

Testing...
Test loss: 0.6691 score: 0.6977 time: 0.05s
test Score 0.6977
Epoch Time List: [0.38405931601300836, 0.4532472229329869, 0.43730750505346805, 0.4142737650545314, 0.2867722490336746, 0.5461238081334159, 0.39286150492262095, 0.38095744710881263, 0.41283306013792753, 0.36855892080347985, 0.32161380792967975, 0.2584205811144784, 0.3387497519142926, 0.331867488916032, 0.3350329709937796, 0.3029268509708345, 0.3042493970133364, 0.3793594790622592, 0.38424927892629057, 0.4982128470437601, 0.46056465804576874, 0.3626019798684865, 0.34691718802787364, 0.30504643300082535, 0.40601907507516444, 0.4929779340745881, 0.332930939970538, 0.31115156097803265, 0.4384310218738392, 0.287150812917389, 0.3169685780303553, 0.3914646990597248, 0.3027127201203257, 0.3823989639058709, 0.44508058892097324, 0.352671192958951, 0.37931668490637094, 0.3904419739264995, 0.3992563111241907, 0.47390871890820563, 0.4473526959773153, 0.36746215203311294, 0.3042053319513798, 0.43760637985542417, 0.3992423058953136, 0.4136696219211444, 0.38833095179870725, 0.34781899699009955, 0.5901568591361865, 0.29481745685916394, 0.28165476804133505, 0.2711167059605941, 0.5220932370284572, 0.3743679510662332, 0.2675560798961669, 0.27409036201424897, 0.26714549504686147, 0.29230794205795974, 0.2717406530864537, 0.29987047181930393, 0.26886543398723006, 0.2658355610910803, 0.30233274796046317, 0.26578368397895247, 0.2864411890041083, 0.3476528689498082, 0.28683401888702065, 0.262478073942475, 0.39379186392761767, 0.27532112691551447, 0.3028584960848093, 0.33258497586939484, 0.3731036368990317, 0.2608722730074078, 0.259700943948701, 0.26333679910749197, 0.28662291914224625, 0.471891111927107, 0.36317244707606733, 0.36153769807424396, 0.4203811491606757, 0.4430512059479952, 0.43481054494623095, 0.48803975293412805, 0.35846959811169654, 0.38586751697584987, 0.4194201290374622, 0.36922870005946606, 0.4593671830371022, 0.4546385460998863, 0.3514567611273378, 0.36703482107259333, 0.39201955404132605, 0.3153774128295481, 0.36837235011626035, 0.6329558949219063, 0.411482171039097, 0.439166750991717, 0.3647835870506242, 0.35477323015220463, 0.42557965498417616, 0.3701065859058872, 0.35135946585796773, 0.4030566430883482, 0.3676572209224105, 0.3614227749640122, 0.4822019509738311, 0.41260434896685183, 0.5685432779137045, 0.4258463999722153, 0.419287362950854, 0.3231190770165995, 0.4195492248982191, 0.3558101119706407, 0.4548873120220378, 0.5986276470357552, 0.4543629849795252, 0.3825905618723482, 0.48951610713265836, 0.5833981268806383, 0.5618587909266353, 0.4123559969011694, 0.7121632180642337, 0.5052569512045011, 0.4155740989372134, 0.4124662339454517, 0.5164359329501167, 0.3537593550281599, 0.5813765629427508, 0.7409081899095327, 0.46677818102762103, 0.6004265379160643, 0.847860774025321, 1.1102872979827225, 0.458735438878648, 0.399495986988768, 0.5346379830734804, 1.3479445679113269, 1.5093769760569558, 0.6871965769678354, 0.38185193203389645, 0.5129652279429138, 0.31063259195070714, 0.3339076539268717, 0.2900970350019634, 0.41552780207712203, 0.42161115701310337, 0.30054495914373547, 0.31861773901619017, 0.41368168604094535, 0.47222542704548687, 0.2971579519798979, 0.28379261889494956, 0.31690853904001415, 0.33796844095923007, 0.33308387198485434, 0.27926635288167745]
Total Epoch List: [23, 99, 35]
Total Time List: [0.1141808689571917, 0.12501757103018463, 0.05453815998043865]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d829b370>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7313;  Loss pred: 0.7313; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9495 score: 0.4884 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0829 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7472;  Loss pred: 0.7472; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8578 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9417 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.7367;  Loss pred: 0.7367; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7932 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8504 score: 0.5000 time: 0.10s
Epoch 4/1000, LR 0.000060
Train loss: 0.7125;  Loss pred: 0.7125; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7570 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8078 score: 0.5000 time: 0.16s
Epoch 5/1000, LR 0.000090
Train loss: 0.7330;  Loss pred: 0.7330; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7429 score: 0.4884 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7902 score: 0.5000 time: 0.14s
Epoch 6/1000, LR 0.000120
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7367 score: 0.4884 time: 0.07s
Test loss: 0.7706 score: 0.4773 time: 0.17s
Epoch 7/1000, LR 0.000150
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.17s
Val loss: 0.7304 score: 0.5116 time: 0.17s
Test loss: 0.7528 score: 0.4773 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.21s
Val loss: 0.7214 score: 0.5116 time: 0.06s
Test loss: 0.7402 score: 0.4545 time: 0.13s
Epoch 9/1000, LR 0.000210
Train loss: 0.6209;  Loss pred: 0.6209; Loss self: 0.0000; time: 0.29s
Val loss: 0.7143 score: 0.5116 time: 0.09s
Test loss: 0.7255 score: 0.4545 time: 0.20s
Epoch 10/1000, LR 0.000240
Train loss: 0.6034;  Loss pred: 0.6034; Loss self: 0.0000; time: 0.16s
Val loss: 0.7074 score: 0.5116 time: 0.20s
Test loss: 0.7141 score: 0.5000 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.17s
Val loss: 0.7031 score: 0.5581 time: 0.07s
Test loss: 0.7104 score: 0.4318 time: 0.11s
Epoch 12/1000, LR 0.000270
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.27s
Val loss: 0.6980 score: 0.6279 time: 0.06s
Test loss: 0.7050 score: 0.5000 time: 0.10s
Epoch 13/1000, LR 0.000270
Train loss: 0.5280;  Loss pred: 0.5280; Loss self: 0.0000; time: 0.51s
Val loss: 0.6909 score: 0.6512 time: 0.06s
Test loss: 0.6961 score: 0.5682 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.5189;  Loss pred: 0.5189; Loss self: 0.0000; time: 0.20s
Val loss: 0.6835 score: 0.6279 time: 0.19s
Test loss: 0.6928 score: 0.5455 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.5216;  Loss pred: 0.5216; Loss self: 0.0000; time: 0.16s
Val loss: 0.6785 score: 0.6279 time: 0.08s
Test loss: 0.6926 score: 0.5227 time: 0.15s
Epoch 16/1000, LR 0.000270
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.16s
Val loss: 0.6787 score: 0.6047 time: 0.07s
Test loss: 0.6992 score: 0.4318 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4682;  Loss pred: 0.4682; Loss self: 0.0000; time: 0.26s
Val loss: 0.6781 score: 0.6047 time: 0.15s
Test loss: 0.7063 score: 0.4318 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.4328;  Loss pred: 0.4328; Loss self: 0.0000; time: 0.31s
Val loss: 0.6746 score: 0.6512 time: 0.09s
Test loss: 0.7012 score: 0.5455 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4037;  Loss pred: 0.4037; Loss self: 0.0000; time: 0.22s
Val loss: 0.6725 score: 0.6512 time: 0.06s
Test loss: 0.6910 score: 0.5682 time: 0.29s
Epoch 20/1000, LR 0.000270
Train loss: 0.3884;  Loss pred: 0.3884; Loss self: 0.0000; time: 0.16s
Val loss: 0.6732 score: 0.6512 time: 0.22s
Test loss: 0.6861 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3823;  Loss pred: 0.3823; Loss self: 0.0000; time: 0.35s
Val loss: 0.6750 score: 0.5814 time: 0.10s
Test loss: 0.6836 score: 0.6364 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3430;  Loss pred: 0.3430; Loss self: 0.0000; time: 0.28s
Val loss: 0.6762 score: 0.5814 time: 0.10s
Test loss: 0.6834 score: 0.6591 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3067;  Loss pred: 0.3067; Loss self: 0.0000; time: 0.15s
Val loss: 0.6766 score: 0.6047 time: 0.20s
Test loss: 0.6814 score: 0.5682 time: 0.20s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2905;  Loss pred: 0.2905; Loss self: 0.0000; time: 0.22s
Val loss: 0.6763 score: 0.6047 time: 0.05s
Test loss: 0.6802 score: 0.6136 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2949;  Loss pred: 0.2949; Loss self: 0.0000; time: 0.22s
Val loss: 0.6753 score: 0.5581 time: 0.12s
Test loss: 0.6786 score: 0.5909 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.36s
Val loss: 0.6748 score: 0.5581 time: 0.21s
Test loss: 0.6773 score: 0.5909 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2710;  Loss pred: 0.2710; Loss self: 0.0000; time: 0.23s
Val loss: 0.6722 score: 0.5814 time: 0.06s
Test loss: 0.6793 score: 0.5909 time: 0.11s
Epoch 28/1000, LR 0.000270
Train loss: 0.2580;  Loss pred: 0.2580; Loss self: 0.0000; time: 0.20s
Val loss: 0.6697 score: 0.5581 time: 0.10s
Test loss: 0.6848 score: 0.5227 time: 0.17s
Epoch 29/1000, LR 0.000270
Train loss: 0.2311;  Loss pred: 0.2311; Loss self: 0.0000; time: 0.16s
Val loss: 0.6699 score: 0.5581 time: 0.16s
Test loss: 0.6918 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2188;  Loss pred: 0.2188; Loss self: 0.0000; time: 0.15s
Val loss: 0.6734 score: 0.5814 time: 0.06s
Test loss: 0.7013 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2125;  Loss pred: 0.2125; Loss self: 0.0000; time: 0.33s
Val loss: 0.6752 score: 0.5814 time: 0.07s
Test loss: 0.7122 score: 0.4545 time: 0.19s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1947;  Loss pred: 0.1947; Loss self: 0.0000; time: 1.11s
Val loss: 0.6798 score: 0.5814 time: 0.21s
Test loss: 0.7233 score: 0.4545 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.2065;  Loss pred: 0.2065; Loss self: 0.0000; time: 0.33s
Val loss: 0.6871 score: 0.5814 time: 0.08s
Test loss: 0.7290 score: 0.5227 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.44s
Val loss: 0.6905 score: 0.5814 time: 0.07s
Test loss: 0.7334 score: 0.5227 time: 0.23s
     INFO: Early stopping counter 6 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1807;  Loss pred: 0.1807; Loss self: 0.0000; time: 0.31s
Val loss: 0.6883 score: 0.5814 time: 0.09s
Test loss: 0.7368 score: 0.5455 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1581;  Loss pred: 0.1581; Loss self: 0.0000; time: 0.18s
Val loss: 0.6862 score: 0.5581 time: 0.05s
Test loss: 0.7407 score: 0.5682 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1681;  Loss pred: 0.1681; Loss self: 0.0000; time: 0.20s
Val loss: 0.6893 score: 0.5581 time: 0.06s
Test loss: 0.7466 score: 0.5682 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1457;  Loss pred: 0.1457; Loss self: 0.0000; time: 0.27s
Val loss: 0.6936 score: 0.5814 time: 0.09s
Test loss: 0.7540 score: 0.5909 time: 0.16s
     INFO: Early stopping counter 10 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1415;  Loss pred: 0.1415; Loss self: 0.0000; time: 0.18s
Val loss: 0.6934 score: 0.5814 time: 0.08s
Test loss: 0.7559 score: 0.5909 time: 0.13s
     INFO: Early stopping counter 11 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1455;  Loss pred: 0.1455; Loss self: 0.0000; time: 0.31s
Val loss: 0.6931 score: 0.6279 time: 0.09s
Test loss: 0.7587 score: 0.5909 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1452;  Loss pred: 0.1452; Loss self: 0.0000; time: 0.37s
Val loss: 0.6871 score: 0.6047 time: 0.06s
Test loss: 0.7546 score: 0.5909 time: 0.42s
     INFO: Early stopping counter 13 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.1428;  Loss pred: 0.1428; Loss self: 0.0000; time: 0.44s
Val loss: 0.6807 score: 0.6047 time: 0.16s
Test loss: 0.7515 score: 0.6136 time: 0.16s
     INFO: Early stopping counter 14 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.1146;  Loss pred: 0.1146; Loss self: 0.0000; time: 0.27s
Val loss: 0.6759 score: 0.6047 time: 0.23s
Test loss: 0.7511 score: 0.5909 time: 0.17s
     INFO: Early stopping counter 15 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.1300;  Loss pred: 0.1300; Loss self: 0.0000; time: 0.28s
Val loss: 0.6744 score: 0.6047 time: 0.10s
Test loss: 0.7538 score: 0.5682 time: 0.33s
     INFO: Early stopping counter 16 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.1062;  Loss pred: 0.1062; Loss self: 0.0000; time: 0.31s
Val loss: 0.6714 score: 0.6279 time: 0.36s
Test loss: 0.7544 score: 0.5682 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.1067;  Loss pred: 0.1067; Loss self: 0.0000; time: 0.20s
Val loss: 0.6650 score: 0.6512 time: 0.45s
Test loss: 0.7507 score: 0.5682 time: 0.89s
Epoch 47/1000, LR 0.000269
Train loss: 0.1048;  Loss pred: 0.1048; Loss self: 0.0000; time: 0.29s
Val loss: 0.6559 score: 0.6279 time: 0.12s
Test loss: 0.7459 score: 0.5682 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.21s
Val loss: 0.6432 score: 0.6279 time: 0.12s
Test loss: 0.7385 score: 0.5682 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.0901;  Loss pred: 0.0901; Loss self: 0.0000; time: 0.17s
Val loss: 0.6284 score: 0.5814 time: 0.18s
Test loss: 0.7287 score: 0.5682 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0931;  Loss pred: 0.0931; Loss self: 0.0000; time: 0.32s
Val loss: 0.6140 score: 0.6512 time: 0.06s
Test loss: 0.7187 score: 0.6136 time: 0.11s
Epoch 51/1000, LR 0.000269
Train loss: 0.0834;  Loss pred: 0.0834; Loss self: 0.0000; time: 0.16s
Val loss: 0.6085 score: 0.6512 time: 0.07s
Test loss: 0.7202 score: 0.6136 time: 0.15s
Epoch 52/1000, LR 0.000269
Train loss: 0.0742;  Loss pred: 0.0742; Loss self: 0.0000; time: 0.48s
Val loss: 0.6043 score: 0.6512 time: 0.05s
Test loss: 0.7240 score: 0.6364 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.32s
Val loss: 0.6009 score: 0.6512 time: 0.08s
Test loss: 0.7278 score: 0.6364 time: 0.10s
Epoch 54/1000, LR 0.000269
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.31s
Val loss: 0.5962 score: 0.6512 time: 0.07s
Test loss: 0.7260 score: 0.6591 time: 0.12s
Epoch 55/1000, LR 0.000269
Train loss: 0.0625;  Loss pred: 0.0625; Loss self: 0.0000; time: 0.23s
Val loss: 0.5904 score: 0.6279 time: 0.05s
Test loss: 0.7191 score: 0.6591 time: 0.07s
Epoch 56/1000, LR 0.000269
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.28s
Val loss: 0.5870 score: 0.6279 time: 0.07s
Test loss: 0.7157 score: 0.6591 time: 0.17s
Epoch 57/1000, LR 0.000269
Train loss: 0.0455;  Loss pred: 0.0455; Loss self: 0.0000; time: 0.30s
Val loss: 0.5856 score: 0.6047 time: 0.24s
Test loss: 0.7157 score: 0.6591 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.34s
Val loss: 0.5819 score: 0.6047 time: 0.10s
Test loss: 0.7174 score: 0.6364 time: 0.10s
Epoch 59/1000, LR 0.000268
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.38s
Val loss: 0.5718 score: 0.6512 time: 0.14s
Test loss: 0.7087 score: 0.6591 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.26s
Val loss: 0.5570 score: 0.6744 time: 0.15s
Test loss: 0.6967 score: 0.6818 time: 0.18s
Epoch 61/1000, LR 0.000268
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.35s
Val loss: 0.5410 score: 0.7209 time: 0.11s
Test loss: 0.6816 score: 0.7500 time: 0.12s
Epoch 62/1000, LR 0.000268
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.24s
Val loss: 0.5221 score: 0.7442 time: 0.08s
Test loss: 0.6514 score: 0.7273 time: 0.14s
Epoch 63/1000, LR 0.000268
Train loss: 0.0344;  Loss pred: 0.0344; Loss self: 0.0000; time: 0.21s
Val loss: 0.5054 score: 0.7674 time: 0.07s
Test loss: 0.6236 score: 0.7500 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.49s
Val loss: 0.4877 score: 0.7674 time: 0.08s
Test loss: 0.5951 score: 0.7500 time: 0.11s
Epoch 65/1000, LR 0.000268
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.21s
Val loss: 0.4689 score: 0.7907 time: 0.06s
Test loss: 0.5658 score: 0.7955 time: 0.12s
Epoch 66/1000, LR 0.000268
Train loss: 0.0255;  Loss pred: 0.0255; Loss self: 0.0000; time: 0.23s
Val loss: 0.4534 score: 0.7907 time: 0.34s
Test loss: 0.5401 score: 0.7955 time: 0.11s
Epoch 67/1000, LR 0.000268
Train loss: 0.0218;  Loss pred: 0.0218; Loss self: 0.0000; time: 0.36s
Val loss: 0.4419 score: 0.8140 time: 0.07s
Test loss: 0.5256 score: 0.7955 time: 0.20s
Epoch 68/1000, LR 0.000268
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.51s
Val loss: 0.4300 score: 0.7907 time: 0.16s
Test loss: 0.5059 score: 0.7955 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.14s
Val loss: 0.4212 score: 0.7907 time: 0.16s
Test loss: 0.4928 score: 0.8182 time: 0.13s
Epoch 70/1000, LR 0.000268
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.19s
Val loss: 0.4258 score: 0.7907 time: 0.12s
Test loss: 0.5146 score: 0.7955 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.24s
Val loss: 0.4307 score: 0.8372 time: 0.19s
Test loss: 0.5468 score: 0.7955 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0187;  Loss pred: 0.0187; Loss self: 0.0000; time: 0.20s
Val loss: 0.4472 score: 0.8372 time: 0.14s
Test loss: 0.5845 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.33s
Val loss: 0.4545 score: 0.8837 time: 0.12s
Test loss: 0.6079 score: 0.7955 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.20s
Val loss: 0.4506 score: 0.8837 time: 0.12s
Test loss: 0.6173 score: 0.7727 time: 0.25s
     INFO: Early stopping counter 5 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.16s
Val loss: 0.4427 score: 0.8837 time: 0.16s
Test loss: 0.6277 score: 0.7727 time: 0.10s
     INFO: Early stopping counter 6 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0141;  Loss pred: 0.0141; Loss self: 0.0000; time: 0.27s
Val loss: 0.4314 score: 0.8837 time: 0.07s
Test loss: 0.6275 score: 0.7955 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.36s
Val loss: 0.4151 score: 0.8837 time: 0.19s
Test loss: 0.6083 score: 0.7955 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.33s
Val loss: 0.4117 score: 0.8605 time: 0.06s
Test loss: 0.5997 score: 0.7955 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.18s
Val loss: 0.4111 score: 0.8372 time: 0.06s
Test loss: 0.5973 score: 0.7955 time: 0.09s
Epoch 80/1000, LR 0.000267
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.21s
Val loss: 0.4136 score: 0.8372 time: 0.08s
Test loss: 0.6031 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.29s
Val loss: 0.4245 score: 0.8605 time: 0.05s
Test loss: 0.6354 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.21s
Val loss: 0.4405 score: 0.8837 time: 0.05s
Test loss: 0.6729 score: 0.7955 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.19s
Val loss: 0.4653 score: 0.8837 time: 0.06s
Test loss: 0.7105 score: 0.7727 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.15s
Val loss: 0.4782 score: 0.8837 time: 0.05s
Test loss: 0.7236 score: 0.7727 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.15s
Val loss: 0.4764 score: 0.8837 time: 0.07s
Test loss: 0.7158 score: 0.7727 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.21s
Val loss: 0.4668 score: 0.8605 time: 0.05s
Test loss: 0.6902 score: 0.7955 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.17s
Val loss: 0.4580 score: 0.8605 time: 0.07s
Test loss: 0.6505 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.14s
Val loss: 0.4583 score: 0.8372 time: 0.12s
Test loss: 0.6280 score: 0.8182 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.14s
Val loss: 0.4605 score: 0.8372 time: 0.07s
Test loss: 0.6175 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.19s
Val loss: 0.4699 score: 0.8372 time: 0.05s
Test loss: 0.6232 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 11 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.29s
Val loss: 0.4898 score: 0.8372 time: 0.05s
Test loss: 0.6509 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 12 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.55s
Val loss: 0.5119 score: 0.8372 time: 0.08s
Test loss: 0.6869 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 13 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.77s
Val loss: 0.5344 score: 0.8605 time: 0.06s
Test loss: 0.7250 score: 0.8182 time: 0.22s
     INFO: Early stopping counter 14 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.5520 score: 0.8605 time: 0.17s
Test loss: 0.7744 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.28s
Val loss: 0.5720 score: 0.8837 time: 0.07s
Test loss: 0.8338 score: 0.7955 time: 0.13s
     INFO: Early stopping counter 16 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.24s
Val loss: 0.5991 score: 0.8837 time: 0.06s
Test loss: 0.8799 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.28s
Val loss: 0.6024 score: 0.8837 time: 0.09s
Test loss: 0.8712 score: 0.7955 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.26s
Val loss: 0.5878 score: 0.8605 time: 0.13s
Test loss: 0.8230 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.16s
Val loss: 0.5809 score: 0.8605 time: 0.15s
Test loss: 0.7660 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 078,   Train_Loss: 0.0092,   Val_Loss: 0.4111,   Val_Precision: 0.8571,   Val_Recall: 0.8182,   Val_accuracy: 0.8372,   Val_Score: 0.8372,   Val_Loss: 0.4111,   Test_Precision: 0.7600,   Test_Recall: 0.8636,   Test_accuracy: 0.8085,   Test_Score: 0.7955,   Test_loss: 0.5973


[0.06469048408325762, 0.06694789603352547, 0.10453739808872342, 0.16405232693068683, 0.14759672596119344, 0.17779956897720695, 0.08205236901994795, 0.13865196704864502, 0.2036738310707733, 0.05887714703567326, 0.11812300293240696, 0.10409378097392619, 0.0648948730668053, 0.07542767701670527, 0.1505339479772374, 0.10275276401080191, 0.07353987207170576, 0.0991678669815883, 0.2966999029740691, 0.05886252410709858, 0.14276397705543786, 0.08856840303633362, 0.20828626200091094, 0.17299460200592875, 0.07768872706219554, 0.06320747802965343, 0.11495336703956127, 0.17629632796160877, 0.0626385579816997, 0.10843987495172769, 0.19566548394504935, 0.14280048396904022, 0.13871260301675647, 0.23712066502775997, 0.08562011201865971, 0.1039951810380444, 0.11297202506102622, 0.15972129604779184, 0.13346444605849683, 0.15932127693668008, 0.4241844420321286, 0.167830022983253, 0.17536176601424813, 0.33902697905432433, 0.07009757100604475, 0.8965894590364769, 0.07004636607598513, 0.2425253950059414, 0.059888896998018026, 0.11198855889961123, 0.15577246609609574, 0.05956713098566979, 0.10292874195147306, 0.1228866579476744, 0.07721672405023128, 0.17410763597581536, 0.06400257896166295, 0.11186883097980171, 0.08337332191877067, 0.1846981009002775, 0.12040967994835228, 0.1460047389846295, 0.05812757299281657, 0.11405906802974641, 0.12657405505888164, 0.11515477497596294, 0.2016636369517073, 0.061708244029432535, 0.13510499394033104, 0.09982037101872265, 0.16448401100933552, 0.0581976140383631, 0.1094035079004243, 0.2507846358930692, 0.10248585604131222, 0.08696549001615494, 0.07196963799651712, 0.07713989308103919, 0.09002470201812685, 0.06425190297886729, 0.07696220302022994, 0.1323920110007748, 0.15458476601634175, 0.15049829299096018, 0.09106910694390535, 0.12094854400493205, 0.059014888014644384, 0.08546614099759609, 0.09594767598900944, 0.10268657491542399, 0.1285738479346037, 0.10195122100412846, 0.22005064494442195, 0.07698672905098647, 0.13713834597729146, 0.06202966195996851, 0.08744245197158307, 0.07543051394168288, 0.06864873203448951]
[0.0014702382746194912, 0.0015215430916710334, 0.002375849956561896, 0.003728461975697428, 0.003354471044572578, 0.004040899294936521, 0.0018648265686351806, 0.003151181069287387, 0.004628950706153939, 0.0013381169780834832, 0.002684613703009249, 0.0023657677494074133, 0.0014748834787910296, 0.0017142653867433016, 0.003421226090391759, 0.002335290091154589, 0.0016713607289024037, 0.0022538151586724616, 0.006743179613047026, 0.001337784638797695, 0.0032446358421690425, 0.002012918250825764, 0.004733778681838885, 0.003931695500134744, 0.0017656528877771714, 0.0014365335915830326, 0.0026125765236263924, 0.004006734726400199, 0.001423603590493175, 0.0024645426125392655, 0.0044469428169329394, 0.003245465544750914, 0.003152559159471738, 0.005389106023358181, 0.0019459116367877207, 0.002363526841773736, 0.0025675460241142323, 0.0036300294556316326, 0.0030332828649658372, 0.0036209381121972747, 0.009640555500730195, 0.003814318704164841, 0.003985494682142003, 0.0077051586148710076, 0.0015931266137737441, 0.02037703315991993, 0.0015919628653632985, 0.005511940795589577, 0.0013611112954095006, 0.0025451945204457097, 0.0035402833203658124, 0.0013537984314924952, 0.0023392895898062056, 0.0027928785897198727, 0.0017549255465961653, 0.003956991726723077, 0.0014546040673105215, 0.00254247343135913, 0.0018948482254266062, 0.004197684111369943, 0.0027365836351898247, 0.0033182895223779433, 0.0013210812043821948, 0.0025922515461306002, 0.0028766830695200374, 0.0026171539767264303, 0.004583264476175166, 0.0014024600915780122, 0.003070568044098433, 0.0022686447958800604, 0.003738272977484898, 0.0013226730463264341, 0.0024864433613732795, 0.005699650815751573, 0.002329224000938914, 0.001976488409458067, 0.0016356735908299345, 0.0017531793882054362, 0.0020460159549574282, 0.001460270522246984, 0.0017491409777324986, 0.0030089093409267, 0.0035132901367350396, 0.0034204157497945494, 0.0020697524305433035, 0.0027488305455666373, 0.0013412474548782814, 0.001942412295399911, 0.0021806289997502145, 0.002333785793532363, 0.0029221329076046295, 0.002317073204639283, 0.005001151021464135, 0.0017496983875224198, 0.003116780590392988, 0.0014097650445447389, 0.0019873284538996154, 0.0017143298623109745, 0.001560198455329307]
[680.1618603343785, 657.2275247898177, 420.90200066636567, 268.20710698355583, 298.1095936475488, 247.46966628271517, 536.2428961594401, 317.34133266614907, 216.03168049954698, 747.3188191904187, 372.4930700007512, 422.6957613444869, 678.0196635056932, 583.3402504263142, 292.2928720812753, 428.21232522148495, 598.3148836198325, 443.69210853520855, 148.29799254718833, 747.5044719445489, 308.20099655050933, 496.7911635704866, 211.24772981814598, 254.3431962026888, 566.3627358030305, 696.1201644425308, 382.7639079493633, 249.57978710470746, 702.4427352375338, 405.7548020927424, 224.87359095156992, 308.1222050307574, 317.2026120415663, 185.55953356005, 513.8979494725587, 423.0965277506806, 389.4769521590119, 275.4798582828574, 329.67581479126665, 276.17152489612016, 103.72846252731577, 262.17001712733224, 250.90988189765952, 129.78318162977118, 627.6965002996428, 49.07485756890869, 628.1553557292256, 181.42429991268372, 734.6937780713534, 392.89727836789535, 282.4632690404765, 738.6624010913869, 427.4802078193505, 358.05351642596844, 569.8247438129721, 252.71723295417024, 687.4722974265718, 393.3177777458347, 527.7467538461345, 238.22659673017728, 365.41912592802396, 301.360081227446, 756.9557394979753, 385.7650317511351, 347.6225833132344, 382.0944464455289, 218.18509605941873, 713.032767210385, 325.67263960229735, 440.79178980157474, 267.503204293229, 756.0447404423793, 402.18088838657206, 175.4493445872854, 429.3275355212287, 505.9478189776938, 611.3689220186063, 570.392286566639, 488.75474190562073, 684.8046199421016, 571.7092062506886, 332.3463377238261, 284.6334806066763, 292.36212003177275, 483.14957153474785, 363.79106802811754, 745.5745741495183, 514.823759285418, 458.5832803812787, 428.4883397487922, 342.2157826557361, 431.5789410527829, 199.95396973779856, 571.5270741124727, 320.8438871450724, 709.3380587564037, 503.1880855113608, 583.3183111282716, 640.9441033505783]
Elapsed: 0.13145783296057184~0.09929877170931538
Time per graph: 0.0029876780218311776~0.0022567902661208036
Speed: 427.925064978277~177.57014648378276
Total Time: 0.0693
best val loss: 0.41107216477394104 test_score: 0.7955

Testing...
Test loss: 0.6079 score: 0.7955 time: 0.10s
test Score 0.7955
Epoch Time List: [0.36352727690245956, 0.5321273959707469, 0.36697704903781414, 0.4469629409722984, 0.5816478339256719, 0.4315505011472851, 0.4163844969589263, 0.39802614715881646, 0.5728914530482143, 0.41522003698628396, 0.35061438800767064, 0.4225173940649256, 0.6283350280718878, 0.4584067548858002, 0.3856563330627978, 0.32642854703590274, 0.4772777180187404, 0.4968305059010163, 0.5760984711814672, 0.43658047216013074, 0.5878285599173978, 0.46762972604483366, 0.5549804440233856, 0.43818666390143335, 0.4092302630888298, 0.6361403070623055, 0.3924390790052712, 0.47047198796644807, 0.3758252840489149, 0.3206544261192903, 0.5869206987554207, 1.457119970000349, 0.5434779580682516, 0.7420723100658506, 0.4745497229741886, 0.33183669892605394, 0.3633680040948093, 0.5226953282253817, 0.38804091897327453, 0.5503016440197825, 0.8508459649747238, 0.7645577889634296, 0.6656546430895105, 0.7128302650526166, 0.7327678221045062, 1.5363611460197717, 0.474586385069415, 0.5688536108937114, 0.409268478048034, 0.4866577619686723, 0.38163493713364005, 0.5868648729519919, 0.5000793510116637, 0.5009536249563098, 0.3544112879317254, 0.5170912750763819, 0.5939841091167182, 0.5385150881484151, 0.6005392540246248, 0.5980050190119073, 0.578388512134552, 0.45746204804163426, 0.3384616761468351, 0.6761911220382899, 0.3865913460031152, 0.6842388298828155, 0.6343815339496359, 0.7268538549542427, 0.4375939769670367, 0.4069297999376431, 0.5906424077693373, 0.39035598491318524, 0.5541818781057373, 0.5722312839934602, 0.4175340619403869, 0.4181440939428285, 0.6186783188022673, 0.46482063096482307, 0.3247744250111282, 0.34680193895474076, 0.40887143195141107, 0.3885649968869984, 0.4055224269395694, 0.3477077210554853, 0.3095712539507076, 0.3770979300606996, 0.30177655606530607, 0.3431923760799691, 0.30124518694356084, 0.3338173540541902, 0.46806167194154114, 0.7257903119316325, 1.0405019300524145, 0.3809595570201054, 0.4744351841509342, 0.3602096471004188, 0.45847562700510025, 0.46117651101667434, 0.3722154780989513]
Total Epoch List: [99]
Total Time List: [0.06931772094685584]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d829b2b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4881 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5877 score: 0.4884 time: 0.11s
Epoch 2/1000, LR 0.000000
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2314 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2733 score: 0.4884 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0282 score: 0.5000 time: 3.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0286 score: 0.4884 time: 1.97s
Epoch 4/1000, LR 0.000060
Train loss: 0.7180;  Loss pred: 0.7180; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8846 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8593 score: 0.4884 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 2.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7967 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7720 score: 0.4884 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7502 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7337 score: 0.4884 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7169 score: 0.5000 time: 0.09s
Test loss: 0.7049 score: 0.5116 time: 0.21s
Epoch 8/1000, LR 0.000180
Train loss: 0.6423;  Loss pred: 0.6423; Loss self: 0.0000; time: 0.20s
Val loss: 0.6928 score: 0.5455 time: 0.19s
Test loss: 0.6802 score: 0.6279 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 0.22s
Val loss: 0.6756 score: 0.6591 time: 0.08s
Test loss: 0.6588 score: 0.7442 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.50s
Val loss: 0.6617 score: 0.5909 time: 1.05s
Test loss: 0.6427 score: 0.6279 time: 0.34s
Epoch 11/1000, LR 0.000270
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.19s
Val loss: 0.6534 score: 0.6136 time: 0.05s
Test loss: 0.6358 score: 0.6279 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.26s
Val loss: 0.6497 score: 0.5909 time: 0.07s
Test loss: 0.6335 score: 0.6512 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5403;  Loss pred: 0.5403; Loss self: 0.0000; time: 0.37s
Val loss: 0.6476 score: 0.5682 time: 0.11s
Test loss: 0.6325 score: 0.5814 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.5455;  Loss pred: 0.5455; Loss self: 0.0000; time: 0.21s
Val loss: 0.6453 score: 0.5682 time: 0.10s
Test loss: 0.6318 score: 0.5581 time: 0.11s
Epoch 15/1000, LR 0.000270
Train loss: 0.4652;  Loss pred: 0.4652; Loss self: 0.0000; time: 0.15s
Val loss: 0.6424 score: 0.5682 time: 0.05s
Test loss: 0.6308 score: 0.5814 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.5036;  Loss pred: 0.5036; Loss self: 0.0000; time: 0.15s
Val loss: 0.6398 score: 0.5682 time: 0.05s
Test loss: 0.6287 score: 0.5814 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.4721;  Loss pred: 0.4721; Loss self: 0.0000; time: 0.15s
Val loss: 0.6365 score: 0.5682 time: 0.05s
Test loss: 0.6269 score: 0.5814 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.4818;  Loss pred: 0.4818; Loss self: 0.0000; time: 0.15s
Val loss: 0.6331 score: 0.5909 time: 0.05s
Test loss: 0.6255 score: 0.5814 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.4511;  Loss pred: 0.4511; Loss self: 0.0000; time: 0.15s
Val loss: 0.6311 score: 0.5682 time: 0.05s
Test loss: 0.6256 score: 0.6047 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.4281;  Loss pred: 0.4281; Loss self: 0.0000; time: 0.27s
Val loss: 0.6286 score: 0.5682 time: 0.05s
Test loss: 0.6259 score: 0.6047 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.3861;  Loss pred: 0.3861; Loss self: 0.0000; time: 0.16s
Val loss: 0.6266 score: 0.5909 time: 0.05s
Test loss: 0.6258 score: 0.6047 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.3655;  Loss pred: 0.3655; Loss self: 0.0000; time: 0.16s
Val loss: 0.6243 score: 0.6136 time: 0.05s
Test loss: 0.6243 score: 0.6047 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.3665;  Loss pred: 0.3665; Loss self: 0.0000; time: 0.16s
Val loss: 0.6219 score: 0.6136 time: 0.05s
Test loss: 0.6234 score: 0.6047 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.3622;  Loss pred: 0.3622; Loss self: 0.0000; time: 0.16s
Val loss: 0.6194 score: 0.6136 time: 0.05s
Test loss: 0.6218 score: 0.6279 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.3341;  Loss pred: 0.3341; Loss self: 0.0000; time: 0.16s
Val loss: 0.6178 score: 0.6136 time: 0.08s
Test loss: 0.6190 score: 0.6744 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.3011;  Loss pred: 0.3011; Loss self: 0.0000; time: 0.15s
Val loss: 0.6161 score: 0.6364 time: 0.05s
Test loss: 0.6150 score: 0.6977 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.2719;  Loss pred: 0.2719; Loss self: 0.0000; time: 0.15s
Val loss: 0.6146 score: 0.6364 time: 0.05s
Test loss: 0.6127 score: 0.6977 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.2536;  Loss pred: 0.2536; Loss self: 0.0000; time: 0.15s
Val loss: 0.6133 score: 0.6364 time: 0.05s
Test loss: 0.6097 score: 0.6977 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.2455;  Loss pred: 0.2455; Loss self: 0.0000; time: 0.15s
Val loss: 0.6082 score: 0.6591 time: 0.05s
Test loss: 0.6071 score: 0.6977 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.2222;  Loss pred: 0.2222; Loss self: 0.0000; time: 0.16s
Val loss: 0.6001 score: 0.7045 time: 0.05s
Test loss: 0.6044 score: 0.7209 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.1915;  Loss pred: 0.1915; Loss self: 0.0000; time: 0.16s
Val loss: 0.5921 score: 0.7273 time: 0.05s
Test loss: 0.6016 score: 0.6977 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.1794;  Loss pred: 0.1794; Loss self: 0.0000; time: 0.16s
Val loss: 0.5835 score: 0.7727 time: 0.05s
Test loss: 0.5999 score: 0.6977 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.1519;  Loss pred: 0.1519; Loss self: 0.0000; time: 0.16s
Val loss: 0.5753 score: 0.7727 time: 0.05s
Test loss: 0.5994 score: 0.6744 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.1324;  Loss pred: 0.1324; Loss self: 0.0000; time: 0.16s
Val loss: 0.5687 score: 0.7955 time: 0.05s
Test loss: 0.5985 score: 0.6744 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 0.1330;  Loss pred: 0.1330; Loss self: 0.0000; time: 0.16s
Val loss: 0.5627 score: 0.7955 time: 0.05s
Test loss: 0.5983 score: 0.6744 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.16s
Val loss: 0.5568 score: 0.7727 time: 0.05s
Test loss: 0.5970 score: 0.6744 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.1009;  Loss pred: 0.1009; Loss self: 0.0000; time: 0.17s
Val loss: 0.5500 score: 0.7727 time: 0.05s
Test loss: 0.5949 score: 0.6744 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.0762;  Loss pred: 0.0762; Loss self: 0.0000; time: 0.17s
Val loss: 0.5437 score: 0.7727 time: 0.05s
Test loss: 0.5916 score: 0.6744 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.0791;  Loss pred: 0.0791; Loss self: 0.0000; time: 0.17s
Val loss: 0.5379 score: 0.7727 time: 0.05s
Test loss: 0.5874 score: 0.6744 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.0826;  Loss pred: 0.0826; Loss self: 0.0000; time: 0.16s
Val loss: 0.5320 score: 0.7727 time: 0.05s
Test loss: 0.5818 score: 0.6744 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.15s
Val loss: 0.5265 score: 0.7727 time: 0.05s
Test loss: 0.5758 score: 0.6977 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.15s
Val loss: 0.5212 score: 0.7727 time: 0.05s
Test loss: 0.5695 score: 0.6977 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0602;  Loss pred: 0.0602; Loss self: 0.0000; time: 0.15s
Val loss: 0.5152 score: 0.7727 time: 0.05s
Test loss: 0.5637 score: 0.6977 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.15s
Val loss: 0.5088 score: 0.7727 time: 0.05s
Test loss: 0.5570 score: 0.6977 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.15s
Val loss: 0.5024 score: 0.7727 time: 0.05s
Test loss: 0.5501 score: 0.6977 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.16s
Val loss: 0.4955 score: 0.7727 time: 0.05s
Test loss: 0.5440 score: 0.6977 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.18s
Val loss: 0.4887 score: 0.7727 time: 0.06s
Test loss: 0.5392 score: 0.7209 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.17s
Val loss: 0.4811 score: 0.7727 time: 0.05s
Test loss: 0.5341 score: 0.7442 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.16s
Val loss: 0.4691 score: 0.7955 time: 0.05s
Test loss: 0.5305 score: 0.7209 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.16s
Val loss: 0.4582 score: 0.7955 time: 0.05s
Test loss: 0.5254 score: 0.7209 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.16s
Val loss: 0.4487 score: 0.7955 time: 0.05s
Test loss: 0.5197 score: 0.7442 time: 0.06s
Epoch 52/1000, LR 0.000269
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.16s
Val loss: 0.4398 score: 0.7955 time: 0.05s
Test loss: 0.5148 score: 0.7442 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.16s
Val loss: 0.4315 score: 0.7955 time: 0.05s
Test loss: 0.5106 score: 0.7442 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.0212;  Loss pred: 0.0212; Loss self: 0.0000; time: 0.16s
Val loss: 0.4275 score: 0.7955 time: 0.05s
Test loss: 0.5064 score: 0.7209 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 0.0146;  Loss pred: 0.0146; Loss self: 0.0000; time: 0.16s
Val loss: 0.4193 score: 0.7955 time: 0.05s
Test loss: 0.5007 score: 0.7209 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.16s
Val loss: 0.4087 score: 0.7727 time: 0.05s
Test loss: 0.4947 score: 0.7442 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.16s
Val loss: 0.3945 score: 0.8182 time: 0.05s
Test loss: 0.4906 score: 0.7442 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.17s
Val loss: 0.3803 score: 0.8182 time: 0.05s
Test loss: 0.4891 score: 0.7442 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.16s
Val loss: 0.3661 score: 0.8409 time: 0.05s
Test loss: 0.4883 score: 0.7674 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.16s
Val loss: 0.3521 score: 0.8636 time: 0.05s
Test loss: 0.4904 score: 0.7674 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.15s
Val loss: 0.3394 score: 0.8636 time: 0.05s
Test loss: 0.4922 score: 0.7674 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.15s
Val loss: 0.3253 score: 0.8636 time: 0.05s
Test loss: 0.4986 score: 0.7674 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.16s
Val loss: 0.3168 score: 0.8636 time: 0.05s
Test loss: 0.5045 score: 0.7674 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.16s
Val loss: 0.3109 score: 0.8636 time: 0.05s
Test loss: 0.5068 score: 0.7907 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.15s
Val loss: 0.3077 score: 0.8636 time: 0.05s
Test loss: 0.5049 score: 0.8140 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.15s
Val loss: 0.3051 score: 0.8864 time: 0.05s
Test loss: 0.5021 score: 0.8140 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.14s
Val loss: 0.3034 score: 0.8864 time: 0.05s
Test loss: 0.4971 score: 0.8140 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.14s
Val loss: 0.3015 score: 0.8864 time: 0.05s
Test loss: 0.4938 score: 0.8140 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.15s
Val loss: 0.2990 score: 0.8864 time: 0.05s
Test loss: 0.4925 score: 0.8140 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.14s
Val loss: 0.2975 score: 0.8636 time: 0.05s
Test loss: 0.4887 score: 0.8140 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.14s
Val loss: 0.2966 score: 0.8636 time: 0.05s
Test loss: 0.4809 score: 0.8140 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.14s
Val loss: 0.2958 score: 0.8636 time: 0.05s
Test loss: 0.4765 score: 0.8140 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.14s
Val loss: 0.2952 score: 0.8636 time: 0.05s
Test loss: 0.4724 score: 0.8140 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.14s
Val loss: 0.2933 score: 0.8636 time: 0.05s
Test loss: 0.4758 score: 0.8140 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.15s
Val loss: 0.2922 score: 0.8864 time: 0.05s
Test loss: 0.4840 score: 0.8140 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.15s
Val loss: 0.2926 score: 0.9091 time: 0.05s
Test loss: 0.4980 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.15s
Val loss: 0.2956 score: 0.9091 time: 0.05s
Test loss: 0.5171 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.17s
Val loss: 0.3018 score: 0.8864 time: 0.05s
Test loss: 0.5370 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.16s
Val loss: 0.3099 score: 0.8864 time: 0.05s
Test loss: 0.5558 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.16s
Val loss: 0.3195 score: 0.8864 time: 0.05s
Test loss: 0.5751 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.15s
Val loss: 0.3310 score: 0.8864 time: 0.05s
Test loss: 0.5947 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.15s
Val loss: 0.3442 score: 0.8864 time: 0.05s
Test loss: 0.6144 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.15s
Val loss: 0.3552 score: 0.8864 time: 0.05s
Test loss: 0.6286 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.15s
Val loss: 0.3684 score: 0.8864 time: 0.05s
Test loss: 0.6459 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.15s
Val loss: 0.3790 score: 0.8864 time: 0.05s
Test loss: 0.6527 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.16s
Val loss: 0.3892 score: 0.8864 time: 0.05s
Test loss: 0.6573 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.16s
Val loss: 0.3962 score: 0.8864 time: 0.05s
Test loss: 0.6491 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.16s
Val loss: 0.4013 score: 0.8864 time: 0.05s
Test loss: 0.6373 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.16s
Val loss: 0.4067 score: 0.8864 time: 0.05s
Test loss: 0.6250 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.16s
Val loss: 0.4105 score: 0.8864 time: 0.05s
Test loss: 0.6098 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.16s
Val loss: 0.4175 score: 0.8864 time: 0.13s
Test loss: 0.6032 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.16s
Val loss: 0.4238 score: 0.8864 time: 0.05s
Test loss: 0.5974 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.17s
Val loss: 0.4326 score: 0.8864 time: 0.05s
Test loss: 0.5986 score: 0.8140 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.16s
Val loss: 0.4444 score: 0.8864 time: 0.05s
Test loss: 0.6077 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.16s
Val loss: 0.4592 score: 0.8864 time: 0.05s
Test loss: 0.6250 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0038,   Val_Loss: 0.2922,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.2922,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4840


[0.06469048408325762, 0.06694789603352547, 0.10453739808872342, 0.16405232693068683, 0.14759672596119344, 0.17779956897720695, 0.08205236901994795, 0.13865196704864502, 0.2036738310707733, 0.05887714703567326, 0.11812300293240696, 0.10409378097392619, 0.0648948730668053, 0.07542767701670527, 0.1505339479772374, 0.10275276401080191, 0.07353987207170576, 0.0991678669815883, 0.2966999029740691, 0.05886252410709858, 0.14276397705543786, 0.08856840303633362, 0.20828626200091094, 0.17299460200592875, 0.07768872706219554, 0.06320747802965343, 0.11495336703956127, 0.17629632796160877, 0.0626385579816997, 0.10843987495172769, 0.19566548394504935, 0.14280048396904022, 0.13871260301675647, 0.23712066502775997, 0.08562011201865971, 0.1039951810380444, 0.11297202506102622, 0.15972129604779184, 0.13346444605849683, 0.15932127693668008, 0.4241844420321286, 0.167830022983253, 0.17536176601424813, 0.33902697905432433, 0.07009757100604475, 0.8965894590364769, 0.07004636607598513, 0.2425253950059414, 0.059888896998018026, 0.11198855889961123, 0.15577246609609574, 0.05956713098566979, 0.10292874195147306, 0.1228866579476744, 0.07721672405023128, 0.17410763597581536, 0.06400257896166295, 0.11186883097980171, 0.08337332191877067, 0.1846981009002775, 0.12040967994835228, 0.1460047389846295, 0.05812757299281657, 0.11405906802974641, 0.12657405505888164, 0.11515477497596294, 0.2016636369517073, 0.061708244029432535, 0.13510499394033104, 0.09982037101872265, 0.16448401100933552, 0.0581976140383631, 0.1094035079004243, 0.2507846358930692, 0.10248585604131222, 0.08696549001615494, 0.07196963799651712, 0.07713989308103919, 0.09002470201812685, 0.06425190297886729, 0.07696220302022994, 0.1323920110007748, 0.15458476601634175, 0.15049829299096018, 0.09106910694390535, 0.12094854400493205, 0.059014888014644384, 0.08546614099759609, 0.09594767598900944, 0.10268657491542399, 0.1285738479346037, 0.10195122100412846, 0.22005064494442195, 0.07698672905098647, 0.13713834597729146, 0.06202966195996851, 0.08744245197158307, 0.07543051394168288, 0.06864873203448951, 0.11917406995780766, 0.05538000795058906, 1.9741497099166736, 0.07133440801408142, 0.07157575897872448, 0.0843239719979465, 0.21619140799157321, 0.05877910496201366, 0.09361600794363767, 0.34461629297584295, 0.099300964968279, 0.08926645503379405, 0.09799770300742239, 0.11837517295498401, 0.07990865700412542, 0.05152112594805658, 0.0514357479987666, 0.05156436294782907, 0.05193865392357111, 0.05703437398187816, 0.05915492807980627, 0.05425220006145537, 0.054100556997582316, 0.057964115985669196, 0.0514782730024308, 0.05231048399582505, 0.05208674492314458, 0.05430261103902012, 0.05570248095318675, 0.05526026606094092, 0.055585702997632325, 0.05519061803352088, 0.05557457299437374, 0.055661931983195245, 0.05512534605804831, 0.05538325000088662, 0.05550745700020343, 0.056309250998310745, 0.054845661972649395, 0.053952745045535266, 0.05461416102480143, 0.054255377035588026, 0.05443820497021079, 0.05442592909093946, 0.054007646976970136, 0.05651397898327559, 0.05680014495737851, 0.05768093699589372, 0.05622402299195528, 0.061722186976112425, 0.05996127799153328, 0.056690116063691676, 0.0562205909518525, 0.05573432601522654, 0.05660843406803906, 0.055756881018169224, 0.05611609108746052, 0.054147830000147223, 0.055042182095348835, 0.054157169070094824, 0.05509230098687112, 0.05429840390570462, 0.05455181410070509, 0.05402524501550943, 0.053700397023931146, 0.050499287084676325, 0.050776915973983705, 0.051300455001182854, 0.050944317947141826, 0.050474860006943345, 0.05103515600785613, 0.051126110018230975, 0.05123683204874396, 0.05123690399341285, 0.05194549506995827, 0.05177818494848907, 0.05158869095612317, 0.05348735908046365, 0.05378373898565769, 0.05423479701858014, 0.05292502592783421, 0.05376743001397699, 0.053373378003016114, 0.05350268096663058, 0.05614765302743763, 0.055482983007095754, 0.05561702896375209, 0.05507643602322787, 0.05560150800738484, 0.05539595091249794, 0.055030946037732065, 0.05525348300579935, 0.06006657995749265, 0.05570041202008724, 0.055255193961784244]
[0.0014702382746194912, 0.0015215430916710334, 0.002375849956561896, 0.003728461975697428, 0.003354471044572578, 0.004040899294936521, 0.0018648265686351806, 0.003151181069287387, 0.004628950706153939, 0.0013381169780834832, 0.002684613703009249, 0.0023657677494074133, 0.0014748834787910296, 0.0017142653867433016, 0.003421226090391759, 0.002335290091154589, 0.0016713607289024037, 0.0022538151586724616, 0.006743179613047026, 0.001337784638797695, 0.0032446358421690425, 0.002012918250825764, 0.004733778681838885, 0.003931695500134744, 0.0017656528877771714, 0.0014365335915830326, 0.0026125765236263924, 0.004006734726400199, 0.001423603590493175, 0.0024645426125392655, 0.0044469428169329394, 0.003245465544750914, 0.003152559159471738, 0.005389106023358181, 0.0019459116367877207, 0.002363526841773736, 0.0025675460241142323, 0.0036300294556316326, 0.0030332828649658372, 0.0036209381121972747, 0.009640555500730195, 0.003814318704164841, 0.003985494682142003, 0.0077051586148710076, 0.0015931266137737441, 0.02037703315991993, 0.0015919628653632985, 0.005511940795589577, 0.0013611112954095006, 0.0025451945204457097, 0.0035402833203658124, 0.0013537984314924952, 0.0023392895898062056, 0.0027928785897198727, 0.0017549255465961653, 0.003956991726723077, 0.0014546040673105215, 0.00254247343135913, 0.0018948482254266062, 0.004197684111369943, 0.0027365836351898247, 0.0033182895223779433, 0.0013210812043821948, 0.0025922515461306002, 0.0028766830695200374, 0.0026171539767264303, 0.004583264476175166, 0.0014024600915780122, 0.003070568044098433, 0.0022686447958800604, 0.003738272977484898, 0.0013226730463264341, 0.0024864433613732795, 0.005699650815751573, 0.002329224000938914, 0.001976488409458067, 0.0016356735908299345, 0.0017531793882054362, 0.0020460159549574282, 0.001460270522246984, 0.0017491409777324986, 0.0030089093409267, 0.0035132901367350396, 0.0034204157497945494, 0.0020697524305433035, 0.0027488305455666373, 0.0013412474548782814, 0.001942412295399911, 0.0021806289997502145, 0.002333785793532363, 0.0029221329076046295, 0.002317073204639283, 0.005001151021464135, 0.0017496983875224198, 0.003116780590392988, 0.0014097650445447389, 0.0019873284538996154, 0.0017143298623109745, 0.001560198455329307, 0.002771489999018783, 0.0012879071616416062, 0.0459104583701552, 0.0016589397212577073, 0.0016645525343889414, 0.001961022604603407, 0.005027707162594726, 0.001366955929349155, 0.002177116463805527, 0.008014332394787046, 0.002309324766704163, 0.00207596407055335, 0.002279016349009823, 0.0027529109989531163, 0.0018583408605610562, 0.001198165719722246, 0.001196180186017828, 0.001199171231344862, 0.0012078756726411886, 0.0013263807902762362, 0.0013756960018559597, 0.0012616790711966364, 0.0012581524883158678, 0.001348002697341144, 0.0011971691395914138, 0.0012165228836238384, 0.0012113196493754553, 0.0012628514195120958, 0.0012954065337950408, 0.0012851224665335098, 0.0012926907673867982, 0.0012835027449656017, 0.0012924319301017147, 0.0012944635344929126, 0.001281984792047635, 0.001287982558160154, 0.0012908710930279869, 0.0013095174650769941, 0.0012754805109918463, 0.0012547150010589598, 0.001270096768018638, 0.0012617529543160007, 0.001266004766749088, 0.0012657192811846387, 0.0012559917901620963, 0.001314278581006409, 0.0013209336036599654, 0.0013414171394393888, 0.0013075354184175646, 0.0014353996971188936, 0.001394448325384495, 0.001318374792178876, 0.0013074556035314534, 0.0012961471166331754, 0.0013164752108846293, 0.0012966716515853307, 0.0013050253741269887, 0.00125925186046854, 0.0012800507464034612, 0.00125946904814174, 0.0012812163020202586, 0.001262753579202433, 0.0012686468395512812, 0.0012564010468723123, 0.0012488464424170035, 0.0011744020252250308, 0.001180858511022877, 0.0011930338372368105, 0.001184751580166089, 0.0011738339536498453, 0.0011868640932059566, 0.0011889793027495576, 0.00119155423369172, 0.0011915559068235548, 0.001208034769068797, 0.0012041438360113738, 0.0011997369989796087, 0.0012438920716386895, 0.0012507846275734346, 0.0012612743492693055, 0.0012308145564612607, 0.0012504053491622555, 0.0012412413489073515, 0.0012442483945728042, 0.0013057593727311076, 0.0012903019303975756, 0.0012934192782267928, 0.0012808473493773924, 0.0012930583257531358, 0.0012882779281976264, 0.001279789442737955, 0.0012849647210651013, 0.0013968972083137825, 0.0012953584190717963, 0.0012850045107391684]
[680.1618603343785, 657.2275247898177, 420.90200066636567, 268.20710698355583, 298.1095936475488, 247.46966628271517, 536.2428961594401, 317.34133266614907, 216.03168049954698, 747.3188191904187, 372.4930700007512, 422.6957613444869, 678.0196635056932, 583.3402504263142, 292.2928720812753, 428.21232522148495, 598.3148836198325, 443.69210853520855, 148.29799254718833, 747.5044719445489, 308.20099655050933, 496.7911635704866, 211.24772981814598, 254.3431962026888, 566.3627358030305, 696.1201644425308, 382.7639079493633, 249.57978710470746, 702.4427352375338, 405.7548020927424, 224.87359095156992, 308.1222050307574, 317.2026120415663, 185.55953356005, 513.8979494725587, 423.0965277506806, 389.4769521590119, 275.4798582828574, 329.67581479126665, 276.17152489612016, 103.72846252731577, 262.17001712733224, 250.90988189765952, 129.78318162977118, 627.6965002996428, 49.07485756890869, 628.1553557292256, 181.42429991268372, 734.6937780713534, 392.89727836789535, 282.4632690404765, 738.6624010913869, 427.4802078193505, 358.05351642596844, 569.8247438129721, 252.71723295417024, 687.4722974265718, 393.3177777458347, 527.7467538461345, 238.22659673017728, 365.41912592802396, 301.360081227446, 756.9557394979753, 385.7650317511351, 347.6225833132344, 382.0944464455289, 218.18509605941873, 713.032767210385, 325.67263960229735, 440.79178980157474, 267.503204293229, 756.0447404423793, 402.18088838657206, 175.4493445872854, 429.3275355212287, 505.9478189776938, 611.3689220186063, 570.392286566639, 488.75474190562073, 684.8046199421016, 571.7092062506886, 332.3463377238261, 284.6334806066763, 292.36212003177275, 483.14957153474785, 363.79106802811754, 745.5745741495183, 514.823759285418, 458.5832803812787, 428.4883397487922, 342.2157826557361, 431.5789410527829, 199.95396973779856, 571.5270741124727, 320.8438871450724, 709.3380587564037, 503.1880855113608, 583.3183111282716, 640.9441033505783, 360.81674491123533, 776.4534818840275, 21.78152942707419, 602.7946568437463, 600.7620542700989, 509.9380280739996, 198.89782114595445, 731.5524798785044, 459.32315364150674, 124.77645682009071, 433.0270105002106, 481.70390527686214, 438.7857947725893, 363.2518451850722, 538.1144122817639, 834.6090891599002, 835.994452749693, 833.9092648833036, 827.899776980656, 753.9313048945294, 726.9047803082178, 792.5945851281756, 794.8162160681934, 741.8382781966543, 835.3038571820299, 822.0149521734853, 825.5459246579467, 791.8587923719095, 771.9584346007475, 778.1359567212304, 773.5802136357183, 779.1179285921982, 773.7351397077443, 772.5207959540826, 780.0404546162844, 776.4080294910758, 774.6706897389014, 763.6400633581502, 784.0182514606784, 796.9937389415252, 787.341583082688, 792.5481740140664, 789.8864413977296, 790.064601895026, 796.183548198943, 760.8736948556598, 757.0403215038657, 745.480261582109, 764.7976383004927, 696.670064796015, 717.1294782287952, 758.5096483430948, 764.8443261086555, 771.5173587683191, 759.6041245076175, 771.2052613916442, 766.2686257491056, 794.1223129326344, 781.2190280812573, 793.9853714352339, 780.5083329201879, 791.920146947126, 788.2414308096166, 795.9242015034947, 800.7389587983381, 851.4971692154455, 846.8415061291174, 838.1991933406542, 844.0588024873629, 851.9092473775044, 842.5564525242319, 841.0575337076633, 839.2400209109751, 839.2388424860368, 827.7907437803642, 830.465572379141, 833.5160129682693, 803.9282690198445, 799.4981533631689, 792.8489155268481, 812.4700790630229, 799.740660634552, 805.6450914081189, 803.6980432217769, 765.8378878096156, 775.012403253457, 773.1444991070068, 780.7331611265702, 773.3603195490464, 776.2300184705147, 781.3785350976335, 778.2314826286473, 715.8722875587363, 771.9871081832017, 778.2073849879123]
Elapsed: 0.10797933022222798~0.1576315633562949
Time per graph: 0.0024756904745805498~0.0036439886458908767
Speed: 578.5537222309451~226.26434612218668
Total Time: 0.0560
best val loss: 0.29218801856040955 test_score: 0.8140

Testing...
Test loss: 0.4980 score: 0.8140 time: 0.05s
test Score 0.8140
Epoch Time List: [0.36352727690245956, 0.5321273959707469, 0.36697704903781414, 0.4469629409722984, 0.5816478339256719, 0.4315505011472851, 0.4163844969589263, 0.39802614715881646, 0.5728914530482143, 0.41522003698628396, 0.35061438800767064, 0.4225173940649256, 0.6283350280718878, 0.4584067548858002, 0.3856563330627978, 0.32642854703590274, 0.4772777180187404, 0.4968305059010163, 0.5760984711814672, 0.43658047216013074, 0.5878285599173978, 0.46762972604483366, 0.5549804440233856, 0.43818666390143335, 0.4092302630888298, 0.6361403070623055, 0.3924390790052712, 0.47047198796644807, 0.3758252840489149, 0.3206544261192903, 0.5869206987554207, 1.457119970000349, 0.5434779580682516, 0.7420723100658506, 0.4745497229741886, 0.33183669892605394, 0.3633680040948093, 0.5226953282253817, 0.38804091897327453, 0.5503016440197825, 0.8508459649747238, 0.7645577889634296, 0.6656546430895105, 0.7128302650526166, 0.7327678221045062, 1.5363611460197717, 0.474586385069415, 0.5688536108937114, 0.409268478048034, 0.4866577619686723, 0.38163493713364005, 0.5868648729519919, 0.5000793510116637, 0.5009536249563098, 0.3544112879317254, 0.5170912750763819, 0.5939841091167182, 0.5385150881484151, 0.6005392540246248, 0.5980050190119073, 0.578388512134552, 0.45746204804163426, 0.3384616761468351, 0.6761911220382899, 0.3865913460031152, 0.6842388298828155, 0.6343815339496359, 0.7268538549542427, 0.4375939769670367, 0.4069297999376431, 0.5906424077693373, 0.39035598491318524, 0.5541818781057373, 0.5722312839934602, 0.4175340619403869, 0.4181440939428285, 0.6186783188022673, 0.46482063096482307, 0.3247744250111282, 0.34680193895474076, 0.40887143195141107, 0.3885649968869984, 0.4055224269395694, 0.3477077210554853, 0.3095712539507076, 0.3770979300606996, 0.30177655606530607, 0.3431923760799691, 0.30124518694356084, 0.3338173540541902, 0.46806167194154114, 0.7257903119316325, 1.0405019300524145, 0.3809595570201054, 0.4744351841509342, 0.3602096471004188, 0.45847562700510025, 0.46117651101667434, 0.3722154780989513, 0.4289987019728869, 0.5758693440584466, 5.520177360158414, 1.0634270169539377, 2.8973690370330587, 0.44613897206727415, 0.6714173100190237, 0.44829268706962466, 0.3876839819131419, 1.888203123002313, 0.337775842868723, 0.406929649063386, 0.5683618830516934, 0.4154773319605738, 0.2776017241412774, 0.2481043168809265, 0.24545814003795385, 0.2456778499763459, 0.24629673396702856, 0.3773668259382248, 0.26508025999646634, 0.2557738679461181, 0.2554451118921861, 0.2582182699115947, 0.29088157205842435, 0.24335359607357532, 0.2461760510923341, 0.2539342959644273, 0.24582377402111888, 0.2584664688911289, 0.2616305019473657, 0.2584359759930521, 0.26043273508548737, 0.26061328290961683, 0.25970967614557594, 0.25949959887657315, 0.27184858196415007, 0.2693347439635545, 0.2662622359348461, 0.2546390931820497, 0.2520498260855675, 0.25126787181943655, 0.2512102120090276, 0.2543595328461379, 0.2536790670128539, 0.2622519520809874, 0.2892376579111442, 0.2703149380395189, 0.2683083269512281, 0.26355571299791336, 0.2665528111392632, 0.2674374261405319, 0.26479971793014556, 0.2628989510703832, 0.2666801770683378, 0.25917071604635566, 0.2622036269167438, 0.266917428933084, 0.256706882850267, 0.25546350597869605, 0.26016533793881536, 0.25343572616111487, 0.25796418997924775, 0.26104360807221383, 0.2531897989101708, 0.248871369054541, 0.2364588340278715, 0.23667120991740376, 0.23902089893817902, 0.23658119491301477, 0.2373186880722642, 0.236593073932454, 0.23755139915738255, 0.23701272788457572, 0.2464653920615092, 0.24475674785207957, 0.24871479312423617, 0.2679834308801219, 0.25812046008650213, 0.253964347881265, 0.24748450703918934, 0.2469820739934221, 0.24662855407223105, 0.25202749797608703, 0.2529052671743557, 0.25836284703109413, 0.26143586297985166, 0.2595934090204537, 0.2604291900061071, 0.257728929980658, 0.33468467299826443, 0.2597065760055557, 0.2720158010488376, 0.26281768397893757, 0.26388042501639575]
Total Epoch List: [99, 95]
Total Time List: [0.06931772094685584, 0.05599458294454962]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858d829acb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8231;  Loss pred: 0.8231; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1634 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9737 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8469;  Loss pred: 0.8469; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0569 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9145 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.8167;  Loss pred: 0.8167; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9953 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8832 score: 0.5116 time: 0.15s
Epoch 4/1000, LR 0.000060
Train loss: 0.7881;  Loss pred: 0.7881; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9538 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8619 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.7797;  Loss pred: 0.7797; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9374 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8433 score: 0.5116 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.7276;  Loss pred: 0.7276; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9212 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8232 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.7455;  Loss pred: 0.7455; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9079 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8066 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.7048;  Loss pred: 0.7048; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8904 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7932 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8749 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7800 score: 0.5116 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6582;  Loss pred: 0.6582; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8470 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7637 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8237 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7470 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7968 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7291 score: 0.5116 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7761 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7140 score: 0.5116 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.5740;  Loss pred: 0.5740; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7554 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5116 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7347 score: 0.5000 time: 0.06s
Test loss: 0.6816 score: 0.5349 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.4877;  Loss pred: 0.4877; Loss self: 0.0000; time: 0.15s
Val loss: 0.7156 score: 0.5227 time: 0.05s
Test loss: 0.6677 score: 0.5814 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.4697;  Loss pred: 0.4697; Loss self: 0.0000; time: 0.14s
Val loss: 0.6983 score: 0.6136 time: 0.05s
Test loss: 0.6559 score: 0.6744 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.4745;  Loss pred: 0.4745; Loss self: 0.0000; time: 0.14s
Val loss: 0.6840 score: 0.6136 time: 0.05s
Test loss: 0.6451 score: 0.7209 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.4613;  Loss pred: 0.4613; Loss self: 0.0000; time: 0.14s
Val loss: 0.6721 score: 0.6591 time: 0.05s
Test loss: 0.6351 score: 0.7442 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.4225;  Loss pred: 0.4225; Loss self: 0.0000; time: 0.14s
Val loss: 0.6550 score: 0.7045 time: 0.05s
Test loss: 0.6251 score: 0.7674 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.4424;  Loss pred: 0.4424; Loss self: 0.0000; time: 0.14s
Val loss: 0.6357 score: 0.7273 time: 0.05s
Test loss: 0.6173 score: 0.6744 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.4057;  Loss pred: 0.4057; Loss self: 0.0000; time: 0.14s
Val loss: 0.6228 score: 0.7273 time: 0.05s
Test loss: 0.6131 score: 0.6977 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.3727;  Loss pred: 0.3727; Loss self: 0.0000; time: 0.14s
Val loss: 0.6156 score: 0.7273 time: 0.05s
Test loss: 0.6095 score: 0.6744 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.3674;  Loss pred: 0.3674; Loss self: 0.0000; time: 0.19s
Val loss: 0.6101 score: 0.7273 time: 0.13s
Test loss: 0.6062 score: 0.6977 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.3259;  Loss pred: 0.3259; Loss self: 0.0000; time: 0.14s
Val loss: 0.6057 score: 0.7045 time: 0.05s
Test loss: 0.6023 score: 0.6977 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.3001;  Loss pred: 0.3001; Loss self: 0.0000; time: 0.14s
Val loss: 0.6005 score: 0.7273 time: 0.05s
Test loss: 0.6002 score: 0.6977 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.2863;  Loss pred: 0.2863; Loss self: 0.0000; time: 0.14s
Val loss: 0.5937 score: 0.7500 time: 0.05s
Test loss: 0.5993 score: 0.6744 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 0.14s
Val loss: 0.5881 score: 0.7500 time: 0.05s
Test loss: 0.5982 score: 0.6744 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 0.2578;  Loss pred: 0.2578; Loss self: 0.0000; time: 0.14s
Val loss: 0.5837 score: 0.7500 time: 0.05s
Test loss: 0.5975 score: 0.6512 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.2406;  Loss pred: 0.2406; Loss self: 0.0000; time: 0.14s
Val loss: 0.5801 score: 0.7500 time: 0.05s
Test loss: 0.5964 score: 0.6512 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 0.2288;  Loss pred: 0.2288; Loss self: 0.0000; time: 0.14s
Val loss: 0.5790 score: 0.7273 time: 0.05s
Test loss: 0.5954 score: 0.6744 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.2054;  Loss pred: 0.2054; Loss self: 0.0000; time: 0.14s
Val loss: 0.5790 score: 0.7273 time: 0.05s
Test loss: 0.5930 score: 0.6744 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.2113;  Loss pred: 0.2113; Loss self: 0.0000; time: 0.14s
Val loss: 0.5790 score: 0.7273 time: 0.05s
Test loss: 0.5897 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.2061;  Loss pred: 0.2061; Loss self: 0.0000; time: 0.14s
Val loss: 0.5759 score: 0.7045 time: 0.05s
Test loss: 0.5893 score: 0.6744 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 0.1791;  Loss pred: 0.1791; Loss self: 0.0000; time: 0.14s
Val loss: 0.5738 score: 0.7045 time: 0.05s
Test loss: 0.5890 score: 0.6744 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.1788;  Loss pred: 0.1788; Loss self: 0.0000; time: 0.14s
Val loss: 0.5716 score: 0.6818 time: 0.05s
Test loss: 0.5892 score: 0.6512 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.1462;  Loss pred: 0.1462; Loss self: 0.0000; time: 0.14s
Val loss: 0.5700 score: 0.6591 time: 0.05s
Test loss: 0.5899 score: 0.6512 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.1567;  Loss pred: 0.1567; Loss self: 0.0000; time: 0.14s
Val loss: 0.5697 score: 0.6591 time: 0.05s
Test loss: 0.5907 score: 0.6279 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.1400;  Loss pred: 0.1400; Loss self: 0.0000; time: 0.14s
Val loss: 0.5694 score: 0.6591 time: 0.05s
Test loss: 0.5912 score: 0.6047 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.1227;  Loss pred: 0.1227; Loss self: 0.0000; time: 0.53s
Val loss: 0.5689 score: 0.6591 time: 0.06s
Test loss: 0.5923 score: 0.6047 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.16s
Val loss: 0.5678 score: 0.6591 time: 0.09s
Test loss: 0.5948 score: 0.6047 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.1134;  Loss pred: 0.1134; Loss self: 0.0000; time: 0.17s
Val loss: 0.5676 score: 0.6136 time: 0.06s
Test loss: 0.5983 score: 0.6047 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.1060;  Loss pred: 0.1060; Loss self: 0.0000; time: 0.16s
Val loss: 0.5668 score: 0.6136 time: 0.07s
Test loss: 0.6016 score: 0.6047 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0984;  Loss pred: 0.0984; Loss self: 0.0000; time: 0.16s
Val loss: 0.5671 score: 0.5909 time: 0.09s
Test loss: 0.6057 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 0.17s
Val loss: 0.5670 score: 0.5909 time: 0.07s
Test loss: 0.6105 score: 0.6047 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0953;  Loss pred: 0.0953; Loss self: 0.0000; time: 0.18s
Val loss: 0.5662 score: 0.5909 time: 0.11s
Test loss: 0.6155 score: 0.6047 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.16s
Val loss: 0.5665 score: 0.6136 time: 0.06s
Test loss: 0.6238 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0813;  Loss pred: 0.0813; Loss self: 0.0000; time: 0.17s
Val loss: 0.5672 score: 0.6136 time: 0.05s
Test loss: 0.6313 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.16s
Val loss: 0.5675 score: 0.6364 time: 0.06s
Test loss: 0.6377 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0815;  Loss pred: 0.0815; Loss self: 0.0000; time: 0.16s
Val loss: 0.5675 score: 0.6591 time: 0.05s
Test loss: 0.6419 score: 0.6047 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.19s
Val loss: 0.5660 score: 0.6364 time: 0.06s
Test loss: 0.6447 score: 0.6047 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.23s
Val loss: 0.5639 score: 0.6364 time: 0.09s
Test loss: 0.6497 score: 0.6047 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0642;  Loss pred: 0.0642; Loss self: 0.0000; time: 0.15s
Val loss: 0.5637 score: 0.6364 time: 0.07s
Test loss: 0.6543 score: 0.6047 time: 0.09s
Epoch 54/1000, LR 0.000269
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.17s
Val loss: 0.5636 score: 0.6364 time: 0.06s
Test loss: 0.6586 score: 0.6047 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 0.16s
Val loss: 0.5630 score: 0.6591 time: 0.08s
Test loss: 0.6623 score: 0.5814 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.20s
Val loss: 0.5638 score: 0.6364 time: 0.07s
Test loss: 0.6671 score: 0.5814 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0468;  Loss pred: 0.0468; Loss self: 0.0000; time: 0.20s
Val loss: 0.5653 score: 0.6364 time: 0.13s
Test loss: 0.6714 score: 0.5814 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.34s
Val loss: 0.5651 score: 0.6364 time: 0.72s
Test loss: 0.6753 score: 0.6047 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.34s
Val loss: 0.5635 score: 0.6364 time: 0.37s
Test loss: 0.6787 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0511;  Loss pred: 0.0511; Loss self: 0.0000; time: 0.50s
Val loss: 0.5611 score: 0.6591 time: 0.38s
Test loss: 0.6827 score: 0.6047 time: 0.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.0453;  Loss pred: 0.0453; Loss self: 0.0000; time: 0.42s
Val loss: 0.5558 score: 0.6591 time: 0.16s
Test loss: 0.6821 score: 0.6279 time: 0.19s
Epoch 62/1000, LR 0.000268
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.21s
Val loss: 0.5510 score: 0.6364 time: 0.11s
Test loss: 0.6802 score: 0.6279 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.20s
Val loss: 0.5455 score: 0.6364 time: 0.06s
Test loss: 0.6758 score: 0.6279 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0389;  Loss pred: 0.0389; Loss self: 0.0000; time: 0.37s
Val loss: 0.5404 score: 0.6591 time: 0.06s
Test loss: 0.6748 score: 0.6279 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.15s
Val loss: 0.5347 score: 0.6591 time: 0.05s
Test loss: 0.6746 score: 0.6279 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.16s
Val loss: 0.5291 score: 0.6591 time: 0.09s
Test loss: 0.6761 score: 0.6279 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.16s
Val loss: 0.5245 score: 0.6591 time: 0.06s
Test loss: 0.6798 score: 0.6512 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.16s
Val loss: 0.5168 score: 0.6591 time: 0.07s
Test loss: 0.6857 score: 0.6512 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.19s
Val loss: 0.5123 score: 0.6818 time: 0.07s
Test loss: 0.6935 score: 0.6512 time: 0.12s
Epoch 70/1000, LR 0.000268
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.26s
Val loss: 0.5104 score: 0.6818 time: 0.05s
Test loss: 0.6972 score: 0.6512 time: 0.07s
Epoch 71/1000, LR 0.000268
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.26s
Val loss: 0.5052 score: 0.6818 time: 0.09s
Test loss: 0.6927 score: 0.6744 time: 0.06s
Epoch 72/1000, LR 0.000267
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.16s
Val loss: 0.4958 score: 0.6818 time: 0.11s
Test loss: 0.6837 score: 0.6744 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.21s
Val loss: 0.4773 score: 0.7273 time: 0.19s
Test loss: 0.6625 score: 0.6744 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.44s
Val loss: 0.4538 score: 0.7273 time: 0.07s
Test loss: 0.6292 score: 0.6744 time: 0.13s
Epoch 75/1000, LR 0.000267
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.19s
Val loss: 0.4284 score: 0.7273 time: 0.06s
Test loss: 0.5952 score: 0.6744 time: 0.11s
Epoch 76/1000, LR 0.000267
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.20s
Val loss: 0.4026 score: 0.7273 time: 0.21s
Test loss: 0.5598 score: 0.6744 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.29s
Val loss: 0.3818 score: 0.7727 time: 0.07s
Test loss: 0.5294 score: 0.6744 time: 0.07s
Epoch 78/1000, LR 0.000267
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.17s
Val loss: 0.3646 score: 0.8636 time: 0.06s
Test loss: 0.5024 score: 0.7209 time: 0.07s
Epoch 79/1000, LR 0.000267
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.35s
Val loss: 0.3470 score: 0.8864 time: 0.13s
Test loss: 0.4789 score: 0.7442 time: 0.10s
Epoch 80/1000, LR 0.000267
Train loss: 0.0182;  Loss pred: 0.0182; Loss self: 0.0000; time: 0.18s
Val loss: 0.3323 score: 0.8864 time: 0.06s
Test loss: 0.4650 score: 0.7442 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.33s
Val loss: 0.3225 score: 0.8864 time: 0.14s
Test loss: 0.4577 score: 0.7442 time: 0.10s
Epoch 82/1000, LR 0.000267
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.21s
Val loss: 0.3149 score: 0.8864 time: 0.08s
Test loss: 0.4569 score: 0.7442 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.33s
Val loss: 0.3057 score: 0.8864 time: 0.13s
Test loss: 0.4633 score: 0.7442 time: 0.09s
Epoch 84/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.18s
Val loss: 0.2967 score: 0.8636 time: 0.06s
Test loss: 0.4658 score: 0.7442 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.32s
Val loss: 0.2877 score: 0.8636 time: 0.26s
Test loss: 0.4632 score: 0.7442 time: 0.06s
Epoch 86/1000, LR 0.000266
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.20s
Val loss: 0.2777 score: 0.8864 time: 0.06s
Test loss: 0.4567 score: 0.7674 time: 0.10s
Epoch 87/1000, LR 0.000266
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.40s
Val loss: 0.2692 score: 0.8864 time: 0.07s
Test loss: 0.4492 score: 0.7907 time: 0.17s
Epoch 88/1000, LR 0.000266
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.43s
Val loss: 0.2598 score: 0.8864 time: 0.15s
Test loss: 0.4397 score: 0.7907 time: 0.11s
Epoch 89/1000, LR 0.000266
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.37s
Val loss: 0.2510 score: 0.9318 time: 0.13s
Test loss: 0.4237 score: 0.7907 time: 0.09s
Epoch 90/1000, LR 0.000266
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.19s
Val loss: 0.2461 score: 0.9318 time: 0.07s
Test loss: 0.4085 score: 0.7907 time: 0.07s
Epoch 91/1000, LR 0.000266
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.33s
Val loss: 0.2457 score: 0.9091 time: 0.14s
Test loss: 0.3935 score: 0.7674 time: 0.07s
Epoch 92/1000, LR 0.000266
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.22s
Val loss: 0.2477 score: 0.9091 time: 0.07s
Test loss: 0.3789 score: 0.7907 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.18s
Val loss: 0.2556 score: 0.9091 time: 0.16s
Test loss: 0.3640 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.18s
Val loss: 0.2607 score: 0.9091 time: 0.11s
Test loss: 0.3531 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.19s
Val loss: 0.2669 score: 0.9091 time: 0.06s
Test loss: 0.3437 score: 0.8372 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.16s
Val loss: 0.2746 score: 0.9091 time: 0.29s
Test loss: 0.3397 score: 0.8372 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.21s
Val loss: 0.2815 score: 0.9091 time: 0.09s
Test loss: 0.3393 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 6 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.19s
Val loss: 0.2866 score: 0.9091 time: 0.11s
Test loss: 0.3397 score: 0.8372 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.17s
Val loss: 0.2786 score: 0.9091 time: 0.06s
Test loss: 0.3443 score: 0.8372 time: 0.22s
     INFO: Early stopping counter 8 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.19s
Val loss: 0.2672 score: 0.9091 time: 0.10s
Test loss: 0.3510 score: 0.8372 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.25s
Val loss: 0.2415 score: 0.9091 time: 0.06s
Test loss: 0.3618 score: 0.8372 time: 0.07s
Epoch 102/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.29s
Val loss: 0.2176 score: 0.9091 time: 0.12s
Test loss: 0.3742 score: 0.8372 time: 0.06s
Epoch 103/1000, LR 0.000264
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.33s
Val loss: 0.2004 score: 0.9091 time: 0.05s
Test loss: 0.3839 score: 0.8372 time: 0.06s
Epoch 104/1000, LR 0.000264
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.15s
Val loss: 0.1884 score: 0.9091 time: 0.07s
Test loss: 0.3901 score: 0.8605 time: 0.07s
Epoch 105/1000, LR 0.000264
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.18s
Val loss: 0.1805 score: 0.9318 time: 0.17s
Test loss: 0.3955 score: 0.8605 time: 0.05s
Epoch 106/1000, LR 0.000264
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.31s
Val loss: 0.1768 score: 0.9091 time: 0.05s
Test loss: 0.3993 score: 0.8605 time: 0.10s
Epoch 107/1000, LR 0.000264
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.31s
Val loss: 0.1779 score: 0.9091 time: 0.16s
Test loss: 0.4003 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.18s
Val loss: 0.1866 score: 0.9091 time: 0.21s
Test loss: 0.3984 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.21s
Val loss: 0.2023 score: 0.9091 time: 0.07s
Test loss: 0.3951 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.19s
Val loss: 0.2208 score: 0.9091 time: 0.05s
Test loss: 0.3939 score: 0.8372 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.31s
Val loss: 0.2416 score: 0.9091 time: 0.14s
Test loss: 0.3940 score: 0.8372 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.16s
Val loss: 0.2477 score: 0.9091 time: 0.06s
Test loss: 0.3969 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.21s
Val loss: 0.2448 score: 0.9091 time: 0.25s
Test loss: 0.3997 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.18s
Val loss: 0.2405 score: 0.9091 time: 0.11s
Test loss: 0.4021 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.18s
Val loss: 0.2342 score: 0.9091 time: 0.06s
Test loss: 0.4042 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.28s
Val loss: 0.2136 score: 0.9091 time: 0.06s
Test loss: 0.4087 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.33s
Val loss: 0.2011 score: 0.9091 time: 0.08s
Test loss: 0.4137 score: 0.8372 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.18s
Val loss: 0.2099 score: 0.9091 time: 0.22s
Test loss: 0.4176 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.19s
Val loss: 0.2269 score: 0.9091 time: 0.12s
Test loss: 0.4216 score: 0.8372 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.19s
Val loss: 0.2445 score: 0.9091 time: 0.33s
Test loss: 0.4284 score: 0.8372 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 121/1000, LR 0.000262
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.18s
Val loss: 0.2588 score: 0.9091 time: 0.18s
Test loss: 0.4341 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 122/1000, LR 0.000262
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.32s
Val loss: 0.2732 score: 0.9091 time: 0.08s
Test loss: 0.4393 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 123/1000, LR 0.000262
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.21s
Val loss: 0.2878 score: 0.9091 time: 0.05s
Test loss: 0.4438 score: 0.8372 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 124/1000, LR 0.000261
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.30s
Val loss: 0.2942 score: 0.9091 time: 0.08s
Test loss: 0.4467 score: 0.8372 time: 0.38s
     INFO: Early stopping counter 18 of 20
Epoch 125/1000, LR 0.000261
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.20s
Val loss: 0.2955 score: 0.9091 time: 0.06s
Test loss: 0.4489 score: 0.8372 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 126/1000, LR 0.000261
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.15s
Val loss: 0.2928 score: 0.9091 time: 0.06s
Test loss: 0.4484 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 105,   Train_Loss: 0.0039,   Val_Loss: 0.1768,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.1768,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.3993


[0.06469048408325762, 0.06694789603352547, 0.10453739808872342, 0.16405232693068683, 0.14759672596119344, 0.17779956897720695, 0.08205236901994795, 0.13865196704864502, 0.2036738310707733, 0.05887714703567326, 0.11812300293240696, 0.10409378097392619, 0.0648948730668053, 0.07542767701670527, 0.1505339479772374, 0.10275276401080191, 0.07353987207170576, 0.0991678669815883, 0.2966999029740691, 0.05886252410709858, 0.14276397705543786, 0.08856840303633362, 0.20828626200091094, 0.17299460200592875, 0.07768872706219554, 0.06320747802965343, 0.11495336703956127, 0.17629632796160877, 0.0626385579816997, 0.10843987495172769, 0.19566548394504935, 0.14280048396904022, 0.13871260301675647, 0.23712066502775997, 0.08562011201865971, 0.1039951810380444, 0.11297202506102622, 0.15972129604779184, 0.13346444605849683, 0.15932127693668008, 0.4241844420321286, 0.167830022983253, 0.17536176601424813, 0.33902697905432433, 0.07009757100604475, 0.8965894590364769, 0.07004636607598513, 0.2425253950059414, 0.059888896998018026, 0.11198855889961123, 0.15577246609609574, 0.05956713098566979, 0.10292874195147306, 0.1228866579476744, 0.07721672405023128, 0.17410763597581536, 0.06400257896166295, 0.11186883097980171, 0.08337332191877067, 0.1846981009002775, 0.12040967994835228, 0.1460047389846295, 0.05812757299281657, 0.11405906802974641, 0.12657405505888164, 0.11515477497596294, 0.2016636369517073, 0.061708244029432535, 0.13510499394033104, 0.09982037101872265, 0.16448401100933552, 0.0581976140383631, 0.1094035079004243, 0.2507846358930692, 0.10248585604131222, 0.08696549001615494, 0.07196963799651712, 0.07713989308103919, 0.09002470201812685, 0.06425190297886729, 0.07696220302022994, 0.1323920110007748, 0.15458476601634175, 0.15049829299096018, 0.09106910694390535, 0.12094854400493205, 0.059014888014644384, 0.08546614099759609, 0.09594767598900944, 0.10268657491542399, 0.1285738479346037, 0.10195122100412846, 0.22005064494442195, 0.07698672905098647, 0.13713834597729146, 0.06202966195996851, 0.08744245197158307, 0.07543051394168288, 0.06864873203448951, 0.11917406995780766, 0.05538000795058906, 1.9741497099166736, 0.07133440801408142, 0.07157575897872448, 0.0843239719979465, 0.21619140799157321, 0.05877910496201366, 0.09361600794363767, 0.34461629297584295, 0.099300964968279, 0.08926645503379405, 0.09799770300742239, 0.11837517295498401, 0.07990865700412542, 0.05152112594805658, 0.0514357479987666, 0.05156436294782907, 0.05193865392357111, 0.05703437398187816, 0.05915492807980627, 0.05425220006145537, 0.054100556997582316, 0.057964115985669196, 0.0514782730024308, 0.05231048399582505, 0.05208674492314458, 0.05430261103902012, 0.05570248095318675, 0.05526026606094092, 0.055585702997632325, 0.05519061803352088, 0.05557457299437374, 0.055661931983195245, 0.05512534605804831, 0.05538325000088662, 0.05550745700020343, 0.056309250998310745, 0.054845661972649395, 0.053952745045535266, 0.05461416102480143, 0.054255377035588026, 0.05443820497021079, 0.05442592909093946, 0.054007646976970136, 0.05651397898327559, 0.05680014495737851, 0.05768093699589372, 0.05622402299195528, 0.061722186976112425, 0.05996127799153328, 0.056690116063691676, 0.0562205909518525, 0.05573432601522654, 0.05660843406803906, 0.055756881018169224, 0.05611609108746052, 0.054147830000147223, 0.055042182095348835, 0.054157169070094824, 0.05509230098687112, 0.05429840390570462, 0.05455181410070509, 0.05402524501550943, 0.053700397023931146, 0.050499287084676325, 0.050776915973983705, 0.051300455001182854, 0.050944317947141826, 0.050474860006943345, 0.05103515600785613, 0.051126110018230975, 0.05123683204874396, 0.05123690399341285, 0.05194549506995827, 0.05177818494848907, 0.05158869095612317, 0.05348735908046365, 0.05378373898565769, 0.05423479701858014, 0.05292502592783421, 0.05376743001397699, 0.053373378003016114, 0.05350268096663058, 0.05614765302743763, 0.055482983007095754, 0.05561702896375209, 0.05507643602322787, 0.05560150800738484, 0.05539595091249794, 0.055030946037732065, 0.05525348300579935, 0.06006657995749265, 0.05570041202008724, 0.055255193961784244, 0.052363094058819115, 0.052071803947910666, 0.15113962604664266, 0.05202264105901122, 0.051952674984931946, 0.05245671502780169, 0.052896452019922435, 0.05213995394296944, 0.062385934055782855, 0.049803039990365505, 0.04976497299503535, 0.049497882020659745, 0.050676437094807625, 0.05080602201633155, 0.05309950897935778, 0.049681958043947816, 0.049585563014261425, 0.04977288993541151, 0.04990750795695931, 0.050289719947613776, 0.0503203549887985, 0.049952905043028295, 0.049973199027590454, 0.048768455046229064, 0.04923931497614831, 0.04946718504652381, 0.048924279981292784, 0.0493488700594753, 0.050036474014632404, 0.04925871605519205, 0.048936809063889086, 0.04940911498852074, 0.050355968065559864, 0.049881379934959114, 0.04933334200177342, 0.04913826391566545, 0.04874858201947063, 0.0497332070954144, 0.053993190987966955, 0.06002983800135553, 0.06884071999229491, 0.06143523100763559, 0.06607816996984184, 0.07782033493276685, 0.08375016809441149, 0.06686450494453311, 0.06986498704645783, 0.052977503975853324, 0.0781245119869709, 0.06375158799346536, 0.05164041894022375, 0.054464090964756906, 0.0917320690350607, 0.06784318503923714, 0.05778596398886293, 0.14218262291979045, 0.11660279205534607, 0.12892210099380463, 0.07827502593863755, 0.096331553068012, 0.19579167501069605, 0.08228144200984389, 0.05300959502346814, 0.057457961956970394, 0.0582221569493413, 0.05775490193627775, 0.05710625904612243, 0.05566775402985513, 0.11991198698524386, 0.07308812707196921, 0.06036927399691194, 0.05320386902894825, 0.05870612501166761, 0.13071565190330148, 0.11290581605862826, 0.052779011079110205, 0.07666961406357586, 0.0796097080456093, 0.10273654398042709, 0.06526618299540132, 0.10929115896578878, 0.061293395003303885, 0.09172445302829146, 0.0582438389537856, 0.06581926299259067, 0.10558567603584379, 0.17753876594360918, 0.11179922102019191, 0.0985566379968077, 0.07674257596954703, 0.07538602105341852, 0.08918779203668237, 0.05412575707305223, 0.10138165508396924, 0.08275311905890703, 0.12560061190743, 0.10790454596281052, 0.07675147510599345, 0.22338702902197838, 0.07961248198989779, 0.07333516702055931, 0.06549218809232116, 0.06332191301044077, 0.07226109004113823, 0.05710549105424434, 0.10117001400794834, 0.0753166601061821, 0.06707751902285963, 0.09802896797191352, 0.08331953396555036, 0.08489933703094721, 0.09497319196816534, 0.05431776400655508, 0.1047735590254888, 0.09471749106887728, 0.05209693394135684, 0.08037469699047506, 0.05370506690815091, 0.08296642499044538, 0.07177910499740392, 0.06363740703091025, 0.09138557605911046, 0.08486437099054456, 0.38710305094718933, 0.07170519500505179, 0.054521779995411634]
[0.0014702382746194912, 0.0015215430916710334, 0.002375849956561896, 0.003728461975697428, 0.003354471044572578, 0.004040899294936521, 0.0018648265686351806, 0.003151181069287387, 0.004628950706153939, 0.0013381169780834832, 0.002684613703009249, 0.0023657677494074133, 0.0014748834787910296, 0.0017142653867433016, 0.003421226090391759, 0.002335290091154589, 0.0016713607289024037, 0.0022538151586724616, 0.006743179613047026, 0.001337784638797695, 0.0032446358421690425, 0.002012918250825764, 0.004733778681838885, 0.003931695500134744, 0.0017656528877771714, 0.0014365335915830326, 0.0026125765236263924, 0.004006734726400199, 0.001423603590493175, 0.0024645426125392655, 0.0044469428169329394, 0.003245465544750914, 0.003152559159471738, 0.005389106023358181, 0.0019459116367877207, 0.002363526841773736, 0.0025675460241142323, 0.0036300294556316326, 0.0030332828649658372, 0.0036209381121972747, 0.009640555500730195, 0.003814318704164841, 0.003985494682142003, 0.0077051586148710076, 0.0015931266137737441, 0.02037703315991993, 0.0015919628653632985, 0.005511940795589577, 0.0013611112954095006, 0.0025451945204457097, 0.0035402833203658124, 0.0013537984314924952, 0.0023392895898062056, 0.0027928785897198727, 0.0017549255465961653, 0.003956991726723077, 0.0014546040673105215, 0.00254247343135913, 0.0018948482254266062, 0.004197684111369943, 0.0027365836351898247, 0.0033182895223779433, 0.0013210812043821948, 0.0025922515461306002, 0.0028766830695200374, 0.0026171539767264303, 0.004583264476175166, 0.0014024600915780122, 0.003070568044098433, 0.0022686447958800604, 0.003738272977484898, 0.0013226730463264341, 0.0024864433613732795, 0.005699650815751573, 0.002329224000938914, 0.001976488409458067, 0.0016356735908299345, 0.0017531793882054362, 0.0020460159549574282, 0.001460270522246984, 0.0017491409777324986, 0.0030089093409267, 0.0035132901367350396, 0.0034204157497945494, 0.0020697524305433035, 0.0027488305455666373, 0.0013412474548782814, 0.001942412295399911, 0.0021806289997502145, 0.002333785793532363, 0.0029221329076046295, 0.002317073204639283, 0.005001151021464135, 0.0017496983875224198, 0.003116780590392988, 0.0014097650445447389, 0.0019873284538996154, 0.0017143298623109745, 0.001560198455329307, 0.002771489999018783, 0.0012879071616416062, 0.0459104583701552, 0.0016589397212577073, 0.0016645525343889414, 0.001961022604603407, 0.005027707162594726, 0.001366955929349155, 0.002177116463805527, 0.008014332394787046, 0.002309324766704163, 0.00207596407055335, 0.002279016349009823, 0.0027529109989531163, 0.0018583408605610562, 0.001198165719722246, 0.001196180186017828, 0.001199171231344862, 0.0012078756726411886, 0.0013263807902762362, 0.0013756960018559597, 0.0012616790711966364, 0.0012581524883158678, 0.001348002697341144, 0.0011971691395914138, 0.0012165228836238384, 0.0012113196493754553, 0.0012628514195120958, 0.0012954065337950408, 0.0012851224665335098, 0.0012926907673867982, 0.0012835027449656017, 0.0012924319301017147, 0.0012944635344929126, 0.001281984792047635, 0.001287982558160154, 0.0012908710930279869, 0.0013095174650769941, 0.0012754805109918463, 0.0012547150010589598, 0.001270096768018638, 0.0012617529543160007, 0.001266004766749088, 0.0012657192811846387, 0.0012559917901620963, 0.001314278581006409, 0.0013209336036599654, 0.0013414171394393888, 0.0013075354184175646, 0.0014353996971188936, 0.001394448325384495, 0.001318374792178876, 0.0013074556035314534, 0.0012961471166331754, 0.0013164752108846293, 0.0012966716515853307, 0.0013050253741269887, 0.00125925186046854, 0.0012800507464034612, 0.00125946904814174, 0.0012812163020202586, 0.001262753579202433, 0.0012686468395512812, 0.0012564010468723123, 0.0012488464424170035, 0.0011744020252250308, 0.001180858511022877, 0.0011930338372368105, 0.001184751580166089, 0.0011738339536498453, 0.0011868640932059566, 0.0011889793027495576, 0.00119155423369172, 0.0011915559068235548, 0.001208034769068797, 0.0012041438360113738, 0.0011997369989796087, 0.0012438920716386895, 0.0012507846275734346, 0.0012612743492693055, 0.0012308145564612607, 0.0012504053491622555, 0.0012412413489073515, 0.0012442483945728042, 0.0013057593727311076, 0.0012903019303975756, 0.0012934192782267928, 0.0012808473493773924, 0.0012930583257531358, 0.0012882779281976264, 0.001279789442737955, 0.0012849647210651013, 0.0013968972083137825, 0.0012953584190717963, 0.0012850045107391684, 0.0012177463734609096, 0.0012109721848351318, 0.003514875024340527, 0.0012098288618374703, 0.0012082017438356267, 0.0012199236052977138, 0.0012301500469749404, 0.0012125570684411498, 0.0014508356757158803, 0.0011582102323340815, 0.001157324953372915, 0.0011511135353641801, 0.0011785217929025029, 0.0011815353957286409, 0.0012348723018455298, 0.0011553943731150654, 0.0011531526282386377, 0.001157509068265384, 0.0011606397199292863, 0.0011695283708747389, 0.0011702408136929883, 0.001161695466116937, 0.0011621674192462896, 0.0011341501173541644, 0.0011451003482825187, 0.0011503996522447399, 0.0011377739530533206, 0.0011476481409180303, 0.0011636389305728467, 0.0011455515361672572, 0.001138065327067188, 0.0011490491857795522, 0.001171069024780462, 0.0011600320915106772, 0.0011472870232970562, 0.0011427503236201267, 0.0011336879539411775, 0.0011565862115212652, 0.0012556556043713245, 0.0013960427442175705, 0.001600946976564998, 0.0014287263025031533, 0.0015367016272056242, 0.001809775230994578, 0.0019476783277770115, 0.0015549884870821654, 0.0016247671406152983, 0.0012320349761826355, 0.0018168491159760674, 0.0014825950696154735, 0.0012009399753540408, 0.0012666067666222537, 0.0021333039310479232, 0.0015777484892845848, 0.0013438596276479752, 0.0033065726260416385, 0.0027116928384964202, 0.002998188395204759, 0.0018203494404334314, 0.002240268676000279, 0.004553294767690606, 0.0019135219072056718, 0.001232781279615538, 0.001336231673417916, 0.0013540036499846814, 0.0013431372543320406, 0.0013280525359563357, 0.0012945989309268635, 0.00278865086012195, 0.0016997238853946328, 0.0014039366045793475, 0.0012372992797429825, 0.0013652587212015723, 0.0030398988814721276, 0.0026257166525262385, 0.0012274188623048884, 0.0017830142805482759, 0.0018513885592002161, 0.002389221953033188, 0.0015178182091953795, 0.0025416548596695065, 0.001425427790774509, 0.0021331268146114295, 0.0013545078826461767, 0.0015306805347114108, 0.002455480838042879, 0.0041288085103164925, 0.0025999818841905093, 0.002292014837135063, 0.0017847110690592334, 0.0017531632803120586, 0.0020741346985274967, 0.0012587385365826099, 0.0023577129089295172, 0.0019244911409048147, 0.0029209444629634883, 0.002509408045646756, 0.0017849180257207779, 0.005195047186557637, 0.0018514530695325068, 0.0017054690004781234, 0.0015230741416818875, 0.0014726026281497854, 0.001680490466072982, 0.001328034675680101, 0.002352791023440659, 0.0017515502350274907, 0.0015599423028572008, 0.002279743441207291, 0.0019376635805941945, 0.001974403186766214, 0.002208678882980589, 0.001263203814105932, 0.0024365943959415996, 0.0022027323504390066, 0.0012115566032873683, 0.0018691789997784898, 0.0012489550443756025, 0.001929451743963846, 0.001669281511567533, 0.0014799396983932616, 0.002125245954863034, 0.00197359002303592, 0.009002396533655565, 0.0016675626745360883, 0.001267948371986317]
[680.1618603343785, 657.2275247898177, 420.90200066636567, 268.20710698355583, 298.1095936475488, 247.46966628271517, 536.2428961594401, 317.34133266614907, 216.03168049954698, 747.3188191904187, 372.4930700007512, 422.6957613444869, 678.0196635056932, 583.3402504263142, 292.2928720812753, 428.21232522148495, 598.3148836198325, 443.69210853520855, 148.29799254718833, 747.5044719445489, 308.20099655050933, 496.7911635704866, 211.24772981814598, 254.3431962026888, 566.3627358030305, 696.1201644425308, 382.7639079493633, 249.57978710470746, 702.4427352375338, 405.7548020927424, 224.87359095156992, 308.1222050307574, 317.2026120415663, 185.55953356005, 513.8979494725587, 423.0965277506806, 389.4769521590119, 275.4798582828574, 329.67581479126665, 276.17152489612016, 103.72846252731577, 262.17001712733224, 250.90988189765952, 129.78318162977118, 627.6965002996428, 49.07485756890869, 628.1553557292256, 181.42429991268372, 734.6937780713534, 392.89727836789535, 282.4632690404765, 738.6624010913869, 427.4802078193505, 358.05351642596844, 569.8247438129721, 252.71723295417024, 687.4722974265718, 393.3177777458347, 527.7467538461345, 238.22659673017728, 365.41912592802396, 301.360081227446, 756.9557394979753, 385.7650317511351, 347.6225833132344, 382.0944464455289, 218.18509605941873, 713.032767210385, 325.67263960229735, 440.79178980157474, 267.503204293229, 756.0447404423793, 402.18088838657206, 175.4493445872854, 429.3275355212287, 505.9478189776938, 611.3689220186063, 570.392286566639, 488.75474190562073, 684.8046199421016, 571.7092062506886, 332.3463377238261, 284.6334806066763, 292.36212003177275, 483.14957153474785, 363.79106802811754, 745.5745741495183, 514.823759285418, 458.5832803812787, 428.4883397487922, 342.2157826557361, 431.5789410527829, 199.95396973779856, 571.5270741124727, 320.8438871450724, 709.3380587564037, 503.1880855113608, 583.3183111282716, 640.9441033505783, 360.81674491123533, 776.4534818840275, 21.78152942707419, 602.7946568437463, 600.7620542700989, 509.9380280739996, 198.89782114595445, 731.5524798785044, 459.32315364150674, 124.77645682009071, 433.0270105002106, 481.70390527686214, 438.7857947725893, 363.2518451850722, 538.1144122817639, 834.6090891599002, 835.994452749693, 833.9092648833036, 827.899776980656, 753.9313048945294, 726.9047803082178, 792.5945851281756, 794.8162160681934, 741.8382781966543, 835.3038571820299, 822.0149521734853, 825.5459246579467, 791.8587923719095, 771.9584346007475, 778.1359567212304, 773.5802136357183, 779.1179285921982, 773.7351397077443, 772.5207959540826, 780.0404546162844, 776.4080294910758, 774.6706897389014, 763.6400633581502, 784.0182514606784, 796.9937389415252, 787.341583082688, 792.5481740140664, 789.8864413977296, 790.064601895026, 796.183548198943, 760.8736948556598, 757.0403215038657, 745.480261582109, 764.7976383004927, 696.670064796015, 717.1294782287952, 758.5096483430948, 764.8443261086555, 771.5173587683191, 759.6041245076175, 771.2052613916442, 766.2686257491056, 794.1223129326344, 781.2190280812573, 793.9853714352339, 780.5083329201879, 791.920146947126, 788.2414308096166, 795.9242015034947, 800.7389587983381, 851.4971692154455, 846.8415061291174, 838.1991933406542, 844.0588024873629, 851.9092473775044, 842.5564525242319, 841.0575337076633, 839.2400209109751, 839.2388424860368, 827.7907437803642, 830.465572379141, 833.5160129682693, 803.9282690198445, 799.4981533631689, 792.8489155268481, 812.4700790630229, 799.740660634552, 805.6450914081189, 803.6980432217769, 765.8378878096156, 775.012403253457, 773.1444991070068, 780.7331611265702, 773.3603195490464, 776.2300184705147, 781.3785350976335, 778.2314826286473, 715.8722875587363, 771.9871081832017, 778.2073849879123, 821.1890602128741, 825.782798748714, 284.5051369038714, 826.5631871942737, 827.6763422185956, 819.723461089973, 812.9089637960004, 824.7034519254332, 689.2579337122888, 863.4011098181644, 864.0615559921988, 868.7240391831792, 848.5205840251512, 846.3563627590778, 809.8003319901899, 865.5053402275919, 867.1878947433281, 863.9241172413246, 861.5938114378221, 855.0455251051827, 854.5249732354225, 860.8107969488617, 860.4612239504517, 881.7174946230924, 873.2859102696564, 869.2631278605921, 878.9092045185324, 871.3472050763546, 859.3731042564132, 872.9419571517159, 878.6841811418818, 870.2847644607725, 853.9206305003823, 862.045116956832, 871.6214684675984, 875.0817911230977, 882.0769388292239, 864.6134546984558, 796.3967162004386, 716.3104454659534, 624.630306086468, 699.9241199997387, 650.744414072382, 552.5548050795463, 513.431805313228, 643.0915780453364, 615.4728114585704, 811.6652687072426, 550.4034381318281, 674.4930025022682, 832.6810835863773, 789.5110197988031, 468.7564605521442, 633.8145824835749, 744.125338261855, 302.4279558006017, 368.77333074142723, 333.53474438076654, 549.3450750653109, 446.37503113482603, 219.62118663957963, 522.5965776688214, 811.1739012713314, 748.3732199238498, 738.5504463088513, 744.5255477611727, 752.9822600578796, 772.4400013864166, 358.5963428768093, 588.330851023974, 712.2828742681181, 808.2118985858657, 732.4619022538776, 328.9583104539752, 380.84840534407476, 814.7178039305721, 560.848003804266, 540.1351299437604, 418.5462965173538, 658.8404289405096, 393.4444506482012, 701.5437796793958, 468.795382042094, 738.2755115801855, 653.3041855063092, 407.2522108529431, 242.20062458729657, 384.61806448753185, 436.29735017333684, 560.3147855899865, 570.3975272753845, 482.12876468916704, 794.4461625167467, 424.1398501966189, 519.6178765103808, 342.355019987417, 398.50035618351075, 560.2498185294444, 192.49103311083186, 540.1163099707954, 586.3489748096581, 656.5668555673386, 679.0698188936584, 595.0643697115571, 752.9923866542786, 425.027123121894, 570.9228202548841, 641.0493504589198, 438.64585019726024, 516.0854598368128, 506.4821646878796, 452.75934301980124, 791.6378883860306, 410.4088894177888, 453.9816196010828, 825.3844659726647, 534.9942408503983, 800.669331136683, 518.2819436290281, 599.0601304036211, 675.7032067493549, 470.5337740847257, 506.6908467958949, 111.08153215218728, 599.6776104851336, 788.6756449187596]
Elapsed: 0.09537696899715228~0.12638200676441236
Time per graph: 0.002196573397887169~0.002920483485120536
Speed: 607.1143934154454~216.27061887781875
Total Time: 0.0561
best val loss: 0.17677944898605347 test_score: 0.8605

Testing...
Test loss: 0.4237 score: 0.7907 time: 0.08s
test Score 0.7907
Epoch Time List: [0.36352727690245956, 0.5321273959707469, 0.36697704903781414, 0.4469629409722984, 0.5816478339256719, 0.4315505011472851, 0.4163844969589263, 0.39802614715881646, 0.5728914530482143, 0.41522003698628396, 0.35061438800767064, 0.4225173940649256, 0.6283350280718878, 0.4584067548858002, 0.3856563330627978, 0.32642854703590274, 0.4772777180187404, 0.4968305059010163, 0.5760984711814672, 0.43658047216013074, 0.5878285599173978, 0.46762972604483366, 0.5549804440233856, 0.43818666390143335, 0.4092302630888298, 0.6361403070623055, 0.3924390790052712, 0.47047198796644807, 0.3758252840489149, 0.3206544261192903, 0.5869206987554207, 1.457119970000349, 0.5434779580682516, 0.7420723100658506, 0.4745497229741886, 0.33183669892605394, 0.3633680040948093, 0.5226953282253817, 0.38804091897327453, 0.5503016440197825, 0.8508459649747238, 0.7645577889634296, 0.6656546430895105, 0.7128302650526166, 0.7327678221045062, 1.5363611460197717, 0.474586385069415, 0.5688536108937114, 0.409268478048034, 0.4866577619686723, 0.38163493713364005, 0.5868648729519919, 0.5000793510116637, 0.5009536249563098, 0.3544112879317254, 0.5170912750763819, 0.5939841091167182, 0.5385150881484151, 0.6005392540246248, 0.5980050190119073, 0.578388512134552, 0.45746204804163426, 0.3384616761468351, 0.6761911220382899, 0.3865913460031152, 0.6842388298828155, 0.6343815339496359, 0.7268538549542427, 0.4375939769670367, 0.4069297999376431, 0.5906424077693373, 0.39035598491318524, 0.5541818781057373, 0.5722312839934602, 0.4175340619403869, 0.4181440939428285, 0.6186783188022673, 0.46482063096482307, 0.3247744250111282, 0.34680193895474076, 0.40887143195141107, 0.3885649968869984, 0.4055224269395694, 0.3477077210554853, 0.3095712539507076, 0.3770979300606996, 0.30177655606530607, 0.3431923760799691, 0.30124518694356084, 0.3338173540541902, 0.46806167194154114, 0.7257903119316325, 1.0405019300524145, 0.3809595570201054, 0.4744351841509342, 0.3602096471004188, 0.45847562700510025, 0.46117651101667434, 0.3722154780989513, 0.4289987019728869, 0.5758693440584466, 5.520177360158414, 1.0634270169539377, 2.8973690370330587, 0.44613897206727415, 0.6714173100190237, 0.44829268706962466, 0.3876839819131419, 1.888203123002313, 0.337775842868723, 0.406929649063386, 0.5683618830516934, 0.4154773319605738, 0.2776017241412774, 0.2481043168809265, 0.24545814003795385, 0.2456778499763459, 0.24629673396702856, 0.3773668259382248, 0.26508025999646634, 0.2557738679461181, 0.2554451118921861, 0.2582182699115947, 0.29088157205842435, 0.24335359607357532, 0.2461760510923341, 0.2539342959644273, 0.24582377402111888, 0.2584664688911289, 0.2616305019473657, 0.2584359759930521, 0.26043273508548737, 0.26061328290961683, 0.25970967614557594, 0.25949959887657315, 0.27184858196415007, 0.2693347439635545, 0.2662622359348461, 0.2546390931820497, 0.2520498260855675, 0.25126787181943655, 0.2512102120090276, 0.2543595328461379, 0.2536790670128539, 0.2622519520809874, 0.2892376579111442, 0.2703149380395189, 0.2683083269512281, 0.26355571299791336, 0.2665528111392632, 0.2674374261405319, 0.26479971793014556, 0.2628989510703832, 0.2666801770683378, 0.25917071604635566, 0.2622036269167438, 0.266917428933084, 0.256706882850267, 0.25546350597869605, 0.26016533793881536, 0.25343572616111487, 0.25796418997924775, 0.26104360807221383, 0.2531897989101708, 0.248871369054541, 0.2364588340278715, 0.23667120991740376, 0.23902089893817902, 0.23658119491301477, 0.2373186880722642, 0.236593073932454, 0.23755139915738255, 0.23701272788457572, 0.2464653920615092, 0.24475674785207957, 0.24871479312423617, 0.2679834308801219, 0.25812046008650213, 0.253964347881265, 0.24748450703918934, 0.2469820739934221, 0.24662855407223105, 0.25202749797608703, 0.2529052671743557, 0.25836284703109413, 0.26143586297985166, 0.2595934090204537, 0.2604291900061071, 0.257728929980658, 0.33468467299826443, 0.2597065760055557, 0.2720158010488376, 0.26281768397893757, 0.26388042501639575, 0.25252190802711993, 0.2513765689218417, 0.350886870874092, 0.25029482995159924, 0.2515430118655786, 0.2524426691234112, 0.2559083668747917, 0.25415271893143654, 0.2628777549834922, 0.2964296459686011, 0.2384395960252732, 0.24245849391445518, 0.2407805898692459, 0.24784173700027168, 0.256175025831908, 0.24856000510044396, 0.23806717502884567, 0.23820742405951023, 0.24074398900847882, 0.24097034404985607, 0.24051932501606643, 0.2412978099891916, 0.24030519474763423, 0.36594756692647934, 0.23631324397865683, 0.2372434779535979, 0.23666123300790787, 0.23846141307149082, 0.2386202939087525, 0.2376615790417418, 0.23653463798109442, 0.23644924093969166, 0.23968889901880175, 0.23931532201822847, 0.24045498203486204, 0.2392047889297828, 0.2363490789430216, 0.2385073599871248, 0.24087774602230638, 0.6432293989928439, 0.31547550903633237, 0.2842567670159042, 0.2856971900910139, 0.31632236018776894, 0.3186076879501343, 0.3461932569043711, 0.27891132701188326, 0.26825169299263507, 0.29483573290053755, 0.27317406504880637, 0.3037046669051051, 0.3690193931106478, 0.30679469811730087, 0.28923855698667467, 0.29070912790484726, 0.40846784005407244, 0.44104781898204237, 1.1854337089462206, 0.7783182020066306, 0.9700604821555316, 0.773361389990896, 0.3975260431179777, 0.30427982995752245, 0.48469814797863364, 0.26207557413727045, 0.3015659569064155, 0.26835884689353406, 0.28519264387432486, 0.37613906990736723, 0.3856096100062132, 0.4011270080227405, 0.30967851204331964, 0.45077092107385397, 0.6314280709484592, 0.35823601786978543, 0.4504597418708727, 0.4321129819145426, 0.3085987810045481, 0.5793447518954054, 0.30923433299176395, 0.574524657917209, 0.3396398340119049, 0.5432171409483999, 0.3017631300026551, 0.6381031430792063, 0.3607197511009872, 0.6367781659355387, 0.6840507370652631, 0.5996194540057331, 0.3298476729542017, 0.5442866268567741, 0.37218026095069945, 0.389302974101156, 0.3880102429538965, 0.32301299599930644, 0.5661217009183019, 0.4011757140979171, 0.37321882508695126, 0.44359744305256754, 0.3635762839112431, 0.3795394420158118, 0.46534052898641676, 0.4406676370417699, 0.29342503007501364, 0.3951387370470911, 0.45517058321274817, 0.5355013200314716, 0.45431676297448575, 0.3763714049709961, 0.3219761449145153, 0.5341123329708353, 0.3155307499691844, 0.5053849419346079, 0.3853292039129883, 0.33182654390111566, 0.3829374989727512, 0.48730938392691314, 0.44841943390201777, 0.38968896388541907, 0.5891280728392303, 0.4204483829671517, 0.4909213320352137, 0.33807380904909223, 0.7648713900707662, 0.32981947192456573, 0.2583678098162636]
Total Epoch List: [99, 95, 126]
Total Time List: [0.06931772094685584, 0.05599458294454962, 0.056135792983695865]
T-times Epoch Time: 0.41076385021905376 ~ 0.0077796041461037765
T-times Total Epoch: 75.1111111111111 ~ 23.033523609655195
T-times Total Time: 0.1013826764214577 ~ 0.03489789650786297
T-times Inference Elapsed: 0.09599492733059223 ~ 0.00043909295975456296
T-times Time Per Graph: 0.002216750672920103 ~ 1.4499227205178862e-05
T-times Speed: 557.8783646677915 ~ 34.855549840752474
T-times cross validation test micro f1 score:0.7390714228169263 ~ 0.08640351394159224
T-times cross validation test precision:0.7319236884200165 ~ 0.08888533254704307
T-times cross validation test recall:0.796055796055796 ~ 0.05724410954214309
T-times cross validation test f1_score:0.7390714228169263 ~ 0.056094232569313306
