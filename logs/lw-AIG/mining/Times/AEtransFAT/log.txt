Namespace(seed=15, model='AEtransGAT', dataset='mining/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Times/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bfa7a00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.9379;  Loss pred: 3.9379; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.33s
Epoch 2/1000, LR 0.000000
Train loss: 3.8441;  Loss pred: 3.8441; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 3.8877;  Loss pred: 3.8877; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 3.9379,   Val_Loss: 0.6924,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.6924,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6930


[0.33250294404570013, 0.14788256993051618, 0.051649183966219425]
[0.00755688509194773, 0.0033609674984208223, 0.0011738450901413505]
[132.3296553848032, 297.53337408643733, 851.9011651525358]
Elapsed: 0.17734489931414524~0.11653534593197526
Time per graph: 0.004030565893503301~0.002648530589363074
Speed: 427.25473154125876~307.75153652721724
Total Time: 0.0519
best val loss: 0.6923655271530151 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.04s
test Score 0.5000
Epoch Time List: [1.760885203955695, 1.4541658852249384, 0.2209522071061656]
Total Epoch List: [3]
Total Time List: [0.05192692100536078]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bf65c90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.9090;  Loss pred: 2.9090; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.8919;  Loss pred: 2.8919; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.9214;  Loss pred: 2.9214; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 2.8707;  Loss pred: 2.8707; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 2.8891;  Loss pred: 2.8891; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 2.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 2.00s
Epoch 6/1000, LR 0.000120
Train loss: 2.8427;  Loss pred: 2.8427; Loss self: 0.0000; time: 3.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 1.61s
Epoch 7/1000, LR 0.000150
Train loss: 2.7665;  Loss pred: 2.7665; Loss self: 0.0000; time: 1.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.10s
Epoch 8/1000, LR 0.000180
Train loss: 2.7127;  Loss pred: 2.7127; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 2.7630;  Loss pred: 2.7630; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.7095;  Loss pred: 2.7095; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 2.6567;  Loss pred: 2.6567; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 2.6279;  Loss pred: 2.6279; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 2.5404;  Loss pred: 2.5404; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 2.5368;  Loss pred: 2.5368; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 2.4812;  Loss pred: 2.4812; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 2.4921;  Loss pred: 2.4921; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 2.4032;  Loss pred: 2.4032; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 2.3729;  Loss pred: 2.3729; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 2.3007;  Loss pred: 2.3007; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 2.2179;  Loss pred: 2.2179; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 2.2484;  Loss pred: 2.2484; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 2.1978;  Loss pred: 2.1978; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 2.1496;  Loss pred: 2.1496; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 2.1287;  Loss pred: 2.1287; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 1.28s
Epoch 25/1000, LR 0.000270
Train loss: 2.0996;  Loss pred: 2.0996; Loss self: 0.0000; time: 2.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.21s
Epoch 26/1000, LR 0.000270
Train loss: 2.0830;  Loss pred: 2.0830; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.35s
Epoch 27/1000, LR 0.000270
Train loss: 1.9913;  Loss pred: 1.9913; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 2.0287;  Loss pred: 2.0287; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 1.9794;  Loss pred: 1.9794; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 1.9287;  Loss pred: 1.9287; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 1.8847;  Loss pred: 1.8847; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 1.8664;  Loss pred: 1.8664; Loss self: 0.0000; time: 0.13s
Val loss: 0.6927 score: 0.8182 time: 0.04s
Test loss: 0.6928 score: 0.6047 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 1.8294;  Loss pred: 1.8294; Loss self: 0.0000; time: 0.13s
Val loss: 0.6927 score: 0.5455 time: 0.04s
Test loss: 0.6928 score: 0.5581 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 1.8193;  Loss pred: 1.8193; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
Test loss: 0.6928 score: 0.5581 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 1.8195;  Loss pred: 1.8195; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.04s
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 1.7694;  Loss pred: 1.7694; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 1.7332;  Loss pred: 1.7332; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 1.7569;  Loss pred: 1.7569; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 1.7129;  Loss pred: 1.7129; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 1.6471;  Loss pred: 1.6471; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 1.6573;  Loss pred: 1.6573; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 1.6395;  Loss pred: 1.6395; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 1.6217;  Loss pred: 1.6217; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 1.5830;  Loss pred: 1.5830; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 1.5560;  Loss pred: 1.5560; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 1.5608;  Loss pred: 1.5608; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 1.5245;  Loss pred: 1.5245; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 1.5237;  Loss pred: 1.5237; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 1.4860;  Loss pred: 1.4860; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 1.4624;  Loss pred: 1.4624; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 1.51s
Epoch 51/1000, LR 0.000269
Train loss: 1.4757;  Loss pred: 1.4757; Loss self: 0.0000; time: 3.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.43s
Epoch 52/1000, LR 0.000269
Train loss: 1.4354;  Loss pred: 1.4354; Loss self: 0.0000; time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.69s
Epoch 53/1000, LR 0.000269
Train loss: 1.4270;  Loss pred: 1.4270; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 1.3991;  Loss pred: 1.3991; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.57s
Epoch 55/1000, LR 0.000269
Train loss: 1.4029;  Loss pred: 1.4029; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.16s
Epoch 56/1000, LR 0.000269
Train loss: 1.4078;  Loss pred: 1.4078; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.16s
Epoch 57/1000, LR 0.000269
Train loss: 1.3758;  Loss pred: 1.3758; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.04s
Test loss: 0.6925 score: 0.5116 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 1.3637;  Loss pred: 1.3637; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.04s
Test loss: 0.6924 score: 0.5116 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 1.3515;  Loss pred: 1.3515; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.04s
Test loss: 0.6924 score: 0.5116 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 1.3452;  Loss pred: 1.3452; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.04s
Test loss: 0.6924 score: 0.5116 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 1.3296;  Loss pred: 1.3296; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.04s
Test loss: 0.6923 score: 0.5116 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 1.3168;  Loss pred: 1.3168; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.05s
Test loss: 0.6922 score: 0.5116 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 1.2923;  Loss pred: 1.2923; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.04s
Test loss: 0.6922 score: 0.5116 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 1.2892;  Loss pred: 1.2892; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.04s
Test loss: 0.6921 score: 0.5116 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 1.2703;  Loss pred: 1.2703; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.04s
Test loss: 0.6921 score: 0.5116 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 1.2643;  Loss pred: 1.2643; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.05s
Test loss: 0.6920 score: 0.5116 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 1.2608;  Loss pred: 1.2608; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.04s
Test loss: 0.6920 score: 0.5116 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 1.2508;  Loss pred: 1.2508; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.04s
Test loss: 0.6919 score: 0.5116 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.2429;  Loss pred: 1.2429; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.04s
Test loss: 0.6919 score: 0.5116 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 1.2401;  Loss pred: 1.2401; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.04s
Test loss: 0.6918 score: 0.5116 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 1.2287;  Loss pred: 1.2287; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.04s
Test loss: 0.6918 score: 0.5116 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 1.2186;  Loss pred: 1.2186; Loss self: 0.0000; time: 2.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.20s
Test loss: 0.6917 score: 0.5116 time: 0.32s
Epoch 73/1000, LR 0.000267
Train loss: 1.2131;  Loss pred: 1.2131; Loss self: 0.0000; time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.21s
Test loss: 0.6916 score: 0.5116 time: 0.48s
Epoch 74/1000, LR 0.000267
Train loss: 1.2084;  Loss pred: 1.2084; Loss self: 0.0000; time: 2.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.31s
Test loss: 0.6916 score: 0.5116 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 1.2010;  Loss pred: 1.2010; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.05s
Test loss: 0.6915 score: 0.5116 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 1.1857;  Loss pred: 1.1857; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.06s
Test loss: 0.6914 score: 0.5116 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 1.1815;  Loss pred: 1.1815; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.05s
Test loss: 0.6914 score: 0.5116 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 1.1712;  Loss pred: 1.1712; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.04s
Test loss: 0.6913 score: 0.5116 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 1.1651;  Loss pred: 1.1651; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.04s
Test loss: 0.6912 score: 0.5116 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 1.1618;  Loss pred: 1.1618; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.04s
Test loss: 0.6912 score: 0.5116 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 1.1518;  Loss pred: 1.1518; Loss self: 0.0000; time: 0.13s
Val loss: 0.6902 score: 0.5227 time: 0.04s
Test loss: 0.6911 score: 0.5116 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 1.1543;  Loss pred: 1.1543; Loss self: 0.0000; time: 0.13s
Val loss: 0.6901 score: 0.5000 time: 0.04s
Test loss: 0.6910 score: 0.4884 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 1.1485;  Loss pred: 1.1485; Loss self: 0.0000; time: 0.13s
Val loss: 0.6900 score: 0.5000 time: 0.04s
Test loss: 0.6909 score: 0.4884 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.1385;  Loss pred: 1.1385; Loss self: 0.0000; time: 0.13s
Val loss: 0.6899 score: 0.5000 time: 0.04s
Test loss: 0.6909 score: 0.4884 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 1.1409;  Loss pred: 1.1409; Loss self: 0.0000; time: 0.13s
Val loss: 0.6898 score: 0.5000 time: 0.05s
Test loss: 0.6908 score: 0.4884 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 1.1309;  Loss pred: 1.1309; Loss self: 0.0000; time: 0.14s
Val loss: 0.6896 score: 0.5000 time: 0.04s
Test loss: 0.6907 score: 0.4884 time: 0.04s
Epoch 87/1000, LR 0.000266
Train loss: 1.1289;  Loss pred: 1.1289; Loss self: 0.0000; time: 0.14s
Val loss: 0.6895 score: 0.5000 time: 0.05s
Test loss: 0.6906 score: 0.4884 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 1.1170;  Loss pred: 1.1170; Loss self: 0.0000; time: 0.14s
Val loss: 0.6894 score: 0.5000 time: 0.04s
Test loss: 0.6904 score: 0.4884 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 1.1220;  Loss pred: 1.1220; Loss self: 0.0000; time: 0.14s
Val loss: 0.6892 score: 0.5000 time: 0.04s
Test loss: 0.6903 score: 0.4884 time: 0.04s
Epoch 90/1000, LR 0.000266
Train loss: 1.1121;  Loss pred: 1.1121; Loss self: 0.0000; time: 0.14s
Val loss: 0.6891 score: 0.5000 time: 0.04s
Test loss: 0.6902 score: 0.4884 time: 0.04s
Epoch 91/1000, LR 0.000266
Train loss: 1.1109;  Loss pred: 1.1109; Loss self: 0.0000; time: 0.14s
Val loss: 0.6890 score: 0.5227 time: 0.04s
Test loss: 0.6901 score: 0.4884 time: 0.04s
Epoch 92/1000, LR 0.000266
Train loss: 1.1012;  Loss pred: 1.1012; Loss self: 0.0000; time: 0.14s
Val loss: 0.6888 score: 0.5227 time: 0.04s
Test loss: 0.6900 score: 0.4884 time: 0.04s
Epoch 93/1000, LR 0.000265
Train loss: 1.1016;  Loss pred: 1.1016; Loss self: 0.0000; time: 0.14s
Val loss: 0.6887 score: 0.5227 time: 0.04s
Test loss: 0.6899 score: 0.4884 time: 0.04s
Epoch 94/1000, LR 0.000265
Train loss: 1.1018;  Loss pred: 1.1018; Loss self: 0.0000; time: 0.14s
Val loss: 0.6885 score: 0.5227 time: 0.04s
Test loss: 0.6898 score: 0.4884 time: 0.04s
Epoch 95/1000, LR 0.000265
Train loss: 1.0952;  Loss pred: 1.0952; Loss self: 0.0000; time: 0.14s
Val loss: 0.6884 score: 0.5227 time: 0.04s
Test loss: 0.6896 score: 0.4884 time: 0.04s
Epoch 96/1000, LR 0.000265
Train loss: 1.0901;  Loss pred: 1.0901; Loss self: 0.0000; time: 0.14s
Val loss: 0.6882 score: 0.5227 time: 0.04s
Test loss: 0.6895 score: 0.4884 time: 0.04s
Epoch 97/1000, LR 0.000265
Train loss: 1.0845;  Loss pred: 1.0845; Loss self: 0.0000; time: 2.15s
Val loss: 0.6880 score: 0.5227 time: 0.07s
Test loss: 0.6894 score: 0.4884 time: 0.88s
Epoch 98/1000, LR 0.000265
Train loss: 1.0814;  Loss pred: 1.0814; Loss self: 0.0000; time: 3.32s
Val loss: 0.6879 score: 0.5227 time: 0.44s
Test loss: 0.6893 score: 0.4884 time: 0.13s
Epoch 99/1000, LR 0.000265
Train loss: 1.0847;  Loss pred: 1.0847; Loss self: 0.0000; time: 0.68s
Val loss: 0.6877 score: 0.5227 time: 0.05s
Test loss: 0.6891 score: 0.4884 time: 0.04s
Epoch 100/1000, LR 0.000265
Train loss: 1.0729;  Loss pred: 1.0729; Loss self: 0.0000; time: 0.14s
Val loss: 0.6875 score: 0.5227 time: 0.04s
Test loss: 0.6890 score: 0.4884 time: 0.06s
Epoch 101/1000, LR 0.000265
Train loss: 1.0734;  Loss pred: 1.0734; Loss self: 0.0000; time: 0.13s
Val loss: 0.6872 score: 0.5227 time: 0.04s
Test loss: 0.6888 score: 0.4884 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 1.0726;  Loss pred: 1.0726; Loss self: 0.0000; time: 0.14s
Val loss: 0.6870 score: 0.5227 time: 0.05s
Test loss: 0.6886 score: 0.4884 time: 0.04s
Epoch 103/1000, LR 0.000264
Train loss: 1.0699;  Loss pred: 1.0699; Loss self: 0.0000; time: 0.13s
Val loss: 0.6868 score: 0.5455 time: 0.04s
Test loss: 0.6885 score: 0.4884 time: 0.04s
Epoch 104/1000, LR 0.000264
Train loss: 1.0635;  Loss pred: 1.0635; Loss self: 0.0000; time: 0.13s
Val loss: 0.6866 score: 0.5455 time: 0.04s
Test loss: 0.6883 score: 0.4884 time: 0.04s
Epoch 105/1000, LR 0.000264
Train loss: 1.0614;  Loss pred: 1.0614; Loss self: 0.0000; time: 0.13s
Val loss: 0.6864 score: 0.5455 time: 0.04s
Test loss: 0.6881 score: 0.4884 time: 0.04s
Epoch 106/1000, LR 0.000264
Train loss: 1.0592;  Loss pred: 1.0592; Loss self: 0.0000; time: 0.14s
Val loss: 0.6861 score: 0.5455 time: 0.11s
Test loss: 0.6879 score: 0.4884 time: 0.05s
Epoch 107/1000, LR 0.000264
Train loss: 1.0622;  Loss pred: 1.0622; Loss self: 0.0000; time: 0.13s
Val loss: 0.6859 score: 0.5455 time: 0.04s
Test loss: 0.6878 score: 0.4884 time: 0.04s
Epoch 108/1000, LR 0.000264
Train loss: 1.0535;  Loss pred: 1.0535; Loss self: 0.0000; time: 0.14s
Val loss: 0.6856 score: 0.5455 time: 0.04s
Test loss: 0.6876 score: 0.4884 time: 0.04s
Epoch 109/1000, LR 0.000264
Train loss: 1.0502;  Loss pred: 1.0502; Loss self: 0.0000; time: 0.13s
Val loss: 0.6854 score: 0.5455 time: 0.04s
Test loss: 0.6874 score: 0.4884 time: 0.04s
Epoch 110/1000, LR 0.000263
Train loss: 1.0548;  Loss pred: 1.0548; Loss self: 0.0000; time: 0.13s
Val loss: 0.6851 score: 0.5455 time: 0.04s
Test loss: 0.6872 score: 0.4884 time: 0.04s
Epoch 111/1000, LR 0.000263
Train loss: 1.0496;  Loss pred: 1.0496; Loss self: 0.0000; time: 0.13s
Val loss: 0.6849 score: 0.5455 time: 0.04s
Test loss: 0.6870 score: 0.4884 time: 0.04s
Epoch 112/1000, LR 0.000263
Train loss: 1.0440;  Loss pred: 1.0440; Loss self: 0.0000; time: 0.44s
Val loss: 0.6846 score: 0.5455 time: 0.06s
Test loss: 0.6868 score: 0.4884 time: 0.71s
Epoch 113/1000, LR 0.000263
Train loss: 1.0374;  Loss pred: 1.0374; Loss self: 0.0000; time: 1.91s
Val loss: 0.6843 score: 0.5682 time: 0.07s
Test loss: 0.6865 score: 0.4884 time: 0.50s
Epoch 114/1000, LR 0.000263
Train loss: 1.0398;  Loss pred: 1.0398; Loss self: 0.0000; time: 1.15s
Val loss: 0.6840 score: 0.5682 time: 0.26s
Test loss: 0.6863 score: 0.4884 time: 0.29s
Epoch 115/1000, LR 0.000263
Train loss: 1.0360;  Loss pred: 1.0360; Loss self: 0.0000; time: 0.44s
Val loss: 0.6837 score: 0.5682 time: 0.14s
Test loss: 0.6861 score: 0.4884 time: 0.21s
Epoch 116/1000, LR 0.000263
Train loss: 1.0352;  Loss pred: 1.0352; Loss self: 0.0000; time: 1.06s
Val loss: 0.6834 score: 0.5909 time: 0.26s
Test loss: 0.6858 score: 0.5116 time: 0.40s
Epoch 117/1000, LR 0.000262
Train loss: 1.0318;  Loss pred: 1.0318; Loss self: 0.0000; time: 0.65s
Val loss: 0.6830 score: 0.5909 time: 0.10s
Test loss: 0.6856 score: 0.5116 time: 0.27s
Epoch 118/1000, LR 0.000262
Train loss: 1.0321;  Loss pred: 1.0321; Loss self: 0.0000; time: 0.51s
Val loss: 0.6827 score: 0.6136 time: 0.05s
Test loss: 0.6853 score: 0.5116 time: 0.06s
Epoch 119/1000, LR 0.000262
Train loss: 1.0323;  Loss pred: 1.0323; Loss self: 0.0000; time: 0.14s
Val loss: 0.6823 score: 0.6136 time: 0.04s
Test loss: 0.6851 score: 0.5116 time: 0.05s
Epoch 120/1000, LR 0.000262
Train loss: 1.0285;  Loss pred: 1.0285; Loss self: 0.0000; time: 0.14s
Val loss: 0.6820 score: 0.6136 time: 0.04s
Test loss: 0.6848 score: 0.5116 time: 0.04s
Epoch 121/1000, LR 0.000262
Train loss: 1.0246;  Loss pred: 1.0246; Loss self: 0.0000; time: 0.14s
Val loss: 0.6816 score: 0.6136 time: 0.05s
Test loss: 0.6845 score: 0.5116 time: 0.04s
Epoch 122/1000, LR 0.000262
Train loss: 1.0223;  Loss pred: 1.0223; Loss self: 0.0000; time: 0.14s
Val loss: 0.6812 score: 0.6364 time: 0.04s
Test loss: 0.6842 score: 0.5116 time: 0.04s
Epoch 123/1000, LR 0.000262
Train loss: 1.0240;  Loss pred: 1.0240; Loss self: 0.0000; time: 0.14s
Val loss: 0.6808 score: 0.6364 time: 0.04s
Test loss: 0.6839 score: 0.5116 time: 0.04s
Epoch 124/1000, LR 0.000261
Train loss: 1.0221;  Loss pred: 1.0221; Loss self: 0.0000; time: 0.14s
Val loss: 0.6803 score: 0.6364 time: 0.04s
Test loss: 0.6836 score: 0.5116 time: 0.04s
Epoch 125/1000, LR 0.000261
Train loss: 1.0154;  Loss pred: 1.0154; Loss self: 0.0000; time: 0.13s
Val loss: 0.6798 score: 0.6591 time: 0.05s
Test loss: 0.6832 score: 0.5116 time: 0.04s
Epoch 126/1000, LR 0.000261
Train loss: 1.0139;  Loss pred: 1.0139; Loss self: 0.0000; time: 0.13s
Val loss: 0.6793 score: 0.6591 time: 0.04s
Test loss: 0.6829 score: 0.5116 time: 0.04s
Epoch 127/1000, LR 0.000261
Train loss: 1.0174;  Loss pred: 1.0174; Loss self: 0.0000; time: 0.13s
Val loss: 0.6788 score: 0.6591 time: 0.04s
Test loss: 0.6825 score: 0.5116 time: 0.04s
Epoch 128/1000, LR 0.000261
Train loss: 1.0137;  Loss pred: 1.0137; Loss self: 0.0000; time: 0.13s
Val loss: 0.6783 score: 0.6818 time: 0.05s
Test loss: 0.6821 score: 0.5116 time: 0.04s
Epoch 129/1000, LR 0.000261
Train loss: 1.0124;  Loss pred: 1.0124; Loss self: 0.0000; time: 0.14s
Val loss: 0.6778 score: 0.6818 time: 0.05s
Test loss: 0.6817 score: 0.5581 time: 0.04s
Epoch 130/1000, LR 0.000260
Train loss: 1.0106;  Loss pred: 1.0106; Loss self: 0.0000; time: 0.14s
Val loss: 0.6772 score: 0.7045 time: 0.05s
Test loss: 0.6813 score: 0.5581 time: 0.04s
Epoch 131/1000, LR 0.000260
Train loss: 1.0080;  Loss pred: 1.0080; Loss self: 0.0000; time: 3.11s
Val loss: 0.6767 score: 0.7045 time: 0.44s
Test loss: 0.6809 score: 0.5814 time: 0.43s
Epoch 132/1000, LR 0.000260
Train loss: 1.0061;  Loss pred: 1.0061; Loss self: 0.0000; time: 1.77s
Val loss: 0.6761 score: 0.7045 time: 0.16s
Test loss: 0.6805 score: 0.6047 time: 0.55s
Epoch 133/1000, LR 0.000260
Train loss: 1.0057;  Loss pred: 1.0057; Loss self: 0.0000; time: 1.71s
Val loss: 0.6755 score: 0.7045 time: 0.05s
Test loss: 0.6800 score: 0.6047 time: 0.04s
Epoch 134/1000, LR 0.000260
Train loss: 1.0015;  Loss pred: 1.0015; Loss self: 0.0000; time: 0.14s
Val loss: 0.6749 score: 0.7045 time: 0.04s
Test loss: 0.6795 score: 0.6047 time: 0.04s
Epoch 135/1000, LR 0.000260
Train loss: 0.9996;  Loss pred: 0.9996; Loss self: 0.0000; time: 0.13s
Val loss: 0.6743 score: 0.7273 time: 0.04s
Test loss: 0.6791 score: 0.6047 time: 0.04s
Epoch 136/1000, LR 0.000260
Train loss: 0.9974;  Loss pred: 0.9974; Loss self: 0.0000; time: 0.13s
Val loss: 0.6736 score: 0.7273 time: 0.04s
Test loss: 0.6786 score: 0.6047 time: 0.04s
Epoch 137/1000, LR 0.000259
Train loss: 0.9943;  Loss pred: 0.9943; Loss self: 0.0000; time: 0.13s
Val loss: 0.6730 score: 0.7273 time: 0.04s
Test loss: 0.6781 score: 0.6047 time: 0.04s
Epoch 138/1000, LR 0.000259
Train loss: 0.9977;  Loss pred: 0.9977; Loss self: 0.0000; time: 0.13s
Val loss: 0.6723 score: 0.7500 time: 0.04s
Test loss: 0.6776 score: 0.6047 time: 0.04s
Epoch 139/1000, LR 0.000259
Train loss: 0.9911;  Loss pred: 0.9911; Loss self: 0.0000; time: 0.13s
Val loss: 0.6717 score: 0.7500 time: 0.04s
Test loss: 0.6771 score: 0.6047 time: 0.04s
Epoch 140/1000, LR 0.000259
Train loss: 0.9942;  Loss pred: 0.9942; Loss self: 0.0000; time: 0.15s
Val loss: 0.6710 score: 0.7500 time: 0.04s
Test loss: 0.6766 score: 0.6047 time: 0.04s
Epoch 141/1000, LR 0.000259
Train loss: 0.9902;  Loss pred: 0.9902; Loss self: 0.0000; time: 0.13s
Val loss: 0.6703 score: 0.7500 time: 0.04s
Test loss: 0.6761 score: 0.6279 time: 0.04s
Epoch 142/1000, LR 0.000259
Train loss: 0.9909;  Loss pred: 0.9909; Loss self: 0.0000; time: 0.13s
Val loss: 0.6696 score: 0.7500 time: 0.04s
Test loss: 0.6756 score: 0.6279 time: 0.04s
Epoch 143/1000, LR 0.000258
Train loss: 0.9891;  Loss pred: 0.9891; Loss self: 0.0000; time: 0.13s
Val loss: 0.6688 score: 0.7727 time: 0.04s
Test loss: 0.6751 score: 0.6279 time: 0.04s
Epoch 144/1000, LR 0.000258
Train loss: 0.9854;  Loss pred: 0.9854; Loss self: 0.0000; time: 0.14s
Val loss: 0.6680 score: 0.7955 time: 0.04s
Test loss: 0.6745 score: 0.6279 time: 0.05s
Epoch 145/1000, LR 0.000258
Train loss: 0.9847;  Loss pred: 0.9847; Loss self: 0.0000; time: 0.14s
Val loss: 0.6672 score: 0.7955 time: 0.05s
Test loss: 0.6739 score: 0.6279 time: 0.04s
Epoch 146/1000, LR 0.000258
Train loss: 0.9853;  Loss pred: 0.9853; Loss self: 0.0000; time: 0.13s
Val loss: 0.6664 score: 0.8182 time: 0.05s
Test loss: 0.6733 score: 0.6512 time: 0.04s
Epoch 147/1000, LR 0.000258
Train loss: 0.9829;  Loss pred: 0.9829; Loss self: 0.0000; time: 0.13s
Val loss: 0.6656 score: 0.8182 time: 0.04s
Test loss: 0.6727 score: 0.6512 time: 0.04s
Epoch 148/1000, LR 0.000257
Train loss: 0.9782;  Loss pred: 0.9782; Loss self: 0.0000; time: 0.13s
Val loss: 0.6647 score: 0.8182 time: 0.04s
Test loss: 0.6720 score: 0.6744 time: 0.04s
Epoch 149/1000, LR 0.000257
Train loss: 0.9783;  Loss pred: 0.9783; Loss self: 0.0000; time: 0.13s
Val loss: 0.6638 score: 0.8182 time: 0.05s
Test loss: 0.6714 score: 0.6744 time: 0.04s
Epoch 150/1000, LR 0.000257
Train loss: 0.9757;  Loss pred: 0.9757; Loss self: 0.0000; time: 0.13s
Val loss: 0.6629 score: 0.8182 time: 0.04s
Test loss: 0.6707 score: 0.6744 time: 0.04s
Epoch 151/1000, LR 0.000257
Train loss: 0.9790;  Loss pred: 0.9790; Loss self: 0.0000; time: 0.13s
Val loss: 0.6620 score: 0.8182 time: 0.04s
Test loss: 0.6700 score: 0.6744 time: 0.04s
Epoch 152/1000, LR 0.000257
Train loss: 0.9749;  Loss pred: 0.9749; Loss self: 0.0000; time: 0.13s
Val loss: 0.6610 score: 0.8182 time: 0.04s
Test loss: 0.6693 score: 0.6744 time: 0.04s
Epoch 153/1000, LR 0.000257
Train loss: 0.9715;  Loss pred: 0.9715; Loss self: 0.0000; time: 0.13s
Val loss: 0.6600 score: 0.8182 time: 0.04s
Test loss: 0.6686 score: 0.6744 time: 0.04s
Epoch 154/1000, LR 0.000256
Train loss: 0.9699;  Loss pred: 0.9699; Loss self: 0.0000; time: 0.14s
Val loss: 0.6590 score: 0.8182 time: 0.04s
Test loss: 0.6679 score: 0.6744 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9676;  Loss pred: 0.9676; Loss self: 0.0000; time: 2.29s
Val loss: 0.6580 score: 0.8182 time: 0.58s
Test loss: 0.6671 score: 0.6977 time: 0.35s
Epoch 156/1000, LR 0.000256
Train loss: 0.9675;  Loss pred: 0.9675; Loss self: 0.0000; time: 0.44s
Val loss: 0.6569 score: 0.8182 time: 0.06s
Test loss: 0.6663 score: 0.6977 time: 0.10s
Epoch 157/1000, LR 0.000256
Train loss: 0.9673;  Loss pred: 0.9673; Loss self: 0.0000; time: 0.34s
Val loss: 0.6558 score: 0.8182 time: 0.06s
Test loss: 0.6654 score: 0.6977 time: 0.12s
Epoch 158/1000, LR 0.000256
Train loss: 0.9663;  Loss pred: 0.9663; Loss self: 0.0000; time: 0.14s
Val loss: 0.6547 score: 0.8182 time: 0.04s
Test loss: 0.6646 score: 0.6977 time: 0.04s
Epoch 159/1000, LR 0.000255
Train loss: 0.9625;  Loss pred: 0.9625; Loss self: 0.0000; time: 0.14s
Val loss: 0.6535 score: 0.8182 time: 0.04s
Test loss: 0.6637 score: 0.7209 time: 0.04s
Epoch 160/1000, LR 0.000255
Train loss: 0.9618;  Loss pred: 0.9618; Loss self: 0.0000; time: 0.14s
Val loss: 0.6524 score: 0.8182 time: 0.05s
Test loss: 0.6628 score: 0.7209 time: 0.04s
Epoch 161/1000, LR 0.000255
Train loss: 0.9571;  Loss pred: 0.9571; Loss self: 0.0000; time: 0.14s
Val loss: 0.6512 score: 0.8182 time: 0.04s
Test loss: 0.6620 score: 0.7209 time: 0.04s
Epoch 162/1000, LR 0.000255
Train loss: 0.9582;  Loss pred: 0.9582; Loss self: 0.0000; time: 0.13s
Val loss: 0.6500 score: 0.8182 time: 0.04s
Test loss: 0.6611 score: 0.7209 time: 0.04s
Epoch 163/1000, LR 0.000255
Train loss: 0.9560;  Loss pred: 0.9560; Loss self: 0.0000; time: 0.13s
Val loss: 0.6487 score: 0.8182 time: 0.04s
Test loss: 0.6603 score: 0.7209 time: 0.04s
Epoch 164/1000, LR 0.000254
Train loss: 0.9543;  Loss pred: 0.9543; Loss self: 0.0000; time: 0.13s
Val loss: 0.6475 score: 0.8182 time: 0.04s
Test loss: 0.6593 score: 0.7209 time: 0.04s
Epoch 165/1000, LR 0.000254
Train loss: 0.9508;  Loss pred: 0.9508; Loss self: 0.0000; time: 0.13s
Val loss: 0.6462 score: 0.8182 time: 0.06s
Test loss: 0.6584 score: 0.7209 time: 0.06s
Epoch 166/1000, LR 0.000254
Train loss: 0.9494;  Loss pred: 0.9494; Loss self: 0.0000; time: 0.16s
Val loss: 0.6448 score: 0.8182 time: 0.04s
Test loss: 0.6574 score: 0.7209 time: 0.04s
Epoch 167/1000, LR 0.000254
Train loss: 0.9487;  Loss pred: 0.9487; Loss self: 0.0000; time: 0.14s
Val loss: 0.6435 score: 0.8182 time: 0.05s
Test loss: 0.6564 score: 0.7209 time: 0.04s
Epoch 168/1000, LR 0.000254
Train loss: 0.9485;  Loss pred: 0.9485; Loss self: 0.0000; time: 0.14s
Val loss: 0.6421 score: 0.8409 time: 0.05s
Test loss: 0.6554 score: 0.7209 time: 0.04s
Epoch 169/1000, LR 0.000253
Train loss: 0.9457;  Loss pred: 0.9457; Loss self: 0.0000; time: 0.14s
Val loss: 0.6406 score: 0.8409 time: 0.05s
Test loss: 0.6543 score: 0.7209 time: 0.04s
Epoch 170/1000, LR 0.000253
Train loss: 0.9408;  Loss pred: 0.9408; Loss self: 0.0000; time: 0.14s
Val loss: 0.6392 score: 0.8409 time: 0.05s
Test loss: 0.6532 score: 0.7209 time: 0.04s
Epoch 171/1000, LR 0.000253
Train loss: 0.9403;  Loss pred: 0.9403; Loss self: 0.0000; time: 0.14s
Val loss: 0.6377 score: 0.8409 time: 0.04s
Test loss: 0.6521 score: 0.7209 time: 0.04s
Epoch 172/1000, LR 0.000253
Train loss: 0.9383;  Loss pred: 0.9383; Loss self: 0.0000; time: 0.13s
Val loss: 0.6362 score: 0.8409 time: 0.04s
Test loss: 0.6510 score: 0.7442 time: 0.04s
Epoch 173/1000, LR 0.000253
Train loss: 0.9367;  Loss pred: 0.9367; Loss self: 0.0000; time: 0.13s
Val loss: 0.6346 score: 0.8409 time: 0.04s
Test loss: 0.6498 score: 0.7674 time: 0.04s
Epoch 174/1000, LR 0.000252
Train loss: 0.9335;  Loss pred: 0.9335; Loss self: 0.0000; time: 0.13s
Val loss: 0.6331 score: 0.8409 time: 0.05s
Test loss: 0.6487 score: 0.7907 time: 0.06s
Epoch 175/1000, LR 0.000252
Train loss: 0.9336;  Loss pred: 0.9336; Loss self: 0.0000; time: 0.16s
Val loss: 0.6315 score: 0.8409 time: 0.05s
Test loss: 0.6475 score: 0.7907 time: 0.04s
Epoch 176/1000, LR 0.000252
Train loss: 0.9328;  Loss pred: 0.9328; Loss self: 0.0000; time: 0.13s
Val loss: 0.6299 score: 0.8409 time: 0.04s
Test loss: 0.6464 score: 0.7907 time: 0.04s
Epoch 177/1000, LR 0.000252
Train loss: 0.9293;  Loss pred: 0.9293; Loss self: 0.0000; time: 3.78s
Val loss: 0.6282 score: 0.8409 time: 0.94s
Test loss: 0.6452 score: 0.7907 time: 0.89s
Epoch 178/1000, LR 0.000251
Train loss: 0.9264;  Loss pred: 0.9264; Loss self: 0.0000; time: 2.69s
Val loss: 0.6266 score: 0.8409 time: 0.05s
Test loss: 0.6440 score: 0.7907 time: 0.33s
Epoch 179/1000, LR 0.000251
Train loss: 0.9255;  Loss pred: 0.9255; Loss self: 0.0000; time: 2.14s
Val loss: 0.6249 score: 0.8409 time: 0.19s
Test loss: 0.6428 score: 0.7907 time: 0.54s
Epoch 180/1000, LR 0.000251
Train loss: 0.9234;  Loss pred: 0.9234; Loss self: 0.0000; time: 0.34s
Val loss: 0.6232 score: 0.8409 time: 0.04s
Test loss: 0.6415 score: 0.7907 time: 0.04s
Epoch 181/1000, LR 0.000251
Train loss: 0.9197;  Loss pred: 0.9197; Loss self: 0.0000; time: 0.13s
Val loss: 0.6214 score: 0.8409 time: 0.04s
Test loss: 0.6402 score: 0.7907 time: 0.04s
Epoch 182/1000, LR 0.000251
Train loss: 0.9191;  Loss pred: 0.9191; Loss self: 0.0000; time: 0.13s
Val loss: 0.6196 score: 0.8409 time: 0.04s
Test loss: 0.6389 score: 0.7907 time: 0.04s
Epoch 183/1000, LR 0.000250
Train loss: 0.9172;  Loss pred: 0.9172; Loss self: 0.0000; time: 0.13s
Val loss: 0.6178 score: 0.8182 time: 0.04s
Test loss: 0.6376 score: 0.7674 time: 0.04s
Epoch 184/1000, LR 0.000250
Train loss: 0.9152;  Loss pred: 0.9152; Loss self: 0.0000; time: 0.13s
Val loss: 0.6160 score: 0.8182 time: 0.04s
Test loss: 0.6362 score: 0.7674 time: 0.04s
Epoch 185/1000, LR 0.000250
Train loss: 0.9128;  Loss pred: 0.9128; Loss self: 0.0000; time: 0.13s
Val loss: 0.6141 score: 0.8182 time: 0.04s
Test loss: 0.6349 score: 0.7674 time: 0.04s
Epoch 186/1000, LR 0.000250
Train loss: 0.9103;  Loss pred: 0.9103; Loss self: 0.0000; time: 0.13s
Val loss: 0.6122 score: 0.8182 time: 0.04s
Test loss: 0.6335 score: 0.7674 time: 0.04s
Epoch 187/1000, LR 0.000249
Train loss: 0.9082;  Loss pred: 0.9082; Loss self: 0.0000; time: 0.13s
Val loss: 0.6103 score: 0.8182 time: 0.04s
Test loss: 0.6321 score: 0.7674 time: 0.04s
Epoch 188/1000, LR 0.000249
Train loss: 0.9083;  Loss pred: 0.9083; Loss self: 0.0000; time: 0.14s
Val loss: 0.6084 score: 0.8182 time: 0.05s
Test loss: 0.6307 score: 0.7674 time: 0.04s
Epoch 189/1000, LR 0.000249
Train loss: 0.9047;  Loss pred: 0.9047; Loss self: 0.0000; time: 0.14s
Val loss: 0.6065 score: 0.8182 time: 0.04s
Test loss: 0.6293 score: 0.7674 time: 0.04s
Epoch 190/1000, LR 0.000249
Train loss: 0.9006;  Loss pred: 0.9006; Loss self: 0.0000; time: 0.14s
Val loss: 0.6045 score: 0.8182 time: 0.04s
Test loss: 0.6280 score: 0.7674 time: 0.04s
Epoch 191/1000, LR 0.000249
Train loss: 0.9009;  Loss pred: 0.9009; Loss self: 0.0000; time: 0.13s
Val loss: 0.6026 score: 0.8182 time: 0.04s
Test loss: 0.6266 score: 0.7674 time: 0.04s
Epoch 192/1000, LR 0.000248
Train loss: 0.8979;  Loss pred: 0.8979; Loss self: 0.0000; time: 0.13s
Val loss: 0.6006 score: 0.8182 time: 0.04s
Test loss: 0.6252 score: 0.7674 time: 0.04s
Epoch 193/1000, LR 0.000248
Train loss: 0.8949;  Loss pred: 0.8949; Loss self: 0.0000; time: 0.13s
Val loss: 0.5985 score: 0.8182 time: 0.04s
Test loss: 0.6238 score: 0.7674 time: 0.04s
Epoch 194/1000, LR 0.000248
Train loss: 0.8919;  Loss pred: 0.8919; Loss self: 0.0000; time: 0.13s
Val loss: 0.5965 score: 0.8182 time: 0.04s
Test loss: 0.6223 score: 0.7674 time: 0.04s
Epoch 195/1000, LR 0.000248
Train loss: 0.8898;  Loss pred: 0.8898; Loss self: 0.0000; time: 0.13s
Val loss: 0.5944 score: 0.8182 time: 0.04s
Test loss: 0.6208 score: 0.7674 time: 0.19s
Epoch 196/1000, LR 0.000247
Train loss: 0.8879;  Loss pred: 0.8879; Loss self: 0.0000; time: 5.49s
Val loss: 0.5923 score: 0.8182 time: 0.18s
Test loss: 0.6192 score: 0.7907 time: 0.69s
Epoch 197/1000, LR 0.000247
Train loss: 0.8847;  Loss pred: 0.8847; Loss self: 0.0000; time: 0.72s
Val loss: 0.5901 score: 0.8182 time: 0.07s
Test loss: 0.6176 score: 0.7907 time: 0.71s
Epoch 198/1000, LR 0.000247
Train loss: 0.8822;  Loss pred: 0.8822; Loss self: 0.0000; time: 0.34s
Val loss: 0.5879 score: 0.8182 time: 0.94s
Test loss: 0.6159 score: 0.7907 time: 0.49s
Epoch 199/1000, LR 0.000247
Train loss: 0.8813;  Loss pred: 0.8813; Loss self: 0.0000; time: 0.79s
Val loss: 0.5857 score: 0.8409 time: 0.05s
Test loss: 0.6142 score: 0.7907 time: 0.04s
Epoch 200/1000, LR 0.000246
Train loss: 0.8794;  Loss pred: 0.8794; Loss self: 0.0000; time: 0.13s
Val loss: 0.5835 score: 0.8409 time: 0.04s
Test loss: 0.6125 score: 0.7907 time: 0.04s
Epoch 201/1000, LR 0.000246
Train loss: 0.8762;  Loss pred: 0.8762; Loss self: 0.0000; time: 0.13s
Val loss: 0.5813 score: 0.8409 time: 0.04s
Test loss: 0.6109 score: 0.8140 time: 0.04s
Epoch 202/1000, LR 0.000246
Train loss: 0.8730;  Loss pred: 0.8730; Loss self: 0.0000; time: 0.13s
Val loss: 0.5791 score: 0.8409 time: 0.04s
Test loss: 0.6093 score: 0.8140 time: 0.04s
Epoch 203/1000, LR 0.000246
Train loss: 0.8715;  Loss pred: 0.8715; Loss self: 0.0000; time: 0.13s
Val loss: 0.5769 score: 0.8409 time: 0.04s
Test loss: 0.6078 score: 0.8140 time: 0.04s
Epoch 204/1000, LR 0.000245
Train loss: 0.8693;  Loss pred: 0.8693; Loss self: 0.0000; time: 0.13s
Val loss: 0.5746 score: 0.8409 time: 0.04s
Test loss: 0.6062 score: 0.8140 time: 0.04s
Epoch 205/1000, LR 0.000245
Train loss: 0.8655;  Loss pred: 0.8655; Loss self: 0.0000; time: 0.13s
Val loss: 0.5724 score: 0.8409 time: 0.04s
Test loss: 0.6047 score: 0.7907 time: 0.04s
Epoch 206/1000, LR 0.000245
Train loss: 0.8635;  Loss pred: 0.8635; Loss self: 0.0000; time: 0.13s
Val loss: 0.5702 score: 0.8409 time: 0.04s
Test loss: 0.6031 score: 0.7907 time: 0.04s
Epoch 207/1000, LR 0.000245
Train loss: 0.8606;  Loss pred: 0.8606; Loss self: 0.0000; time: 0.13s
Val loss: 0.5679 score: 0.8409 time: 0.04s
Test loss: 0.6016 score: 0.7907 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.8587;  Loss pred: 0.8587; Loss self: 0.0000; time: 0.13s
Val loss: 0.5657 score: 0.8409 time: 0.04s
Test loss: 0.6000 score: 0.7907 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.8539;  Loss pred: 0.8539; Loss self: 0.0000; time: 0.13s
Val loss: 0.5635 score: 0.8409 time: 0.04s
Test loss: 0.5985 score: 0.7907 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.8535;  Loss pred: 0.8535; Loss self: 0.0000; time: 0.13s
Val loss: 0.5612 score: 0.8409 time: 0.04s
Test loss: 0.5969 score: 0.7907 time: 0.04s
Epoch 211/1000, LR 0.000244
Train loss: 0.8522;  Loss pred: 0.8522; Loss self: 0.0000; time: 0.13s
Val loss: 0.5589 score: 0.8409 time: 0.04s
Test loss: 0.5953 score: 0.7674 time: 0.04s
Epoch 212/1000, LR 0.000243
Train loss: 0.8494;  Loss pred: 0.8494; Loss self: 0.0000; time: 0.13s
Val loss: 0.5566 score: 0.8409 time: 0.04s
Test loss: 0.5936 score: 0.7674 time: 0.04s
Epoch 213/1000, LR 0.000243
Train loss: 0.8474;  Loss pred: 0.8474; Loss self: 0.0000; time: 0.14s
Val loss: 0.5542 score: 0.8409 time: 1.79s
Test loss: 0.5918 score: 0.8140 time: 1.37s
Epoch 214/1000, LR 0.000243
Train loss: 0.8434;  Loss pred: 0.8434; Loss self: 0.0000; time: 0.99s
Val loss: 0.5518 score: 0.8409 time: 0.08s
Test loss: 0.5900 score: 0.8140 time: 0.22s
Epoch 215/1000, LR 0.000243
Train loss: 0.8389;  Loss pred: 0.8389; Loss self: 0.0000; time: 0.18s
Val loss: 0.5495 score: 0.8409 time: 0.04s
Test loss: 0.5882 score: 0.8140 time: 0.04s
Epoch 216/1000, LR 0.000242
Train loss: 0.8374;  Loss pred: 0.8374; Loss self: 0.0000; time: 0.14s
Val loss: 0.5471 score: 0.8409 time: 0.04s
Test loss: 0.5865 score: 0.8140 time: 0.04s
Epoch 217/1000, LR 0.000242
Train loss: 0.8355;  Loss pred: 0.8355; Loss self: 0.0000; time: 0.13s
Val loss: 0.5447 score: 0.8409 time: 0.04s
Test loss: 0.5848 score: 0.8140 time: 0.04s
Epoch 218/1000, LR 0.000242
Train loss: 0.8353;  Loss pred: 0.8353; Loss self: 0.0000; time: 0.13s
Val loss: 0.5423 score: 0.8409 time: 0.05s
Test loss: 0.5831 score: 0.8140 time: 0.05s
Epoch 219/1000, LR 0.000242
Train loss: 0.8291;  Loss pred: 0.8291; Loss self: 0.0000; time: 0.13s
Val loss: 0.5400 score: 0.8409 time: 0.05s
Test loss: 0.5814 score: 0.8140 time: 0.04s
Epoch 220/1000, LR 0.000241
Train loss: 0.8289;  Loss pred: 0.8289; Loss self: 0.0000; time: 0.13s
Val loss: 0.5376 score: 0.8409 time: 0.05s
Test loss: 0.5797 score: 0.8140 time: 0.04s
Epoch 221/1000, LR 0.000241
Train loss: 0.8255;  Loss pred: 0.8255; Loss self: 0.0000; time: 0.13s
Val loss: 0.5351 score: 0.8409 time: 0.04s
Test loss: 0.5778 score: 0.8140 time: 0.04s
Epoch 222/1000, LR 0.000241
Train loss: 0.8218;  Loss pred: 0.8218; Loss self: 0.0000; time: 0.13s
Val loss: 0.5327 score: 0.8409 time: 0.04s
Test loss: 0.5760 score: 0.8140 time: 0.04s
Epoch 223/1000, LR 0.000241
Train loss: 0.8196;  Loss pred: 0.8196; Loss self: 0.0000; time: 0.13s
Val loss: 0.5302 score: 0.8409 time: 0.04s
Test loss: 0.5741 score: 0.8140 time: 0.04s
Epoch 224/1000, LR 0.000240
Train loss: 0.8178;  Loss pred: 0.8178; Loss self: 0.0000; time: 0.13s
Val loss: 0.5278 score: 0.8409 time: 0.04s
Test loss: 0.5723 score: 0.8140 time: 0.04s
Epoch 225/1000, LR 0.000240
Train loss: 0.8159;  Loss pred: 0.8159; Loss self: 0.0000; time: 0.13s
Val loss: 0.5254 score: 0.8409 time: 0.04s
Test loss: 0.5704 score: 0.8140 time: 0.04s
Epoch 226/1000, LR 0.000240
Train loss: 0.8096;  Loss pred: 0.8096; Loss self: 0.0000; time: 0.14s
Val loss: 0.5230 score: 0.8409 time: 0.04s
Test loss: 0.5687 score: 0.8140 time: 0.04s
Epoch 227/1000, LR 0.000240
Train loss: 0.8103;  Loss pred: 0.8103; Loss self: 0.0000; time: 0.13s
Val loss: 0.5206 score: 0.8409 time: 0.04s
Test loss: 0.5671 score: 0.8140 time: 0.04s
Epoch 228/1000, LR 0.000239
Train loss: 0.8068;  Loss pred: 0.8068; Loss self: 0.0000; time: 0.13s
Val loss: 0.5183 score: 0.8409 time: 0.04s
Test loss: 0.5656 score: 0.7907 time: 0.04s
Epoch 229/1000, LR 0.000239
Train loss: 0.8059;  Loss pred: 0.8059; Loss self: 0.0000; time: 0.13s
Val loss: 0.5161 score: 0.8409 time: 0.04s
Test loss: 0.5641 score: 0.7907 time: 0.04s
Epoch 230/1000, LR 0.000239
Train loss: 0.8035;  Loss pred: 0.8035; Loss self: 0.0000; time: 0.13s
Val loss: 0.5138 score: 0.8409 time: 0.04s
Test loss: 0.5625 score: 0.7907 time: 0.04s
Epoch 231/1000, LR 0.000238
Train loss: 0.7995;  Loss pred: 0.7995; Loss self: 0.0000; time: 0.13s
Val loss: 0.5114 score: 0.8182 time: 0.04s
Test loss: 0.5608 score: 0.7907 time: 0.04s
Epoch 232/1000, LR 0.000238
Train loss: 0.7947;  Loss pred: 0.7947; Loss self: 0.0000; time: 0.13s
Val loss: 0.5091 score: 0.8182 time: 0.04s
Test loss: 0.5591 score: 0.7907 time: 0.04s
Epoch 233/1000, LR 0.000238
Train loss: 0.7955;  Loss pred: 0.7955; Loss self: 0.0000; time: 0.13s
Val loss: 0.5067 score: 0.8182 time: 0.04s
Test loss: 0.5573 score: 0.7907 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.7897;  Loss pred: 0.7897; Loss self: 0.0000; time: 0.13s
Val loss: 0.5043 score: 0.8182 time: 0.04s
Test loss: 0.5554 score: 0.7907 time: 0.04s
Epoch 235/1000, LR 0.000237
Train loss: 0.7883;  Loss pred: 0.7883; Loss self: 0.0000; time: 0.13s
Val loss: 0.5020 score: 0.8182 time: 0.04s
Test loss: 0.5537 score: 0.7907 time: 0.04s
Epoch 236/1000, LR 0.000237
Train loss: 0.7845;  Loss pred: 0.7845; Loss self: 0.0000; time: 0.13s
Val loss: 0.4997 score: 0.8182 time: 0.04s
Test loss: 0.5521 score: 0.7907 time: 0.04s
Epoch 237/1000, LR 0.000237
Train loss: 0.7828;  Loss pred: 0.7828; Loss self: 0.0000; time: 0.13s
Val loss: 0.4975 score: 0.8182 time: 0.04s
Test loss: 0.5505 score: 0.7907 time: 0.04s
Epoch 238/1000, LR 0.000236
Train loss: 0.7807;  Loss pred: 0.7807; Loss self: 0.0000; time: 0.13s
Val loss: 0.4952 score: 0.8182 time: 0.04s
Test loss: 0.5488 score: 0.7907 time: 0.04s
Epoch 239/1000, LR 0.000236
Train loss: 0.7756;  Loss pred: 0.7756; Loss self: 0.0000; time: 0.13s
Val loss: 0.4929 score: 0.8182 time: 0.04s
Test loss: 0.5472 score: 0.7907 time: 0.04s
Epoch 240/1000, LR 0.000236
Train loss: 0.7742;  Loss pred: 0.7742; Loss self: 0.0000; time: 0.13s
Val loss: 0.4907 score: 0.8182 time: 0.04s
Test loss: 0.5454 score: 0.7907 time: 0.04s
Epoch 241/1000, LR 0.000236
Train loss: 0.7710;  Loss pred: 0.7710; Loss self: 0.0000; time: 0.13s
Val loss: 0.4884 score: 0.8182 time: 0.04s
Test loss: 0.5438 score: 0.7907 time: 0.04s
Epoch 242/1000, LR 0.000235
Train loss: 0.7661;  Loss pred: 0.7661; Loss self: 0.0000; time: 0.13s
Val loss: 0.4862 score: 0.8182 time: 0.04s
Test loss: 0.5422 score: 0.7907 time: 0.04s
Epoch 243/1000, LR 0.000235
Train loss: 0.7669;  Loss pred: 0.7669; Loss self: 0.0000; time: 0.13s
Val loss: 0.4841 score: 0.8182 time: 0.04s
Test loss: 0.5406 score: 0.7907 time: 0.04s
Epoch 244/1000, LR 0.000235
Train loss: 0.7637;  Loss pred: 0.7637; Loss self: 0.0000; time: 0.13s
Val loss: 0.4820 score: 0.8182 time: 0.04s
Test loss: 0.5391 score: 0.7907 time: 0.04s
Epoch 245/1000, LR 0.000234
Train loss: 0.7603;  Loss pred: 0.7603; Loss self: 0.0000; time: 0.13s
Val loss: 0.4798 score: 0.8409 time: 0.04s
Test loss: 0.5375 score: 0.7907 time: 0.04s
Epoch 246/1000, LR 0.000234
Train loss: 0.7618;  Loss pred: 0.7618; Loss self: 0.0000; time: 0.13s
Val loss: 0.4777 score: 0.8636 time: 0.04s
Test loss: 0.5360 score: 0.8140 time: 0.04s
Epoch 247/1000, LR 0.000234
Train loss: 0.7564;  Loss pred: 0.7564; Loss self: 0.0000; time: 0.13s
Val loss: 0.4756 score: 0.8636 time: 0.04s
Test loss: 0.5345 score: 0.8140 time: 0.04s
Epoch 248/1000, LR 0.000234
Train loss: 0.7556;  Loss pred: 0.7556; Loss self: 0.0000; time: 0.13s
Val loss: 0.4736 score: 0.8636 time: 0.04s
Test loss: 0.5330 score: 0.8140 time: 0.04s
Epoch 249/1000, LR 0.000233
Train loss: 0.7516;  Loss pred: 0.7516; Loss self: 0.0000; time: 0.13s
Val loss: 0.4715 score: 0.8636 time: 0.04s
Test loss: 0.5315 score: 0.8140 time: 0.04s
Epoch 250/1000, LR 0.000233
Train loss: 0.7490;  Loss pred: 0.7490; Loss self: 0.0000; time: 0.13s
Val loss: 0.4695 score: 0.8636 time: 0.04s
Test loss: 0.5299 score: 0.8140 time: 0.04s
Epoch 251/1000, LR 0.000233
Train loss: 0.7466;  Loss pred: 0.7466; Loss self: 0.0000; time: 0.13s
Val loss: 0.4674 score: 0.8636 time: 0.04s
Test loss: 0.5284 score: 0.8140 time: 0.04s
Epoch 252/1000, LR 0.000232
Train loss: 0.7453;  Loss pred: 0.7453; Loss self: 0.0000; time: 0.13s
Val loss: 0.4655 score: 0.8636 time: 0.04s
Test loss: 0.5270 score: 0.8140 time: 0.04s
Epoch 253/1000, LR 0.000232
Train loss: 0.7414;  Loss pred: 0.7414; Loss self: 0.0000; time: 0.13s
Val loss: 0.4637 score: 0.8636 time: 0.04s
Test loss: 0.5258 score: 0.8140 time: 0.04s
Epoch 254/1000, LR 0.000232
Train loss: 0.7379;  Loss pred: 0.7379; Loss self: 0.0000; time: 0.13s
Val loss: 0.4619 score: 0.8636 time: 0.04s
Test loss: 0.5245 score: 0.8140 time: 0.04s
Epoch 255/1000, LR 0.000232
Train loss: 0.7376;  Loss pred: 0.7376; Loss self: 0.0000; time: 5.23s
Val loss: 0.4601 score: 0.8636 time: 0.06s
Test loss: 0.5233 score: 0.8140 time: 0.61s
Epoch 256/1000, LR 0.000231
Train loss: 0.7377;  Loss pred: 0.7377; Loss self: 0.0000; time: 3.84s
Val loss: 0.4584 score: 0.8636 time: 0.05s
Test loss: 0.5221 score: 0.8140 time: 0.04s
Epoch 257/1000, LR 0.000231
Train loss: 0.7297;  Loss pred: 0.7297; Loss self: 0.0000; time: 0.14s
Val loss: 0.4567 score: 0.8636 time: 0.05s
Test loss: 0.5211 score: 0.8140 time: 0.04s
Epoch 258/1000, LR 0.000231
Train loss: 0.7287;  Loss pred: 0.7287; Loss self: 0.0000; time: 0.14s
Val loss: 0.4551 score: 0.8636 time: 0.04s
Test loss: 0.5200 score: 0.8140 time: 0.04s
Epoch 259/1000, LR 0.000230
Train loss: 0.7246;  Loss pred: 0.7246; Loss self: 0.0000; time: 0.14s
Val loss: 0.4533 score: 0.8636 time: 0.04s
Test loss: 0.5187 score: 0.8140 time: 0.04s
Epoch 260/1000, LR 0.000230
Train loss: 0.7290;  Loss pred: 0.7290; Loss self: 0.0000; time: 0.14s
Val loss: 0.4514 score: 0.8636 time: 0.04s
Test loss: 0.5171 score: 0.8140 time: 0.04s
Epoch 261/1000, LR 0.000230
Train loss: 0.7221;  Loss pred: 0.7221; Loss self: 0.0000; time: 0.14s
Val loss: 0.4495 score: 0.8636 time: 0.04s
Test loss: 0.5157 score: 0.8140 time: 0.04s
Epoch 262/1000, LR 0.000229
Train loss: 0.7200;  Loss pred: 0.7200; Loss self: 0.0000; time: 0.14s
Val loss: 0.4478 score: 0.8636 time: 0.04s
Test loss: 0.5144 score: 0.8140 time: 0.04s
Epoch 263/1000, LR 0.000229
Train loss: 0.7168;  Loss pred: 0.7168; Loss self: 0.0000; time: 0.14s
Val loss: 0.4461 score: 0.8636 time: 0.04s
Test loss: 0.5132 score: 0.8140 time: 0.04s
Epoch 264/1000, LR 0.000229
Train loss: 0.7151;  Loss pred: 0.7151; Loss self: 0.0000; time: 0.14s
Val loss: 0.4445 score: 0.8636 time: 0.04s
Test loss: 0.5121 score: 0.8140 time: 0.04s
Epoch 265/1000, LR 0.000228
Train loss: 0.7143;  Loss pred: 0.7143; Loss self: 0.0000; time: 0.14s
Val loss: 0.4430 score: 0.8636 time: 0.04s
Test loss: 0.5111 score: 0.8140 time: 0.04s
Epoch 266/1000, LR 0.000228
Train loss: 0.7109;  Loss pred: 0.7109; Loss self: 0.0000; time: 0.14s
Val loss: 0.4416 score: 0.8636 time: 0.04s
Test loss: 0.5102 score: 0.8140 time: 0.04s
Epoch 267/1000, LR 0.000228
Train loss: 0.7090;  Loss pred: 0.7090; Loss self: 0.0000; time: 0.14s
Val loss: 0.4403 score: 0.8636 time: 0.04s
Test loss: 0.5094 score: 0.8140 time: 0.04s
Epoch 268/1000, LR 0.000228
Train loss: 0.7081;  Loss pred: 0.7081; Loss self: 0.0000; time: 0.14s
Val loss: 0.4388 score: 0.8636 time: 0.04s
Test loss: 0.5084 score: 0.8140 time: 0.04s
Epoch 269/1000, LR 0.000227
Train loss: 0.7025;  Loss pred: 0.7025; Loss self: 0.0000; time: 0.14s
Val loss: 0.4373 score: 0.8636 time: 0.05s
Test loss: 0.5073 score: 0.8140 time: 0.04s
Epoch 270/1000, LR 0.000227
Train loss: 0.7020;  Loss pred: 0.7020; Loss self: 0.0000; time: 0.14s
Val loss: 0.4358 score: 0.8636 time: 0.04s
Test loss: 0.5062 score: 0.8140 time: 0.04s
Epoch 271/1000, LR 0.000227
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.14s
Val loss: 0.4344 score: 0.8636 time: 0.04s
Test loss: 0.5052 score: 0.8140 time: 0.04s
Epoch 272/1000, LR 0.000226
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.14s
Val loss: 0.4331 score: 0.8636 time: 0.05s
Test loss: 0.5044 score: 0.8140 time: 0.04s
Epoch 273/1000, LR 0.000226
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.14s
Val loss: 0.4319 score: 0.8636 time: 0.05s
Test loss: 0.5036 score: 0.8140 time: 0.04s
Epoch 274/1000, LR 0.000226
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.14s
Val loss: 0.4306 score: 0.8636 time: 0.05s
Test loss: 0.5028 score: 0.8140 time: 1.78s
Epoch 275/1000, LR 0.000225
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 5.00s
Val loss: 0.4293 score: 0.8636 time: 0.24s
Test loss: 0.5019 score: 0.8140 time: 1.35s
Epoch 276/1000, LR 0.000225
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 2.32s
Val loss: 0.4280 score: 0.8636 time: 0.05s
Test loss: 0.5010 score: 0.8140 time: 0.06s
Epoch 277/1000, LR 0.000225
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.14s
Val loss: 0.4267 score: 0.8636 time: 0.04s
Test loss: 0.5000 score: 0.8140 time: 0.05s
Epoch 278/1000, LR 0.000224
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.14s
Val loss: 0.4253 score: 0.8636 time: 0.04s
Test loss: 0.4990 score: 0.8140 time: 0.04s
Epoch 279/1000, LR 0.000224
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.13s
Val loss: 0.4241 score: 0.8636 time: 0.04s
Test loss: 0.4981 score: 0.8140 time: 0.04s
Epoch 280/1000, LR 0.000224
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 0.13s
Val loss: 0.4230 score: 0.8636 time: 0.04s
Test loss: 0.4975 score: 0.8140 time: 0.04s
Epoch 281/1000, LR 0.000223
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.13s
Val loss: 0.4220 score: 0.8636 time: 0.04s
Test loss: 0.4968 score: 0.8140 time: 0.04s
Epoch 282/1000, LR 0.000223
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.13s
Val loss: 0.4209 score: 0.8636 time: 0.04s
Test loss: 0.4961 score: 0.8140 time: 0.04s
Epoch 283/1000, LR 0.000223
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.13s
Val loss: 0.4200 score: 0.8636 time: 0.04s
Test loss: 0.4955 score: 0.8140 time: 0.04s
Epoch 284/1000, LR 0.000222
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.13s
Val loss: 0.4189 score: 0.8636 time: 0.04s
Test loss: 0.4949 score: 0.8140 time: 0.04s
Epoch 285/1000, LR 0.000222
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.13s
Val loss: 0.4181 score: 0.8636 time: 0.04s
Test loss: 0.4944 score: 0.8140 time: 0.04s
Epoch 286/1000, LR 0.000222
Train loss: 0.6705;  Loss pred: 0.6705; Loss self: 0.0000; time: 0.13s
Val loss: 0.4174 score: 0.8636 time: 0.04s
Test loss: 0.4941 score: 0.8140 time: 0.04s
Epoch 287/1000, LR 0.000221
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.13s
Val loss: 0.4168 score: 0.8636 time: 0.04s
Test loss: 0.4940 score: 0.8140 time: 0.04s
Epoch 288/1000, LR 0.000221
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.13s
Val loss: 0.4162 score: 0.8636 time: 0.04s
Test loss: 0.4937 score: 0.8140 time: 0.04s
Epoch 289/1000, LR 0.000221
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.13s
Val loss: 0.4155 score: 0.8636 time: 0.04s
Test loss: 0.4934 score: 0.8140 time: 0.04s
Epoch 290/1000, LR 0.000220
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.14s
Val loss: 0.4147 score: 0.8636 time: 0.04s
Test loss: 0.4929 score: 0.8140 time: 0.04s
Epoch 291/1000, LR 0.000220
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.13s
Val loss: 0.4138 score: 0.8636 time: 0.04s
Test loss: 0.4924 score: 0.8140 time: 0.04s
Epoch 292/1000, LR 0.000220
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 3.60s
Val loss: 0.4129 score: 0.8636 time: 1.97s
Test loss: 0.4918 score: 0.8140 time: 2.16s
Epoch 293/1000, LR 0.000219
Train loss: 0.6579;  Loss pred: 0.6579; Loss self: 0.0000; time: 5.73s
Val loss: 0.4121 score: 0.8636 time: 1.16s
Test loss: 0.4913 score: 0.8140 time: 1.25s
Epoch 294/1000, LR 0.000219
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 4.98s
Val loss: 0.4112 score: 0.8636 time: 0.05s
Test loss: 0.4907 score: 0.8140 time: 0.05s
Epoch 295/1000, LR 0.000219
Train loss: 0.6586;  Loss pred: 0.6586; Loss self: 0.0000; time: 0.16s
Val loss: 0.4102 score: 0.8636 time: 0.04s
Test loss: 0.4899 score: 0.8140 time: 0.04s
Epoch 296/1000, LR 0.000218
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.14s
Val loss: 0.4092 score: 0.8636 time: 0.04s
Test loss: 0.4892 score: 0.8140 time: 0.04s
Epoch 297/1000, LR 0.000218
Train loss: 0.6547;  Loss pred: 0.6547; Loss self: 0.0000; time: 0.14s
Val loss: 0.4083 score: 0.8636 time: 0.04s
Test loss: 0.4886 score: 0.8140 time: 0.04s
Epoch 298/1000, LR 0.000218
Train loss: 0.6533;  Loss pred: 0.6533; Loss self: 0.0000; time: 0.14s
Val loss: 0.4074 score: 0.8636 time: 0.04s
Test loss: 0.4880 score: 0.8140 time: 0.04s
Epoch 299/1000, LR 0.000217
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 0.14s
Val loss: 0.4068 score: 0.8636 time: 0.04s
Test loss: 0.4877 score: 0.8140 time: 0.04s
Epoch 300/1000, LR 0.000217
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.14s
Val loss: 0.4064 score: 0.8636 time: 0.04s
Test loss: 0.4877 score: 0.8140 time: 0.04s
Epoch 301/1000, LR 0.000217
Train loss: 0.6438;  Loss pred: 0.6438; Loss self: 0.0000; time: 0.14s
Val loss: 0.4061 score: 0.8636 time: 0.04s
Test loss: 0.4878 score: 0.8140 time: 0.04s
Epoch 302/1000, LR 0.000216
Train loss: 0.6459;  Loss pred: 0.6459; Loss self: 0.0000; time: 0.14s
Val loss: 0.4058 score: 0.8636 time: 0.04s
Test loss: 0.4879 score: 0.8140 time: 0.04s
Epoch 303/1000, LR 0.000216
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 0.14s
Val loss: 0.4054 score: 0.8636 time: 0.04s
Test loss: 0.4877 score: 0.8140 time: 0.04s
Epoch 304/1000, LR 0.000216
Train loss: 0.6411;  Loss pred: 0.6411; Loss self: 0.0000; time: 0.14s
Val loss: 0.4049 score: 0.8636 time: 0.04s
Test loss: 0.4875 score: 0.8140 time: 0.04s
Epoch 305/1000, LR 0.000215
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 0.14s
Val loss: 0.4045 score: 0.8636 time: 0.04s
Test loss: 0.4875 score: 0.8140 time: 0.04s
Epoch 306/1000, LR 0.000215
Train loss: 0.6403;  Loss pred: 0.6403; Loss self: 0.0000; time: 0.14s
Val loss: 0.4039 score: 0.8636 time: 0.04s
Test loss: 0.4872 score: 0.8140 time: 0.04s
Epoch 307/1000, LR 0.000215
Train loss: 0.6404;  Loss pred: 0.6404; Loss self: 0.0000; time: 0.14s
Val loss: 0.4034 score: 0.8636 time: 0.04s
Test loss: 0.4870 score: 0.8140 time: 0.04s
Epoch 308/1000, LR 0.000214
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.14s
Val loss: 0.4030 score: 0.8636 time: 0.04s
Test loss: 0.4869 score: 0.8140 time: 0.04s
Epoch 309/1000, LR 0.000214
Train loss: 0.6368;  Loss pred: 0.6368; Loss self: 0.0000; time: 0.14s
Val loss: 0.4025 score: 0.8636 time: 0.04s
Test loss: 0.4868 score: 0.8140 time: 0.04s
Epoch 310/1000, LR 0.000214
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 0.14s
Val loss: 0.4022 score: 0.8636 time: 0.04s
Test loss: 0.4868 score: 0.8140 time: 0.04s
Epoch 311/1000, LR 0.000213
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.14s
Val loss: 0.4022 score: 0.8636 time: 1.70s
Test loss: 0.4871 score: 0.8140 time: 1.65s
Epoch 312/1000, LR 0.000213
Train loss: 0.6324;  Loss pred: 0.6324; Loss self: 0.0000; time: 1.82s
Val loss: 0.4021 score: 0.8636 time: 0.05s
Test loss: 0.4873 score: 0.8140 time: 0.04s
Epoch 313/1000, LR 0.000213
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.15s
Val loss: 0.4018 score: 0.8636 time: 0.04s
Test loss: 0.4874 score: 0.8140 time: 0.04s
Epoch 314/1000, LR 0.000212
Train loss: 0.6276;  Loss pred: 0.6276; Loss self: 0.0000; time: 0.14s
Val loss: 0.4013 score: 0.8636 time: 0.04s
Test loss: 0.4871 score: 0.8140 time: 0.04s
Epoch 315/1000, LR 0.000212
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.14s
Val loss: 0.4005 score: 0.8636 time: 0.04s
Test loss: 0.4866 score: 0.8140 time: 0.04s
Epoch 316/1000, LR 0.000212
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.14s
Val loss: 0.3998 score: 0.8636 time: 0.04s
Test loss: 0.4861 score: 0.8140 time: 0.04s
Epoch 317/1000, LR 0.000211
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.14s
Val loss: 0.3992 score: 0.8636 time: 0.04s
Test loss: 0.4858 score: 0.8140 time: 0.04s
Epoch 318/1000, LR 0.000211
Train loss: 0.6262;  Loss pred: 0.6262; Loss self: 0.0000; time: 0.14s
Val loss: 0.3990 score: 0.8636 time: 0.04s
Test loss: 0.4859 score: 0.8140 time: 0.04s
Epoch 319/1000, LR 0.000210
Train loss: 0.6230;  Loss pred: 0.6230; Loss self: 0.0000; time: 0.14s
Val loss: 0.3990 score: 0.8636 time: 0.05s
Test loss: 0.4862 score: 0.8140 time: 0.04s
Epoch 320/1000, LR 0.000210
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.14s
Val loss: 0.3994 score: 0.8636 time: 0.04s
Test loss: 0.4869 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 321/1000, LR 0.000210
Train loss: 0.6205;  Loss pred: 0.6205; Loss self: 0.0000; time: 0.14s
Val loss: 0.3999 score: 0.8636 time: 0.04s
Test loss: 0.4878 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 318,   Train_Loss: 0.6230,   Val_Loss: 0.3990,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.3990,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.4862


[0.33250294404570013, 0.14788256993051618, 0.051649183966219425, 0.050250438041985035, 0.04995393508579582, 0.05017997603863478, 0.049621292972005904, 2.0078830720158294, 1.616255378932692, 0.10659568605478853, 0.051093806978315115, 0.04433942504692823, 0.04404964589048177, 0.04440575302578509, 0.04413984902203083, 0.04428357898723334, 0.04549109807703644, 0.044279072899371386, 0.043845223961398005, 0.04411821695975959, 0.04552204895298928, 0.044381623971275985, 0.044386795023456216, 0.04519682703539729, 0.04449186602141708, 0.044372529024258256, 1.2837840910069644, 0.21830776694696397, 0.3511064019985497, 0.04487957898527384, 0.044701058068312705, 0.044218591996468604, 0.04400554299354553, 0.043892105924896896, 0.043158947955816984, 0.04516299196984619, 0.04419064591638744, 0.043748129974119365, 0.04429854603949934, 0.04344443301670253, 0.04369727906305343, 0.04359021398704499, 0.043960153008811176, 0.04380691994447261, 0.043484770925715566, 0.04347184405196458, 0.04447558103129268, 0.04341161798220128, 0.04354512505233288, 0.04521078092511743, 0.04438699805177748, 0.04433779790997505, 1.5187855020631105, 0.43709631403908134, 0.6939144999487326, 0.24969108996447176, 0.5744091080268845, 0.16866561397910118, 0.16591266600880772, 0.045000461977906525, 0.04500921594444662, 0.04496285296045244, 0.04487543599680066, 0.047188538941554725, 0.0476233420195058, 0.047584370942786336, 0.0473057609051466, 0.04760158504359424, 0.047538893995806575, 0.0474229590035975, 0.044359161984175444, 0.04444468405563384, 0.04432155494578183, 0.0473564820131287, 0.32814135192893445, 0.4834479090059176, 0.23144679400138557, 0.06201808899641037, 0.05661897291429341, 0.04753734800033271, 0.044258964946493506, 0.044010693090967834, 0.04382053902372718, 0.04377688898239285, 0.04396779800299555, 0.04355318192392588, 0.043378469068557024, 0.04697330598719418, 0.048386966926045716, 0.050395063939504325, 0.04669867199845612, 0.046870376099832356, 0.04663114796858281, 0.04700609599240124, 0.04666998703032732, 0.046495474060066044, 0.04674007301218808, 0.04648632393218577, 0.046639948966912925, 0.8816470709862188, 0.13815491402056068, 0.0461471329908818, 0.06372730690054595, 0.05259922600816935, 0.04490901797544211, 0.04498730506747961, 0.04490284400526434, 0.044423232902772725, 0.056140154018066823, 0.044041831977665424, 0.04441910807508975, 0.045315410010516644, 0.044713495997712016, 0.044202062068507075, 0.7130301810102537, 0.5093295950209722, 0.2968564690090716, 0.2187679980415851, 0.4060529329581186, 0.2708043709862977, 0.0625018710270524, 0.05028783902525902, 0.04812309192493558, 0.04792594793252647, 0.04718960705213249, 0.04783413000404835, 0.04383932799100876, 0.044157200027257204, 0.04352420999202877, 0.04335438401903957, 0.04844825202599168, 0.04786004300694913, 0.050072067067958415, 0.4318213809747249, 0.5596575029194355, 0.04910995298996568, 0.04485780303366482, 0.04474257095716894, 0.044897929998114705, 0.04417476092930883, 0.0454223029082641, 0.04479625797830522, 0.04839083494152874, 0.04442828497849405, 0.04452406894415617, 0.04452878201846033, 0.05375222396105528, 0.04456723004113883, 0.04512501903809607, 0.045226254034787416, 0.044867414981126785, 0.04685225198045373, 0.04549814108759165, 0.045108768972568214, 0.04507294192444533, 0.04479995591100305, 0.05604087398387492, 0.35514339606743306, 0.10647937806788832, 0.12609890999738127, 0.04887503490317613, 0.04762419196777046, 0.04754811804741621, 0.04438772797584534, 0.04431555001065135, 0.044339472078718245, 0.04429228603839874, 0.06519811600446701, 0.04414273507427424, 0.04905968904495239, 0.048483446007594466, 0.04849557299166918, 0.04884831001982093, 0.046382652944885194, 0.04446251399349421, 0.044632224016822875, 0.06509882607497275, 0.0480335260508582, 0.04401703702751547, 0.8991890200413764, 0.33795673691201955, 0.5421451260335743, 0.044912916026078165, 0.044809454935602844, 0.044366224901750684, 0.04453107400331646, 0.044017742038704455, 0.04401982598938048, 0.044995803968049586, 0.047997913090512156, 0.04845966899301857, 0.047630168963223696, 0.04453192406799644, 0.04435556207317859, 0.04475989006459713, 0.044541033916175365, 0.04356571601238102, 0.19416746101342142, 0.6920839060330763, 0.7106654279632494, 0.49560784292407334, 0.04698946001008153, 0.043938069022260606, 0.04428493801970035, 0.044250225997529924, 0.04426403099205345, 0.04343856300693005, 0.04423335497267544, 0.04427285003475845, 0.046225472004152834, 0.044133574003353715, 0.043667580001056194, 0.04452111397404224, 0.04412758897524327, 0.044196064001880586, 1.3767270310781896, 0.2239536540582776, 0.044338450068607926, 0.04495495604351163, 0.04430477705318481, 0.04995714803226292, 0.0470239530550316, 0.043855053023435175, 0.04438550490885973, 0.04445808194577694, 0.04417664301581681, 0.04441589198540896, 0.04402935702819377, 0.04445037606637925, 0.044383336091414094, 0.04399225197266787, 0.04404731607064605, 0.044581189984455705, 0.044276723987422884, 0.04536080698017031, 0.04475039406679571, 0.04494372894987464, 0.0445749779464677, 0.04506944795139134, 0.04462720302399248, 0.044897246989421546, 0.04502321092877537, 0.04494221694767475, 0.045435182051733136, 0.044618463027291, 0.0450876479735598, 0.044293805956840515, 0.04459670803043991, 0.0446380089269951, 0.044341400964185596, 0.04496544704306871, 0.04576978296972811, 0.04502075898926705, 0.04473387903999537, 0.044790538027882576, 0.044940138002857566, 0.04543810500763357, 0.611328796017915, 0.047313306014984846, 0.047673937981016934, 0.04723978997208178, 0.04738432995509356, 0.04676746693439782, 0.04670725704636425, 0.04710597300436348, 0.04834633902646601, 0.047019963967613876, 0.0471157890278846, 0.04720715596340597, 0.04754118202254176, 0.047362836077809334, 0.047537010977976024, 0.047266779001802206, 0.0469696510117501, 0.04749183903913945, 0.048459023935720325, 1.783630969002843, 1.358131992048584, 0.06227271805983037, 0.04957197292242199, 0.047311380971223116, 0.045209327014163136, 0.0456042819423601, 0.045925842016004026, 0.04552538401912898, 0.045745570911094546, 0.04568877595011145, 0.045314055983908474, 0.04665595106780529, 0.04559206904377788, 0.0454632859909907, 0.047556086094118655, 0.04391602496616542, 0.044248403050005436, 2.1620016320375726, 1.2543033569818363, 0.0581271160626784, 0.04676774796098471, 0.04698184400331229, 0.04648665897548199, 0.047131709987297654, 0.049057588912546635, 0.04680529097095132, 0.046774827991612256, 0.04640672798268497, 0.04681554506532848, 0.04698424704838544, 0.047177986009046435, 0.04658744402695447, 0.04681282991077751, 0.04672457906417549, 0.046724141109734774, 0.04651232995092869, 1.659642904996872, 0.04596014507114887, 0.04730223398655653, 0.045929078944027424, 0.04600880807265639, 0.04607545700855553, 0.047047434956766665, 0.04717469902243465, 0.04756184504367411, 0.04708406294230372, 0.047099333023652434]
[0.00755688509194773, 0.0033609674984208223, 0.0011738450901413505, 0.0011686148381856985, 0.0011617194205999029, 0.0011669761869449948, 0.0011539835574885095, 0.04669495516315882, 0.03758733439378354, 0.002478969443134617, 0.0011882280692631423, 0.0010311494196960052, 0.0010244103695460876, 0.0010326919308322113, 0.0010265081167914146, 0.0010298506741217056, 0.001057932513419452, 0.00102974588138073, 0.0010196563711953024, 0.0010260050455758044, 0.0010586523012323088, 0.001032130790029674, 0.0010322510470571213, 0.001051089000823193, 0.0010346945586376065, 0.0010319192796339129, 0.02985544397690615, 0.005076924812720093, 0.008165265162756969, 0.0010437111391924149, 0.0010395594899607606, 0.0010283393487550837, 0.0010233847207801287, 0.001020746649416207, 0.001003696464088767, 0.0010503021388336322, 0.001027689439915987, 0.001017398371491148, 0.001030198745104636, 0.0010103356515512216, 0.0010162157921640332, 0.0010137259066754648, 0.0010223291397397948, 0.0010187655801040141, 0.0010112737424585016, 0.0010109731174875484, 0.001034315837937039, 0.0010095725112139833, 0.001012677326798439, 0.001051413509886452, 0.001032255768645988, 0.0010311115793017454, 0.035320593071235126, 0.010165030559048403, 0.01613754651043564, 0.005806769534057483, 0.01335835134946243, 0.003922456139048865, 0.0038584340932280863, 0.0010465223715792214, 0.0010467259521964331, 0.0010456477432663357, 0.0010436147906232713, 0.0010974078823617378, 0.0011075195818489722, 0.001106613277739217, 0.001100133974538293, 0.0011070136056649824, 0.0011055556743210832, 0.0011028595117115697, 0.0010316084182366383, 0.0010335973036193916, 0.0010307338359484145, 0.0011013135351890394, 0.007631194230905453, 0.011242974628044595, 0.005382483581427571, 0.0014422811394514039, 0.001316720300332405, 0.00110551972093797, 0.0010292782545696165, 0.001023504490487624, 0.0010190823028773763, 0.001018067185637043, 0.001022506930302222, 0.001012864695905253, 0.0010088016062455123, 0.0010924024648184692, 0.0011252783006057143, 0.0011719782311512634, 0.0010860156278710725, 0.0010900087465077293, 0.001084445301594949, 0.0010931650230790986, 0.0010853485355890075, 0.0010812900944201405, 0.0010869784421439087, 0.0010810773007485063, 0.0010846499759747193, 0.020503420255493458, 0.0032129049772223416, 0.0010731891393228326, 0.0014820303930359524, 0.0012232378141434734, 0.0010443957668707469, 0.0010462163969181305, 0.0010442521861689382, 0.0010330984395993657, 0.0013055849771643447, 0.001024228650643382, 0.0010330025133741803, 0.0010538467444306196, 0.0010398487441328375, 0.0010279549318257459, 0.0165820972327966, 0.011844874302813307, 0.006903638814164456, 0.005087627861432211, 0.009443091464142293, 0.006297776069448784, 0.001453531884350056, 0.0011694846284943958, 0.0011191416726729205, 0.0011145569286634062, 0.001097432722142616, 0.0011124216280011243, 0.0010195192556048548, 0.0010269116285408651, 0.0010121909300471808, 0.0010082414888148738, 0.0011267035354881785, 0.001113024255975561, 0.0011644666759990329, 0.010042357697086625, 0.013015290765568267, 0.0011420919299992018, 0.0010432047217131355, 0.001040524905980673, 0.0010441379069329002, 0.0010273200216118334, 0.0010563326257735837, 0.0010417734413559353, 0.0011253682544541566, 0.0010332159297324198, 0.0010354434638175853, 0.0010355530701967519, 0.0012500517200245414, 0.0010364472102590424, 0.001049419047397583, 0.001051773349646219, 0.0010434282553750415, 0.0010895872553593892, 0.0010580963043625964, 0.0010490411388969353, 0.0010482079517312867, 0.0010418594397907687, 0.0013032761391598819, 0.008259148745754258, 0.002476264606229961, 0.0029325327906367738, 0.0011366287186785147, 0.001107539348087685, 0.0011057701871492142, 0.0010322727436243102, 0.0010305941862942174, 0.0010311505134585639, 0.0010300531636836918, 0.0015162352559178374, 0.0010265752342854474, 0.0011409230010454045, 0.0011275220001766154, 0.001127804023062074, 0.0011360072097632774, 0.0010786663475554697, 0.0010340119533370747, 0.0010379586980656483, 0.001513926187790064, 0.0011170587453687953, 0.0010236520238957087, 0.020911372559101776, 0.007859458997953943, 0.01260802618682731, 0.00104448641921112, 0.001042080347339601, 0.001031772672133737, 0.0010356063721701503, 0.0010236684195047549, 0.0010237168834739646, 0.001046414045768595, 0.0011162305369886549, 0.001126969046349269, 0.0011076783479819464, 0.0010356261411161964, 0.0010315246993762462, 0.0010409276759208634, 0.0010358379980505898, 0.0010131561863344423, 0.004515522349149335, 0.01609497455890875, 0.01652710297588952, 0.011525763788931939, 0.0010927781397693378, 0.0010218155586572235, 0.0010298822795279151, 0.0010290750231983703, 0.0010293960695826384, 0.0010101991396960477, 0.0010286826737831499, 0.0010296011635990336, 0.0010750109768407636, 0.001026362186124505, 0.0010155251163036325, 0.0010353747435823777, 0.0010262229994242621, 0.0010278154419041997, 0.03201690769949278, 0.0052082245129832, 0.0010311267457815798, 0.001045464094035154, 0.0010303436523996466, 0.0011617941402851841, 0.001093580303605386, 0.0010198849540333762, 0.0010322210443920868, 0.0010339088824599287, 0.0010273637910655073, 0.001032927720590906, 0.00102393853553939, 0.0010337296759623082, 0.001032170606777072, 0.0010230756272713458, 0.001024356187689443, 0.001036771860103621, 0.0010296912555214624, 0.0010549024879109375, 0.0010407068387626908, 0.001045202998834294, 0.0010366273941039, 0.0010481266965439847, 0.0010378419307905228, 0.0010441220230098035, 0.0010470514169482643, 0.0010451678359924361, 0.00105663214073798, 0.0010376386750532791, 0.0010485499528734836, 0.001030088510624198, 0.0010371327448939515, 0.0010380932308603512, 0.0010311953712601302, 0.0010457080707690397, 0.0010644135574355376, 0.0010469943950992338, 0.0010403227683719854, 0.0010416404192530831, 0.001045119488438548, 0.0010567001164565946, 0.014216948744602675, 0.0011003094422089498, 0.0011086962321166728, 0.0010985997667925995, 0.001101961161746362, 0.0010876155101022748, 0.0010862152801480057, 0.0010954877442875227, 0.0011243334657317678, 0.0010934875341305553, 0.001095716023904293, 0.0010978408363582783, 0.0011056088842451572, 0.0011014613041351008, 0.0011055118832087448, 0.0010992274186465629, 0.0010923174653895372, 0.001104461373003243, 0.0011269540450167517, 0.0414797899768103, 0.03158446493136242, 0.0014482027455774505, 0.0011528365795912092, 0.0011002646737493748, 0.001051379698003794, 0.001060564696333956, 0.001068042837581489, 0.0010587298609099763, 0.0010638504863045243, 0.0010625296732584058, 0.001053815255439732, 0.001085022117855937, 0.0010602806754366949, 0.0010572857207207138, 0.001105955490560899, 0.0010213029061898935, 0.0010290326290698939, 0.050279107721804016, 0.029169845511205494, 0.0013517933968064744, 0.0010876220456042954, 0.001092601023332844, 0.0010810850924530694, 0.001096086278774364, 0.0011408741607568985, 0.001088495138859333, 0.0010877866974793547, 0.0010792262321554643, 0.0010887336061704298, 0.001092656908101987, 0.0010971624653266614, 0.0010834289308594063, 0.0010886704630413374, 0.001086618117771523, 0.0010866079327845296, 0.0010816820918820625, 0.03859634662783423, 0.0010688405830499737, 0.0011000519531757333, 0.0010681181149773818, 0.001069972280759451, 0.0010715222560129193, 0.0010941263943434107, 0.00109708602377755, 0.0011060894196203283, 0.0010949782079605515, 0.001095333326131452]
[132.3296553848032, 297.53337408643733, 851.9011651525358, 855.7139335596004, 860.7930471572971, 856.9155148040178, 866.5634735527481, 21.41558968213713, 26.604706508940076, 403.39343543320007, 841.5892755505529, 969.7915558104194, 976.1712978785017, 968.3429976974197, 974.1764177430259, 971.0145607787621, 945.2398780786096, 971.1133766897467, 980.7225534497861, 974.6540763245368, 944.5972004556779, 968.8694588514792, 968.7565857655782, 951.3942199155534, 966.4687918304227, 969.0680460537217, 33.49472882645866, 196.9696296258964, 122.46999700158591, 958.1195049559049, 961.9459104141757, 972.4416372966846, 977.1496287707887, 979.6750256999889, 996.3171494360868, 952.1069823874745, 973.0566075309185, 982.8991553567673, 970.686486225948, 989.7700813236147, 984.0429638182441, 986.4599428848777, 978.1585608080406, 981.580080373251, 988.8519379222741, 989.145984895406, 966.8226699443348, 990.5182529163036, 987.4813758904644, 951.1005808818222, 968.75215462511, 969.8271458431165, 28.312095382520454, 98.37648732987329, 61.967288481760946, 172.2127930400657, 74.85953721678703, 254.9423026161574, 259.17249740123685, 955.5457457550397, 955.3598990276452, 956.3450085745474, 958.207960432198, 911.2382151364671, 902.9185726274283, 903.6580530128598, 908.9801998158292, 903.331264297606, 904.5225158959955, 906.7338036991329, 969.3600617464256, 967.4947839920416, 970.1825681116452, 908.0066375724248, 131.04108868702568, 88.94443268648756, 185.78784029189265, 693.3460978213768, 759.4627346047228, 904.5519325078682, 971.5545777445197, 977.0352834735235, 981.2750129960089, 982.2534446724775, 977.9884814124734, 987.2987024256432, 991.2751861307305, 915.4135332037864, 888.6690514352943, 853.2581693242503, 920.7970625250671, 917.4238309590564, 922.1304186843255, 914.7749689093762, 921.3630158512264, 924.8211975309607, 919.9814469435508, 925.00323455837, 921.9564118842592, 48.772350541469834, 311.2448102540934, 931.8021990336075, 674.7499948037442, 817.5025235793685, 957.4914335359987, 955.8252030323063, 957.623084964479, 967.9619692270552, 765.9401858099978, 976.3444904336914, 968.0518556858284, 948.9045777147489, 961.678326431919, 972.805294317626, 60.30600267028758, 84.4247034147494, 144.85114689781602, 196.55525664145006, 105.89752347493851, 158.78621103267102, 687.9794043507674, 855.0775064802762, 893.5419209362775, 897.2175169187772, 911.2175897649659, 898.9397318684541, 980.8544512547979, 973.7936276180806, 987.9558987486555, 991.8258781191785, 887.5449206491746, 898.4530163033187, 858.7622304795183, 99.57820963598107, 76.8327053165407, 875.5862586304229, 958.5846183267026, 961.0534012710834, 957.7278952906203, 973.406513027003, 946.671508198159, 959.9016065320646, 888.598017619606, 967.8518993207723, 965.769773960513, 965.6675536773823, 799.9669005538169, 964.8344750236405, 952.908185228641, 950.7751839656007, 958.3792607193371, 917.7787231644522, 945.0935570580278, 953.251462618042, 954.0091718903072, 959.822373160841, 767.297098406651, 121.0778532732039, 403.8340642127378, 341.0021545855788, 879.7947681302967, 902.9024582527328, 904.3470439170545, 968.7362241969108, 970.3140317487845, 969.7905271325692, 970.8236771234068, 659.5282599431843, 974.1127260838848, 876.4833376868731, 886.9006545711388, 886.6788728815878, 880.2761033606303, 927.0707316180324, 967.1068083620236, 963.4294715807204, 660.5341846023143, 895.2080668504605, 976.8944686831212, 47.82086862895785, 127.23522067617256, 79.31455607577863, 957.4083316040434, 959.618903238094, 969.2057436760464, 965.6178514086043, 976.8788222301462, 976.8325756302057, 955.6446647899267, 895.872283424337, 887.3358174649289, 902.7891551928201, 965.5994188425965, 969.4387353057964, 960.6815373751529, 965.4019276006135, 987.0146513322484, 221.45832146936152, 62.13119482357234, 60.50667206822907, 86.76214594648285, 915.0988326057461, 978.6501991749946, 970.9847619267586, 971.7464494396094, 971.4433827257989, 989.90383252641, 972.1170828340439, 971.2498735961399, 930.2230596182325, 974.3149285107168, 984.7122281326318, 965.8338743516365, 974.4470749155165, 972.9373185397303, 31.233497294176296, 192.00401163720448, 969.8128809974897, 956.5130028907282, 970.5499691011081, 860.7376860710721, 914.4275886307897, 980.5027479278556, 968.7847437647787, 967.2032197080548, 973.3650423506483, 968.1219509027513, 976.6211205960915, 967.3708932357883, 968.8320839928547, 977.4448470316011, 976.2229310642607, 964.5323513121335, 971.1648949505491, 947.9549166485872, 960.8853932284256, 956.7519430343116, 964.6667700349922, 954.0831306914765, 963.5378667330403, 957.7424649251085, 955.062935605015, 956.7841312782582, 946.4031628846474, 963.7266073844573, 953.6980067182916, 970.7903638242063, 964.1967288403874, 963.3046149151938, 969.7483404895339, 956.2898364785261, 939.4844635474888, 955.1149506442395, 961.240136621169, 960.0241902258922, 956.8283924109401, 946.3422823811871, 70.33858094055802, 908.8352436496668, 901.9603125112503, 910.2496015628467, 907.4729987899244, 919.4425702020047, 920.6278150163214, 912.8354061599972, 889.4158454575166, 914.5051669885855, 912.6452275807426, 910.8788513617031, 904.4789837074612, 907.884821959523, 904.5583454946709, 909.7298548386485, 915.4847667325218, 905.418717615092, 887.3476291441284, 24.108125922504918, 31.66113474371479, 690.5110510623042, 867.4256331756889, 908.8722230736514, 951.1311678346594, 942.8939162850609, 936.2920332525543, 944.5280018271158, 939.9817106571803, 941.150186359832, 948.9329318759234, 921.6401984284474, 943.1464924022431, 945.8181269282022, 904.1955201043739, 979.1414417203933, 971.7864834897068, 19.88897666070435, 34.28197792874335, 739.7580150653466, 919.4370452875368, 915.2471749931407, 924.9965677825777, 912.3369385831496, 876.5208595280681, 918.6995552850427, 919.2978755092554, 926.5897827583088, 918.498330842798, 915.200363979817, 911.4420440023595, 922.9954743840667, 918.5516039503585, 920.2865143191588, 920.2951403432247, 924.4860458585013, 25.909187976844358, 935.593217415515, 909.047974609841, 936.2260465184373, 934.6036509377741, 933.2517307862087, 913.9711875793871, 911.5055504551432, 904.0860370432401, 913.26018429403, 912.9640960819164]
Elapsed: 0.13302869871503065~0.2977129824781764
Time per graph: 0.0030928227595456085~0.006923213647764864
Speed: 818.6231023178678~288.5560610807927
Total Time: 0.0480
best val loss: 0.39902356266975403 test_score: 0.8140

Testing...
Test loss: 0.5360 score: 0.8140 time: 0.04s
test Score 0.8140
Epoch Time List: [1.760885203955695, 1.4541658852249384, 0.2209522071061656, 0.22270667681004852, 0.22136555798351765, 0.2331642931094393, 0.23820276302285492, 4.449915056116879, 4.879011059994809, 1.895834185066633, 0.2404429200105369, 0.2249704470159486, 0.2126144740032032, 0.21121248695999384, 0.21181732893455774, 0.2115320519078523, 0.2165017049992457, 0.2159705680096522, 0.2098183010239154, 0.21026071102824062, 0.21244881907477975, 0.21115479595027864, 0.21109085087664425, 0.2148179739015177, 0.21383202506694943, 0.2132463379530236, 3.1174011919647455, 3.3207743890816346, 2.27547162596602, 1.1010917159728706, 0.21898035902995616, 0.2123507511569187, 0.21088609902653843, 0.21097214194014668, 0.2075667860917747, 0.2104884780710563, 0.2138799618696794, 0.2081631701439619, 0.20876182802021503, 0.20822102995589375, 0.20736582297831774, 0.20687860099133104, 0.20719385205302387, 0.2073697600280866, 0.2065826968755573, 0.20720379101112485, 0.20799094915855676, 0.21209805097896606, 0.207026811898686, 0.21212644095066935, 0.21389185113366693, 0.21120259014423937, 1.8051904940512031, 4.162532573915087, 1.5755671618971974, 0.8071818889584392, 1.444942408008501, 0.7699359479593113, 0.6232738670660183, 0.26483158802147955, 0.21603251993656158, 0.21149602194782346, 0.2114055819110945, 0.2180495710344985, 0.22916711890138686, 0.22608981898520142, 0.22617851197719574, 0.22657037305179983, 0.22969038819428533, 0.2256405318621546, 0.2180116280214861, 0.2115110158920288, 0.21131166198756546, 0.21296081005129963, 3.2042024572147056, 1.5776964579708874, 2.85256390995346, 0.42832084104884416, 0.26572151097934693, 0.2344079059548676, 0.21381305111572146, 0.2108328090980649, 0.20804117084480822, 0.20752554503269494, 0.2088469760492444, 0.20745201909448951, 0.20719615288544446, 0.21562095975968987, 0.22606707399245352, 0.23020587919745594, 0.2253660128917545, 0.2226003489922732, 0.22355908900499344, 0.22283752693329006, 0.22459934698417783, 0.22121242003049701, 0.2234329970087856, 0.2223664381308481, 0.22253697202540934, 3.0949309478746727, 3.8928131581051275, 0.7741388108115643, 0.24129677284508944, 0.22572587488684803, 0.22153735382016748, 0.21415577793959528, 0.21467925107572228, 0.21244482905603945, 0.30397706909570843, 0.2129397930111736, 0.22223872400354594, 0.21441930497530848, 0.21572480001486838, 0.2120277089998126, 1.2047746300231665, 2.488362512085587, 1.6997869920451194, 0.7943090581102297, 1.7197729609906673, 1.018124410067685, 0.6162843339843675, 0.23123429797124118, 0.22949941304977983, 0.22831000899896026, 0.2263080650009215, 0.22569638991262764, 0.21929980511777103, 0.21817939798347652, 0.20640650193672627, 0.20659093384165317, 0.21802374394610524, 0.23008007102180272, 0.23063977400306612, 3.9708665400976315, 2.493306268006563, 1.8014369669836015, 0.21997647907119244, 0.2157942660851404, 0.212019273894839, 0.21102982503362, 0.21170452993828803, 0.2149085160344839, 0.23774729203432798, 0.21235629997681826, 0.21234437101520598, 0.21227786096278578, 0.2272006031125784, 0.2230275518959388, 0.21842363791074604, 0.2131313479039818, 0.212895811884664, 0.22235197201371193, 0.2160270569147542, 0.21427336696069688, 0.2157179251080379, 0.21354156406596303, 0.23023704497609288, 3.222882607136853, 0.6042214500484988, 0.5268602349096909, 0.2310294311027974, 0.22851942316628993, 0.22784989001229405, 0.22035290487110615, 0.21142161602620035, 0.21085759194102138, 0.21034621400758624, 0.2518993130652234, 0.2413920130347833, 0.23291347199119627, 0.23186738207004964, 0.23096705100033432, 0.23275374702643603, 0.22527592699043453, 0.2120342020643875, 0.21154232998378575, 0.24126834515482187, 0.2503277459181845, 0.20844934787601233, 5.611600985052064, 3.073703495087102, 2.8694653939455748, 0.42368433508090675, 0.21615427709184587, 0.2109808629611507, 0.21203926706220955, 0.2107727228431031, 0.20867454691324383, 0.21419741096906364, 0.21870357799343765, 0.23220559896435589, 0.22935333487112075, 0.2248473110375926, 0.21127294888719916, 0.21054296707734466, 0.20958639401942492, 0.2081478878390044, 0.35752423806115985, 6.360100543824956, 1.494166059885174, 1.7704595539253205, 0.8781531290151179, 0.21552622399758548, 0.21076827694196254, 0.20987764606252313, 0.20774909504689276, 0.20674400497227907, 0.21097082493361086, 0.20836548309307545, 0.21229644888080657, 0.2160586949903518, 0.20693916617892683, 0.20848859602119774, 0.20889686804730445, 0.2086156930308789, 3.3044701101025566, 1.2872987861046568, 0.26518429594580084, 0.21798469009809196, 0.2100831689313054, 0.22357522207312286, 0.2220461859833449, 0.21592215495184064, 0.2090636850334704, 0.21119102009106427, 0.21131110994610935, 0.20999799692071974, 0.20935574581380934, 0.21911332302261144, 0.20965983893256634, 0.20962799596600235, 0.21005751506891102, 0.210639962926507, 0.21234628290403634, 0.2111750920303166, 0.21227116708178073, 0.211488478933461, 0.2119127430487424, 0.2129174090223387, 0.21050212404225022, 0.21102388994768262, 0.2121561060193926, 0.2134137840475887, 0.21578315796796232, 0.21268467779736966, 0.2136140470393002, 0.21243738289922476, 0.21266932203434408, 0.21237440896220505, 0.21217329311184585, 0.21343459596391767, 0.21513980394229293, 0.2155150338076055, 0.21307235409040004, 0.2145700219552964, 0.21569621202070266, 0.21612438710872084, 5.901394676067866, 3.9268855380360037, 0.2283101207576692, 0.22941265895497054, 0.22925825486890972, 0.22462354600429535, 0.22476225101854652, 0.22365784691646695, 0.22594855388160795, 0.22734706511255354, 0.22512478602584451, 0.2258762918645516, 0.22580934001598507, 0.22525030001997948, 0.23234483308624476, 0.22557126905303448, 0.22568557795602828, 0.22843288502190262, 0.22892209608107805, 1.9648293929640204, 6.595871908124536, 2.4227411379106343, 0.2313485809136182, 0.22812628804240376, 0.2170654599322006, 0.21639358799438924, 0.22019909287337214, 0.21612494089640677, 0.2159813529578969, 0.21593930292874575, 0.21583653008565307, 0.21987867599818856, 0.21715307189151645, 0.2155319699086249, 0.22128825914114714, 0.22358287300448865, 0.21065550204366446, 7.725392063031904, 8.143738385871984, 5.08633343083784, 0.2488213828764856, 0.22655996808316559, 0.2229379640193656, 0.22242275590542704, 0.22461695806123316, 0.2231667210580781, 0.22250619705300778, 0.222669766866602, 0.2226794009329751, 0.2224067939678207, 0.22268695197999477, 0.22533336002379656, 0.22235895600169897, 0.22422250208910555, 0.22299897903576493, 0.22255426493939012, 3.4918223458807915, 1.9053320590173826, 0.233442670898512, 0.2212779929395765, 0.22227816190570593, 0.223782074986957, 0.2233410340268165, 0.22482329013291746, 0.230592702049762, 0.22985356603749096, 0.22460476902779192]
Total Epoch List: [3, 321]
Total Time List: [0.05192692100536078, 0.04800463002175093]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bf64400>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.8810;  Loss pred: 2.8810; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.8533;  Loss pred: 2.8533; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 2.8368;  Loss pred: 2.8368; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 2.8810,   Val_Loss: 0.6930,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6930,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6933


[0.33250294404570013, 0.14788256993051618, 0.051649183966219425, 0.050250438041985035, 0.04995393508579582, 0.05017997603863478, 0.049621292972005904, 2.0078830720158294, 1.616255378932692, 0.10659568605478853, 0.051093806978315115, 0.04433942504692823, 0.04404964589048177, 0.04440575302578509, 0.04413984902203083, 0.04428357898723334, 0.04549109807703644, 0.044279072899371386, 0.043845223961398005, 0.04411821695975959, 0.04552204895298928, 0.044381623971275985, 0.044386795023456216, 0.04519682703539729, 0.04449186602141708, 0.044372529024258256, 1.2837840910069644, 0.21830776694696397, 0.3511064019985497, 0.04487957898527384, 0.044701058068312705, 0.044218591996468604, 0.04400554299354553, 0.043892105924896896, 0.043158947955816984, 0.04516299196984619, 0.04419064591638744, 0.043748129974119365, 0.04429854603949934, 0.04344443301670253, 0.04369727906305343, 0.04359021398704499, 0.043960153008811176, 0.04380691994447261, 0.043484770925715566, 0.04347184405196458, 0.04447558103129268, 0.04341161798220128, 0.04354512505233288, 0.04521078092511743, 0.04438699805177748, 0.04433779790997505, 1.5187855020631105, 0.43709631403908134, 0.6939144999487326, 0.24969108996447176, 0.5744091080268845, 0.16866561397910118, 0.16591266600880772, 0.045000461977906525, 0.04500921594444662, 0.04496285296045244, 0.04487543599680066, 0.047188538941554725, 0.0476233420195058, 0.047584370942786336, 0.0473057609051466, 0.04760158504359424, 0.047538893995806575, 0.0474229590035975, 0.044359161984175444, 0.04444468405563384, 0.04432155494578183, 0.0473564820131287, 0.32814135192893445, 0.4834479090059176, 0.23144679400138557, 0.06201808899641037, 0.05661897291429341, 0.04753734800033271, 0.044258964946493506, 0.044010693090967834, 0.04382053902372718, 0.04377688898239285, 0.04396779800299555, 0.04355318192392588, 0.043378469068557024, 0.04697330598719418, 0.048386966926045716, 0.050395063939504325, 0.04669867199845612, 0.046870376099832356, 0.04663114796858281, 0.04700609599240124, 0.04666998703032732, 0.046495474060066044, 0.04674007301218808, 0.04648632393218577, 0.046639948966912925, 0.8816470709862188, 0.13815491402056068, 0.0461471329908818, 0.06372730690054595, 0.05259922600816935, 0.04490901797544211, 0.04498730506747961, 0.04490284400526434, 0.044423232902772725, 0.056140154018066823, 0.044041831977665424, 0.04441910807508975, 0.045315410010516644, 0.044713495997712016, 0.044202062068507075, 0.7130301810102537, 0.5093295950209722, 0.2968564690090716, 0.2187679980415851, 0.4060529329581186, 0.2708043709862977, 0.0625018710270524, 0.05028783902525902, 0.04812309192493558, 0.04792594793252647, 0.04718960705213249, 0.04783413000404835, 0.04383932799100876, 0.044157200027257204, 0.04352420999202877, 0.04335438401903957, 0.04844825202599168, 0.04786004300694913, 0.050072067067958415, 0.4318213809747249, 0.5596575029194355, 0.04910995298996568, 0.04485780303366482, 0.04474257095716894, 0.044897929998114705, 0.04417476092930883, 0.0454223029082641, 0.04479625797830522, 0.04839083494152874, 0.04442828497849405, 0.04452406894415617, 0.04452878201846033, 0.05375222396105528, 0.04456723004113883, 0.04512501903809607, 0.045226254034787416, 0.044867414981126785, 0.04685225198045373, 0.04549814108759165, 0.045108768972568214, 0.04507294192444533, 0.04479995591100305, 0.05604087398387492, 0.35514339606743306, 0.10647937806788832, 0.12609890999738127, 0.04887503490317613, 0.04762419196777046, 0.04754811804741621, 0.04438772797584534, 0.04431555001065135, 0.044339472078718245, 0.04429228603839874, 0.06519811600446701, 0.04414273507427424, 0.04905968904495239, 0.048483446007594466, 0.04849557299166918, 0.04884831001982093, 0.046382652944885194, 0.04446251399349421, 0.044632224016822875, 0.06509882607497275, 0.0480335260508582, 0.04401703702751547, 0.8991890200413764, 0.33795673691201955, 0.5421451260335743, 0.044912916026078165, 0.044809454935602844, 0.044366224901750684, 0.04453107400331646, 0.044017742038704455, 0.04401982598938048, 0.044995803968049586, 0.047997913090512156, 0.04845966899301857, 0.047630168963223696, 0.04453192406799644, 0.04435556207317859, 0.04475989006459713, 0.044541033916175365, 0.04356571601238102, 0.19416746101342142, 0.6920839060330763, 0.7106654279632494, 0.49560784292407334, 0.04698946001008153, 0.043938069022260606, 0.04428493801970035, 0.044250225997529924, 0.04426403099205345, 0.04343856300693005, 0.04423335497267544, 0.04427285003475845, 0.046225472004152834, 0.044133574003353715, 0.043667580001056194, 0.04452111397404224, 0.04412758897524327, 0.044196064001880586, 1.3767270310781896, 0.2239536540582776, 0.044338450068607926, 0.04495495604351163, 0.04430477705318481, 0.04995714803226292, 0.0470239530550316, 0.043855053023435175, 0.04438550490885973, 0.04445808194577694, 0.04417664301581681, 0.04441589198540896, 0.04402935702819377, 0.04445037606637925, 0.044383336091414094, 0.04399225197266787, 0.04404731607064605, 0.044581189984455705, 0.044276723987422884, 0.04536080698017031, 0.04475039406679571, 0.04494372894987464, 0.0445749779464677, 0.04506944795139134, 0.04462720302399248, 0.044897246989421546, 0.04502321092877537, 0.04494221694767475, 0.045435182051733136, 0.044618463027291, 0.0450876479735598, 0.044293805956840515, 0.04459670803043991, 0.0446380089269951, 0.044341400964185596, 0.04496544704306871, 0.04576978296972811, 0.04502075898926705, 0.04473387903999537, 0.044790538027882576, 0.044940138002857566, 0.04543810500763357, 0.611328796017915, 0.047313306014984846, 0.047673937981016934, 0.04723978997208178, 0.04738432995509356, 0.04676746693439782, 0.04670725704636425, 0.04710597300436348, 0.04834633902646601, 0.047019963967613876, 0.0471157890278846, 0.04720715596340597, 0.04754118202254176, 0.047362836077809334, 0.047537010977976024, 0.047266779001802206, 0.0469696510117501, 0.04749183903913945, 0.048459023935720325, 1.783630969002843, 1.358131992048584, 0.06227271805983037, 0.04957197292242199, 0.047311380971223116, 0.045209327014163136, 0.0456042819423601, 0.045925842016004026, 0.04552538401912898, 0.045745570911094546, 0.04568877595011145, 0.045314055983908474, 0.04665595106780529, 0.04559206904377788, 0.0454632859909907, 0.047556086094118655, 0.04391602496616542, 0.044248403050005436, 2.1620016320375726, 1.2543033569818363, 0.0581271160626784, 0.04676774796098471, 0.04698184400331229, 0.04648665897548199, 0.047131709987297654, 0.049057588912546635, 0.04680529097095132, 0.046774827991612256, 0.04640672798268497, 0.04681554506532848, 0.04698424704838544, 0.047177986009046435, 0.04658744402695447, 0.04681282991077751, 0.04672457906417549, 0.046724141109734774, 0.04651232995092869, 1.659642904996872, 0.04596014507114887, 0.04730223398655653, 0.045929078944027424, 0.04600880807265639, 0.04607545700855553, 0.047047434956766665, 0.04717469902243465, 0.04756184504367411, 0.04708406294230372, 0.047099333023652434, 0.050546668004244566, 0.04944203305058181, 0.05010357208084315]
[0.00755688509194773, 0.0033609674984208223, 0.0011738450901413505, 0.0011686148381856985, 0.0011617194205999029, 0.0011669761869449948, 0.0011539835574885095, 0.04669495516315882, 0.03758733439378354, 0.002478969443134617, 0.0011882280692631423, 0.0010311494196960052, 0.0010244103695460876, 0.0010326919308322113, 0.0010265081167914146, 0.0010298506741217056, 0.001057932513419452, 0.00102974588138073, 0.0010196563711953024, 0.0010260050455758044, 0.0010586523012323088, 0.001032130790029674, 0.0010322510470571213, 0.001051089000823193, 0.0010346945586376065, 0.0010319192796339129, 0.02985544397690615, 0.005076924812720093, 0.008165265162756969, 0.0010437111391924149, 0.0010395594899607606, 0.0010283393487550837, 0.0010233847207801287, 0.001020746649416207, 0.001003696464088767, 0.0010503021388336322, 0.001027689439915987, 0.001017398371491148, 0.001030198745104636, 0.0010103356515512216, 0.0010162157921640332, 0.0010137259066754648, 0.0010223291397397948, 0.0010187655801040141, 0.0010112737424585016, 0.0010109731174875484, 0.001034315837937039, 0.0010095725112139833, 0.001012677326798439, 0.001051413509886452, 0.001032255768645988, 0.0010311115793017454, 0.035320593071235126, 0.010165030559048403, 0.01613754651043564, 0.005806769534057483, 0.01335835134946243, 0.003922456139048865, 0.0038584340932280863, 0.0010465223715792214, 0.0010467259521964331, 0.0010456477432663357, 0.0010436147906232713, 0.0010974078823617378, 0.0011075195818489722, 0.001106613277739217, 0.001100133974538293, 0.0011070136056649824, 0.0011055556743210832, 0.0011028595117115697, 0.0010316084182366383, 0.0010335973036193916, 0.0010307338359484145, 0.0011013135351890394, 0.007631194230905453, 0.011242974628044595, 0.005382483581427571, 0.0014422811394514039, 0.001316720300332405, 0.00110551972093797, 0.0010292782545696165, 0.001023504490487624, 0.0010190823028773763, 0.001018067185637043, 0.001022506930302222, 0.001012864695905253, 0.0010088016062455123, 0.0010924024648184692, 0.0011252783006057143, 0.0011719782311512634, 0.0010860156278710725, 0.0010900087465077293, 0.001084445301594949, 0.0010931650230790986, 0.0010853485355890075, 0.0010812900944201405, 0.0010869784421439087, 0.0010810773007485063, 0.0010846499759747193, 0.020503420255493458, 0.0032129049772223416, 0.0010731891393228326, 0.0014820303930359524, 0.0012232378141434734, 0.0010443957668707469, 0.0010462163969181305, 0.0010442521861689382, 0.0010330984395993657, 0.0013055849771643447, 0.001024228650643382, 0.0010330025133741803, 0.0010538467444306196, 0.0010398487441328375, 0.0010279549318257459, 0.0165820972327966, 0.011844874302813307, 0.006903638814164456, 0.005087627861432211, 0.009443091464142293, 0.006297776069448784, 0.001453531884350056, 0.0011694846284943958, 0.0011191416726729205, 0.0011145569286634062, 0.001097432722142616, 0.0011124216280011243, 0.0010195192556048548, 0.0010269116285408651, 0.0010121909300471808, 0.0010082414888148738, 0.0011267035354881785, 0.001113024255975561, 0.0011644666759990329, 0.010042357697086625, 0.013015290765568267, 0.0011420919299992018, 0.0010432047217131355, 0.001040524905980673, 0.0010441379069329002, 0.0010273200216118334, 0.0010563326257735837, 0.0010417734413559353, 0.0011253682544541566, 0.0010332159297324198, 0.0010354434638175853, 0.0010355530701967519, 0.0012500517200245414, 0.0010364472102590424, 0.001049419047397583, 0.001051773349646219, 0.0010434282553750415, 0.0010895872553593892, 0.0010580963043625964, 0.0010490411388969353, 0.0010482079517312867, 0.0010418594397907687, 0.0013032761391598819, 0.008259148745754258, 0.002476264606229961, 0.0029325327906367738, 0.0011366287186785147, 0.001107539348087685, 0.0011057701871492142, 0.0010322727436243102, 0.0010305941862942174, 0.0010311505134585639, 0.0010300531636836918, 0.0015162352559178374, 0.0010265752342854474, 0.0011409230010454045, 0.0011275220001766154, 0.001127804023062074, 0.0011360072097632774, 0.0010786663475554697, 0.0010340119533370747, 0.0010379586980656483, 0.001513926187790064, 0.0011170587453687953, 0.0010236520238957087, 0.020911372559101776, 0.007859458997953943, 0.01260802618682731, 0.00104448641921112, 0.001042080347339601, 0.001031772672133737, 0.0010356063721701503, 0.0010236684195047549, 0.0010237168834739646, 0.001046414045768595, 0.0011162305369886549, 0.001126969046349269, 0.0011076783479819464, 0.0010356261411161964, 0.0010315246993762462, 0.0010409276759208634, 0.0010358379980505898, 0.0010131561863344423, 0.004515522349149335, 0.01609497455890875, 0.01652710297588952, 0.011525763788931939, 0.0010927781397693378, 0.0010218155586572235, 0.0010298822795279151, 0.0010290750231983703, 0.0010293960695826384, 0.0010101991396960477, 0.0010286826737831499, 0.0010296011635990336, 0.0010750109768407636, 0.001026362186124505, 0.0010155251163036325, 0.0010353747435823777, 0.0010262229994242621, 0.0010278154419041997, 0.03201690769949278, 0.0052082245129832, 0.0010311267457815798, 0.001045464094035154, 0.0010303436523996466, 0.0011617941402851841, 0.001093580303605386, 0.0010198849540333762, 0.0010322210443920868, 0.0010339088824599287, 0.0010273637910655073, 0.001032927720590906, 0.00102393853553939, 0.0010337296759623082, 0.001032170606777072, 0.0010230756272713458, 0.001024356187689443, 0.001036771860103621, 0.0010296912555214624, 0.0010549024879109375, 0.0010407068387626908, 0.001045202998834294, 0.0010366273941039, 0.0010481266965439847, 0.0010378419307905228, 0.0010441220230098035, 0.0010470514169482643, 0.0010451678359924361, 0.00105663214073798, 0.0010376386750532791, 0.0010485499528734836, 0.001030088510624198, 0.0010371327448939515, 0.0010380932308603512, 0.0010311953712601302, 0.0010457080707690397, 0.0010644135574355376, 0.0010469943950992338, 0.0010403227683719854, 0.0010416404192530831, 0.001045119488438548, 0.0010567001164565946, 0.014216948744602675, 0.0011003094422089498, 0.0011086962321166728, 0.0010985997667925995, 0.001101961161746362, 0.0010876155101022748, 0.0010862152801480057, 0.0010954877442875227, 0.0011243334657317678, 0.0010934875341305553, 0.001095716023904293, 0.0010978408363582783, 0.0011056088842451572, 0.0011014613041351008, 0.0011055118832087448, 0.0010992274186465629, 0.0010923174653895372, 0.001104461373003243, 0.0011269540450167517, 0.0414797899768103, 0.03158446493136242, 0.0014482027455774505, 0.0011528365795912092, 0.0011002646737493748, 0.001051379698003794, 0.001060564696333956, 0.001068042837581489, 0.0010587298609099763, 0.0010638504863045243, 0.0010625296732584058, 0.001053815255439732, 0.001085022117855937, 0.0010602806754366949, 0.0010572857207207138, 0.001105955490560899, 0.0010213029061898935, 0.0010290326290698939, 0.050279107721804016, 0.029169845511205494, 0.0013517933968064744, 0.0010876220456042954, 0.001092601023332844, 0.0010810850924530694, 0.001096086278774364, 0.0011408741607568985, 0.001088495138859333, 0.0010877866974793547, 0.0010792262321554643, 0.0010887336061704298, 0.001092656908101987, 0.0010971624653266614, 0.0010834289308594063, 0.0010886704630413374, 0.001086618117771523, 0.0010866079327845296, 0.0010816820918820625, 0.03859634662783423, 0.0010688405830499737, 0.0011000519531757333, 0.0010681181149773818, 0.001069972280759451, 0.0010715222560129193, 0.0010941263943434107, 0.00109708602377755, 0.0011060894196203283, 0.0010949782079605515, 0.001095333326131452, 0.001175503907075455, 0.001149814722106554, 0.0011651993507172825]
[132.3296553848032, 297.53337408643733, 851.9011651525358, 855.7139335596004, 860.7930471572971, 856.9155148040178, 866.5634735527481, 21.41558968213713, 26.604706508940076, 403.39343543320007, 841.5892755505529, 969.7915558104194, 976.1712978785017, 968.3429976974197, 974.1764177430259, 971.0145607787621, 945.2398780786096, 971.1133766897467, 980.7225534497861, 974.6540763245368, 944.5972004556779, 968.8694588514792, 968.7565857655782, 951.3942199155534, 966.4687918304227, 969.0680460537217, 33.49472882645866, 196.9696296258964, 122.46999700158591, 958.1195049559049, 961.9459104141757, 972.4416372966846, 977.1496287707887, 979.6750256999889, 996.3171494360868, 952.1069823874745, 973.0566075309185, 982.8991553567673, 970.686486225948, 989.7700813236147, 984.0429638182441, 986.4599428848777, 978.1585608080406, 981.580080373251, 988.8519379222741, 989.145984895406, 966.8226699443348, 990.5182529163036, 987.4813758904644, 951.1005808818222, 968.75215462511, 969.8271458431165, 28.312095382520454, 98.37648732987329, 61.967288481760946, 172.2127930400657, 74.85953721678703, 254.9423026161574, 259.17249740123685, 955.5457457550397, 955.3598990276452, 956.3450085745474, 958.207960432198, 911.2382151364671, 902.9185726274283, 903.6580530128598, 908.9801998158292, 903.331264297606, 904.5225158959955, 906.7338036991329, 969.3600617464256, 967.4947839920416, 970.1825681116452, 908.0066375724248, 131.04108868702568, 88.94443268648756, 185.78784029189265, 693.3460978213768, 759.4627346047228, 904.5519325078682, 971.5545777445197, 977.0352834735235, 981.2750129960089, 982.2534446724775, 977.9884814124734, 987.2987024256432, 991.2751861307305, 915.4135332037864, 888.6690514352943, 853.2581693242503, 920.7970625250671, 917.4238309590564, 922.1304186843255, 914.7749689093762, 921.3630158512264, 924.8211975309607, 919.9814469435508, 925.00323455837, 921.9564118842592, 48.772350541469834, 311.2448102540934, 931.8021990336075, 674.7499948037442, 817.5025235793685, 957.4914335359987, 955.8252030323063, 957.623084964479, 967.9619692270552, 765.9401858099978, 976.3444904336914, 968.0518556858284, 948.9045777147489, 961.678326431919, 972.805294317626, 60.30600267028758, 84.4247034147494, 144.85114689781602, 196.55525664145006, 105.89752347493851, 158.78621103267102, 687.9794043507674, 855.0775064802762, 893.5419209362775, 897.2175169187772, 911.2175897649659, 898.9397318684541, 980.8544512547979, 973.7936276180806, 987.9558987486555, 991.8258781191785, 887.5449206491746, 898.4530163033187, 858.7622304795183, 99.57820963598107, 76.8327053165407, 875.5862586304229, 958.5846183267026, 961.0534012710834, 957.7278952906203, 973.406513027003, 946.671508198159, 959.9016065320646, 888.598017619606, 967.8518993207723, 965.769773960513, 965.6675536773823, 799.9669005538169, 964.8344750236405, 952.908185228641, 950.7751839656007, 958.3792607193371, 917.7787231644522, 945.0935570580278, 953.251462618042, 954.0091718903072, 959.822373160841, 767.297098406651, 121.0778532732039, 403.8340642127378, 341.0021545855788, 879.7947681302967, 902.9024582527328, 904.3470439170545, 968.7362241969108, 970.3140317487845, 969.7905271325692, 970.8236771234068, 659.5282599431843, 974.1127260838848, 876.4833376868731, 886.9006545711388, 886.6788728815878, 880.2761033606303, 927.0707316180324, 967.1068083620236, 963.4294715807204, 660.5341846023143, 895.2080668504605, 976.8944686831212, 47.82086862895785, 127.23522067617256, 79.31455607577863, 957.4083316040434, 959.618903238094, 969.2057436760464, 965.6178514086043, 976.8788222301462, 976.8325756302057, 955.6446647899267, 895.872283424337, 887.3358174649289, 902.7891551928201, 965.5994188425965, 969.4387353057964, 960.6815373751529, 965.4019276006135, 987.0146513322484, 221.45832146936152, 62.13119482357234, 60.50667206822907, 86.76214594648285, 915.0988326057461, 978.6501991749946, 970.9847619267586, 971.7464494396094, 971.4433827257989, 989.90383252641, 972.1170828340439, 971.2498735961399, 930.2230596182325, 974.3149285107168, 984.7122281326318, 965.8338743516365, 974.4470749155165, 972.9373185397303, 31.233497294176296, 192.00401163720448, 969.8128809974897, 956.5130028907282, 970.5499691011081, 860.7376860710721, 914.4275886307897, 980.5027479278556, 968.7847437647787, 967.2032197080548, 973.3650423506483, 968.1219509027513, 976.6211205960915, 967.3708932357883, 968.8320839928547, 977.4448470316011, 976.2229310642607, 964.5323513121335, 971.1648949505491, 947.9549166485872, 960.8853932284256, 956.7519430343116, 964.6667700349922, 954.0831306914765, 963.5378667330403, 957.7424649251085, 955.062935605015, 956.7841312782582, 946.4031628846474, 963.7266073844573, 953.6980067182916, 970.7903638242063, 964.1967288403874, 963.3046149151938, 969.7483404895339, 956.2898364785261, 939.4844635474888, 955.1149506442395, 961.240136621169, 960.0241902258922, 956.8283924109401, 946.3422823811871, 70.33858094055802, 908.8352436496668, 901.9603125112503, 910.2496015628467, 907.4729987899244, 919.4425702020047, 920.6278150163214, 912.8354061599972, 889.4158454575166, 914.5051669885855, 912.6452275807426, 910.8788513617031, 904.4789837074612, 907.884821959523, 904.5583454946709, 909.7298548386485, 915.4847667325218, 905.418717615092, 887.3476291441284, 24.108125922504918, 31.66113474371479, 690.5110510623042, 867.4256331756889, 908.8722230736514, 951.1311678346594, 942.8939162850609, 936.2920332525543, 944.5280018271158, 939.9817106571803, 941.150186359832, 948.9329318759234, 921.6401984284474, 943.1464924022431, 945.8181269282022, 904.1955201043739, 979.1414417203933, 971.7864834897068, 19.88897666070435, 34.28197792874335, 739.7580150653466, 919.4370452875368, 915.2471749931407, 924.9965677825777, 912.3369385831496, 876.5208595280681, 918.6995552850427, 919.2978755092554, 926.5897827583088, 918.498330842798, 915.200363979817, 911.4420440023595, 922.9954743840667, 918.5516039503585, 920.2865143191588, 920.2951403432247, 924.4860458585013, 25.909187976844358, 935.593217415515, 909.047974609841, 936.2260465184373, 934.6036509377741, 933.2517307862087, 913.9711875793871, 911.5055504551432, 904.0860370432401, 913.26018429403, 912.9640960819164, 850.6990014928215, 869.7053366719109, 858.2222427298918]
Elapsed: 0.13226724971500184~0.2964498165775455
Time per graph: 0.0030751226057268394~0.00689383719221016
Speed: 818.9985068253327~287.25682864092704
Total Time: 0.0510
best val loss: 0.6930040717124939 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.05s
test Score 0.4884
Epoch Time List: [1.760885203955695, 1.4541658852249384, 0.2209522071061656, 0.22270667681004852, 0.22136555798351765, 0.2331642931094393, 0.23820276302285492, 4.449915056116879, 4.879011059994809, 1.895834185066633, 0.2404429200105369, 0.2249704470159486, 0.2126144740032032, 0.21121248695999384, 0.21181732893455774, 0.2115320519078523, 0.2165017049992457, 0.2159705680096522, 0.2098183010239154, 0.21026071102824062, 0.21244881907477975, 0.21115479595027864, 0.21109085087664425, 0.2148179739015177, 0.21383202506694943, 0.2132463379530236, 3.1174011919647455, 3.3207743890816346, 2.27547162596602, 1.1010917159728706, 0.21898035902995616, 0.2123507511569187, 0.21088609902653843, 0.21097214194014668, 0.2075667860917747, 0.2104884780710563, 0.2138799618696794, 0.2081631701439619, 0.20876182802021503, 0.20822102995589375, 0.20736582297831774, 0.20687860099133104, 0.20719385205302387, 0.2073697600280866, 0.2065826968755573, 0.20720379101112485, 0.20799094915855676, 0.21209805097896606, 0.207026811898686, 0.21212644095066935, 0.21389185113366693, 0.21120259014423937, 1.8051904940512031, 4.162532573915087, 1.5755671618971974, 0.8071818889584392, 1.444942408008501, 0.7699359479593113, 0.6232738670660183, 0.26483158802147955, 0.21603251993656158, 0.21149602194782346, 0.2114055819110945, 0.2180495710344985, 0.22916711890138686, 0.22608981898520142, 0.22617851197719574, 0.22657037305179983, 0.22969038819428533, 0.2256405318621546, 0.2180116280214861, 0.2115110158920288, 0.21131166198756546, 0.21296081005129963, 3.2042024572147056, 1.5776964579708874, 2.85256390995346, 0.42832084104884416, 0.26572151097934693, 0.2344079059548676, 0.21381305111572146, 0.2108328090980649, 0.20804117084480822, 0.20752554503269494, 0.2088469760492444, 0.20745201909448951, 0.20719615288544446, 0.21562095975968987, 0.22606707399245352, 0.23020587919745594, 0.2253660128917545, 0.2226003489922732, 0.22355908900499344, 0.22283752693329006, 0.22459934698417783, 0.22121242003049701, 0.2234329970087856, 0.2223664381308481, 0.22253697202540934, 3.0949309478746727, 3.8928131581051275, 0.7741388108115643, 0.24129677284508944, 0.22572587488684803, 0.22153735382016748, 0.21415577793959528, 0.21467925107572228, 0.21244482905603945, 0.30397706909570843, 0.2129397930111736, 0.22223872400354594, 0.21441930497530848, 0.21572480001486838, 0.2120277089998126, 1.2047746300231665, 2.488362512085587, 1.6997869920451194, 0.7943090581102297, 1.7197729609906673, 1.018124410067685, 0.6162843339843675, 0.23123429797124118, 0.22949941304977983, 0.22831000899896026, 0.2263080650009215, 0.22569638991262764, 0.21929980511777103, 0.21817939798347652, 0.20640650193672627, 0.20659093384165317, 0.21802374394610524, 0.23008007102180272, 0.23063977400306612, 3.9708665400976315, 2.493306268006563, 1.8014369669836015, 0.21997647907119244, 0.2157942660851404, 0.212019273894839, 0.21102982503362, 0.21170452993828803, 0.2149085160344839, 0.23774729203432798, 0.21235629997681826, 0.21234437101520598, 0.21227786096278578, 0.2272006031125784, 0.2230275518959388, 0.21842363791074604, 0.2131313479039818, 0.212895811884664, 0.22235197201371193, 0.2160270569147542, 0.21427336696069688, 0.2157179251080379, 0.21354156406596303, 0.23023704497609288, 3.222882607136853, 0.6042214500484988, 0.5268602349096909, 0.2310294311027974, 0.22851942316628993, 0.22784989001229405, 0.22035290487110615, 0.21142161602620035, 0.21085759194102138, 0.21034621400758624, 0.2518993130652234, 0.2413920130347833, 0.23291347199119627, 0.23186738207004964, 0.23096705100033432, 0.23275374702643603, 0.22527592699043453, 0.2120342020643875, 0.21154232998378575, 0.24126834515482187, 0.2503277459181845, 0.20844934787601233, 5.611600985052064, 3.073703495087102, 2.8694653939455748, 0.42368433508090675, 0.21615427709184587, 0.2109808629611507, 0.21203926706220955, 0.2107727228431031, 0.20867454691324383, 0.21419741096906364, 0.21870357799343765, 0.23220559896435589, 0.22935333487112075, 0.2248473110375926, 0.21127294888719916, 0.21054296707734466, 0.20958639401942492, 0.2081478878390044, 0.35752423806115985, 6.360100543824956, 1.494166059885174, 1.7704595539253205, 0.8781531290151179, 0.21552622399758548, 0.21076827694196254, 0.20987764606252313, 0.20774909504689276, 0.20674400497227907, 0.21097082493361086, 0.20836548309307545, 0.21229644888080657, 0.2160586949903518, 0.20693916617892683, 0.20848859602119774, 0.20889686804730445, 0.2086156930308789, 3.3044701101025566, 1.2872987861046568, 0.26518429594580084, 0.21798469009809196, 0.2100831689313054, 0.22357522207312286, 0.2220461859833449, 0.21592215495184064, 0.2090636850334704, 0.21119102009106427, 0.21131110994610935, 0.20999799692071974, 0.20935574581380934, 0.21911332302261144, 0.20965983893256634, 0.20962799596600235, 0.21005751506891102, 0.210639962926507, 0.21234628290403634, 0.2111750920303166, 0.21227116708178073, 0.211488478933461, 0.2119127430487424, 0.2129174090223387, 0.21050212404225022, 0.21102388994768262, 0.2121561060193926, 0.2134137840475887, 0.21578315796796232, 0.21268467779736966, 0.2136140470393002, 0.21243738289922476, 0.21266932203434408, 0.21237440896220505, 0.21217329311184585, 0.21343459596391767, 0.21513980394229293, 0.2155150338076055, 0.21307235409040004, 0.2145700219552964, 0.21569621202070266, 0.21612438710872084, 5.901394676067866, 3.9268855380360037, 0.2283101207576692, 0.22941265895497054, 0.22925825486890972, 0.22462354600429535, 0.22476225101854652, 0.22365784691646695, 0.22594855388160795, 0.22734706511255354, 0.22512478602584451, 0.2258762918645516, 0.22580934001598507, 0.22525030001997948, 0.23234483308624476, 0.22557126905303448, 0.22568557795602828, 0.22843288502190262, 0.22892209608107805, 1.9648293929640204, 6.595871908124536, 2.4227411379106343, 0.2313485809136182, 0.22812628804240376, 0.2170654599322006, 0.21639358799438924, 0.22019909287337214, 0.21612494089640677, 0.2159813529578969, 0.21593930292874575, 0.21583653008565307, 0.21987867599818856, 0.21715307189151645, 0.2155319699086249, 0.22128825914114714, 0.22358287300448865, 0.21065550204366446, 7.725392063031904, 8.143738385871984, 5.08633343083784, 0.2488213828764856, 0.22655996808316559, 0.2229379640193656, 0.22242275590542704, 0.22461695806123316, 0.2231667210580781, 0.22250619705300778, 0.222669766866602, 0.2226794009329751, 0.2224067939678207, 0.22268695197999477, 0.22533336002379656, 0.22235895600169897, 0.22422250208910555, 0.22299897903576493, 0.22255426493939012, 3.4918223458807915, 1.9053320590173826, 0.233442670898512, 0.2212779929395765, 0.22227816190570593, 0.223782074986957, 0.2233410340268165, 0.22482329013291746, 0.230592702049762, 0.22985356603749096, 0.22460476902779192, 0.24209744192194194, 0.2274790791561827, 0.2294226341182366]
Total Epoch List: [3, 321, 3]
Total Time List: [0.05192692100536078, 0.04800463002175093, 0.05096161609981209]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bfa6200>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.1890;  Loss pred: 2.1890; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.2359;  Loss pred: 2.2359; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.2237;  Loss pred: 2.2237; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5000 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.1555;  Loss pred: 2.1555; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.1861;  Loss pred: 2.1861; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.1161;  Loss pred: 2.1161; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 2.1770;  Loss pred: 2.1770; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.1119;  Loss pred: 2.1119; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 2.1208;  Loss pred: 2.1208; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.0770;  Loss pred: 2.0770; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 2.0082;  Loss pred: 2.0082; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 2.0195;  Loss pred: 2.0195; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 1.9163;  Loss pred: 1.9163; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 1.9396;  Loss pred: 1.9396; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 1.8878;  Loss pred: 1.8878; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 1.9005;  Loss pred: 1.9005; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 1.8315;  Loss pred: 1.8315; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 1.8260;  Loss pred: 1.8260; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 1.7693;  Loss pred: 1.7693; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 1.7562;  Loss pred: 1.7562; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 1.7150;  Loss pred: 1.7150; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 1.7062;  Loss pred: 1.7062; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 1.6643;  Loss pred: 1.6643; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 1.6447;  Loss pred: 1.6447; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 1.6344;  Loss pred: 1.6344; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 1.6080;  Loss pred: 1.6080; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 1.5866;  Loss pred: 1.5866; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 1.5762;  Loss pred: 1.5762; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 1.5504;  Loss pred: 1.5504; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 1.4982;  Loss pred: 1.4982; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 1.4757;  Loss pred: 1.4757; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 1.4776;  Loss pred: 1.4776; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 1.4679;  Loss pred: 1.4679; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 1.4540;  Loss pred: 1.4540; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 1.4010;  Loss pred: 1.4010; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 1.3997;  Loss pred: 1.3997; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 1.3865;  Loss pred: 1.3865; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 1.3929;  Loss pred: 1.3929; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 1.3612;  Loss pred: 1.3612; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 1.3492;  Loss pred: 1.3492; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 2.04s
Epoch 41/1000, LR 0.000269
Train loss: 1.3361;  Loss pred: 1.3361; Loss self: 0.0000; time: 2.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.10s
Epoch 42/1000, LR 0.000269
Train loss: 1.3196;  Loss pred: 1.3196; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 1.2957;  Loss pred: 1.2957; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 1.3034;  Loss pred: 1.3034; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 1.2840;  Loss pred: 1.2840; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 1.2684;  Loss pred: 1.2684; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 1.2559;  Loss pred: 1.2559; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 1.2512;  Loss pred: 1.2512; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 1.2473;  Loss pred: 1.2473; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 1.2325;  Loss pred: 1.2325; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 1.2326;  Loss pred: 1.2326; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 1.2200;  Loss pred: 1.2200; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 1.2071;  Loss pred: 1.2071; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 1.1982;  Loss pred: 1.1982; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 1.1946;  Loss pred: 1.1946; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 1.1878;  Loss pred: 1.1878; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 1.1697;  Loss pred: 1.1697; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 1.1636;  Loss pred: 1.1636; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 1.1586;  Loss pred: 1.1586; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 1.1463;  Loss pred: 1.1463; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 1.1525;  Loss pred: 1.1525; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5000 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 1.1428;  Loss pred: 1.1428; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 1.1233;  Loss pred: 1.1233; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 1.1282;  Loss pred: 1.1282; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 1.1150;  Loss pred: 1.1150; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.5000 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 1.1138;  Loss pred: 1.1138; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4884 time: 0.05s
Test loss: 0.6896 score: 0.5227 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 1.1078;  Loss pred: 1.1078; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4884 time: 0.05s
Test loss: 0.6894 score: 0.5227 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 1.1060;  Loss pred: 1.1060; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4884 time: 0.05s
Test loss: 0.6893 score: 0.5227 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 1.1054;  Loss pred: 1.1054; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4884 time: 0.05s
Test loss: 0.6891 score: 0.5227 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 1.0918;  Loss pred: 1.0918; Loss self: 0.0000; time: 0.13s
Val loss: 0.6914 score: 0.5116 time: 0.05s
Test loss: 0.6889 score: 0.5455 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 1.0975;  Loss pred: 1.0975; Loss self: 0.0000; time: 0.13s
Val loss: 0.6913 score: 0.5349 time: 0.05s
Test loss: 0.6887 score: 0.5455 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 1.0855;  Loss pred: 1.0855; Loss self: 0.0000; time: 0.13s
Val loss: 0.6912 score: 0.5581 time: 0.05s
Test loss: 0.6886 score: 0.5909 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 1.0873;  Loss pred: 1.0873; Loss self: 0.0000; time: 0.13s
Val loss: 0.6910 score: 0.5581 time: 0.05s
Test loss: 0.6884 score: 0.5909 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 1.0806;  Loss pred: 1.0806; Loss self: 0.0000; time: 0.13s
Val loss: 0.6909 score: 0.5581 time: 0.05s
Test loss: 0.6881 score: 0.6364 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 1.0812;  Loss pred: 1.0812; Loss self: 0.0000; time: 0.13s
Val loss: 0.6907 score: 0.5814 time: 0.05s
Test loss: 0.6879 score: 0.6591 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 1.0764;  Loss pred: 1.0764; Loss self: 0.0000; time: 0.13s
Val loss: 0.6906 score: 0.5814 time: 0.05s
Test loss: 0.6877 score: 0.6591 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 1.0736;  Loss pred: 1.0736; Loss self: 0.0000; time: 0.13s
Val loss: 0.6904 score: 0.6047 time: 0.05s
Test loss: 0.6875 score: 0.6818 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 1.0643;  Loss pred: 1.0643; Loss self: 0.0000; time: 0.12s
Val loss: 0.6902 score: 0.6279 time: 0.05s
Test loss: 0.6872 score: 0.6818 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 1.0589;  Loss pred: 1.0589; Loss self: 0.0000; time: 0.12s
Val loss: 0.6900 score: 0.6279 time: 0.05s
Test loss: 0.6870 score: 0.6818 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 1.0569;  Loss pred: 1.0569; Loss self: 0.0000; time: 0.12s
Val loss: 0.6898 score: 0.6744 time: 0.05s
Test loss: 0.6866 score: 0.6818 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 1.0575;  Loss pred: 1.0575; Loss self: 0.0000; time: 0.12s
Val loss: 0.6896 score: 0.6977 time: 0.05s
Test loss: 0.6863 score: 0.6818 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 1.0543;  Loss pred: 1.0543; Loss self: 0.0000; time: 0.12s
Val loss: 0.6894 score: 0.6977 time: 0.05s
Test loss: 0.6861 score: 0.6818 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 1.0514;  Loss pred: 1.0514; Loss self: 0.0000; time: 0.12s
Val loss: 0.6891 score: 0.7209 time: 0.05s
Test loss: 0.6858 score: 0.6818 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 1.0480;  Loss pred: 1.0480; Loss self: 0.0000; time: 0.13s
Val loss: 0.6889 score: 0.7209 time: 0.05s
Test loss: 0.6855 score: 0.7045 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 1.0435;  Loss pred: 1.0435; Loss self: 0.0000; time: 0.13s
Val loss: 0.6887 score: 0.7209 time: 0.05s
Test loss: 0.6852 score: 0.7273 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 1.0424;  Loss pred: 1.0424; Loss self: 0.0000; time: 0.13s
Val loss: 0.6884 score: 0.7209 time: 0.05s
Test loss: 0.6848 score: 0.7500 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 1.0381;  Loss pred: 1.0381; Loss self: 0.0000; time: 1.36s
Val loss: 0.6882 score: 0.7209 time: 0.33s
Test loss: 0.6845 score: 0.7500 time: 0.44s
Epoch 88/1000, LR 0.000266
Train loss: 1.0392;  Loss pred: 1.0392; Loss self: 0.0000; time: 0.74s
Val loss: 0.6879 score: 0.7209 time: 0.29s
Test loss: 0.6842 score: 0.7500 time: 0.40s
Epoch 89/1000, LR 0.000266
Train loss: 1.0383;  Loss pred: 1.0383; Loss self: 0.0000; time: 0.73s
Val loss: 0.6877 score: 0.7209 time: 0.39s
Test loss: 0.6838 score: 0.7500 time: 0.49s
Epoch 90/1000, LR 0.000266
Train loss: 1.0303;  Loss pred: 1.0303; Loss self: 0.0000; time: 0.97s
Val loss: 0.6874 score: 0.7209 time: 0.13s
Test loss: 0.6835 score: 0.7500 time: 0.36s
Epoch 91/1000, LR 0.000266
Train loss: 1.0286;  Loss pred: 1.0286; Loss self: 0.0000; time: 4.68s
Val loss: 0.6871 score: 0.7209 time: 0.95s
Test loss: 0.6831 score: 0.7500 time: 0.65s
Epoch 92/1000, LR 0.000266
Train loss: 1.0216;  Loss pred: 1.0216; Loss self: 0.0000; time: 1.07s
Val loss: 0.6868 score: 0.7209 time: 0.09s
Test loss: 0.6827 score: 0.7500 time: 0.05s
Epoch 93/1000, LR 0.000265
Train loss: 1.0215;  Loss pred: 1.0215; Loss self: 0.0000; time: 0.14s
Val loss: 0.6865 score: 0.7442 time: 0.06s
Test loss: 0.6823 score: 0.7500 time: 0.05s
Epoch 94/1000, LR 0.000265
Train loss: 1.0171;  Loss pred: 1.0171; Loss self: 0.0000; time: 0.15s
Val loss: 0.6861 score: 0.7442 time: 0.06s
Test loss: 0.6819 score: 0.7727 time: 0.05s
Epoch 95/1000, LR 0.000265
Train loss: 1.0191;  Loss pred: 1.0191; Loss self: 0.0000; time: 0.13s
Val loss: 0.6858 score: 0.7442 time: 0.05s
Test loss: 0.6814 score: 0.7727 time: 0.05s
Epoch 96/1000, LR 0.000265
Train loss: 1.0145;  Loss pred: 1.0145; Loss self: 0.0000; time: 0.13s
Val loss: 0.6854 score: 0.7442 time: 0.05s
Test loss: 0.6810 score: 0.7727 time: 0.05s
Epoch 97/1000, LR 0.000265
Train loss: 1.0171;  Loss pred: 1.0171; Loss self: 0.0000; time: 0.14s
Val loss: 0.6851 score: 0.7674 time: 0.05s
Test loss: 0.6805 score: 0.7727 time: 0.05s
Epoch 98/1000, LR 0.000265
Train loss: 1.0141;  Loss pred: 1.0141; Loss self: 0.0000; time: 0.14s
Val loss: 0.6847 score: 0.7674 time: 0.05s
Test loss: 0.6801 score: 0.7727 time: 0.05s
Epoch 99/1000, LR 0.000265
Train loss: 1.0177;  Loss pred: 1.0177; Loss self: 0.0000; time: 0.13s
Val loss: 0.6844 score: 0.7442 time: 0.05s
Test loss: 0.6796 score: 0.7955 time: 0.05s
Epoch 100/1000, LR 0.000265
Train loss: 1.0128;  Loss pred: 1.0128; Loss self: 0.0000; time: 0.13s
Val loss: 0.6840 score: 0.7442 time: 0.05s
Test loss: 0.6791 score: 0.7955 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 1.0087;  Loss pred: 1.0087; Loss self: 0.0000; time: 0.14s
Val loss: 0.6836 score: 0.7442 time: 0.05s
Test loss: 0.6786 score: 0.7955 time: 0.05s
Epoch 102/1000, LR 0.000264
Train loss: 1.0018;  Loss pred: 1.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.6832 score: 0.7442 time: 0.05s
Test loss: 0.6781 score: 0.7955 time: 0.05s
Epoch 103/1000, LR 0.000264
Train loss: 1.0060;  Loss pred: 1.0060; Loss self: 0.0000; time: 0.14s
Val loss: 0.6828 score: 0.7442 time: 0.05s
Test loss: 0.6775 score: 0.7955 time: 0.05s
Epoch 104/1000, LR 0.000264
Train loss: 1.0046;  Loss pred: 1.0046; Loss self: 0.0000; time: 0.14s
Val loss: 0.6823 score: 0.7442 time: 0.05s
Test loss: 0.6770 score: 0.7955 time: 0.05s
Epoch 105/1000, LR 0.000264
Train loss: 1.0015;  Loss pred: 1.0015; Loss self: 0.0000; time: 0.13s
Val loss: 0.6819 score: 0.7442 time: 0.05s
Test loss: 0.6764 score: 0.7955 time: 0.05s
Epoch 106/1000, LR 0.000264
Train loss: 1.0015;  Loss pred: 1.0015; Loss self: 0.0000; time: 0.14s
Val loss: 0.6814 score: 0.7442 time: 0.05s
Test loss: 0.6758 score: 0.8409 time: 0.05s
Epoch 107/1000, LR 0.000264
Train loss: 0.9985;  Loss pred: 0.9985; Loss self: 0.0000; time: 0.14s
Val loss: 0.6809 score: 0.7674 time: 0.05s
Test loss: 0.6752 score: 0.8409 time: 0.05s
Epoch 108/1000, LR 0.000264
Train loss: 0.9954;  Loss pred: 0.9954; Loss self: 0.0000; time: 0.42s
Val loss: 0.6804 score: 0.7674 time: 0.14s
Test loss: 0.6746 score: 0.8409 time: 0.61s
Epoch 109/1000, LR 0.000264
Train loss: 0.9903;  Loss pred: 0.9903; Loss self: 0.0000; time: 1.39s
Val loss: 0.6798 score: 0.7907 time: 0.36s
Test loss: 0.6740 score: 0.8409 time: 0.33s
Epoch 110/1000, LR 0.000263
Train loss: 0.9915;  Loss pred: 0.9915; Loss self: 0.0000; time: 0.46s
Val loss: 0.6793 score: 0.8140 time: 0.08s
Test loss: 0.6734 score: 0.8409 time: 0.15s
Epoch 111/1000, LR 0.000263
Train loss: 0.9911;  Loss pred: 0.9911; Loss self: 0.0000; time: 0.33s
Val loss: 0.6788 score: 0.8372 time: 0.05s
Test loss: 0.6727 score: 0.8409 time: 0.04s
Epoch 112/1000, LR 0.000263
Train loss: 0.9870;  Loss pred: 0.9870; Loss self: 0.0000; time: 0.13s
Val loss: 0.6782 score: 0.8372 time: 0.05s
Test loss: 0.6720 score: 0.8409 time: 0.04s
Epoch 113/1000, LR 0.000263
Train loss: 0.9872;  Loss pred: 0.9872; Loss self: 0.0000; time: 0.13s
Val loss: 0.6776 score: 0.8372 time: 0.05s
Test loss: 0.6713 score: 0.8409 time: 0.04s
Epoch 114/1000, LR 0.000263
Train loss: 0.9849;  Loss pred: 0.9849; Loss self: 0.0000; time: 0.12s
Val loss: 0.6771 score: 0.8372 time: 0.05s
Test loss: 0.6706 score: 0.8409 time: 0.04s
Epoch 115/1000, LR 0.000263
Train loss: 0.9832;  Loss pred: 0.9832; Loss self: 0.0000; time: 0.12s
Val loss: 0.6765 score: 0.8372 time: 0.05s
Test loss: 0.6699 score: 0.8409 time: 0.04s
Epoch 116/1000, LR 0.000263
Train loss: 0.9795;  Loss pred: 0.9795; Loss self: 0.0000; time: 0.12s
Val loss: 0.6759 score: 0.8372 time: 0.05s
Test loss: 0.6692 score: 0.8409 time: 0.04s
Epoch 117/1000, LR 0.000262
Train loss: 0.9824;  Loss pred: 0.9824; Loss self: 0.0000; time: 0.12s
Val loss: 0.6753 score: 0.8372 time: 0.05s
Test loss: 0.6684 score: 0.8409 time: 0.04s
Epoch 118/1000, LR 0.000262
Train loss: 0.9794;  Loss pred: 0.9794; Loss self: 0.0000; time: 0.12s
Val loss: 0.6747 score: 0.8372 time: 0.05s
Test loss: 0.6677 score: 0.8409 time: 0.04s
Epoch 119/1000, LR 0.000262
Train loss: 0.9784;  Loss pred: 0.9784; Loss self: 0.0000; time: 0.12s
Val loss: 0.6740 score: 0.8372 time: 0.05s
Test loss: 0.6669 score: 0.8409 time: 0.05s
Epoch 120/1000, LR 0.000262
Train loss: 0.9714;  Loss pred: 0.9714; Loss self: 0.0000; time: 0.13s
Val loss: 0.6733 score: 0.8372 time: 0.05s
Test loss: 0.6661 score: 0.8409 time: 0.05s
Epoch 121/1000, LR 0.000262
Train loss: 0.9705;  Loss pred: 0.9705; Loss self: 0.0000; time: 0.13s
Val loss: 0.6727 score: 0.8372 time: 0.05s
Test loss: 0.6652 score: 0.8182 time: 0.05s
Epoch 122/1000, LR 0.000262
Train loss: 0.9665;  Loss pred: 0.9665; Loss self: 0.0000; time: 0.13s
Val loss: 0.6720 score: 0.8372 time: 0.05s
Test loss: 0.6644 score: 0.8409 time: 0.05s
Epoch 123/1000, LR 0.000262
Train loss: 0.9706;  Loss pred: 0.9706; Loss self: 0.0000; time: 0.13s
Val loss: 0.6713 score: 0.8372 time: 0.05s
Test loss: 0.6635 score: 0.8409 time: 0.05s
Epoch 124/1000, LR 0.000261
Train loss: 0.9679;  Loss pred: 0.9679; Loss self: 0.0000; time: 0.13s
Val loss: 0.6706 score: 0.8372 time: 0.05s
Test loss: 0.6627 score: 0.8409 time: 0.05s
Epoch 125/1000, LR 0.000261
Train loss: 0.9662;  Loss pred: 0.9662; Loss self: 0.0000; time: 0.13s
Val loss: 0.6698 score: 0.8372 time: 0.05s
Test loss: 0.6618 score: 0.8409 time: 0.05s
Epoch 126/1000, LR 0.000261
Train loss: 0.9655;  Loss pred: 0.9655; Loss self: 0.0000; time: 0.13s
Val loss: 0.6691 score: 0.8372 time: 0.05s
Test loss: 0.6609 score: 0.8409 time: 0.05s
Epoch 127/1000, LR 0.000261
Train loss: 0.9604;  Loss pred: 0.9604; Loss self: 0.0000; time: 0.14s
Val loss: 0.6683 score: 0.8372 time: 0.05s
Test loss: 0.6599 score: 0.8409 time: 0.05s
Epoch 128/1000, LR 0.000261
Train loss: 0.9633;  Loss pred: 0.9633; Loss self: 0.0000; time: 0.13s
Val loss: 0.6675 score: 0.8372 time: 0.05s
Test loss: 0.6590 score: 0.8636 time: 0.05s
Epoch 129/1000, LR 0.000261
Train loss: 0.9594;  Loss pred: 0.9594; Loss self: 0.0000; time: 0.13s
Val loss: 0.6667 score: 0.8372 time: 0.05s
Test loss: 0.6580 score: 0.8409 time: 0.05s
Epoch 130/1000, LR 0.000260
Train loss: 0.9599;  Loss pred: 0.9599; Loss self: 0.0000; time: 0.13s
Val loss: 0.6658 score: 0.8605 time: 0.05s
Test loss: 0.6570 score: 0.8182 time: 0.05s
Epoch 131/1000, LR 0.000260
Train loss: 0.9530;  Loss pred: 0.9530; Loss self: 0.0000; time: 0.13s
Val loss: 0.6649 score: 0.8605 time: 0.05s
Test loss: 0.6559 score: 0.8182 time: 0.05s
Epoch 132/1000, LR 0.000260
Train loss: 0.9556;  Loss pred: 0.9556; Loss self: 0.0000; time: 0.13s
Val loss: 0.6640 score: 0.8372 time: 0.05s
Test loss: 0.6549 score: 0.8182 time: 0.05s
Epoch 133/1000, LR 0.000260
Train loss: 0.9569;  Loss pred: 0.9569; Loss self: 0.0000; time: 0.13s
Val loss: 0.6631 score: 0.8372 time: 0.05s
Test loss: 0.6538 score: 0.8182 time: 0.05s
Epoch 134/1000, LR 0.000260
Train loss: 0.9502;  Loss pred: 0.9502; Loss self: 0.0000; time: 0.14s
Val loss: 0.6621 score: 0.8372 time: 0.05s
Test loss: 0.6527 score: 0.8182 time: 0.05s
Epoch 135/1000, LR 0.000260
Train loss: 0.9491;  Loss pred: 0.9491; Loss self: 0.0000; time: 0.13s
Val loss: 0.6611 score: 0.8372 time: 0.05s
Test loss: 0.6515 score: 0.8182 time: 0.05s
Epoch 136/1000, LR 0.000260
Train loss: 0.9521;  Loss pred: 0.9521; Loss self: 0.0000; time: 0.13s
Val loss: 0.6601 score: 0.8372 time: 0.05s
Test loss: 0.6504 score: 0.8182 time: 0.05s
Epoch 137/1000, LR 0.000259
Train loss: 0.9471;  Loss pred: 0.9471; Loss self: 0.0000; time: 0.13s
Val loss: 0.6591 score: 0.8605 time: 0.05s
Test loss: 0.6492 score: 0.8182 time: 0.05s
Epoch 138/1000, LR 0.000259
Train loss: 0.9464;  Loss pred: 0.9464; Loss self: 0.0000; time: 0.13s
Val loss: 0.6580 score: 0.8605 time: 0.05s
Test loss: 0.6480 score: 0.8182 time: 0.05s
Epoch 139/1000, LR 0.000259
Train loss: 0.9428;  Loss pred: 0.9428; Loss self: 0.0000; time: 0.13s
Val loss: 0.6569 score: 0.8605 time: 0.05s
Test loss: 0.6467 score: 0.8182 time: 0.05s
Epoch 140/1000, LR 0.000259
Train loss: 0.9429;  Loss pred: 0.9429; Loss self: 0.0000; time: 0.13s
Val loss: 0.6558 score: 0.8605 time: 0.05s
Test loss: 0.6455 score: 0.8182 time: 0.05s
Epoch 141/1000, LR 0.000259
Train loss: 0.9398;  Loss pred: 0.9398; Loss self: 0.0000; time: 0.13s
Val loss: 0.6547 score: 0.8605 time: 0.05s
Test loss: 0.6442 score: 0.8182 time: 0.05s
Epoch 142/1000, LR 0.000259
Train loss: 0.9386;  Loss pred: 0.9386; Loss self: 0.0000; time: 0.13s
Val loss: 0.6536 score: 0.8605 time: 0.05s
Test loss: 0.6429 score: 0.8409 time: 0.05s
Epoch 143/1000, LR 0.000258
Train loss: 0.9354;  Loss pred: 0.9354; Loss self: 0.0000; time: 0.13s
Val loss: 0.6524 score: 0.8605 time: 0.05s
Test loss: 0.6416 score: 0.8409 time: 0.05s
Epoch 144/1000, LR 0.000258
Train loss: 0.9323;  Loss pred: 0.9323; Loss self: 0.0000; time: 0.13s
Val loss: 0.6513 score: 0.8605 time: 0.05s
Test loss: 0.6403 score: 0.8409 time: 0.05s
Epoch 145/1000, LR 0.000258
Train loss: 0.9331;  Loss pred: 0.9331; Loss self: 0.0000; time: 0.13s
Val loss: 0.6501 score: 0.8605 time: 0.05s
Test loss: 0.6390 score: 0.8409 time: 0.05s
Epoch 146/1000, LR 0.000258
Train loss: 0.9301;  Loss pred: 0.9301; Loss self: 0.0000; time: 0.13s
Val loss: 0.6489 score: 0.8605 time: 0.05s
Test loss: 0.6377 score: 0.8182 time: 0.05s
Epoch 147/1000, LR 0.000258
Train loss: 0.9305;  Loss pred: 0.9305; Loss self: 0.0000; time: 0.13s
Val loss: 0.6476 score: 0.8605 time: 0.05s
Test loss: 0.6364 score: 0.8182 time: 0.05s
Epoch 148/1000, LR 0.000257
Train loss: 0.9287;  Loss pred: 0.9287; Loss self: 0.0000; time: 0.14s
Val loss: 0.6464 score: 0.8605 time: 0.05s
Test loss: 0.6350 score: 0.8182 time: 0.05s
Epoch 149/1000, LR 0.000257
Train loss: 0.9214;  Loss pred: 0.9214; Loss self: 0.0000; time: 0.13s
Val loss: 0.6452 score: 0.8605 time: 0.05s
Test loss: 0.6337 score: 0.8182 time: 0.05s
Epoch 150/1000, LR 0.000257
Train loss: 0.9243;  Loss pred: 0.9243; Loss self: 0.0000; time: 0.13s
Val loss: 0.6439 score: 0.8605 time: 0.05s
Test loss: 0.6323 score: 0.8182 time: 0.05s
Epoch 151/1000, LR 0.000257
Train loss: 0.9242;  Loss pred: 0.9242; Loss self: 0.0000; time: 0.13s
Val loss: 0.6427 score: 0.8605 time: 0.05s
Test loss: 0.6309 score: 0.8182 time: 0.05s
Epoch 152/1000, LR 0.000257
Train loss: 0.9221;  Loss pred: 0.9221; Loss self: 0.0000; time: 0.13s
Val loss: 0.6414 score: 0.8605 time: 0.05s
Test loss: 0.6294 score: 0.8182 time: 0.05s
Epoch 153/1000, LR 0.000257
Train loss: 0.9162;  Loss pred: 0.9162; Loss self: 0.0000; time: 0.13s
Val loss: 0.6402 score: 0.8605 time: 0.05s
Test loss: 0.6280 score: 0.8409 time: 0.05s
Epoch 154/1000, LR 0.000256
Train loss: 0.9180;  Loss pred: 0.9180; Loss self: 0.0000; time: 0.13s
Val loss: 0.6389 score: 0.8605 time: 0.05s
Test loss: 0.6265 score: 0.8409 time: 0.05s
Epoch 155/1000, LR 0.000256
Train loss: 0.9194;  Loss pred: 0.9194; Loss self: 0.0000; time: 0.14s
Val loss: 0.6376 score: 0.8605 time: 0.05s
Test loss: 0.6250 score: 0.8409 time: 0.05s
Epoch 156/1000, LR 0.000256
Train loss: 0.9163;  Loss pred: 0.9163; Loss self: 0.0000; time: 0.13s
Val loss: 0.6363 score: 0.8605 time: 0.05s
Test loss: 0.6235 score: 0.8409 time: 0.05s
Epoch 157/1000, LR 0.000256
Train loss: 0.9138;  Loss pred: 0.9138; Loss self: 0.0000; time: 0.13s
Val loss: 0.6349 score: 0.8605 time: 0.05s
Test loss: 0.6220 score: 0.8636 time: 0.05s
Epoch 158/1000, LR 0.000256
Train loss: 0.9051;  Loss pred: 0.9051; Loss self: 0.0000; time: 0.14s
Val loss: 0.6336 score: 0.8605 time: 0.05s
Test loss: 0.6204 score: 0.8864 time: 0.05s
Epoch 159/1000, LR 0.000255
Train loss: 0.9098;  Loss pred: 0.9098; Loss self: 0.0000; time: 0.13s
Val loss: 0.6322 score: 0.8605 time: 0.05s
Test loss: 0.6189 score: 0.8636 time: 0.05s
Epoch 160/1000, LR 0.000255
Train loss: 0.9094;  Loss pred: 0.9094; Loss self: 0.0000; time: 0.13s
Val loss: 0.6308 score: 0.8605 time: 0.05s
Test loss: 0.6173 score: 0.8636 time: 0.04s
Epoch 161/1000, LR 0.000255
Train loss: 0.9023;  Loss pred: 0.9023; Loss self: 0.0000; time: 0.14s
Val loss: 0.6294 score: 0.8605 time: 0.05s
Test loss: 0.6157 score: 0.8636 time: 0.04s
Epoch 162/1000, LR 0.000255
Train loss: 0.9057;  Loss pred: 0.9057; Loss self: 0.0000; time: 0.13s
Val loss: 0.6279 score: 0.8605 time: 0.05s
Test loss: 0.6142 score: 0.8636 time: 0.04s
Epoch 163/1000, LR 0.000255
Train loss: 0.9027;  Loss pred: 0.9027; Loss self: 0.0000; time: 0.55s
Val loss: 0.6264 score: 0.8605 time: 0.78s
Test loss: 0.6126 score: 0.8636 time: 0.45s
Epoch 164/1000, LR 0.000254
Train loss: 0.9008;  Loss pred: 0.9008; Loss self: 0.0000; time: 0.67s
Val loss: 0.6249 score: 0.8605 time: 0.49s
Test loss: 0.6110 score: 0.8864 time: 0.45s
Epoch 165/1000, LR 0.000254
Train loss: 0.8995;  Loss pred: 0.8995; Loss self: 0.0000; time: 0.69s
Val loss: 0.6233 score: 0.8605 time: 0.40s
Test loss: 0.6094 score: 0.8636 time: 1.57s
Epoch 166/1000, LR 0.000254
Train loss: 0.8983;  Loss pred: 0.8983; Loss self: 0.0000; time: 4.26s
Val loss: 0.6217 score: 0.8605 time: 0.75s
Test loss: 0.6078 score: 0.8409 time: 0.28s
Epoch 167/1000, LR 0.000254
Train loss: 0.8979;  Loss pred: 0.8979; Loss self: 0.0000; time: 1.12s
Val loss: 0.6201 score: 0.8837 time: 0.09s
Test loss: 0.6062 score: 0.8409 time: 0.22s
Epoch 168/1000, LR 0.000254
Train loss: 0.8963;  Loss pred: 0.8963; Loss self: 0.0000; time: 0.16s
Val loss: 0.6185 score: 0.8837 time: 0.05s
Test loss: 0.6045 score: 0.8409 time: 0.05s
Epoch 169/1000, LR 0.000253
Train loss: 0.8865;  Loss pred: 0.8865; Loss self: 0.0000; time: 0.14s
Val loss: 0.6169 score: 0.8837 time: 0.05s
Test loss: 0.6028 score: 0.8409 time: 0.05s
Epoch 170/1000, LR 0.000253
Train loss: 0.8913;  Loss pred: 0.8913; Loss self: 0.0000; time: 0.14s
Val loss: 0.6154 score: 0.8837 time: 0.05s
Test loss: 0.6010 score: 0.8636 time: 0.05s
Epoch 171/1000, LR 0.000253
Train loss: 0.8897;  Loss pred: 0.8897; Loss self: 0.0000; time: 0.13s
Val loss: 0.6138 score: 0.8605 time: 0.05s
Test loss: 0.5992 score: 0.8636 time: 0.05s
Epoch 172/1000, LR 0.000253
Train loss: 0.8905;  Loss pred: 0.8905; Loss self: 0.0000; time: 0.13s
Val loss: 0.6122 score: 0.8605 time: 0.05s
Test loss: 0.5974 score: 0.8636 time: 0.05s
Epoch 173/1000, LR 0.000253
Train loss: 0.8875;  Loss pred: 0.8875; Loss self: 0.0000; time: 0.14s
Val loss: 0.6105 score: 0.8605 time: 0.05s
Test loss: 0.5956 score: 0.8636 time: 0.05s
Epoch 174/1000, LR 0.000252
Train loss: 0.8846;  Loss pred: 0.8846; Loss self: 0.0000; time: 0.13s
Val loss: 0.6089 score: 0.8605 time: 0.05s
Test loss: 0.5937 score: 0.8636 time: 0.05s
Epoch 175/1000, LR 0.000252
Train loss: 0.8830;  Loss pred: 0.8830; Loss self: 0.0000; time: 0.14s
Val loss: 0.6071 score: 0.8605 time: 0.05s
Test loss: 0.5919 score: 0.8636 time: 0.05s
Epoch 176/1000, LR 0.000252
Train loss: 0.8798;  Loss pred: 0.8798; Loss self: 0.0000; time: 0.14s
Val loss: 0.6053 score: 0.8837 time: 0.05s
Test loss: 0.5901 score: 0.8636 time: 0.05s
Epoch 177/1000, LR 0.000252
Train loss: 0.8793;  Loss pred: 0.8793; Loss self: 0.0000; time: 0.13s
Val loss: 0.6034 score: 0.8837 time: 0.05s
Test loss: 0.5883 score: 0.8636 time: 0.05s
Epoch 178/1000, LR 0.000251
Train loss: 0.8779;  Loss pred: 0.8779; Loss self: 0.0000; time: 0.13s
Val loss: 0.6015 score: 0.9070 time: 0.05s
Test loss: 0.5865 score: 0.8636 time: 0.05s
Epoch 179/1000, LR 0.000251
Train loss: 0.8772;  Loss pred: 0.8772; Loss self: 0.0000; time: 0.13s
Val loss: 0.5996 score: 0.9070 time: 0.05s
Test loss: 0.5847 score: 0.8636 time: 0.05s
Epoch 180/1000, LR 0.000251
Train loss: 0.8731;  Loss pred: 0.8731; Loss self: 0.0000; time: 0.13s
Val loss: 0.5976 score: 0.9070 time: 0.05s
Test loss: 0.5829 score: 0.8636 time: 0.05s
Epoch 181/1000, LR 0.000251
Train loss: 0.8717;  Loss pred: 0.8717; Loss self: 0.0000; time: 0.13s
Val loss: 0.5957 score: 0.9070 time: 0.05s
Test loss: 0.5811 score: 0.8636 time: 0.05s
Epoch 182/1000, LR 0.000251
Train loss: 0.8661;  Loss pred: 0.8661; Loss self: 0.0000; time: 0.14s
Val loss: 0.5938 score: 0.9070 time: 0.05s
Test loss: 0.5793 score: 0.8409 time: 0.05s
Epoch 183/1000, LR 0.000250
Train loss: 0.8658;  Loss pred: 0.8658; Loss self: 0.0000; time: 0.13s
Val loss: 0.5919 score: 0.9070 time: 0.05s
Test loss: 0.5773 score: 0.8636 time: 0.05s
Epoch 184/1000, LR 0.000250
Train loss: 0.8661;  Loss pred: 0.8661; Loss self: 0.0000; time: 0.14s
Val loss: 0.5901 score: 0.9070 time: 0.05s
Test loss: 0.5753 score: 0.8636 time: 0.05s
Epoch 185/1000, LR 0.000250
Train loss: 0.8647;  Loss pred: 0.8647; Loss self: 0.0000; time: 0.14s
Val loss: 0.5882 score: 0.9070 time: 0.05s
Test loss: 0.5732 score: 0.8636 time: 0.05s
Epoch 186/1000, LR 0.000250
Train loss: 0.8634;  Loss pred: 0.8634; Loss self: 0.0000; time: 0.14s
Val loss: 0.5864 score: 0.9070 time: 0.05s
Test loss: 0.5712 score: 0.8636 time: 0.05s
Epoch 187/1000, LR 0.000249
Train loss: 0.8619;  Loss pred: 0.8619; Loss self: 0.0000; time: 1.19s
Val loss: 0.5846 score: 0.9070 time: 0.45s
Test loss: 0.5691 score: 0.8636 time: 0.40s
Epoch 188/1000, LR 0.000249
Train loss: 0.8571;  Loss pred: 0.8571; Loss self: 0.0000; time: 0.93s
Val loss: 0.5827 score: 0.9070 time: 0.89s
Test loss: 0.5671 score: 0.8636 time: 0.39s
Epoch 189/1000, LR 0.000249
Train loss: 0.8550;  Loss pred: 0.8550; Loss self: 0.0000; time: 0.66s
Val loss: 0.5808 score: 0.9070 time: 0.08s
Test loss: 0.5650 score: 0.8636 time: 0.12s
Epoch 190/1000, LR 0.000249
Train loss: 0.8573;  Loss pred: 0.8573; Loss self: 0.0000; time: 0.86s
Val loss: 0.5788 score: 0.9070 time: 0.34s
Test loss: 0.5630 score: 0.8636 time: 0.06s
Epoch 191/1000, LR 0.000249
Train loss: 0.8529;  Loss pred: 0.8529; Loss self: 0.0000; time: 0.23s
Val loss: 0.5768 score: 0.9070 time: 0.05s
Test loss: 0.5610 score: 0.8636 time: 0.05s
Epoch 192/1000, LR 0.000248
Train loss: 0.8536;  Loss pred: 0.8536; Loss self: 0.0000; time: 0.13s
Val loss: 0.5746 score: 0.9070 time: 0.05s
Test loss: 0.5590 score: 0.8636 time: 0.04s
Epoch 193/1000, LR 0.000248
Train loss: 0.8488;  Loss pred: 0.8488; Loss self: 0.0000; time: 0.12s
Val loss: 0.5724 score: 0.9070 time: 0.05s
Test loss: 0.5570 score: 0.8636 time: 0.04s
Epoch 194/1000, LR 0.000248
Train loss: 0.8461;  Loss pred: 0.8461; Loss self: 0.0000; time: 0.12s
Val loss: 0.5702 score: 0.9070 time: 0.05s
Test loss: 0.5550 score: 0.8636 time: 0.04s
Epoch 195/1000, LR 0.000248
Train loss: 0.8460;  Loss pred: 0.8460; Loss self: 0.0000; time: 0.12s
Val loss: 0.5681 score: 0.9070 time: 0.05s
Test loss: 0.5530 score: 0.8636 time: 0.04s
Epoch 196/1000, LR 0.000247
Train loss: 0.8436;  Loss pred: 0.8436; Loss self: 0.0000; time: 0.12s
Val loss: 0.5660 score: 0.9070 time: 0.05s
Test loss: 0.5510 score: 0.8636 time: 0.04s
Epoch 197/1000, LR 0.000247
Train loss: 0.8439;  Loss pred: 0.8439; Loss self: 0.0000; time: 0.12s
Val loss: 0.5639 score: 0.9070 time: 0.05s
Test loss: 0.5489 score: 0.8864 time: 0.04s
Epoch 198/1000, LR 0.000247
Train loss: 0.8399;  Loss pred: 0.8399; Loss self: 0.0000; time: 0.12s
Val loss: 0.5618 score: 0.9070 time: 0.05s
Test loss: 0.5468 score: 0.8864 time: 0.04s
Epoch 199/1000, LR 0.000247
Train loss: 0.8344;  Loss pred: 0.8344; Loss self: 0.0000; time: 0.12s
Val loss: 0.5598 score: 0.9070 time: 0.05s
Test loss: 0.5446 score: 0.8864 time: 0.04s
Epoch 200/1000, LR 0.000246
Train loss: 0.8362;  Loss pred: 0.8362; Loss self: 0.0000; time: 0.12s
Val loss: 0.5577 score: 0.9070 time: 0.05s
Test loss: 0.5425 score: 0.8864 time: 0.04s
Epoch 201/1000, LR 0.000246
Train loss: 0.8350;  Loss pred: 0.8350; Loss self: 0.0000; time: 0.12s
Val loss: 0.5557 score: 0.9070 time: 0.05s
Test loss: 0.5403 score: 0.8864 time: 0.04s
Epoch 202/1000, LR 0.000246
Train loss: 0.8302;  Loss pred: 0.8302; Loss self: 0.0000; time: 0.13s
Val loss: 0.5538 score: 0.9070 time: 0.05s
Test loss: 0.5380 score: 0.8864 time: 0.04s
Epoch 203/1000, LR 0.000246
Train loss: 0.8317;  Loss pred: 0.8317; Loss self: 0.0000; time: 0.13s
Val loss: 0.5518 score: 0.9070 time: 0.05s
Test loss: 0.5359 score: 0.8864 time: 0.04s
Epoch 204/1000, LR 0.000245
Train loss: 0.8301;  Loss pred: 0.8301; Loss self: 0.0000; time: 0.13s
Val loss: 0.5497 score: 0.9070 time: 0.05s
Test loss: 0.5337 score: 0.8864 time: 0.04s
Epoch 205/1000, LR 0.000245
Train loss: 0.8232;  Loss pred: 0.8232; Loss self: 0.0000; time: 0.13s
Val loss: 0.5477 score: 0.9070 time: 0.05s
Test loss: 0.5315 score: 0.8864 time: 0.04s
Epoch 206/1000, LR 0.000245
Train loss: 0.8272;  Loss pred: 0.8272; Loss self: 0.0000; time: 0.13s
Val loss: 0.5456 score: 0.9070 time: 0.05s
Test loss: 0.5293 score: 0.8864 time: 0.04s
Epoch 207/1000, LR 0.000245
Train loss: 0.8256;  Loss pred: 0.8256; Loss self: 0.0000; time: 0.13s
Val loss: 0.5435 score: 0.9070 time: 0.05s
Test loss: 0.5272 score: 0.8864 time: 0.04s
Epoch 208/1000, LR 0.000244
Train loss: 0.8224;  Loss pred: 0.8224; Loss self: 0.0000; time: 0.13s
Val loss: 0.5413 score: 0.9070 time: 0.05s
Test loss: 0.5250 score: 0.8864 time: 0.04s
Epoch 209/1000, LR 0.000244
Train loss: 0.8200;  Loss pred: 0.8200; Loss self: 0.0000; time: 0.13s
Val loss: 0.5391 score: 0.9070 time: 0.05s
Test loss: 0.5228 score: 0.8864 time: 0.04s
Epoch 210/1000, LR 0.000244
Train loss: 0.8175;  Loss pred: 0.8175; Loss self: 0.0000; time: 0.13s
Val loss: 0.5370 score: 0.8837 time: 0.05s
Test loss: 0.5207 score: 0.8636 time: 0.04s
Epoch 211/1000, LR 0.000244
Train loss: 0.8118;  Loss pred: 0.8118; Loss self: 0.0000; time: 0.13s
Val loss: 0.5348 score: 0.8837 time: 0.05s
Test loss: 0.5185 score: 0.8636 time: 0.04s
Epoch 212/1000, LR 0.000243
Train loss: 0.8146;  Loss pred: 0.8146; Loss self: 0.0000; time: 0.13s
Val loss: 0.5327 score: 0.8837 time: 0.05s
Test loss: 0.5163 score: 0.8636 time: 0.04s
Epoch 213/1000, LR 0.000243
Train loss: 0.8088;  Loss pred: 0.8088; Loss self: 0.0000; time: 0.13s
Val loss: 0.5306 score: 0.8837 time: 0.05s
Test loss: 0.5141 score: 0.8636 time: 0.04s
Epoch 214/1000, LR 0.000243
Train loss: 0.8066;  Loss pred: 0.8066; Loss self: 0.0000; time: 0.13s
Val loss: 0.5286 score: 0.8837 time: 0.05s
Test loss: 0.5119 score: 0.8636 time: 0.04s
Epoch 215/1000, LR 0.000243
Train loss: 0.8116;  Loss pred: 0.8116; Loss self: 0.0000; time: 0.13s
Val loss: 0.5265 score: 0.8837 time: 0.05s
Test loss: 0.5096 score: 0.8636 time: 0.04s
Epoch 216/1000, LR 0.000242
Train loss: 0.8060;  Loss pred: 0.8060; Loss self: 0.0000; time: 0.13s
Val loss: 0.5245 score: 0.8837 time: 0.05s
Test loss: 0.5074 score: 0.8636 time: 0.04s
Epoch 217/1000, LR 0.000242
Train loss: 0.8070;  Loss pred: 0.8070; Loss self: 0.0000; time: 0.13s
Val loss: 0.5224 score: 0.8837 time: 0.05s
Test loss: 0.5052 score: 0.8636 time: 0.04s
Epoch 218/1000, LR 0.000242
Train loss: 0.8030;  Loss pred: 0.8030; Loss self: 0.0000; time: 0.12s
Val loss: 0.5202 score: 0.8837 time: 0.05s
Test loss: 0.5029 score: 0.8636 time: 0.04s
Epoch 219/1000, LR 0.000242
Train loss: 0.8026;  Loss pred: 0.8026; Loss self: 0.0000; time: 0.13s
Val loss: 0.5181 score: 0.8837 time: 0.05s
Test loss: 0.5007 score: 0.8636 time: 0.04s
Epoch 220/1000, LR 0.000241
Train loss: 0.8003;  Loss pred: 0.8003; Loss self: 0.0000; time: 0.13s
Val loss: 0.5160 score: 0.8837 time: 0.05s
Test loss: 0.4984 score: 0.8636 time: 0.04s
Epoch 221/1000, LR 0.000241
Train loss: 0.7984;  Loss pred: 0.7984; Loss self: 0.0000; time: 0.12s
Val loss: 0.5138 score: 0.8837 time: 0.05s
Test loss: 0.4961 score: 0.8636 time: 0.04s
Epoch 222/1000, LR 0.000241
Train loss: 0.7978;  Loss pred: 0.7978; Loss self: 0.0000; time: 0.13s
Val loss: 0.5117 score: 0.8837 time: 0.05s
Test loss: 0.4939 score: 0.8636 time: 0.04s
Epoch 223/1000, LR 0.000241
Train loss: 0.7892;  Loss pred: 0.7892; Loss self: 0.0000; time: 0.13s
Val loss: 0.5096 score: 0.8837 time: 0.05s
Test loss: 0.4916 score: 0.8636 time: 0.04s
Epoch 224/1000, LR 0.000240
Train loss: 0.7917;  Loss pred: 0.7917; Loss self: 0.0000; time: 0.13s
Val loss: 0.5074 score: 0.8837 time: 0.05s
Test loss: 0.4893 score: 0.8636 time: 0.04s
Epoch 225/1000, LR 0.000240
Train loss: 0.7908;  Loss pred: 0.7908; Loss self: 0.0000; time: 0.13s
Val loss: 0.5052 score: 0.8837 time: 1.59s
Test loss: 0.4871 score: 0.8636 time: 2.39s
Epoch 226/1000, LR 0.000240
Train loss: 0.7883;  Loss pred: 0.7883; Loss self: 0.0000; time: 2.59s
Val loss: 0.5031 score: 0.8837 time: 1.32s
Test loss: 0.4849 score: 0.8636 time: 0.26s
Epoch 227/1000, LR 0.000240
Train loss: 0.7861;  Loss pred: 0.7861; Loss self: 0.0000; time: 0.13s
Val loss: 0.5009 score: 0.8837 time: 0.05s
Test loss: 0.4827 score: 0.8636 time: 0.05s
Epoch 228/1000, LR 0.000239
Train loss: 0.7827;  Loss pred: 0.7827; Loss self: 0.0000; time: 0.13s
Val loss: 0.4987 score: 0.8837 time: 0.05s
Test loss: 0.4804 score: 0.8636 time: 0.04s
Epoch 229/1000, LR 0.000239
Train loss: 0.7779;  Loss pred: 0.7779; Loss self: 0.0000; time: 0.13s
Val loss: 0.4967 score: 0.8837 time: 0.05s
Test loss: 0.4781 score: 0.8636 time: 0.04s
Epoch 230/1000, LR 0.000239
Train loss: 0.7848;  Loss pred: 0.7848; Loss self: 0.0000; time: 0.13s
Val loss: 0.4946 score: 0.8837 time: 0.05s
Test loss: 0.4758 score: 0.8636 time: 0.06s
Epoch 231/1000, LR 0.000238
Train loss: 0.7820;  Loss pred: 0.7820; Loss self: 0.0000; time: 0.15s
Val loss: 0.4925 score: 0.8837 time: 0.05s
Test loss: 0.4736 score: 0.8636 time: 0.04s
Epoch 232/1000, LR 0.000238
Train loss: 0.7742;  Loss pred: 0.7742; Loss self: 0.0000; time: 0.13s
Val loss: 0.4905 score: 0.8837 time: 0.05s
Test loss: 0.4713 score: 0.8636 time: 0.05s
Epoch 233/1000, LR 0.000238
Train loss: 0.7745;  Loss pred: 0.7745; Loss self: 0.0000; time: 0.13s
Val loss: 0.4884 score: 0.8837 time: 0.05s
Test loss: 0.4691 score: 0.8636 time: 0.04s
Epoch 234/1000, LR 0.000238
Train loss: 0.7747;  Loss pred: 0.7747; Loss self: 0.0000; time: 0.13s
Val loss: 0.4863 score: 0.8837 time: 0.05s
Test loss: 0.4669 score: 0.8636 time: 0.05s
Epoch 235/1000, LR 0.000237
Train loss: 0.7678;  Loss pred: 0.7678; Loss self: 0.0000; time: 0.13s
Val loss: 0.4842 score: 0.8837 time: 0.05s
Test loss: 0.4647 score: 0.8636 time: 0.04s
Epoch 236/1000, LR 0.000237
Train loss: 0.7680;  Loss pred: 0.7680; Loss self: 0.0000; time: 0.13s
Val loss: 0.4822 score: 0.8837 time: 0.05s
Test loss: 0.4625 score: 0.8636 time: 0.04s
Epoch 237/1000, LR 0.000237
Train loss: 0.7706;  Loss pred: 0.7706; Loss self: 0.0000; time: 0.13s
Val loss: 0.4801 score: 0.8837 time: 0.14s
Test loss: 0.4603 score: 0.8636 time: 0.78s
Epoch 238/1000, LR 0.000236
Train loss: 0.7671;  Loss pred: 0.7671; Loss self: 0.0000; time: 2.40s
Val loss: 0.4780 score: 0.8837 time: 0.37s
Test loss: 0.4581 score: 0.8636 time: 0.21s
Epoch 239/1000, LR 0.000236
Train loss: 0.7660;  Loss pred: 0.7660; Loss self: 0.0000; time: 0.14s
Val loss: 0.4759 score: 0.8837 time: 0.05s
Test loss: 0.4561 score: 0.8864 time: 0.05s
Epoch 240/1000, LR 0.000236
Train loss: 0.7549;  Loss pred: 0.7549; Loss self: 0.0000; time: 0.13s
Val loss: 0.4738 score: 0.8837 time: 0.05s
Test loss: 0.4538 score: 0.8864 time: 0.05s
Epoch 241/1000, LR 0.000236
Train loss: 0.7638;  Loss pred: 0.7638; Loss self: 0.0000; time: 0.13s
Val loss: 0.4717 score: 0.8837 time: 0.05s
Test loss: 0.4515 score: 0.8864 time: 0.04s
Epoch 242/1000, LR 0.000235
Train loss: 0.7608;  Loss pred: 0.7608; Loss self: 0.0000; time: 0.13s
Val loss: 0.4696 score: 0.8837 time: 0.05s
Test loss: 0.4493 score: 0.8864 time: 0.04s
Epoch 243/1000, LR 0.000235
Train loss: 0.7566;  Loss pred: 0.7566; Loss self: 0.0000; time: 0.13s
Val loss: 0.4675 score: 0.8837 time: 0.05s
Test loss: 0.4472 score: 0.9091 time: 0.04s
Epoch 244/1000, LR 0.000235
Train loss: 0.7572;  Loss pred: 0.7572; Loss self: 0.0000; time: 0.13s
Val loss: 0.4653 score: 0.8837 time: 0.05s
Test loss: 0.4451 score: 0.9091 time: 0.04s
Epoch 245/1000, LR 0.000234
Train loss: 0.7522;  Loss pred: 0.7522; Loss self: 0.0000; time: 0.13s
Val loss: 0.4631 score: 0.8837 time: 0.05s
Test loss: 0.4431 score: 0.9091 time: 0.04s
Epoch 246/1000, LR 0.000234
Train loss: 0.7527;  Loss pred: 0.7527; Loss self: 0.0000; time: 0.13s
Val loss: 0.4609 score: 0.8837 time: 0.05s
Test loss: 0.4411 score: 0.9091 time: 0.04s
Epoch 247/1000, LR 0.000234
Train loss: 0.7518;  Loss pred: 0.7518; Loss self: 0.0000; time: 0.13s
Val loss: 0.4587 score: 0.8837 time: 0.05s
Test loss: 0.4390 score: 0.9091 time: 0.04s
Epoch 248/1000, LR 0.000234
Train loss: 0.7496;  Loss pred: 0.7496; Loss self: 0.0000; time: 0.13s
Val loss: 0.4565 score: 0.8837 time: 0.05s
Test loss: 0.4368 score: 0.9091 time: 0.04s
Epoch 249/1000, LR 0.000233
Train loss: 0.7475;  Loss pred: 0.7475; Loss self: 0.0000; time: 0.12s
Val loss: 0.4543 score: 0.8837 time: 0.05s
Test loss: 0.4346 score: 0.9091 time: 0.04s
Epoch 250/1000, LR 0.000233
Train loss: 0.7462;  Loss pred: 0.7462; Loss self: 0.0000; time: 0.13s
Val loss: 0.4522 score: 0.8837 time: 0.05s
Test loss: 0.4324 score: 0.9091 time: 0.04s
Epoch 251/1000, LR 0.000233
Train loss: 0.7389;  Loss pred: 0.7389; Loss self: 0.0000; time: 0.13s
Val loss: 0.4500 score: 0.8837 time: 0.05s
Test loss: 0.4302 score: 0.9091 time: 0.04s
Epoch 252/1000, LR 0.000232
Train loss: 0.7409;  Loss pred: 0.7409; Loss self: 0.0000; time: 0.13s
Val loss: 0.4480 score: 0.8605 time: 0.05s
Test loss: 0.4279 score: 0.9091 time: 0.04s
Epoch 253/1000, LR 0.000232
Train loss: 0.7441;  Loss pred: 0.7441; Loss self: 0.0000; time: 0.13s
Val loss: 0.4460 score: 0.8605 time: 0.05s
Test loss: 0.4257 score: 0.9091 time: 0.04s
Epoch 254/1000, LR 0.000232
Train loss: 0.7387;  Loss pred: 0.7387; Loss self: 0.0000; time: 0.13s
Val loss: 0.4439 score: 0.8605 time: 0.05s
Test loss: 0.4236 score: 0.9091 time: 0.04s
Epoch 255/1000, LR 0.000232
Train loss: 0.7379;  Loss pred: 0.7379; Loss self: 0.0000; time: 0.13s
Val loss: 0.4418 score: 0.8605 time: 0.05s
Test loss: 0.4216 score: 0.9091 time: 0.04s
Epoch 256/1000, LR 0.000231
Train loss: 0.7368;  Loss pred: 0.7368; Loss self: 0.0000; time: 0.13s
Val loss: 0.4396 score: 0.8605 time: 0.05s
Test loss: 0.4196 score: 0.9091 time: 0.04s
Epoch 257/1000, LR 0.000231
Train loss: 0.7304;  Loss pred: 0.7304; Loss self: 0.0000; time: 0.13s
Val loss: 0.4375 score: 0.8605 time: 0.05s
Test loss: 0.4177 score: 0.9091 time: 0.04s
Epoch 258/1000, LR 0.000231
Train loss: 0.7335;  Loss pred: 0.7335; Loss self: 0.0000; time: 0.13s
Val loss: 0.4353 score: 0.8605 time: 0.05s
Test loss: 0.4159 score: 0.9091 time: 0.04s
Epoch 259/1000, LR 0.000230
Train loss: 0.7277;  Loss pred: 0.7277; Loss self: 0.0000; time: 0.12s
Val loss: 0.4333 score: 0.8605 time: 0.05s
Test loss: 0.4140 score: 0.9091 time: 0.04s
Epoch 260/1000, LR 0.000230
Train loss: 0.7290;  Loss pred: 0.7290; Loss self: 0.0000; time: 0.13s
Val loss: 0.4313 score: 0.8605 time: 0.05s
Test loss: 0.4122 score: 0.9318 time: 0.04s
Epoch 261/1000, LR 0.000230
Train loss: 0.7272;  Loss pred: 0.7272; Loss self: 0.0000; time: 0.13s
Val loss: 0.4292 score: 0.8605 time: 0.05s
Test loss: 0.4105 score: 0.9318 time: 0.04s
Epoch 262/1000, LR 0.000229
Train loss: 0.7260;  Loss pred: 0.7260; Loss self: 0.0000; time: 0.13s
Val loss: 0.4272 score: 0.8605 time: 0.05s
Test loss: 0.4088 score: 0.9318 time: 0.04s
Epoch 263/1000, LR 0.000229
Train loss: 0.7245;  Loss pred: 0.7245; Loss self: 0.0000; time: 0.13s
Val loss: 0.4251 score: 0.8605 time: 0.05s
Test loss: 0.4072 score: 0.9318 time: 0.04s
Epoch 264/1000, LR 0.000229
Train loss: 0.7234;  Loss pred: 0.7234; Loss self: 0.0000; time: 0.12s
Val loss: 0.4231 score: 0.8605 time: 0.05s
Test loss: 0.4055 score: 0.9318 time: 0.04s
Epoch 265/1000, LR 0.000228
Train loss: 0.7227;  Loss pred: 0.7227; Loss self: 0.0000; time: 0.12s
Val loss: 0.4210 score: 0.8605 time: 0.05s
Test loss: 0.4038 score: 0.9091 time: 0.04s
Epoch 266/1000, LR 0.000228
Train loss: 0.7178;  Loss pred: 0.7178; Loss self: 0.0000; time: 0.12s
Val loss: 0.4191 score: 0.8605 time: 0.05s
Test loss: 0.4019 score: 0.9318 time: 0.04s
Epoch 267/1000, LR 0.000228
Train loss: 0.7213;  Loss pred: 0.7213; Loss self: 0.0000; time: 0.13s
Val loss: 0.4171 score: 0.8605 time: 0.05s
Test loss: 0.3998 score: 0.9318 time: 0.04s
Epoch 268/1000, LR 0.000228
Train loss: 0.7171;  Loss pred: 0.7171; Loss self: 0.0000; time: 0.13s
Val loss: 0.4152 score: 0.8605 time: 0.05s
Test loss: 0.3979 score: 0.9318 time: 0.04s
Epoch 269/1000, LR 0.000227
Train loss: 0.7170;  Loss pred: 0.7170; Loss self: 0.0000; time: 0.12s
Val loss: 0.4133 score: 0.8605 time: 0.05s
Test loss: 0.3961 score: 0.9318 time: 0.04s
Epoch 270/1000, LR 0.000227
Train loss: 0.7130;  Loss pred: 0.7130; Loss self: 0.0000; time: 0.13s
Val loss: 0.4114 score: 0.8605 time: 0.05s
Test loss: 0.3942 score: 0.9318 time: 0.04s
Epoch 271/1000, LR 0.000227
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.12s
Val loss: 0.4095 score: 0.8605 time: 0.05s
Test loss: 0.3924 score: 0.9318 time: 0.04s
Epoch 272/1000, LR 0.000226
Train loss: 0.7108;  Loss pred: 0.7108; Loss self: 0.0000; time: 0.13s
Val loss: 0.4076 score: 0.8605 time: 0.05s
Test loss: 0.3906 score: 0.9318 time: 0.04s
Epoch 273/1000, LR 0.000226
Train loss: 0.7099;  Loss pred: 0.7099; Loss self: 0.0000; time: 0.13s
Val loss: 0.4057 score: 0.8605 time: 0.05s
Test loss: 0.3889 score: 0.9318 time: 0.04s
Epoch 274/1000, LR 0.000226
Train loss: 0.7100;  Loss pred: 0.7100; Loss self: 0.0000; time: 0.13s
Val loss: 0.4038 score: 0.8605 time: 0.05s
Test loss: 0.3872 score: 0.9318 time: 0.04s
Epoch 275/1000, LR 0.000225
Train loss: 0.7103;  Loss pred: 0.7103; Loss self: 0.0000; time: 0.13s
Val loss: 0.4019 score: 0.8605 time: 0.05s
Test loss: 0.3857 score: 0.9318 time: 0.04s
Epoch 276/1000, LR 0.000225
Train loss: 0.7034;  Loss pred: 0.7034; Loss self: 0.0000; time: 0.13s
Val loss: 0.4001 score: 0.8605 time: 0.05s
Test loss: 0.3841 score: 0.9318 time: 0.04s
Epoch 277/1000, LR 0.000225
Train loss: 0.7035;  Loss pred: 0.7035; Loss self: 0.0000; time: 0.13s
Val loss: 0.3982 score: 0.8605 time: 0.05s
Test loss: 0.3827 score: 0.9318 time: 0.04s
Epoch 278/1000, LR 0.000224
Train loss: 0.7018;  Loss pred: 0.7018; Loss self: 0.0000; time: 0.13s
Val loss: 0.3964 score: 0.8605 time: 0.05s
Test loss: 0.3813 score: 0.9318 time: 0.04s
Epoch 279/1000, LR 0.000224
Train loss: 0.7016;  Loss pred: 0.7016; Loss self: 0.0000; time: 0.13s
Val loss: 0.3946 score: 0.8605 time: 0.05s
Test loss: 0.3799 score: 0.9318 time: 0.04s
Epoch 280/1000, LR 0.000224
Train loss: 0.7027;  Loss pred: 0.7027; Loss self: 0.0000; time: 0.13s
Val loss: 0.3929 score: 0.8605 time: 0.05s
Test loss: 0.3784 score: 0.9318 time: 0.04s
Epoch 281/1000, LR 0.000223
Train loss: 0.6992;  Loss pred: 0.6992; Loss self: 0.0000; time: 0.13s
Val loss: 0.3912 score: 0.8605 time: 0.05s
Test loss: 0.3767 score: 0.9318 time: 0.04s
Epoch 282/1000, LR 0.000223
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.13s
Val loss: 0.3896 score: 0.8605 time: 0.05s
Test loss: 0.3752 score: 0.9318 time: 0.04s
Epoch 283/1000, LR 0.000223
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.13s
Val loss: 0.3879 score: 0.8605 time: 0.05s
Test loss: 0.3737 score: 0.9318 time: 0.04s
Epoch 284/1000, LR 0.000222
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.13s
Val loss: 0.3862 score: 0.8605 time: 0.05s
Test loss: 0.3723 score: 0.9318 time: 0.04s
Epoch 285/1000, LR 0.000222
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.13s
Val loss: 0.3845 score: 0.8605 time: 0.05s
Test loss: 0.3709 score: 0.9318 time: 0.04s
Epoch 286/1000, LR 0.000222
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.13s
Val loss: 0.3829 score: 0.8605 time: 0.05s
Test loss: 0.3695 score: 0.9318 time: 0.04s
Epoch 287/1000, LR 0.000221
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 2.25s
Val loss: 0.3813 score: 0.8605 time: 0.49s
Test loss: 0.3680 score: 0.9318 time: 0.28s
Epoch 288/1000, LR 0.000221
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 1.42s
Val loss: 0.3796 score: 0.8605 time: 0.14s
Test loss: 0.3665 score: 0.9318 time: 0.73s
Epoch 289/1000, LR 0.000221
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.93s
Val loss: 0.3780 score: 0.8605 time: 0.24s
Test loss: 0.3651 score: 0.9318 time: 0.45s
Epoch 290/1000, LR 0.000220
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 1.02s
Val loss: 0.3764 score: 0.8605 time: 0.32s
Test loss: 0.3638 score: 0.9318 time: 1.23s
Epoch 291/1000, LR 0.000220
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 1.20s
Val loss: 0.3748 score: 0.8605 time: 0.18s
Test loss: 0.3626 score: 0.9318 time: 0.09s
Epoch 292/1000, LR 0.000220
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.25s
Val loss: 0.3732 score: 0.8605 time: 0.09s
Test loss: 0.3615 score: 0.9318 time: 0.13s
Epoch 293/1000, LR 0.000219
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.43s
Val loss: 0.3716 score: 0.8605 time: 0.06s
Test loss: 0.3604 score: 0.9318 time: 0.05s
Epoch 294/1000, LR 0.000219
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.13s
Val loss: 0.3701 score: 0.8605 time: 0.05s
Test loss: 0.3594 score: 0.9318 time: 0.05s
Epoch 295/1000, LR 0.000219
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.13s
Val loss: 0.3685 score: 0.8605 time: 0.05s
Test loss: 0.3583 score: 0.9318 time: 0.04s
Epoch 296/1000, LR 0.000218
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.13s
Val loss: 0.3671 score: 0.8605 time: 0.05s
Test loss: 0.3569 score: 0.9318 time: 0.04s
Epoch 297/1000, LR 0.000218
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.13s
Val loss: 0.3656 score: 0.8605 time: 0.05s
Test loss: 0.3554 score: 0.9318 time: 0.04s
Epoch 298/1000, LR 0.000218
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.13s
Val loss: 0.3642 score: 0.8605 time: 0.05s
Test loss: 0.3539 score: 0.9318 time: 0.04s
Epoch 299/1000, LR 0.000217
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.13s
Val loss: 0.3628 score: 0.8605 time: 0.05s
Test loss: 0.3525 score: 0.9318 time: 0.04s
Epoch 300/1000, LR 0.000217
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.13s
Val loss: 0.3613 score: 0.8605 time: 0.05s
Test loss: 0.3513 score: 0.9318 time: 0.04s
Epoch 301/1000, LR 0.000217
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.13s
Val loss: 0.3599 score: 0.8605 time: 0.05s
Test loss: 0.3502 score: 0.9318 time: 0.04s
Epoch 302/1000, LR 0.000216
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.13s
Val loss: 0.3585 score: 0.8605 time: 0.05s
Test loss: 0.3491 score: 0.9318 time: 0.04s
Epoch 303/1000, LR 0.000216
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.13s
Val loss: 0.3570 score: 0.8605 time: 0.05s
Test loss: 0.3482 score: 0.9318 time: 0.04s
Epoch 304/1000, LR 0.000216
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.13s
Val loss: 0.3556 score: 0.8605 time: 0.05s
Test loss: 0.3472 score: 0.9318 time: 0.04s
Epoch 305/1000, LR 0.000215
Train loss: 0.6696;  Loss pred: 0.6696; Loss self: 0.0000; time: 0.13s
Val loss: 0.3543 score: 0.8605 time: 0.05s
Test loss: 0.3463 score: 0.9091 time: 0.04s
Epoch 306/1000, LR 0.000215
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.13s
Val loss: 0.3529 score: 0.8605 time: 0.05s
Test loss: 0.3453 score: 0.9091 time: 0.04s
Epoch 307/1000, LR 0.000215
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.13s
Val loss: 0.3516 score: 0.8605 time: 0.05s
Test loss: 0.3444 score: 0.9091 time: 0.04s
Epoch 308/1000, LR 0.000214
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.13s
Val loss: 0.3503 score: 0.8605 time: 0.05s
Test loss: 0.3436 score: 0.9091 time: 0.04s
Epoch 309/1000, LR 0.000214
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.13s
Val loss: 0.3491 score: 0.8605 time: 0.05s
Test loss: 0.3427 score: 0.9091 time: 0.05s
Epoch 310/1000, LR 0.000214
Train loss: 0.6651;  Loss pred: 0.6651; Loss self: 0.0000; time: 0.13s
Val loss: 0.3478 score: 0.8605 time: 0.05s
Test loss: 0.3417 score: 0.9091 time: 0.04s
Epoch 311/1000, LR 0.000213
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.13s
Val loss: 0.3465 score: 0.8605 time: 0.05s
Test loss: 0.3405 score: 0.9091 time: 0.04s
Epoch 312/1000, LR 0.000213
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.13s
Val loss: 0.3453 score: 0.8605 time: 0.05s
Test loss: 0.3391 score: 0.9091 time: 0.04s
Epoch 313/1000, LR 0.000213
Train loss: 0.6625;  Loss pred: 0.6625; Loss self: 0.0000; time: 0.13s
Val loss: 0.3440 score: 0.8605 time: 0.05s
Test loss: 0.3378 score: 0.9091 time: 0.04s
Epoch 314/1000, LR 0.000212
Train loss: 0.6584;  Loss pred: 0.6584; Loss self: 0.0000; time: 0.13s
Val loss: 0.3428 score: 0.8605 time: 0.05s
Test loss: 0.3366 score: 0.9091 time: 0.04s
Epoch 315/1000, LR 0.000212
Train loss: 0.6631;  Loss pred: 0.6631; Loss self: 0.0000; time: 1.46s
Val loss: 0.3416 score: 0.8605 time: 1.15s
Test loss: 0.3354 score: 0.9091 time: 0.86s
Epoch 316/1000, LR 0.000212
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 0.88s
Val loss: 0.3404 score: 0.8605 time: 0.29s
Test loss: 0.3341 score: 0.9091 time: 0.33s
Epoch 317/1000, LR 0.000211
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.58s
Val loss: 0.3393 score: 0.8605 time: 0.53s
Test loss: 0.3330 score: 0.9091 time: 0.18s
Epoch 318/1000, LR 0.000211
Train loss: 0.6584;  Loss pred: 0.6584; Loss self: 0.0000; time: 1.52s
Val loss: 0.3381 score: 0.8605 time: 0.58s
Test loss: 0.3318 score: 0.9091 time: 0.47s
Epoch 319/1000, LR 0.000210
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.58s
Val loss: 0.3371 score: 0.8605 time: 0.11s
Test loss: 0.3306 score: 0.9318 time: 0.14s
Epoch 320/1000, LR 0.000210
Train loss: 0.6568;  Loss pred: 0.6568; Loss self: 0.0000; time: 0.16s
Val loss: 0.3360 score: 0.8605 time: 0.05s
Test loss: 0.3295 score: 0.9318 time: 0.05s
Epoch 321/1000, LR 0.000210
Train loss: 0.6542;  Loss pred: 0.6542; Loss self: 0.0000; time: 0.14s
Val loss: 0.3349 score: 0.8605 time: 0.05s
Test loss: 0.3287 score: 0.9091 time: 0.05s
Epoch 322/1000, LR 0.000209
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 0.14s
Val loss: 0.3338 score: 0.8605 time: 0.05s
Test loss: 0.3280 score: 0.9091 time: 0.05s
Epoch 323/1000, LR 0.000209
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.14s
Val loss: 0.3328 score: 0.8605 time: 0.05s
Test loss: 0.3272 score: 0.9091 time: 0.05s
Epoch 324/1000, LR 0.000209
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.14s
Val loss: 0.3318 score: 0.8605 time: 0.05s
Test loss: 0.3266 score: 0.9091 time: 0.04s
Epoch 325/1000, LR 0.000208
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.13s
Val loss: 0.3309 score: 0.8605 time: 0.05s
Test loss: 0.3259 score: 0.9091 time: 0.04s
Epoch 326/1000, LR 0.000208
Train loss: 0.6496;  Loss pred: 0.6496; Loss self: 0.0000; time: 0.13s
Val loss: 0.3300 score: 0.8605 time: 0.05s
Test loss: 0.3254 score: 0.9091 time: 0.04s
Epoch 327/1000, LR 0.000208
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 0.12s
Val loss: 0.3291 score: 0.8605 time: 0.05s
Test loss: 0.3249 score: 0.9091 time: 0.04s
Epoch 328/1000, LR 0.000207
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 0.13s
Val loss: 0.3282 score: 0.8605 time: 0.05s
Test loss: 0.3243 score: 0.9091 time: 0.04s
Epoch 329/1000, LR 0.000207
Train loss: 0.6459;  Loss pred: 0.6459; Loss self: 0.0000; time: 0.13s
Val loss: 0.3273 score: 0.8605 time: 0.05s
Test loss: 0.3236 score: 0.9091 time: 0.04s
Epoch 330/1000, LR 0.000207
Train loss: 0.6464;  Loss pred: 0.6464; Loss self: 0.0000; time: 0.13s
Val loss: 0.3264 score: 0.8605 time: 0.05s
Test loss: 0.3227 score: 0.9091 time: 0.05s
Epoch 331/1000, LR 0.000206
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.13s
Val loss: 0.3254 score: 0.8605 time: 0.05s
Test loss: 0.3217 score: 0.9091 time: 0.05s
Epoch 332/1000, LR 0.000206
Train loss: 0.6448;  Loss pred: 0.6448; Loss self: 0.0000; time: 0.13s
Val loss: 0.3245 score: 0.8605 time: 0.05s
Test loss: 0.3208 score: 0.9091 time: 0.04s
Epoch 333/1000, LR 0.000205
Train loss: 0.6479;  Loss pred: 0.6479; Loss self: 0.0000; time: 0.13s
Val loss: 0.3236 score: 0.8605 time: 0.05s
Test loss: 0.3199 score: 0.9091 time: 0.04s
Epoch 334/1000, LR 0.000205
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.13s
Val loss: 0.3227 score: 0.8605 time: 0.05s
Test loss: 0.3189 score: 0.9091 time: 0.05s
Epoch 335/1000, LR 0.000205
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 0.13s
Val loss: 0.3218 score: 0.8605 time: 0.05s
Test loss: 0.3181 score: 0.9091 time: 0.05s
Epoch 336/1000, LR 0.000204
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.13s
Val loss: 0.3209 score: 0.8605 time: 0.05s
Test loss: 0.3172 score: 0.9091 time: 0.04s
Epoch 337/1000, LR 0.000204
Train loss: 0.6385;  Loss pred: 0.6385; Loss self: 0.0000; time: 1.77s
Val loss: 0.3201 score: 0.8605 time: 0.76s
Test loss: 0.3164 score: 0.9091 time: 0.62s
Epoch 338/1000, LR 0.000204
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.41s
Val loss: 0.3192 score: 0.8605 time: 0.37s
Test loss: 0.3157 score: 0.9091 time: 0.60s
Epoch 339/1000, LR 0.000203
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 3.45s
Val loss: 0.3184 score: 0.8605 time: 0.06s
Test loss: 0.3149 score: 0.9091 time: 0.05s
Epoch 340/1000, LR 0.000203
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.14s
Val loss: 0.3175 score: 0.8605 time: 0.05s
Test loss: 0.3137 score: 0.9091 time: 0.04s
Epoch 341/1000, LR 0.000203
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.13s
Val loss: 0.3166 score: 0.8605 time: 0.05s
Test loss: 0.3129 score: 0.9091 time: 0.11s
Epoch 342/1000, LR 0.000202
Train loss: 0.6395;  Loss pred: 0.6395; Loss self: 0.0000; time: 0.13s
Val loss: 0.3158 score: 0.8605 time: 0.05s
Test loss: 0.3124 score: 0.9091 time: 0.04s
Epoch 343/1000, LR 0.000202
Train loss: 0.6399;  Loss pred: 0.6399; Loss self: 0.0000; time: 0.13s
Val loss: 0.3149 score: 0.8605 time: 0.05s
Test loss: 0.3119 score: 0.9091 time: 0.04s
Epoch 344/1000, LR 0.000201
Train loss: 0.6352;  Loss pred: 0.6352; Loss self: 0.0000; time: 0.13s
Val loss: 0.3141 score: 0.8605 time: 0.05s
Test loss: 0.3113 score: 0.9091 time: 0.04s
Epoch 345/1000, LR 0.000201
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.13s
Val loss: 0.3133 score: 0.8605 time: 0.05s
Test loss: 0.3108 score: 0.9091 time: 0.04s
Epoch 346/1000, LR 0.000201
Train loss: 0.6348;  Loss pred: 0.6348; Loss self: 0.0000; time: 0.13s
Val loss: 0.3125 score: 0.8605 time: 0.05s
Test loss: 0.3101 score: 0.9091 time: 0.05s
Epoch 347/1000, LR 0.000200
Train loss: 0.6319;  Loss pred: 0.6319; Loss self: 0.0000; time: 0.13s
Val loss: 0.3116 score: 0.8605 time: 0.05s
Test loss: 0.3094 score: 0.9091 time: 0.05s
Epoch 348/1000, LR 0.000200
Train loss: 0.6311;  Loss pred: 0.6311; Loss self: 0.0000; time: 0.13s
Val loss: 0.3108 score: 0.8605 time: 0.05s
Test loss: 0.3087 score: 0.9091 time: 0.04s
Epoch 349/1000, LR 0.000200
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.18s
Val loss: 0.3099 score: 0.8605 time: 0.05s
Test loss: 0.3079 score: 0.9091 time: 0.04s
Epoch 350/1000, LR 0.000199
Train loss: 0.6314;  Loss pred: 0.6314; Loss self: 0.0000; time: 0.13s
Val loss: 0.3091 score: 0.8605 time: 0.05s
Test loss: 0.3072 score: 0.9091 time: 0.04s
Epoch 351/1000, LR 0.000199
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.13s
Val loss: 0.3083 score: 0.8605 time: 0.05s
Test loss: 0.3066 score: 0.9091 time: 0.04s
Epoch 352/1000, LR 0.000198
Train loss: 0.6332;  Loss pred: 0.6332; Loss self: 0.0000; time: 0.13s
Val loss: 0.3075 score: 0.8837 time: 0.05s
Test loss: 0.3062 score: 0.9091 time: 0.04s
Epoch 353/1000, LR 0.000198
Train loss: 0.6312;  Loss pred: 0.6312; Loss self: 0.0000; time: 0.13s
Val loss: 0.3067 score: 0.8837 time: 0.05s
Test loss: 0.3056 score: 0.9091 time: 0.05s
Epoch 354/1000, LR 0.000198
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.13s
Val loss: 0.3059 score: 0.8837 time: 0.05s
Test loss: 0.3049 score: 0.9091 time: 0.04s
Epoch 355/1000, LR 0.000197
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.13s
Val loss: 0.3051 score: 0.8605 time: 0.05s
Test loss: 0.3040 score: 0.9091 time: 0.04s
Epoch 356/1000, LR 0.000197
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 0.13s
Val loss: 0.3043 score: 0.8605 time: 0.05s
Test loss: 0.3031 score: 0.9091 time: 0.04s
Epoch 357/1000, LR 0.000196
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.13s
Val loss: 0.3035 score: 0.8605 time: 0.05s
Test loss: 0.3021 score: 0.9091 time: 0.04s
Epoch 358/1000, LR 0.000196
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.13s
Val loss: 0.3027 score: 0.8605 time: 0.05s
Test loss: 0.3011 score: 0.9091 time: 0.04s
Epoch 359/1000, LR 0.000196
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.13s
Val loss: 0.3020 score: 0.8605 time: 0.05s
Test loss: 0.3003 score: 0.9091 time: 0.04s
Epoch 360/1000, LR 0.000195
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.13s
Val loss: 0.3012 score: 0.8605 time: 0.05s
Test loss: 0.2998 score: 0.9091 time: 0.04s
Epoch 361/1000, LR 0.000195
Train loss: 0.6239;  Loss pred: 0.6239; Loss self: 0.0000; time: 0.13s
Val loss: 0.3005 score: 0.8605 time: 0.05s
Test loss: 0.2996 score: 0.9091 time: 0.04s
Epoch 362/1000, LR 0.000195
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 0.13s
Val loss: 0.2998 score: 0.8605 time: 0.05s
Test loss: 0.2991 score: 0.9091 time: 0.04s
Epoch 363/1000, LR 0.000194
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.13s
Val loss: 0.2991 score: 0.8605 time: 0.05s
Test loss: 0.2985 score: 0.9091 time: 0.04s
Epoch 364/1000, LR 0.000194
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 0.13s
Val loss: 0.2984 score: 0.8605 time: 0.05s
Test loss: 0.2978 score: 0.9091 time: 0.04s
Epoch 365/1000, LR 0.000193
Train loss: 0.6203;  Loss pred: 0.6203; Loss self: 0.0000; time: 0.13s
Val loss: 0.2977 score: 0.8605 time: 0.05s
Test loss: 0.2971 score: 0.9091 time: 0.04s
Epoch 366/1000, LR 0.000193
Train loss: 0.6231;  Loss pred: 0.6231; Loss self: 0.0000; time: 0.13s
Val loss: 0.2970 score: 0.8605 time: 0.05s
Test loss: 0.2965 score: 0.9091 time: 0.04s
Epoch 367/1000, LR 0.000193
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 0.13s
Val loss: 0.2963 score: 0.8605 time: 0.05s
Test loss: 0.2959 score: 0.9091 time: 0.85s
Epoch 368/1000, LR 0.000192
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 1.54s
Val loss: 0.2956 score: 0.8605 time: 0.45s
Test loss: 0.2956 score: 0.9091 time: 0.30s
Epoch 369/1000, LR 0.000192
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.87s
Val loss: 0.2950 score: 0.8837 time: 0.08s
Test loss: 0.2953 score: 0.9091 time: 0.31s
Epoch 370/1000, LR 0.000191
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 1.79s
Val loss: 0.2943 score: 0.8837 time: 0.18s
Test loss: 0.2948 score: 0.9091 time: 0.19s
Epoch 371/1000, LR 0.000191
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.13s
Val loss: 0.2937 score: 0.8837 time: 0.05s
Test loss: 0.2944 score: 0.9091 time: 0.04s
Epoch 372/1000, LR 0.000191
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.13s
Val loss: 0.2930 score: 0.8837 time: 0.05s
Test loss: 0.2939 score: 0.9091 time: 0.04s
Epoch 373/1000, LR 0.000190
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.12s
Val loss: 0.2924 score: 0.8837 time: 0.05s
Test loss: 0.2934 score: 0.9091 time: 0.04s
Epoch 374/1000, LR 0.000190
Train loss: 0.6137;  Loss pred: 0.6137; Loss self: 0.0000; time: 0.12s
Val loss: 0.2918 score: 0.8837 time: 0.05s
Test loss: 0.2931 score: 0.9091 time: 0.04s
Epoch 375/1000, LR 0.000190
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.12s
Val loss: 0.2912 score: 0.8837 time: 0.05s
Test loss: 0.2928 score: 0.8864 time: 0.04s
Epoch 376/1000, LR 0.000189
Train loss: 0.6163;  Loss pred: 0.6163; Loss self: 0.0000; time: 0.12s
Val loss: 0.2906 score: 0.8837 time: 0.05s
Test loss: 0.2926 score: 0.8864 time: 0.04s
Epoch 377/1000, LR 0.000189
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.12s
Val loss: 0.2901 score: 0.8837 time: 0.05s
Test loss: 0.2923 score: 0.8864 time: 0.04s
Epoch 378/1000, LR 0.000188
Train loss: 0.6142;  Loss pred: 0.6142; Loss self: 0.0000; time: 0.12s
Val loss: 0.2895 score: 0.8837 time: 0.05s
Test loss: 0.2916 score: 0.8864 time: 0.04s
Epoch 379/1000, LR 0.000188
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 0.12s
Val loss: 0.2889 score: 0.8837 time: 0.05s
Test loss: 0.2909 score: 0.8864 time: 0.04s
Epoch 380/1000, LR 0.000188
Train loss: 0.6109;  Loss pred: 0.6109; Loss self: 0.0000; time: 0.12s
Val loss: 0.2883 score: 0.8837 time: 0.05s
Test loss: 0.2901 score: 0.8864 time: 0.04s
Epoch 381/1000, LR 0.000187
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.12s
Val loss: 0.2877 score: 0.8837 time: 0.05s
Test loss: 0.2890 score: 0.9091 time: 0.04s
Epoch 382/1000, LR 0.000187
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 0.13s
Val loss: 0.2872 score: 0.8837 time: 0.05s
Test loss: 0.2881 score: 0.9091 time: 0.04s
Epoch 383/1000, LR 0.000186
Train loss: 0.6096;  Loss pred: 0.6096; Loss self: 0.0000; time: 0.13s
Val loss: 0.2866 score: 0.8837 time: 0.05s
Test loss: 0.2873 score: 0.9091 time: 0.04s
Epoch 384/1000, LR 0.000186
Train loss: 0.6093;  Loss pred: 0.6093; Loss self: 0.0000; time: 0.13s
Val loss: 0.2861 score: 0.8605 time: 0.05s
Test loss: 0.2864 score: 0.9091 time: 0.04s
Epoch 385/1000, LR 0.000186
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.13s
Val loss: 0.2856 score: 0.8605 time: 0.05s
Test loss: 0.2857 score: 0.9091 time: 0.04s
Epoch 386/1000, LR 0.000185
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.13s
Val loss: 0.2851 score: 0.8837 time: 0.05s
Test loss: 0.2855 score: 0.9091 time: 0.04s
Epoch 387/1000, LR 0.000185
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.12s
Val loss: 0.2846 score: 0.8837 time: 0.05s
Test loss: 0.2853 score: 0.9091 time: 0.04s
Epoch 388/1000, LR 0.000184
Train loss: 0.6067;  Loss pred: 0.6067; Loss self: 0.0000; time: 0.12s
Val loss: 0.2842 score: 0.8837 time: 0.05s
Test loss: 0.2854 score: 0.8864 time: 0.04s
Epoch 389/1000, LR 0.000184
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.12s
Val loss: 0.2838 score: 0.8837 time: 0.05s
Test loss: 0.2856 score: 0.8864 time: 0.04s
Epoch 390/1000, LR 0.000184
Train loss: 0.6095;  Loss pred: 0.6095; Loss self: 0.0000; time: 0.12s
Val loss: 0.2833 score: 0.8837 time: 0.05s
Test loss: 0.2857 score: 0.8864 time: 0.04s
Epoch 391/1000, LR 0.000183
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.12s
Val loss: 0.2828 score: 0.8837 time: 0.05s
Test loss: 0.2853 score: 0.9091 time: 0.04s
Epoch 392/1000, LR 0.000183
Train loss: 0.6040;  Loss pred: 0.6040; Loss self: 0.0000; time: 0.12s
Val loss: 0.2824 score: 0.8837 time: 0.05s
Test loss: 0.2849 score: 0.9091 time: 0.04s
Epoch 393/1000, LR 0.000182
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.12s
Val loss: 0.2819 score: 0.8837 time: 0.05s
Test loss: 0.2848 score: 0.9091 time: 0.04s
Epoch 394/1000, LR 0.000182
Train loss: 0.6102;  Loss pred: 0.6102; Loss self: 0.0000; time: 0.12s
Val loss: 0.2814 score: 0.8837 time: 0.05s
Test loss: 0.2842 score: 0.9091 time: 0.04s
Epoch 395/1000, LR 0.000182
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 0.12s
Val loss: 0.2808 score: 0.8837 time: 0.05s
Test loss: 0.2838 score: 0.9091 time: 0.04s
Epoch 396/1000, LR 0.000181
Train loss: 0.6064;  Loss pred: 0.6064; Loss self: 0.0000; time: 0.13s
Val loss: 0.2804 score: 0.8837 time: 0.05s
Test loss: 0.2836 score: 0.9091 time: 0.04s
Epoch 397/1000, LR 0.000181
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.12s
Val loss: 0.2799 score: 0.8837 time: 0.05s
Test loss: 0.2832 score: 0.9091 time: 0.04s
Epoch 398/1000, LR 0.000180
Train loss: 0.6024;  Loss pred: 0.6024; Loss self: 0.0000; time: 0.12s
Val loss: 0.2793 score: 0.8837 time: 0.05s
Test loss: 0.2824 score: 0.9091 time: 0.04s
Epoch 399/1000, LR 0.000180
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.13s
Val loss: 0.2788 score: 0.8837 time: 0.05s
Test loss: 0.2815 score: 0.9091 time: 0.04s
Epoch 400/1000, LR 0.000180
Train loss: 0.6016;  Loss pred: 0.6016; Loss self: 0.0000; time: 0.13s
Val loss: 0.2782 score: 0.8837 time: 0.05s
Test loss: 0.2804 score: 0.8864 time: 0.04s
Epoch 401/1000, LR 0.000179
Train loss: 0.6011;  Loss pred: 0.6011; Loss self: 0.0000; time: 0.13s
Val loss: 0.2776 score: 0.8837 time: 0.05s
Test loss: 0.2794 score: 0.8864 time: 0.79s
Epoch 402/1000, LR 0.000179
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 1.46s
Val loss: 0.2772 score: 0.8837 time: 0.62s
Test loss: 0.2787 score: 0.8864 time: 0.21s
Epoch 403/1000, LR 0.000178
Train loss: 0.6004;  Loss pred: 0.6004; Loss self: 0.0000; time: 0.52s
Val loss: 0.2767 score: 0.8837 time: 0.17s
Test loss: 0.2782 score: 0.8864 time: 0.11s
Epoch 404/1000, LR 0.000178
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 0.31s
Val loss: 0.2763 score: 0.8837 time: 0.07s
Test loss: 0.2778 score: 0.8864 time: 0.07s
Epoch 405/1000, LR 0.000178
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.14s
Val loss: 0.2758 score: 0.8837 time: 0.05s
Test loss: 0.2776 score: 0.8864 time: 0.05s
Epoch 406/1000, LR 0.000177
Train loss: 0.6000;  Loss pred: 0.6000; Loss self: 0.0000; time: 0.13s
Val loss: 0.2755 score: 0.8837 time: 0.05s
Test loss: 0.2776 score: 0.8864 time: 0.05s
Epoch 407/1000, LR 0.000177
Train loss: 0.5983;  Loss pred: 0.5983; Loss self: 0.0000; time: 0.14s
Val loss: 0.2751 score: 0.8837 time: 0.05s
Test loss: 0.2779 score: 0.9091 time: 0.05s
Epoch 408/1000, LR 0.000176
Train loss: 0.5986;  Loss pred: 0.5986; Loss self: 0.0000; time: 0.13s
Val loss: 0.2748 score: 0.8837 time: 0.05s
Test loss: 0.2782 score: 0.9091 time: 0.04s
Epoch 409/1000, LR 0.000176
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 0.12s
Val loss: 0.2745 score: 0.8837 time: 0.05s
Test loss: 0.2783 score: 0.9091 time: 0.04s
Epoch 410/1000, LR 0.000175
Train loss: 0.5958;  Loss pred: 0.5958; Loss self: 0.0000; time: 0.13s
Val loss: 0.2741 score: 0.8837 time: 0.05s
Test loss: 0.2782 score: 0.9091 time: 0.04s
Epoch 411/1000, LR 0.000175
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 0.12s
Val loss: 0.2739 score: 0.8837 time: 0.05s
Test loss: 0.2784 score: 0.9091 time: 0.04s
Epoch 412/1000, LR 0.000175
Train loss: 0.6023;  Loss pred: 0.6023; Loss self: 0.0000; time: 0.13s
Val loss: 0.2735 score: 0.8837 time: 0.05s
Test loss: 0.2785 score: 0.9091 time: 0.04s
Epoch 413/1000, LR 0.000174
Train loss: 0.5958;  Loss pred: 0.5958; Loss self: 0.0000; time: 0.13s
Val loss: 0.2732 score: 0.8837 time: 0.05s
Test loss: 0.2784 score: 0.9091 time: 0.04s
Epoch 414/1000, LR 0.000174
Train loss: 0.5953;  Loss pred: 0.5953; Loss self: 0.0000; time: 0.13s
Val loss: 0.2728 score: 0.8837 time: 0.05s
Test loss: 0.2779 score: 0.9091 time: 0.04s
Epoch 415/1000, LR 0.000173
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.13s
Val loss: 0.2723 score: 0.8837 time: 0.05s
Test loss: 0.2768 score: 0.9091 time: 0.04s
Epoch 416/1000, LR 0.000173
Train loss: 0.5968;  Loss pred: 0.5968; Loss self: 0.0000; time: 0.46s
Val loss: 0.2718 score: 0.8837 time: 0.12s
Test loss: 0.2760 score: 0.9091 time: 0.37s
Epoch 417/1000, LR 0.000173
Train loss: 0.5935;  Loss pred: 0.5935; Loss self: 0.0000; time: 1.10s
Val loss: 0.2714 score: 0.8837 time: 0.30s
Test loss: 0.2751 score: 0.9091 time: 0.28s
Epoch 418/1000, LR 0.000172
Train loss: 0.5940;  Loss pred: 0.5940; Loss self: 0.0000; time: 0.66s
Val loss: 0.2710 score: 0.8837 time: 0.31s
Test loss: 0.2744 score: 0.9091 time: 0.54s
Epoch 419/1000, LR 0.000172
Train loss: 0.5951;  Loss pred: 0.5951; Loss self: 0.0000; time: 1.71s
Val loss: 0.2706 score: 0.8837 time: 0.27s
Test loss: 0.2735 score: 0.9091 time: 1.73s
Epoch 420/1000, LR 0.000171
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 1.02s
Val loss: 0.2703 score: 0.8837 time: 0.16s
Test loss: 0.2727 score: 0.9091 time: 0.14s
Epoch 421/1000, LR 0.000171
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 1.76s
Val loss: 0.2699 score: 0.8837 time: 0.05s
Test loss: 0.2720 score: 0.9091 time: 0.04s
Epoch 422/1000, LR 0.000171
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 0.13s
Val loss: 0.2696 score: 0.8837 time: 0.05s
Test loss: 0.2714 score: 0.9091 time: 0.05s
Epoch 423/1000, LR 0.000170
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.13s
Val loss: 0.2694 score: 0.8837 time: 0.05s
Test loss: 0.2709 score: 0.9091 time: 0.05s
Epoch 424/1000, LR 0.000170
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.13s
Val loss: 0.2691 score: 0.8837 time: 0.05s
Test loss: 0.2707 score: 0.9091 time: 0.04s
Epoch 425/1000, LR 0.000169
Train loss: 0.5902;  Loss pred: 0.5902; Loss self: 0.0000; time: 0.13s
Val loss: 0.2689 score: 0.8837 time: 0.05s
Test loss: 0.2705 score: 0.9091 time: 0.05s
Epoch 426/1000, LR 0.000169
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.13s
Val loss: 0.2686 score: 0.8837 time: 0.05s
Test loss: 0.2700 score: 0.9091 time: 0.04s
Epoch 427/1000, LR 0.000168
Train loss: 0.5889;  Loss pred: 0.5889; Loss self: 0.0000; time: 0.12s
Val loss: 0.2683 score: 0.8837 time: 0.05s
Test loss: 0.2695 score: 0.9091 time: 0.04s
Epoch 428/1000, LR 0.000168
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.13s
Val loss: 0.2681 score: 0.8837 time: 0.05s
Test loss: 0.2693 score: 0.9091 time: 0.04s
Epoch 429/1000, LR 0.000168
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.12s
Val loss: 0.2679 score: 0.8837 time: 0.05s
Test loss: 0.2692 score: 0.9091 time: 0.04s
Epoch 430/1000, LR 0.000167
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 0.13s
Val loss: 0.2677 score: 0.8837 time: 0.05s
Test loss: 0.2693 score: 0.9091 time: 0.04s
Epoch 431/1000, LR 0.000167
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.12s
Val loss: 0.2676 score: 0.8837 time: 0.05s
Test loss: 0.2696 score: 0.9091 time: 0.04s
Epoch 432/1000, LR 0.000166
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.12s
Val loss: 0.2675 score: 0.8837 time: 0.05s
Test loss: 0.2701 score: 0.9091 time: 0.04s
Epoch 433/1000, LR 0.000166
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 0.13s
Val loss: 0.2674 score: 0.8837 time: 0.05s
Test loss: 0.2705 score: 0.9091 time: 0.04s
Epoch 434/1000, LR 0.000166
Train loss: 0.5863;  Loss pred: 0.5863; Loss self: 0.0000; time: 0.13s
Val loss: 0.2673 score: 0.8837 time: 0.05s
Test loss: 0.2708 score: 0.9091 time: 0.04s
Epoch 435/1000, LR 0.000165
Train loss: 0.5851;  Loss pred: 0.5851; Loss self: 0.0000; time: 0.13s
Val loss: 0.2672 score: 0.9070 time: 0.05s
Test loss: 0.2707 score: 0.9091 time: 0.04s
Epoch 436/1000, LR 0.000165
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.13s
Val loss: 0.2670 score: 0.9070 time: 0.05s
Test loss: 0.2703 score: 0.9091 time: 0.04s
Epoch 437/1000, LR 0.000164
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.13s
Val loss: 0.2668 score: 0.9070 time: 0.05s
Test loss: 0.2702 score: 0.9091 time: 0.04s
Epoch 438/1000, LR 0.000164
Train loss: 0.5883;  Loss pred: 0.5883; Loss self: 0.0000; time: 0.13s
Val loss: 0.2666 score: 0.9070 time: 0.05s
Test loss: 0.2700 score: 0.9091 time: 0.04s
Epoch 439/1000, LR 0.000163
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 2.70s
Val loss: 0.2663 score: 0.9070 time: 1.29s
Test loss: 0.2697 score: 0.9091 time: 0.66s
Epoch 440/1000, LR 0.000163
Train loss: 0.5876;  Loss pred: 0.5876; Loss self: 0.0000; time: 0.72s
Val loss: 0.2660 score: 0.9070 time: 0.07s
Test loss: 0.2685 score: 0.9091 time: 0.05s
Epoch 441/1000, LR 0.000163
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 0.13s
Val loss: 0.2656 score: 0.9070 time: 0.05s
Test loss: 0.2675 score: 0.9091 time: 0.04s
Epoch 442/1000, LR 0.000162
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.13s
Val loss: 0.2653 score: 0.9070 time: 0.05s
Test loss: 0.2666 score: 0.9091 time: 0.04s
Epoch 443/1000, LR 0.000162
Train loss: 0.5832;  Loss pred: 0.5832; Loss self: 0.0000; time: 0.13s
Val loss: 0.2650 score: 0.9070 time: 0.05s
Test loss: 0.2659 score: 0.9091 time: 0.04s
Epoch 444/1000, LR 0.000161
Train loss: 0.5836;  Loss pred: 0.5836; Loss self: 0.0000; time: 0.13s
Val loss: 0.2647 score: 0.9070 time: 0.05s
Test loss: 0.2653 score: 0.9091 time: 0.04s
Epoch 445/1000, LR 0.000161
Train loss: 0.5866;  Loss pred: 0.5866; Loss self: 0.0000; time: 0.12s
Val loss: 0.2645 score: 0.8837 time: 0.05s
Test loss: 0.2645 score: 0.9091 time: 0.04s
Epoch 446/1000, LR 0.000161
Train loss: 0.5839;  Loss pred: 0.5839; Loss self: 0.0000; time: 0.13s
Val loss: 0.2643 score: 0.8837 time: 0.05s
Test loss: 0.2639 score: 0.9091 time: 0.04s
Epoch 447/1000, LR 0.000160
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 0.13s
Val loss: 0.2641 score: 0.8837 time: 0.05s
Test loss: 0.2635 score: 0.9091 time: 0.04s
Epoch 448/1000, LR 0.000160
Train loss: 0.5805;  Loss pred: 0.5805; Loss self: 0.0000; time: 0.13s
Val loss: 0.2639 score: 0.8837 time: 0.05s
Test loss: 0.2632 score: 0.9091 time: 0.04s
Epoch 449/1000, LR 0.000159
Train loss: 0.5822;  Loss pred: 0.5822; Loss self: 0.0000; time: 0.12s
Val loss: 0.2637 score: 0.9070 time: 0.05s
Test loss: 0.2631 score: 0.9091 time: 0.04s
Epoch 450/1000, LR 0.000159
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.12s
Val loss: 0.2636 score: 0.9070 time: 0.05s
Test loss: 0.2630 score: 0.9091 time: 0.04s
Epoch 451/1000, LR 0.000158
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.13s
Val loss: 0.2634 score: 0.9070 time: 0.05s
Test loss: 0.2633 score: 0.9091 time: 0.04s
Epoch 452/1000, LR 0.000158
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.13s
Val loss: 0.2634 score: 0.9070 time: 0.05s
Test loss: 0.2638 score: 0.9091 time: 0.04s
Epoch 453/1000, LR 0.000158
Train loss: 0.5788;  Loss pred: 0.5788; Loss self: 0.0000; time: 0.12s
Val loss: 0.2633 score: 0.9070 time: 0.05s
Test loss: 0.2645 score: 0.9091 time: 0.04s
Epoch 454/1000, LR 0.000157
Train loss: 0.5808;  Loss pred: 0.5808; Loss self: 0.0000; time: 0.12s
Val loss: 0.2633 score: 0.9070 time: 0.05s
Test loss: 0.2649 score: 0.9091 time: 0.04s
Epoch 455/1000, LR 0.000157
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.12s
Val loss: 0.2632 score: 0.9070 time: 0.05s
Test loss: 0.2651 score: 0.9091 time: 0.04s
Epoch 456/1000, LR 0.000156
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 0.12s
Val loss: 0.2631 score: 0.9070 time: 0.05s
Test loss: 0.2651 score: 0.9091 time: 0.04s
Epoch 457/1000, LR 0.000156
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 0.12s
Val loss: 0.2630 score: 0.9070 time: 0.05s
Test loss: 0.2648 score: 0.9091 time: 0.04s
Epoch 458/1000, LR 0.000155
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 0.12s
Val loss: 0.2628 score: 0.9070 time: 0.05s
Test loss: 0.2644 score: 0.9091 time: 0.04s
Epoch 459/1000, LR 0.000155
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.12s
Val loss: 0.2625 score: 0.9070 time: 0.05s
Test loss: 0.2638 score: 0.9091 time: 0.05s
Epoch 460/1000, LR 0.000155
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 0.12s
Val loss: 0.2623 score: 0.9070 time: 0.05s
Test loss: 0.2633 score: 0.9091 time: 0.04s
Epoch 461/1000, LR 0.000154
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 0.13s
Val loss: 0.2622 score: 0.9070 time: 0.05s
Test loss: 0.2629 score: 0.9091 time: 0.04s
Epoch 462/1000, LR 0.000154
Train loss: 0.5792;  Loss pred: 0.5792; Loss self: 0.0000; time: 0.12s
Val loss: 0.2620 score: 0.9070 time: 0.05s
Test loss: 0.2625 score: 0.9091 time: 0.04s
Epoch 463/1000, LR 0.000153
Train loss: 0.5830;  Loss pred: 0.5830; Loss self: 0.0000; time: 0.12s
Val loss: 0.2618 score: 0.9070 time: 0.05s
Test loss: 0.2621 score: 0.9091 time: 0.04s
Epoch 464/1000, LR 0.000153
Train loss: 0.5772;  Loss pred: 0.5772; Loss self: 0.0000; time: 0.13s
Val loss: 0.2616 score: 0.9070 time: 0.05s
Test loss: 0.2616 score: 0.9091 time: 0.04s
Epoch 465/1000, LR 0.000153
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 0.12s
Val loss: 0.2615 score: 0.9070 time: 0.05s
Test loss: 0.2610 score: 0.9091 time: 0.04s
Epoch 466/1000, LR 0.000152
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.12s
Val loss: 0.2613 score: 0.9070 time: 0.05s
Test loss: 0.2606 score: 0.9091 time: 0.19s
Epoch 467/1000, LR 0.000152
Train loss: 0.5775;  Loss pred: 0.5775; Loss self: 0.0000; time: 1.84s
Val loss: 0.2612 score: 0.9070 time: 0.27s
Test loss: 0.2603 score: 0.9091 time: 1.27s
Epoch 468/1000, LR 0.000151
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 0.49s
Val loss: 0.2611 score: 0.9070 time: 0.28s
Test loss: 0.2603 score: 0.9091 time: 1.14s
Epoch 469/1000, LR 0.000151
Train loss: 0.5779;  Loss pred: 0.5779; Loss self: 0.0000; time: 1.60s
Val loss: 0.2611 score: 0.9070 time: 0.20s
Test loss: 0.2604 score: 0.9091 time: 0.68s
Epoch 470/1000, LR 0.000150
Train loss: 0.5734;  Loss pred: 0.5734; Loss self: 0.0000; time: 0.47s
Val loss: 0.2610 score: 0.9070 time: 0.24s
Test loss: 0.2605 score: 0.9091 time: 0.19s
Epoch 471/1000, LR 0.000150
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.15s
Val loss: 0.2610 score: 0.9070 time: 0.05s
Test loss: 0.2607 score: 0.9091 time: 0.05s
Epoch 472/1000, LR 0.000150
Train loss: 0.5745;  Loss pred: 0.5745; Loss self: 0.0000; time: 0.14s
Val loss: 0.2609 score: 0.9070 time: 0.05s
Test loss: 0.2610 score: 0.9091 time: 0.05s
Epoch 473/1000, LR 0.000149
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.14s
Val loss: 0.2609 score: 0.9070 time: 0.05s
Test loss: 0.2612 score: 0.9091 time: 0.05s
Epoch 474/1000, LR 0.000149
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 0.14s
Val loss: 0.2608 score: 0.9070 time: 0.05s
Test loss: 0.2611 score: 0.9091 time: 0.05s
Epoch 475/1000, LR 0.000148
Train loss: 0.5741;  Loss pred: 0.5741; Loss self: 0.0000; time: 0.14s
Val loss: 0.2607 score: 0.9070 time: 0.13s
Test loss: 0.2612 score: 0.9091 time: 0.04s
Epoch 476/1000, LR 0.000148
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.12s
Val loss: 0.2607 score: 0.9070 time: 0.05s
Test loss: 0.2616 score: 0.9091 time: 0.04s
Epoch 477/1000, LR 0.000147
Train loss: 0.5735;  Loss pred: 0.5735; Loss self: 0.0000; time: 0.12s
Val loss: 0.2606 score: 0.9070 time: 0.05s
Test loss: 0.2617 score: 0.9091 time: 0.04s
Epoch 478/1000, LR 0.000147
Train loss: 0.5731;  Loss pred: 0.5731; Loss self: 0.0000; time: 0.12s
Val loss: 0.2605 score: 0.9070 time: 0.05s
Test loss: 0.2616 score: 0.9091 time: 0.04s
Epoch 479/1000, LR 0.000147
Train loss: 0.5718;  Loss pred: 0.5718; Loss self: 0.0000; time: 0.12s
Val loss: 0.2603 score: 0.9070 time: 0.05s
Test loss: 0.2614 score: 0.9091 time: 0.04s
Epoch 480/1000, LR 0.000146
Train loss: 0.5717;  Loss pred: 0.5717; Loss self: 0.0000; time: 0.12s
Val loss: 0.2602 score: 0.9070 time: 0.05s
Test loss: 0.2612 score: 0.9091 time: 0.04s
Epoch 481/1000, LR 0.000146
Train loss: 0.5712;  Loss pred: 0.5712; Loss self: 0.0000; time: 0.12s
Val loss: 0.2599 score: 0.9070 time: 0.05s
Test loss: 0.2604 score: 0.9091 time: 0.04s
Epoch 482/1000, LR 0.000145
Train loss: 0.5733;  Loss pred: 0.5733; Loss self: 0.0000; time: 0.12s
Val loss: 0.2596 score: 0.9070 time: 0.05s
Test loss: 0.2593 score: 0.9091 time: 0.04s
Epoch 483/1000, LR 0.000145
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 0.13s
Val loss: 0.2592 score: 0.9070 time: 0.05s
Test loss: 0.2580 score: 0.9091 time: 0.04s
Epoch 484/1000, LR 0.000144
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.13s
Val loss: 0.2589 score: 0.9070 time: 0.05s
Test loss: 0.2568 score: 0.9091 time: 0.04s
Epoch 485/1000, LR 0.000144
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.12s
Val loss: 0.2586 score: 0.9070 time: 0.05s
Test loss: 0.2559 score: 0.9091 time: 0.04s
Epoch 486/1000, LR 0.000144
Train loss: 0.5695;  Loss pred: 0.5695; Loss self: 0.0000; time: 0.12s
Val loss: 0.2585 score: 0.9070 time: 0.05s
Test loss: 0.2552 score: 0.9091 time: 0.04s
Epoch 487/1000, LR 0.000143
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.13s
Val loss: 0.2583 score: 0.9070 time: 0.05s
Test loss: 0.2553 score: 0.9091 time: 0.04s
Epoch 488/1000, LR 0.000143
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.12s
Val loss: 0.2582 score: 0.9070 time: 0.05s
Test loss: 0.2553 score: 0.9091 time: 0.04s
Epoch 489/1000, LR 0.000142
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 0.12s
Val loss: 0.2581 score: 0.9070 time: 0.05s
Test loss: 0.2555 score: 0.9091 time: 0.04s
Epoch 490/1000, LR 0.000142
Train loss: 0.5698;  Loss pred: 0.5698; Loss self: 0.0000; time: 0.12s
Val loss: 0.2580 score: 0.9070 time: 0.05s
Test loss: 0.2559 score: 0.9091 time: 0.04s
Epoch 491/1000, LR 0.000141
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 0.12s
Val loss: 0.2579 score: 0.9070 time: 0.05s
Test loss: 0.2564 score: 0.9091 time: 0.04s
Epoch 492/1000, LR 0.000141
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 0.12s
Val loss: 0.2579 score: 0.9070 time: 0.05s
Test loss: 0.2567 score: 0.9091 time: 0.04s
Epoch 493/1000, LR 0.000141
Train loss: 0.5672;  Loss pred: 0.5672; Loss self: 0.0000; time: 0.12s
Val loss: 0.2578 score: 0.9070 time: 0.05s
Test loss: 0.2569 score: 0.9091 time: 0.04s
Epoch 494/1000, LR 0.000140
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 0.12s
Val loss: 0.2577 score: 0.9070 time: 0.05s
Test loss: 0.2567 score: 0.9091 time: 0.04s
Epoch 495/1000, LR 0.000140
Train loss: 0.5680;  Loss pred: 0.5680; Loss self: 0.0000; time: 0.12s
Val loss: 0.2575 score: 0.9070 time: 0.05s
Test loss: 0.2564 score: 0.9091 time: 0.04s
Epoch 496/1000, LR 0.000139
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 0.12s
Val loss: 0.2573 score: 0.9070 time: 0.05s
Test loss: 0.2559 score: 0.9091 time: 0.04s
Epoch 497/1000, LR 0.000139
Train loss: 0.5642;  Loss pred: 0.5642; Loss self: 0.0000; time: 0.12s
Val loss: 0.2571 score: 0.9070 time: 0.05s
Test loss: 0.2557 score: 0.9091 time: 0.04s
Epoch 498/1000, LR 0.000138
Train loss: 0.5694;  Loss pred: 0.5694; Loss self: 0.0000; time: 0.12s
Val loss: 0.2569 score: 0.9070 time: 0.05s
Test loss: 0.2552 score: 0.9091 time: 0.04s
Epoch 499/1000, LR 0.000138
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 0.12s
Val loss: 0.2567 score: 0.9070 time: 0.05s
Test loss: 0.2545 score: 0.9091 time: 0.04s
Epoch 500/1000, LR 0.000138
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.13s
Val loss: 0.2564 score: 0.9070 time: 0.05s
Test loss: 0.2535 score: 0.9091 time: 0.04s
Epoch 501/1000, LR 0.000137
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.12s
Val loss: 0.2562 score: 0.9070 time: 0.05s
Test loss: 0.2529 score: 0.9091 time: 0.04s
Epoch 502/1000, LR 0.000137
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 0.12s
Val loss: 0.2561 score: 0.9070 time: 0.05s
Test loss: 0.2526 score: 0.9091 time: 0.04s
Epoch 503/1000, LR 0.000136
Train loss: 0.5720;  Loss pred: 0.5720; Loss self: 0.0000; time: 0.12s
Val loss: 0.2560 score: 0.9070 time: 0.05s
Test loss: 0.2528 score: 0.9091 time: 0.04s
Epoch 504/1000, LR 0.000136
Train loss: 0.5652;  Loss pred: 0.5652; Loss self: 0.0000; time: 0.13s
Val loss: 0.2559 score: 0.9070 time: 0.05s
Test loss: 0.2531 score: 0.9091 time: 0.05s
Epoch 505/1000, LR 0.000135
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.12s
Val loss: 0.2558 score: 0.9070 time: 0.05s
Test loss: 0.2532 score: 0.9091 time: 0.04s
Epoch 506/1000, LR 0.000135
Train loss: 0.5660;  Loss pred: 0.5660; Loss self: 0.0000; time: 0.12s
Val loss: 0.2557 score: 0.9070 time: 0.05s
Test loss: 0.2532 score: 0.9091 time: 0.06s
Epoch 507/1000, LR 0.000135
Train loss: 0.5605;  Loss pred: 0.5605; Loss self: 0.0000; time: 0.13s
Val loss: 0.2556 score: 0.9070 time: 0.05s
Test loss: 0.2534 score: 0.9091 time: 0.04s
Epoch 508/1000, LR 0.000134
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.12s
Val loss: 0.2556 score: 0.9070 time: 0.05s
Test loss: 0.2535 score: 0.9091 time: 0.05s
Epoch 509/1000, LR 0.000134
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.12s
Val loss: 0.2555 score: 0.9070 time: 0.05s
Test loss: 0.2535 score: 0.9091 time: 0.04s
Epoch 510/1000, LR 0.000133
Train loss: 0.5687;  Loss pred: 0.5687; Loss self: 0.0000; time: 0.12s
Val loss: 0.2554 score: 0.9070 time: 0.04s
Test loss: 0.2538 score: 0.9091 time: 0.04s
Epoch 511/1000, LR 0.000133
Train loss: 0.5674;  Loss pred: 0.5674; Loss self: 0.0000; time: 0.12s
Val loss: 0.2554 score: 0.9070 time: 0.04s
Test loss: 0.2541 score: 0.9091 time: 0.04s
Epoch 512/1000, LR 0.000132
Train loss: 0.5598;  Loss pred: 0.5598; Loss self: 0.0000; time: 0.12s
Val loss: 0.2553 score: 0.9070 time: 0.04s
Test loss: 0.2543 score: 0.9091 time: 0.04s
Epoch 513/1000, LR 0.000132
Train loss: 0.5643;  Loss pred: 0.5643; Loss self: 0.0000; time: 0.12s
Val loss: 0.2553 score: 0.9070 time: 0.04s
Test loss: 0.2544 score: 0.9091 time: 0.04s
Epoch 514/1000, LR 0.000132
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.12s
Val loss: 0.2552 score: 0.9070 time: 0.04s
Test loss: 0.2543 score: 0.9091 time: 0.04s
Epoch 515/1000, LR 0.000131
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.12s
Val loss: 0.2550 score: 0.9070 time: 0.04s
Test loss: 0.2541 score: 0.9091 time: 0.04s
Epoch 516/1000, LR 0.000131
Train loss: 0.5655;  Loss pred: 0.5655; Loss self: 0.0000; time: 0.12s
Val loss: 0.2550 score: 0.9070 time: 0.05s
Test loss: 0.2539 score: 0.9091 time: 0.04s
Epoch 517/1000, LR 0.000130
Train loss: 0.5626;  Loss pred: 0.5626; Loss self: 0.0000; time: 0.12s
Val loss: 0.2548 score: 0.9070 time: 0.04s
Test loss: 0.2536 score: 0.9091 time: 0.04s
Epoch 518/1000, LR 0.000130
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.12s
Val loss: 0.2547 score: 0.9070 time: 0.04s
Test loss: 0.2532 score: 0.9091 time: 0.04s
Epoch 519/1000, LR 0.000129
Train loss: 0.5628;  Loss pred: 0.5628; Loss self: 0.0000; time: 0.12s
Val loss: 0.2545 score: 0.9070 time: 0.04s
Test loss: 0.2527 score: 0.9091 time: 0.04s
Epoch 520/1000, LR 0.000129
Train loss: 0.5634;  Loss pred: 0.5634; Loss self: 0.0000; time: 0.12s
Val loss: 0.2543 score: 0.9070 time: 0.04s
Test loss: 0.2522 score: 0.9091 time: 0.04s
Epoch 521/1000, LR 0.000129
Train loss: 0.5571;  Loss pred: 0.5571; Loss self: 0.0000; time: 0.12s
Val loss: 0.2542 score: 0.9070 time: 0.04s
Test loss: 0.2516 score: 0.9091 time: 0.04s
Epoch 522/1000, LR 0.000128
Train loss: 0.5622;  Loss pred: 0.5622; Loss self: 0.0000; time: 0.12s
Val loss: 0.2540 score: 0.9070 time: 0.04s
Test loss: 0.2512 score: 0.9091 time: 0.04s
Epoch 523/1000, LR 0.000128
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.12s
Val loss: 0.2539 score: 0.9070 time: 0.04s
Test loss: 0.2507 score: 0.9091 time: 0.04s
Epoch 524/1000, LR 0.000127
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 0.12s
Val loss: 0.2537 score: 0.9070 time: 0.04s
Test loss: 0.2501 score: 0.9091 time: 0.04s
Epoch 525/1000, LR 0.000127
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 0.12s
Val loss: 0.2536 score: 0.9070 time: 0.04s
Test loss: 0.2497 score: 0.9091 time: 0.04s
Epoch 526/1000, LR 0.000126
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.12s
Val loss: 0.2535 score: 0.9070 time: 0.04s
Test loss: 0.2493 score: 0.9091 time: 0.04s
Epoch 527/1000, LR 0.000126
Train loss: 0.5620;  Loss pred: 0.5620; Loss self: 0.0000; time: 0.12s
Val loss: 0.2534 score: 0.9070 time: 0.04s
Test loss: 0.2490 score: 0.9091 time: 0.04s
Epoch 528/1000, LR 0.000126
Train loss: 0.5630;  Loss pred: 0.5630; Loss self: 0.0000; time: 0.12s
Val loss: 0.2533 score: 0.9070 time: 0.04s
Test loss: 0.2487 score: 0.9091 time: 0.04s
Epoch 529/1000, LR 0.000125
Train loss: 0.5572;  Loss pred: 0.5572; Loss self: 0.0000; time: 0.12s
Val loss: 0.2533 score: 0.9070 time: 0.04s
Test loss: 0.2485 score: 0.9091 time: 0.04s
Epoch 530/1000, LR 0.000125
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.12s
Val loss: 0.2532 score: 0.9070 time: 0.04s
Test loss: 0.2485 score: 0.9091 time: 0.04s
Epoch 531/1000, LR 0.000124
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.12s
Val loss: 0.2532 score: 0.9070 time: 0.04s
Test loss: 0.2486 score: 0.9091 time: 0.04s
Epoch 532/1000, LR 0.000124
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.12s
Val loss: 0.2533 score: 0.9070 time: 0.05s
Test loss: 0.2488 score: 0.9091 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 533/1000, LR 0.000123
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.12s
Val loss: 0.2533 score: 0.9070 time: 0.05s
Test loss: 0.2490 score: 0.9091 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 530,   Train_Loss: 0.5592,   Val_Loss: 0.2532,   Val_Precision: 0.9091,   Val_Recall: 0.9091,   Val_accuracy: 0.9091,   Val_Score: 0.9070,   Val_Loss: 0.2532,   Test_Precision: 0.8462,   Test_Recall: 1.0000,   Test_accuracy: 0.9167,   Test_Score: 0.9091,   Test_loss: 0.2486


[0.048719901009462774, 0.04843972297385335, 0.04878209496382624, 0.04846269800327718, 0.04854442900978029, 0.04944563889876008, 0.04849230800755322, 0.04869470407720655, 0.048650266020558774, 0.048511328059248626, 0.048447447014041245, 0.04857634101063013, 0.04865861497819424, 0.04891877109184861, 0.04955034202430397, 0.048693803953938186, 0.04887965496163815, 0.04850545199587941, 0.049117146991193295, 0.04886791994795203, 0.04892664903309196, 0.04875802400056273, 0.0489187550265342, 0.049039085977710783, 0.049202810972929, 0.0491185620194301, 0.04893514991272241, 0.04922198795247823, 0.04884139203932136, 0.049199708038941026, 0.04912001302000135, 0.04919311206322163, 0.05057511106133461, 0.050049636978656054, 0.05010462598875165, 0.05077693203929812, 0.05064067791681737, 0.050038640038110316, 0.050069955992512405, 2.041475855978206, 0.10327968094497919, 0.05219653807580471, 0.05108784302137792, 0.04788946302141994, 0.047622455982491374, 0.04804917797446251, 0.04835395701229572, 0.048112966935150325, 0.047881923033855855, 0.047630815068259835, 0.050002154894173145, 0.04803178203292191, 0.04779623099602759, 0.04842487093992531, 0.04796124703716487, 0.04785138298757374, 0.047611702932044864, 0.04775148199405521, 0.04822769504971802, 0.04841428296640515, 0.04773385694716126, 0.04757533501833677, 0.04787307302467525, 0.04833253694232553, 0.04828893393278122, 0.04805391398258507, 0.04827512102201581, 0.047752336016856134, 0.04820447601377964, 0.04798825900070369, 0.04761714208871126, 0.04809394804760814, 0.04764544602949172, 0.047969033010303974, 0.04769421799574047, 0.04900119197554886, 0.047721152077429, 0.04761639004573226, 0.04758261900860816, 0.0477078229887411, 0.048326091025955975, 0.04751961899455637, 0.04773753497283906, 0.048833750071935356, 0.04824724793434143, 0.05048157298006117, 0.44252417609095573, 0.4006156630348414, 0.5000023739412427, 0.36204768903553486, 0.653738570981659, 0.05455024994444102, 0.05920404801145196, 0.05615403805859387, 0.05106244399212301, 0.05070537095889449, 0.05137146497145295, 0.051393480971455574, 0.050792037043720484, 0.05187812994699925, 0.050632495898753405, 0.050776202930137515, 0.050769782974384725, 0.05060841899830848, 0.05241295194718987, 0.0513810709817335, 0.05127360799815506, 0.6136464800219983, 0.33236312202643603, 0.15049156197346747, 0.047740194015204906, 0.047472766949795187, 0.04805603891145438, 0.047379914904013276, 0.04797000391408801, 0.04711998603306711, 0.04730746801942587, 0.047435994958505034, 0.050683238077908754, 0.04986973700579256, 0.050138568971306086, 0.05024028196930885, 0.05027289595454931, 0.049925948958843946, 0.050074666971340775, 0.0503633520565927, 0.051685575977899134, 0.05107265501283109, 0.050600153976120055, 0.05087169399484992, 0.050950036966241896, 0.05084042204543948, 0.050955082988366485, 0.05107141600456089, 0.05144475609995425, 0.051093677058815956, 0.050542755983769894, 0.05076140398159623, 0.05093701707664877, 0.05035517492797226, 0.0509150440338999, 0.050380176049657166, 0.050330887897871435, 0.05026034195907414, 0.050488968030549586, 0.05189831997267902, 0.050787186017259955, 0.05016087705735117, 0.0508672830183059, 0.05031625006813556, 0.05048103292938322, 0.05031345994211733, 0.050709221977740526, 0.0505685469834134, 0.05214811896439642, 0.05072355701122433, 0.054152825963683426, 0.05081368191167712, 0.050782763049937785, 0.04818062297999859, 0.048500805045478046, 0.04865046206396073, 0.458077208022587, 0.4525735219940543, 1.576850524987094, 0.28232625406235456, 0.22851383197121322, 0.05224173306487501, 0.05187846999615431, 0.05145821091718972, 0.051545051974244416, 0.05162431998178363, 0.05118443793617189, 0.051324820960871875, 0.051490581943653524, 0.05156312498729676, 0.05127959104720503, 0.05084381392225623, 0.051133931963704526, 0.051397881004959345, 0.05099173996131867, 0.051329904003068805, 0.051287299022078514, 0.05212726700119674, 0.051359634031541646, 0.05117790901567787, 0.40525953099131584, 0.3955076450947672, 0.1289836650248617, 0.0631486129714176, 0.05097526800818741, 0.04789715900551528, 0.04721590399276465, 0.04732513800263405, 0.04739703901577741, 0.047754994011484087, 0.04837694298475981, 0.04750566801521927, 0.04774624097626656, 0.04772044799756259, 0.04767238500062376, 0.04807174589950591, 0.048268140060827136, 0.04785160592291504, 0.048468675930052996, 0.04887597600463778, 0.04809110704809427, 0.047575504053384066, 0.04777751804795116, 0.04816150804981589, 0.048277751076966524, 0.04813643801026046, 0.04849596903659403, 0.04849418601952493, 0.0479559029918164, 0.04806069296319038, 0.04818103008437902, 0.048526049009524286, 0.04885623196605593, 0.04777152999304235, 0.04749674000777304, 0.048602978931739926, 0.04824055708013475, 0.04814718407578766, 2.3947698320262134, 0.26355008501559496, 0.05041735200211406, 0.04869071999564767, 0.04848608293104917, 0.06538930896203965, 0.0489547080360353, 0.0493519720621407, 0.048757737968117, 0.04959038598462939, 0.04822538897860795, 0.04871467698831111, 0.7884109150618315, 0.21266760409343988, 0.05351870600134134, 0.04957054799888283, 0.04859499796293676, 0.04833708389196545, 0.04824184800963849, 0.048053211998194456, 0.047829907038249075, 0.048016485990956426, 0.04766812291927636, 0.04756736394483596, 0.04830897506326437, 0.047584335901774466, 0.04820343595929444, 0.04762212501373142, 0.04826712200883776, 0.04767665406689048, 0.04768961202353239, 0.04866640595719218, 0.047835079953074455, 0.047625549021176994, 0.04728390905074775, 0.04744243191089481, 0.04859746899455786, 0.047569366986863315, 0.04754374991171062, 0.04765801294706762, 0.047587255015969276, 0.04738205100875348, 0.04749208700377494, 0.04764461098238826, 0.047676275949925184, 0.047664131969213486, 0.04731318599078804, 0.04839880403596908, 0.04760862106923014, 0.04835478204768151, 0.04791871807537973, 0.048014341038651764, 0.048158177989535034, 0.04776678094640374, 0.0475079930620268, 0.04773250792641193, 0.04833689797669649, 0.04855369601864368, 0.04827885399572551, 0.04927168495487422, 0.048920986941084266, 0.04791063000448048, 0.2897897729417309, 0.7381726250750944, 0.4593653449555859, 1.2395813229959458, 0.09344092209357768, 0.14008201903197914, 0.0583133939653635, 0.04977984295692295, 0.0483482510317117, 0.04851078300271183, 0.04809967498295009, 0.048206466948613524, 0.04917947796639055, 0.04853068199008703, 0.04800450999755412, 0.04815323697403073, 0.048342218971811235, 0.047838842030614614, 0.04818425502162427, 0.047898245975375175, 0.048205555067397654, 0.047995011089369655, 0.049249119008891284, 0.04830540798138827, 0.04902787902392447, 0.04862347198650241, 0.04883534798864275, 0.048173050978221, 0.8612362559651956, 0.3387226879131049, 0.19007719797082245, 0.47557216498535126, 0.14095541497226804, 0.05526425701100379, 0.052614322979934514, 0.05225832492578775, 0.0521233530016616, 0.048828131053596735, 0.04771686694584787, 0.04839643894229084, 0.047753500984981656, 0.04743780696298927, 0.04807711497414857, 0.05204032291658223, 0.04972281900700182, 0.048701749998144805, 0.047989295911975205, 0.04938708804547787, 0.050288203987292945, 0.04851317300926894, 0.6251256630057469, 0.6036308910697699, 0.0527810410130769, 0.049154203035868704, 0.11117297504097223, 0.04815120901912451, 0.048075916012749076, 0.04804728494491428, 0.04827727598603815, 0.04974147607572377, 0.050802918965928257, 0.04885225393809378, 0.04894773301202804, 0.0486807000124827, 0.04857354809064418, 0.049261410953477025, 0.049839157960377634, 0.0491335749393329, 0.04879976308438927, 0.049002116080373526, 0.0489936739904806, 0.048990379087626934, 0.04867244104389101, 0.048876340966671705, 0.0489846559939906, 0.049175970954820514, 0.04873936309013516, 0.0491667561000213, 0.04903751995880157, 0.0485197160160169, 0.8526058230781928, 0.30939263408072293, 0.31975850905291736, 0.19693535706028342, 0.04912204900756478, 0.04714282404165715, 0.04736068600323051, 0.04722484201192856, 0.04686315392609686, 0.04749090597033501, 0.04738075891509652, 0.047099807066842914, 0.04718406801111996, 0.04729530494660139, 0.047964540077373385, 0.04775561403948814, 0.04875132301822305, 0.04751951701473445, 0.048102684086188674, 0.04753272305242717, 0.0474733259761706, 0.04768811189569533, 0.0473485499387607, 0.04741847398690879, 0.047457140986807644, 0.04782584204804152, 0.04775499098468572, 0.04734906705562025, 0.04749180004000664, 0.04733386996667832, 0.047590932925231755, 0.04749152495060116, 0.048018497065640986, 0.048212944995611906, 0.7981649809516966, 0.2172218560008332, 0.11254435207229108, 0.07818256900645792, 0.053816459025256336, 0.05097218498121947, 0.05322369304485619, 0.04742358298972249, 0.04807779204566032, 0.047357489936985075, 0.048764994950033724, 0.04752459400333464, 0.048253042972646654, 0.04890897299628705, 0.048658602056093514, 0.37424834293778986, 0.28681241802405566, 0.5432043320033699, 1.7325441029388458, 0.1477621759986505, 0.04789954505395144, 0.05146313295699656, 0.051390461972914636, 0.04776714800391346, 0.04946132795885205, 0.047519418061710894, 0.04717760509811342, 0.04746461100876331, 0.04807021107990295, 0.047600410995073617, 0.04768546193372458, 0.04772773594595492, 0.04892608802765608, 0.04795405105687678, 0.04764874407555908, 0.0491566980490461, 0.047673003980889916, 0.0480427440488711, 0.6652906959643587, 0.05082465405575931, 0.04857730900403112, 0.048106097034178674, 0.04749472509138286, 0.047246453003026545, 0.04747472598683089, 0.04759870900306851, 0.04783343605231494, 0.04747703904286027, 0.047563771018758416, 0.0473217029357329, 0.04898884007707238, 0.04690191801637411, 0.047084485995583236, 0.04753799398895353, 0.04710252198856324, 0.047126140096224844, 0.04740215803030878, 0.04701379395555705, 0.04979936999734491, 0.04726958810351789, 0.04762652900535613, 0.04736364202108234, 0.04765110998414457, 0.04789217503275722, 0.04777521302457899, 0.19681170105468482, 1.2796548580517992, 1.1473033140646294, 0.6859100220026448, 0.19122674502432346, 0.05540991295129061, 0.05316598305944353, 0.053115046001039445, 0.05264941102359444, 0.04718711704481393, 0.047146411961875856, 0.047542148968204856, 0.04722428205423057, 0.04745142697356641, 0.0485511819133535, 0.047447280026972294, 0.047279729042202234, 0.04794124001637101, 0.04731797194108367, 0.047766458010300994, 0.04847475909627974, 0.04748609592206776, 0.04734762804582715, 0.047394071938470006, 0.046891938080079854, 0.04687481699511409, 0.04725347296334803, 0.04676702793221921, 0.046567909070290625, 0.04675654193852097, 0.04687063698656857, 0.0474167870124802, 0.04689379199407995, 0.04690806195139885, 0.04663229000288993, 0.04628025589045137, 0.048841719049960375, 0.04688908299431205, 0.049675244954414666, 0.046493593021295965, 0.06602652999572456, 0.0490193230798468, 0.05352960689924657, 0.04515134391840547, 0.04524934699293226, 0.04468431998975575, 0.04461457999423146, 0.044769377913326025, 0.04460369108710438, 0.04477824503555894, 0.045741156907752156, 0.04535843292251229, 0.045256206998601556, 0.044813734013587236, 0.04592818103265017, 0.04498231294564903, 0.0448489710688591, 0.044970363029278815, 0.04499766801018268, 0.045212651952169836, 0.044661305961199105, 0.04532068897970021, 0.04475832695607096, 0.04529496200848371, 0.04543884703889489, 0.04605495999567211, 0.0480882580159232, 0.04638985707424581]
[0.0011072704774877902, 0.0011009027948603034, 0.0011086839764505964, 0.001101424954619936, 0.0011032824774950066, 0.0011237645204263654, 0.0011020979092625732, 0.0011066978199365126, 0.0011056878641036085, 0.0011025301831647414, 0.00110107834122821, 0.0011040077502415938, 0.001105877613140778, 0.0011117902520874684, 0.0011261441369159993, 0.0011066773625895041, 0.0011109012491281399, 0.0011023966362699866, 0.001116298795254393, 0.0011106345442716372, 0.0011119692962066356, 0.0011081369091036984, 0.0011117898869666863, 0.0011145246813116087, 0.0011182457039302046, 0.0011163309549870478, 0.0011121624980164183, 0.0011186815443745052, 0.0011100316372573036, 0.0011181751827032051, 0.001116363932272758, 0.001118025274164128, 0.0011494343423030593, 0.0011374917495149102, 0.0011387414997443557, 0.0011540211827113208, 0.0011509244981094855, 0.0011372418190479618, 0.001137953545284373, 0.04639717854495922, 0.0023472654760222545, 0.0011862849562682889, 0.0011610873413949528, 0.0010883968868504533, 0.001082328545056622, 0.0010920267721468751, 0.0010989535684612665, 0.0010934765212534164, 0.001088225523496724, 0.0010825185242786326, 0.0011364126112312078, 0.0010916314098391342, 0.0010862779771824453, 0.0011005652486346662, 0.0010900283417537469, 0.0010875314315357668, 0.0010820841575464742, 0.0010852609544103457, 0.0010960839784026823, 0.0011003246128728445, 0.0010848603851627558, 0.0010812576140531085, 0.0010880243869244375, 0.0010984667486892167, 0.0010974757711995733, 0.0010921344086951153, 0.0010971618414094503, 0.0010852803640194577, 0.0010955562730404463, 0.0010906422500159931, 0.0010822077747434378, 0.0010930442738092759, 0.0010828510461248118, 0.0010902052956887267, 0.0010839594999031926, 0.0011136634539897468, 0.0010845716381233863, 0.0010821906828575513, 0.0010814231592865492, 0.0010842687042895705, 0.0010983202505899085, 0.001079991340785372, 0.001084943976655433, 0.001109857956180349, 0.0010965283621441233, 0.0011473084768195722, 0.010057367638430813, 0.009104901432610031, 0.011363690316846425, 0.00822835656898943, 0.014857694795037705, 0.001239778407828205, 0.0013455465457148173, 0.0012762281376953151, 0.0011605100907300684, 0.0011523947945203292, 0.001167533294805749, 0.001168033658442172, 0.0011543644782663746, 0.0011790484078863467, 0.0011507385431534865, 0.00115400461204858, 0.0011538587039632891, 0.0011501913408706473, 0.0011912034533452243, 0.0011677516132212159, 0.0011653092726853422, 0.01394651090959087, 0.007553707318782637, 0.0034202627721242607, 0.001085004409436475, 0.0010789265215862542, 0.001092182702533054, 0.0010768162478184836, 0.0010902273616838183, 0.001070908773478798, 0.0010751697277142243, 0.001078090794511478, 0.0011518917744979262, 0.0011334031137680126, 0.0011395129311660473, 0.0011418245902115648, 0.001142565817148848, 0.0011346806581555443, 0.0011380606129850175, 0.0011446216376498342, 0.0011746721813158895, 0.0011607421593825247, 0.001150003499457274, 0.0011561748635193164, 0.0011579553855964068, 0.0011554641373963518, 0.0011580700679174201, 0.0011607140001036566, 0.0011691990022716875, 0.0011612199331549082, 0.001148698999631134, 0.0011536682723090053, 0.0011576594790147449, 0.0011444357938175513, 0.0011571600916795433, 0.0011450040011285719, 0.0011438838158607143, 0.0011422804990698669, 0.0011474765461488541, 0.0011795072721063414, 0.001154254227664999, 0.0011400199331216174, 0.0011560746140524068, 0.0011435511379121717, 0.0011472962029405278, 0.0011434877259572122, 0.001152482317675921, 0.001149285158713941, 0.0011851845219181005, 0.001152808113891462, 0.0012307460446291689, 0.001154856407083571, 0.0011541537056804043, 0.0010950141586363316, 0.0011022910237608646, 0.0011056923196354712, 0.010410845636876977, 0.010285761863501235, 0.03583751193152487, 0.006416505774144422, 0.005193496181163937, 0.0011873121151107955, 0.0011790561362762344, 0.0011695047935724936, 0.0011714784539601003, 0.0011732799995859916, 0.001163282680367543, 0.001166473203656179, 0.0011702404987193984, 0.0011718892042567445, 0.0011654452510728415, 0.0011555412255058234, 0.001162134817356921, 0.0011681336592036214, 0.0011589031809390608, 0.0011665887273424728, 0.0011656204323199663, 0.0011847106136635623, 0.0011672644098077647, 0.0011631342958108607, 0.00921044388616627, 0.008988810115790164, 0.0029314469323832204, 0.0014351957493504, 0.0011585288183678958, 0.0010885717955798927, 0.0010730887271082875, 0.001075571318241683, 0.0010772054321767594, 0.0010853407729882747, 0.0010994759769263592, 0.0010796742730731653, 0.0010851418403696946, 0.0010845556363082405, 0.001083463295468722, 0.0010925396795342253, 0.0010970031832006168, 0.0010875364982480692, 0.0011015608165921135, 0.0011108176364690405, 0.0010929797056385062, 0.0010812614557587287, 0.001085852682907981, 0.0010945797284049067, 0.0010972216153856027, 0.001094009954778647, 0.0011021811144680462, 0.0011021405913528395, 0.0010899068861776454, 0.001092288476436145, 0.001095023411008614, 0.001102864750216461, 0.0011103689083194529, 0.0010857165907509625, 0.0010794713638130236, 0.0011046131575395439, 0.0010963762972757897, 0.0010942541835406287, 0.05442658709150485, 0.00598977465944534, 0.0011458489091389558, 0.0011066072726283562, 0.0011019564302511174, 0.0014861206582281739, 0.001112607000818984, 0.001121635728685016, 0.0011081304083662953, 0.0011270542269233954, 0.0010960315676956352, 0.0011071517497343434, 0.017918429887768896, 0.00483335463848727, 0.0012163342273032124, 0.0011266033636109735, 0.0011044317718849263, 0.0010985700884537603, 0.001096405636582693, 0.0010921184545044196, 0.0010870433417783881, 0.0010912837725217369, 0.0010833664299835536, 0.0010810764532917265, 0.0010979312514378266, 0.0010814621795857834, 0.00109553263543851, 0.0010823210230393504, 0.0010969800456554037, 0.0010835603197020564, 0.0010838548187166452, 0.0011060546808452768, 0.0010871609080244195, 0.0010823988413903862, 0.0010746342966079035, 0.001078237088883973, 0.0011044879316944969, 0.0010811219769741663, 0.001080539770720696, 0.0010831366578879004, 0.001081528523090211, 0.001076864795653488, 0.0010793656137221578, 0.0010828320677815514, 0.0010835517261346633, 0.0010832757265730338, 0.0010752996816088191, 0.0010999728189992973, 0.0010820141152097758, 0.001098972319265489, 0.0010890617744404483, 0.0010912350236057218, 0.0010945040452167052, 0.0010856086578728123, 0.0010797271150460635, 0.001084829725600271, 0.0010985658631067383, 0.0011034930913328108, 0.0010972466817210343, 0.0011198110217016867, 0.0011118406122973697, 0.0010888779546472836, 0.006586131203221157, 0.01677665056988851, 0.010440121476263315, 0.028172302795362404, 0.0021236573203085836, 0.003183682250726799, 0.001325304408303716, 0.0011313600672027942, 0.0010988238870843568, 0.001102517795516178, 0.0010931744314306839, 0.0010956015215593982, 0.001117715408327058, 0.0011029700452292507, 0.0010910115908535029, 0.0010943917494097893, 0.0010986867948138918, 0.0010872464097866957, 0.0010950967050369152, 0.0010885964994403448, 0.0010955807969863104, 0.0010907957065765831, 0.0011192981592929837, 0.001097850181395188, 0.0011142699778164651, 0.0011050789087841456, 0.0011098942724691535, 0.001094842067686841, 0.019573551271936263, 0.00769824290711602, 0.004319936317518692, 0.01080845829512162, 0.0032035321584606372, 0.0012560058411591772, 0.0011957800677257844, 0.0011876892028588124, 0.0011846216591286727, 0.0011097302512181077, 0.0010844742487692697, 0.0010999190668702465, 0.001085306840567765, 0.0010781319764315742, 0.001092661703957922, 0.0011827346117405052, 0.0011300640683409504, 0.001106857954503291, 0.0010906658161812547, 0.0011224338192154062, 0.0011429137269839305, 0.0011025721138470215, 0.014207401431948792, 0.013718883887949314, 0.0011995691139335659, 0.001117140978087925, 0.0025266585236584597, 0.001094345659525557, 0.0010926344548352063, 0.0010919837487480518, 0.0010972108178645033, 0.0011304880926300857, 0.0011546117946801876, 0.0011102784985930405, 0.0011124484775460917, 0.0011063795457382432, 0.0011039442747873677, 0.0011195775216699324, 0.001132708135463128, 0.0011166721577121114, 0.0011090855246452106, 0.0011136844563721256, 0.001113492590692741, 0.0011134177065369759, 0.001106191841906614, 0.0011108259310607207, 0.001113287636227059, 0.0011176357035186481, 0.0011077127975030717, 0.0011174262750004841, 0.001114489089972763, 0.0011027208185458387, 0.019377405069958928, 0.007031650774561885, 0.0072672388421117584, 0.004475803569551896, 0.0011164102047173815, 0.0010714278191285716, 0.001076379227346148, 0.0010732918639074671, 0.001065071680138565, 0.0010793387720530684, 0.0010768354298885572, 0.0010704501606100662, 0.0010723651820709083, 0.0010748932942409408, 0.0010901031835766678, 0.0010853548645338212, 0.0011079846140505238, 0.0010799890230621465, 0.0010932428201406517, 0.0010802891602824357, 0.00107893922673115, 0.0010838207249021666, 0.0010761034076991068, 0.0010776925906115634, 0.00107857138606381, 0.0010869509556373073, 0.0010853407041974026, 0.0010761151603550059, 0.0010793590918183327, 0.0010757697719699618, 0.0010816121119370853, 0.00107935283978639, 0.001091329478764568, 0.0010957487499002707, 0.01814011320344765, 0.004936860363655301, 0.0025578261834611608, 0.0017768765683285892, 0.0012231013414830986, 0.0011584587495731698, 0.0012096293873830953, 0.0010778087043118749, 0.0010926770919468254, 0.0010763065894769336, 0.0011082953397734937, 0.0010801044091666963, 0.0010966600675601512, 0.001111567568097433, 0.0011058773194566709, 0.008505644157677043, 0.006518464046001265, 0.012345553000076588, 0.03937600233951922, 0.003358231272696602, 0.001088626023953442, 0.0011696166581135583, 0.001167965044838969, 0.0010856170000889424, 0.0011241210899739103, 0.001079986774129793, 0.0010722182976843958, 0.0010787411592900753, 0.0010925047972705215, 0.0010818275226153094, 0.0010837604984937404, 0.0010847212714989755, 0.0011119565460830927, 0.0010898647967471995, 0.0010829260017172519, 0.001117197682932866, 0.0010834773632020435, 0.0010918805465652522, 0.015120243090099062, 0.0011551057739945297, 0.0011040297500916165, 0.0010933203871404244, 0.0010794255702587013, 0.0010737830227960578, 0.0010789710451552476, 0.0010817888409788297, 0.0010871235466435212, 0.0010790236146104608, 0.0010809947958808732, 0.001075493248539384, 0.0011133827290243723, 0.0010659526821903207, 0.0010701019544450735, 0.0010804089542943984, 0.0010705118633764373, 0.0010710486385505646, 0.0010773217734161087, 0.0010684953171717511, 0.0011318038635760206, 0.0010743088205344975, 0.001082421113758094, 0.0010764464095700532, 0.001082979772366922, 0.0010884585234717551, 0.001085800296013159, 0.0044729932057882915, 0.02908306495572271, 0.026075075319650667, 0.015588864136423746, 0.004346062386916442, 0.001259316203438423, 0.001208317796805535, 0.00120716013638726, 0.0011965775232635099, 0.0010724344782912258, 0.001071509362769906, 0.0010805033856410194, 0.0010732791375961494, 0.0010784415221265094, 0.0011034359525762159, 0.0010783472733402793, 0.0010745392964136872, 0.0010895736367357049, 0.0010754084532064471, 0.0010856013184159317, 0.0011016990703699942, 0.0010792294527742672, 0.0010760824555869806, 0.001077137998601591, 0.0010657258654563602, 0.0010653367498889565, 0.0010739425673488188, 0.0010628869984595274, 0.0010583615697793323, 0.001062648680420931, 0.0010652417496947403, 0.001077654250283641, 0.0010657679998654533, 0.0010660923170772467, 0.001059824772792953, 0.0010518239975102585, 0.0011100390693172812, 0.0010656609771434557, 0.0011289828398730606, 0.0010566725686658174, 0.0015006029544482853, 0.0011140755245419728, 0.0012165819749828768, 0.001026166907236488, 0.0010283942498393696, 0.0010155527270399034, 0.0010139677271416242, 0.0010174858616665006, 0.001013720251979645, 0.0010176873871717942, 0.0010395717479034581, 0.001030873475511643, 0.0010285501590591264, 0.0010184939548542554, 0.0010438222961965948, 0.001022325294219296, 0.001019294797019525, 0.001022053705210882, 0.0010226742729586972, 0.0010275602716402236, 0.0010150296809363433, 0.0010300156586295502, 0.0010172347035470673, 0.0010294309547382661, 0.001032701069065793, 0.0010467036362652752, 0.0010929149549073454, 0.0010543149335055866]
[903.1217036227961, 908.3454094844884, 901.9702830029678, 907.9147842124802, 906.3861888484729, 889.8661435053955, 907.360400192676, 903.589021307882, 904.4143763038489, 907.0046473735212, 908.2005907813416, 905.7907426656801, 904.259194794551, 899.4502318422257, 887.9857979268522, 903.605724490568, 900.1700203189279, 907.1145240278937, 895.8175035673224, 900.3861847785474, 899.3054065533943, 902.4155695787053, 899.4505272289493, 897.2434767646129, 894.257850922551, 895.7916964791167, 899.1491816920061, 893.909446373441, 900.8752241249872, 894.3142501003153, 895.7652348765356, 894.4341627228708, 869.993146364806, 879.1272555835728, 878.1624277542332, 866.5352204805678, 868.866725525962, 879.320460477919, 878.7704947569732, 21.55303471807003, 426.0276522682171, 842.9677833441573, 861.2616504789215, 918.782488338191, 923.9338688491163, 915.728465185927, 909.9565520317484, 914.5143773674561, 918.9271694223504, 923.7717208270212, 879.9620754970184, 916.0601197315883, 920.5746788623752, 908.6240013852655, 917.4073385936916, 919.5136535850199, 924.1425382914833, 921.4373703726671, 912.3388533215265, 908.8227131347119, 921.7775979993733, 924.8489786365404, 919.0970460016438, 910.3598276354605, 911.1818467818845, 915.6382145259962, 911.4425623072774, 921.4208910003566, 912.7783068821709, 916.8909419980165, 924.036976390299, 914.8760246599937, 923.4880490522588, 917.2584319252088, 922.5436929048632, 897.937340421343, 922.0230041514649, 924.0515704307074, 924.7074019200155, 922.2806081590406, 910.4812548643251, 925.9333498663035, 921.7065779587157, 901.0162016061654, 911.969115002758, 871.605170016766, 99.4295958893697, 109.83095285561348, 87.99958218832501, 121.5309511219706, 67.30519194229163, 806.5957542781864, 743.1924248066448, 783.558966037104, 861.6900516314403, 867.7581717264161, 856.5066233647558, 856.1397120470983, 866.2775222447946, 848.1415973349875, 869.0071310721876, 866.5476632929636, 866.6572402367697, 869.4205602722077, 839.4871566160484, 856.3464941328773, 858.1412878450687, 71.70252161867313, 132.38532521818186, 292.37519647618154, 921.6552405712117, 926.8471763302146, 915.5977270842519, 928.6635505601766, 917.2398667884602, 933.7863548839422, 930.0857103984571, 927.565660602024, 868.1371133463201, 882.2986171932134, 877.5679263040167, 875.7912630123978, 875.2231031166275, 881.3052313992278, 878.6878208332871, 873.6511412218496, 851.3013382846792, 861.5177728462675, 869.5625713068997, 864.9210699461751, 863.5911300545905, 865.4530829951462, 863.5056096375234, 861.5386735325807, 855.2863952646697, 861.1633089031712, 870.5500747551067, 866.8002960665232, 863.8118705261024, 873.7930125937868, 864.1846596598094, 873.3593935168358, 874.2146589840078, 875.4417157732075, 871.4775071927935, 847.8116444455821, 866.3602662499683, 877.1776448344959, 864.996071918476, 874.4689824940763, 871.6144945280856, 874.5174760515258, 867.6922714238127, 870.1060763013836, 843.7504721894292, 867.4470520721464, 812.5153067636354, 865.9085180341691, 866.4357226236808, 913.2302008271229, 907.2014363213611, 904.4107318477927, 96.05367660604156, 97.22177251142423, 27.903722834071495, 155.84806360333093, 192.54851936290163, 842.2385211715655, 848.1360379992253, 855.0627628855575, 853.6221870915085, 852.3114690038731, 859.6362834904812, 857.2850168058834, 854.5252032332726, 853.3229902345904, 858.0411641640461, 865.3953471562754, 860.4853628551726, 856.0664202431706, 862.884852201112, 857.2001225127835, 857.9122090453344, 844.0879894775578, 856.7039238047948, 859.745949888672, 108.57240024033601, 111.24942980421326, 341.12846763595195, 696.7690647443884, 863.1636815118446, 918.6348608887946, 931.889390632922, 929.738440436265, 928.3280330096864, 921.3696056462475, 909.5241924207845, 926.2052685145661, 921.5385148722237, 922.0366079180016, 922.9661993924635, 915.2985641915751, 911.5743831138209, 919.5093696725736, 907.802805744025, 900.2377772635149, 914.930071291499, 924.8456926620888, 920.9352389515103, 913.5926548331619, 911.3929091239825, 914.0684649458532, 907.2919022774558, 907.3252612650213, 917.5095713974675, 915.5090633774162, 913.2224845119165, 906.7294967979785, 900.6015861102445, 921.0506761330096, 926.3793682008326, 905.2943043222815, 912.0956030194561, 913.8644521918529, 18.373373261835198, 166.95118879356994, 872.7154095311279, 903.6629568002494, 907.4768952272611, 672.8928734442392, 898.7899584164986, 891.5550516319418, 902.4208635103598, 887.2687543435911, 912.3824800981436, 903.2185517838417, 55.80846124707606, 206.89563973583722, 822.1424486402423, 887.6238366578402, 905.4429847606672, 910.2741923435237, 912.0711957636659, 915.6515906086205, 919.926521387835, 916.3519381298977, 923.0487232424036, 925.0039596692168, 910.8038401224321, 924.6740374989492, 912.7980013116897, 923.9402900923255, 911.5936100756858, 922.8835550889934, 922.6327942925651, 904.1144324218881, 919.8270399707367, 923.8738640143577, 930.5491209023502, 927.4398092121353, 905.3969457736019, 924.9650097751138, 925.4633907024286, 923.2445349509123, 924.6173158177443, 928.6216840185185, 926.4701295713248, 923.504234639769, 922.890874409184, 923.1260107373786, 929.9733061427501, 909.1133732829437, 924.2023610811441, 909.9410262383695, 918.2215586565715, 916.3928744659809, 913.6558282907085, 921.142248404174, 926.1599399190209, 921.8036493668792, 910.2776934757514, 906.2131950388464, 911.3720885730976, 893.0078206235016, 899.4094917379605, 918.376568955266, 151.83420571866503, 59.60665365438589, 95.78432609942348, 35.49585588596668, 470.8857641188044, 314.10169773434876, 754.5436306817393, 883.8919005444725, 910.0639435982974, 907.014838279158, 914.7670959439267, 912.7406089913729, 894.6821279817117, 906.6429358851277, 916.5805463328726, 913.7495787402497, 910.1774998300508, 919.7547041762019, 913.1613631932994, 918.614014020904, 912.7578748648835, 916.7619509050493, 893.4169968006205, 910.8710978479446, 897.4485716285834, 904.9127551445554, 900.9867199110106, 913.3737454140567, 51.08934940353711, 129.89977220329467, 231.48489387324702, 92.52013309348183, 312.1554429722099, 796.1746412557227, 836.274183681509, 841.9711129754843, 844.1513729670721, 901.1198882812638, 922.1058048495513, 909.1578008966059, 921.3984125234688, 927.5302299351353, 915.1963470282926, 845.4982124251915, 884.9055801483028, 903.4582946542185, 916.8711306101968, 890.9211241505635, 874.9566799227533, 906.9701540980086, 70.38584816441218, 72.8922271059092, 833.6326672507022, 895.1421706073113, 395.7796396451928, 913.7880625702306, 915.219171036321, 915.7645442493899, 911.4018780331529, 884.5736691250728, 866.0919666743807, 900.6749218932125, 898.9180354724044, 903.8489583904407, 905.8428245326164, 893.1940670874012, 882.8399555823196, 895.5179844806423, 901.6437215875607, 897.9204066989877, 898.0751271796668, 898.1355282289026, 904.0023277304383, 900.2310551439022, 898.2404613681044, 894.7459327325568, 902.7610787327993, 894.913626404182, 897.2721303395075, 906.8478468727045, 51.606497174914026, 142.2141161528754, 137.6038440081617, 223.42356729031275, 895.7281076207552, 933.3339886706822, 929.0405970259536, 931.7130164011138, 938.903942943916, 926.4931696077648, 928.6470079309063, 934.1864168903337, 932.5181540012708, 930.3249032790476, 917.3443533289793, 921.3576431792355, 902.5396086902704, 925.9353369765281, 914.7098719306883, 925.678083947964, 926.8362621587953, 922.6618176085045, 929.2787225143828, 927.9083930905798, 927.1523544208304, 920.0047111727081, 921.3696640443324, 929.2685735141064, 926.4757276610873, 929.5669259871363, 924.5458597991066, 926.4810941692668, 916.3135601652062, 912.6179702153572, 55.12644760176818, 202.55788625538318, 390.95697998009973, 562.7852929259152, 817.5937398510478, 863.2158895329218, 826.699491952154, 927.808428341139, 915.1834584710635, 929.103296195541, 902.286569394376, 925.8364205470681, 911.8595903877484, 899.6304216680298, 904.2594349356134, 117.56899083268348, 153.41037289504533, 81.00082677493639, 25.39617890555545, 297.7757988648045, 918.5891003858344, 854.980982925527, 856.19000707155, 921.135170062805, 889.5838792804866, 925.9372651167483, 932.6459007085019, 927.0064383731356, 915.3277884896868, 924.3617666358755, 922.7130914900897, 921.8958144133201, 899.3157183367788, 917.5450046506602, 923.4241290856885, 895.0967364833781, 922.9542157158322, 915.8511003293515, 66.13650283538189, 865.7215836969192, 905.7726930972797, 914.6449766801627, 926.4186689225216, 931.2868417271751, 926.808930128533, 924.3948191360292, 919.8586518408933, 926.7637764915933, 925.0738336673743, 929.805929844831, 898.1637436358236, 938.1279457407048, 934.49039677586, 925.5754462467294, 934.132571726912, 933.6644144875495, 928.2277817787651, 935.8955382667896, 883.545313973769, 930.8310430723943, 923.8548539838314, 928.9826145636113, 923.3782804774236, 918.7304600366332, 920.9796715582042, 223.56394342516478, 34.384271448777575, 38.35080005488541, 64.1483555984991, 230.09333759460966, 794.0817383827915, 827.5968479846356, 828.3905091438485, 835.7168512347029, 932.4578985872965, 933.2629604047038, 925.4945549353739, 931.7240641048195, 927.2640003958345, 906.2601210929173, 927.3450443310423, 930.6313909016965, 917.7901945167636, 929.8792445032317, 921.1484759977651, 907.6888842832194, 926.5870176443016, 929.2968162504989, 928.3861504266524, 938.3276060131885, 938.6703313334804, 931.1484900618506, 940.833787081156, 944.856680886948, 941.0447859436338, 938.7540436586942, 927.9414058236191, 938.2905098729216, 938.005071400906, 943.5522037899747, 950.7294018458131, 900.8691924826036, 938.384740971315, 885.7530554781833, 946.3669538262229, 666.398794588314, 897.605214342293, 821.9750255744788, 974.500340001261, 972.3897232566162, 984.685455884466, 986.2246827312747, 982.8146391755644, 986.4654455182765, 982.6200192763042, 961.9345677840284, 970.0511495881491, 972.2423269223518, 981.8418609495803, 958.017474472167, 978.1622401934749, 981.0704449037274, 978.422165979691, 977.8284507997856, 973.1789244866085, 985.1928655697253, 970.859026871993, 983.0572988839543, 971.4104626417136, 968.3344289598003, 955.380267492031, 914.9842771478751, 948.4831981607337]
Elapsed: 0.09937191308837727~0.21671906813009
Time per graph: 0.002258452570190392~0.004925433366592954
Speed: 826.0000065951466~233.4902416335844
Total Time: 0.0477
best val loss: 0.2532271146774292 test_score: 0.9091

Testing...
Test loss: 0.5865 score: 0.8636 time: 0.04s
test Score 0.8636
Epoch Time List: [0.2221887189662084, 0.21879838698077947, 0.21888815390411764, 0.2182237949455157, 0.21839313895907253, 0.21927956503350288, 0.21919276693370193, 0.21806929889135063, 0.2185741850407794, 0.21920281706843525, 0.21973347919993103, 0.21854582394007593, 0.21906391391530633, 0.21956816804595292, 0.22182805591728538, 0.2190710991853848, 0.21875356405507773, 0.21894723293371499, 0.2195804949151352, 0.2198140249820426, 0.22195211588405073, 0.21972695703152567, 0.22043813299387693, 0.22369571204762906, 0.2205815240740776, 0.21986972098238766, 0.21976964897476137, 0.22459958191029727, 0.22259786201175302, 0.2203634651377797, 0.22115140897221863, 0.22090061893686652, 0.23516627203207463, 0.22600082110147923, 0.2273967940127477, 0.226615167921409, 0.22674889804329723, 0.2262208639876917, 0.22563136892858893, 2.840052238898352, 2.701293746009469, 0.7694677889812738, 0.23397356900386512, 0.2254609428346157, 0.21509795589372516, 0.2148421419551596, 0.2185568769928068, 0.21535623294766992, 0.2139667080482468, 0.21560618106741458, 0.21656710596289486, 0.21803855197504163, 0.21422174095641822, 0.21879350999370217, 0.21497068705502898, 0.21538893110118806, 0.21443196898326278, 0.21684108104091138, 0.21476066508330405, 0.2146580540575087, 0.21340137906372547, 0.2138677501352504, 0.2139333221130073, 0.21494484099093825, 0.21518847986590117, 0.21523487498052418, 0.21535328216850758, 0.2175669150892645, 0.21534826804418117, 0.21659081592224538, 0.2154666899004951, 0.21585781197063625, 0.21538291708566248, 0.21589584706816822, 0.2155170898186043, 0.2194649459561333, 0.2180603400338441, 0.21405468590091914, 0.2145777610130608, 0.21438175591174513, 0.21720388007815927, 0.21493565593846142, 0.2143475681077689, 0.2190624208888039, 0.22098834393545985, 0.21816996706184, 2.1226917629828677, 1.4243875689571723, 1.6215737418970093, 1.4583273930475116, 6.280375770991668, 1.2095164688071236, 0.2545517919352278, 0.26375989301595837, 0.2314099611248821, 0.23084459407255054, 0.2324997839750722, 0.2335036030272022, 0.23000160395167768, 0.23202980612404644, 0.231538139982149, 0.22888159495778382, 0.2345208500046283, 0.23270403104834259, 0.23095053213182837, 0.23522782896179706, 0.23258656612597406, 1.1670551371062174, 2.0769924808992073, 0.6807311279699206, 0.42454286909196526, 0.21589793195016682, 0.2159855788340792, 0.21384755696635693, 0.21442639688029885, 0.21373596787452698, 0.21375496906694025, 0.21378079487476498, 0.22107251500710845, 0.22828507411759347, 0.22764450800605118, 0.22746775404084474, 0.2274223860586062, 0.22814591496717185, 0.22538260801229626, 0.2269485059659928, 0.2325821239501238, 0.22872723219916224, 0.2281952981138602, 0.22960407403297722, 0.23081456893123686, 0.2304770271293819, 0.23032777290791273, 0.23278732399921864, 0.23152160993777215, 0.23053061601240188, 0.22859255992807448, 0.22949114593211561, 0.2282177358865738, 0.2293053090106696, 0.22867500002030283, 0.22815450804773718, 0.22819594899192452, 0.22800338990055025, 0.22837052715476602, 0.2298734940122813, 0.2307015439728275, 0.23195219412446022, 0.23036472604144365, 0.22928308392874897, 0.2285224871011451, 0.2293388230027631, 0.22879723797086626, 0.22879949503112584, 0.23341600294224918, 0.23019540205132216, 0.23133209999650717, 0.23266299511305988, 0.2270784379215911, 0.22626708494499326, 0.22658863989636302, 0.21834261692129076, 1.7823610419873148, 1.6005414259852841, 2.6642240419751033, 5.288845176110044, 1.4268652991158888, 0.26401400892063975, 0.23822427808772773, 0.23378223390318453, 0.23252401396166533, 0.23294799297582358, 0.2325448620831594, 0.2323283018777147, 0.23451051500160247, 0.23252127598971128, 0.23159180209040642, 0.2317353510297835, 0.23205947584938258, 0.23182883707340807, 0.23108636192046106, 0.23419197695329785, 0.23153201106470078, 0.23390811018180102, 0.23412775283213705, 0.2344903239281848, 2.038397151976824, 2.208601661026478, 0.8683553390437737, 1.264057244057767, 0.32568161305971444, 0.21932252310216427, 0.2141661550849676, 0.2137871041195467, 0.21318968711420894, 0.21389646001625806, 0.21512952412012964, 0.21298102603759617, 0.21482334204483777, 0.2147174949059263, 0.21479846886359155, 0.21847736393101513, 0.21826549794059247, 0.21655368304345757, 0.2162444598507136, 0.21688546903897077, 0.21641402004752308, 0.21598602691665292, 0.21580079512204975, 0.21641402097884566, 0.22091433499008417, 0.21710664010606706, 0.21630384190939367, 0.22085959813557565, 0.21629633090924472, 0.21651442209258676, 0.21671163605060428, 0.21599351905751973, 0.21893572888802737, 0.2161895700264722, 0.2141606518998742, 0.2167536470806226, 0.21857147687114775, 0.2158934709150344, 4.10794165788684, 4.176006245077588, 0.22477252292446792, 0.2215604929951951, 0.21970651601441205, 0.23608530720230192, 0.24627357407007366, 0.2226156231481582, 0.22072784788906574, 0.22353355900850147, 0.22357792500406504, 0.21974121301900595, 1.0525550181046128, 2.976795630995184, 0.24357300612609833, 0.22913385787978768, 0.22003029787447304, 0.2179747159825638, 0.21827314386609942, 0.21721899590920657, 0.21750387887004763, 0.21591482602525502, 0.21520867303479463, 0.21712993900291622, 0.21611050108913332, 0.21556056383997202, 0.21642728592269123, 0.21562537003774196, 0.21663325000554323, 0.21853555494453758, 0.21566255507059395, 0.21738920791540295, 0.21572855988051742, 0.21595398208592087, 0.21396752796135843, 0.21502648596651852, 0.2157071459805593, 0.2148455900605768, 0.21668947907164693, 0.21499823813792318, 0.21472871105652303, 0.21449619194027036, 0.21675983385648578, 0.2149906470440328, 0.21398007310926914, 0.21512489998713136, 0.21443588694091886, 0.22314213891513646, 0.21622480498626828, 0.21728968096431345, 0.21620940591674298, 0.21635560400318354, 0.21628824202343822, 0.21641660493332893, 0.21537356602493674, 0.21601286705117673, 0.22255424223840237, 0.2191697210073471, 0.2185317069524899, 0.22185741493012756, 0.21983225108124316, 0.2185474169673398, 3.0253526330925524, 2.298358035972342, 1.6246135389665142, 2.5704064291203395, 1.4726843710523099, 0.48100950196385384, 0.5444759350502864, 0.2241118341917172, 0.2182209170423448, 0.21758621092885733, 0.2174853840842843, 0.2171828020364046, 0.2178924580803141, 0.21877215208951384, 0.21729668998159468, 0.21757608896587044, 0.21740323095582426, 0.21785682218614966, 0.2166032741079107, 0.21625932201277465, 0.21698736492544413, 0.21640075708273798, 0.22165121987927705, 0.21996120002586395, 0.21939142991323024, 0.22075862402562052, 0.21863810601644218, 0.21836244990117848, 3.4626650229329243, 1.5071855259593576, 1.2903997969115153, 2.567687910166569, 0.8271925749722868, 0.25639134692028165, 0.24394063197541982, 0.242593496106565, 0.24065875890664756, 0.23280374996829778, 0.21596100507304072, 0.21658712101634592, 0.21558479708619416, 0.21576750802341849, 0.2163506749784574, 0.22671057796105742, 0.22430913604330271, 0.21999193797819316, 0.219236196950078, 0.22070480801630765, 0.22310033102985471, 0.2187192119890824, 3.140990707091987, 1.3764974189689383, 3.555018612067215, 0.23503474902827293, 0.2803349200403318, 0.22213376383297145, 0.21774428989738226, 0.2170537451747805, 0.21734541095793247, 0.22011782403569669, 0.22642140602692962, 0.22003083385061473, 0.27284736814908683, 0.2196957259438932, 0.21909562509972602, 0.22064504795707762, 0.22142786299809813, 0.22313017898704857, 0.22001091518905014, 0.2215674639446661, 0.22027519112452865, 0.22017256787512451, 0.2200928347883746, 0.2207542760297656, 0.22112789005041122, 0.22475142497569323, 0.22027167293708771, 0.22044740000274032, 0.22201101400423795, 0.22045114915817976, 1.0233185470569879, 2.2975324409781024, 1.267995783011429, 2.153455749968998, 0.21878530899994075, 0.21528458909597248, 0.21362447598949075, 0.21161206590477377, 0.2124596789944917, 0.21359396202024072, 0.213367359014228, 0.21286856301594526, 0.21238533407449722, 0.212987937964499, 0.21499363996554166, 0.21531070792116225, 0.21895337698515505, 0.21492436493281275, 0.21557514392770827, 0.21507600112818182, 0.21426670416258276, 0.21417504001874477, 0.21307403000537306, 0.21376624703407288, 0.21384747000411153, 0.21383531694300473, 0.21441422076895833, 0.21433459396939725, 0.21399263781495392, 0.21690178604330868, 0.21438165905419737, 0.2147409999743104, 0.21810443105641752, 0.21673116798046976, 0.9660004910547286, 2.2908820919692516, 0.7967712730169296, 0.4609520899830386, 0.2379025089321658, 0.2300738369813189, 0.23653154796920717, 0.2273145040962845, 0.21463683701585978, 0.21882974402979016, 0.21538078598678112, 0.21449841395951807, 0.21821478195488453, 0.21734658407513052, 0.21572340396232903, 0.946317947935313, 1.6775145541178063, 1.5083342341240495, 3.700482068932615, 1.3314040121622384, 1.8511265519773588, 0.22642861492931843, 0.23189689591526985, 0.22564219997730106, 0.21798181696794927, 0.21462334494572133, 0.21382560895290226, 0.2148390719667077, 0.21417882898822427, 0.21509014803450555, 0.2143852470908314, 0.21448653389234096, 0.21695299295242876, 0.2167965481057763, 0.21708734286949039, 0.21910850086715072, 0.2163205649703741, 0.21726734493859112, 4.643219177960418, 0.8370220757788047, 0.2217893599299714, 0.2185458909953013, 0.2171713940333575, 0.2161017549224198, 0.21389612695202231, 0.2153927399776876, 0.21494673285633326, 0.2158771260874346, 0.21431846893392503, 0.21430210291873664, 0.2158266268670559, 0.21440512989647686, 0.21198886597994715, 0.21204868296626955, 0.2115721518639475, 0.21179674984887242, 0.21141006401740015, 0.21156360302120447, 0.22319916100241244, 0.21167018893174827, 0.22108301508706063, 0.21349541493691504, 0.2146123159909621, 0.2157871728995815, 0.21502453391440213, 0.3614606020273641, 3.3829637290909886, 1.9076089169830084, 2.4795968459220603, 0.8961751179303974, 0.2483565459260717, 0.24211702193133533, 0.2427444829372689, 0.23950547398999333, 0.3164437370141968, 0.2127577739302069, 0.21295459708198905, 0.21264777798205614, 0.21472692117094994, 0.21538550511468202, 0.2137823790544644, 0.2130701350979507, 0.2165115368552506, 0.21424856677185744, 0.21302012004889548, 0.21395528002176434, 0.2170127488207072, 0.21348089794628322, 0.21249318402260542, 0.21171259810216725, 0.21259177091997117, 0.21241604292299598, 0.21111658425070345, 0.21139692910946906, 0.21036885608918965, 0.21098275505937636, 0.21134853491093963, 0.2113493619253859, 0.211144485976547, 0.21504153194837272, 0.210543729015626, 0.21250680019147694, 0.21227776899468154, 0.21638541296124458, 0.2100989300524816, 0.22965735103935003, 0.2185701810522005, 0.21740383910946548, 0.2190712089650333, 0.20444848202168941, 0.20256917795632035, 0.2020920329960063, 0.20208180614281446, 0.2024381220107898, 0.20286930992733687, 0.20574866607785225, 0.20605979883112013, 0.20736436592414975, 0.20408575993496925, 0.20438064192421734, 0.20234323292970657, 0.20304498705081642, 0.20260675088502467, 0.20368340890854597, 0.2030074851354584, 0.20244413893669844, 0.20351631590165198, 0.20410884206648916, 0.20299411879386753, 0.20498376106843352, 0.20558217400684953, 0.213721111882478, 0.21132853697054088]
Total Epoch List: [533]
Total Time List: [0.04771926999092102]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bfa5ed0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.8020;  Loss pred: 2.8020; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.7911;  Loss pred: 2.7911; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.7772;  Loss pred: 2.7772; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.7818;  Loss pred: 2.7818; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.7727;  Loss pred: 2.7727; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.7821;  Loss pred: 2.7821; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.6710;  Loss pred: 2.6710; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 1.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 1.01s
Epoch 8/1000, LR 0.000180
Train loss: 2.7426;  Loss pred: 2.7426; Loss self: 0.0000; time: 3.76s
Val loss: 0.6931 score: 0.5000 time: 0.12s
Test loss: 0.6930 score: 0.5116 time: 0.81s
Epoch 9/1000, LR 0.000210
Train loss: 2.6149;  Loss pred: 2.6149; Loss self: 0.0000; time: 2.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.24s
Test loss: 0.6931 score: 0.5116 time: 0.94s
Epoch 10/1000, LR 0.000240
Train loss: 2.6364;  Loss pred: 2.6364; Loss self: 0.0000; time: 1.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 1 of 2
Epoch 11/1000, LR 0.000270
Train loss: 2.5812;  Loss pred: 2.5812; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 2.6149,   Val_Loss: 0.6931,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.5000,   Val_Loss: 0.6931,   Test_Precision: 1.0000,   Test_Recall: 0.0455,   Test_accuracy: 0.0870,   Test_Score: 0.5116,   Test_loss: 0.6931


[0.048719901009462774, 0.04843972297385335, 0.04878209496382624, 0.04846269800327718, 0.04854442900978029, 0.04944563889876008, 0.04849230800755322, 0.04869470407720655, 0.048650266020558774, 0.048511328059248626, 0.048447447014041245, 0.04857634101063013, 0.04865861497819424, 0.04891877109184861, 0.04955034202430397, 0.048693803953938186, 0.04887965496163815, 0.04850545199587941, 0.049117146991193295, 0.04886791994795203, 0.04892664903309196, 0.04875802400056273, 0.0489187550265342, 0.049039085977710783, 0.049202810972929, 0.0491185620194301, 0.04893514991272241, 0.04922198795247823, 0.04884139203932136, 0.049199708038941026, 0.04912001302000135, 0.04919311206322163, 0.05057511106133461, 0.050049636978656054, 0.05010462598875165, 0.05077693203929812, 0.05064067791681737, 0.050038640038110316, 0.050069955992512405, 2.041475855978206, 0.10327968094497919, 0.05219653807580471, 0.05108784302137792, 0.04788946302141994, 0.047622455982491374, 0.04804917797446251, 0.04835395701229572, 0.048112966935150325, 0.047881923033855855, 0.047630815068259835, 0.050002154894173145, 0.04803178203292191, 0.04779623099602759, 0.04842487093992531, 0.04796124703716487, 0.04785138298757374, 0.047611702932044864, 0.04775148199405521, 0.04822769504971802, 0.04841428296640515, 0.04773385694716126, 0.04757533501833677, 0.04787307302467525, 0.04833253694232553, 0.04828893393278122, 0.04805391398258507, 0.04827512102201581, 0.047752336016856134, 0.04820447601377964, 0.04798825900070369, 0.04761714208871126, 0.04809394804760814, 0.04764544602949172, 0.047969033010303974, 0.04769421799574047, 0.04900119197554886, 0.047721152077429, 0.04761639004573226, 0.04758261900860816, 0.0477078229887411, 0.048326091025955975, 0.04751961899455637, 0.04773753497283906, 0.048833750071935356, 0.04824724793434143, 0.05048157298006117, 0.44252417609095573, 0.4006156630348414, 0.5000023739412427, 0.36204768903553486, 0.653738570981659, 0.05455024994444102, 0.05920404801145196, 0.05615403805859387, 0.05106244399212301, 0.05070537095889449, 0.05137146497145295, 0.051393480971455574, 0.050792037043720484, 0.05187812994699925, 0.050632495898753405, 0.050776202930137515, 0.050769782974384725, 0.05060841899830848, 0.05241295194718987, 0.0513810709817335, 0.05127360799815506, 0.6136464800219983, 0.33236312202643603, 0.15049156197346747, 0.047740194015204906, 0.047472766949795187, 0.04805603891145438, 0.047379914904013276, 0.04797000391408801, 0.04711998603306711, 0.04730746801942587, 0.047435994958505034, 0.050683238077908754, 0.04986973700579256, 0.050138568971306086, 0.05024028196930885, 0.05027289595454931, 0.049925948958843946, 0.050074666971340775, 0.0503633520565927, 0.051685575977899134, 0.05107265501283109, 0.050600153976120055, 0.05087169399484992, 0.050950036966241896, 0.05084042204543948, 0.050955082988366485, 0.05107141600456089, 0.05144475609995425, 0.051093677058815956, 0.050542755983769894, 0.05076140398159623, 0.05093701707664877, 0.05035517492797226, 0.0509150440338999, 0.050380176049657166, 0.050330887897871435, 0.05026034195907414, 0.050488968030549586, 0.05189831997267902, 0.050787186017259955, 0.05016087705735117, 0.0508672830183059, 0.05031625006813556, 0.05048103292938322, 0.05031345994211733, 0.050709221977740526, 0.0505685469834134, 0.05214811896439642, 0.05072355701122433, 0.054152825963683426, 0.05081368191167712, 0.050782763049937785, 0.04818062297999859, 0.048500805045478046, 0.04865046206396073, 0.458077208022587, 0.4525735219940543, 1.576850524987094, 0.28232625406235456, 0.22851383197121322, 0.05224173306487501, 0.05187846999615431, 0.05145821091718972, 0.051545051974244416, 0.05162431998178363, 0.05118443793617189, 0.051324820960871875, 0.051490581943653524, 0.05156312498729676, 0.05127959104720503, 0.05084381392225623, 0.051133931963704526, 0.051397881004959345, 0.05099173996131867, 0.051329904003068805, 0.051287299022078514, 0.05212726700119674, 0.051359634031541646, 0.05117790901567787, 0.40525953099131584, 0.3955076450947672, 0.1289836650248617, 0.0631486129714176, 0.05097526800818741, 0.04789715900551528, 0.04721590399276465, 0.04732513800263405, 0.04739703901577741, 0.047754994011484087, 0.04837694298475981, 0.04750566801521927, 0.04774624097626656, 0.04772044799756259, 0.04767238500062376, 0.04807174589950591, 0.048268140060827136, 0.04785160592291504, 0.048468675930052996, 0.04887597600463778, 0.04809110704809427, 0.047575504053384066, 0.04777751804795116, 0.04816150804981589, 0.048277751076966524, 0.04813643801026046, 0.04849596903659403, 0.04849418601952493, 0.0479559029918164, 0.04806069296319038, 0.04818103008437902, 0.048526049009524286, 0.04885623196605593, 0.04777152999304235, 0.04749674000777304, 0.048602978931739926, 0.04824055708013475, 0.04814718407578766, 2.3947698320262134, 0.26355008501559496, 0.05041735200211406, 0.04869071999564767, 0.04848608293104917, 0.06538930896203965, 0.0489547080360353, 0.0493519720621407, 0.048757737968117, 0.04959038598462939, 0.04822538897860795, 0.04871467698831111, 0.7884109150618315, 0.21266760409343988, 0.05351870600134134, 0.04957054799888283, 0.04859499796293676, 0.04833708389196545, 0.04824184800963849, 0.048053211998194456, 0.047829907038249075, 0.048016485990956426, 0.04766812291927636, 0.04756736394483596, 0.04830897506326437, 0.047584335901774466, 0.04820343595929444, 0.04762212501373142, 0.04826712200883776, 0.04767665406689048, 0.04768961202353239, 0.04866640595719218, 0.047835079953074455, 0.047625549021176994, 0.04728390905074775, 0.04744243191089481, 0.04859746899455786, 0.047569366986863315, 0.04754374991171062, 0.04765801294706762, 0.047587255015969276, 0.04738205100875348, 0.04749208700377494, 0.04764461098238826, 0.047676275949925184, 0.047664131969213486, 0.04731318599078804, 0.04839880403596908, 0.04760862106923014, 0.04835478204768151, 0.04791871807537973, 0.048014341038651764, 0.048158177989535034, 0.04776678094640374, 0.0475079930620268, 0.04773250792641193, 0.04833689797669649, 0.04855369601864368, 0.04827885399572551, 0.04927168495487422, 0.048920986941084266, 0.04791063000448048, 0.2897897729417309, 0.7381726250750944, 0.4593653449555859, 1.2395813229959458, 0.09344092209357768, 0.14008201903197914, 0.0583133939653635, 0.04977984295692295, 0.0483482510317117, 0.04851078300271183, 0.04809967498295009, 0.048206466948613524, 0.04917947796639055, 0.04853068199008703, 0.04800450999755412, 0.04815323697403073, 0.048342218971811235, 0.047838842030614614, 0.04818425502162427, 0.047898245975375175, 0.048205555067397654, 0.047995011089369655, 0.049249119008891284, 0.04830540798138827, 0.04902787902392447, 0.04862347198650241, 0.04883534798864275, 0.048173050978221, 0.8612362559651956, 0.3387226879131049, 0.19007719797082245, 0.47557216498535126, 0.14095541497226804, 0.05526425701100379, 0.052614322979934514, 0.05225832492578775, 0.0521233530016616, 0.048828131053596735, 0.04771686694584787, 0.04839643894229084, 0.047753500984981656, 0.04743780696298927, 0.04807711497414857, 0.05204032291658223, 0.04972281900700182, 0.048701749998144805, 0.047989295911975205, 0.04938708804547787, 0.050288203987292945, 0.04851317300926894, 0.6251256630057469, 0.6036308910697699, 0.0527810410130769, 0.049154203035868704, 0.11117297504097223, 0.04815120901912451, 0.048075916012749076, 0.04804728494491428, 0.04827727598603815, 0.04974147607572377, 0.050802918965928257, 0.04885225393809378, 0.04894773301202804, 0.0486807000124827, 0.04857354809064418, 0.049261410953477025, 0.049839157960377634, 0.0491335749393329, 0.04879976308438927, 0.049002116080373526, 0.0489936739904806, 0.048990379087626934, 0.04867244104389101, 0.048876340966671705, 0.0489846559939906, 0.049175970954820514, 0.04873936309013516, 0.0491667561000213, 0.04903751995880157, 0.0485197160160169, 0.8526058230781928, 0.30939263408072293, 0.31975850905291736, 0.19693535706028342, 0.04912204900756478, 0.04714282404165715, 0.04736068600323051, 0.04722484201192856, 0.04686315392609686, 0.04749090597033501, 0.04738075891509652, 0.047099807066842914, 0.04718406801111996, 0.04729530494660139, 0.047964540077373385, 0.04775561403948814, 0.04875132301822305, 0.04751951701473445, 0.048102684086188674, 0.04753272305242717, 0.0474733259761706, 0.04768811189569533, 0.0473485499387607, 0.04741847398690879, 0.047457140986807644, 0.04782584204804152, 0.04775499098468572, 0.04734906705562025, 0.04749180004000664, 0.04733386996667832, 0.047590932925231755, 0.04749152495060116, 0.048018497065640986, 0.048212944995611906, 0.7981649809516966, 0.2172218560008332, 0.11254435207229108, 0.07818256900645792, 0.053816459025256336, 0.05097218498121947, 0.05322369304485619, 0.04742358298972249, 0.04807779204566032, 0.047357489936985075, 0.048764994950033724, 0.04752459400333464, 0.048253042972646654, 0.04890897299628705, 0.048658602056093514, 0.37424834293778986, 0.28681241802405566, 0.5432043320033699, 1.7325441029388458, 0.1477621759986505, 0.04789954505395144, 0.05146313295699656, 0.051390461972914636, 0.04776714800391346, 0.04946132795885205, 0.047519418061710894, 0.04717760509811342, 0.04746461100876331, 0.04807021107990295, 0.047600410995073617, 0.04768546193372458, 0.04772773594595492, 0.04892608802765608, 0.04795405105687678, 0.04764874407555908, 0.0491566980490461, 0.047673003980889916, 0.0480427440488711, 0.6652906959643587, 0.05082465405575931, 0.04857730900403112, 0.048106097034178674, 0.04749472509138286, 0.047246453003026545, 0.04747472598683089, 0.04759870900306851, 0.04783343605231494, 0.04747703904286027, 0.047563771018758416, 0.0473217029357329, 0.04898884007707238, 0.04690191801637411, 0.047084485995583236, 0.04753799398895353, 0.04710252198856324, 0.047126140096224844, 0.04740215803030878, 0.04701379395555705, 0.04979936999734491, 0.04726958810351789, 0.04762652900535613, 0.04736364202108234, 0.04765110998414457, 0.04789217503275722, 0.04777521302457899, 0.19681170105468482, 1.2796548580517992, 1.1473033140646294, 0.6859100220026448, 0.19122674502432346, 0.05540991295129061, 0.05316598305944353, 0.053115046001039445, 0.05264941102359444, 0.04718711704481393, 0.047146411961875856, 0.047542148968204856, 0.04722428205423057, 0.04745142697356641, 0.0485511819133535, 0.047447280026972294, 0.047279729042202234, 0.04794124001637101, 0.04731797194108367, 0.047766458010300994, 0.04847475909627974, 0.04748609592206776, 0.04734762804582715, 0.047394071938470006, 0.046891938080079854, 0.04687481699511409, 0.04725347296334803, 0.04676702793221921, 0.046567909070290625, 0.04675654193852097, 0.04687063698656857, 0.0474167870124802, 0.04689379199407995, 0.04690806195139885, 0.04663229000288993, 0.04628025589045137, 0.048841719049960375, 0.04688908299431205, 0.049675244954414666, 0.046493593021295965, 0.06602652999572456, 0.0490193230798468, 0.05352960689924657, 0.04515134391840547, 0.04524934699293226, 0.04468431998975575, 0.04461457999423146, 0.044769377913326025, 0.04460369108710438, 0.04477824503555894, 0.045741156907752156, 0.04535843292251229, 0.045256206998601556, 0.044813734013587236, 0.04592818103265017, 0.04498231294564903, 0.0448489710688591, 0.044970363029278815, 0.04499766801018268, 0.045212651952169836, 0.044661305961199105, 0.04532068897970021, 0.04475832695607096, 0.04529496200848371, 0.04543884703889489, 0.04605495999567211, 0.0480882580159232, 0.04638985707424581, 0.045239776954986155, 0.04535305697936565, 0.04533586895558983, 0.04651268501766026, 0.04510007903445512, 0.0455114709911868, 1.0141871079104021, 0.814690904924646, 0.9420117940753698, 0.09992014605086297, 0.049771548015996814]
[0.0011072704774877902, 0.0011009027948603034, 0.0011086839764505964, 0.001101424954619936, 0.0011032824774950066, 0.0011237645204263654, 0.0011020979092625732, 0.0011066978199365126, 0.0011056878641036085, 0.0011025301831647414, 0.00110107834122821, 0.0011040077502415938, 0.001105877613140778, 0.0011117902520874684, 0.0011261441369159993, 0.0011066773625895041, 0.0011109012491281399, 0.0011023966362699866, 0.001116298795254393, 0.0011106345442716372, 0.0011119692962066356, 0.0011081369091036984, 0.0011117898869666863, 0.0011145246813116087, 0.0011182457039302046, 0.0011163309549870478, 0.0011121624980164183, 0.0011186815443745052, 0.0011100316372573036, 0.0011181751827032051, 0.001116363932272758, 0.001118025274164128, 0.0011494343423030593, 0.0011374917495149102, 0.0011387414997443557, 0.0011540211827113208, 0.0011509244981094855, 0.0011372418190479618, 0.001137953545284373, 0.04639717854495922, 0.0023472654760222545, 0.0011862849562682889, 0.0011610873413949528, 0.0010883968868504533, 0.001082328545056622, 0.0010920267721468751, 0.0010989535684612665, 0.0010934765212534164, 0.001088225523496724, 0.0010825185242786326, 0.0011364126112312078, 0.0010916314098391342, 0.0010862779771824453, 0.0011005652486346662, 0.0010900283417537469, 0.0010875314315357668, 0.0010820841575464742, 0.0010852609544103457, 0.0010960839784026823, 0.0011003246128728445, 0.0010848603851627558, 0.0010812576140531085, 0.0010880243869244375, 0.0010984667486892167, 0.0010974757711995733, 0.0010921344086951153, 0.0010971618414094503, 0.0010852803640194577, 0.0010955562730404463, 0.0010906422500159931, 0.0010822077747434378, 0.0010930442738092759, 0.0010828510461248118, 0.0010902052956887267, 0.0010839594999031926, 0.0011136634539897468, 0.0010845716381233863, 0.0010821906828575513, 0.0010814231592865492, 0.0010842687042895705, 0.0010983202505899085, 0.001079991340785372, 0.001084943976655433, 0.001109857956180349, 0.0010965283621441233, 0.0011473084768195722, 0.010057367638430813, 0.009104901432610031, 0.011363690316846425, 0.00822835656898943, 0.014857694795037705, 0.001239778407828205, 0.0013455465457148173, 0.0012762281376953151, 0.0011605100907300684, 0.0011523947945203292, 0.001167533294805749, 0.001168033658442172, 0.0011543644782663746, 0.0011790484078863467, 0.0011507385431534865, 0.00115400461204858, 0.0011538587039632891, 0.0011501913408706473, 0.0011912034533452243, 0.0011677516132212159, 0.0011653092726853422, 0.01394651090959087, 0.007553707318782637, 0.0034202627721242607, 0.001085004409436475, 0.0010789265215862542, 0.001092182702533054, 0.0010768162478184836, 0.0010902273616838183, 0.001070908773478798, 0.0010751697277142243, 0.001078090794511478, 0.0011518917744979262, 0.0011334031137680126, 0.0011395129311660473, 0.0011418245902115648, 0.001142565817148848, 0.0011346806581555443, 0.0011380606129850175, 0.0011446216376498342, 0.0011746721813158895, 0.0011607421593825247, 0.001150003499457274, 0.0011561748635193164, 0.0011579553855964068, 0.0011554641373963518, 0.0011580700679174201, 0.0011607140001036566, 0.0011691990022716875, 0.0011612199331549082, 0.001148698999631134, 0.0011536682723090053, 0.0011576594790147449, 0.0011444357938175513, 0.0011571600916795433, 0.0011450040011285719, 0.0011438838158607143, 0.0011422804990698669, 0.0011474765461488541, 0.0011795072721063414, 0.001154254227664999, 0.0011400199331216174, 0.0011560746140524068, 0.0011435511379121717, 0.0011472962029405278, 0.0011434877259572122, 0.001152482317675921, 0.001149285158713941, 0.0011851845219181005, 0.001152808113891462, 0.0012307460446291689, 0.001154856407083571, 0.0011541537056804043, 0.0010950141586363316, 0.0011022910237608646, 0.0011056923196354712, 0.010410845636876977, 0.010285761863501235, 0.03583751193152487, 0.006416505774144422, 0.005193496181163937, 0.0011873121151107955, 0.0011790561362762344, 0.0011695047935724936, 0.0011714784539601003, 0.0011732799995859916, 0.001163282680367543, 0.001166473203656179, 0.0011702404987193984, 0.0011718892042567445, 0.0011654452510728415, 0.0011555412255058234, 0.001162134817356921, 0.0011681336592036214, 0.0011589031809390608, 0.0011665887273424728, 0.0011656204323199663, 0.0011847106136635623, 0.0011672644098077647, 0.0011631342958108607, 0.00921044388616627, 0.008988810115790164, 0.0029314469323832204, 0.0014351957493504, 0.0011585288183678958, 0.0010885717955798927, 0.0010730887271082875, 0.001075571318241683, 0.0010772054321767594, 0.0010853407729882747, 0.0010994759769263592, 0.0010796742730731653, 0.0010851418403696946, 0.0010845556363082405, 0.001083463295468722, 0.0010925396795342253, 0.0010970031832006168, 0.0010875364982480692, 0.0011015608165921135, 0.0011108176364690405, 0.0010929797056385062, 0.0010812614557587287, 0.001085852682907981, 0.0010945797284049067, 0.0010972216153856027, 0.001094009954778647, 0.0011021811144680462, 0.0011021405913528395, 0.0010899068861776454, 0.001092288476436145, 0.001095023411008614, 0.001102864750216461, 0.0011103689083194529, 0.0010857165907509625, 0.0010794713638130236, 0.0011046131575395439, 0.0010963762972757897, 0.0010942541835406287, 0.05442658709150485, 0.00598977465944534, 0.0011458489091389558, 0.0011066072726283562, 0.0011019564302511174, 0.0014861206582281739, 0.001112607000818984, 0.001121635728685016, 0.0011081304083662953, 0.0011270542269233954, 0.0010960315676956352, 0.0011071517497343434, 0.017918429887768896, 0.00483335463848727, 0.0012163342273032124, 0.0011266033636109735, 0.0011044317718849263, 0.0010985700884537603, 0.001096405636582693, 0.0010921184545044196, 0.0010870433417783881, 0.0010912837725217369, 0.0010833664299835536, 0.0010810764532917265, 0.0010979312514378266, 0.0010814621795857834, 0.00109553263543851, 0.0010823210230393504, 0.0010969800456554037, 0.0010835603197020564, 0.0010838548187166452, 0.0011060546808452768, 0.0010871609080244195, 0.0010823988413903862, 0.0010746342966079035, 0.001078237088883973, 0.0011044879316944969, 0.0010811219769741663, 0.001080539770720696, 0.0010831366578879004, 0.001081528523090211, 0.001076864795653488, 0.0010793656137221578, 0.0010828320677815514, 0.0010835517261346633, 0.0010832757265730338, 0.0010752996816088191, 0.0010999728189992973, 0.0010820141152097758, 0.001098972319265489, 0.0010890617744404483, 0.0010912350236057218, 0.0010945040452167052, 0.0010856086578728123, 0.0010797271150460635, 0.001084829725600271, 0.0010985658631067383, 0.0011034930913328108, 0.0010972466817210343, 0.0011198110217016867, 0.0011118406122973697, 0.0010888779546472836, 0.006586131203221157, 0.01677665056988851, 0.010440121476263315, 0.028172302795362404, 0.0021236573203085836, 0.003183682250726799, 0.001325304408303716, 0.0011313600672027942, 0.0010988238870843568, 0.001102517795516178, 0.0010931744314306839, 0.0010956015215593982, 0.001117715408327058, 0.0011029700452292507, 0.0010910115908535029, 0.0010943917494097893, 0.0010986867948138918, 0.0010872464097866957, 0.0010950967050369152, 0.0010885964994403448, 0.0010955807969863104, 0.0010907957065765831, 0.0011192981592929837, 0.001097850181395188, 0.0011142699778164651, 0.0011050789087841456, 0.0011098942724691535, 0.001094842067686841, 0.019573551271936263, 0.00769824290711602, 0.004319936317518692, 0.01080845829512162, 0.0032035321584606372, 0.0012560058411591772, 0.0011957800677257844, 0.0011876892028588124, 0.0011846216591286727, 0.0011097302512181077, 0.0010844742487692697, 0.0010999190668702465, 0.001085306840567765, 0.0010781319764315742, 0.001092661703957922, 0.0011827346117405052, 0.0011300640683409504, 0.001106857954503291, 0.0010906658161812547, 0.0011224338192154062, 0.0011429137269839305, 0.0011025721138470215, 0.014207401431948792, 0.013718883887949314, 0.0011995691139335659, 0.001117140978087925, 0.0025266585236584597, 0.001094345659525557, 0.0010926344548352063, 0.0010919837487480518, 0.0010972108178645033, 0.0011304880926300857, 0.0011546117946801876, 0.0011102784985930405, 0.0011124484775460917, 0.0011063795457382432, 0.0011039442747873677, 0.0011195775216699324, 0.001132708135463128, 0.0011166721577121114, 0.0011090855246452106, 0.0011136844563721256, 0.001113492590692741, 0.0011134177065369759, 0.001106191841906614, 0.0011108259310607207, 0.001113287636227059, 0.0011176357035186481, 0.0011077127975030717, 0.0011174262750004841, 0.001114489089972763, 0.0011027208185458387, 0.019377405069958928, 0.007031650774561885, 0.0072672388421117584, 0.004475803569551896, 0.0011164102047173815, 0.0010714278191285716, 0.001076379227346148, 0.0010732918639074671, 0.001065071680138565, 0.0010793387720530684, 0.0010768354298885572, 0.0010704501606100662, 0.0010723651820709083, 0.0010748932942409408, 0.0010901031835766678, 0.0010853548645338212, 0.0011079846140505238, 0.0010799890230621465, 0.0010932428201406517, 0.0010802891602824357, 0.00107893922673115, 0.0010838207249021666, 0.0010761034076991068, 0.0010776925906115634, 0.00107857138606381, 0.0010869509556373073, 0.0010853407041974026, 0.0010761151603550059, 0.0010793590918183327, 0.0010757697719699618, 0.0010816121119370853, 0.00107935283978639, 0.001091329478764568, 0.0010957487499002707, 0.01814011320344765, 0.004936860363655301, 0.0025578261834611608, 0.0017768765683285892, 0.0012231013414830986, 0.0011584587495731698, 0.0012096293873830953, 0.0010778087043118749, 0.0010926770919468254, 0.0010763065894769336, 0.0011082953397734937, 0.0010801044091666963, 0.0010966600675601512, 0.001111567568097433, 0.0011058773194566709, 0.008505644157677043, 0.006518464046001265, 0.012345553000076588, 0.03937600233951922, 0.003358231272696602, 0.001088626023953442, 0.0011696166581135583, 0.001167965044838969, 0.0010856170000889424, 0.0011241210899739103, 0.001079986774129793, 0.0010722182976843958, 0.0010787411592900753, 0.0010925047972705215, 0.0010818275226153094, 0.0010837604984937404, 0.0010847212714989755, 0.0011119565460830927, 0.0010898647967471995, 0.0010829260017172519, 0.001117197682932866, 0.0010834773632020435, 0.0010918805465652522, 0.015120243090099062, 0.0011551057739945297, 0.0011040297500916165, 0.0010933203871404244, 0.0010794255702587013, 0.0010737830227960578, 0.0010789710451552476, 0.0010817888409788297, 0.0010871235466435212, 0.0010790236146104608, 0.0010809947958808732, 0.001075493248539384, 0.0011133827290243723, 0.0010659526821903207, 0.0010701019544450735, 0.0010804089542943984, 0.0010705118633764373, 0.0010710486385505646, 0.0010773217734161087, 0.0010684953171717511, 0.0011318038635760206, 0.0010743088205344975, 0.001082421113758094, 0.0010764464095700532, 0.001082979772366922, 0.0010884585234717551, 0.001085800296013159, 0.0044729932057882915, 0.02908306495572271, 0.026075075319650667, 0.015588864136423746, 0.004346062386916442, 0.001259316203438423, 0.001208317796805535, 0.00120716013638726, 0.0011965775232635099, 0.0010724344782912258, 0.001071509362769906, 0.0010805033856410194, 0.0010732791375961494, 0.0010784415221265094, 0.0011034359525762159, 0.0010783472733402793, 0.0010745392964136872, 0.0010895736367357049, 0.0010754084532064471, 0.0010856013184159317, 0.0011016990703699942, 0.0010792294527742672, 0.0010760824555869806, 0.001077137998601591, 0.0010657258654563602, 0.0010653367498889565, 0.0010739425673488188, 0.0010628869984595274, 0.0010583615697793323, 0.001062648680420931, 0.0010652417496947403, 0.001077654250283641, 0.0010657679998654533, 0.0010660923170772467, 0.001059824772792953, 0.0010518239975102585, 0.0011100390693172812, 0.0010656609771434557, 0.0011289828398730606, 0.0010566725686658174, 0.0015006029544482853, 0.0011140755245419728, 0.0012165819749828768, 0.001026166907236488, 0.0010283942498393696, 0.0010155527270399034, 0.0010139677271416242, 0.0010174858616665006, 0.001013720251979645, 0.0010176873871717942, 0.0010395717479034581, 0.001030873475511643, 0.0010285501590591264, 0.0010184939548542554, 0.0010438222961965948, 0.001022325294219296, 0.001019294797019525, 0.001022053705210882, 0.0010226742729586972, 0.0010275602716402236, 0.0010150296809363433, 0.0010300156586295502, 0.0010172347035470673, 0.0010294309547382661, 0.001032701069065793, 0.0010467036362652752, 0.0010929149549073454, 0.0010543149335055866, 0.0010520878361624686, 0.0010547222553340847, 0.0010543225338509263, 0.001081690349247913, 0.0010488390473129099, 0.0010584063021206232, 0.023585746695590746, 0.01894630011452665, 0.021907251025008602, 0.002323724326764255, 0.0011574778608371352]
[903.1217036227961, 908.3454094844884, 901.9702830029678, 907.9147842124802, 906.3861888484729, 889.8661435053955, 907.360400192676, 903.589021307882, 904.4143763038489, 907.0046473735212, 908.2005907813416, 905.7907426656801, 904.259194794551, 899.4502318422257, 887.9857979268522, 903.605724490568, 900.1700203189279, 907.1145240278937, 895.8175035673224, 900.3861847785474, 899.3054065533943, 902.4155695787053, 899.4505272289493, 897.2434767646129, 894.257850922551, 895.7916964791167, 899.1491816920061, 893.909446373441, 900.8752241249872, 894.3142501003153, 895.7652348765356, 894.4341627228708, 869.993146364806, 879.1272555835728, 878.1624277542332, 866.5352204805678, 868.866725525962, 879.320460477919, 878.7704947569732, 21.55303471807003, 426.0276522682171, 842.9677833441573, 861.2616504789215, 918.782488338191, 923.9338688491163, 915.728465185927, 909.9565520317484, 914.5143773674561, 918.9271694223504, 923.7717208270212, 879.9620754970184, 916.0601197315883, 920.5746788623752, 908.6240013852655, 917.4073385936916, 919.5136535850199, 924.1425382914833, 921.4373703726671, 912.3388533215265, 908.8227131347119, 921.7775979993733, 924.8489786365404, 919.0970460016438, 910.3598276354605, 911.1818467818845, 915.6382145259962, 911.4425623072774, 921.4208910003566, 912.7783068821709, 916.8909419980165, 924.036976390299, 914.8760246599937, 923.4880490522588, 917.2584319252088, 922.5436929048632, 897.937340421343, 922.0230041514649, 924.0515704307074, 924.7074019200155, 922.2806081590406, 910.4812548643251, 925.9333498663035, 921.7065779587157, 901.0162016061654, 911.969115002758, 871.605170016766, 99.4295958893697, 109.83095285561348, 87.99958218832501, 121.5309511219706, 67.30519194229163, 806.5957542781864, 743.1924248066448, 783.558966037104, 861.6900516314403, 867.7581717264161, 856.5066233647558, 856.1397120470983, 866.2775222447946, 848.1415973349875, 869.0071310721876, 866.5476632929636, 866.6572402367697, 869.4205602722077, 839.4871566160484, 856.3464941328773, 858.1412878450687, 71.70252161867313, 132.38532521818186, 292.37519647618154, 921.6552405712117, 926.8471763302146, 915.5977270842519, 928.6635505601766, 917.2398667884602, 933.7863548839422, 930.0857103984571, 927.565660602024, 868.1371133463201, 882.2986171932134, 877.5679263040167, 875.7912630123978, 875.2231031166275, 881.3052313992278, 878.6878208332871, 873.6511412218496, 851.3013382846792, 861.5177728462675, 869.5625713068997, 864.9210699461751, 863.5911300545905, 865.4530829951462, 863.5056096375234, 861.5386735325807, 855.2863952646697, 861.1633089031712, 870.5500747551067, 866.8002960665232, 863.8118705261024, 873.7930125937868, 864.1846596598094, 873.3593935168358, 874.2146589840078, 875.4417157732075, 871.4775071927935, 847.8116444455821, 866.3602662499683, 877.1776448344959, 864.996071918476, 874.4689824940763, 871.6144945280856, 874.5174760515258, 867.6922714238127, 870.1060763013836, 843.7504721894292, 867.4470520721464, 812.5153067636354, 865.9085180341691, 866.4357226236808, 913.2302008271229, 907.2014363213611, 904.4107318477927, 96.05367660604156, 97.22177251142423, 27.903722834071495, 155.84806360333093, 192.54851936290163, 842.2385211715655, 848.1360379992253, 855.0627628855575, 853.6221870915085, 852.3114690038731, 859.6362834904812, 857.2850168058834, 854.5252032332726, 853.3229902345904, 858.0411641640461, 865.3953471562754, 860.4853628551726, 856.0664202431706, 862.884852201112, 857.2001225127835, 857.9122090453344, 844.0879894775578, 856.7039238047948, 859.745949888672, 108.57240024033601, 111.24942980421326, 341.12846763595195, 696.7690647443884, 863.1636815118446, 918.6348608887946, 931.889390632922, 929.738440436265, 928.3280330096864, 921.3696056462475, 909.5241924207845, 926.2052685145661, 921.5385148722237, 922.0366079180016, 922.9661993924635, 915.2985641915751, 911.5743831138209, 919.5093696725736, 907.802805744025, 900.2377772635149, 914.930071291499, 924.8456926620888, 920.9352389515103, 913.5926548331619, 911.3929091239825, 914.0684649458532, 907.2919022774558, 907.3252612650213, 917.5095713974675, 915.5090633774162, 913.2224845119165, 906.7294967979785, 900.6015861102445, 921.0506761330096, 926.3793682008326, 905.2943043222815, 912.0956030194561, 913.8644521918529, 18.373373261835198, 166.95118879356994, 872.7154095311279, 903.6629568002494, 907.4768952272611, 672.8928734442392, 898.7899584164986, 891.5550516319418, 902.4208635103598, 887.2687543435911, 912.3824800981436, 903.2185517838417, 55.80846124707606, 206.89563973583722, 822.1424486402423, 887.6238366578402, 905.4429847606672, 910.2741923435237, 912.0711957636659, 915.6515906086205, 919.926521387835, 916.3519381298977, 923.0487232424036, 925.0039596692168, 910.8038401224321, 924.6740374989492, 912.7980013116897, 923.9402900923255, 911.5936100756858, 922.8835550889934, 922.6327942925651, 904.1144324218881, 919.8270399707367, 923.8738640143577, 930.5491209023502, 927.4398092121353, 905.3969457736019, 924.9650097751138, 925.4633907024286, 923.2445349509123, 924.6173158177443, 928.6216840185185, 926.4701295713248, 923.504234639769, 922.890874409184, 923.1260107373786, 929.9733061427501, 909.1133732829437, 924.2023610811441, 909.9410262383695, 918.2215586565715, 916.3928744659809, 913.6558282907085, 921.142248404174, 926.1599399190209, 921.8036493668792, 910.2776934757514, 906.2131950388464, 911.3720885730976, 893.0078206235016, 899.4094917379605, 918.376568955266, 151.83420571866503, 59.60665365438589, 95.78432609942348, 35.49585588596668, 470.8857641188044, 314.10169773434876, 754.5436306817393, 883.8919005444725, 910.0639435982974, 907.014838279158, 914.7670959439267, 912.7406089913729, 894.6821279817117, 906.6429358851277, 916.5805463328726, 913.7495787402497, 910.1774998300508, 919.7547041762019, 913.1613631932994, 918.614014020904, 912.7578748648835, 916.7619509050493, 893.4169968006205, 910.8710978479446, 897.4485716285834, 904.9127551445554, 900.9867199110106, 913.3737454140567, 51.08934940353711, 129.89977220329467, 231.48489387324702, 92.52013309348183, 312.1554429722099, 796.1746412557227, 836.274183681509, 841.9711129754843, 844.1513729670721, 901.1198882812638, 922.1058048495513, 909.1578008966059, 921.3984125234688, 927.5302299351353, 915.1963470282926, 845.4982124251915, 884.9055801483028, 903.4582946542185, 916.8711306101968, 890.9211241505635, 874.9566799227533, 906.9701540980086, 70.38584816441218, 72.8922271059092, 833.6326672507022, 895.1421706073113, 395.7796396451928, 913.7880625702306, 915.219171036321, 915.7645442493899, 911.4018780331529, 884.5736691250728, 866.0919666743807, 900.6749218932125, 898.9180354724044, 903.8489583904407, 905.8428245326164, 893.1940670874012, 882.8399555823196, 895.5179844806423, 901.6437215875607, 897.9204066989877, 898.0751271796668, 898.1355282289026, 904.0023277304383, 900.2310551439022, 898.2404613681044, 894.7459327325568, 902.7610787327993, 894.913626404182, 897.2721303395075, 906.8478468727045, 51.606497174914026, 142.2141161528754, 137.6038440081617, 223.42356729031275, 895.7281076207552, 933.3339886706822, 929.0405970259536, 931.7130164011138, 938.903942943916, 926.4931696077648, 928.6470079309063, 934.1864168903337, 932.5181540012708, 930.3249032790476, 917.3443533289793, 921.3576431792355, 902.5396086902704, 925.9353369765281, 914.7098719306883, 925.678083947964, 926.8362621587953, 922.6618176085045, 929.2787225143828, 927.9083930905798, 927.1523544208304, 920.0047111727081, 921.3696640443324, 929.2685735141064, 926.4757276610873, 929.5669259871363, 924.5458597991066, 926.4810941692668, 916.3135601652062, 912.6179702153572, 55.12644760176818, 202.55788625538318, 390.95697998009973, 562.7852929259152, 817.5937398510478, 863.2158895329218, 826.699491952154, 927.808428341139, 915.1834584710635, 929.103296195541, 902.286569394376, 925.8364205470681, 911.8595903877484, 899.6304216680298, 904.2594349356134, 117.56899083268348, 153.41037289504533, 81.00082677493639, 25.39617890555545, 297.7757988648045, 918.5891003858344, 854.980982925527, 856.19000707155, 921.135170062805, 889.5838792804866, 925.9372651167483, 932.6459007085019, 927.0064383731356, 915.3277884896868, 924.3617666358755, 922.7130914900897, 921.8958144133201, 899.3157183367788, 917.5450046506602, 923.4241290856885, 895.0967364833781, 922.9542157158322, 915.8511003293515, 66.13650283538189, 865.7215836969192, 905.7726930972797, 914.6449766801627, 926.4186689225216, 931.2868417271751, 926.808930128533, 924.3948191360292, 919.8586518408933, 926.7637764915933, 925.0738336673743, 929.805929844831, 898.1637436358236, 938.1279457407048, 934.49039677586, 925.5754462467294, 934.132571726912, 933.6644144875495, 928.2277817787651, 935.8955382667896, 883.545313973769, 930.8310430723943, 923.8548539838314, 928.9826145636113, 923.3782804774236, 918.7304600366332, 920.9796715582042, 223.56394342516478, 34.384271448777575, 38.35080005488541, 64.1483555984991, 230.09333759460966, 794.0817383827915, 827.5968479846356, 828.3905091438485, 835.7168512347029, 932.4578985872965, 933.2629604047038, 925.4945549353739, 931.7240641048195, 927.2640003958345, 906.2601210929173, 927.3450443310423, 930.6313909016965, 917.7901945167636, 929.8792445032317, 921.1484759977651, 907.6888842832194, 926.5870176443016, 929.2968162504989, 928.3861504266524, 938.3276060131885, 938.6703313334804, 931.1484900618506, 940.833787081156, 944.856680886948, 941.0447859436338, 938.7540436586942, 927.9414058236191, 938.2905098729216, 938.005071400906, 943.5522037899747, 950.7294018458131, 900.8691924826036, 938.384740971315, 885.7530554781833, 946.3669538262229, 666.398794588314, 897.605214342293, 821.9750255744788, 974.500340001261, 972.3897232566162, 984.685455884466, 986.2246827312747, 982.8146391755644, 986.4654455182765, 982.6200192763042, 961.9345677840284, 970.0511495881491, 972.2423269223518, 981.8418609495803, 958.017474472167, 978.1622401934749, 981.0704449037274, 978.422165979691, 977.8284507997856, 973.1789244866085, 985.1928655697253, 970.859026871993, 983.0572988839543, 971.4104626417136, 968.3344289598003, 955.380267492031, 914.9842771478751, 948.4831981607337, 950.4909814826287, 948.1169046568081, 948.4763607844816, 924.4789885527671, 953.4351362699226, 944.8167475915437, 42.39848807444988, 52.780753706802756, 45.64698687473078, 430.3436463965079, 863.9474099977681]
Elapsed: 0.10323320609377869~0.22321127963189902
Time per graph: 0.0023493121107688152~0.005083006601982354
Speed: 822.358338087503~239.1583600787235
Total Time: 0.0511
best val loss: 0.6930761933326721 test_score: 0.5116

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
test Score 0.5116
Epoch Time List: [0.2221887189662084, 0.21879838698077947, 0.21888815390411764, 0.2182237949455157, 0.21839313895907253, 0.21927956503350288, 0.21919276693370193, 0.21806929889135063, 0.2185741850407794, 0.21920281706843525, 0.21973347919993103, 0.21854582394007593, 0.21906391391530633, 0.21956816804595292, 0.22182805591728538, 0.2190710991853848, 0.21875356405507773, 0.21894723293371499, 0.2195804949151352, 0.2198140249820426, 0.22195211588405073, 0.21972695703152567, 0.22043813299387693, 0.22369571204762906, 0.2205815240740776, 0.21986972098238766, 0.21976964897476137, 0.22459958191029727, 0.22259786201175302, 0.2203634651377797, 0.22115140897221863, 0.22090061893686652, 0.23516627203207463, 0.22600082110147923, 0.2273967940127477, 0.226615167921409, 0.22674889804329723, 0.2262208639876917, 0.22563136892858893, 2.840052238898352, 2.701293746009469, 0.7694677889812738, 0.23397356900386512, 0.2254609428346157, 0.21509795589372516, 0.2148421419551596, 0.2185568769928068, 0.21535623294766992, 0.2139667080482468, 0.21560618106741458, 0.21656710596289486, 0.21803855197504163, 0.21422174095641822, 0.21879350999370217, 0.21497068705502898, 0.21538893110118806, 0.21443196898326278, 0.21684108104091138, 0.21476066508330405, 0.2146580540575087, 0.21340137906372547, 0.2138677501352504, 0.2139333221130073, 0.21494484099093825, 0.21518847986590117, 0.21523487498052418, 0.21535328216850758, 0.2175669150892645, 0.21534826804418117, 0.21659081592224538, 0.2154666899004951, 0.21585781197063625, 0.21538291708566248, 0.21589584706816822, 0.2155170898186043, 0.2194649459561333, 0.2180603400338441, 0.21405468590091914, 0.2145777610130608, 0.21438175591174513, 0.21720388007815927, 0.21493565593846142, 0.2143475681077689, 0.2190624208888039, 0.22098834393545985, 0.21816996706184, 2.1226917629828677, 1.4243875689571723, 1.6215737418970093, 1.4583273930475116, 6.280375770991668, 1.2095164688071236, 0.2545517919352278, 0.26375989301595837, 0.2314099611248821, 0.23084459407255054, 0.2324997839750722, 0.2335036030272022, 0.23000160395167768, 0.23202980612404644, 0.231538139982149, 0.22888159495778382, 0.2345208500046283, 0.23270403104834259, 0.23095053213182837, 0.23522782896179706, 0.23258656612597406, 1.1670551371062174, 2.0769924808992073, 0.6807311279699206, 0.42454286909196526, 0.21589793195016682, 0.2159855788340792, 0.21384755696635693, 0.21442639688029885, 0.21373596787452698, 0.21375496906694025, 0.21378079487476498, 0.22107251500710845, 0.22828507411759347, 0.22764450800605118, 0.22746775404084474, 0.2274223860586062, 0.22814591496717185, 0.22538260801229626, 0.2269485059659928, 0.2325821239501238, 0.22872723219916224, 0.2281952981138602, 0.22960407403297722, 0.23081456893123686, 0.2304770271293819, 0.23032777290791273, 0.23278732399921864, 0.23152160993777215, 0.23053061601240188, 0.22859255992807448, 0.22949114593211561, 0.2282177358865738, 0.2293053090106696, 0.22867500002030283, 0.22815450804773718, 0.22819594899192452, 0.22800338990055025, 0.22837052715476602, 0.2298734940122813, 0.2307015439728275, 0.23195219412446022, 0.23036472604144365, 0.22928308392874897, 0.2285224871011451, 0.2293388230027631, 0.22879723797086626, 0.22879949503112584, 0.23341600294224918, 0.23019540205132216, 0.23133209999650717, 0.23266299511305988, 0.2270784379215911, 0.22626708494499326, 0.22658863989636302, 0.21834261692129076, 1.7823610419873148, 1.6005414259852841, 2.6642240419751033, 5.288845176110044, 1.4268652991158888, 0.26401400892063975, 0.23822427808772773, 0.23378223390318453, 0.23252401396166533, 0.23294799297582358, 0.2325448620831594, 0.2323283018777147, 0.23451051500160247, 0.23252127598971128, 0.23159180209040642, 0.2317353510297835, 0.23205947584938258, 0.23182883707340807, 0.23108636192046106, 0.23419197695329785, 0.23153201106470078, 0.23390811018180102, 0.23412775283213705, 0.2344903239281848, 2.038397151976824, 2.208601661026478, 0.8683553390437737, 1.264057244057767, 0.32568161305971444, 0.21932252310216427, 0.2141661550849676, 0.2137871041195467, 0.21318968711420894, 0.21389646001625806, 0.21512952412012964, 0.21298102603759617, 0.21482334204483777, 0.2147174949059263, 0.21479846886359155, 0.21847736393101513, 0.21826549794059247, 0.21655368304345757, 0.2162444598507136, 0.21688546903897077, 0.21641402004752308, 0.21598602691665292, 0.21580079512204975, 0.21641402097884566, 0.22091433499008417, 0.21710664010606706, 0.21630384190939367, 0.22085959813557565, 0.21629633090924472, 0.21651442209258676, 0.21671163605060428, 0.21599351905751973, 0.21893572888802737, 0.2161895700264722, 0.2141606518998742, 0.2167536470806226, 0.21857147687114775, 0.2158934709150344, 4.10794165788684, 4.176006245077588, 0.22477252292446792, 0.2215604929951951, 0.21970651601441205, 0.23608530720230192, 0.24627357407007366, 0.2226156231481582, 0.22072784788906574, 0.22353355900850147, 0.22357792500406504, 0.21974121301900595, 1.0525550181046128, 2.976795630995184, 0.24357300612609833, 0.22913385787978768, 0.22003029787447304, 0.2179747159825638, 0.21827314386609942, 0.21721899590920657, 0.21750387887004763, 0.21591482602525502, 0.21520867303479463, 0.21712993900291622, 0.21611050108913332, 0.21556056383997202, 0.21642728592269123, 0.21562537003774196, 0.21663325000554323, 0.21853555494453758, 0.21566255507059395, 0.21738920791540295, 0.21572855988051742, 0.21595398208592087, 0.21396752796135843, 0.21502648596651852, 0.2157071459805593, 0.2148455900605768, 0.21668947907164693, 0.21499823813792318, 0.21472871105652303, 0.21449619194027036, 0.21675983385648578, 0.2149906470440328, 0.21398007310926914, 0.21512489998713136, 0.21443588694091886, 0.22314213891513646, 0.21622480498626828, 0.21728968096431345, 0.21620940591674298, 0.21635560400318354, 0.21628824202343822, 0.21641660493332893, 0.21537356602493674, 0.21601286705117673, 0.22255424223840237, 0.2191697210073471, 0.2185317069524899, 0.22185741493012756, 0.21983225108124316, 0.2185474169673398, 3.0253526330925524, 2.298358035972342, 1.6246135389665142, 2.5704064291203395, 1.4726843710523099, 0.48100950196385384, 0.5444759350502864, 0.2241118341917172, 0.2182209170423448, 0.21758621092885733, 0.2174853840842843, 0.2171828020364046, 0.2178924580803141, 0.21877215208951384, 0.21729668998159468, 0.21757608896587044, 0.21740323095582426, 0.21785682218614966, 0.2166032741079107, 0.21625932201277465, 0.21698736492544413, 0.21640075708273798, 0.22165121987927705, 0.21996120002586395, 0.21939142991323024, 0.22075862402562052, 0.21863810601644218, 0.21836244990117848, 3.4626650229329243, 1.5071855259593576, 1.2903997969115153, 2.567687910166569, 0.8271925749722868, 0.25639134692028165, 0.24394063197541982, 0.242593496106565, 0.24065875890664756, 0.23280374996829778, 0.21596100507304072, 0.21658712101634592, 0.21558479708619416, 0.21576750802341849, 0.2163506749784574, 0.22671057796105742, 0.22430913604330271, 0.21999193797819316, 0.219236196950078, 0.22070480801630765, 0.22310033102985471, 0.2187192119890824, 3.140990707091987, 1.3764974189689383, 3.555018612067215, 0.23503474902827293, 0.2803349200403318, 0.22213376383297145, 0.21774428989738226, 0.2170537451747805, 0.21734541095793247, 0.22011782403569669, 0.22642140602692962, 0.22003083385061473, 0.27284736814908683, 0.2196957259438932, 0.21909562509972602, 0.22064504795707762, 0.22142786299809813, 0.22313017898704857, 0.22001091518905014, 0.2215674639446661, 0.22027519112452865, 0.22017256787512451, 0.2200928347883746, 0.2207542760297656, 0.22112789005041122, 0.22475142497569323, 0.22027167293708771, 0.22044740000274032, 0.22201101400423795, 0.22045114915817976, 1.0233185470569879, 2.2975324409781024, 1.267995783011429, 2.153455749968998, 0.21878530899994075, 0.21528458909597248, 0.21362447598949075, 0.21161206590477377, 0.2124596789944917, 0.21359396202024072, 0.213367359014228, 0.21286856301594526, 0.21238533407449722, 0.212987937964499, 0.21499363996554166, 0.21531070792116225, 0.21895337698515505, 0.21492436493281275, 0.21557514392770827, 0.21507600112818182, 0.21426670416258276, 0.21417504001874477, 0.21307403000537306, 0.21376624703407288, 0.21384747000411153, 0.21383531694300473, 0.21441422076895833, 0.21433459396939725, 0.21399263781495392, 0.21690178604330868, 0.21438165905419737, 0.2147409999743104, 0.21810443105641752, 0.21673116798046976, 0.9660004910547286, 2.2908820919692516, 0.7967712730169296, 0.4609520899830386, 0.2379025089321658, 0.2300738369813189, 0.23653154796920717, 0.2273145040962845, 0.21463683701585978, 0.21882974402979016, 0.21538078598678112, 0.21449841395951807, 0.21821478195488453, 0.21734658407513052, 0.21572340396232903, 0.946317947935313, 1.6775145541178063, 1.5083342341240495, 3.700482068932615, 1.3314040121622384, 1.8511265519773588, 0.22642861492931843, 0.23189689591526985, 0.22564219997730106, 0.21798181696794927, 0.21462334494572133, 0.21382560895290226, 0.2148390719667077, 0.21417882898822427, 0.21509014803450555, 0.2143852470908314, 0.21448653389234096, 0.21695299295242876, 0.2167965481057763, 0.21708734286949039, 0.21910850086715072, 0.2163205649703741, 0.21726734493859112, 4.643219177960418, 0.8370220757788047, 0.2217893599299714, 0.2185458909953013, 0.2171713940333575, 0.2161017549224198, 0.21389612695202231, 0.2153927399776876, 0.21494673285633326, 0.2158771260874346, 0.21431846893392503, 0.21430210291873664, 0.2158266268670559, 0.21440512989647686, 0.21198886597994715, 0.21204868296626955, 0.2115721518639475, 0.21179674984887242, 0.21141006401740015, 0.21156360302120447, 0.22319916100241244, 0.21167018893174827, 0.22108301508706063, 0.21349541493691504, 0.2146123159909621, 0.2157871728995815, 0.21502453391440213, 0.3614606020273641, 3.3829637290909886, 1.9076089169830084, 2.4795968459220603, 0.8961751179303974, 0.2483565459260717, 0.24211702193133533, 0.2427444829372689, 0.23950547398999333, 0.3164437370141968, 0.2127577739302069, 0.21295459708198905, 0.21264777798205614, 0.21472692117094994, 0.21538550511468202, 0.2137823790544644, 0.2130701350979507, 0.2165115368552506, 0.21424856677185744, 0.21302012004889548, 0.21395528002176434, 0.2170127488207072, 0.21348089794628322, 0.21249318402260542, 0.21171259810216725, 0.21259177091997117, 0.21241604292299598, 0.21111658425070345, 0.21139692910946906, 0.21036885608918965, 0.21098275505937636, 0.21134853491093963, 0.2113493619253859, 0.211144485976547, 0.21504153194837272, 0.210543729015626, 0.21250680019147694, 0.21227776899468154, 0.21638541296124458, 0.2100989300524816, 0.22965735103935003, 0.2185701810522005, 0.21740383910946548, 0.2190712089650333, 0.20444848202168941, 0.20256917795632035, 0.2020920329960063, 0.20208180614281446, 0.2024381220107898, 0.20286930992733687, 0.20574866607785225, 0.20605979883112013, 0.20736436592414975, 0.20408575993496925, 0.20438064192421734, 0.20234323292970657, 0.20304498705081642, 0.20260675088502467, 0.20368340890854597, 0.2030074851354584, 0.20244413893669844, 0.20351631590165198, 0.20410884206648916, 0.20299411879386753, 0.20498376106843352, 0.20558217400684953, 0.213721111882478, 0.21132853697054088, 0.21433241409249604, 0.21706022892612964, 0.21647272701375186, 0.2137633899692446, 0.21662371489219368, 0.2157467819051817, 2.7935083229094744, 4.693727066740394, 4.046426284941845, 1.6970701969694346, 0.23252573504578322]
Total Epoch List: [533, 11]
Total Time List: [0.04771926999092102, 0.05113759997766465]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bfa7ca0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.7134;  Loss pred: 2.7134; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.7056;  Loss pred: 2.7056; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 2.6775;  Loss pred: 2.6775; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 2.7134,   Val_Loss: 0.6931,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6931,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6935


[0.048719901009462774, 0.04843972297385335, 0.04878209496382624, 0.04846269800327718, 0.04854442900978029, 0.04944563889876008, 0.04849230800755322, 0.04869470407720655, 0.048650266020558774, 0.048511328059248626, 0.048447447014041245, 0.04857634101063013, 0.04865861497819424, 0.04891877109184861, 0.04955034202430397, 0.048693803953938186, 0.04887965496163815, 0.04850545199587941, 0.049117146991193295, 0.04886791994795203, 0.04892664903309196, 0.04875802400056273, 0.0489187550265342, 0.049039085977710783, 0.049202810972929, 0.0491185620194301, 0.04893514991272241, 0.04922198795247823, 0.04884139203932136, 0.049199708038941026, 0.04912001302000135, 0.04919311206322163, 0.05057511106133461, 0.050049636978656054, 0.05010462598875165, 0.05077693203929812, 0.05064067791681737, 0.050038640038110316, 0.050069955992512405, 2.041475855978206, 0.10327968094497919, 0.05219653807580471, 0.05108784302137792, 0.04788946302141994, 0.047622455982491374, 0.04804917797446251, 0.04835395701229572, 0.048112966935150325, 0.047881923033855855, 0.047630815068259835, 0.050002154894173145, 0.04803178203292191, 0.04779623099602759, 0.04842487093992531, 0.04796124703716487, 0.04785138298757374, 0.047611702932044864, 0.04775148199405521, 0.04822769504971802, 0.04841428296640515, 0.04773385694716126, 0.04757533501833677, 0.04787307302467525, 0.04833253694232553, 0.04828893393278122, 0.04805391398258507, 0.04827512102201581, 0.047752336016856134, 0.04820447601377964, 0.04798825900070369, 0.04761714208871126, 0.04809394804760814, 0.04764544602949172, 0.047969033010303974, 0.04769421799574047, 0.04900119197554886, 0.047721152077429, 0.04761639004573226, 0.04758261900860816, 0.0477078229887411, 0.048326091025955975, 0.04751961899455637, 0.04773753497283906, 0.048833750071935356, 0.04824724793434143, 0.05048157298006117, 0.44252417609095573, 0.4006156630348414, 0.5000023739412427, 0.36204768903553486, 0.653738570981659, 0.05455024994444102, 0.05920404801145196, 0.05615403805859387, 0.05106244399212301, 0.05070537095889449, 0.05137146497145295, 0.051393480971455574, 0.050792037043720484, 0.05187812994699925, 0.050632495898753405, 0.050776202930137515, 0.050769782974384725, 0.05060841899830848, 0.05241295194718987, 0.0513810709817335, 0.05127360799815506, 0.6136464800219983, 0.33236312202643603, 0.15049156197346747, 0.047740194015204906, 0.047472766949795187, 0.04805603891145438, 0.047379914904013276, 0.04797000391408801, 0.04711998603306711, 0.04730746801942587, 0.047435994958505034, 0.050683238077908754, 0.04986973700579256, 0.050138568971306086, 0.05024028196930885, 0.05027289595454931, 0.049925948958843946, 0.050074666971340775, 0.0503633520565927, 0.051685575977899134, 0.05107265501283109, 0.050600153976120055, 0.05087169399484992, 0.050950036966241896, 0.05084042204543948, 0.050955082988366485, 0.05107141600456089, 0.05144475609995425, 0.051093677058815956, 0.050542755983769894, 0.05076140398159623, 0.05093701707664877, 0.05035517492797226, 0.0509150440338999, 0.050380176049657166, 0.050330887897871435, 0.05026034195907414, 0.050488968030549586, 0.05189831997267902, 0.050787186017259955, 0.05016087705735117, 0.0508672830183059, 0.05031625006813556, 0.05048103292938322, 0.05031345994211733, 0.050709221977740526, 0.0505685469834134, 0.05214811896439642, 0.05072355701122433, 0.054152825963683426, 0.05081368191167712, 0.050782763049937785, 0.04818062297999859, 0.048500805045478046, 0.04865046206396073, 0.458077208022587, 0.4525735219940543, 1.576850524987094, 0.28232625406235456, 0.22851383197121322, 0.05224173306487501, 0.05187846999615431, 0.05145821091718972, 0.051545051974244416, 0.05162431998178363, 0.05118443793617189, 0.051324820960871875, 0.051490581943653524, 0.05156312498729676, 0.05127959104720503, 0.05084381392225623, 0.051133931963704526, 0.051397881004959345, 0.05099173996131867, 0.051329904003068805, 0.051287299022078514, 0.05212726700119674, 0.051359634031541646, 0.05117790901567787, 0.40525953099131584, 0.3955076450947672, 0.1289836650248617, 0.0631486129714176, 0.05097526800818741, 0.04789715900551528, 0.04721590399276465, 0.04732513800263405, 0.04739703901577741, 0.047754994011484087, 0.04837694298475981, 0.04750566801521927, 0.04774624097626656, 0.04772044799756259, 0.04767238500062376, 0.04807174589950591, 0.048268140060827136, 0.04785160592291504, 0.048468675930052996, 0.04887597600463778, 0.04809110704809427, 0.047575504053384066, 0.04777751804795116, 0.04816150804981589, 0.048277751076966524, 0.04813643801026046, 0.04849596903659403, 0.04849418601952493, 0.0479559029918164, 0.04806069296319038, 0.04818103008437902, 0.048526049009524286, 0.04885623196605593, 0.04777152999304235, 0.04749674000777304, 0.048602978931739926, 0.04824055708013475, 0.04814718407578766, 2.3947698320262134, 0.26355008501559496, 0.05041735200211406, 0.04869071999564767, 0.04848608293104917, 0.06538930896203965, 0.0489547080360353, 0.0493519720621407, 0.048757737968117, 0.04959038598462939, 0.04822538897860795, 0.04871467698831111, 0.7884109150618315, 0.21266760409343988, 0.05351870600134134, 0.04957054799888283, 0.04859499796293676, 0.04833708389196545, 0.04824184800963849, 0.048053211998194456, 0.047829907038249075, 0.048016485990956426, 0.04766812291927636, 0.04756736394483596, 0.04830897506326437, 0.047584335901774466, 0.04820343595929444, 0.04762212501373142, 0.04826712200883776, 0.04767665406689048, 0.04768961202353239, 0.04866640595719218, 0.047835079953074455, 0.047625549021176994, 0.04728390905074775, 0.04744243191089481, 0.04859746899455786, 0.047569366986863315, 0.04754374991171062, 0.04765801294706762, 0.047587255015969276, 0.04738205100875348, 0.04749208700377494, 0.04764461098238826, 0.047676275949925184, 0.047664131969213486, 0.04731318599078804, 0.04839880403596908, 0.04760862106923014, 0.04835478204768151, 0.04791871807537973, 0.048014341038651764, 0.048158177989535034, 0.04776678094640374, 0.0475079930620268, 0.04773250792641193, 0.04833689797669649, 0.04855369601864368, 0.04827885399572551, 0.04927168495487422, 0.048920986941084266, 0.04791063000448048, 0.2897897729417309, 0.7381726250750944, 0.4593653449555859, 1.2395813229959458, 0.09344092209357768, 0.14008201903197914, 0.0583133939653635, 0.04977984295692295, 0.0483482510317117, 0.04851078300271183, 0.04809967498295009, 0.048206466948613524, 0.04917947796639055, 0.04853068199008703, 0.04800450999755412, 0.04815323697403073, 0.048342218971811235, 0.047838842030614614, 0.04818425502162427, 0.047898245975375175, 0.048205555067397654, 0.047995011089369655, 0.049249119008891284, 0.04830540798138827, 0.04902787902392447, 0.04862347198650241, 0.04883534798864275, 0.048173050978221, 0.8612362559651956, 0.3387226879131049, 0.19007719797082245, 0.47557216498535126, 0.14095541497226804, 0.05526425701100379, 0.052614322979934514, 0.05225832492578775, 0.0521233530016616, 0.048828131053596735, 0.04771686694584787, 0.04839643894229084, 0.047753500984981656, 0.04743780696298927, 0.04807711497414857, 0.05204032291658223, 0.04972281900700182, 0.048701749998144805, 0.047989295911975205, 0.04938708804547787, 0.050288203987292945, 0.04851317300926894, 0.6251256630057469, 0.6036308910697699, 0.0527810410130769, 0.049154203035868704, 0.11117297504097223, 0.04815120901912451, 0.048075916012749076, 0.04804728494491428, 0.04827727598603815, 0.04974147607572377, 0.050802918965928257, 0.04885225393809378, 0.04894773301202804, 0.0486807000124827, 0.04857354809064418, 0.049261410953477025, 0.049839157960377634, 0.0491335749393329, 0.04879976308438927, 0.049002116080373526, 0.0489936739904806, 0.048990379087626934, 0.04867244104389101, 0.048876340966671705, 0.0489846559939906, 0.049175970954820514, 0.04873936309013516, 0.0491667561000213, 0.04903751995880157, 0.0485197160160169, 0.8526058230781928, 0.30939263408072293, 0.31975850905291736, 0.19693535706028342, 0.04912204900756478, 0.04714282404165715, 0.04736068600323051, 0.04722484201192856, 0.04686315392609686, 0.04749090597033501, 0.04738075891509652, 0.047099807066842914, 0.04718406801111996, 0.04729530494660139, 0.047964540077373385, 0.04775561403948814, 0.04875132301822305, 0.04751951701473445, 0.048102684086188674, 0.04753272305242717, 0.0474733259761706, 0.04768811189569533, 0.0473485499387607, 0.04741847398690879, 0.047457140986807644, 0.04782584204804152, 0.04775499098468572, 0.04734906705562025, 0.04749180004000664, 0.04733386996667832, 0.047590932925231755, 0.04749152495060116, 0.048018497065640986, 0.048212944995611906, 0.7981649809516966, 0.2172218560008332, 0.11254435207229108, 0.07818256900645792, 0.053816459025256336, 0.05097218498121947, 0.05322369304485619, 0.04742358298972249, 0.04807779204566032, 0.047357489936985075, 0.048764994950033724, 0.04752459400333464, 0.048253042972646654, 0.04890897299628705, 0.048658602056093514, 0.37424834293778986, 0.28681241802405566, 0.5432043320033699, 1.7325441029388458, 0.1477621759986505, 0.04789954505395144, 0.05146313295699656, 0.051390461972914636, 0.04776714800391346, 0.04946132795885205, 0.047519418061710894, 0.04717760509811342, 0.04746461100876331, 0.04807021107990295, 0.047600410995073617, 0.04768546193372458, 0.04772773594595492, 0.04892608802765608, 0.04795405105687678, 0.04764874407555908, 0.0491566980490461, 0.047673003980889916, 0.0480427440488711, 0.6652906959643587, 0.05082465405575931, 0.04857730900403112, 0.048106097034178674, 0.04749472509138286, 0.047246453003026545, 0.04747472598683089, 0.04759870900306851, 0.04783343605231494, 0.04747703904286027, 0.047563771018758416, 0.0473217029357329, 0.04898884007707238, 0.04690191801637411, 0.047084485995583236, 0.04753799398895353, 0.04710252198856324, 0.047126140096224844, 0.04740215803030878, 0.04701379395555705, 0.04979936999734491, 0.04726958810351789, 0.04762652900535613, 0.04736364202108234, 0.04765110998414457, 0.04789217503275722, 0.04777521302457899, 0.19681170105468482, 1.2796548580517992, 1.1473033140646294, 0.6859100220026448, 0.19122674502432346, 0.05540991295129061, 0.05316598305944353, 0.053115046001039445, 0.05264941102359444, 0.04718711704481393, 0.047146411961875856, 0.047542148968204856, 0.04722428205423057, 0.04745142697356641, 0.0485511819133535, 0.047447280026972294, 0.047279729042202234, 0.04794124001637101, 0.04731797194108367, 0.047766458010300994, 0.04847475909627974, 0.04748609592206776, 0.04734762804582715, 0.047394071938470006, 0.046891938080079854, 0.04687481699511409, 0.04725347296334803, 0.04676702793221921, 0.046567909070290625, 0.04675654193852097, 0.04687063698656857, 0.0474167870124802, 0.04689379199407995, 0.04690806195139885, 0.04663229000288993, 0.04628025589045137, 0.048841719049960375, 0.04688908299431205, 0.049675244954414666, 0.046493593021295965, 0.06602652999572456, 0.0490193230798468, 0.05352960689924657, 0.04515134391840547, 0.04524934699293226, 0.04468431998975575, 0.04461457999423146, 0.044769377913326025, 0.04460369108710438, 0.04477824503555894, 0.045741156907752156, 0.04535843292251229, 0.045256206998601556, 0.044813734013587236, 0.04592818103265017, 0.04498231294564903, 0.0448489710688591, 0.044970363029278815, 0.04499766801018268, 0.045212651952169836, 0.044661305961199105, 0.04532068897970021, 0.04475832695607096, 0.04529496200848371, 0.04543884703889489, 0.04605495999567211, 0.0480882580159232, 0.04638985707424581, 0.045239776954986155, 0.04535305697936565, 0.04533586895558983, 0.04651268501766026, 0.04510007903445512, 0.0455114709911868, 1.0141871079104021, 0.814690904924646, 0.9420117940753698, 0.09992014605086297, 0.049771548015996814, 0.05084224499296397, 0.0506880390457809, 0.05040266807191074]
[0.0011072704774877902, 0.0011009027948603034, 0.0011086839764505964, 0.001101424954619936, 0.0011032824774950066, 0.0011237645204263654, 0.0011020979092625732, 0.0011066978199365126, 0.0011056878641036085, 0.0011025301831647414, 0.00110107834122821, 0.0011040077502415938, 0.001105877613140778, 0.0011117902520874684, 0.0011261441369159993, 0.0011066773625895041, 0.0011109012491281399, 0.0011023966362699866, 0.001116298795254393, 0.0011106345442716372, 0.0011119692962066356, 0.0011081369091036984, 0.0011117898869666863, 0.0011145246813116087, 0.0011182457039302046, 0.0011163309549870478, 0.0011121624980164183, 0.0011186815443745052, 0.0011100316372573036, 0.0011181751827032051, 0.001116363932272758, 0.001118025274164128, 0.0011494343423030593, 0.0011374917495149102, 0.0011387414997443557, 0.0011540211827113208, 0.0011509244981094855, 0.0011372418190479618, 0.001137953545284373, 0.04639717854495922, 0.0023472654760222545, 0.0011862849562682889, 0.0011610873413949528, 0.0010883968868504533, 0.001082328545056622, 0.0010920267721468751, 0.0010989535684612665, 0.0010934765212534164, 0.001088225523496724, 0.0010825185242786326, 0.0011364126112312078, 0.0010916314098391342, 0.0010862779771824453, 0.0011005652486346662, 0.0010900283417537469, 0.0010875314315357668, 0.0010820841575464742, 0.0010852609544103457, 0.0010960839784026823, 0.0011003246128728445, 0.0010848603851627558, 0.0010812576140531085, 0.0010880243869244375, 0.0010984667486892167, 0.0010974757711995733, 0.0010921344086951153, 0.0010971618414094503, 0.0010852803640194577, 0.0010955562730404463, 0.0010906422500159931, 0.0010822077747434378, 0.0010930442738092759, 0.0010828510461248118, 0.0010902052956887267, 0.0010839594999031926, 0.0011136634539897468, 0.0010845716381233863, 0.0010821906828575513, 0.0010814231592865492, 0.0010842687042895705, 0.0010983202505899085, 0.001079991340785372, 0.001084943976655433, 0.001109857956180349, 0.0010965283621441233, 0.0011473084768195722, 0.010057367638430813, 0.009104901432610031, 0.011363690316846425, 0.00822835656898943, 0.014857694795037705, 0.001239778407828205, 0.0013455465457148173, 0.0012762281376953151, 0.0011605100907300684, 0.0011523947945203292, 0.001167533294805749, 0.001168033658442172, 0.0011543644782663746, 0.0011790484078863467, 0.0011507385431534865, 0.00115400461204858, 0.0011538587039632891, 0.0011501913408706473, 0.0011912034533452243, 0.0011677516132212159, 0.0011653092726853422, 0.01394651090959087, 0.007553707318782637, 0.0034202627721242607, 0.001085004409436475, 0.0010789265215862542, 0.001092182702533054, 0.0010768162478184836, 0.0010902273616838183, 0.001070908773478798, 0.0010751697277142243, 0.001078090794511478, 0.0011518917744979262, 0.0011334031137680126, 0.0011395129311660473, 0.0011418245902115648, 0.001142565817148848, 0.0011346806581555443, 0.0011380606129850175, 0.0011446216376498342, 0.0011746721813158895, 0.0011607421593825247, 0.001150003499457274, 0.0011561748635193164, 0.0011579553855964068, 0.0011554641373963518, 0.0011580700679174201, 0.0011607140001036566, 0.0011691990022716875, 0.0011612199331549082, 0.001148698999631134, 0.0011536682723090053, 0.0011576594790147449, 0.0011444357938175513, 0.0011571600916795433, 0.0011450040011285719, 0.0011438838158607143, 0.0011422804990698669, 0.0011474765461488541, 0.0011795072721063414, 0.001154254227664999, 0.0011400199331216174, 0.0011560746140524068, 0.0011435511379121717, 0.0011472962029405278, 0.0011434877259572122, 0.001152482317675921, 0.001149285158713941, 0.0011851845219181005, 0.001152808113891462, 0.0012307460446291689, 0.001154856407083571, 0.0011541537056804043, 0.0010950141586363316, 0.0011022910237608646, 0.0011056923196354712, 0.010410845636876977, 0.010285761863501235, 0.03583751193152487, 0.006416505774144422, 0.005193496181163937, 0.0011873121151107955, 0.0011790561362762344, 0.0011695047935724936, 0.0011714784539601003, 0.0011732799995859916, 0.001163282680367543, 0.001166473203656179, 0.0011702404987193984, 0.0011718892042567445, 0.0011654452510728415, 0.0011555412255058234, 0.001162134817356921, 0.0011681336592036214, 0.0011589031809390608, 0.0011665887273424728, 0.0011656204323199663, 0.0011847106136635623, 0.0011672644098077647, 0.0011631342958108607, 0.00921044388616627, 0.008988810115790164, 0.0029314469323832204, 0.0014351957493504, 0.0011585288183678958, 0.0010885717955798927, 0.0010730887271082875, 0.001075571318241683, 0.0010772054321767594, 0.0010853407729882747, 0.0010994759769263592, 0.0010796742730731653, 0.0010851418403696946, 0.0010845556363082405, 0.001083463295468722, 0.0010925396795342253, 0.0010970031832006168, 0.0010875364982480692, 0.0011015608165921135, 0.0011108176364690405, 0.0010929797056385062, 0.0010812614557587287, 0.001085852682907981, 0.0010945797284049067, 0.0010972216153856027, 0.001094009954778647, 0.0011021811144680462, 0.0011021405913528395, 0.0010899068861776454, 0.001092288476436145, 0.001095023411008614, 0.001102864750216461, 0.0011103689083194529, 0.0010857165907509625, 0.0010794713638130236, 0.0011046131575395439, 0.0010963762972757897, 0.0010942541835406287, 0.05442658709150485, 0.00598977465944534, 0.0011458489091389558, 0.0011066072726283562, 0.0011019564302511174, 0.0014861206582281739, 0.001112607000818984, 0.001121635728685016, 0.0011081304083662953, 0.0011270542269233954, 0.0010960315676956352, 0.0011071517497343434, 0.017918429887768896, 0.00483335463848727, 0.0012163342273032124, 0.0011266033636109735, 0.0011044317718849263, 0.0010985700884537603, 0.001096405636582693, 0.0010921184545044196, 0.0010870433417783881, 0.0010912837725217369, 0.0010833664299835536, 0.0010810764532917265, 0.0010979312514378266, 0.0010814621795857834, 0.00109553263543851, 0.0010823210230393504, 0.0010969800456554037, 0.0010835603197020564, 0.0010838548187166452, 0.0011060546808452768, 0.0010871609080244195, 0.0010823988413903862, 0.0010746342966079035, 0.001078237088883973, 0.0011044879316944969, 0.0010811219769741663, 0.001080539770720696, 0.0010831366578879004, 0.001081528523090211, 0.001076864795653488, 0.0010793656137221578, 0.0010828320677815514, 0.0010835517261346633, 0.0010832757265730338, 0.0010752996816088191, 0.0010999728189992973, 0.0010820141152097758, 0.001098972319265489, 0.0010890617744404483, 0.0010912350236057218, 0.0010945040452167052, 0.0010856086578728123, 0.0010797271150460635, 0.001084829725600271, 0.0010985658631067383, 0.0011034930913328108, 0.0010972466817210343, 0.0011198110217016867, 0.0011118406122973697, 0.0010888779546472836, 0.006586131203221157, 0.01677665056988851, 0.010440121476263315, 0.028172302795362404, 0.0021236573203085836, 0.003183682250726799, 0.001325304408303716, 0.0011313600672027942, 0.0010988238870843568, 0.001102517795516178, 0.0010931744314306839, 0.0010956015215593982, 0.001117715408327058, 0.0011029700452292507, 0.0010910115908535029, 0.0010943917494097893, 0.0010986867948138918, 0.0010872464097866957, 0.0010950967050369152, 0.0010885964994403448, 0.0010955807969863104, 0.0010907957065765831, 0.0011192981592929837, 0.001097850181395188, 0.0011142699778164651, 0.0011050789087841456, 0.0011098942724691535, 0.001094842067686841, 0.019573551271936263, 0.00769824290711602, 0.004319936317518692, 0.01080845829512162, 0.0032035321584606372, 0.0012560058411591772, 0.0011957800677257844, 0.0011876892028588124, 0.0011846216591286727, 0.0011097302512181077, 0.0010844742487692697, 0.0010999190668702465, 0.001085306840567765, 0.0010781319764315742, 0.001092661703957922, 0.0011827346117405052, 0.0011300640683409504, 0.001106857954503291, 0.0010906658161812547, 0.0011224338192154062, 0.0011429137269839305, 0.0011025721138470215, 0.014207401431948792, 0.013718883887949314, 0.0011995691139335659, 0.001117140978087925, 0.0025266585236584597, 0.001094345659525557, 0.0010926344548352063, 0.0010919837487480518, 0.0010972108178645033, 0.0011304880926300857, 0.0011546117946801876, 0.0011102784985930405, 0.0011124484775460917, 0.0011063795457382432, 0.0011039442747873677, 0.0011195775216699324, 0.001132708135463128, 0.0011166721577121114, 0.0011090855246452106, 0.0011136844563721256, 0.001113492590692741, 0.0011134177065369759, 0.001106191841906614, 0.0011108259310607207, 0.001113287636227059, 0.0011176357035186481, 0.0011077127975030717, 0.0011174262750004841, 0.001114489089972763, 0.0011027208185458387, 0.019377405069958928, 0.007031650774561885, 0.0072672388421117584, 0.004475803569551896, 0.0011164102047173815, 0.0010714278191285716, 0.001076379227346148, 0.0010732918639074671, 0.001065071680138565, 0.0010793387720530684, 0.0010768354298885572, 0.0010704501606100662, 0.0010723651820709083, 0.0010748932942409408, 0.0010901031835766678, 0.0010853548645338212, 0.0011079846140505238, 0.0010799890230621465, 0.0010932428201406517, 0.0010802891602824357, 0.00107893922673115, 0.0010838207249021666, 0.0010761034076991068, 0.0010776925906115634, 0.00107857138606381, 0.0010869509556373073, 0.0010853407041974026, 0.0010761151603550059, 0.0010793590918183327, 0.0010757697719699618, 0.0010816121119370853, 0.00107935283978639, 0.001091329478764568, 0.0010957487499002707, 0.01814011320344765, 0.004936860363655301, 0.0025578261834611608, 0.0017768765683285892, 0.0012231013414830986, 0.0011584587495731698, 0.0012096293873830953, 0.0010778087043118749, 0.0010926770919468254, 0.0010763065894769336, 0.0011082953397734937, 0.0010801044091666963, 0.0010966600675601512, 0.001111567568097433, 0.0011058773194566709, 0.008505644157677043, 0.006518464046001265, 0.012345553000076588, 0.03937600233951922, 0.003358231272696602, 0.001088626023953442, 0.0011696166581135583, 0.001167965044838969, 0.0010856170000889424, 0.0011241210899739103, 0.001079986774129793, 0.0010722182976843958, 0.0010787411592900753, 0.0010925047972705215, 0.0010818275226153094, 0.0010837604984937404, 0.0010847212714989755, 0.0011119565460830927, 0.0010898647967471995, 0.0010829260017172519, 0.001117197682932866, 0.0010834773632020435, 0.0010918805465652522, 0.015120243090099062, 0.0011551057739945297, 0.0011040297500916165, 0.0010933203871404244, 0.0010794255702587013, 0.0010737830227960578, 0.0010789710451552476, 0.0010817888409788297, 0.0010871235466435212, 0.0010790236146104608, 0.0010809947958808732, 0.001075493248539384, 0.0011133827290243723, 0.0010659526821903207, 0.0010701019544450735, 0.0010804089542943984, 0.0010705118633764373, 0.0010710486385505646, 0.0010773217734161087, 0.0010684953171717511, 0.0011318038635760206, 0.0010743088205344975, 0.001082421113758094, 0.0010764464095700532, 0.001082979772366922, 0.0010884585234717551, 0.001085800296013159, 0.0044729932057882915, 0.02908306495572271, 0.026075075319650667, 0.015588864136423746, 0.004346062386916442, 0.001259316203438423, 0.001208317796805535, 0.00120716013638726, 0.0011965775232635099, 0.0010724344782912258, 0.001071509362769906, 0.0010805033856410194, 0.0010732791375961494, 0.0010784415221265094, 0.0011034359525762159, 0.0010783472733402793, 0.0010745392964136872, 0.0010895736367357049, 0.0010754084532064471, 0.0010856013184159317, 0.0011016990703699942, 0.0010792294527742672, 0.0010760824555869806, 0.001077137998601591, 0.0010657258654563602, 0.0010653367498889565, 0.0010739425673488188, 0.0010628869984595274, 0.0010583615697793323, 0.001062648680420931, 0.0010652417496947403, 0.001077654250283641, 0.0010657679998654533, 0.0010660923170772467, 0.001059824772792953, 0.0010518239975102585, 0.0011100390693172812, 0.0010656609771434557, 0.0011289828398730606, 0.0010566725686658174, 0.0015006029544482853, 0.0011140755245419728, 0.0012165819749828768, 0.001026166907236488, 0.0010283942498393696, 0.0010155527270399034, 0.0010139677271416242, 0.0010174858616665006, 0.001013720251979645, 0.0010176873871717942, 0.0010395717479034581, 0.001030873475511643, 0.0010285501590591264, 0.0010184939548542554, 0.0010438222961965948, 0.001022325294219296, 0.001019294797019525, 0.001022053705210882, 0.0010226742729586972, 0.0010275602716402236, 0.0010150296809363433, 0.0010300156586295502, 0.0010172347035470673, 0.0010294309547382661, 0.001032701069065793, 0.0010467036362652752, 0.0010929149549073454, 0.0010543149335055866, 0.0010520878361624686, 0.0010547222553340847, 0.0010543225338509263, 0.001081690349247913, 0.0010488390473129099, 0.0010584063021206232, 0.023585746695590746, 0.01894630011452665, 0.021907251025008602, 0.002323724326764255, 0.0011574778608371352, 0.0011823777905340459, 0.001178791605715835, 0.0011721550714397847]
[903.1217036227961, 908.3454094844884, 901.9702830029678, 907.9147842124802, 906.3861888484729, 889.8661435053955, 907.360400192676, 903.589021307882, 904.4143763038489, 907.0046473735212, 908.2005907813416, 905.7907426656801, 904.259194794551, 899.4502318422257, 887.9857979268522, 903.605724490568, 900.1700203189279, 907.1145240278937, 895.8175035673224, 900.3861847785474, 899.3054065533943, 902.4155695787053, 899.4505272289493, 897.2434767646129, 894.257850922551, 895.7916964791167, 899.1491816920061, 893.909446373441, 900.8752241249872, 894.3142501003153, 895.7652348765356, 894.4341627228708, 869.993146364806, 879.1272555835728, 878.1624277542332, 866.5352204805678, 868.866725525962, 879.320460477919, 878.7704947569732, 21.55303471807003, 426.0276522682171, 842.9677833441573, 861.2616504789215, 918.782488338191, 923.9338688491163, 915.728465185927, 909.9565520317484, 914.5143773674561, 918.9271694223504, 923.7717208270212, 879.9620754970184, 916.0601197315883, 920.5746788623752, 908.6240013852655, 917.4073385936916, 919.5136535850199, 924.1425382914833, 921.4373703726671, 912.3388533215265, 908.8227131347119, 921.7775979993733, 924.8489786365404, 919.0970460016438, 910.3598276354605, 911.1818467818845, 915.6382145259962, 911.4425623072774, 921.4208910003566, 912.7783068821709, 916.8909419980165, 924.036976390299, 914.8760246599937, 923.4880490522588, 917.2584319252088, 922.5436929048632, 897.937340421343, 922.0230041514649, 924.0515704307074, 924.7074019200155, 922.2806081590406, 910.4812548643251, 925.9333498663035, 921.7065779587157, 901.0162016061654, 911.969115002758, 871.605170016766, 99.4295958893697, 109.83095285561348, 87.99958218832501, 121.5309511219706, 67.30519194229163, 806.5957542781864, 743.1924248066448, 783.558966037104, 861.6900516314403, 867.7581717264161, 856.5066233647558, 856.1397120470983, 866.2775222447946, 848.1415973349875, 869.0071310721876, 866.5476632929636, 866.6572402367697, 869.4205602722077, 839.4871566160484, 856.3464941328773, 858.1412878450687, 71.70252161867313, 132.38532521818186, 292.37519647618154, 921.6552405712117, 926.8471763302146, 915.5977270842519, 928.6635505601766, 917.2398667884602, 933.7863548839422, 930.0857103984571, 927.565660602024, 868.1371133463201, 882.2986171932134, 877.5679263040167, 875.7912630123978, 875.2231031166275, 881.3052313992278, 878.6878208332871, 873.6511412218496, 851.3013382846792, 861.5177728462675, 869.5625713068997, 864.9210699461751, 863.5911300545905, 865.4530829951462, 863.5056096375234, 861.5386735325807, 855.2863952646697, 861.1633089031712, 870.5500747551067, 866.8002960665232, 863.8118705261024, 873.7930125937868, 864.1846596598094, 873.3593935168358, 874.2146589840078, 875.4417157732075, 871.4775071927935, 847.8116444455821, 866.3602662499683, 877.1776448344959, 864.996071918476, 874.4689824940763, 871.6144945280856, 874.5174760515258, 867.6922714238127, 870.1060763013836, 843.7504721894292, 867.4470520721464, 812.5153067636354, 865.9085180341691, 866.4357226236808, 913.2302008271229, 907.2014363213611, 904.4107318477927, 96.05367660604156, 97.22177251142423, 27.903722834071495, 155.84806360333093, 192.54851936290163, 842.2385211715655, 848.1360379992253, 855.0627628855575, 853.6221870915085, 852.3114690038731, 859.6362834904812, 857.2850168058834, 854.5252032332726, 853.3229902345904, 858.0411641640461, 865.3953471562754, 860.4853628551726, 856.0664202431706, 862.884852201112, 857.2001225127835, 857.9122090453344, 844.0879894775578, 856.7039238047948, 859.745949888672, 108.57240024033601, 111.24942980421326, 341.12846763595195, 696.7690647443884, 863.1636815118446, 918.6348608887946, 931.889390632922, 929.738440436265, 928.3280330096864, 921.3696056462475, 909.5241924207845, 926.2052685145661, 921.5385148722237, 922.0366079180016, 922.9661993924635, 915.2985641915751, 911.5743831138209, 919.5093696725736, 907.802805744025, 900.2377772635149, 914.930071291499, 924.8456926620888, 920.9352389515103, 913.5926548331619, 911.3929091239825, 914.0684649458532, 907.2919022774558, 907.3252612650213, 917.5095713974675, 915.5090633774162, 913.2224845119165, 906.7294967979785, 900.6015861102445, 921.0506761330096, 926.3793682008326, 905.2943043222815, 912.0956030194561, 913.8644521918529, 18.373373261835198, 166.95118879356994, 872.7154095311279, 903.6629568002494, 907.4768952272611, 672.8928734442392, 898.7899584164986, 891.5550516319418, 902.4208635103598, 887.2687543435911, 912.3824800981436, 903.2185517838417, 55.80846124707606, 206.89563973583722, 822.1424486402423, 887.6238366578402, 905.4429847606672, 910.2741923435237, 912.0711957636659, 915.6515906086205, 919.926521387835, 916.3519381298977, 923.0487232424036, 925.0039596692168, 910.8038401224321, 924.6740374989492, 912.7980013116897, 923.9402900923255, 911.5936100756858, 922.8835550889934, 922.6327942925651, 904.1144324218881, 919.8270399707367, 923.8738640143577, 930.5491209023502, 927.4398092121353, 905.3969457736019, 924.9650097751138, 925.4633907024286, 923.2445349509123, 924.6173158177443, 928.6216840185185, 926.4701295713248, 923.504234639769, 922.890874409184, 923.1260107373786, 929.9733061427501, 909.1133732829437, 924.2023610811441, 909.9410262383695, 918.2215586565715, 916.3928744659809, 913.6558282907085, 921.142248404174, 926.1599399190209, 921.8036493668792, 910.2776934757514, 906.2131950388464, 911.3720885730976, 893.0078206235016, 899.4094917379605, 918.376568955266, 151.83420571866503, 59.60665365438589, 95.78432609942348, 35.49585588596668, 470.8857641188044, 314.10169773434876, 754.5436306817393, 883.8919005444725, 910.0639435982974, 907.014838279158, 914.7670959439267, 912.7406089913729, 894.6821279817117, 906.6429358851277, 916.5805463328726, 913.7495787402497, 910.1774998300508, 919.7547041762019, 913.1613631932994, 918.614014020904, 912.7578748648835, 916.7619509050493, 893.4169968006205, 910.8710978479446, 897.4485716285834, 904.9127551445554, 900.9867199110106, 913.3737454140567, 51.08934940353711, 129.89977220329467, 231.48489387324702, 92.52013309348183, 312.1554429722099, 796.1746412557227, 836.274183681509, 841.9711129754843, 844.1513729670721, 901.1198882812638, 922.1058048495513, 909.1578008966059, 921.3984125234688, 927.5302299351353, 915.1963470282926, 845.4982124251915, 884.9055801483028, 903.4582946542185, 916.8711306101968, 890.9211241505635, 874.9566799227533, 906.9701540980086, 70.38584816441218, 72.8922271059092, 833.6326672507022, 895.1421706073113, 395.7796396451928, 913.7880625702306, 915.219171036321, 915.7645442493899, 911.4018780331529, 884.5736691250728, 866.0919666743807, 900.6749218932125, 898.9180354724044, 903.8489583904407, 905.8428245326164, 893.1940670874012, 882.8399555823196, 895.5179844806423, 901.6437215875607, 897.9204066989877, 898.0751271796668, 898.1355282289026, 904.0023277304383, 900.2310551439022, 898.2404613681044, 894.7459327325568, 902.7610787327993, 894.913626404182, 897.2721303395075, 906.8478468727045, 51.606497174914026, 142.2141161528754, 137.6038440081617, 223.42356729031275, 895.7281076207552, 933.3339886706822, 929.0405970259536, 931.7130164011138, 938.903942943916, 926.4931696077648, 928.6470079309063, 934.1864168903337, 932.5181540012708, 930.3249032790476, 917.3443533289793, 921.3576431792355, 902.5396086902704, 925.9353369765281, 914.7098719306883, 925.678083947964, 926.8362621587953, 922.6618176085045, 929.2787225143828, 927.9083930905798, 927.1523544208304, 920.0047111727081, 921.3696640443324, 929.2685735141064, 926.4757276610873, 929.5669259871363, 924.5458597991066, 926.4810941692668, 916.3135601652062, 912.6179702153572, 55.12644760176818, 202.55788625538318, 390.95697998009973, 562.7852929259152, 817.5937398510478, 863.2158895329218, 826.699491952154, 927.808428341139, 915.1834584710635, 929.103296195541, 902.286569394376, 925.8364205470681, 911.8595903877484, 899.6304216680298, 904.2594349356134, 117.56899083268348, 153.41037289504533, 81.00082677493639, 25.39617890555545, 297.7757988648045, 918.5891003858344, 854.980982925527, 856.19000707155, 921.135170062805, 889.5838792804866, 925.9372651167483, 932.6459007085019, 927.0064383731356, 915.3277884896868, 924.3617666358755, 922.7130914900897, 921.8958144133201, 899.3157183367788, 917.5450046506602, 923.4241290856885, 895.0967364833781, 922.9542157158322, 915.8511003293515, 66.13650283538189, 865.7215836969192, 905.7726930972797, 914.6449766801627, 926.4186689225216, 931.2868417271751, 926.808930128533, 924.3948191360292, 919.8586518408933, 926.7637764915933, 925.0738336673743, 929.805929844831, 898.1637436358236, 938.1279457407048, 934.49039677586, 925.5754462467294, 934.132571726912, 933.6644144875495, 928.2277817787651, 935.8955382667896, 883.545313973769, 930.8310430723943, 923.8548539838314, 928.9826145636113, 923.3782804774236, 918.7304600366332, 920.9796715582042, 223.56394342516478, 34.384271448777575, 38.35080005488541, 64.1483555984991, 230.09333759460966, 794.0817383827915, 827.5968479846356, 828.3905091438485, 835.7168512347029, 932.4578985872965, 933.2629604047038, 925.4945549353739, 931.7240641048195, 927.2640003958345, 906.2601210929173, 927.3450443310423, 930.6313909016965, 917.7901945167636, 929.8792445032317, 921.1484759977651, 907.6888842832194, 926.5870176443016, 929.2968162504989, 928.3861504266524, 938.3276060131885, 938.6703313334804, 931.1484900618506, 940.833787081156, 944.856680886948, 941.0447859436338, 938.7540436586942, 927.9414058236191, 938.2905098729216, 938.005071400906, 943.5522037899747, 950.7294018458131, 900.8691924826036, 938.384740971315, 885.7530554781833, 946.3669538262229, 666.398794588314, 897.605214342293, 821.9750255744788, 974.500340001261, 972.3897232566162, 984.685455884466, 986.2246827312747, 982.8146391755644, 986.4654455182765, 982.6200192763042, 961.9345677840284, 970.0511495881491, 972.2423269223518, 981.8418609495803, 958.017474472167, 978.1622401934749, 981.0704449037274, 978.422165979691, 977.8284507997856, 973.1789244866085, 985.1928655697253, 970.859026871993, 983.0572988839543, 971.4104626417136, 968.3344289598003, 955.380267492031, 914.9842771478751, 948.4831981607337, 950.4909814826287, 948.1169046568081, 948.4763607844816, 924.4789885527671, 953.4351362699226, 944.8167475915437, 42.39848807444988, 52.780753706802756, 45.64698687473078, 430.3436463965079, 863.9474099977681, 845.7533691903405, 848.3263667225883, 853.1294402639724]
Elapsed: 0.10294478440059646~0.22263222213028136
Time per graph: 0.002342886860559278~0.005069787034998063
Speed: 822.5048356412769~238.50989699507954
Total Time: 0.0518
best val loss: 0.6931243538856506 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.05s
test Score 0.4884
Epoch Time List: [0.2221887189662084, 0.21879838698077947, 0.21888815390411764, 0.2182237949455157, 0.21839313895907253, 0.21927956503350288, 0.21919276693370193, 0.21806929889135063, 0.2185741850407794, 0.21920281706843525, 0.21973347919993103, 0.21854582394007593, 0.21906391391530633, 0.21956816804595292, 0.22182805591728538, 0.2190710991853848, 0.21875356405507773, 0.21894723293371499, 0.2195804949151352, 0.2198140249820426, 0.22195211588405073, 0.21972695703152567, 0.22043813299387693, 0.22369571204762906, 0.2205815240740776, 0.21986972098238766, 0.21976964897476137, 0.22459958191029727, 0.22259786201175302, 0.2203634651377797, 0.22115140897221863, 0.22090061893686652, 0.23516627203207463, 0.22600082110147923, 0.2273967940127477, 0.226615167921409, 0.22674889804329723, 0.2262208639876917, 0.22563136892858893, 2.840052238898352, 2.701293746009469, 0.7694677889812738, 0.23397356900386512, 0.2254609428346157, 0.21509795589372516, 0.2148421419551596, 0.2185568769928068, 0.21535623294766992, 0.2139667080482468, 0.21560618106741458, 0.21656710596289486, 0.21803855197504163, 0.21422174095641822, 0.21879350999370217, 0.21497068705502898, 0.21538893110118806, 0.21443196898326278, 0.21684108104091138, 0.21476066508330405, 0.2146580540575087, 0.21340137906372547, 0.2138677501352504, 0.2139333221130073, 0.21494484099093825, 0.21518847986590117, 0.21523487498052418, 0.21535328216850758, 0.2175669150892645, 0.21534826804418117, 0.21659081592224538, 0.2154666899004951, 0.21585781197063625, 0.21538291708566248, 0.21589584706816822, 0.2155170898186043, 0.2194649459561333, 0.2180603400338441, 0.21405468590091914, 0.2145777610130608, 0.21438175591174513, 0.21720388007815927, 0.21493565593846142, 0.2143475681077689, 0.2190624208888039, 0.22098834393545985, 0.21816996706184, 2.1226917629828677, 1.4243875689571723, 1.6215737418970093, 1.4583273930475116, 6.280375770991668, 1.2095164688071236, 0.2545517919352278, 0.26375989301595837, 0.2314099611248821, 0.23084459407255054, 0.2324997839750722, 0.2335036030272022, 0.23000160395167768, 0.23202980612404644, 0.231538139982149, 0.22888159495778382, 0.2345208500046283, 0.23270403104834259, 0.23095053213182837, 0.23522782896179706, 0.23258656612597406, 1.1670551371062174, 2.0769924808992073, 0.6807311279699206, 0.42454286909196526, 0.21589793195016682, 0.2159855788340792, 0.21384755696635693, 0.21442639688029885, 0.21373596787452698, 0.21375496906694025, 0.21378079487476498, 0.22107251500710845, 0.22828507411759347, 0.22764450800605118, 0.22746775404084474, 0.2274223860586062, 0.22814591496717185, 0.22538260801229626, 0.2269485059659928, 0.2325821239501238, 0.22872723219916224, 0.2281952981138602, 0.22960407403297722, 0.23081456893123686, 0.2304770271293819, 0.23032777290791273, 0.23278732399921864, 0.23152160993777215, 0.23053061601240188, 0.22859255992807448, 0.22949114593211561, 0.2282177358865738, 0.2293053090106696, 0.22867500002030283, 0.22815450804773718, 0.22819594899192452, 0.22800338990055025, 0.22837052715476602, 0.2298734940122813, 0.2307015439728275, 0.23195219412446022, 0.23036472604144365, 0.22928308392874897, 0.2285224871011451, 0.2293388230027631, 0.22879723797086626, 0.22879949503112584, 0.23341600294224918, 0.23019540205132216, 0.23133209999650717, 0.23266299511305988, 0.2270784379215911, 0.22626708494499326, 0.22658863989636302, 0.21834261692129076, 1.7823610419873148, 1.6005414259852841, 2.6642240419751033, 5.288845176110044, 1.4268652991158888, 0.26401400892063975, 0.23822427808772773, 0.23378223390318453, 0.23252401396166533, 0.23294799297582358, 0.2325448620831594, 0.2323283018777147, 0.23451051500160247, 0.23252127598971128, 0.23159180209040642, 0.2317353510297835, 0.23205947584938258, 0.23182883707340807, 0.23108636192046106, 0.23419197695329785, 0.23153201106470078, 0.23390811018180102, 0.23412775283213705, 0.2344903239281848, 2.038397151976824, 2.208601661026478, 0.8683553390437737, 1.264057244057767, 0.32568161305971444, 0.21932252310216427, 0.2141661550849676, 0.2137871041195467, 0.21318968711420894, 0.21389646001625806, 0.21512952412012964, 0.21298102603759617, 0.21482334204483777, 0.2147174949059263, 0.21479846886359155, 0.21847736393101513, 0.21826549794059247, 0.21655368304345757, 0.2162444598507136, 0.21688546903897077, 0.21641402004752308, 0.21598602691665292, 0.21580079512204975, 0.21641402097884566, 0.22091433499008417, 0.21710664010606706, 0.21630384190939367, 0.22085959813557565, 0.21629633090924472, 0.21651442209258676, 0.21671163605060428, 0.21599351905751973, 0.21893572888802737, 0.2161895700264722, 0.2141606518998742, 0.2167536470806226, 0.21857147687114775, 0.2158934709150344, 4.10794165788684, 4.176006245077588, 0.22477252292446792, 0.2215604929951951, 0.21970651601441205, 0.23608530720230192, 0.24627357407007366, 0.2226156231481582, 0.22072784788906574, 0.22353355900850147, 0.22357792500406504, 0.21974121301900595, 1.0525550181046128, 2.976795630995184, 0.24357300612609833, 0.22913385787978768, 0.22003029787447304, 0.2179747159825638, 0.21827314386609942, 0.21721899590920657, 0.21750387887004763, 0.21591482602525502, 0.21520867303479463, 0.21712993900291622, 0.21611050108913332, 0.21556056383997202, 0.21642728592269123, 0.21562537003774196, 0.21663325000554323, 0.21853555494453758, 0.21566255507059395, 0.21738920791540295, 0.21572855988051742, 0.21595398208592087, 0.21396752796135843, 0.21502648596651852, 0.2157071459805593, 0.2148455900605768, 0.21668947907164693, 0.21499823813792318, 0.21472871105652303, 0.21449619194027036, 0.21675983385648578, 0.2149906470440328, 0.21398007310926914, 0.21512489998713136, 0.21443588694091886, 0.22314213891513646, 0.21622480498626828, 0.21728968096431345, 0.21620940591674298, 0.21635560400318354, 0.21628824202343822, 0.21641660493332893, 0.21537356602493674, 0.21601286705117673, 0.22255424223840237, 0.2191697210073471, 0.2185317069524899, 0.22185741493012756, 0.21983225108124316, 0.2185474169673398, 3.0253526330925524, 2.298358035972342, 1.6246135389665142, 2.5704064291203395, 1.4726843710523099, 0.48100950196385384, 0.5444759350502864, 0.2241118341917172, 0.2182209170423448, 0.21758621092885733, 0.2174853840842843, 0.2171828020364046, 0.2178924580803141, 0.21877215208951384, 0.21729668998159468, 0.21757608896587044, 0.21740323095582426, 0.21785682218614966, 0.2166032741079107, 0.21625932201277465, 0.21698736492544413, 0.21640075708273798, 0.22165121987927705, 0.21996120002586395, 0.21939142991323024, 0.22075862402562052, 0.21863810601644218, 0.21836244990117848, 3.4626650229329243, 1.5071855259593576, 1.2903997969115153, 2.567687910166569, 0.8271925749722868, 0.25639134692028165, 0.24394063197541982, 0.242593496106565, 0.24065875890664756, 0.23280374996829778, 0.21596100507304072, 0.21658712101634592, 0.21558479708619416, 0.21576750802341849, 0.2163506749784574, 0.22671057796105742, 0.22430913604330271, 0.21999193797819316, 0.219236196950078, 0.22070480801630765, 0.22310033102985471, 0.2187192119890824, 3.140990707091987, 1.3764974189689383, 3.555018612067215, 0.23503474902827293, 0.2803349200403318, 0.22213376383297145, 0.21774428989738226, 0.2170537451747805, 0.21734541095793247, 0.22011782403569669, 0.22642140602692962, 0.22003083385061473, 0.27284736814908683, 0.2196957259438932, 0.21909562509972602, 0.22064504795707762, 0.22142786299809813, 0.22313017898704857, 0.22001091518905014, 0.2215674639446661, 0.22027519112452865, 0.22017256787512451, 0.2200928347883746, 0.2207542760297656, 0.22112789005041122, 0.22475142497569323, 0.22027167293708771, 0.22044740000274032, 0.22201101400423795, 0.22045114915817976, 1.0233185470569879, 2.2975324409781024, 1.267995783011429, 2.153455749968998, 0.21878530899994075, 0.21528458909597248, 0.21362447598949075, 0.21161206590477377, 0.2124596789944917, 0.21359396202024072, 0.213367359014228, 0.21286856301594526, 0.21238533407449722, 0.212987937964499, 0.21499363996554166, 0.21531070792116225, 0.21895337698515505, 0.21492436493281275, 0.21557514392770827, 0.21507600112818182, 0.21426670416258276, 0.21417504001874477, 0.21307403000537306, 0.21376624703407288, 0.21384747000411153, 0.21383531694300473, 0.21441422076895833, 0.21433459396939725, 0.21399263781495392, 0.21690178604330868, 0.21438165905419737, 0.2147409999743104, 0.21810443105641752, 0.21673116798046976, 0.9660004910547286, 2.2908820919692516, 0.7967712730169296, 0.4609520899830386, 0.2379025089321658, 0.2300738369813189, 0.23653154796920717, 0.2273145040962845, 0.21463683701585978, 0.21882974402979016, 0.21538078598678112, 0.21449841395951807, 0.21821478195488453, 0.21734658407513052, 0.21572340396232903, 0.946317947935313, 1.6775145541178063, 1.5083342341240495, 3.700482068932615, 1.3314040121622384, 1.8511265519773588, 0.22642861492931843, 0.23189689591526985, 0.22564219997730106, 0.21798181696794927, 0.21462334494572133, 0.21382560895290226, 0.2148390719667077, 0.21417882898822427, 0.21509014803450555, 0.2143852470908314, 0.21448653389234096, 0.21695299295242876, 0.2167965481057763, 0.21708734286949039, 0.21910850086715072, 0.2163205649703741, 0.21726734493859112, 4.643219177960418, 0.8370220757788047, 0.2217893599299714, 0.2185458909953013, 0.2171713940333575, 0.2161017549224198, 0.21389612695202231, 0.2153927399776876, 0.21494673285633326, 0.2158771260874346, 0.21431846893392503, 0.21430210291873664, 0.2158266268670559, 0.21440512989647686, 0.21198886597994715, 0.21204868296626955, 0.2115721518639475, 0.21179674984887242, 0.21141006401740015, 0.21156360302120447, 0.22319916100241244, 0.21167018893174827, 0.22108301508706063, 0.21349541493691504, 0.2146123159909621, 0.2157871728995815, 0.21502453391440213, 0.3614606020273641, 3.3829637290909886, 1.9076089169830084, 2.4795968459220603, 0.8961751179303974, 0.2483565459260717, 0.24211702193133533, 0.2427444829372689, 0.23950547398999333, 0.3164437370141968, 0.2127577739302069, 0.21295459708198905, 0.21264777798205614, 0.21472692117094994, 0.21538550511468202, 0.2137823790544644, 0.2130701350979507, 0.2165115368552506, 0.21424856677185744, 0.21302012004889548, 0.21395528002176434, 0.2170127488207072, 0.21348089794628322, 0.21249318402260542, 0.21171259810216725, 0.21259177091997117, 0.21241604292299598, 0.21111658425070345, 0.21139692910946906, 0.21036885608918965, 0.21098275505937636, 0.21134853491093963, 0.2113493619253859, 0.211144485976547, 0.21504153194837272, 0.210543729015626, 0.21250680019147694, 0.21227776899468154, 0.21638541296124458, 0.2100989300524816, 0.22965735103935003, 0.2185701810522005, 0.21740383910946548, 0.2190712089650333, 0.20444848202168941, 0.20256917795632035, 0.2020920329960063, 0.20208180614281446, 0.2024381220107898, 0.20286930992733687, 0.20574866607785225, 0.20605979883112013, 0.20736436592414975, 0.20408575993496925, 0.20438064192421734, 0.20234323292970657, 0.20304498705081642, 0.20260675088502467, 0.20368340890854597, 0.2030074851354584, 0.20244413893669844, 0.20351631590165198, 0.20410884206648916, 0.20299411879386753, 0.20498376106843352, 0.20558217400684953, 0.213721111882478, 0.21132853697054088, 0.21433241409249604, 0.21706022892612964, 0.21647272701375186, 0.2137633899692446, 0.21662371489219368, 0.2157467819051817, 2.7935083229094744, 4.693727066740394, 4.046426284941845, 1.6970701969694346, 0.23252573504578322, 0.2324053660267964, 0.23186330508906394, 0.23107463994529098]
Total Epoch List: [533, 11, 3]
Total Time List: [0.04771926999092102, 0.05113759997766465, 0.051786876982077956]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d1a89452d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.6456;  Loss pred: 2.6456; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 2.6344;  Loss pred: 2.6344; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 2.6717;  Loss pred: 2.6717; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 2.6513;  Loss pred: 2.6513; Loss self: 0.0000; time: 1.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.51s
Epoch 5/1000, LR 0.000090
Train loss: 2.6815;  Loss pred: 2.6815; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.39s
Epoch 6/1000, LR 0.000120
Train loss: 2.6026;  Loss pred: 2.6026; Loss self: 0.0000; time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.60s
Epoch 7/1000, LR 0.000150
Train loss: 2.5848;  Loss pred: 2.5848; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 2.5634;  Loss pred: 2.5634; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.32s
Epoch 9/1000, LR 0.000210
Train loss: 2.4770;  Loss pred: 2.4770; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.18s
     INFO: Early stopping counter 1 of 2
Epoch 10/1000, LR 0.000240
Train loss: 2.4884;  Loss pred: 2.4884; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 007,   Train_Loss: 2.5634,   Val_Loss: 0.6923,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.6923,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.6932


[0.0513054879847914, 0.05087170598562807, 0.05073950393125415, 0.5123441020259634, 0.3952328839804977, 0.6027891109697521, 0.23330782598350197, 0.32334646792151034, 0.18324758601374924, 0.051202005008235574]
[0.001166033817836168, 0.0011561751360370015, 0.0011531705438921397, 0.011644184136953712, 0.008982565545011312, 0.01369975252203982, 0.005302450590534136, 0.007348783361852507, 0.0041647178639488466, 0.001163681932005354]
[857.6080596493501, 864.9208660788884, 867.1744221152545, 85.8797824079768, 111.32676906046898, 72.99401930007312, 188.59204492828027, 136.0769464495299, 240.11230356234344, 859.3413479203174]
Elapsed: 0.24543866798048838~0.19561193988846945
Time per graph: 0.0055781515450111~0.004445725906556124
Speed: 428.4026561472483~357.15336869574315
Total Time: 0.0514
best val loss: 0.6923459768295288 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.05s
test Score 0.5000
Epoch Time List: [0.22854635189287364, 0.22825091204140335, 0.22860669496003538, 3.01909156399779, 1.3434162809280679, 1.9007264809915796, 1.2736709690652788, 1.1232845148770139, 0.7288478829432279, 0.3891998468898237]
Total Epoch List: [10]
Total Time List: [0.05143608001526445]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bfa6b30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 3.7499;  Loss pred: 3.7499; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.4884 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 3.7775;  Loss pred: 3.7775; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.4884 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 3.7247;  Loss pred: 3.7247; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.4884 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 3.7522;  Loss pred: 3.7522; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4884 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 3.7278;  Loss pred: 3.7278; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4884 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 3.7053;  Loss pred: 3.7053; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.4884 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 3.6817;  Loss pred: 3.6817; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4884 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 3.5587;  Loss pred: 3.5587; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 3.5706;  Loss pred: 3.5706; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 3.5634;  Loss pred: 3.5634; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 3.4503;  Loss pred: 3.4503; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 3.3986;  Loss pred: 3.3986; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 3.3515;  Loss pred: 3.3515; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 3.3258;  Loss pred: 3.3258; Loss self: 0.0000; time: 0.13s
Val loss: 0.6931 score: 0.5000 time: 0.04s
Test loss: 0.6931 score: 0.4651 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 3.2949;  Loss pred: 3.2949; Loss self: 0.0000; time: 0.13s
Val loss: 0.6930 score: 0.6591 time: 0.04s
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 3.2166;  Loss pred: 3.2166; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 3.1402;  Loss pred: 3.1402; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 3.0900;  Loss pred: 3.0900; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 3.0448;  Loss pred: 3.0448; Loss self: 0.0000; time: 0.13s
Val loss: 0.6929 score: 0.5227 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 3.0017;  Loss pred: 3.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.6929 score: 0.6591 time: 0.04s
Test loss: 0.6928 score: 0.6047 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 3.0138;  Loss pred: 3.0138; Loss self: 0.0000; time: 0.13s
Val loss: 0.6928 score: 0.5227 time: 0.04s
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 2.9390;  Loss pred: 2.9390; Loss self: 0.0000; time: 0.13s
Val loss: 0.6928 score: 0.4773 time: 0.04s
Test loss: 0.6929 score: 0.4884 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 2.9049;  Loss pred: 2.9049; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
Test loss: 0.6929 score: 0.5116 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 2.8523;  Loss pred: 2.8523; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 2.8288;  Loss pred: 2.8288; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 2.7167;  Loss pred: 2.7167; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 2.6967;  Loss pred: 2.6967; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 28/1000, LR 0.000270
Train loss: 2.6530;  Loss pred: 2.6530; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 025,   Train_Loss: 2.7167,   Val_Loss: 0.6928,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.5000,   Val_Loss: 0.6928,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.4884,   Test_loss: 0.6931


[0.0513054879847914, 0.05087170598562807, 0.05073950393125415, 0.5123441020259634, 0.3952328839804977, 0.6027891109697521, 0.23330782598350197, 0.32334646792151034, 0.18324758601374924, 0.051202005008235574, 0.04553925700020045, 0.04560476599726826, 0.045328682055696845, 0.04820904601365328, 0.0482196849770844, 0.048398007987998426, 0.04812268493697047, 0.04835996008478105, 0.04813533998094499, 0.04812275303993374, 0.04816255997866392, 0.04461480304598808, 0.04488811292685568, 0.04486228304449469, 0.045231298077851534, 0.045135750086046755, 0.04481655696872622, 0.04504651704337448, 0.04456731001846492, 0.044685250031761825, 0.04472573404200375, 0.044434698997065425, 0.04533366300165653, 0.04483446606900543, 0.04506159608718008, 0.04530135099776089, 0.045144564006477594, 0.044797113980166614]
[0.001166033817836168, 0.0011561751360370015, 0.0011531705438921397, 0.011644184136953712, 0.008982565545011312, 0.01369975252203982, 0.005302450590534136, 0.007348783361852507, 0.0041647178639488466, 0.001163681932005354, 0.0010590524883767546, 0.0010605759534248432, 0.0010541553966441127, 0.001121140604968681, 0.001121388022722893, 0.0011255350694883354, 0.0011191322078365226, 0.001124650234529792, 0.0011194265111847672, 0.001119133791626366, 0.001120059534387533, 0.001037553559209025, 0.001043909602950132, 0.0010433089080115045, 0.0010518906529732915, 0.00104966860665225, 0.0010422455109006098, 0.00104759341961336, 0.0010364490701968586, 0.0010391918612037633, 0.0010401333498140407, 0.0010333650929550099, 0.0010542712325966636, 0.0010426620016047775, 0.0010479440950506994, 0.001053519790645602, 0.0010498735815459906, 0.0010417933483759677]
[857.6080596493501, 864.9208660788884, 867.1744221152545, 85.8797824079768, 111.32676906046898, 72.99401930007312, 188.59204492828027, 136.0769464495299, 240.11230356234344, 859.3413479203174, 944.240262853009, 942.8839082865969, 948.626742493075, 891.9487846289672, 891.7519892640325, 888.4663189167412, 893.5494778880272, 889.1653327383982, 893.3145588464134, 893.5482133434319, 892.8096849305572, 963.8056668248972, 957.9373512552794, 958.4888927153426, 950.6691566974034, 952.6816308142622, 959.4668334295773, 954.5688062541234, 964.8327436002854, 962.2862123281403, 961.4151879456457, 967.7121927356777, 948.5225140185283, 959.0835749848794, 954.2493771593989, 949.1990647723808, 952.4956314525514, 959.8832643334509]
Elapsed: 0.09842290763902527~0.13338009615016785
Time per graph: 0.0022547668144631875~0.0030243434527676637
Speed: 803.4639457100935~290.59446534581315
Total Time: 0.0452
best val loss: 0.6927598118782043 test_score: 0.4884

Testing...
Test loss: 0.6930 score: 0.5116 time: 0.04s
test Score 0.5116
Epoch Time List: [0.22854635189287364, 0.22825091204140335, 0.22860669496003538, 3.01909156399779, 1.3434162809280679, 1.9007264809915796, 1.2736709690652788, 1.1232845148770139, 0.7288478829432279, 0.3891998468898237, 0.21734788292087615, 0.2171152930241078, 0.21627490501850843, 0.22307301801629364, 0.23076412419322878, 0.23031643510330468, 0.23028078291099519, 0.22967591602355242, 0.23046007694210857, 0.23021238297224045, 0.23014133190736175, 0.22724068991374224, 0.2145591500448063, 0.21525105193722993, 0.2157666350249201, 0.21612035587895662, 0.2136727700708434, 0.2129178128670901, 0.21271736396010965, 0.21334920008666813, 0.21329901705030352, 0.21296006790362298, 0.2132969059748575, 0.21542264008894563, 0.21383884001988918, 0.2138549129012972, 0.21392762602772564, 0.2135733150644228]
Total Epoch List: [10, 28]
Total Time List: [0.05143608001526445, 0.04517164395656437]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: AEtransGAT
AEtransGAT(
  (gae): GAE(
    (encoder): transGAT(
      (encoder): EdgeFeatureGAT()
      (classifier): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU(inplace=True)
        (2): Linear(in_features=64, out_features=2, bias=True)
      )
    )
    (decoder): InnerProductDecoder()
  )
)
Total number of parameters: 1914242
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x74d17bfa60e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 2.2313;  Loss pred: 2.2313; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 2.2820;  Loss pred: 2.2820; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 2.2296;  Loss pred: 2.2296; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 2.2029;  Loss pred: 2.2029; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 2.2055;  Loss pred: 2.2055; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 2.2126;  Loss pred: 2.2126; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 2.2021;  Loss pred: 2.2021; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 2.1307;  Loss pred: 2.1307; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.10s
Epoch 9/1000, LR 0.000210
Train loss: 2.1403;  Loss pred: 2.1403; Loss self: 0.0000; time: 0.13s
Val loss: 0.6933 score: 0.4545 time: 0.04s
Test loss: 0.6932 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 2.0577;  Loss pred: 2.0577; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 2
Epoch 11/1000, LR 0.000270
Train loss: 2.0526;  Loss pred: 2.0526; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 2.1403,   Val_Loss: 0.6933,   Val_Precision: 0.4688,   Val_Recall: 0.6818,   Val_accuracy: 0.5556,   Val_Score: 0.4545,   Val_Loss: 0.6933,   Test_Precision: 0.4857,   Test_Recall: 0.8095,   Test_accuracy: 0.6071,   Test_Score: 0.4884,   Test_loss: 0.6932


[0.0513054879847914, 0.05087170598562807, 0.05073950393125415, 0.5123441020259634, 0.3952328839804977, 0.6027891109697521, 0.23330782598350197, 0.32334646792151034, 0.18324758601374924, 0.051202005008235574, 0.04553925700020045, 0.04560476599726826, 0.045328682055696845, 0.04820904601365328, 0.0482196849770844, 0.048398007987998426, 0.04812268493697047, 0.04835996008478105, 0.04813533998094499, 0.04812275303993374, 0.04816255997866392, 0.04461480304598808, 0.04488811292685568, 0.04486228304449469, 0.045231298077851534, 0.045135750086046755, 0.04481655696872622, 0.04504651704337448, 0.04456731001846492, 0.044685250031761825, 0.04472573404200375, 0.044434698997065425, 0.04533366300165653, 0.04483446606900543, 0.04506159608718008, 0.04530135099776089, 0.045144564006477594, 0.044797113980166614, 0.047773347934708, 0.047840630053542554, 0.04787441994994879, 0.04786184011027217, 0.047513028024695814, 0.04830523196142167, 0.04807894700206816, 0.10170226008631289, 0.04712472192477435, 0.04679329798091203, 0.047402576892636716]
[0.001166033817836168, 0.0011561751360370015, 0.0011531705438921397, 0.011644184136953712, 0.008982565545011312, 0.01369975252203982, 0.005302450590534136, 0.007348783361852507, 0.0041647178639488466, 0.001163681932005354, 0.0010590524883767546, 0.0010605759534248432, 0.0010541553966441127, 0.001121140604968681, 0.001121388022722893, 0.0011255350694883354, 0.0011191322078365226, 0.001124650234529792, 0.0011194265111847672, 0.001119133791626366, 0.001120059534387533, 0.001037553559209025, 0.001043909602950132, 0.0010433089080115045, 0.0010518906529732915, 0.00104966860665225, 0.0010422455109006098, 0.00104759341961336, 0.0010364490701968586, 0.0010391918612037633, 0.0010401333498140407, 0.0010333650929550099, 0.0010542712325966636, 0.0010426620016047775, 0.0010479440950506994, 0.001053519790645602, 0.0010498735815459906, 0.0010417933483759677, 0.0011110080915048373, 0.00111257279194285, 0.001113358603487181, 0.001113066049076097, 0.001104954140109205, 0.0011233774874749225, 0.0011181150465597246, 0.002365168839216579, 0.0010959237656924267, 0.0010882162321142334, 0.0011023855091310865]
[857.6080596493501, 864.9208660788884, 867.1744221152545, 85.8797824079768, 111.32676906046898, 72.99401930007312, 188.59204492828027, 136.0769464495299, 240.11230356234344, 859.3413479203174, 944.240262853009, 942.8839082865969, 948.626742493075, 891.9487846289672, 891.7519892640325, 888.4663189167412, 893.5494778880272, 889.1653327383982, 893.3145588464134, 893.5482133434319, 892.8096849305572, 963.8056668248972, 957.9373512552794, 958.4888927153426, 950.6691566974034, 952.6816308142622, 959.4668334295773, 954.5688062541234, 964.8327436002854, 962.2862123281403, 961.4151879456457, 967.7121927356777, 948.5225140185283, 959.0835749848794, 954.2493771593989, 949.1990647723808, 952.4956314525514, 959.8832643334509, 900.0834536187049, 898.8175939964631, 898.1832060828133, 898.4192814344237, 905.0149356435443, 890.1727256861406, 894.3623494530842, 422.8027967471585, 912.4722278179448, 918.9350153849084, 907.1236801617722]
Elapsed: 0.08812940392253578~0.11923413969643389
Time per graph: 0.0020230466429777606~0.002703357361022645
Speed: 815.8779021022553~265.14294556388415
Total Time: 0.0478
best val loss: 0.6933119297027588 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.04s
test Score 0.5116
Epoch Time List: [0.22854635189287364, 0.22825091204140335, 0.22860669496003538, 3.01909156399779, 1.3434162809280679, 1.9007264809915796, 1.2736709690652788, 1.1232845148770139, 0.7288478829432279, 0.3891998468898237, 0.21734788292087615, 0.2171152930241078, 0.21627490501850843, 0.22307301801629364, 0.23076412419322878, 0.23031643510330468, 0.23028078291099519, 0.22967591602355242, 0.23046007694210857, 0.23021238297224045, 0.23014133190736175, 0.22724068991374224, 0.2145591500448063, 0.21525105193722993, 0.2157666350249201, 0.21612035587895662, 0.2136727700708434, 0.2129178128670901, 0.21271736396010965, 0.21334920008666813, 0.21329901705030352, 0.21296006790362298, 0.2132969059748575, 0.21542264008894563, 0.21383884001988918, 0.2138549129012972, 0.21392762602772564, 0.2135733150644228, 0.21715196105651557, 0.21737162303179502, 0.21773004007991403, 0.21720077598001808, 0.2173281020950526, 0.22048375895246863, 0.21841148706153035, 0.27826644107699394, 0.21517594682518393, 0.21245741099119186, 0.21374622406437993]
Total Epoch List: [10, 28, 11]
Total Time List: [0.05143608001526445, 0.04517164395656437, 0.04777011997066438]
T-times Epoch Time: 0.48835472864722423 ~ 0.10518941196567916
T-times Total Epoch: 102.55555555555556 ~ 67.92225039532651
T-times Total Time: 0.0495460842244534 ~ 0.001004761424478369
T-times Inference Elapsed: 0.10778047934604469 ~ 0.018340761834485023
T-times Time Per Graph: 0.0024803520364212925 ~ 0.00044036989283985736
T-times Speed: 819.127081522955 ~ 2.706961472734551
T-times cross validation test micro f1 score:0.5606747847880571 ~ 0.061295254798158526
T-times cross validation test precision:0.5829013686571826 ~ 0.18824030954473922
T-times cross validation test recall:0.7263107263107264 ~ 0.12280263507946108
T-times cross validation test f1_score:0.5606747847880571 ~ 0.11423577194698258
