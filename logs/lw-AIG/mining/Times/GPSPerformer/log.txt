Namespace(seed=60, model='GPSPerformer', dataset='mining/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Times/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 68], edge_attr=[68, 2], x=[24, 14887], y=[1, 1], num_nodes=25)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e9b2b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7445;  Loss pred: 0.7445; Loss self: 0.0000; time: 0.40s
Val loss: 1.0002 score: 0.4884 time: 0.07s
Test loss: 0.9593 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7987;  Loss pred: 0.7987; Loss self: 0.0000; time: 0.24s
Val loss: 0.8261 score: 0.4186 time: 0.17s
Test loss: 0.8073 score: 0.5000 time: 0.18s
Epoch 3/1000, LR 0.000030
Train loss: 0.7191;  Loss pred: 0.7191; Loss self: 0.0000; time: 0.36s
Val loss: 0.7441 score: 0.4186 time: 0.12s
Test loss: 0.7356 score: 0.5227 time: 0.10s
Epoch 4/1000, LR 0.000060
Train loss: 0.7771;  Loss pred: 0.7771; Loss self: 0.0000; time: 0.18s
Val loss: 0.7176 score: 0.4419 time: 0.40s
Test loss: 0.7130 score: 0.4545 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7078;  Loss pred: 0.7078; Loss self: 0.0000; time: 0.39s
Val loss: 0.7186 score: 0.4186 time: 0.22s
Test loss: 0.7131 score: 0.4545 time: 0.23s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 2.30s
Val loss: 0.7247 score: 0.3953 time: 0.20s
Test loss: 0.7177 score: 0.4545 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 1.04s
Val loss: 0.7282 score: 0.4186 time: 0.08s
Test loss: 0.7214 score: 0.4773 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6553;  Loss pred: 0.6553; Loss self: 0.0000; time: 0.48s
Val loss: 0.7290 score: 0.4186 time: 0.23s
Test loss: 0.7258 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.25s
Val loss: 0.7285 score: 0.4651 time: 0.21s
Test loss: 0.7278 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.19s
Val loss: 0.7264 score: 0.4186 time: 0.05s
Test loss: 0.7279 score: 0.5227 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6361;  Loss pred: 0.6361; Loss self: 0.0000; time: 0.25s
Val loss: 0.7239 score: 0.4186 time: 0.07s
Test loss: 0.7264 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.19s
Val loss: 0.7193 score: 0.4186 time: 0.09s
Test loss: 0.7261 score: 0.4773 time: 0.29s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.26s
Val loss: 0.7134 score: 0.4419 time: 0.13s
Test loss: 0.7225 score: 0.4773 time: 0.11s
Epoch 14/1000, LR 0.000270
Train loss: 0.6197;  Loss pred: 0.6197; Loss self: 0.0000; time: 0.28s
Val loss: 0.7079 score: 0.4419 time: 0.12s
Test loss: 0.7186 score: 0.4773 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 0.5988;  Loss pred: 0.5988; Loss self: 0.0000; time: 0.25s
Val loss: 0.7030 score: 0.4419 time: 0.14s
Test loss: 0.7143 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.5424;  Loss pred: 0.5424; Loss self: 0.0000; time: 0.17s
Val loss: 0.6978 score: 0.4651 time: 0.16s
Test loss: 0.7101 score: 0.5227 time: 0.14s
Epoch 17/1000, LR 0.000270
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.16s
Val loss: 0.6924 score: 0.4651 time: 0.07s
Test loss: 0.7057 score: 0.5682 time: 0.12s
Epoch 18/1000, LR 0.000270
Train loss: 0.5194;  Loss pred: 0.5194; Loss self: 0.0000; time: 0.30s
Val loss: 0.6882 score: 0.4651 time: 0.07s
Test loss: 0.7011 score: 0.5682 time: 0.07s
Epoch 19/1000, LR 0.000270
Train loss: 0.5040;  Loss pred: 0.5040; Loss self: 0.0000; time: 0.13s
Val loss: 0.6824 score: 0.5349 time: 0.15s
Test loss: 0.6952 score: 0.5682 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.18s
Val loss: 0.6763 score: 0.5581 time: 0.07s
Test loss: 0.6895 score: 0.5682 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.4904;  Loss pred: 0.4904; Loss self: 0.0000; time: 0.18s
Val loss: 0.6702 score: 0.5581 time: 0.06s
Test loss: 0.6843 score: 0.5682 time: 0.14s
Epoch 22/1000, LR 0.000270
Train loss: 0.4431;  Loss pred: 0.4431; Loss self: 0.0000; time: 0.14s
Val loss: 0.6647 score: 0.5116 time: 0.14s
Test loss: 0.6809 score: 0.5227 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.4717;  Loss pred: 0.4717; Loss self: 0.0000; time: 0.17s
Val loss: 0.6608 score: 0.5116 time: 0.06s
Test loss: 0.6790 score: 0.5227 time: 0.10s
Epoch 24/1000, LR 0.000270
Train loss: 0.4189;  Loss pred: 0.4189; Loss self: 0.0000; time: 0.16s
Val loss: 0.6581 score: 0.5116 time: 0.12s
Test loss: 0.6768 score: 0.5227 time: 0.13s
Epoch 25/1000, LR 0.000270
Train loss: 0.4034;  Loss pred: 0.4034; Loss self: 0.0000; time: 0.19s
Val loss: 0.6560 score: 0.5116 time: 0.07s
Test loss: 0.6748 score: 0.5227 time: 0.12s
Epoch 26/1000, LR 0.000270
Train loss: 0.4018;  Loss pred: 0.4018; Loss self: 0.0000; time: 0.21s
Val loss: 0.6543 score: 0.5581 time: 0.08s
Test loss: 0.6722 score: 0.5455 time: 0.09s
Epoch 27/1000, LR 0.000270
Train loss: 0.3882;  Loss pred: 0.3882; Loss self: 0.0000; time: 0.18s
Val loss: 0.6520 score: 0.5349 time: 0.16s
Test loss: 0.6698 score: 0.5455 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.4241;  Loss pred: 0.4241; Loss self: 0.0000; time: 0.29s
Val loss: 0.6481 score: 0.5349 time: 0.06s
Test loss: 0.6670 score: 0.5455 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.3311;  Loss pred: 0.3311; Loss self: 0.0000; time: 0.17s
Val loss: 0.6491 score: 0.5349 time: 0.05s
Test loss: 0.6670 score: 0.5455 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.3642;  Loss pred: 0.3642; Loss self: 0.0000; time: 0.20s
Val loss: 0.6509 score: 0.5116 time: 0.06s
Test loss: 0.6670 score: 0.5455 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.3642;  Loss pred: 0.3642; Loss self: 0.0000; time: 0.28s
Val loss: 0.6447 score: 0.5116 time: 0.06s
Test loss: 0.6632 score: 0.5682 time: 0.07s
Epoch 32/1000, LR 0.000270
Train loss: 0.3347;  Loss pred: 0.3347; Loss self: 0.0000; time: 0.19s
Val loss: 0.6349 score: 0.5116 time: 0.05s
Test loss: 0.6568 score: 0.5682 time: 0.10s
Epoch 33/1000, LR 0.000270
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.23s
Val loss: 0.6173 score: 0.5581 time: 0.14s
Test loss: 0.6462 score: 0.5682 time: 0.11s
Epoch 34/1000, LR 0.000270
Train loss: 0.2914;  Loss pred: 0.2914; Loss self: 0.0000; time: 0.18s
Val loss: 0.5994 score: 0.5581 time: 0.05s
Test loss: 0.6349 score: 0.5682 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.2973;  Loss pred: 0.2973; Loss self: 0.0000; time: 0.20s
Val loss: 0.5792 score: 0.5814 time: 0.09s
Test loss: 0.6222 score: 0.5909 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.2709;  Loss pred: 0.2709; Loss self: 0.0000; time: 0.30s
Val loss: 0.5621 score: 0.6047 time: 0.05s
Test loss: 0.6105 score: 0.6136 time: 0.10s
Epoch 37/1000, LR 0.000270
Train loss: 0.2273;  Loss pred: 0.2273; Loss self: 0.0000; time: 0.21s
Val loss: 0.5481 score: 0.6279 time: 0.18s
Test loss: 0.6022 score: 0.6364 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.2208;  Loss pred: 0.2208; Loss self: 0.0000; time: 0.39s
Val loss: 0.5366 score: 0.6279 time: 0.13s
Test loss: 0.5979 score: 0.6136 time: 0.10s
Epoch 39/1000, LR 0.000269
Train loss: 0.2092;  Loss pred: 0.2092; Loss self: 0.0000; time: 0.19s
Val loss: 0.5345 score: 0.6047 time: 0.12s
Test loss: 0.5993 score: 0.6136 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.30s
Val loss: 0.5466 score: 0.6279 time: 0.15s
Test loss: 0.6073 score: 0.6136 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1905;  Loss pred: 0.1905; Loss self: 0.0000; time: 0.19s
Val loss: 0.5664 score: 0.6744 time: 0.06s
Test loss: 0.6175 score: 0.6136 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 0.29s
Val loss: 0.5728 score: 0.6744 time: 0.14s
Test loss: 0.6142 score: 0.6136 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.1824;  Loss pred: 0.1824; Loss self: 0.0000; time: 0.18s
Val loss: 0.5733 score: 0.6744 time: 0.11s
Test loss: 0.6112 score: 0.6364 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.1705;  Loss pred: 0.1705; Loss self: 0.0000; time: 0.23s
Val loss: 0.5755 score: 0.6744 time: 0.14s
Test loss: 0.6121 score: 0.6591 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.1638;  Loss pred: 0.1638; Loss self: 0.0000; time: 0.17s
Val loss: 0.5690 score: 0.7442 time: 0.06s
Test loss: 0.6073 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.1494;  Loss pred: 0.1494; Loss self: 0.0000; time: 0.29s
Val loss: 0.5628 score: 0.7907 time: 0.14s
Test loss: 0.6022 score: 0.7045 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 0.17s
Val loss: 0.5480 score: 0.7907 time: 0.06s
Test loss: 0.5857 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 0.27s
Val loss: 0.5178 score: 0.7907 time: 0.14s
Test loss: 0.5639 score: 0.7955 time: 0.10s
Epoch 49/1000, LR 0.000269
Train loss: 0.1155;  Loss pred: 0.1155; Loss self: 0.0000; time: 0.24s
Val loss: 0.4857 score: 0.8140 time: 0.34s
Test loss: 0.5394 score: 0.7727 time: 0.07s
Epoch 50/1000, LR 0.000269
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 0.13s
Val loss: 0.4557 score: 0.8140 time: 0.11s
Test loss: 0.5178 score: 0.7727 time: 0.07s
Epoch 51/1000, LR 0.000269
Train loss: 0.1002;  Loss pred: 0.1002; Loss self: 0.0000; time: 0.19s
Val loss: 0.4432 score: 0.8140 time: 0.05s
Test loss: 0.5050 score: 0.7727 time: 0.11s
Epoch 52/1000, LR 0.000269
Train loss: 0.1026;  Loss pred: 0.1026; Loss self: 0.0000; time: 0.22s
Val loss: 0.4426 score: 0.8140 time: 0.06s
Test loss: 0.4990 score: 0.7955 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.0963;  Loss pred: 0.0963; Loss self: 0.0000; time: 0.16s
Val loss: 0.4381 score: 0.8140 time: 0.14s
Test loss: 0.4905 score: 0.7955 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.0925;  Loss pred: 0.0925; Loss self: 0.0000; time: 0.16s
Val loss: 0.4389 score: 0.8140 time: 0.05s
Test loss: 0.4854 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0910;  Loss pred: 0.0910; Loss self: 0.0000; time: 0.14s
Val loss: 0.4311 score: 0.7907 time: 0.05s
Test loss: 0.4743 score: 0.7955 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.14s
Val loss: 0.4275 score: 0.8140 time: 0.07s
Test loss: 0.4676 score: 0.8182 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 0.14s
Val loss: 0.4271 score: 0.8140 time: 0.06s
Test loss: 0.4647 score: 0.8182 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0686;  Loss pred: 0.0686; Loss self: 0.0000; time: 0.16s
Val loss: 0.4156 score: 0.8372 time: 0.09s
Test loss: 0.4533 score: 0.8182 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0758;  Loss pred: 0.0758; Loss self: 0.0000; time: 0.14s
Val loss: 0.4014 score: 0.8140 time: 0.06s
Test loss: 0.4410 score: 0.8409 time: 0.08s
Epoch 60/1000, LR 0.000268
Train loss: 0.0628;  Loss pred: 0.0628; Loss self: 0.0000; time: 0.17s
Val loss: 0.3952 score: 0.8140 time: 0.11s
Test loss: 0.4352 score: 0.8409 time: 0.06s
Epoch 61/1000, LR 0.000268
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.14s
Val loss: 0.3966 score: 0.8140 time: 0.05s
Test loss: 0.4364 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.15s
Val loss: 0.4021 score: 0.8140 time: 0.06s
Test loss: 0.4435 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0527;  Loss pred: 0.0527; Loss self: 0.0000; time: 0.14s
Val loss: 0.4150 score: 0.8372 time: 0.06s
Test loss: 0.4631 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0534;  Loss pred: 0.0534; Loss self: 0.0000; time: 0.14s
Val loss: 0.4274 score: 0.8372 time: 0.06s
Test loss: 0.4826 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.30s
Val loss: 0.4234 score: 0.8372 time: 0.06s
Test loss: 0.4835 score: 0.7727 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0424;  Loss pred: 0.0424; Loss self: 0.0000; time: 0.15s
Val loss: 0.4059 score: 0.8372 time: 0.06s
Test loss: 0.4677 score: 0.7727 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.23s
Val loss: 0.3815 score: 0.8372 time: 0.26s
Test loss: 0.4438 score: 0.7727 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.25s
Val loss: 0.3540 score: 0.8372 time: 0.07s
Test loss: 0.4180 score: 0.7955 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.15s
Val loss: 0.3356 score: 0.8837 time: 0.07s
Test loss: 0.3991 score: 0.8182 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.24s
Val loss: 0.3264 score: 0.9070 time: 0.26s
Test loss: 0.3926 score: 0.8182 time: 0.08s
Epoch 71/1000, LR 0.000268
Train loss: 0.0430;  Loss pred: 0.0430; Loss self: 0.0000; time: 0.39s
Val loss: 0.3280 score: 0.9070 time: 0.08s
Test loss: 0.3961 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.75s
Val loss: 0.3404 score: 0.9070 time: 0.27s
Test loss: 0.4089 score: 0.7955 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0337;  Loss pred: 0.0337; Loss self: 0.0000; time: 0.54s
Val loss: 0.3719 score: 0.8837 time: 0.13s
Test loss: 0.4455 score: 0.8182 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.32s
Val loss: 0.4205 score: 0.8605 time: 0.15s
Test loss: 0.5050 score: 0.7955 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.27s
Val loss: 0.4724 score: 0.8372 time: 0.05s
Test loss: 0.5648 score: 0.7727 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.14s
Val loss: 0.4760 score: 0.8372 time: 0.06s
Test loss: 0.5697 score: 0.7727 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0258;  Loss pred: 0.0258; Loss self: 0.0000; time: 0.14s
Val loss: 0.4580 score: 0.8605 time: 0.06s
Test loss: 0.5524 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.14s
Val loss: 0.4243 score: 0.8372 time: 0.10s
Test loss: 0.5136 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0243;  Loss pred: 0.0243; Loss self: 0.0000; time: 0.13s
Val loss: 0.3919 score: 0.8372 time: 0.06s
Test loss: 0.4725 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.29s
Val loss: 0.3778 score: 0.8605 time: 0.06s
Test loss: 0.4564 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.14s
Val loss: 0.3717 score: 0.8605 time: 0.06s
Test loss: 0.4509 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.17s
Val loss: 0.3582 score: 0.8837 time: 0.06s
Test loss: 0.4362 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.13s
Val loss: 0.3545 score: 0.8837 time: 0.06s
Test loss: 0.4301 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.17s
Val loss: 0.3677 score: 0.8837 time: 0.07s
Test loss: 0.4490 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.22s
Val loss: 0.3969 score: 0.8837 time: 0.14s
Test loss: 0.4950 score: 0.7955 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.24s
Val loss: 0.4296 score: 0.8372 time: 0.10s
Test loss: 0.5405 score: 0.7955 time: 0.13s
     INFO: Early stopping counter 16 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.30s
Val loss: 0.4536 score: 0.8372 time: 0.15s
Test loss: 0.5780 score: 0.7955 time: 0.10s
     INFO: Early stopping counter 17 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.14s
Val loss: 0.4719 score: 0.8372 time: 0.14s
Test loss: 0.6037 score: 0.7955 time: 0.26s
     INFO: Early stopping counter 18 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.15s
Val loss: 0.4818 score: 0.8372 time: 0.07s
Test loss: 0.6165 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.19s
Val loss: 0.4812 score: 0.8372 time: 0.08s
Test loss: 0.6135 score: 0.7955 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 069,   Train_Loss: 0.0400,   Val_Loss: 0.3264,   Val_Precision: 0.8750,   Val_Recall: 0.9545,   Val_accuracy: 0.9130,   Val_Score: 0.9070,   Val_Loss: 0.3264,   Test_Precision: 0.7333,   Test_Recall: 1.0000,   Test_accuracy: 0.8462,   Test_Score: 0.8182,   Test_loss: 0.3926


[0.07817787607200444, 0.18715743196662515, 0.10836088401265442, 0.08249276201240718, 0.24026729597244412, 0.0538825299590826, 0.15814112301450223, 0.14972497802227736, 0.05415725603234023, 0.08816591708455235, 0.055279394960962236, 0.29787713708356023, 0.11414051603060216, 0.10357435198966414, 0.05503704503644258, 0.14809274894651026, 0.12454730900935829, 0.07097878004424274, 0.08510718692559749, 0.07101422001142055, 0.1459593849722296, 0.06009215803351253, 0.10866432101465762, 0.13734256592579186, 0.12231818994041532, 0.09949472907464951, 0.05017339496407658, 0.0777217058930546, 0.10464374104049057, 0.05059477605391294, 0.07286150404252112, 0.10465692903380841, 0.11474718409590423, 0.06713303492870182, 0.053151287022046745, 0.10306187998503447, 0.05181479104794562, 0.11107332201208919, 0.07217698497697711, 0.11545445502270013, 0.07156648801174015, 0.09823879797477275, 0.11722268094308674, 0.10123336396645755, 0.06971952400635928, 0.10454829398076981, 0.07907373295165598, 0.10722941695712507, 0.07911491999402642, 0.0744080679723993, 0.11422912694979459, 0.07602204196155071, 0.24332184996455908, 0.054593705921433866, 0.05732961499597877, 0.05422020296100527, 0.060327618033625185, 0.06498506106436253, 0.08747386105824262, 0.06386870401911438, 0.057631614967249334, 0.059978723991662264, 0.055833760066889226, 0.058077023015357554, 0.06414843804668635, 0.06945808394812047, 0.055428875028155744, 0.06038570206146687, 0.06575274397619069, 0.08581747894641012, 0.060653446009382606, 0.13432132196612656, 0.1269124469254166, 0.10995649104006588, 0.061119148042052984, 0.062020947923883796, 0.05568422598298639, 0.05305616103578359, 0.05566504807211459, 0.0598726540338248, 0.055208327947184443, 0.06121435505338013, 0.06230943603441119, 0.05679467797745019, 0.09751246497035027, 0.13382228498812765, 0.1075464989989996, 0.2677892600186169, 0.05847543792333454, 0.1027983269887045]
[0.0017767699107273736, 0.004253577999241481, 0.0024627473639239643, 0.0018748355002819815, 0.005460620363010094, 0.0012246029536155138, 0.0035941164321477777, 0.0034028404095972128, 0.0012308467280077325, 0.002003770842830735, 0.0012563498854764146, 0.006769934933717278, 0.00259410263705914, 0.0023539625452196396, 0.0012508419326464223, 0.0033657442942388693, 0.0028306206593035972, 0.0016131540919146078, 0.0019342542483090338, 0.0016139595457141033, 0.0033172587493688547, 0.001365730864398012, 0.0024696436594240367, 0.003121421952858906, 0.0027799588622821666, 0.0022612438426056706, 0.0011403044310017403, 0.001766402406660332, 0.002378266841829331, 0.0011498812739525667, 0.0016559432736936617, 0.002378566568950191, 0.002607890547634187, 0.0015257507938341323, 0.0012079837959556078, 0.002342315454205329, 0.0011776088874533095, 0.002524393682092936, 0.0016403860222040253, 0.0026239648868795484, 0.0016265110911759125, 0.002232699953972108, 0.002664151839615608, 0.002300758271964944, 0.0015845346365081655, 0.0023760975904720412, 0.0017971302943558178, 0.002437032203571024, 0.0017980663635006006, 0.0016910924539181658, 0.0025961165215862407, 0.0017277736809443343, 0.00553004204464907, 0.0012407660436689514, 0.0013029457953631538, 0.001232277340022847, 0.001371082228036936, 0.0014769332060082393, 0.0019880422967782415, 0.0014515614549798722, 0.0013098094310738486, 0.0013631528179923241, 0.0012689490924293007, 0.0013199323412581261, 0.001457919046515599, 0.001578592817002738, 0.0012597471597308124, 0.0013724023195787925, 0.0014943805449134247, 0.0019503972487820481, 0.00137848740930415, 0.0030527573174119675, 0.002884373793759468, 0.0024990111600014975, 0.001389071546410295, 0.0014095669982700863, 0.001265550590522418, 0.0012058218417223543, 0.0012651147289116952, 0.001360742137132382, 0.0012547347260723736, 0.0013912353421222758, 0.001416123546236618, 0.0012907881358511406, 0.002216192385689779, 0.003041415567911992, 0.002444238613613627, 0.006086119545877657, 0.0013289872255303305, 0.0023363256133796476]
[562.8190763263322, 235.0961943517493, 406.0505818212194, 533.3801284697226, 183.1293760639246, 816.5912037428975, 278.232499942251, 293.87214198457457, 812.448842934827, 499.0590633544183, 795.9566133289333, 147.71190709966157, 385.48975885305424, 424.81559531640454, 799.4615257935008, 297.11110309588753, 353.2794112532284, 619.90358206458, 516.9951162698602, 619.5942163826329, 301.45372295431014, 732.2086847914803, 404.9167158930196, 320.36681201786945, 359.7175532227353, 442.23448226073776, 876.9587952241095, 566.1224170831272, 420.4742640362481, 869.6550006094374, 603.8854204041975, 420.4212793764111, 383.4516754958044, 655.4150284838142, 827.8256739436835, 426.9279777002826, 849.1783737829921, 396.1347261695392, 609.6126073156844, 381.1026607102248, 614.812899478622, 447.88821633687934, 375.3539813797862, 434.63931530101945, 631.1001204768216, 420.8581347878635, 556.4426815020947, 410.33516033751346, 556.1529987431223, 591.3337249439298, 385.1907230223221, 578.7795074256708, 180.83045154559198, 805.9537131133883, 767.4916359212644, 811.5056306898084, 729.3508584322929, 677.0786897687378, 503.0074066434947, 688.9133054403586, 763.4698424641431, 733.5934656782047, 788.0536784068939, 757.6145903409149, 685.9091404217419, 633.4755797880116, 793.8100850441162, 728.6493076657816, 669.1735939709613, 512.7160636759837, 725.4328137133964, 327.5727141153064, 346.6957029506944, 400.15827700401337, 719.905322792226, 709.4377218161791, 790.1699129919422, 829.3099074832104, 790.442145006281, 734.893094519284, 796.9812098293035, 718.7856502225846, 706.1530772915427, 774.7204767578716, 451.22436411979413, 328.79426624574194, 409.12535888694333, 164.30830720000154, 752.4526803490917, 428.02253002458616]
Elapsed: 0.09270317292151352~0.047395812607088986
Time per graph: 0.0021068902936707615~0.0010771775592520224
Speed: 561.8570205110299~194.56918243226698
Total Time: 0.1037
best val loss: 0.32638102769851685 test_score: 0.8182

Testing...
Test loss: 0.3926 score: 0.8182 time: 0.08s
test Score 0.8182
Epoch Time List: [0.534372441121377, 0.5870699569350109, 0.5878437240608037, 0.6556914530228823, 0.8454219148261473, 2.548865271033719, 1.275696034077555, 0.8480014899978414, 0.501764185144566, 0.32009117887355387, 0.3630479439161718, 0.5781870650826022, 0.489841116941534, 0.503419512999244, 0.444738598074764, 0.4778377279872075, 0.35429201694205403, 0.4334925019647926, 0.36262192495632917, 0.3074229540070519, 0.3831784059293568, 0.33144825289491564, 0.33324444794561714, 0.40409336995799094, 0.3815480340272188, 0.3854925730265677, 0.3812997799832374, 0.4157088450156152, 0.32389982894528657, 0.3026494571240619, 0.3977325869491324, 0.3378820400685072, 0.4804638840723783, 0.29770529898814857, 0.33975651091896, 0.44952807610388845, 0.436183353071101, 0.624627573066391, 0.37159465183503926, 0.5556284879567102, 0.3079379330156371, 0.5258591329911724, 0.39436580101028085, 0.4589794591302052, 0.296481047058478, 0.5274444359820336, 0.2934090949129313, 0.5090238609118387, 0.6595762309152633, 0.31056449096649885, 0.356258892104961, 0.34682694100774825, 0.5334683139808476, 0.26233364397194237, 0.24791688995901495, 0.2562859619501978, 0.2563798010814935, 0.31347635912243277, 0.2841022699140012, 0.3379472979577258, 0.24899251700844616, 0.2671848100144416, 0.24305619904771447, 0.2539011419285089, 0.4206208400428295, 0.2705510649830103, 0.5428781590890139, 0.3711378868902102, 0.28140052780508995, 0.5796008928446099, 0.531951230019331, 1.152573607978411, 0.7870432729832828, 0.5734154309611768, 0.37794298597145826, 0.24869435094296932, 0.2463589730905369, 0.2857665990013629, 0.24168383795768023, 0.40840602689422667, 0.24729409581050277, 0.27818500506691635, 0.2470443529309705, 0.2890633438946679, 0.4460507279727608, 0.47466192895080894, 0.5482969029108062, 0.5407487470656633, 0.27583460498135537, 0.37077230506110936]
Total Epoch List: [90]
Total Time List: [0.10373744799289852]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e991b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7074;  Loss pred: 0.7074; Loss self: 0.0000; time: 0.18s
Val loss: 0.7088 score: 0.4773 time: 0.08s
Test loss: 0.7085 score: 0.4884 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.15s
Val loss: 0.7366 score: 0.4773 time: 0.14s
Test loss: 0.7655 score: 0.4419 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7366;  Loss pred: 0.7366; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7824 score: 0.5000 time: 0.06s
Test loss: 0.8371 score: 0.4419 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8159 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8728 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8412 score: 0.5000 time: 0.15s
Test loss: 0.8986 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.7752;  Loss pred: 0.7752; Loss self: 0.0000; time: 0.51s
Val loss: 0.8792 score: 0.4773 time: 0.07s
Test loss: 0.9359 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.24s
Val loss: 0.9178 score: 0.5000 time: 0.05s
Test loss: 0.9807 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6471;  Loss pred: 0.6471; Loss self: 0.0000; time: 0.29s
Val loss: 0.9546 score: 0.5000 time: 0.08s
Test loss: 1.0210 score: 0.5349 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.24s
Val loss: 0.9741 score: 0.5000 time: 0.18s
Test loss: 1.0278 score: 0.5349 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5811;  Loss pred: 0.5811; Loss self: 0.0000; time: 0.15s
Val loss: 0.9811 score: 0.5000 time: 0.05s
Test loss: 1.0284 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.29s
Val loss: 0.9718 score: 0.5000 time: 0.07s
Test loss: 1.0116 score: 0.5116 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5929;  Loss pred: 0.5929; Loss self: 0.0000; time: 0.17s
Val loss: 0.9626 score: 0.4545 time: 0.05s
Test loss: 0.9917 score: 0.4884 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5290;  Loss pred: 0.5290; Loss self: 0.0000; time: 0.23s
Val loss: 0.9477 score: 0.4773 time: 0.05s
Test loss: 0.9598 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5278;  Loss pred: 0.5278; Loss self: 0.0000; time: 0.32s
Val loss: 0.9214 score: 0.5000 time: 0.06s
Test loss: 0.9191 score: 0.5581 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5345;  Loss pred: 0.5345; Loss self: 0.0000; time: 0.19s
Val loss: 0.8929 score: 0.5455 time: 0.06s
Test loss: 0.8779 score: 0.6047 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4948;  Loss pred: 0.4948; Loss self: 0.0000; time: 0.17s
Val loss: 0.8685 score: 0.5682 time: 0.05s
Test loss: 0.8400 score: 0.6512 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4767;  Loss pred: 0.4767; Loss self: 0.0000; time: 0.30s
Val loss: 0.8498 score: 0.6136 time: 0.09s
Test loss: 0.8145 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4191;  Loss pred: 0.4191; Loss self: 0.0000; time: 0.40s
Val loss: 0.8377 score: 0.5909 time: 0.06s
Test loss: 0.7996 score: 0.6279 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4287;  Loss pred: 0.4287; Loss self: 0.0000; time: 0.21s
Val loss: 0.8309 score: 0.5682 time: 0.06s
Test loss: 0.7907 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3844;  Loss pred: 0.3844; Loss self: 0.0000; time: 0.29s
Val loss: 0.8233 score: 0.5909 time: 0.07s
Test loss: 0.7823 score: 0.5814 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3472;  Loss pred: 0.3472; Loss self: 0.0000; time: 0.19s
Val loss: 0.8213 score: 0.5455 time: 0.07s
Test loss: 0.7782 score: 0.5814 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.7074,   Val_Loss: 0.7088,   Val_Precision: 0.4737,   Val_Recall: 0.4091,   Val_accuracy: 0.4390,   Val_Score: 0.4773,   Val_Loss: 0.7088,   Test_Precision: 0.5000,   Test_Recall: 0.4545,   Test_accuracy: 0.4762,   Test_Score: 0.4884,   Test_loss: 0.7085


[0.07817787607200444, 0.18715743196662515, 0.10836088401265442, 0.08249276201240718, 0.24026729597244412, 0.0538825299590826, 0.15814112301450223, 0.14972497802227736, 0.05415725603234023, 0.08816591708455235, 0.055279394960962236, 0.29787713708356023, 0.11414051603060216, 0.10357435198966414, 0.05503704503644258, 0.14809274894651026, 0.12454730900935829, 0.07097878004424274, 0.08510718692559749, 0.07101422001142055, 0.1459593849722296, 0.06009215803351253, 0.10866432101465762, 0.13734256592579186, 0.12231818994041532, 0.09949472907464951, 0.05017339496407658, 0.0777217058930546, 0.10464374104049057, 0.05059477605391294, 0.07286150404252112, 0.10465692903380841, 0.11474718409590423, 0.06713303492870182, 0.053151287022046745, 0.10306187998503447, 0.05181479104794562, 0.11107332201208919, 0.07217698497697711, 0.11545445502270013, 0.07156648801174015, 0.09823879797477275, 0.11722268094308674, 0.10123336396645755, 0.06971952400635928, 0.10454829398076981, 0.07907373295165598, 0.10722941695712507, 0.07911491999402642, 0.0744080679723993, 0.11422912694979459, 0.07602204196155071, 0.24332184996455908, 0.054593705921433866, 0.05732961499597877, 0.05422020296100527, 0.060327618033625185, 0.06498506106436253, 0.08747386105824262, 0.06386870401911438, 0.057631614967249334, 0.059978723991662264, 0.055833760066889226, 0.058077023015357554, 0.06414843804668635, 0.06945808394812047, 0.055428875028155744, 0.06038570206146687, 0.06575274397619069, 0.08581747894641012, 0.060653446009382606, 0.13432132196612656, 0.1269124469254166, 0.10995649104006588, 0.061119148042052984, 0.062020947923883796, 0.05568422598298639, 0.05305616103578359, 0.05566504807211459, 0.0598726540338248, 0.055208327947184443, 0.06121435505338013, 0.06230943603441119, 0.05679467797745019, 0.09751246497035027, 0.13382228498812765, 0.1075464989989996, 0.2677892600186169, 0.05847543792333454, 0.1027983269887045, 0.106169275008142, 0.07807935902383178, 0.0827130830148235, 0.06873180798720568, 0.1419521280331537, 0.08034870494157076, 0.05310828797519207, 0.09139219392091036, 0.08168519602622837, 0.05588647397235036, 0.0783628550125286, 0.13068708300124854, 0.050649059005081654, 0.0877474220469594, 0.13896490610204637, 0.04735059605445713, 0.076532636070624, 0.08538460894487798, 0.055414104950614274, 0.09261479601264, 0.10671081696636975]
[0.0017767699107273736, 0.004253577999241481, 0.0024627473639239643, 0.0018748355002819815, 0.005460620363010094, 0.0012246029536155138, 0.0035941164321477777, 0.0034028404095972128, 0.0012308467280077325, 0.002003770842830735, 0.0012563498854764146, 0.006769934933717278, 0.00259410263705914, 0.0023539625452196396, 0.0012508419326464223, 0.0033657442942388693, 0.0028306206593035972, 0.0016131540919146078, 0.0019342542483090338, 0.0016139595457141033, 0.0033172587493688547, 0.001365730864398012, 0.0024696436594240367, 0.003121421952858906, 0.0027799588622821666, 0.0022612438426056706, 0.0011403044310017403, 0.001766402406660332, 0.002378266841829331, 0.0011498812739525667, 0.0016559432736936617, 0.002378566568950191, 0.002607890547634187, 0.0015257507938341323, 0.0012079837959556078, 0.002342315454205329, 0.0011776088874533095, 0.002524393682092936, 0.0016403860222040253, 0.0026239648868795484, 0.0016265110911759125, 0.002232699953972108, 0.002664151839615608, 0.002300758271964944, 0.0015845346365081655, 0.0023760975904720412, 0.0017971302943558178, 0.002437032203571024, 0.0017980663635006006, 0.0016910924539181658, 0.0025961165215862407, 0.0017277736809443343, 0.00553004204464907, 0.0012407660436689514, 0.0013029457953631538, 0.001232277340022847, 0.001371082228036936, 0.0014769332060082393, 0.0019880422967782415, 0.0014515614549798722, 0.0013098094310738486, 0.0013631528179923241, 0.0012689490924293007, 0.0013199323412581261, 0.001457919046515599, 0.001578592817002738, 0.0012597471597308124, 0.0013724023195787925, 0.0014943805449134247, 0.0019503972487820481, 0.00137848740930415, 0.0030527573174119675, 0.002884373793759468, 0.0024990111600014975, 0.001389071546410295, 0.0014095669982700863, 0.001265550590522418, 0.0012058218417223543, 0.0012651147289116952, 0.001360742137132382, 0.0012547347260723736, 0.0013912353421222758, 0.001416123546236618, 0.0012907881358511406, 0.002216192385689779, 0.003041415567911992, 0.002444238613613627, 0.006086119545877657, 0.0013289872255303305, 0.0023363256133796476, 0.002469052907166093, 0.0018157990470658554, 0.0019235600701121743, 0.0015984141392373415, 0.003301212279840784, 0.0018685745335249014, 0.0012350764645393504, 0.0021253998586258224, 0.0018996557215401947, 0.0012996854412174502, 0.0018223919770355488, 0.0030392344884011286, 0.0011778850931414338, 0.002040637722022312, 0.0032317420023731712, 0.0011011766524292356, 0.001779828745828465, 0.0019856885801134414, 0.0012887001151305644, 0.0021538324654102325, 0.0024816469061946455]
[562.8190763263322, 235.0961943517493, 406.0505818212194, 533.3801284697226, 183.1293760639246, 816.5912037428975, 278.232499942251, 293.87214198457457, 812.448842934827, 499.0590633544183, 795.9566133289333, 147.71190709966157, 385.48975885305424, 424.81559531640454, 799.4615257935008, 297.11110309588753, 353.2794112532284, 619.90358206458, 516.9951162698602, 619.5942163826329, 301.45372295431014, 732.2086847914803, 404.9167158930196, 320.36681201786945, 359.7175532227353, 442.23448226073776, 876.9587952241095, 566.1224170831272, 420.4742640362481, 869.6550006094374, 603.8854204041975, 420.4212793764111, 383.4516754958044, 655.4150284838142, 827.8256739436835, 426.9279777002826, 849.1783737829921, 396.1347261695392, 609.6126073156844, 381.1026607102248, 614.812899478622, 447.88821633687934, 375.3539813797862, 434.63931530101945, 631.1001204768216, 420.8581347878635, 556.4426815020947, 410.33516033751346, 556.1529987431223, 591.3337249439298, 385.1907230223221, 578.7795074256708, 180.83045154559198, 805.9537131133883, 767.4916359212644, 811.5056306898084, 729.3508584322929, 677.0786897687378, 503.0074066434947, 688.9133054403586, 763.4698424641431, 733.5934656782047, 788.0536784068939, 757.6145903409149, 685.9091404217419, 633.4755797880116, 793.8100850441162, 728.6493076657816, 669.1735939709613, 512.7160636759837, 725.4328137133964, 327.5727141153064, 346.6957029506944, 400.15827700401337, 719.905322792226, 709.4377218161791, 790.1699129919422, 829.3099074832104, 790.442145006281, 734.893094519284, 796.9812098293035, 718.7856502225846, 706.1530772915427, 774.7204767578716, 451.22436411979413, 328.79426624574194, 409.12535888694333, 164.30830720000154, 752.4526803490917, 428.02253002458616, 405.0135973585803, 550.721734112537, 519.8693898556982, 625.6200914721228, 302.91902344681375, 535.1673064459399, 809.6664690092467, 470.49970194622585, 526.4111747518252, 769.4169437362268, 548.7293692033705, 329.03022251701185, 848.9792474858374, 490.042886695724, 309.43064120392904, 908.119508158808, 561.8518086887767, 503.60364158556547, 775.9757202308351, 464.28866500047565, 402.9582119453887]
Elapsed: 0.09129523384691057~0.0443356332756342
Time per graph: 0.002083417312083952~0.0010082595536866714
Speed: 560.5896144220147~190.51793471150427
Total Time: 0.1073
best val loss: 0.7087982296943665 test_score: 0.4884

Testing...
Test loss: 0.8145 score: 0.6744 time: 0.07s
test Score 0.6744
Epoch Time List: [0.534372441121377, 0.5870699569350109, 0.5878437240608037, 0.6556914530228823, 0.8454219148261473, 2.548865271033719, 1.275696034077555, 0.8480014899978414, 0.501764185144566, 0.32009117887355387, 0.3630479439161718, 0.5781870650826022, 0.489841116941534, 0.503419512999244, 0.444738598074764, 0.4778377279872075, 0.35429201694205403, 0.4334925019647926, 0.36262192495632917, 0.3074229540070519, 0.3831784059293568, 0.33144825289491564, 0.33324444794561714, 0.40409336995799094, 0.3815480340272188, 0.3854925730265677, 0.3812997799832374, 0.4157088450156152, 0.32389982894528657, 0.3026494571240619, 0.3977325869491324, 0.3378820400685072, 0.4804638840723783, 0.29770529898814857, 0.33975651091896, 0.44952807610388845, 0.436183353071101, 0.624627573066391, 0.37159465183503926, 0.5556284879567102, 0.3079379330156371, 0.5258591329911724, 0.39436580101028085, 0.4589794591302052, 0.296481047058478, 0.5274444359820336, 0.2934090949129313, 0.5090238609118387, 0.6595762309152633, 0.31056449096649885, 0.356258892104961, 0.34682694100774825, 0.5334683139808476, 0.26233364397194237, 0.24791688995901495, 0.2562859619501978, 0.2563798010814935, 0.31347635912243277, 0.2841022699140012, 0.3379472979577258, 0.24899251700844616, 0.2671848100144416, 0.24305619904771447, 0.2539011419285089, 0.4206208400428295, 0.2705510649830103, 0.5428781590890139, 0.3711378868902102, 0.28140052780508995, 0.5796008928446099, 0.531951230019331, 1.152573607978411, 0.7870432729832828, 0.5734154309611768, 0.37794298597145826, 0.24869435094296932, 0.2463589730905369, 0.2857665990013629, 0.24168383795768023, 0.40840602689422667, 0.24729409581050277, 0.27818500506691635, 0.2470443529309705, 0.2890633438946679, 0.4460507279727608, 0.47466192895080894, 0.5482969029108062, 0.5407487470656633, 0.27583460498135537, 0.37077230506110936, 0.36006737605202943, 0.3605095921084285, 0.3129994128830731, 0.3721988358302042, 0.439458274981007, 0.6465826280182227, 0.3394116919953376, 0.4513794949743897, 0.49930793303065, 0.24511690984945744, 0.4364774409914389, 0.35038893506862223, 0.31939249904826283, 0.4592499090358615, 0.38274331600405276, 0.26561377104371786, 0.4583718511275947, 0.5375031519215554, 0.3142602989682928, 0.4460871870396659, 0.35535598499700427]
Total Epoch List: [90, 21]
Total Time List: [0.10373744799289852, 0.10730049293488264]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e3b010>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7167;  Loss pred: 0.7167; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7629 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7272 score: 0.4884 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7303;  Loss pred: 0.7303; Loss self: 0.0000; time: 0.30s
Val loss: 0.7326 score: 0.4773 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.4884 time: 0.12s
Epoch 3/1000, LR 0.000030
Train loss: 0.7623;  Loss pred: 0.7623; Loss self: 0.0000; time: 0.16s
Val loss: 0.7263 score: 0.4318 time: 0.12s
Test loss: 0.6925 score: 0.5349 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.40s
Val loss: 0.7230 score: 0.3409 time: 0.06s
Test loss: 0.6865 score: 0.4884 time: 0.11s
Epoch 5/1000, LR 0.000090
Train loss: 0.7176;  Loss pred: 0.7176; Loss self: 0.0000; time: 0.21s
Val loss: 0.7190 score: 0.2955 time: 0.17s
Test loss: 0.6814 score: 0.4884 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.23s
Val loss: 0.7168 score: 0.2955 time: 0.05s
Test loss: 0.6810 score: 0.5116 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.29s
Val loss: 0.7158 score: 0.3409 time: 0.05s
Test loss: 0.6839 score: 0.4884 time: 0.20s
Epoch 8/1000, LR 0.000180
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.17s
Val loss: 0.7107 score: 0.4545 time: 0.11s
Test loss: 0.6847 score: 0.4651 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.23s
Val loss: 0.7073 score: 0.5455 time: 0.05s
Test loss: 0.6863 score: 0.6047 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 0.41s
Val loss: 0.7064 score: 0.4773 time: 0.18s
Test loss: 0.6899 score: 0.5349 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5721;  Loss pred: 0.5721; Loss self: 0.0000; time: 0.19s
Val loss: 0.7080 score: 0.4545 time: 0.06s
Test loss: 0.6886 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5668;  Loss pred: 0.5668; Loss self: 0.0000; time: 0.36s
Val loss: 0.7079 score: 0.4545 time: 0.05s
Test loss: 0.6806 score: 0.5116 time: 0.21s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.16s
Val loss: 0.7070 score: 0.4545 time: 0.05s
Test loss: 0.6734 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5344;  Loss pred: 0.5344; Loss self: 0.0000; time: 0.20s
Val loss: 0.7074 score: 0.4545 time: 0.17s
Test loss: 0.6707 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4954;  Loss pred: 0.4954; Loss self: 0.0000; time: 0.26s
Val loss: 0.7070 score: 0.4545 time: 0.35s
Test loss: 0.6693 score: 0.4884 time: 0.14s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4937;  Loss pred: 0.4937; Loss self: 0.0000; time: 0.24s
Val loss: 0.7055 score: 0.4545 time: 0.06s
Test loss: 0.6675 score: 0.4884 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.4417;  Loss pred: 0.4417; Loss self: 0.0000; time: 0.24s
Val loss: 0.7042 score: 0.4545 time: 0.05s
Test loss: 0.6650 score: 0.4884 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.4257;  Loss pred: 0.4257; Loss self: 0.0000; time: 0.46s
Val loss: 0.7050 score: 0.4545 time: 0.05s
Test loss: 0.6637 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4096;  Loss pred: 0.4096; Loss self: 0.0000; time: 0.26s
Val loss: 0.7048 score: 0.4545 time: 0.09s
Test loss: 0.6652 score: 0.4884 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3976;  Loss pred: 0.3976; Loss self: 0.0000; time: 0.25s
Val loss: 0.7050 score: 0.4545 time: 0.14s
Test loss: 0.6691 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3881;  Loss pred: 0.3881; Loss self: 0.0000; time: 0.35s
Val loss: 0.7091 score: 0.4545 time: 0.13s
Test loss: 0.6782 score: 0.4884 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3384;  Loss pred: 0.3384; Loss self: 0.0000; time: 0.61s
Val loss: 0.7133 score: 0.4773 time: 0.05s
Test loss: 0.6863 score: 0.4884 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3418;  Loss pred: 0.3418; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7163 score: 0.5000 time: 0.08s
Test loss: 0.6942 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3277;  Loss pred: 0.3277; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7190 score: 0.5000 time: 0.05s
Test loss: 0.7011 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3200;  Loss pred: 0.3200; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7220 score: 0.5000 time: 0.17s
Test loss: 0.7080 score: 0.4884 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3113;  Loss pred: 0.3113; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7241 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7130 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2987;  Loss pred: 0.2987; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7277 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7197 score: 0.4884 time: 0.09s
     INFO: Early stopping counter 10 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2858;  Loss pred: 0.2858; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7328 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7281 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 11 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2689;  Loss pred: 0.2689; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7347 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7351 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2514;  Loss pred: 0.2514; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7397 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7475 score: 0.4884 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2371;  Loss pred: 0.2371; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7481 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7651 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2214;  Loss pred: 0.2214; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7587 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7869 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.2318;  Loss pred: 0.2318; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7753 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8148 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.2246;  Loss pred: 0.2246; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7891 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8383 score: 0.4884 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.2013;  Loss pred: 0.2013; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8053 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8630 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1840;  Loss pred: 0.1840; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8200 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8849 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1793;  Loss pred: 0.1793; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8270 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8984 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 016,   Train_Loss: 0.4417,   Val_Loss: 0.7042,   Val_Precision: 0.4762,   Val_Recall: 0.9091,   Val_accuracy: 0.6250,   Val_Score: 0.4545,   Val_Loss: 0.7042,   Test_Precision: 0.4878,   Test_Recall: 0.9524,   Test_accuracy: 0.6452,   Test_Score: 0.4884,   Test_loss: 0.6650


[0.07817787607200444, 0.18715743196662515, 0.10836088401265442, 0.08249276201240718, 0.24026729597244412, 0.0538825299590826, 0.15814112301450223, 0.14972497802227736, 0.05415725603234023, 0.08816591708455235, 0.055279394960962236, 0.29787713708356023, 0.11414051603060216, 0.10357435198966414, 0.05503704503644258, 0.14809274894651026, 0.12454730900935829, 0.07097878004424274, 0.08510718692559749, 0.07101422001142055, 0.1459593849722296, 0.06009215803351253, 0.10866432101465762, 0.13734256592579186, 0.12231818994041532, 0.09949472907464951, 0.05017339496407658, 0.0777217058930546, 0.10464374104049057, 0.05059477605391294, 0.07286150404252112, 0.10465692903380841, 0.11474718409590423, 0.06713303492870182, 0.053151287022046745, 0.10306187998503447, 0.05181479104794562, 0.11107332201208919, 0.07217698497697711, 0.11545445502270013, 0.07156648801174015, 0.09823879797477275, 0.11722268094308674, 0.10123336396645755, 0.06971952400635928, 0.10454829398076981, 0.07907373295165598, 0.10722941695712507, 0.07911491999402642, 0.0744080679723993, 0.11422912694979459, 0.07602204196155071, 0.24332184996455908, 0.054593705921433866, 0.05732961499597877, 0.05422020296100527, 0.060327618033625185, 0.06498506106436253, 0.08747386105824262, 0.06386870401911438, 0.057631614967249334, 0.059978723991662264, 0.055833760066889226, 0.058077023015357554, 0.06414843804668635, 0.06945808394812047, 0.055428875028155744, 0.06038570206146687, 0.06575274397619069, 0.08581747894641012, 0.060653446009382606, 0.13432132196612656, 0.1269124469254166, 0.10995649104006588, 0.061119148042052984, 0.062020947923883796, 0.05568422598298639, 0.05305616103578359, 0.05566504807211459, 0.0598726540338248, 0.055208327947184443, 0.06121435505338013, 0.06230943603441119, 0.05679467797745019, 0.09751246497035027, 0.13382228498812765, 0.1075464989989996, 0.2677892600186169, 0.05847543792333454, 0.1027983269887045, 0.106169275008142, 0.07807935902383178, 0.0827130830148235, 0.06873180798720568, 0.1419521280331537, 0.08034870494157076, 0.05310828797519207, 0.09139219392091036, 0.08168519602622837, 0.05588647397235036, 0.0783628550125286, 0.13068708300124854, 0.050649059005081654, 0.0877474220469594, 0.13896490610204637, 0.04735059605445713, 0.076532636070624, 0.08538460894487798, 0.055414104950614274, 0.09261479601264, 0.10671081696636975, 0.07290827599354088, 0.12447418598458171, 0.05721767898648977, 0.11382851703092456, 0.05506114603485912, 0.059985297033563256, 0.20531274296808988, 0.06583267205860466, 0.08373152907006443, 0.06049368402455002, 0.09207004902418703, 0.21089808910619467, 0.08884118299465626, 0.11470162798650563, 0.14772305102087557, 0.10223663598299026, 0.2139130369760096, 0.06097351002972573, 0.08943102997727692, 0.05677026801276952, 0.0830594029976055, 0.19840024993754923, 0.06373167899437249, 0.10202753904741257, 0.07723924203310162, 0.10763378394767642, 0.09291935805231333, 0.10432293999474496, 0.06644620897714049, 0.07067257689777762, 0.05772453802637756, 0.05445275898091495, 0.06559968204237521, 0.08008587500080466, 0.06112573901191354, 0.054123167996294796, 0.054736581980250776]
[0.0017767699107273736, 0.004253577999241481, 0.0024627473639239643, 0.0018748355002819815, 0.005460620363010094, 0.0012246029536155138, 0.0035941164321477777, 0.0034028404095972128, 0.0012308467280077325, 0.002003770842830735, 0.0012563498854764146, 0.006769934933717278, 0.00259410263705914, 0.0023539625452196396, 0.0012508419326464223, 0.0033657442942388693, 0.0028306206593035972, 0.0016131540919146078, 0.0019342542483090338, 0.0016139595457141033, 0.0033172587493688547, 0.001365730864398012, 0.0024696436594240367, 0.003121421952858906, 0.0027799588622821666, 0.0022612438426056706, 0.0011403044310017403, 0.001766402406660332, 0.002378266841829331, 0.0011498812739525667, 0.0016559432736936617, 0.002378566568950191, 0.002607890547634187, 0.0015257507938341323, 0.0012079837959556078, 0.002342315454205329, 0.0011776088874533095, 0.002524393682092936, 0.0016403860222040253, 0.0026239648868795484, 0.0016265110911759125, 0.002232699953972108, 0.002664151839615608, 0.002300758271964944, 0.0015845346365081655, 0.0023760975904720412, 0.0017971302943558178, 0.002437032203571024, 0.0017980663635006006, 0.0016910924539181658, 0.0025961165215862407, 0.0017277736809443343, 0.00553004204464907, 0.0012407660436689514, 0.0013029457953631538, 0.001232277340022847, 0.001371082228036936, 0.0014769332060082393, 0.0019880422967782415, 0.0014515614549798722, 0.0013098094310738486, 0.0013631528179923241, 0.0012689490924293007, 0.0013199323412581261, 0.001457919046515599, 0.001578592817002738, 0.0012597471597308124, 0.0013724023195787925, 0.0014943805449134247, 0.0019503972487820481, 0.00137848740930415, 0.0030527573174119675, 0.002884373793759468, 0.0024990111600014975, 0.001389071546410295, 0.0014095669982700863, 0.001265550590522418, 0.0012058218417223543, 0.0012651147289116952, 0.001360742137132382, 0.0012547347260723736, 0.0013912353421222758, 0.001416123546236618, 0.0012907881358511406, 0.002216192385689779, 0.003041415567911992, 0.002444238613613627, 0.006086119545877657, 0.0013289872255303305, 0.0023363256133796476, 0.002469052907166093, 0.0018157990470658554, 0.0019235600701121743, 0.0015984141392373415, 0.003301212279840784, 0.0018685745335249014, 0.0012350764645393504, 0.0021253998586258224, 0.0018996557215401947, 0.0012996854412174502, 0.0018223919770355488, 0.0030392344884011286, 0.0011778850931414338, 0.002040637722022312, 0.0032317420023731712, 0.0011011766524292356, 0.001779828745828465, 0.0019856885801134414, 0.0012887001151305644, 0.0021538324654102325, 0.0024816469061946455, 0.0016955413021753694, 0.002894748511269342, 0.0013306436973602273, 0.0026471748146726644, 0.0012804917682525377, 0.001395006907757285, 0.004774714952746276, 0.0015309923734559223, 0.0019472448620945215, 0.001406829861036047, 0.002141163930795047, 0.004904606723399876, 0.002066074023131541, 0.00266747972061641, 0.0034354197911831527, 0.0023775961856509365, 0.004974721790139758, 0.001417988605342459, 0.0020797913948203935, 0.00132023879099464, 0.001931614023200128, 0.004613959300873238, 0.0014821320696365695, 0.002372733466218897, 0.0017962614426302703, 0.002503111254597126, 0.0021609153035421704, 0.00242611488359872, 0.001545260673886988, 0.0016435482999483167, 0.0013424311168925014, 0.0012663432321143012, 0.00152557400098547, 0.0018624622093210386, 0.0014215288142305474, 0.0012586783254952278, 0.001272943766982576]
[562.8190763263322, 235.0961943517493, 406.0505818212194, 533.3801284697226, 183.1293760639246, 816.5912037428975, 278.232499942251, 293.87214198457457, 812.448842934827, 499.0590633544183, 795.9566133289333, 147.71190709966157, 385.48975885305424, 424.81559531640454, 799.4615257935008, 297.11110309588753, 353.2794112532284, 619.90358206458, 516.9951162698602, 619.5942163826329, 301.45372295431014, 732.2086847914803, 404.9167158930196, 320.36681201786945, 359.7175532227353, 442.23448226073776, 876.9587952241095, 566.1224170831272, 420.4742640362481, 869.6550006094374, 603.8854204041975, 420.4212793764111, 383.4516754958044, 655.4150284838142, 827.8256739436835, 426.9279777002826, 849.1783737829921, 396.1347261695392, 609.6126073156844, 381.1026607102248, 614.812899478622, 447.88821633687934, 375.3539813797862, 434.63931530101945, 631.1001204768216, 420.8581347878635, 556.4426815020947, 410.33516033751346, 556.1529987431223, 591.3337249439298, 385.1907230223221, 578.7795074256708, 180.83045154559198, 805.9537131133883, 767.4916359212644, 811.5056306898084, 729.3508584322929, 677.0786897687378, 503.0074066434947, 688.9133054403586, 763.4698424641431, 733.5934656782047, 788.0536784068939, 757.6145903409149, 685.9091404217419, 633.4755797880116, 793.8100850441162, 728.6493076657816, 669.1735939709613, 512.7160636759837, 725.4328137133964, 327.5727141153064, 346.6957029506944, 400.15827700401337, 719.905322792226, 709.4377218161791, 790.1699129919422, 829.3099074832104, 790.442145006281, 734.893094519284, 796.9812098293035, 718.7856502225846, 706.1530772915427, 774.7204767578716, 451.22436411979413, 328.79426624574194, 409.12535888694333, 164.30830720000154, 752.4526803490917, 428.02253002458616, 405.0135973585803, 550.721734112537, 519.8693898556982, 625.6200914721228, 302.91902344681375, 535.1673064459399, 809.6664690092467, 470.49970194622585, 526.4111747518252, 769.4169437362268, 548.7293692033705, 329.03022251701185, 848.9792474858374, 490.042886695724, 309.43064120392904, 908.119508158808, 561.8518086887767, 503.60364158556547, 775.9757202308351, 464.28866500047565, 402.9582119453887, 589.7821531784605, 345.4531528756195, 751.5159783072143, 377.76122470538644, 780.9499637507867, 716.8423284782675, 209.43658624580914, 653.1711178565125, 513.5460975998502, 710.8180084147199, 467.0357022260713, 203.8899460030099, 484.00976383426166, 374.88569913810505, 291.08524162504216, 420.59286855989836, 201.01626627283338, 705.2242847596717, 480.81745240914313, 757.4387351901856, 517.7017706380533, 216.7335979341517, 674.7037058885096, 421.45483858056934, 556.7118328475043, 399.5028180083627, 462.7668647451387, 412.18163523924875, 647.1400048540513, 608.4396789747196, 744.9171785550003, 789.6753223297831, 655.4909819871297, 536.9236460183266, 703.4679775670149, 794.4841662436265, 785.5806563792129]
Elapsed: 0.09192213845420377~0.04460714658559844
Time per graph: 0.002107928404272751~0.0010209983803493939
Speed: 555.3283543855734~188.82611687721501
Total Time: 0.0555
best val loss: 0.704150915145874 test_score: 0.4884

Testing...
Test loss: 0.6863 score: 0.6047 time: 0.05s
test Score 0.6047
Epoch Time List: [0.534372441121377, 0.5870699569350109, 0.5878437240608037, 0.6556914530228823, 0.8454219148261473, 2.548865271033719, 1.275696034077555, 0.8480014899978414, 0.501764185144566, 0.32009117887355387, 0.3630479439161718, 0.5781870650826022, 0.489841116941534, 0.503419512999244, 0.444738598074764, 0.4778377279872075, 0.35429201694205403, 0.4334925019647926, 0.36262192495632917, 0.3074229540070519, 0.3831784059293568, 0.33144825289491564, 0.33324444794561714, 0.40409336995799094, 0.3815480340272188, 0.3854925730265677, 0.3812997799832374, 0.4157088450156152, 0.32389982894528657, 0.3026494571240619, 0.3977325869491324, 0.3378820400685072, 0.4804638840723783, 0.29770529898814857, 0.33975651091896, 0.44952807610388845, 0.436183353071101, 0.624627573066391, 0.37159465183503926, 0.5556284879567102, 0.3079379330156371, 0.5258591329911724, 0.39436580101028085, 0.4589794591302052, 0.296481047058478, 0.5274444359820336, 0.2934090949129313, 0.5090238609118387, 0.6595762309152633, 0.31056449096649885, 0.356258892104961, 0.34682694100774825, 0.5334683139808476, 0.26233364397194237, 0.24791688995901495, 0.2562859619501978, 0.2563798010814935, 0.31347635912243277, 0.2841022699140012, 0.3379472979577258, 0.24899251700844616, 0.2671848100144416, 0.24305619904771447, 0.2539011419285089, 0.4206208400428295, 0.2705510649830103, 0.5428781590890139, 0.3711378868902102, 0.28140052780508995, 0.5796008928446099, 0.531951230019331, 1.152573607978411, 0.7870432729832828, 0.5734154309611768, 0.37794298597145826, 0.24869435094296932, 0.2463589730905369, 0.2857665990013629, 0.24168383795768023, 0.40840602689422667, 0.24729409581050277, 0.27818500506691635, 0.2470443529309705, 0.2890633438946679, 0.4460507279727608, 0.47466192895080894, 0.5482969029108062, 0.5407487470656633, 0.27583460498135537, 0.37077230506110936, 0.36006737605202943, 0.3605095921084285, 0.3129994128830731, 0.3721988358302042, 0.439458274981007, 0.6465826280182227, 0.3394116919953376, 0.4513794949743897, 0.49930793303065, 0.24511690984945744, 0.4364774409914389, 0.35038893506862223, 0.31939249904826283, 0.4592499090358615, 0.38274331600405276, 0.26561377104371786, 0.4583718511275947, 0.5375031519215554, 0.3142602989682928, 0.4460871870396659, 0.35535598499700427, 0.43900521099567413, 0.532073488808237, 0.33221626398153603, 0.5708081980701536, 0.4340022000251338, 0.33882828801870346, 0.5379055859521031, 0.33938341890461743, 0.3660684439819306, 0.6450313498498872, 0.3313916421029717, 0.611354298889637, 0.2958806590177119, 0.4758060029707849, 0.7560116469394416, 0.3910950820427388, 0.4954791610362008, 0.5671861539594829, 0.43799199000932276, 0.44394929672125727, 0.5526066279271618, 0.853898606961593, 0.3227568460861221, 0.3243858879432082, 0.4829691060585901, 0.6725171220023185, 0.3505254570627585, 0.35985062492545694, 1.3677159269573167, 0.27646601700689644, 0.2595579681219533, 0.25507471000310034, 0.3079543380299583, 0.3183511419920251, 0.3802054360276088, 0.2994142669485882, 0.42279163491912186]
Total Epoch List: [90, 21, 37]
Total Time List: [0.10373744799289852, 0.10730049293488264, 0.05551872099749744]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e3b490>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7986;  Loss pred: 0.7986; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7547 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7601 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7943;  Loss pred: 0.7943; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7331 score: 0.5116 time: 0.08s
Test loss: 0.7413 score: 0.5227 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7240 score: 0.5116 time: 0.08s
Test loss: 0.7289 score: 0.5227 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.7145;  Loss pred: 0.7145; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7198 score: 0.5116 time: 0.09s
Test loss: 0.7232 score: 0.5227 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.7297;  Loss pred: 0.7297; Loss self: 0.0000; time: 0.16s
Val loss: 0.7178 score: 0.5349 time: 0.11s
Test loss: 0.7193 score: 0.5000 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7267;  Loss pred: 0.7267; Loss self: 0.0000; time: 0.14s
Val loss: 0.7177 score: 0.5116 time: 0.06s
Test loss: 0.7159 score: 0.5000 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.13s
Val loss: 0.7171 score: 0.4419 time: 0.07s
Test loss: 0.7131 score: 0.4773 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.13s
Val loss: 0.7158 score: 0.4419 time: 0.06s
Test loss: 0.7107 score: 0.4773 time: 0.16s
Epoch 9/1000, LR 0.000210
Train loss: 0.6635;  Loss pred: 0.6635; Loss self: 0.0000; time: 0.36s
Val loss: 0.7152 score: 0.4651 time: 0.47s
Test loss: 0.7086 score: 0.4318 time: 0.19s
Epoch 10/1000, LR 0.000240
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.29s
Val loss: 0.7147 score: 0.4651 time: 0.10s
Test loss: 0.7065 score: 0.4091 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.5991;  Loss pred: 0.5991; Loss self: 0.0000; time: 0.22s
Val loss: 0.7119 score: 0.4651 time: 0.08s
Test loss: 0.7045 score: 0.4545 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.6654;  Loss pred: 0.6654; Loss self: 0.0000; time: 0.33s
Val loss: 0.7070 score: 0.4884 time: 0.06s
Test loss: 0.7027 score: 0.4545 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.14s
Val loss: 0.7044 score: 0.4884 time: 0.05s
Test loss: 0.7007 score: 0.4773 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7019 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.5975;  Loss pred: 0.5975; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.5509;  Loss pred: 0.5509; Loss self: 0.0000; time: 0.14s
Val loss: 0.6945 score: 0.5349 time: 0.07s
Test loss: 0.6891 score: 0.5227 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.5447;  Loss pred: 0.5447; Loss self: 0.0000; time: 0.14s
Val loss: 0.6901 score: 0.5349 time: 0.05s
Test loss: 0.6871 score: 0.5455 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.13s
Val loss: 0.6878 score: 0.5116 time: 0.05s
Test loss: 0.6849 score: 0.5227 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 0.13s
Val loss: 0.6883 score: 0.5814 time: 0.07s
Test loss: 0.6843 score: 0.5227 time: 0.18s
     INFO: Early stopping counter 1 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.5078;  Loss pred: 0.5078; Loss self: 0.0000; time: 0.18s
Val loss: 0.6913 score: 0.5814 time: 0.25s
Test loss: 0.6858 score: 0.5227 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4765;  Loss pred: 0.4765; Loss self: 0.0000; time: 0.15s
Val loss: 0.6942 score: 0.6047 time: 0.08s
Test loss: 0.6871 score: 0.5227 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.4652;  Loss pred: 0.4652; Loss self: 0.0000; time: 0.20s
Val loss: 0.6924 score: 0.6047 time: 0.08s
Test loss: 0.6876 score: 0.5227 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.4150;  Loss pred: 0.4150; Loss self: 0.0000; time: 0.54s
Val loss: 0.6870 score: 0.6047 time: 0.27s
Test loss: 0.6877 score: 0.5000 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.4456;  Loss pred: 0.4456; Loss self: 0.0000; time: 0.21s
Val loss: 0.6801 score: 0.6279 time: 0.18s
Test loss: 0.6868 score: 0.4773 time: 0.31s
Epoch 25/1000, LR 0.000270
Train loss: 0.4241;  Loss pred: 0.4241; Loss self: 0.0000; time: 0.27s
Val loss: 0.6720 score: 0.6279 time: 0.09s
Test loss: 0.6857 score: 0.4773 time: 0.11s
Epoch 26/1000, LR 0.000270
Train loss: 0.3887;  Loss pred: 0.3887; Loss self: 0.0000; time: 0.31s
Val loss: 0.6656 score: 0.6279 time: 0.21s
Test loss: 0.6849 score: 0.4773 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.3901;  Loss pred: 0.3901; Loss self: 0.0000; time: 0.23s
Val loss: 0.6682 score: 0.6279 time: 0.07s
Test loss: 0.6860 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.3836;  Loss pred: 0.3836; Loss self: 0.0000; time: 0.19s
Val loss: 0.6778 score: 0.6279 time: 0.06s
Test loss: 0.6890 score: 0.5227 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.3463;  Loss pred: 0.3463; Loss self: 0.0000; time: 0.18s
Val loss: 0.6927 score: 0.6279 time: 0.17s
Test loss: 0.6937 score: 0.5455 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.3269;  Loss pred: 0.3269; Loss self: 0.0000; time: 0.50s
Val loss: 0.7195 score: 0.6047 time: 0.15s
Test loss: 0.7034 score: 0.5682 time: 0.22s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2927;  Loss pred: 0.2927; Loss self: 0.0000; time: 0.95s
Val loss: 0.7444 score: 0.6512 time: 0.07s
Test loss: 0.7138 score: 0.5682 time: 0.24s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2990;  Loss pred: 0.2990; Loss self: 0.0000; time: 0.18s
Val loss: 0.7604 score: 0.6512 time: 0.11s
Test loss: 0.7219 score: 0.5682 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.2837;  Loss pred: 0.2837; Loss self: 0.0000; time: 0.24s
Val loss: 0.7688 score: 0.6512 time: 0.23s
Test loss: 0.7273 score: 0.5682 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.2461;  Loss pred: 0.2461; Loss self: 0.0000; time: 0.14s
Val loss: 0.7727 score: 0.6512 time: 0.20s
Test loss: 0.7312 score: 0.5682 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.2430;  Loss pred: 0.2430; Loss self: 0.0000; time: 0.16s
Val loss: 0.7653 score: 0.6512 time: 0.05s
Test loss: 0.7281 score: 0.5682 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.2321;  Loss pred: 0.2321; Loss self: 0.0000; time: 0.23s
Val loss: 0.7597 score: 0.6512 time: 0.27s
Test loss: 0.7274 score: 0.5682 time: 0.37s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.2319;  Loss pred: 0.2319; Loss self: 0.0000; time: 0.30s
Val loss: 0.7730 score: 0.6512 time: 0.73s
Test loss: 0.7330 score: 0.5682 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.2016;  Loss pred: 0.2016; Loss self: 0.0000; time: 0.23s
Val loss: 0.7855 score: 0.6512 time: 0.06s
Test loss: 0.7395 score: 0.5682 time: 4.21s
     INFO: Early stopping counter 12 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.2086;  Loss pred: 0.2086; Loss self: 0.0000; time: 13.02s
Val loss: 0.7871 score: 0.6512 time: 4.24s
Test loss: 0.7402 score: 0.5909 time: 4.68s
     INFO: Early stopping counter 13 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1709;  Loss pred: 0.1709; Loss self: 0.0000; time: 12.49s
Val loss: 0.7899 score: 0.6512 time: 3.59s
Test loss: 0.7417 score: 0.5909 time: 4.18s
     INFO: Early stopping counter 14 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1711;  Loss pred: 0.1711; Loss self: 0.0000; time: 11.84s
Val loss: 0.7841 score: 0.6512 time: 3.88s
Test loss: 0.7377 score: 0.6136 time: 4.32s
     INFO: Early stopping counter 15 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.1678;  Loss pred: 0.1678; Loss self: 0.0000; time: 11.82s
Val loss: 0.7724 score: 0.6512 time: 4.20s
Test loss: 0.7295 score: 0.6364 time: 4.21s
     INFO: Early stopping counter 16 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.1548;  Loss pred: 0.1548; Loss self: 0.0000; time: 12.29s
Val loss: 0.7471 score: 0.6512 time: 3.47s
Test loss: 0.7129 score: 0.6364 time: 3.68s
     INFO: Early stopping counter 17 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.1387;  Loss pred: 0.1387; Loss self: 0.0000; time: 11.44s
Val loss: 0.7093 score: 0.6744 time: 3.31s
Test loss: 0.6918 score: 0.6364 time: 3.48s
     INFO: Early stopping counter 18 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.1340;  Loss pred: 0.1340; Loss self: 0.0000; time: 9.91s
Val loss: 0.6714 score: 0.6744 time: 3.52s
Test loss: 0.6666 score: 0.6364 time: 3.55s
     INFO: Early stopping counter 19 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.1334;  Loss pred: 0.1334; Loss self: 0.0000; time: 11.98s
Val loss: 0.6381 score: 0.6744 time: 4.34s
Test loss: 0.6440 score: 0.6364 time: 4.90s
Epoch 47/1000, LR 0.000269
Train loss: 0.1197;  Loss pred: 0.1197; Loss self: 0.0000; time: 11.64s
Val loss: 0.6188 score: 0.6744 time: 3.55s
Test loss: 0.6329 score: 0.6364 time: 4.10s
Epoch 48/1000, LR 0.000269
Train loss: 0.1429;  Loss pred: 0.1429; Loss self: 0.0000; time: 10.21s
Val loss: 0.6146 score: 0.6744 time: 3.08s
Test loss: 0.6290 score: 0.6364 time: 3.77s
Epoch 49/1000, LR 0.000269
Train loss: 0.1029;  Loss pred: 0.1029; Loss self: 0.0000; time: 11.76s
Val loss: 0.6202 score: 0.6744 time: 3.84s
Test loss: 0.6242 score: 0.6591 time: 4.18s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 9.83s
Val loss: 0.6253 score: 0.7442 time: 3.20s
Test loss: 0.6178 score: 0.6591 time: 3.56s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.1012;  Loss pred: 0.1012; Loss self: 0.0000; time: 10.56s
Val loss: 0.6131 score: 0.7442 time: 3.34s
Test loss: 0.6066 score: 0.6818 time: 3.30s
Epoch 52/1000, LR 0.000269
Train loss: 0.0902;  Loss pred: 0.0902; Loss self: 0.0000; time: 9.78s
Val loss: 0.5864 score: 0.7442 time: 4.40s
Test loss: 0.5910 score: 0.7045 time: 4.59s
Epoch 53/1000, LR 0.000269
Train loss: 0.0914;  Loss pred: 0.0914; Loss self: 0.0000; time: 11.95s
Val loss: 0.5492 score: 0.7442 time: 3.39s
Test loss: 0.5731 score: 0.7045 time: 4.89s
Epoch 54/1000, LR 0.000269
Train loss: 0.0762;  Loss pred: 0.0762; Loss self: 0.0000; time: 7.62s
Val loss: 0.5068 score: 0.7442 time: 4.47s
Test loss: 0.5532 score: 0.7273 time: 3.28s
Epoch 55/1000, LR 0.000269
Train loss: 0.0845;  Loss pred: 0.0845; Loss self: 0.0000; time: 9.85s
Val loss: 0.4712 score: 0.7442 time: 3.56s
Test loss: 0.5333 score: 0.7273 time: 3.97s
Epoch 56/1000, LR 0.000269
Train loss: 0.0766;  Loss pred: 0.0766; Loss self: 0.0000; time: 11.22s
Val loss: 0.4423 score: 0.7674 time: 3.47s
Test loss: 0.5195 score: 0.7500 time: 3.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.0745;  Loss pred: 0.0745; Loss self: 0.0000; time: 9.71s
Val loss: 0.4245 score: 0.8140 time: 3.38s
Test loss: 0.5057 score: 0.7500 time: 4.28s
Epoch 58/1000, LR 0.000269
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 9.52s
Val loss: 0.4098 score: 0.8140 time: 2.95s
Test loss: 0.4942 score: 0.7500 time: 4.11s
Epoch 59/1000, LR 0.000268
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 11.31s
Val loss: 0.3997 score: 0.8140 time: 3.68s
Test loss: 0.4851 score: 0.7500 time: 3.57s
Epoch 60/1000, LR 0.000268
Train loss: 0.0567;  Loss pred: 0.0567; Loss self: 0.0000; time: 9.60s
Val loss: 0.3915 score: 0.8140 time: 3.53s
Test loss: 0.4755 score: 0.7500 time: 3.30s
Epoch 61/1000, LR 0.000268
Train loss: 0.0560;  Loss pred: 0.0560; Loss self: 0.0000; time: 9.18s
Val loss: 0.3857 score: 0.8140 time: 3.49s
Test loss: 0.4682 score: 0.7500 time: 3.52s
Epoch 62/1000, LR 0.000268
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 9.10s
Val loss: 0.3866 score: 0.8140 time: 4.09s
Test loss: 0.4662 score: 0.7727 time: 4.21s
     INFO: Early stopping counter 1 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 10.69s
Val loss: 0.3955 score: 0.8140 time: 3.19s
Test loss: 0.4720 score: 0.7727 time: 3.48s
     INFO: Early stopping counter 2 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 10.09s
Val loss: 0.4030 score: 0.8140 time: 3.51s
Test loss: 0.4773 score: 0.7727 time: 4.23s
     INFO: Early stopping counter 3 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0545;  Loss pred: 0.0545; Loss self: 0.0000; time: 9.10s
Val loss: 0.4203 score: 0.8140 time: 2.63s
Test loss: 0.4956 score: 0.7727 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0415;  Loss pred: 0.0415; Loss self: 0.0000; time: 0.14s
Val loss: 0.4395 score: 0.7907 time: 0.09s
Test loss: 0.5196 score: 0.7955 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.13s
Val loss: 0.4536 score: 0.7907 time: 0.07s
Test loss: 0.5398 score: 0.7727 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.19s
Val loss: 0.4717 score: 0.7674 time: 0.06s
Test loss: 0.5621 score: 0.7500 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.15s
Val loss: 0.4727 score: 0.7674 time: 0.12s
Test loss: 0.5613 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.36s
Val loss: 0.4690 score: 0.7674 time: 0.05s
Test loss: 0.5526 score: 0.7500 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0362;  Loss pred: 0.0362; Loss self: 0.0000; time: 0.17s
Val loss: 0.4713 score: 0.7674 time: 0.22s
Test loss: 0.5492 score: 0.7500 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.13s
Val loss: 0.4604 score: 0.7674 time: 0.19s
Test loss: 0.5298 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.22s
Val loss: 0.4592 score: 0.7674 time: 0.08s
Test loss: 0.5179 score: 0.7500 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.18s
Val loss: 0.4657 score: 0.7674 time: 0.06s
Test loss: 0.5124 score: 0.7500 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.16s
Val loss: 0.4869 score: 0.7674 time: 0.16s
Test loss: 0.5248 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.20s
Val loss: 0.5039 score: 0.7674 time: 0.06s
Test loss: 0.5343 score: 0.7273 time: 0.11s
     INFO: Early stopping counter 15 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.38s
Val loss: 0.5203 score: 0.7674 time: 0.42s
Test loss: 0.5491 score: 0.7273 time: 0.17s
     INFO: Early stopping counter 16 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.89s
Val loss: 0.5514 score: 0.7674 time: 0.19s
Test loss: 0.5872 score: 0.7500 time: 0.14s
     INFO: Early stopping counter 17 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.28s
Val loss: 0.5701 score: 0.7907 time: 0.09s
Test loss: 0.6160 score: 0.7727 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0282;  Loss pred: 0.0282; Loss self: 0.0000; time: 0.20s
Val loss: 0.5726 score: 0.8140 time: 0.09s
Test loss: 0.6269 score: 0.7727 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.13s
Val loss: 0.5692 score: 0.8140 time: 0.14s
Test loss: 0.6288 score: 0.7727 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 060,   Train_Loss: 0.0560,   Val_Loss: 0.3857,   Val_Precision: 0.7333,   Val_Recall: 1.0000,   Val_accuracy: 0.8462,   Val_Score: 0.8140,   Val_Loss: 0.3857,   Test_Precision: 0.6774,   Test_Recall: 0.9545,   Test_accuracy: 0.7925,   Test_Score: 0.7500,   Test_loss: 0.4682


[0.05526073009241372, 0.24011326802428812, 0.07254968502093107, 0.05995035800151527, 0.08458997798152268, 0.060609022970311344, 0.05871635803487152, 0.17073126405011863, 0.19221214903518558, 0.07043872401118279, 0.08645387296564877, 0.052556092967279255, 0.09072577394545078, 0.05582262994721532, 0.06027361995074898, 0.05488180508837104, 0.05483183194883168, 0.0565616840030998, 0.18482376297470182, 0.0873353430069983, 0.10241292498540133, 0.13355842290911824, 0.25679181795567274, 0.31450845196377486, 0.11670836096163839, 0.06159682897850871, 0.1070660250261426, 0.10728254704736173, 0.11018073302693665, 0.22666284698061645, 0.2398763169767335, 0.09199287602677941, 0.07596493198070675, 0.057800051057711244, 0.13387534604407847, 0.37498289509676397, 0.12053445307537913, 4.2161102630198, 4.682000249042176, 4.184833992971107, 4.321797244017944, 4.216800067923032, 3.686243324074894, 3.4890068050008267, 3.5567835910478607, 4.906429070048034, 4.109745619003661, 3.778066450962797, 4.182183710043319, 3.5669509419240057, 3.302419172017835, 4.59681926399935, 4.891830081003718, 3.2918047679122537, 3.9750024009263143, 3.2008736489806324, 4.2891042770352215, 4.118595478939824, 3.579961044015363, 3.3071298280265182, 3.5286175329238176, 4.216227023978718, 3.4874641919741407, 4.231093719950877, 0.1793860460165888, 0.08585365395992994, 0.07299589307513088, 0.1066153229912743, 0.050843736971728504, 0.10447391797788441, 0.06956905603874475, 0.05952962092123926, 0.08989010099321604, 0.1805835870327428, 0.055670298053883016, 0.1126758469035849, 0.1767732900334522, 0.1434135230956599, 0.081930385902524, 0.04664967593271285, 0.09726618800777942]
[0.0012559256839184936, 0.00545711972782473, 0.0016488564777484332, 0.0013625081363980744, 0.001922499499580061, 0.0013774777947798032, 0.0013344626826107162, 0.0038802560011390597, 0.004368457932617854, 0.0016008800911632452, 0.00196486074921929, 0.0011944566583472558, 0.0020619494078511543, 0.0012686961351639845, 0.0013698549988806587, 0.0012473137520084326, 0.0012461779988370836, 0.001285492818252268, 0.004200540067606859, 0.0019848941592499614, 0.0023275664769409395, 0.00303541870247996, 0.005836177680810744, 0.007147919362813065, 0.0026524627491281453, 0.0013999279313297434, 0.00243331875059415, 0.0024382397056218574, 0.0025041075687940147, 0.005151428340468556, 0.0054517344767439435, 0.002090747182426805, 0.0017264757268342444, 0.0013136375240388918, 0.0030426215010017836, 0.008522338524926454, 0.0027394193880767984, 0.09582068779590455, 0.10640909656914035, 0.09510986347661608, 0.09822266463677144, 0.0958363651800689, 0.0837782573653385, 0.07929560920456424, 0.0808359907056332, 0.11150975159200077, 0.09340330952281048, 0.08586514661279084, 0.09504962977371179, 0.08106706686190922, 0.07505498118222352, 0.10447316509089433, 0.11117795638644813, 0.07481374472527849, 0.09034096365741623, 0.07274712838592347, 0.0974796426598914, 0.09360444270317782, 0.08136275100034916, 0.07516204154605723, 0.08019585302099586, 0.09582334145406177, 0.0792605498175941, 0.09616122090797448, 0.00407695559128611, 0.001951219408180226, 0.0016589975698893381, 0.0024230755225289613, 0.0011555394766301933, 0.0023744072267701003, 0.0015811149099714714, 0.0013529459300281649, 0.00204295684075491, 0.004104172432562336, 0.0012652340466791595, 0.0025608147023542024, 0.00401757477348755, 0.003259398252174089, 0.0018620542250573635, 0.0010602199075616557, 0.002210595181994987]
[796.2254556973431, 183.24684996394816, 606.4809238979564, 733.9405712787884, 520.1561822088556, 725.9645155730841, 749.3652786480471, 257.7149548139214, 228.9137300678405, 624.6564033870716, 508.94191885981536, 837.200741451497, 484.97795154059713, 788.2108034251603, 730.0042711214866, 801.7229012265707, 802.4535828213839, 777.9117750028205, 238.0646259540912, 503.8052005643834, 429.6332714476427, 329.443842189874, 171.34502317980886, 139.90085075700264, 377.00812210414506, 714.3224859083527, 410.961366962642, 410.1319479353472, 399.34386703747026, 194.12091829836916, 183.42786213558432, 478.29790632039226, 579.2146303925465, 761.2450022936416, 328.66394971269017, 117.33868551163071, 365.0408565962757, 10.436159695805697, 9.397692793587813, 10.51415661264052, 10.180949617870901, 10.434452497453128, 11.936271193123778, 12.6110387451622, 12.370727336558927, 8.967825555372645, 10.706258751525128, 11.64616889911693, 10.520819516927498, 12.335465420297172, 13.323566061153661, 9.571835974625392, 8.994588788123231, 13.366527817476223, 11.069175704081704, 13.746247064145278, 10.25855217267283, 10.683253605504888, 12.29063653459442, 13.304588053096289, 12.469472701265397, 10.435870684799752, 12.616616996744854, 10.399202407766765, 245.2810626972127, 512.500027320164, 602.7736376170244, 412.6986512398512, 865.3966569071439, 421.1577478056691, 632.4651002235146, 739.1278378576316, 489.4866010142836, 243.6544800277008, 790.3676024405798, 390.50072583568124, 248.90638168058948, 306.8050979449898, 537.0412883487286, 943.2005500630975, 452.3668594525453]
Elapsed: 1.396410465293656~1.8352505169977607
Time per graph: 0.031736601483946726~0.04171023902267638
Speed: 338.6137982218022~294.28798540167344
Total Time: 0.0982
best val loss: 0.38569948077201843 test_score: 0.7500

Testing...
Test loss: 0.5057 score: 0.7500 time: 0.05s
test Score 0.7500
Epoch Time List: [0.2527120130835101, 0.45489073207136244, 0.36572073807474226, 0.4766757390461862, 0.3441435049753636, 0.24954736384097487, 0.25492085004225373, 0.35477050696499646, 1.013615798088722, 0.4551418861374259, 0.3734592639375478, 0.4392651579109952, 0.2735367820132524, 0.2518493660027161, 0.24304882797878236, 0.2627406121464446, 0.24332914303522557, 0.23675642092712224, 0.37484650302212685, 0.5025792887900025, 0.32430877594742924, 0.41521838505286723, 1.0605554258218035, 0.7053733309730887, 0.46970954397693276, 0.566779570071958, 0.39183433109428734, 0.3557409649947658, 0.45721942803356797, 0.8737525549950078, 1.2579432261409238, 0.3814164699288085, 0.5369477020576596, 0.38383855402935296, 0.34816617507021874, 0.8667589022079483, 1.1534729440463707, 4.507924158126116, 21.931613394990563, 20.25848874798976, 20.038364618085325, 20.22677334700711, 19.43984421202913, 18.228742890059948, 16.978252914035693, 21.21434587892145, 19.293807941139676, 17.059922640910372, 19.775313613936305, 16.58224428293761, 17.192949536954984, 18.76404243905563, 20.22133793204557, 15.366125197033398, 17.378581136115827, 17.888148608035408, 17.376774145057425, 16.583317924989387, 18.55805362004321, 16.42892918002326, 16.19947156694252, 17.397638550959527, 17.36212866997812, 17.828447391977534, 11.911079859011807, 0.30321758217178285, 0.26908985699992627, 0.3565210660453886, 0.31849073513876647, 0.5044939620420337, 0.45390821411274374, 0.37428910902235657, 0.38228636502753943, 0.42079788993578404, 0.36270680802408606, 0.37293768278323114, 0.9732922631083056, 1.2190153839765117, 0.4439961389871314, 0.334900313988328, 0.37099571397993714]
Total Epoch List: [81]
Total Time List: [0.09817989403381944]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e39180>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7793;  Loss pred: 0.7793; Loss self: 0.0000; time: 0.15s
Val loss: 0.8342 score: 0.5227 time: 0.13s
Test loss: 0.8206 score: 0.5116 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.8038;  Loss pred: 0.8038; Loss self: 0.0000; time: 0.17s
Val loss: 0.7562 score: 0.4773 time: 0.06s
Test loss: 0.7508 score: 0.4419 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.7248;  Loss pred: 0.7248; Loss self: 0.0000; time: 0.20s
Val loss: 0.7228 score: 0.4545 time: 0.05s
Test loss: 0.7163 score: 0.4419 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7052;  Loss pred: 0.7052; Loss self: 0.0000; time: 0.14s
Val loss: 0.7081 score: 0.4545 time: 0.10s
Test loss: 0.7013 score: 0.4186 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7366;  Loss pred: 0.7366; Loss self: 0.0000; time: 0.17s
Val loss: 0.6960 score: 0.4545 time: 0.05s
Test loss: 0.6934 score: 0.5581 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7440;  Loss pred: 0.7440; Loss self: 0.0000; time: 0.17s
Val loss: 0.6894 score: 0.5682 time: 0.06s
Test loss: 0.6902 score: 0.6047 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.6679;  Loss pred: 0.6679; Loss self: 0.0000; time: 0.24s
Val loss: 0.6876 score: 0.5455 time: 0.05s
Test loss: 0.6922 score: 0.5814 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.15s
Val loss: 0.6873 score: 0.5682 time: 0.14s
Test loss: 0.6915 score: 0.5581 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6556;  Loss pred: 0.6556; Loss self: 0.0000; time: 0.19s
Val loss: 0.6873 score: 0.5682 time: 0.06s
Test loss: 0.6887 score: 0.5349 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6272;  Loss pred: 0.6272; Loss self: 0.0000; time: 0.29s
Val loss: 0.6838 score: 0.5455 time: 0.04s
Test loss: 0.6849 score: 0.5116 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.23s
Val loss: 0.6793 score: 0.5455 time: 0.05s
Test loss: 0.6811 score: 0.5581 time: 0.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.6277;  Loss pred: 0.6277; Loss self: 0.0000; time: 0.18s
Val loss: 0.6751 score: 0.5455 time: 0.05s
Test loss: 0.6754 score: 0.5116 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.5723;  Loss pred: 0.5723; Loss self: 0.0000; time: 0.20s
Val loss: 0.6725 score: 0.5455 time: 0.07s
Test loss: 0.6698 score: 0.5116 time: 0.08s
Epoch 14/1000, LR 0.000270
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 0.37s
Val loss: 0.6711 score: 0.5455 time: 0.08s
Test loss: 0.6671 score: 0.5349 time: 0.10s
Epoch 15/1000, LR 0.000270
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.31s
Val loss: 0.6703 score: 0.5227 time: 0.06s
Test loss: 0.6654 score: 0.5349 time: 0.10s
Epoch 16/1000, LR 0.000270
Train loss: 0.4919;  Loss pred: 0.4919; Loss self: 0.0000; time: 0.20s
Val loss: 0.6675 score: 0.5227 time: 0.55s
Test loss: 0.6618 score: 0.5814 time: 0.33s
Epoch 17/1000, LR 0.000270
Train loss: 0.5024;  Loss pred: 0.5024; Loss self: 0.0000; time: 0.39s
Val loss: 0.6644 score: 0.5227 time: 0.12s
Test loss: 0.6566 score: 0.5581 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.4954;  Loss pred: 0.4954; Loss self: 0.0000; time: 0.16s
Val loss: 0.6611 score: 0.5227 time: 0.05s
Test loss: 0.6521 score: 0.5814 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.4516;  Loss pred: 0.4516; Loss self: 0.0000; time: 0.14s
Val loss: 0.6610 score: 0.5227 time: 0.05s
Test loss: 0.6489 score: 0.6047 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.4342;  Loss pred: 0.4342; Loss self: 0.0000; time: 0.14s
Val loss: 0.6613 score: 0.5227 time: 0.19s
Test loss: 0.6440 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4104;  Loss pred: 0.4104; Loss self: 0.0000; time: 0.19s
Val loss: 0.6602 score: 0.5227 time: 0.05s
Test loss: 0.6392 score: 0.6279 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.3747;  Loss pred: 0.3747; Loss self: 0.0000; time: 0.23s
Val loss: 0.6573 score: 0.5227 time: 0.09s
Test loss: 0.6385 score: 0.6279 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.3673;  Loss pred: 0.3673; Loss self: 0.0000; time: 0.17s
Val loss: 0.6531 score: 0.5455 time: 0.22s
Test loss: 0.6383 score: 0.6047 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.3506;  Loss pred: 0.3506; Loss self: 0.0000; time: 0.16s
Val loss: 0.6496 score: 0.5682 time: 0.05s
Test loss: 0.6377 score: 0.6279 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.3486;  Loss pred: 0.3486; Loss self: 0.0000; time: 0.15s
Val loss: 0.6462 score: 0.6136 time: 0.07s
Test loss: 0.6360 score: 0.6279 time: 0.20s
Epoch 26/1000, LR 0.000270
Train loss: 0.3043;  Loss pred: 0.3043; Loss self: 0.0000; time: 0.27s
Val loss: 0.6426 score: 0.6136 time: 0.05s
Test loss: 0.6337 score: 0.6279 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.2771;  Loss pred: 0.2771; Loss self: 0.0000; time: 0.16s
Val loss: 0.6390 score: 0.6364 time: 0.05s
Test loss: 0.6298 score: 0.6279 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.2923;  Loss pred: 0.2923; Loss self: 0.0000; time: 0.15s
Val loss: 0.6385 score: 0.6136 time: 0.05s
Test loss: 0.6281 score: 0.6047 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.2320;  Loss pred: 0.2320; Loss self: 0.0000; time: 0.14s
Val loss: 0.6374 score: 0.6364 time: 0.05s
Test loss: 0.6278 score: 0.6047 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.2418;  Loss pred: 0.2418; Loss self: 0.0000; time: 0.16s
Val loss: 0.6364 score: 0.6136 time: 0.13s
Test loss: 0.6286 score: 0.6279 time: 0.07s
Epoch 31/1000, LR 0.000270
Train loss: 0.2304;  Loss pred: 0.2304; Loss self: 0.0000; time: 0.15s
Val loss: 0.6338 score: 0.6364 time: 0.06s
Test loss: 0.6269 score: 0.6279 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.2190;  Loss pred: 0.2190; Loss self: 0.0000; time: 0.16s
Val loss: 0.6316 score: 0.6591 time: 0.08s
Test loss: 0.6242 score: 0.6279 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.1885;  Loss pred: 0.1885; Loss self: 0.0000; time: 0.15s
Val loss: 0.6295 score: 0.6591 time: 0.06s
Test loss: 0.6218 score: 0.6279 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.1727;  Loss pred: 0.1727; Loss self: 0.0000; time: 0.15s
Val loss: 0.6286 score: 0.6591 time: 0.05s
Test loss: 0.6206 score: 0.6047 time: 0.07s
Epoch 35/1000, LR 0.000270
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.24s
Val loss: 0.6258 score: 0.6591 time: 0.06s
Test loss: 0.6187 score: 0.6279 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 0.17s
Val loss: 0.6217 score: 0.6818 time: 0.19s
Test loss: 0.6152 score: 0.6279 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.1107;  Loss pred: 0.1107; Loss self: 0.0000; time: 0.17s
Val loss: 0.6144 score: 0.6591 time: 0.08s
Test loss: 0.6085 score: 0.6279 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.1033;  Loss pred: 0.1033; Loss self: 0.0000; time: 0.19s
Val loss: 0.6074 score: 0.6364 time: 0.07s
Test loss: 0.6017 score: 0.6512 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.1046;  Loss pred: 0.1046; Loss self: 0.0000; time: 0.16s
Val loss: 0.6009 score: 0.6591 time: 0.05s
Test loss: 0.5976 score: 0.6512 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.16s
Val loss: 0.5961 score: 0.6591 time: 0.05s
Test loss: 0.5961 score: 0.6512 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 0.35s
Val loss: 0.5907 score: 0.6364 time: 0.09s
Test loss: 0.5960 score: 0.6512 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0711;  Loss pred: 0.0711; Loss self: 0.0000; time: 0.17s
Val loss: 0.5837 score: 0.6364 time: 0.05s
Test loss: 0.5948 score: 0.6512 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.25s
Val loss: 0.5763 score: 0.6591 time: 0.06s
Test loss: 0.5926 score: 0.6512 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 0.16s
Val loss: 0.5705 score: 0.6591 time: 0.05s
Test loss: 0.5906 score: 0.6512 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0500;  Loss pred: 0.0500; Loss self: 0.0000; time: 0.15s
Val loss: 0.5641 score: 0.6591 time: 0.05s
Test loss: 0.5885 score: 0.6512 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0512;  Loss pred: 0.0512; Loss self: 0.0000; time: 0.17s
Val loss: 0.5528 score: 0.6591 time: 0.06s
Test loss: 0.5794 score: 0.6744 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.0418;  Loss pred: 0.0418; Loss self: 0.0000; time: 0.17s
Val loss: 0.5415 score: 0.6591 time: 0.07s
Test loss: 0.5707 score: 0.6512 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.18s
Val loss: 0.5326 score: 0.6818 time: 0.05s
Test loss: 0.5630 score: 0.6744 time: 0.06s
Epoch 49/1000, LR 0.000269
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.18s
Val loss: 0.5240 score: 0.6818 time: 0.09s
Test loss: 0.5538 score: 0.6977 time: 0.09s
Epoch 50/1000, LR 0.000269
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.15s
Val loss: 0.5152 score: 0.7045 time: 0.13s
Test loss: 0.5435 score: 0.6744 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.19s
Val loss: 0.5069 score: 0.7273 time: 0.06s
Test loss: 0.5325 score: 0.6512 time: 0.07s
Epoch 52/1000, LR 0.000269
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.30s
Val loss: 0.4993 score: 0.7273 time: 0.05s
Test loss: 0.5229 score: 0.6744 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.28s
Val loss: 0.4941 score: 0.7273 time: 0.05s
Test loss: 0.5165 score: 0.6977 time: 0.07s
Epoch 54/1000, LR 0.000269
Train loss: 0.0156;  Loss pred: 0.0156; Loss self: 0.0000; time: 0.21s
Val loss: 0.4895 score: 0.7500 time: 0.06s
Test loss: 0.5120 score: 0.6977 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.19s
Val loss: 0.4887 score: 0.7955 time: 0.05s
Test loss: 0.5148 score: 0.6977 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.38s
Val loss: 0.4890 score: 0.7955 time: 0.05s
Test loss: 0.5154 score: 0.7209 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.20s
Val loss: 0.4883 score: 0.8182 time: 0.05s
Test loss: 0.5127 score: 0.7209 time: 0.10s
Epoch 58/1000, LR 0.000269
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.25s
Val loss: 0.4846 score: 0.8182 time: 0.12s
Test loss: 0.5056 score: 0.7209 time: 0.10s
Epoch 59/1000, LR 0.000268
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.18s
Val loss: 0.4782 score: 0.8182 time: 0.06s
Test loss: 0.4959 score: 0.7907 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.21s
Val loss: 0.4678 score: 0.8409 time: 0.06s
Test loss: 0.4819 score: 0.8372 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.15s
Val loss: 0.4543 score: 0.8409 time: 0.12s
Test loss: 0.4655 score: 0.8372 time: 0.10s
Epoch 62/1000, LR 0.000268
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.18s
Val loss: 0.4425 score: 0.8636 time: 0.06s
Test loss: 0.4527 score: 0.8372 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.17s
Val loss: 0.4304 score: 0.8636 time: 0.15s
Test loss: 0.4417 score: 0.8372 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.15s
Val loss: 0.4175 score: 0.8864 time: 0.14s
Test loss: 0.4287 score: 0.8372 time: 0.10s
Epoch 65/1000, LR 0.000268
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.21s
Val loss: 0.4043 score: 0.9091 time: 0.05s
Test loss: 0.4157 score: 0.8837 time: 0.08s
Epoch 66/1000, LR 0.000268
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.19s
Val loss: 0.3925 score: 0.8636 time: 0.11s
Test loss: 0.4036 score: 0.8837 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.17s
Val loss: 0.3802 score: 0.8409 time: 0.14s
Test loss: 0.3915 score: 0.8605 time: 0.11s
Epoch 68/1000, LR 0.000268
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.20s
Val loss: 0.3702 score: 0.8409 time: 0.05s
Test loss: 0.3847 score: 0.8605 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.16s
Val loss: 0.3613 score: 0.8409 time: 0.06s
Test loss: 0.3788 score: 0.8140 time: 0.34s
Epoch 70/1000, LR 0.000268
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.15s
Val loss: 0.3541 score: 0.8409 time: 0.17s
Test loss: 0.3760 score: 0.8140 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.25s
Val loss: 0.3469 score: 0.8409 time: 0.08s
Test loss: 0.3739 score: 0.8140 time: 0.07s
Epoch 72/1000, LR 0.000267
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.18s
Val loss: 0.3401 score: 0.8409 time: 0.16s
Test loss: 0.3713 score: 0.8372 time: 0.13s
Epoch 73/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.14s
Val loss: 0.3332 score: 0.8409 time: 0.14s
Test loss: 0.3685 score: 0.8140 time: 0.10s
Epoch 74/1000, LR 0.000267
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.19s
Val loss: 0.3273 score: 0.8636 time: 0.05s
Test loss: 0.3678 score: 0.7907 time: 0.07s
Epoch 75/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.15s
Val loss: 0.3247 score: 0.8636 time: 0.06s
Test loss: 0.3680 score: 0.7907 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.18s
Val loss: 0.3247 score: 0.8409 time: 0.13s
Test loss: 0.3697 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.19s
Val loss: 0.3305 score: 0.8182 time: 0.06s
Test loss: 0.3782 score: 0.7907 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.18s
Val loss: 0.3429 score: 0.8182 time: 0.16s
Test loss: 0.3914 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.33s
Val loss: 0.3566 score: 0.8182 time: 0.05s
Test loss: 0.4026 score: 0.8140 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.19s
Val loss: 0.3697 score: 0.7955 time: 0.11s
Test loss: 0.4106 score: 0.8140 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.15s
Val loss: 0.3693 score: 0.7955 time: 0.14s
Test loss: 0.4116 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.23s
Val loss: 0.3602 score: 0.7955 time: 0.28s
Test loss: 0.4055 score: 0.8140 time: 0.22s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.14s
Val loss: 0.3532 score: 0.8409 time: 0.13s
Test loss: 0.3989 score: 0.8140 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.18s
Val loss: 0.3494 score: 0.8182 time: 0.05s
Test loss: 0.3947 score: 0.8140 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.19s
Val loss: 0.3479 score: 0.8182 time: 0.13s
Test loss: 0.3917 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.20s
Val loss: 0.3445 score: 0.8182 time: 0.06s
Test loss: 0.3886 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.18s
Val loss: 0.3414 score: 0.7955 time: 0.06s
Test loss: 0.3864 score: 0.8140 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.18s
Val loss: 0.3449 score: 0.7955 time: 0.06s
Test loss: 0.3889 score: 0.8140 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.15s
Val loss: 0.3479 score: 0.7955 time: 0.12s
Test loss: 0.3925 score: 0.8140 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.15s
Val loss: 0.3519 score: 0.7955 time: 0.05s
Test loss: 0.3977 score: 0.8140 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.19s
Val loss: 0.3557 score: 0.7955 time: 0.06s
Test loss: 0.4025 score: 0.8140 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.15s
Val loss: 0.3644 score: 0.7955 time: 0.17s
Test loss: 0.4115 score: 0.8140 time: 0.10s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.15s
Val loss: 0.3746 score: 0.7955 time: 0.07s
Test loss: 0.4207 score: 0.8140 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.21s
Val loss: 0.3659 score: 0.8182 time: 0.16s
Test loss: 0.4190 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.30s
Val loss: 0.3595 score: 0.8182 time: 0.17s
Test loss: 0.4183 score: 0.8140 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0042,   Val_Loss: 0.3247,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.3247,   Test_Precision: 0.8421,   Test_Recall: 0.7273,   Test_accuracy: 0.7805,   Test_Score: 0.7907,   Test_loss: 0.3680


[0.05526073009241372, 0.24011326802428812, 0.07254968502093107, 0.05995035800151527, 0.08458997798152268, 0.060609022970311344, 0.05871635803487152, 0.17073126405011863, 0.19221214903518558, 0.07043872401118279, 0.08645387296564877, 0.052556092967279255, 0.09072577394545078, 0.05582262994721532, 0.06027361995074898, 0.05488180508837104, 0.05483183194883168, 0.0565616840030998, 0.18482376297470182, 0.0873353430069983, 0.10241292498540133, 0.13355842290911824, 0.25679181795567274, 0.31450845196377486, 0.11670836096163839, 0.06159682897850871, 0.1070660250261426, 0.10728254704736173, 0.11018073302693665, 0.22666284698061645, 0.2398763169767335, 0.09199287602677941, 0.07596493198070675, 0.057800051057711244, 0.13387534604407847, 0.37498289509676397, 0.12053445307537913, 4.2161102630198, 4.682000249042176, 4.184833992971107, 4.321797244017944, 4.216800067923032, 3.686243324074894, 3.4890068050008267, 3.5567835910478607, 4.906429070048034, 4.109745619003661, 3.778066450962797, 4.182183710043319, 3.5669509419240057, 3.302419172017835, 4.59681926399935, 4.891830081003718, 3.2918047679122537, 3.9750024009263143, 3.2008736489806324, 4.2891042770352215, 4.118595478939824, 3.579961044015363, 3.3071298280265182, 3.5286175329238176, 4.216227023978718, 3.4874641919741407, 4.231093719950877, 0.1793860460165888, 0.08585365395992994, 0.07299589307513088, 0.1066153229912743, 0.050843736971728504, 0.10447391797788441, 0.06956905603874475, 0.05952962092123926, 0.08989010099321604, 0.1805835870327428, 0.055670298053883016, 0.1126758469035849, 0.1767732900334522, 0.1434135230956599, 0.081930385902524, 0.04664967593271285, 0.09726618800777942, 0.10335366195067763, 0.08606857899576426, 0.0917891759891063, 0.0866844350239262, 0.08451750897802413, 0.056254241964779794, 0.04665245907381177, 0.09999329899437726, 0.0889953919686377, 0.048414119984954596, 0.10666774900164455, 0.06363885395694524, 0.0812870099907741, 0.10507707297801971, 0.10790593596175313, 0.33801126305479556, 0.05153962492477149, 0.05529652698896825, 0.05988565401639789, 0.0650225100107491, 0.07012886193115264, 0.06029131100513041, 0.07284032902680337, 0.0530514360871166, 0.20859179203398526, 0.0509045859798789, 0.0533092999830842, 0.051241424982436, 0.06042770796921104, 0.07409918704070151, 0.0584008350269869, 0.22149025194812566, 0.060601142002269626, 0.07685398601461202, 0.06953978899400681, 0.08328306500334293, 0.06309848802629858, 0.06431415700353682, 0.05696093908045441, 0.060670531005598605, 0.06204889004584402, 0.06326848396565765, 0.05434223392512649, 0.05932322493754327, 0.05317632493097335, 0.057826222968287766, 0.05297652294393629, 0.06922696100082248, 0.0944686570437625, 0.09822311194147915, 0.07309767499100417, 0.04978968296200037, 0.0728605620097369, 0.07517757709138095, 0.05632751598022878, 0.11386128293816, 0.11005161597859114, 0.10230901895556599, 0.07519620994571596, 0.07055019692052156, 0.10145026608370245, 0.071732246899046, 0.048041034024208784, 0.10718491696752608, 0.0809009560616687, 0.055324439075775445, 0.11279626202303916, 0.07482220605015755, 0.3468500110320747, 0.06573820998892188, 0.07290520798414946, 0.13817322999238968, 0.10145983600523323, 0.07440577901434153, 0.08921465498860925, 0.056833794922567904, 0.08283755299635231, 0.053147005033679307, 0.10384963208343834, 0.0980237569892779, 0.05912108696065843, 0.22372348699718714, 0.10512248298618942, 0.0843505630036816, 0.04813926701899618, 0.11077298794407398, 0.08663045905996114, 0.09564876405056566, 0.1065133810043335, 0.0937646150123328, 0.15998169605154544, 0.10955806402489543, 0.09687567804940045, 0.05046546796802431, 0.09430843207519501]
[0.0012559256839184936, 0.00545711972782473, 0.0016488564777484332, 0.0013625081363980744, 0.001922499499580061, 0.0013774777947798032, 0.0013344626826107162, 0.0038802560011390597, 0.004368457932617854, 0.0016008800911632452, 0.00196486074921929, 0.0011944566583472558, 0.0020619494078511543, 0.0012686961351639845, 0.0013698549988806587, 0.0012473137520084326, 0.0012461779988370836, 0.001285492818252268, 0.004200540067606859, 0.0019848941592499614, 0.0023275664769409395, 0.00303541870247996, 0.005836177680810744, 0.007147919362813065, 0.0026524627491281453, 0.0013999279313297434, 0.00243331875059415, 0.0024382397056218574, 0.0025041075687940147, 0.005151428340468556, 0.0054517344767439435, 0.002090747182426805, 0.0017264757268342444, 0.0013136375240388918, 0.0030426215010017836, 0.008522338524926454, 0.0027394193880767984, 0.09582068779590455, 0.10640909656914035, 0.09510986347661608, 0.09822266463677144, 0.0958363651800689, 0.0837782573653385, 0.07929560920456424, 0.0808359907056332, 0.11150975159200077, 0.09340330952281048, 0.08586514661279084, 0.09504962977371179, 0.08106706686190922, 0.07505498118222352, 0.10447316509089433, 0.11117795638644813, 0.07481374472527849, 0.09034096365741623, 0.07274712838592347, 0.0974796426598914, 0.09360444270317782, 0.08136275100034916, 0.07516204154605723, 0.08019585302099586, 0.09582334145406177, 0.0792605498175941, 0.09616122090797448, 0.00407695559128611, 0.001951219408180226, 0.0016589975698893381, 0.0024230755225289613, 0.0011555394766301933, 0.0023744072267701003, 0.0015811149099714714, 0.0013529459300281649, 0.00204295684075491, 0.004104172432562336, 0.0012652340466791595, 0.0025608147023542024, 0.00401757477348755, 0.003259398252174089, 0.0018620542250573635, 0.0010602199075616557, 0.002210595181994987, 0.002403573533736689, 0.0020015948603666106, 0.002134631999746658, 0.002015917093579679, 0.001965523464605212, 0.001308238185227437, 0.001084940908693297, 0.002325425558008773, 0.0020696602783404116, 0.0011259097670919674, 0.0024806453256196407, 0.001479973347835936, 0.001890395581180793, 0.0024436528599539467, 0.0025094403712035613, 0.007860727047785943, 0.0011985959284830578, 0.0012859657439294942, 0.0013926896282883232, 0.0015121513955988163, 0.001630903765840759, 0.001402123511747219, 0.001693961140158218, 0.0012337543276073628, 0.004850971907767099, 0.0011838275809274163, 0.001239751162397307, 0.0011916610461031628, 0.0014052955341676986, 0.001723236907923291, 0.0013581589541159743, 0.005150936091816876, 0.0014093288837737123, 0.0017873020003398145, 0.0016172043952094608, 0.0019368154651940215, 0.0014674066982860135, 0.0014956780698496936, 0.0013246730018710328, 0.001410942581525549, 0.001442997442926605, 0.0014713600922245966, 0.001263772881979686, 0.0013796098822684481, 0.0012366587193249616, 0.0013447958829834363, 0.0012320121614868905, 0.0016099293256005228, 0.0021969455126456395, 0.002284258417243701, 0.0016999459300233528, 0.0011578996037674505, 0.0016944316746450442, 0.001748315746311185, 0.0013099422320983437, 0.002647936812515349, 0.0025593399064788636, 0.0023792795105945577, 0.0017487490685050224, 0.0016407022539656176, 0.0023593085135744755, 0.0016681917883499071, 0.0011172333494002043, 0.0024926724876168857, 0.0018814175828295045, 0.001286614862227336, 0.0026231688842567246, 0.001740051303492036, 0.008066279326327318, 0.0015287955811377181, 0.0016954699531197548, 0.003213330930055574, 0.002359531069889145, 0.0017303669538218961, 0.0020747594183397502, 0.0013217161609899513, 0.0019264547208454027, 0.001235976861248356, 0.002415107722870659, 0.0022796222555646023, 0.0013749089990850798, 0.005202871790632259, 0.002444708906655568, 0.0019616410000856187, 0.001119517837651074, 0.002576115998699395, 0.0020146618386037473, 0.0022243898616410617, 0.0024770553721938024, 0.0021805724421472745, 0.003720504559338266, 0.0025478619540673358, 0.002252922745334894, 0.0011736155341401002, 0.0021932193505859307]
[796.2254556973431, 183.24684996394816, 606.4809238979564, 733.9405712787884, 520.1561822088556, 725.9645155730841, 749.3652786480471, 257.7149548139214, 228.9137300678405, 624.6564033870716, 508.94191885981536, 837.200741451497, 484.97795154059713, 788.2108034251603, 730.0042711214866, 801.7229012265707, 802.4535828213839, 777.9117750028205, 238.0646259540912, 503.8052005643834, 429.6332714476427, 329.443842189874, 171.34502317980886, 139.90085075700264, 377.00812210414506, 714.3224859083527, 410.961366962642, 410.1319479353472, 399.34386703747026, 194.12091829836916, 183.42786213558432, 478.29790632039226, 579.2146303925465, 761.2450022936416, 328.66394971269017, 117.33868551163071, 365.0408565962757, 10.436159695805697, 9.397692793587813, 10.51415661264052, 10.180949617870901, 10.434452497453128, 11.936271193123778, 12.6110387451622, 12.370727336558927, 8.967825555372645, 10.706258751525128, 11.64616889911693, 10.520819516927498, 12.335465420297172, 13.323566061153661, 9.571835974625392, 8.994588788123231, 13.366527817476223, 11.069175704081704, 13.746247064145278, 10.25855217267283, 10.683253605504888, 12.29063653459442, 13.304588053096289, 12.469472701265397, 10.435870684799752, 12.616616996744854, 10.399202407766765, 245.2810626972127, 512.500027320164, 602.7736376170244, 412.6986512398512, 865.3966569071439, 421.1577478056691, 632.4651002235146, 739.1278378576316, 489.4866010142836, 243.6544800277008, 790.3676024405798, 390.50072583568124, 248.90638168058948, 306.8050979449898, 537.0412883487286, 943.2005500630975, 452.3668594525453, 416.04718389678766, 499.6016026024571, 468.46482209518166, 496.0521457875495, 508.77031895462835, 764.3867999665139, 921.7091843318916, 430.0288162551567, 483.1710839045841, 888.1706414030223, 403.12090957630545, 675.6878436103135, 528.989810363063, 409.22342792128256, 398.49522286930716, 127.21469578080065, 834.3095252005397, 777.6256908246438, 718.0350737795355, 661.3094448813422, 613.1569629949826, 713.2039307677514, 590.3323141796504, 810.534137650657, 206.14425707121848, 844.7176059342992, 806.6134804554648, 839.164797129258, 711.5940922649133, 580.3032626576696, 736.2908420766552, 194.1394694429751, 709.5575855383975, 559.50253499961, 618.3510278368244, 516.3114493717782, 681.4743323497418, 668.5930750462189, 754.9032844993076, 708.7460631592628, 693.001921037266, 679.6432805840669, 791.281419517019, 724.8425898165734, 808.6305335281642, 743.6072735302364, 811.6802993187342, 621.1452789252025, 455.17742440310434, 437.7788399294373, 588.2540040472127, 863.6327335688746, 590.1683820974863, 571.97906162541, 763.3924424271291, 377.6525162056538, 390.7257482558456, 420.29530181181195, 571.8373310442327, 609.4951095379893, 423.8530036434056, 599.4514581498753, 895.0681614873545, 401.1758483987794, 531.5141142117309, 777.2333659109457, 381.2183066068009, 574.6956989102229, 123.97289500453154, 654.1096876116083, 589.8069724915779, 311.20355225370616, 423.81302486810733, 577.9120999688996, 481.9835934521089, 756.5920955759599, 519.0882449399908, 809.076635132137, 414.0602054849025, 438.66916878837293, 727.3208631738105, 192.20154565417013, 409.04665470705413, 509.7772731893112, 893.2416852760101, 388.1812777471475, 496.3612159810629, 449.56148076589494, 403.7051457248413, 458.59517467590774, 268.78074843103036, 392.4859423422168, 443.8678610132951, 852.067794699644, 455.9507464371243]
Elapsed: 0.6899498368445414~1.4060549918192649
Time per graph: 0.0157056694720454~0.031945605946015206
Speed: 467.28199681430976~269.37693210198506
Total Time: 0.0950
best val loss: 0.3246515691280365 test_score: 0.7907

Testing...
Test loss: 0.4157 score: 0.8837 time: 0.05s
test Score 0.8837
Epoch Time List: [0.2527120130835101, 0.45489073207136244, 0.36572073807474226, 0.4766757390461862, 0.3441435049753636, 0.24954736384097487, 0.25492085004225373, 0.35477050696499646, 1.013615798088722, 0.4551418861374259, 0.3734592639375478, 0.4392651579109952, 0.2735367820132524, 0.2518493660027161, 0.24304882797878236, 0.2627406121464446, 0.24332914303522557, 0.23675642092712224, 0.37484650302212685, 0.5025792887900025, 0.32430877594742924, 0.41521838505286723, 1.0605554258218035, 0.7053733309730887, 0.46970954397693276, 0.566779570071958, 0.39183433109428734, 0.3557409649947658, 0.45721942803356797, 0.8737525549950078, 1.2579432261409238, 0.3814164699288085, 0.5369477020576596, 0.38383855402935296, 0.34816617507021874, 0.8667589022079483, 1.1534729440463707, 4.507924158126116, 21.931613394990563, 20.25848874798976, 20.038364618085325, 20.22677334700711, 19.43984421202913, 18.228742890059948, 16.978252914035693, 21.21434587892145, 19.293807941139676, 17.059922640910372, 19.775313613936305, 16.58224428293761, 17.192949536954984, 18.76404243905563, 20.22133793204557, 15.366125197033398, 17.378581136115827, 17.888148608035408, 17.376774145057425, 16.583317924989387, 18.55805362004321, 16.42892918002326, 16.19947156694252, 17.397638550959527, 17.36212866997812, 17.828447391977534, 11.911079859011807, 0.30321758217178285, 0.26908985699992627, 0.3565210660453886, 0.31849073513876647, 0.5044939620420337, 0.45390821411274374, 0.37428910902235657, 0.38228636502753943, 0.42079788993578404, 0.36270680802408606, 0.37293768278323114, 0.9732922631083056, 1.2190153839765117, 0.4439961389871314, 0.334900313988328, 0.37099571397993714, 0.3836493289563805, 0.31035707308910787, 0.34459645801689476, 0.32489980990067124, 0.2975567679386586, 0.2778857669327408, 0.3285545469261706, 0.3849075500620529, 0.331793752964586, 0.3767813278827816, 0.37935250089503825, 0.2879862899426371, 0.34344724589027464, 0.5504214409738779, 0.4710513091413304, 1.088570796069689, 0.5552727149333805, 0.2637647739611566, 0.24811712792143226, 0.3963576969690621, 0.3063304490642622, 0.379595102975145, 0.4562345129670575, 0.25506587186828256, 0.42250024690292776, 0.36750004801433533, 0.2626929240068421, 0.24919473298359662, 0.2470913629513234, 0.3592810349073261, 0.2596927129197866, 0.4507181409280747, 0.261649587075226, 0.27402882697060704, 0.36828642699401826, 0.44263486796990037, 0.3094595681177452, 0.314095588051714, 0.2615679249865934, 0.2675084810471162, 0.49348061706405133, 0.282637805910781, 0.35568639100529253, 0.2678666439605877, 0.2542953679803759, 0.27929566998500377, 0.2834563790820539, 0.3024838790297508, 0.3588054380379617, 0.3735118268523365, 0.32122034998610616, 0.3902921580011025, 0.39572685887105763, 0.33259052387438715, 0.29550941300112754, 0.5472654839977622, 0.34764158609323204, 0.46233697596471757, 0.3148696100106463, 0.33569537999574095, 0.36201972514390945, 0.3018928321544081, 0.3694165130145848, 0.3900036469567567, 0.337160152150318, 0.34577761485707015, 0.4134599689859897, 0.3190892090788111, 0.5663908759597689, 0.3784897660370916, 0.3993635819060728, 0.4747396589955315, 0.38118638610467315, 0.3114379229955375, 0.2907490690704435, 0.36832039593718946, 0.32777326100040227, 0.394399679149501, 0.47558526101056486, 0.3980371019570157, 0.34432657214347273, 0.728432287927717, 0.37584112712647766, 0.31351051398087293, 0.35902877093758434, 0.3647726019844413, 0.313431782880798, 0.32540053804405034, 0.368089324911125, 0.2858052640222013, 0.40290856698993593, 0.4236466761212796, 0.30335148598533124, 0.42199764191173017, 0.5593887409195304]
Total Epoch List: [81, 95]
Total Time List: [0.09817989403381944, 0.09497222304344177]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e98b20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7536;  Loss pred: 0.7536; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7850 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9841 score: 0.5116 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7504;  Loss pred: 0.7504; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7697 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9438 score: 0.5116 time: 0.09s
Epoch 3/1000, LR 0.000030
Train loss: 0.7124;  Loss pred: 0.7124; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7586 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9158 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.7385;  Loss pred: 0.7385; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7416 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8821 score: 0.5116 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7258 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8416 score: 0.5116 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7464;  Loss pred: 0.7464; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7136 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8062 score: 0.5116 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6654;  Loss pred: 0.6654; Loss self: 0.0000; time: 0.15s
Val loss: 0.7081 score: 0.5455 time: 0.04s
Test loss: 0.7893 score: 0.4884 time: 0.17s
Epoch 8/1000, LR 0.000180
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.16s
Val loss: 0.7042 score: 0.5455 time: 0.04s
Test loss: 0.7773 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.5920;  Loss pred: 0.5920; Loss self: 0.0000; time: 0.14s
Val loss: 0.7004 score: 0.5455 time: 0.04s
Test loss: 0.7629 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.15s
Val loss: 0.6956 score: 0.5455 time: 0.05s
Test loss: 0.7473 score: 0.5116 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.15s
Val loss: 0.6933 score: 0.5227 time: 0.04s
Test loss: 0.7296 score: 0.4884 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.5514;  Loss pred: 0.5514; Loss self: 0.0000; time: 0.17s
Val loss: 0.6913 score: 0.5000 time: 0.06s
Test loss: 0.7161 score: 0.5349 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.5884;  Loss pred: 0.5884; Loss self: 0.0000; time: 0.17s
Val loss: 0.6897 score: 0.5227 time: 0.06s
Test loss: 0.7114 score: 0.5581 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.20s
Val loss: 0.6883 score: 0.5682 time: 0.04s
Test loss: 0.7089 score: 0.5814 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.4742;  Loss pred: 0.4742; Loss self: 0.0000; time: 0.14s
Val loss: 0.6871 score: 0.5909 time: 0.05s
Test loss: 0.7054 score: 0.5349 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.4679;  Loss pred: 0.4679; Loss self: 0.0000; time: 0.14s
Val loss: 0.6857 score: 0.5682 time: 0.06s
Test loss: 0.6988 score: 0.5814 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.4503;  Loss pred: 0.4503; Loss self: 0.0000; time: 0.16s
Val loss: 0.6842 score: 0.5682 time: 0.06s
Test loss: 0.6930 score: 0.5581 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.4487;  Loss pred: 0.4487; Loss self: 0.0000; time: 0.15s
Val loss: 0.6834 score: 0.5455 time: 0.05s
Test loss: 0.6881 score: 0.6047 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.4096;  Loss pred: 0.4096; Loss self: 0.0000; time: 0.14s
Val loss: 0.6826 score: 0.5227 time: 0.04s
Test loss: 0.6848 score: 0.5349 time: 0.09s
Epoch 20/1000, LR 0.000270
Train loss: 0.3979;  Loss pred: 0.3979; Loss self: 0.0000; time: 0.15s
Val loss: 0.6817 score: 0.5227 time: 0.16s
Test loss: 0.6801 score: 0.5349 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.3748;  Loss pred: 0.3748; Loss self: 0.0000; time: 0.15s
Val loss: 0.6810 score: 0.5000 time: 0.05s
Test loss: 0.6748 score: 0.5581 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.3669;  Loss pred: 0.3669; Loss self: 0.0000; time: 0.15s
Val loss: 0.6810 score: 0.5000 time: 0.04s
Test loss: 0.6693 score: 0.5116 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.3472;  Loss pred: 0.3472; Loss self: 0.0000; time: 0.15s
Val loss: 0.6813 score: 0.5227 time: 0.05s
Test loss: 0.6657 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3335;  Loss pred: 0.3335; Loss self: 0.0000; time: 0.15s
Val loss: 0.6816 score: 0.5227 time: 0.05s
Test loss: 0.6632 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3103;  Loss pred: 0.3103; Loss self: 0.0000; time: 0.15s
Val loss: 0.6833 score: 0.5455 time: 0.05s
Test loss: 0.6618 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2955;  Loss pred: 0.2955; Loss self: 0.0000; time: 0.27s
Val loss: 0.6851 score: 0.5455 time: 0.09s
Test loss: 0.6604 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2919;  Loss pred: 0.2919; Loss self: 0.0000; time: 0.16s
Val loss: 0.6870 score: 0.5455 time: 0.11s
Test loss: 0.6592 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.3001;  Loss pred: 0.3001; Loss self: 0.0000; time: 0.16s
Val loss: 0.6897 score: 0.5455 time: 0.05s
Test loss: 0.6593 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2686;  Loss pred: 0.2686; Loss self: 0.0000; time: 0.15s
Val loss: 0.6909 score: 0.5455 time: 0.10s
Test loss: 0.6584 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2293;  Loss pred: 0.2293; Loss self: 0.0000; time: 0.15s
Val loss: 0.6917 score: 0.5455 time: 0.10s
Test loss: 0.6580 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2456;  Loss pred: 0.2456; Loss self: 0.0000; time: 0.15s
Val loss: 0.6912 score: 0.5455 time: 0.05s
Test loss: 0.6568 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2109;  Loss pred: 0.2109; Loss self: 0.0000; time: 0.16s
Val loss: 0.6891 score: 0.5455 time: 0.07s
Test loss: 0.6537 score: 0.5349 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.2093;  Loss pred: 0.2093; Loss self: 0.0000; time: 0.21s
Val loss: 0.6858 score: 0.5455 time: 0.18s
Test loss: 0.6497 score: 0.5349 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.2038;  Loss pred: 0.2038; Loss self: 0.0000; time: 0.19s
Val loss: 0.6808 score: 0.5455 time: 0.16s
Test loss: 0.6446 score: 0.5349 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.1878;  Loss pred: 0.1878; Loss self: 0.0000; time: 0.18s
Val loss: 0.6762 score: 0.5455 time: 0.04s
Test loss: 0.6394 score: 0.5349 time: 0.20s
Epoch 36/1000, LR 0.000270
Train loss: 0.1742;  Loss pred: 0.1742; Loss self: 0.0000; time: 0.35s
Val loss: 0.6729 score: 0.5455 time: 0.21s
Test loss: 0.6356 score: 0.5349 time: 0.10s
Epoch 37/1000, LR 0.000270
Train loss: 0.1705;  Loss pred: 0.1705; Loss self: 0.0000; time: 0.42s
Val loss: 0.6711 score: 0.5455 time: 0.07s
Test loss: 0.6339 score: 0.5349 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.1715;  Loss pred: 0.1715; Loss self: 0.0000; time: 0.20s
Val loss: 0.6726 score: 0.5455 time: 0.05s
Test loss: 0.6359 score: 0.5349 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1545;  Loss pred: 0.1545; Loss self: 0.0000; time: 0.38s
Val loss: 0.6743 score: 0.5455 time: 0.05s
Test loss: 0.6396 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1368;  Loss pred: 0.1368; Loss self: 0.0000; time: 0.19s
Val loss: 0.6750 score: 0.5455 time: 0.11s
Test loss: 0.6435 score: 0.5349 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1458;  Loss pred: 0.1458; Loss self: 0.0000; time: 0.17s
Val loss: 0.6748 score: 0.5455 time: 0.20s
Test loss: 0.6450 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.1271;  Loss pred: 0.1271; Loss self: 0.0000; time: 0.16s
Val loss: 0.6714 score: 0.5455 time: 0.05s
Test loss: 0.6438 score: 0.5349 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.19s
Val loss: 0.6662 score: 0.5455 time: 0.05s
Test loss: 0.6407 score: 0.5349 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 0.21s
Val loss: 0.6592 score: 0.5455 time: 0.05s
Test loss: 0.6354 score: 0.5349 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0971;  Loss pred: 0.0971; Loss self: 0.0000; time: 0.28s
Val loss: 0.6513 score: 0.5227 time: 0.06s
Test loss: 0.6298 score: 0.5349 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.19s
Val loss: 0.6420 score: 0.5227 time: 0.05s
Test loss: 0.6241 score: 0.5349 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0883;  Loss pred: 0.0883; Loss self: 0.0000; time: 0.21s
Val loss: 0.6354 score: 0.5227 time: 0.05s
Test loss: 0.6205 score: 0.5349 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0907;  Loss pred: 0.0907; Loss self: 0.0000; time: 0.27s
Val loss: 0.6308 score: 0.5227 time: 0.07s
Test loss: 0.6197 score: 0.5349 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0760;  Loss pred: 0.0760; Loss self: 0.0000; time: 0.17s
Val loss: 0.6273 score: 0.5227 time: 0.09s
Test loss: 0.6201 score: 0.6047 time: 0.10s
Epoch 50/1000, LR 0.000269
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.20s
Val loss: 0.6275 score: 0.5455 time: 0.04s
Test loss: 0.6238 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0660;  Loss pred: 0.0660; Loss self: 0.0000; time: 0.31s
Val loss: 0.6247 score: 0.5682 time: 0.05s
Test loss: 0.6251 score: 0.6047 time: 0.09s
Epoch 52/1000, LR 0.000269
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.18s
Val loss: 0.6163 score: 0.5682 time: 0.09s
Test loss: 0.6230 score: 0.6047 time: 0.29s
Epoch 53/1000, LR 0.000269
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.16s
Val loss: 0.6016 score: 0.5909 time: 0.06s
Test loss: 0.6147 score: 0.6047 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.18s
Val loss: 0.5916 score: 0.6136 time: 0.09s
Test loss: 0.6119 score: 0.6744 time: 0.17s
Epoch 55/1000, LR 0.000269
Train loss: 0.0455;  Loss pred: 0.0455; Loss self: 0.0000; time: 0.16s
Val loss: 0.5815 score: 0.6591 time: 0.07s
Test loss: 0.6055 score: 0.6744 time: 0.10s
Epoch 56/1000, LR 0.000269
Train loss: 0.0449;  Loss pred: 0.0449; Loss self: 0.0000; time: 0.16s
Val loss: 0.5744 score: 0.6818 time: 0.11s
Test loss: 0.6023 score: 0.6744 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0406;  Loss pred: 0.0406; Loss self: 0.0000; time: 0.24s
Val loss: 0.5692 score: 0.6818 time: 0.06s
Test loss: 0.6007 score: 0.6977 time: 0.08s
Epoch 58/1000, LR 0.000269
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.18s
Val loss: 0.5679 score: 0.7273 time: 0.05s
Test loss: 0.6035 score: 0.6977 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.15s
Val loss: 0.5602 score: 0.7273 time: 0.12s
Test loss: 0.6006 score: 0.6977 time: 0.07s
Epoch 60/1000, LR 0.000268
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.17s
Val loss: 0.5503 score: 0.7727 time: 0.05s
Test loss: 0.5960 score: 0.7209 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.23s
Val loss: 0.5348 score: 0.7727 time: 0.14s
Test loss: 0.5855 score: 0.7209 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.32s
Val loss: 0.5089 score: 0.7955 time: 0.06s
Test loss: 0.5648 score: 0.7442 time: 0.09s
Epoch 63/1000, LR 0.000268
Train loss: 0.0265;  Loss pred: 0.0265; Loss self: 0.0000; time: 0.25s
Val loss: 0.4839 score: 0.8182 time: 0.05s
Test loss: 0.5428 score: 0.7674 time: 0.09s
Epoch 64/1000, LR 0.000268
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.20s
Val loss: 0.4571 score: 0.8182 time: 0.16s
Test loss: 0.5159 score: 0.7674 time: 0.12s
Epoch 65/1000, LR 0.000268
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.20s
Val loss: 0.4329 score: 0.8182 time: 0.06s
Test loss: 0.4921 score: 0.7907 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.0192;  Loss pred: 0.0192; Loss self: 0.0000; time: 0.22s
Val loss: 0.4122 score: 0.8409 time: 0.10s
Test loss: 0.4718 score: 0.7907 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.15s
Val loss: 0.3965 score: 0.8864 time: 0.06s
Test loss: 0.4565 score: 0.8140 time: 0.11s
Epoch 68/1000, LR 0.000268
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.15s
Val loss: 0.3844 score: 0.8864 time: 0.06s
Test loss: 0.4440 score: 0.8140 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.20s
Val loss: 0.3789 score: 0.8636 time: 0.05s
Test loss: 0.4419 score: 0.8140 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.0154;  Loss pred: 0.0154; Loss self: 0.0000; time: 0.19s
Val loss: 0.3748 score: 0.8636 time: 0.05s
Test loss: 0.4426 score: 0.7907 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.15s
Val loss: 0.3711 score: 0.8636 time: 0.12s
Test loss: 0.4437 score: 0.8140 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.15s
Val loss: 0.3668 score: 0.8636 time: 0.05s
Test loss: 0.4443 score: 0.8140 time: 0.08s
Epoch 73/1000, LR 0.000267
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.21s
Val loss: 0.3597 score: 0.8636 time: 0.05s
Test loss: 0.4416 score: 0.8140 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.24s
Val loss: 0.3517 score: 0.8409 time: 0.05s
Test loss: 0.4361 score: 0.8140 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.15s
Val loss: 0.3413 score: 0.8409 time: 0.13s
Test loss: 0.4218 score: 0.8140 time: 0.06s
Epoch 76/1000, LR 0.000267
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.30s
Val loss: 0.3326 score: 0.7727 time: 0.06s
Test loss: 0.4089 score: 0.8140 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.50s
Val loss: 0.3251 score: 0.8182 time: 0.11s
Test loss: 0.3938 score: 0.8605 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.18s
Val loss: 0.3192 score: 0.8409 time: 0.09s
Test loss: 0.3789 score: 0.8605 time: 0.27s
Epoch 79/1000, LR 0.000267
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.20s
Val loss: 0.3153 score: 0.8409 time: 0.06s
Test loss: 0.3606 score: 0.8605 time: 0.14s
Epoch 80/1000, LR 0.000267
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.37s
Val loss: 0.3149 score: 0.8409 time: 0.13s
Test loss: 0.3410 score: 0.8605 time: 0.07s
Epoch 81/1000, LR 0.000267
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.46s
Val loss: 0.3184 score: 0.8409 time: 0.07s
Test loss: 0.3270 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.23s
Val loss: 0.3230 score: 0.8409 time: 0.07s
Test loss: 0.3194 score: 0.8605 time: 0.20s
     INFO: Early stopping counter 2 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.17s
Val loss: 0.3278 score: 0.8409 time: 0.04s
Test loss: 0.3166 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.29s
Val loss: 0.3336 score: 0.8409 time: 0.07s
Test loss: 0.3194 score: 0.8605 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.21s
Val loss: 0.3394 score: 0.8409 time: 0.05s
Test loss: 0.3209 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.46s
Val loss: 0.3473 score: 0.8409 time: 0.10s
Test loss: 0.3173 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.44s
Val loss: 0.3554 score: 0.8182 time: 0.05s
Test loss: 0.3143 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.32s
Val loss: 0.3644 score: 0.8182 time: 0.06s
Test loss: 0.3132 score: 0.8605 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.69s
Val loss: 0.3731 score: 0.8182 time: 0.28s
Test loss: 0.3131 score: 0.8837 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.27s
Val loss: 0.3829 score: 0.8182 time: 0.07s
Test loss: 0.3133 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.19s
Val loss: 0.3938 score: 0.8182 time: 0.15s
Test loss: 0.3145 score: 0.8837 time: 0.14s
     INFO: Early stopping counter 11 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.18s
Val loss: 0.4055 score: 0.8182 time: 0.16s
Test loss: 0.3151 score: 0.8837 time: 0.08s
     INFO: Early stopping counter 12 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.30s
Val loss: 0.4188 score: 0.8182 time: 0.05s
Test loss: 0.3147 score: 0.8837 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.30s
Val loss: 0.4297 score: 0.8182 time: 0.05s
Test loss: 0.3214 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.32s
Val loss: 0.4417 score: 0.8182 time: 0.04s
Test loss: 0.3260 score: 0.8837 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.43s
Val loss: 0.4568 score: 0.8182 time: 0.08s
Test loss: 0.3262 score: 0.8837 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.19s
Val loss: 0.4744 score: 0.8182 time: 0.13s
Test loss: 0.3230 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.19s
Val loss: 0.4991 score: 0.8182 time: 0.05s
Test loss: 0.3152 score: 0.9070 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.42s
Val loss: 0.5212 score: 0.8182 time: 0.05s
Test loss: 0.3135 score: 0.9070 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.15s
Val loss: 0.5376 score: 0.8182 time: 0.15s
Test loss: 0.3158 score: 0.9070 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 079,   Train_Loss: 0.0085,   Val_Loss: 0.3149,   Val_Precision: 0.8947,   Val_Recall: 0.7727,   Val_accuracy: 0.8293,   Val_Score: 0.8409,   Val_Loss: 0.3149,   Test_Precision: 0.8000,   Test_Recall: 0.9524,   Test_accuracy: 0.8696,   Test_Score: 0.8605,   Test_loss: 0.3410


[0.05526073009241372, 0.24011326802428812, 0.07254968502093107, 0.05995035800151527, 0.08458997798152268, 0.060609022970311344, 0.05871635803487152, 0.17073126405011863, 0.19221214903518558, 0.07043872401118279, 0.08645387296564877, 0.052556092967279255, 0.09072577394545078, 0.05582262994721532, 0.06027361995074898, 0.05488180508837104, 0.05483183194883168, 0.0565616840030998, 0.18482376297470182, 0.0873353430069983, 0.10241292498540133, 0.13355842290911824, 0.25679181795567274, 0.31450845196377486, 0.11670836096163839, 0.06159682897850871, 0.1070660250261426, 0.10728254704736173, 0.11018073302693665, 0.22666284698061645, 0.2398763169767335, 0.09199287602677941, 0.07596493198070675, 0.057800051057711244, 0.13387534604407847, 0.37498289509676397, 0.12053445307537913, 4.2161102630198, 4.682000249042176, 4.184833992971107, 4.321797244017944, 4.216800067923032, 3.686243324074894, 3.4890068050008267, 3.5567835910478607, 4.906429070048034, 4.109745619003661, 3.778066450962797, 4.182183710043319, 3.5669509419240057, 3.302419172017835, 4.59681926399935, 4.891830081003718, 3.2918047679122537, 3.9750024009263143, 3.2008736489806324, 4.2891042770352215, 4.118595478939824, 3.579961044015363, 3.3071298280265182, 3.5286175329238176, 4.216227023978718, 3.4874641919741407, 4.231093719950877, 0.1793860460165888, 0.08585365395992994, 0.07299589307513088, 0.1066153229912743, 0.050843736971728504, 0.10447391797788441, 0.06956905603874475, 0.05952962092123926, 0.08989010099321604, 0.1805835870327428, 0.055670298053883016, 0.1126758469035849, 0.1767732900334522, 0.1434135230956599, 0.081930385902524, 0.04664967593271285, 0.09726618800777942, 0.10335366195067763, 0.08606857899576426, 0.0917891759891063, 0.0866844350239262, 0.08451750897802413, 0.056254241964779794, 0.04665245907381177, 0.09999329899437726, 0.0889953919686377, 0.048414119984954596, 0.10666774900164455, 0.06363885395694524, 0.0812870099907741, 0.10507707297801971, 0.10790593596175313, 0.33801126305479556, 0.05153962492477149, 0.05529652698896825, 0.05988565401639789, 0.0650225100107491, 0.07012886193115264, 0.06029131100513041, 0.07284032902680337, 0.0530514360871166, 0.20859179203398526, 0.0509045859798789, 0.0533092999830842, 0.051241424982436, 0.06042770796921104, 0.07409918704070151, 0.0584008350269869, 0.22149025194812566, 0.060601142002269626, 0.07685398601461202, 0.06953978899400681, 0.08328306500334293, 0.06309848802629858, 0.06431415700353682, 0.05696093908045441, 0.060670531005598605, 0.06204889004584402, 0.06326848396565765, 0.05434223392512649, 0.05932322493754327, 0.05317632493097335, 0.057826222968287766, 0.05297652294393629, 0.06922696100082248, 0.0944686570437625, 0.09822311194147915, 0.07309767499100417, 0.04978968296200037, 0.0728605620097369, 0.07517757709138095, 0.05632751598022878, 0.11386128293816, 0.11005161597859114, 0.10230901895556599, 0.07519620994571596, 0.07055019692052156, 0.10145026608370245, 0.071732246899046, 0.048041034024208784, 0.10718491696752608, 0.0809009560616687, 0.055324439075775445, 0.11279626202303916, 0.07482220605015755, 0.3468500110320747, 0.06573820998892188, 0.07290520798414946, 0.13817322999238968, 0.10145983600523323, 0.07440577901434153, 0.08921465498860925, 0.056833794922567904, 0.08283755299635231, 0.053147005033679307, 0.10384963208343834, 0.0980237569892779, 0.05912108696065843, 0.22372348699718714, 0.10512248298618942, 0.0843505630036816, 0.04813926701899618, 0.11077298794407398, 0.08663045905996114, 0.09564876405056566, 0.1065133810043335, 0.0937646150123328, 0.15998169605154544, 0.10955806402489543, 0.09687567804940045, 0.05046546796802431, 0.09430843207519501, 0.08891729288734496, 0.09523942891974002, 0.05201823101378977, 0.08606930903624743, 0.07981620996724814, 0.08606757305096835, 0.17230390198528767, 0.04870512010529637, 0.053447913960553706, 0.06359530100598931, 0.06580318801570684, 0.05953087389934808, 0.06045683997217566, 0.051578991930000484, 0.05305540794506669, 0.05775900394655764, 0.058408081997185946, 0.05732120806351304, 0.09081640501972288, 0.06568327697459608, 0.059939051046967506, 0.05327452998608351, 0.062236467958427966, 0.05484860704746097, 0.05674530297983438, 0.055428256979212165, 0.06431838299613446, 0.06481958494987339, 0.054592355038039386, 0.059593267971649766, 0.055544092087075114, 0.06472062296234071, 0.07911117502953857, 0.08475537097547203, 0.2059427360072732, 0.10760440793819726, 0.0765857599908486, 0.08550290402490646, 0.056432721903547645, 0.1221235430566594, 0.05739931995049119, 0.09140550205484033, 0.09102033299859613, 0.05527986399829388, 0.0872462420957163, 0.08707097009755671, 0.05558214208576828, 0.07937821792438626, 0.10024885192979127, 0.0545056879054755, 0.09250383405014873, 0.29713202407583594, 0.08798587403725833, 0.1711771950358525, 0.10523502598516643, 0.05273802694864571, 0.08374858391471207, 0.08967075892724097, 0.08043208299204707, 0.07853342802263796, 0.05346699105575681, 0.09071868099272251, 0.09782193298451602, 0.1238617820199579, 0.20890395890455693, 0.08662688499316573, 0.11808491696137935, 0.07867295597679913, 0.09459575603250414, 0.05262493307236582, 0.05260196095332503, 0.08417958102654666, 0.09336086607072502, 0.05229026998858899, 0.06859027105383575, 0.09649448900017887, 0.23331076698377728, 0.2786753550171852, 0.14492296800017357, 0.07662798801902682, 0.10703691898379475, 0.2025873310631141, 0.056627435027621686, 0.0836390299955383, 0.10837393603287637, 0.07617523998487741, 0.05545870098285377, 0.09231511491816491, 0.09299767389893532, 0.11427351203747094, 0.14640741201583296, 0.08186884201131761, 0.09484500903636217, 0.05150555504951626, 0.09176985488738865, 0.10131771094165742, 0.0663808910176158, 0.10463443701155484, 0.09702445496805012, 0.07404382294043899]
[0.0012559256839184936, 0.00545711972782473, 0.0016488564777484332, 0.0013625081363980744, 0.001922499499580061, 0.0013774777947798032, 0.0013344626826107162, 0.0038802560011390597, 0.004368457932617854, 0.0016008800911632452, 0.00196486074921929, 0.0011944566583472558, 0.0020619494078511543, 0.0012686961351639845, 0.0013698549988806587, 0.0012473137520084326, 0.0012461779988370836, 0.001285492818252268, 0.004200540067606859, 0.0019848941592499614, 0.0023275664769409395, 0.00303541870247996, 0.005836177680810744, 0.007147919362813065, 0.0026524627491281453, 0.0013999279313297434, 0.00243331875059415, 0.0024382397056218574, 0.0025041075687940147, 0.005151428340468556, 0.0054517344767439435, 0.002090747182426805, 0.0017264757268342444, 0.0013136375240388918, 0.0030426215010017836, 0.008522338524926454, 0.0027394193880767984, 0.09582068779590455, 0.10640909656914035, 0.09510986347661608, 0.09822266463677144, 0.0958363651800689, 0.0837782573653385, 0.07929560920456424, 0.0808359907056332, 0.11150975159200077, 0.09340330952281048, 0.08586514661279084, 0.09504962977371179, 0.08106706686190922, 0.07505498118222352, 0.10447316509089433, 0.11117795638644813, 0.07481374472527849, 0.09034096365741623, 0.07274712838592347, 0.0974796426598914, 0.09360444270317782, 0.08136275100034916, 0.07516204154605723, 0.08019585302099586, 0.09582334145406177, 0.0792605498175941, 0.09616122090797448, 0.00407695559128611, 0.001951219408180226, 0.0016589975698893381, 0.0024230755225289613, 0.0011555394766301933, 0.0023744072267701003, 0.0015811149099714714, 0.0013529459300281649, 0.00204295684075491, 0.004104172432562336, 0.0012652340466791595, 0.0025608147023542024, 0.00401757477348755, 0.003259398252174089, 0.0018620542250573635, 0.0010602199075616557, 0.002210595181994987, 0.002403573533736689, 0.0020015948603666106, 0.002134631999746658, 0.002015917093579679, 0.001965523464605212, 0.001308238185227437, 0.001084940908693297, 0.002325425558008773, 0.0020696602783404116, 0.0011259097670919674, 0.0024806453256196407, 0.001479973347835936, 0.001890395581180793, 0.0024436528599539467, 0.0025094403712035613, 0.007860727047785943, 0.0011985959284830578, 0.0012859657439294942, 0.0013926896282883232, 0.0015121513955988163, 0.001630903765840759, 0.001402123511747219, 0.001693961140158218, 0.0012337543276073628, 0.004850971907767099, 0.0011838275809274163, 0.001239751162397307, 0.0011916610461031628, 0.0014052955341676986, 0.001723236907923291, 0.0013581589541159743, 0.005150936091816876, 0.0014093288837737123, 0.0017873020003398145, 0.0016172043952094608, 0.0019368154651940215, 0.0014674066982860135, 0.0014956780698496936, 0.0013246730018710328, 0.001410942581525549, 0.001442997442926605, 0.0014713600922245966, 0.001263772881979686, 0.0013796098822684481, 0.0012366587193249616, 0.0013447958829834363, 0.0012320121614868905, 0.0016099293256005228, 0.0021969455126456395, 0.002284258417243701, 0.0016999459300233528, 0.0011578996037674505, 0.0016944316746450442, 0.001748315746311185, 0.0013099422320983437, 0.002647936812515349, 0.0025593399064788636, 0.0023792795105945577, 0.0017487490685050224, 0.0016407022539656176, 0.0023593085135744755, 0.0016681917883499071, 0.0011172333494002043, 0.0024926724876168857, 0.0018814175828295045, 0.001286614862227336, 0.0026231688842567246, 0.001740051303492036, 0.008066279326327318, 0.0015287955811377181, 0.0016954699531197548, 0.003213330930055574, 0.002359531069889145, 0.0017303669538218961, 0.0020747594183397502, 0.0013217161609899513, 0.0019264547208454027, 0.001235976861248356, 0.002415107722870659, 0.0022796222555646023, 0.0013749089990850798, 0.005202871790632259, 0.002444708906655568, 0.0019616410000856187, 0.001119517837651074, 0.002576115998699395, 0.0020146618386037473, 0.0022243898616410617, 0.0024770553721938024, 0.0021805724421472745, 0.003720504559338266, 0.0025478619540673358, 0.002252922745334894, 0.0011736155341401002, 0.0021932193505859307, 0.0020678440206359293, 0.002214870439993954, 0.0012097263026462739, 0.002001611838052266, 0.001856190929470887, 0.0020015714663015895, 0.004007067488029946, 0.0011326772117510784, 0.0012429747432686907, 0.0014789604885113794, 0.001530306698039694, 0.0013844389278918159, 0.0014059730226087363, 0.0011995114402325695, 0.0012338466963968998, 0.001343232649919945, 0.00135832748830665, 0.0013330513503142568, 0.002112009419063323, 0.0015275180691766532, 0.0013939314196969187, 0.0012389425578158955, 0.0014473597199634411, 0.0012755490011037436, 0.0013196582088333575, 0.0012890292320747016, 0.001495776348747313, 0.0015074322081365904, 0.0012695896520474276, 0.0013858899528290643, 0.0012917230717924444, 0.0015051307665660632, 0.001839794768128804, 0.001971055138964466, 0.004789365953657516, 0.002502428091585983, 0.0017810641858336885, 0.0019884396284861965, 0.0013123888814778521, 0.0028400823966664977, 0.0013348679058253765, 0.0021257093501125656, 0.00211675193019991, 0.0012855782325184623, 0.0020289823743189837, 0.0020249062813385284, 0.0012926079554829834, 0.001846005068008983, 0.0023313686495300295, 0.0012675741373366395, 0.0021512519546546217, 0.006910047071531068, 0.0020461831171455424, 0.0039808650008337785, 0.002447326185701545, 0.0012264657429917607, 0.0019476414863886528, 0.0020853664866800226, 0.001870513557954583, 0.0018263587912241387, 0.001243418396645507, 0.0021097367672726166, 0.0022749286740585124, 0.002880506558603672, 0.004858231602431556, 0.002014578720771296, 0.0027461608595669615, 0.0018296036273674216, 0.0021999013030814915, 0.0012238356528457167, 0.0012233014175191867, 0.001957664675035969, 0.002171182931877326, 0.001216052790432302, 0.001595122582647343, 0.00224405788372509, 0.005425831790320402, 0.006480822209701981, 0.0033703015813993853, 0.0017820462330006236, 0.0024892306740417385, 0.0047113332805375375, 0.0013169170936656205, 0.001945093720826472, 0.0025203240937878226, 0.0017715172089506374, 0.00128973723215939, 0.002146863137631742, 0.0021627366023008214, 0.002657523535755138, 0.003404823535251929, 0.001903926558402735, 0.0022056978845665623, 0.0011978036058027037, 0.002134182671799736, 0.0023562258358524983, 0.0015437416515724604, 0.0024333590002687173, 0.0022563826736755844, 0.0017219493707078834]
[796.2254556973431, 183.24684996394816, 606.4809238979564, 733.9405712787884, 520.1561822088556, 725.9645155730841, 749.3652786480471, 257.7149548139214, 228.9137300678405, 624.6564033870716, 508.94191885981536, 837.200741451497, 484.97795154059713, 788.2108034251603, 730.0042711214866, 801.7229012265707, 802.4535828213839, 777.9117750028205, 238.0646259540912, 503.8052005643834, 429.6332714476427, 329.443842189874, 171.34502317980886, 139.90085075700264, 377.00812210414506, 714.3224859083527, 410.961366962642, 410.1319479353472, 399.34386703747026, 194.12091829836916, 183.42786213558432, 478.29790632039226, 579.2146303925465, 761.2450022936416, 328.66394971269017, 117.33868551163071, 365.0408565962757, 10.436159695805697, 9.397692793587813, 10.51415661264052, 10.180949617870901, 10.434452497453128, 11.936271193123778, 12.6110387451622, 12.370727336558927, 8.967825555372645, 10.706258751525128, 11.64616889911693, 10.520819516927498, 12.335465420297172, 13.323566061153661, 9.571835974625392, 8.994588788123231, 13.366527817476223, 11.069175704081704, 13.746247064145278, 10.25855217267283, 10.683253605504888, 12.29063653459442, 13.304588053096289, 12.469472701265397, 10.435870684799752, 12.616616996744854, 10.399202407766765, 245.2810626972127, 512.500027320164, 602.7736376170244, 412.6986512398512, 865.3966569071439, 421.1577478056691, 632.4651002235146, 739.1278378576316, 489.4866010142836, 243.6544800277008, 790.3676024405798, 390.50072583568124, 248.90638168058948, 306.8050979449898, 537.0412883487286, 943.2005500630975, 452.3668594525453, 416.04718389678766, 499.6016026024571, 468.46482209518166, 496.0521457875495, 508.77031895462835, 764.3867999665139, 921.7091843318916, 430.0288162551567, 483.1710839045841, 888.1706414030223, 403.12090957630545, 675.6878436103135, 528.989810363063, 409.22342792128256, 398.49522286930716, 127.21469578080065, 834.3095252005397, 777.6256908246438, 718.0350737795355, 661.3094448813422, 613.1569629949826, 713.2039307677514, 590.3323141796504, 810.534137650657, 206.14425707121848, 844.7176059342992, 806.6134804554648, 839.164797129258, 711.5940922649133, 580.3032626576696, 736.2908420766552, 194.1394694429751, 709.5575855383975, 559.50253499961, 618.3510278368244, 516.3114493717782, 681.4743323497418, 668.5930750462189, 754.9032844993076, 708.7460631592628, 693.001921037266, 679.6432805840669, 791.281419517019, 724.8425898165734, 808.6305335281642, 743.6072735302364, 811.6802993187342, 621.1452789252025, 455.17742440310434, 437.7788399294373, 588.2540040472127, 863.6327335688746, 590.1683820974863, 571.97906162541, 763.3924424271291, 377.6525162056538, 390.7257482558456, 420.29530181181195, 571.8373310442327, 609.4951095379893, 423.8530036434056, 599.4514581498753, 895.0681614873545, 401.1758483987794, 531.5141142117309, 777.2333659109457, 381.2183066068009, 574.6956989102229, 123.97289500453154, 654.1096876116083, 589.8069724915779, 311.20355225370616, 423.81302486810733, 577.9120999688996, 481.9835934521089, 756.5920955759599, 519.0882449399908, 809.076635132137, 414.0602054849025, 438.66916878837293, 727.3208631738105, 192.20154565417013, 409.04665470705413, 509.7772731893112, 893.2416852760101, 388.1812777471475, 496.3612159810629, 449.56148076589494, 403.7051457248413, 458.59517467590774, 268.78074843103036, 392.4859423422168, 443.8678610132951, 852.067794699644, 455.9507464371243, 483.5954694941002, 451.4936774372815, 826.6332622614735, 499.5973649781582, 538.7376827043612, 499.60744187053854, 249.5590610807618, 882.8640583790291, 804.5215764966131, 676.1505853388495, 653.4637803526502, 722.3142746518776, 711.2512003569835, 833.6727491370264, 810.4734590773854, 744.472671997363, 736.1994869489415, 750.1586490004737, 473.48273685422316, 654.6567403546392, 717.3954083174544, 807.1399224213251, 690.9132444457271, 783.9761539029009, 757.7719695193265, 775.7775969056132, 668.5491456242659, 663.3797490874553, 787.6560732732274, 721.5580125670627, 774.1597420044233, 664.3940993123723, 543.5388866862947, 507.3424787727503, 208.79590527767576, 399.6118823003711, 561.4620786571572, 502.90689527310525, 761.9692715423815, 352.1024605390796, 749.1377953099257, 470.4311998002199, 472.421914789778, 777.860090273144, 492.85790387195664, 493.8500162777745, 773.6297736356959, 541.7103221057538, 428.9325929649031, 788.908491065578, 464.84559739100746, 144.71681446569778, 488.7148132641304, 251.2016860131035, 408.60920209266766, 815.3509429139579, 513.4415173370617, 479.53201817874975, 534.6125376891176, 547.5375401619406, 804.2345221027765, 473.9927821861682, 439.57422111875826, 347.16116059973524, 205.83621404535296, 496.3816949367671, 364.14472827265064, 546.5664721264677, 454.5658473856346, 817.103176945986, 817.4600189934919, 510.812711059225, 460.5784180218035, 822.3327209705295, 626.9110668224328, 445.62130382306384, 184.30353882034902, 154.30140924140312, 296.70935251580346, 561.1526690394513, 401.73054688270827, 212.25414133425633, 759.349244390559, 514.1140446307642, 396.774368211149, 564.4878835765601, 775.3517344968891, 465.7958779352487, 462.37715630102747, 376.29017637875745, 293.70097734771684, 525.2303433589013, 453.3712468045044, 834.8614039526568, 468.5634520482305, 424.4075354679206, 647.7767824566997, 410.9545693379273, 443.1872357763801, 580.7371674283929]
Elapsed: 0.4724126599355351~1.1596277820749756
Time per graph: 0.010769736216886419~0.026344978035671984
Speed: 501.3561631567699~246.06956748736704
Total Time: 0.0751
best val loss: 0.3148742616176605 test_score: 0.8605

Testing...
Test loss: 0.4565 score: 0.8140 time: 0.06s
test Score 0.8140
Epoch Time List: [0.2527120130835101, 0.45489073207136244, 0.36572073807474226, 0.4766757390461862, 0.3441435049753636, 0.24954736384097487, 0.25492085004225373, 0.35477050696499646, 1.013615798088722, 0.4551418861374259, 0.3734592639375478, 0.4392651579109952, 0.2735367820132524, 0.2518493660027161, 0.24304882797878236, 0.2627406121464446, 0.24332914303522557, 0.23675642092712224, 0.37484650302212685, 0.5025792887900025, 0.32430877594742924, 0.41521838505286723, 1.0605554258218035, 0.7053733309730887, 0.46970954397693276, 0.566779570071958, 0.39183433109428734, 0.3557409649947658, 0.45721942803356797, 0.8737525549950078, 1.2579432261409238, 0.3814164699288085, 0.5369477020576596, 0.38383855402935296, 0.34816617507021874, 0.8667589022079483, 1.1534729440463707, 4.507924158126116, 21.931613394990563, 20.25848874798976, 20.038364618085325, 20.22677334700711, 19.43984421202913, 18.228742890059948, 16.978252914035693, 21.21434587892145, 19.293807941139676, 17.059922640910372, 19.775313613936305, 16.58224428293761, 17.192949536954984, 18.76404243905563, 20.22133793204557, 15.366125197033398, 17.378581136115827, 17.888148608035408, 17.376774145057425, 16.583317924989387, 18.55805362004321, 16.42892918002326, 16.19947156694252, 17.397638550959527, 17.36212866997812, 17.828447391977534, 11.911079859011807, 0.30321758217178285, 0.26908985699992627, 0.3565210660453886, 0.31849073513876647, 0.5044939620420337, 0.45390821411274374, 0.37428910902235657, 0.38228636502753943, 0.42079788993578404, 0.36270680802408606, 0.37293768278323114, 0.9732922631083056, 1.2190153839765117, 0.4439961389871314, 0.334900313988328, 0.37099571397993714, 0.3836493289563805, 0.31035707308910787, 0.34459645801689476, 0.32489980990067124, 0.2975567679386586, 0.2778857669327408, 0.3285545469261706, 0.3849075500620529, 0.331793752964586, 0.3767813278827816, 0.37935250089503825, 0.2879862899426371, 0.34344724589027464, 0.5504214409738779, 0.4710513091413304, 1.088570796069689, 0.5552727149333805, 0.2637647739611566, 0.24811712792143226, 0.3963576969690621, 0.3063304490642622, 0.379595102975145, 0.4562345129670575, 0.25506587186828256, 0.42250024690292776, 0.36750004801433533, 0.2626929240068421, 0.24919473298359662, 0.2470913629513234, 0.3592810349073261, 0.2596927129197866, 0.4507181409280747, 0.261649587075226, 0.27402882697060704, 0.36828642699401826, 0.44263486796990037, 0.3094595681177452, 0.314095588051714, 0.2615679249865934, 0.2675084810471162, 0.49348061706405133, 0.282637805910781, 0.35568639100529253, 0.2678666439605877, 0.2542953679803759, 0.27929566998500377, 0.2834563790820539, 0.3024838790297508, 0.3588054380379617, 0.3735118268523365, 0.32122034998610616, 0.3902921580011025, 0.39572685887105763, 0.33259052387438715, 0.29550941300112754, 0.5472654839977622, 0.34764158609323204, 0.46233697596471757, 0.3148696100106463, 0.33569537999574095, 0.36201972514390945, 0.3018928321544081, 0.3694165130145848, 0.3900036469567567, 0.337160152150318, 0.34577761485707015, 0.4134599689859897, 0.3190892090788111, 0.5663908759597689, 0.3784897660370916, 0.3993635819060728, 0.4747396589955315, 0.38118638610467315, 0.3114379229955375, 0.2907490690704435, 0.36832039593718946, 0.32777326100040227, 0.394399679149501, 0.47558526101056486, 0.3980371019570157, 0.34432657214347273, 0.728432287927717, 0.37584112712647766, 0.31351051398087293, 0.35902877093758434, 0.3647726019844413, 0.313431782880798, 0.32540053804405034, 0.368089324911125, 0.2858052640222013, 0.40290856698993593, 0.4236466761212796, 0.30335148598533124, 0.42199764191173017, 0.5593887409195304, 0.2995173509698361, 0.32870187098160386, 0.29386174911633134, 0.4057801300659776, 0.3779387909453362, 0.5355639300541952, 0.36081140593159944, 0.2494201340014115, 0.23329327604733407, 0.25894663005601615, 0.2572823589434847, 0.2831097471062094, 0.2885718250181526, 0.2927942380774766, 0.2415432840352878, 0.2554195310221985, 0.2677516088588163, 0.25030399207025766, 0.27165957912802696, 0.37656064005568624, 0.253946294891648, 0.2408641860820353, 0.2556701850844547, 0.25319372303783894, 0.25030850095208734, 0.4143452701391652, 0.3301503959810361, 0.26698493503499776, 0.29549938801210374, 0.3013707189820707, 0.24856081698089838, 0.28478117811027914, 0.46824384795036167, 0.4305106858955696, 0.42027183796744794, 0.6637926739640534, 0.5646645990200341, 0.3258510029409081, 0.476990299182944, 0.40858690079767257, 0.42668875108938664, 0.29182015103287995, 0.32254121673759073, 0.3096760349581018, 0.42456248798407614, 0.325858857948333, 0.30958271003328264, 0.4124624659307301, 0.3530292030191049, 0.2970166510203853, 0.44470361806452274, 0.5561135081807151, 0.30474129191134125, 0.43059120210818946, 0.3311100531136617, 0.3170197340659797, 0.3707857569679618, 0.31507387792225927, 0.33785926096607, 0.2969727418385446, 0.41656729101669043, 0.46464536688290536, 0.39148563193157315, 0.47359563189093024, 0.46354145102668554, 0.4031780241057277, 0.32409489003475755, 0.2879461869597435, 0.33774897409603, 0.2878610879415646, 0.3202114888699725, 0.27886115305591375, 0.3466883449582383, 0.33745966700371355, 0.3407933459384367, 0.44606404996011406, 0.833002018975094, 0.5465867359889671, 0.3995701919775456, 0.5709624380106106, 0.6291940418304875, 0.4948114762082696, 0.26533573493361473, 0.4335455539403483, 0.3664791320916265, 0.6320266008842736, 0.5330338570056483, 0.46832233504392207, 1.064238202990964, 0.44377772009465843, 0.477451893966645, 0.4176769129699096, 0.4457220198819414, 0.3959237430244684, 0.45151100994553417, 0.5969676979584619, 0.3871073320042342, 0.33693347801454365, 0.5686045490438119, 0.366815158049576]
Total Epoch List: [81, 95, 100]
Total Time List: [0.09817989403381944, 0.09497222304344177, 0.07513746991753578]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e9ba30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8973;  Loss pred: 0.8973; Loss self: 0.0000; time: 0.13s
Val loss: 1.3314 score: 0.5581 time: 0.15s
Test loss: 1.0807 score: 0.5227 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7311;  Loss pred: 0.7311; Loss self: 0.0000; time: 0.21s
Val loss: 1.1217 score: 0.5581 time: 0.05s
Test loss: 0.9515 score: 0.5227 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.8509;  Loss pred: 0.8509; Loss self: 0.0000; time: 0.15s
Val loss: 0.9929 score: 0.5581 time: 0.08s
Test loss: 0.8711 score: 0.5455 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7878;  Loss pred: 0.7878; Loss self: 0.0000; time: 0.19s
Val loss: 0.9054 score: 0.5116 time: 0.17s
Test loss: 0.8194 score: 0.5227 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7369;  Loss pred: 0.7369; Loss self: 0.0000; time: 0.17s
Val loss: 0.8429 score: 0.5349 time: 0.07s
Test loss: 0.7829 score: 0.4773 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.7413;  Loss pred: 0.7413; Loss self: 0.0000; time: 0.37s
Val loss: 0.8072 score: 0.5349 time: 0.15s
Test loss: 0.7601 score: 0.5227 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.7278;  Loss pred: 0.7278; Loss self: 0.0000; time: 0.13s
Val loss: 0.7768 score: 0.5349 time: 0.15s
Test loss: 0.7372 score: 0.5455 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.7484;  Loss pred: 0.7484; Loss self: 0.0000; time: 0.13s
Val loss: 0.7517 score: 0.5581 time: 0.08s
Test loss: 0.7185 score: 0.5455 time: 0.09s
Epoch 9/1000, LR 0.000210
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.17s
Val loss: 0.7219 score: 0.5814 time: 0.06s
Test loss: 0.7064 score: 0.5682 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.21s
Val loss: 0.7044 score: 0.5581 time: 0.12s
Test loss: 0.7013 score: 0.5909 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 0.13s
Val loss: 0.6975 score: 0.5349 time: 0.13s
Test loss: 0.7000 score: 0.5909 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.13s
Val loss: 0.6926 score: 0.5116 time: 0.08s
Test loss: 0.6993 score: 0.5909 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5782;  Loss pred: 0.5782; Loss self: 0.0000; time: 0.21s
Val loss: 0.6895 score: 0.5116 time: 0.08s
Test loss: 0.6978 score: 0.5455 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.5641;  Loss pred: 0.5641; Loss self: 0.0000; time: 0.14s
Val loss: 0.6899 score: 0.5349 time: 0.15s
Test loss: 0.6973 score: 0.5455 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.18s
Val loss: 0.6912 score: 0.5581 time: 0.10s
Test loss: 0.6980 score: 0.5909 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.19s
Val loss: 0.6935 score: 0.6047 time: 0.06s
Test loss: 0.6992 score: 0.5909 time: 0.32s
     INFO: Early stopping counter 3 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4856;  Loss pred: 0.4856; Loss self: 0.0000; time: 0.13s
Val loss: 0.6984 score: 0.6047 time: 0.05s
Test loss: 0.7013 score: 0.6136 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4861;  Loss pred: 0.4861; Loss self: 0.0000; time: 0.13s
Val loss: 0.7067 score: 0.6047 time: 0.05s
Test loss: 0.7062 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4965;  Loss pred: 0.4965; Loss self: 0.0000; time: 0.13s
Val loss: 0.7209 score: 0.5814 time: 0.06s
Test loss: 0.7125 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4253;  Loss pred: 0.4253; Loss self: 0.0000; time: 0.14s
Val loss: 0.7379 score: 0.6047 time: 0.07s
Test loss: 0.7208 score: 0.6136 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4052;  Loss pred: 0.4052; Loss self: 0.0000; time: 0.15s
Val loss: 0.7604 score: 0.6047 time: 0.05s
Test loss: 0.7326 score: 0.6364 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3777;  Loss pred: 0.3777; Loss self: 0.0000; time: 0.13s
Val loss: 0.7866 score: 0.6279 time: 0.06s
Test loss: 0.7466 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3757;  Loss pred: 0.3757; Loss self: 0.0000; time: 0.15s
Val loss: 0.8084 score: 0.6047 time: 0.05s
Test loss: 0.7593 score: 0.6136 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3471;  Loss pred: 0.3471; Loss self: 0.0000; time: 0.13s
Val loss: 0.8235 score: 0.6047 time: 0.13s
Test loss: 0.7693 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3518;  Loss pred: 0.3518; Loss self: 0.0000; time: 0.13s
Val loss: 0.8245 score: 0.6279 time: 0.05s
Test loss: 0.7725 score: 0.6136 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3073;  Loss pred: 0.3073; Loss self: 0.0000; time: 0.14s
Val loss: 0.8187 score: 0.6279 time: 0.05s
Test loss: 0.7734 score: 0.6136 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.3024;  Loss pred: 0.3024; Loss self: 0.0000; time: 0.14s
Val loss: 0.8033 score: 0.6279 time: 0.05s
Test loss: 0.7654 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.3047;  Loss pred: 0.3047; Loss self: 0.0000; time: 0.15s
Val loss: 0.7777 score: 0.6279 time: 0.07s
Test loss: 0.7484 score: 0.6591 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2946;  Loss pred: 0.2946; Loss self: 0.0000; time: 0.15s
Val loss: 0.7513 score: 0.5814 time: 0.06s
Test loss: 0.7315 score: 0.6591 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2692;  Loss pred: 0.2692; Loss self: 0.0000; time: 0.14s
Val loss: 0.7262 score: 0.6512 time: 0.05s
Test loss: 0.7162 score: 0.6818 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2878;  Loss pred: 0.2878; Loss self: 0.0000; time: 0.13s
Val loss: 0.7063 score: 0.6512 time: 0.05s
Test loss: 0.7053 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2489;  Loss pred: 0.2489; Loss self: 0.0000; time: 0.20s
Val loss: 0.6986 score: 0.6512 time: 0.05s
Test loss: 0.7018 score: 0.7045 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.2440;  Loss pred: 0.2440; Loss self: 0.0000; time: 0.13s
Val loss: 0.6963 score: 0.6512 time: 0.05s
Test loss: 0.6994 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 012,   Train_Loss: 0.5782,   Val_Loss: 0.6895,   Val_Precision: 0.5333,   Val_Recall: 0.3636,   Val_accuracy: 0.4324,   Val_Score: 0.5116,   Val_Loss: 0.6895,   Test_Precision: 0.5714,   Test_Recall: 0.3636,   Test_accuracy: 0.4444,   Test_Score: 0.5455,   Test_loss: 0.6978


[0.05234154895879328, 0.10435782594140619, 0.09259892394766212, 0.08181412890553474, 0.10861501900944859, 0.05076386604923755, 0.05025739793200046, 0.092634552042, 0.0892932410351932, 0.05006870604120195, 0.0491607979638502, 0.09463321999646723, 0.24938921292778105, 0.0769784840522334, 0.08240008703432977, 0.3263337900862098, 0.05130334198474884, 0.05317411699797958, 0.05354859202634543, 0.07076342508662492, 0.07731147203594446, 0.053023722022771835, 0.060262012062594295, 0.05587869498413056, 0.05180633801501244, 0.053437215043231845, 0.05301683093421161, 0.06000712397508323, 0.05415744904894382, 0.07225151604507118, 0.06152941391337663, 0.05178372992668301, 0.05767898098565638]
[0.0011895806581543927, 0.002371768771395595, 0.0021045209988105025, 0.0018594120205803351, 0.0024685231593056496, 0.0011537242283917624, 0.0011422135893636469, 0.0021053307282272726, 0.0020293918417089367, 0.0011379251373000443, 0.0011172908628147773, 0.00215075499991971, 0.005667936657449569, 0.0017495110011871227, 0.0018727292507802222, 0.007416677047413858, 0.0011659850451079283, 0.0012085026590449904, 0.0012170134551442143, 0.0016082596610596572, 0.0017570789099078286, 0.0012050845914266326, 0.0013695911832407794, 0.0012699703405484218, 0.0011774167730684646, 0.001214482160073451, 0.0012049279757775366, 0.0013637982721609826, 0.0012308511147487231, 0.0016420799101152543, 0.0013983957707585598, 0.0011769029528791593, 0.0013108859314921904]
[840.6323632997506, 421.6262614047239, 475.16750869447753, 537.8044182417909, 405.10051373440695, 866.7582559083058, 875.4929982553638, 474.9847549330259, 492.7584606617454, 878.7924330177824, 895.0220871588551, 464.9530048923895, 176.43104721109836, 571.5882891399109, 533.9800185122206, 134.83127195739132, 857.643933081008, 827.4702521467698, 821.6836024064347, 621.7901401202313, 569.1264031234983, 829.8172652063832, 730.1448872018593, 787.4199641293683, 849.3169308212767, 823.396203645775, 829.925124242138, 733.2462728637041, 812.445947375324, 608.9837612895539, 715.1051375516889, 849.6877313066584, 762.8428805103531]
Elapsed: 0.0800780235458109~0.05635372422178929
Time per graph: 0.0018199550805866112~0.0012807664595861203
Speed: 668.9687916377353~202.40353495250173
Total Time: 0.0581
best val loss: 0.6895081400871277 test_score: 0.5455

Testing...
Test loss: 0.7162 score: 0.6818 time: 0.05s
test Score 0.6818
Epoch Time List: [0.32533643394708633, 0.3555040260544047, 0.32359286409337074, 0.42953334480989724, 0.3371338419383392, 0.5603312109597027, 0.3241024250164628, 0.29141403001267463, 0.3165937179001048, 0.36867188999895006, 0.30368647596333176, 0.29590074997395277, 0.5349695300683379, 0.3668816949939355, 0.3579681810224429, 0.5639187239576131, 0.22966309392359108, 0.228890509926714, 0.23579101415816694, 0.2672468830132857, 0.2734047321137041, 0.2372759459540248, 0.2542670149123296, 0.3101370359072462, 0.23653347592335194, 0.24219172890298069, 0.24256062787026167, 0.26929775124881417, 0.2537868960062042, 0.258512009982951, 0.239874763879925, 0.3056842730147764, 0.23406950500793755]
Total Epoch List: [33]
Total Time List: [0.058135161991231143]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e996c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8119;  Loss pred: 0.8119; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1999 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1405 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.9309;  Loss pred: 0.9309; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0384 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9928 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.8929;  Loss pred: 0.8929; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9536 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9211 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.8303;  Loss pred: 0.8303; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9119 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8798 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.8144;  Loss pred: 0.8144; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8797 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8486 score: 0.5116 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.8218;  Loss pred: 0.8218; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8530 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8280 score: 0.5116 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.7908;  Loss pred: 0.7908; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8391 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8235 score: 0.5116 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.7201;  Loss pred: 0.7201; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8250 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8163 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.7373;  Loss pred: 0.7373; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8157 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8088 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8035 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7959 score: 0.5116 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.7003;  Loss pred: 0.7003; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7911 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7834 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7820 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7735 score: 0.5116 time: 0.11s
Epoch 13/1000, LR 0.000270
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7715 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7632 score: 0.5116 time: 0.11s
Epoch 14/1000, LR 0.000270
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7647 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7557 score: 0.5116 time: 0.07s
Epoch 15/1000, LR 0.000270
Train loss: 0.5708;  Loss pred: 0.5708; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7575 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7480 score: 0.5116 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.5756;  Loss pred: 0.5756; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7549 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7431 score: 0.5116 time: 0.12s
Epoch 17/1000, LR 0.000270
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7547 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7416 score: 0.5116 time: 0.10s
Epoch 18/1000, LR 0.000270
Train loss: 0.4989;  Loss pred: 0.4989; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7539 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7404 score: 0.5116 time: 0.10s
Epoch 19/1000, LR 0.000270
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7549 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7402 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4674;  Loss pred: 0.4674; Loss self: 0.0000; time: 0.15s
Val loss: 0.7510 score: 0.5227 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7326 score: 0.5116 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.4707;  Loss pred: 0.4707; Loss self: 0.0000; time: 0.18s
Val loss: 0.7453 score: 0.5000 time: 0.14s
Test loss: 0.7235 score: 0.4651 time: 0.12s
Epoch 22/1000, LR 0.000270
Train loss: 0.4181;  Loss pred: 0.4181; Loss self: 0.0000; time: 0.28s
Val loss: 0.7360 score: 0.4773 time: 0.06s
Test loss: 0.7112 score: 0.4884 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.3995;  Loss pred: 0.3995; Loss self: 0.0000; time: 0.19s
Val loss: 0.7240 score: 0.4773 time: 0.05s
Test loss: 0.6960 score: 0.5116 time: 0.10s
Epoch 24/1000, LR 0.000270
Train loss: 0.3792;  Loss pred: 0.3792; Loss self: 0.0000; time: 0.21s
Val loss: 0.7126 score: 0.4773 time: 0.14s
Test loss: 0.6810 score: 0.5581 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.3583;  Loss pred: 0.3583; Loss self: 0.0000; time: 0.16s
Val loss: 0.7050 score: 0.5000 time: 0.06s
Test loss: 0.6712 score: 0.5814 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.3377;  Loss pred: 0.3377; Loss self: 0.0000; time: 0.18s
Val loss: 0.7019 score: 0.5455 time: 0.07s
Test loss: 0.6657 score: 0.6047 time: 0.19s
Epoch 27/1000, LR 0.000270
Train loss: 0.3187;  Loss pred: 0.3187; Loss self: 0.0000; time: 0.18s
Val loss: 0.6999 score: 0.5682 time: 0.11s
Test loss: 0.6634 score: 0.6047 time: 0.10s
Epoch 28/1000, LR 0.000270
Train loss: 0.3059;  Loss pred: 0.3059; Loss self: 0.0000; time: 0.17s
Val loss: 0.6961 score: 0.5682 time: 0.07s
Test loss: 0.6609 score: 0.6047 time: 0.07s
Epoch 29/1000, LR 0.000270
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 0.22s
Val loss: 0.6912 score: 0.5682 time: 0.07s
Test loss: 0.6570 score: 0.6047 time: 0.19s
Epoch 30/1000, LR 0.000270
Train loss: 0.2733;  Loss pred: 0.2733; Loss self: 0.0000; time: 0.15s
Val loss: 0.6838 score: 0.5909 time: 0.07s
Test loss: 0.6510 score: 0.6047 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.2667;  Loss pred: 0.2667; Loss self: 0.0000; time: 0.23s
Val loss: 0.6673 score: 0.5909 time: 0.06s
Test loss: 0.6377 score: 0.6279 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.2545;  Loss pred: 0.2545; Loss self: 0.0000; time: 0.23s
Val loss: 0.6517 score: 0.5909 time: 0.07s
Test loss: 0.6195 score: 0.6512 time: 0.07s
Epoch 33/1000, LR 0.000270
Train loss: 0.2236;  Loss pred: 0.2236; Loss self: 0.0000; time: 0.19s
Val loss: 0.6310 score: 0.5455 time: 0.06s
Test loss: 0.6025 score: 0.6744 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.29s
Val loss: 0.6127 score: 0.6136 time: 0.06s
Test loss: 0.5879 score: 0.6977 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.1952;  Loss pred: 0.1952; Loss self: 0.0000; time: 0.17s
Val loss: 0.6001 score: 0.5909 time: 0.08s
Test loss: 0.5773 score: 0.6977 time: 0.10s
Epoch 36/1000, LR 0.000270
Train loss: 0.2056;  Loss pred: 0.2056; Loss self: 0.0000; time: 0.24s
Val loss: 0.5900 score: 0.5909 time: 0.20s
Test loss: 0.5683 score: 0.6977 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.1525;  Loss pred: 0.1525; Loss self: 0.0000; time: 0.18s
Val loss: 0.5811 score: 0.6136 time: 0.05s
Test loss: 0.5602 score: 0.6977 time: 0.12s
Epoch 38/1000, LR 0.000270
Train loss: 0.1587;  Loss pred: 0.1587; Loss self: 0.0000; time: 0.19s
Val loss: 0.5716 score: 0.6136 time: 0.06s
Test loss: 0.5543 score: 0.6977 time: 0.15s
Epoch 39/1000, LR 0.000269
Train loss: 0.1640;  Loss pred: 0.1640; Loss self: 0.0000; time: 0.18s
Val loss: 0.5567 score: 0.6364 time: 0.15s
Test loss: 0.5452 score: 0.6977 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.1296;  Loss pred: 0.1296; Loss self: 0.0000; time: 0.21s
Val loss: 0.5418 score: 0.6818 time: 0.07s
Test loss: 0.5368 score: 0.6977 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.1202;  Loss pred: 0.1202; Loss self: 0.0000; time: 0.16s
Val loss: 0.5270 score: 0.6818 time: 0.06s
Test loss: 0.5269 score: 0.6977 time: 0.10s
Epoch 42/1000, LR 0.000269
Train loss: 0.1084;  Loss pred: 0.1084; Loss self: 0.0000; time: 0.21s
Val loss: 0.5121 score: 0.7273 time: 0.14s
Test loss: 0.5187 score: 0.7674 time: 0.29s
Epoch 43/1000, LR 0.000269
Train loss: 0.0939;  Loss pred: 0.0939; Loss self: 0.0000; time: 0.15s
Val loss: 0.4999 score: 0.7045 time: 0.06s
Test loss: 0.5135 score: 0.7674 time: 0.08s
Epoch 44/1000, LR 0.000269
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.20s
Val loss: 0.4893 score: 0.6591 time: 0.07s
Test loss: 0.5103 score: 0.7907 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.14s
Val loss: 0.4760 score: 0.7045 time: 0.15s
Test loss: 0.5042 score: 0.7209 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0778;  Loss pred: 0.0778; Loss self: 0.0000; time: 0.18s
Val loss: 0.4639 score: 0.7045 time: 0.06s
Test loss: 0.4983 score: 0.7442 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 0.19s
Val loss: 0.4562 score: 0.7273 time: 0.15s
Test loss: 0.4964 score: 0.7442 time: 0.10s
Epoch 48/1000, LR 0.000269
Train loss: 0.0677;  Loss pred: 0.0677; Loss self: 0.0000; time: 0.14s
Val loss: 0.4495 score: 0.7273 time: 0.06s
Test loss: 0.4962 score: 0.7674 time: 0.12s
Epoch 49/1000, LR 0.000269
Train loss: 0.0774;  Loss pred: 0.0774; Loss self: 0.0000; time: 0.14s
Val loss: 0.4421 score: 0.7727 time: 0.06s
Test loss: 0.4945 score: 0.7907 time: 0.08s
Epoch 50/1000, LR 0.000269
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 0.19s
Val loss: 0.4339 score: 0.7955 time: 0.05s
Test loss: 0.4962 score: 0.7674 time: 0.09s
Epoch 51/1000, LR 0.000269
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.17s
Val loss: 0.4269 score: 0.7727 time: 0.08s
Test loss: 0.4963 score: 0.7442 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.29s
Val loss: 0.4199 score: 0.7727 time: 0.07s
Test loss: 0.4958 score: 0.7209 time: 0.08s
Epoch 53/1000, LR 0.000269
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.23s
Val loss: 0.4141 score: 0.7955 time: 0.07s
Test loss: 0.4962 score: 0.7442 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.25s
Val loss: 0.4020 score: 0.7955 time: 0.07s
Test loss: 0.4878 score: 0.7442 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.35s
Val loss: 0.3884 score: 0.8409 time: 0.06s
Test loss: 0.4764 score: 0.7907 time: 0.09s
Epoch 56/1000, LR 0.000269
Train loss: 0.0450;  Loss pred: 0.0450; Loss self: 0.0000; time: 0.18s
Val loss: 0.3749 score: 0.8409 time: 0.32s
Test loss: 0.4619 score: 0.7907 time: 0.11s
Epoch 57/1000, LR 0.000269
Train loss: 0.0413;  Loss pred: 0.0413; Loss self: 0.0000; time: 0.14s
Val loss: 0.3643 score: 0.8182 time: 0.06s
Test loss: 0.4474 score: 0.7907 time: 0.07s
Epoch 58/1000, LR 0.000269
Train loss: 0.0430;  Loss pred: 0.0430; Loss self: 0.0000; time: 0.16s
Val loss: 0.3663 score: 0.7955 time: 0.06s
Test loss: 0.4508 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0355;  Loss pred: 0.0355; Loss self: 0.0000; time: 0.18s
Val loss: 0.3783 score: 0.7955 time: 0.05s
Test loss: 0.4734 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.19s
Val loss: 0.3904 score: 0.8409 time: 0.06s
Test loss: 0.4940 score: 0.7674 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.14s
Val loss: 0.3927 score: 0.8409 time: 0.16s
Test loss: 0.4997 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0333;  Loss pred: 0.0333; Loss self: 0.0000; time: 0.29s
Val loss: 0.3830 score: 0.8409 time: 0.06s
Test loss: 0.4912 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.20s
Val loss: 0.3679 score: 0.8409 time: 0.07s
Test loss: 0.4722 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.14s
Val loss: 0.3546 score: 0.8182 time: 0.17s
Test loss: 0.4542 score: 0.7442 time: 0.10s
Epoch 65/1000, LR 0.000268
Train loss: 0.0292;  Loss pred: 0.0292; Loss self: 0.0000; time: 0.15s
Val loss: 0.3415 score: 0.8182 time: 0.07s
Test loss: 0.4382 score: 0.7674 time: 0.11s
Epoch 66/1000, LR 0.000268
Train loss: 0.0234;  Loss pred: 0.0234; Loss self: 0.0000; time: 0.18s
Val loss: 0.3330 score: 0.8182 time: 0.12s
Test loss: 0.4252 score: 0.8140 time: 0.09s
Epoch 67/1000, LR 0.000268
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.15s
Val loss: 0.3287 score: 0.8182 time: 0.13s
Test loss: 0.4185 score: 0.8140 time: 0.10s
Epoch 68/1000, LR 0.000268
Train loss: 0.0230;  Loss pred: 0.0230; Loss self: 0.0000; time: 0.17s
Val loss: 0.3237 score: 0.8409 time: 0.07s
Test loss: 0.4107 score: 0.8140 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.14s
Val loss: 0.3200 score: 0.8409 time: 0.06s
Test loss: 0.4045 score: 0.8140 time: 0.10s
Epoch 70/1000, LR 0.000268
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.17s
Val loss: 0.3226 score: 0.8409 time: 0.30s
Test loss: 0.4071 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.21s
Val loss: 0.3295 score: 0.8182 time: 0.06s
Test loss: 0.4147 score: 0.8140 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0175;  Loss pred: 0.0175; Loss self: 0.0000; time: 0.19s
Val loss: 0.3357 score: 0.8182 time: 0.13s
Test loss: 0.4234 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.17s
Val loss: 0.3359 score: 0.8182 time: 0.14s
Test loss: 0.4219 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.19s
Val loss: 0.3367 score: 0.8182 time: 0.05s
Test loss: 0.4228 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.19s
Val loss: 0.3359 score: 0.8182 time: 0.07s
Test loss: 0.4199 score: 0.8140 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.20s
Val loss: 0.3303 score: 0.8409 time: 0.16s
Test loss: 0.4124 score: 0.8140 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.16s
Val loss: 0.3211 score: 0.8636 time: 0.05s
Test loss: 0.4034 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.18s
Val loss: 0.3130 score: 0.8636 time: 0.07s
Test loss: 0.3969 score: 0.8605 time: 0.15s
Epoch 79/1000, LR 0.000267
Train loss: 0.0109;  Loss pred: 0.0109; Loss self: 0.0000; time: 0.15s
Val loss: 0.3066 score: 0.8636 time: 0.17s
Test loss: 0.3917 score: 0.8372 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.20s
Val loss: 0.2992 score: 0.8864 time: 0.05s
Test loss: 0.3854 score: 0.8605 time: 0.06s
Epoch 81/1000, LR 0.000267
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.18s
Val loss: 0.2914 score: 0.8864 time: 0.05s
Test loss: 0.3755 score: 0.8605 time: 0.10s
Epoch 82/1000, LR 0.000267
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.27s
Val loss: 0.2850 score: 0.8864 time: 0.14s
Test loss: 0.3776 score: 0.8605 time: 0.06s
Epoch 83/1000, LR 0.000266
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.25s
Val loss: 0.2784 score: 0.9091 time: 0.08s
Test loss: 0.3844 score: 0.8372 time: 0.21s
Epoch 84/1000, LR 0.000266
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.20s
Val loss: 0.2725 score: 0.9091 time: 0.06s
Test loss: 0.3933 score: 0.8372 time: 0.10s
Epoch 85/1000, LR 0.000266
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.16s
Val loss: 0.2712 score: 0.9091 time: 0.14s
Test loss: 0.3986 score: 0.8605 time: 0.11s
Epoch 86/1000, LR 0.000266
Train loss: 0.0105;  Loss pred: 0.0105; Loss self: 0.0000; time: 0.16s
Val loss: 0.2769 score: 0.9091 time: 0.12s
Test loss: 0.3954 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.22s
Val loss: 0.2877 score: 0.8864 time: 0.06s
Test loss: 0.3977 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.86s
Val loss: 0.3055 score: 0.8636 time: 0.05s
Test loss: 0.4050 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.15s
Val loss: 0.3260 score: 0.8636 time: 0.05s
Test loss: 0.4154 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.16s
Val loss: 0.3509 score: 0.8409 time: 0.05s
Test loss: 0.4285 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.16s
Val loss: 0.3804 score: 0.8182 time: 0.08s
Test loss: 0.4490 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.24s
Val loss: 0.4103 score: 0.7955 time: 0.05s
Test loss: 0.4712 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.14s
Val loss: 0.4321 score: 0.8182 time: 0.19s
Test loss: 0.4863 score: 0.8140 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.18s
Val loss: 0.4433 score: 0.8182 time: 0.09s
Test loss: 0.4941 score: 0.8140 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.16s
Val loss: 0.4406 score: 0.8182 time: 0.12s
Test loss: 0.4921 score: 0.8140 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.45s
Val loss: 0.4314 score: 0.8182 time: 0.06s
Test loss: 0.4866 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.13s
Val loss: 0.4155 score: 0.8182 time: 0.05s
Test loss: 0.4776 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.14s
Val loss: 0.3995 score: 0.8409 time: 0.05s
Test loss: 0.4716 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.27s
Val loss: 0.3849 score: 0.8636 time: 0.14s
Test loss: 0.4699 score: 0.8837 time: 0.34s
     INFO: Early stopping counter 14 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.34s
Val loss: 0.3730 score: 0.8636 time: 0.09s
Test loss: 0.4723 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.16s
Val loss: 0.3609 score: 0.8636 time: 0.05s
Test loss: 0.4760 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.14s
Val loss: 0.3572 score: 0.8636 time: 0.05s
Test loss: 0.4811 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.15s
Val loss: 0.3630 score: 0.8636 time: 0.05s
Test loss: 0.4855 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.15s
Val loss: 0.3787 score: 0.8636 time: 0.13s
Test loss: 0.4890 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.15s
Val loss: 0.3922 score: 0.8636 time: 0.06s
Test loss: 0.4930 score: 0.8837 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.0099,   Val_Loss: 0.2712,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2712,   Test_Precision: 0.8333,   Test_Recall: 0.9091,   Test_accuracy: 0.8696,   Test_Score: 0.8605,   Test_loss: 0.3986


[0.05234154895879328, 0.10435782594140619, 0.09259892394766212, 0.08181412890553474, 0.10861501900944859, 0.05076386604923755, 0.05025739793200046, 0.092634552042, 0.0892932410351932, 0.05006870604120195, 0.0491607979638502, 0.09463321999646723, 0.24938921292778105, 0.0769784840522334, 0.08240008703432977, 0.3263337900862098, 0.05130334198474884, 0.05317411699797958, 0.05354859202634543, 0.07076342508662492, 0.07731147203594446, 0.053023722022771835, 0.060262012062594295, 0.05587869498413056, 0.05180633801501244, 0.053437215043231845, 0.05301683093421161, 0.06000712397508323, 0.05415744904894382, 0.07225151604507118, 0.06152941391337663, 0.05178372992668301, 0.05767898098565638, 0.050372860045172274, 0.05217859509866685, 0.052187744062393904, 0.05935527803376317, 0.06488222896587104, 0.06646498397458345, 0.06073645199649036, 0.0499346989672631, 0.05108915700111538, 0.05997331510297954, 0.056263150996528566, 0.12226705392822623, 0.10974072804674506, 0.0782076440518722, 0.04780720896087587, 0.12605628895107657, 0.10620910197030753, 0.10475497005973011, 0.06299908796790987, 0.09325145895127207, 0.12121499900240451, 0.09573729999829084, 0.09994207008276135, 0.0636629139771685, 0.08745704800821841, 0.19004128000233322, 0.10431797301862389, 0.07813970500137657, 0.19921187905129045, 0.09818280802574009, 0.08332442701794207, 0.07040681003127247, 0.05032832897268236, 0.06991704099345952, 0.10979722603224218, 0.05232254997827113, 0.12086822499986738, 0.15890699194278568, 0.05778511508833617, 0.07996027905028313, 0.10062793898396194, 0.2900070829782635, 0.08040620503015816, 0.09411215002182871, 0.06991335097700357, 0.08705527591519058, 0.10006450908258557, 0.11965038708876818, 0.08760746696498245, 0.09571605001110584, 0.048016687971539795, 0.08021935797296464, 0.08548919507302344, 0.05041678692214191, 0.09620306000579149, 0.11196502193342894, 0.07646076299715787, 0.0851311479927972, 0.051374700968153775, 0.09315103699918836, 0.05237455607857555, 0.06514574494212866, 0.06615853006951511, 0.10841397393960506, 0.11038434004876763, 0.09525928401853889, 0.10701023100409657, 0.07585710799321532, 0.10666151694022119, 0.11019784503150731, 0.08111780602484941, 0.05122081597801298, 0.053486360935494304, 0.11201051902025938, 0.0924020940437913, 0.07521450007334352, 0.10441943001933396, 0.1508450050605461, 0.05675439001061022, 0.0624177020508796, 0.10368118702899665, 0.06262765103019774, 0.21501047699712217, 0.1014676260529086, 0.11353002593386918, 0.10156408604234457, 0.049630760098807514, 0.05273575196042657, 0.05526908906176686, 0.06471861700993031, 0.06998725899029523, 0.05106296401936561, 0.10612158104777336, 0.06514532899018377, 0.07547811593394727, 0.055528111057356, 0.0499484590254724, 0.05805536999832839, 0.34513748704921454, 0.0552488099783659, 0.053300685016438365, 0.11376191803719848, 0.050667813047766685, 0.049668578896671534, 0.07459025492426008]
[0.0011895806581543927, 0.002371768771395595, 0.0021045209988105025, 0.0018594120205803351, 0.0024685231593056496, 0.0011537242283917624, 0.0011422135893636469, 0.0021053307282272726, 0.0020293918417089367, 0.0011379251373000443, 0.0011172908628147773, 0.00215075499991971, 0.005667936657449569, 0.0017495110011871227, 0.0018727292507802222, 0.007416677047413858, 0.0011659850451079283, 0.0012085026590449904, 0.0012170134551442143, 0.0016082596610596572, 0.0017570789099078286, 0.0012050845914266326, 0.0013695911832407794, 0.0012699703405484218, 0.0011774167730684646, 0.001214482160073451, 0.0012049279757775366, 0.0013637982721609826, 0.0012308511147487231, 0.0016420799101152543, 0.0013983957707585598, 0.0011769029528791593, 0.0013108859314921904, 0.0011714618615156344, 0.0012134556999689964, 0.0012136684665673, 0.0013803553031107715, 0.001508889045717931, 0.0015456973017344987, 0.0014124756278253573, 0.0011612720690061186, 0.0011881199302584973, 0.0013947282582088265, 0.0013084453720122922, 0.002843419858795959, 0.0025521099545754667, 0.0018187824198109812, 0.0011117955572296713, 0.0029315416035134087, 0.002469979115588547, 0.002436162094412328, 0.00146509506902116, 0.0021686385802621414, 0.002818953465172198, 0.0022264488371695544, 0.0023242341879711943, 0.001480532883189965, 0.002033884837400428, 0.004419564651217052, 0.002425999372526137, 0.0018172024418924783, 0.004632834396541639, 0.0022833211168776764, 0.0019377773725102808, 0.0016373676751458713, 0.0011704262551786595, 0.0016259776975223145, 0.0025534238612149344, 0.0012168034878667704, 0.002810888953485288, 0.0036955114405298997, 0.0013438398857752597, 0.0018595413732623986, 0.0023401846275339987, 0.00674435076693636, 0.001869911744887399, 0.002188654651670435, 0.0016258918831861297, 0.002024541300353269, 0.0023270816065717576, 0.00278256714159926, 0.0020373829526740103, 0.002225954651421066, 0.0011166671621288325, 0.00186556646448755, 0.001988120815651708, 0.001172483416793998, 0.002237280465250965, 0.0026038377193820686, 0.0017781572790036713, 0.0019797941393673766, 0.0011947604876314832, 0.0021663031860276365, 0.0012180129320598966, 0.00151501732423555, 0.0015385704667329094, 0.002521255207897792, 0.0025670776755527354, 0.002215332186477649, 0.002488610023351083, 0.0017641187905398912, 0.0024805003939586323, 0.002562740582128077, 0.001886460605229056, 0.0011911817669305344, 0.0012438688589649837, 0.0026048957911688226, 0.002148885907995146, 0.0017491744203103144, 0.0024283588376589295, 0.003508023373501072, 0.0013198695351304703, 0.0014515744662995254, 0.002411190396023178, 0.001456457000702273, 0.005000243651095864, 0.002359712233788572, 0.0026402331612527716, 0.0023619554893568504, 0.0011542037232280817, 0.00122641283628899, 0.0012853276525992294, 0.0015050841165100072, 0.0016276106741929123, 0.0011875107911480374, 0.0024679437452970548, 0.0015150076509345062, 0.001755305021719704, 0.0012913514199385115, 0.0011615920703598234, 0.0013501248836820556, 0.008026453187191036, 0.0012848560460085093, 0.001239550814335776, 0.002645626000865081, 0.0011783212336689928, 0.001155083230155152, 0.0017346570912618623]
[840.6323632997506, 421.6262614047239, 475.16750869447753, 537.8044182417909, 405.10051373440695, 866.7582559083058, 875.4929982553638, 474.9847549330259, 492.7584606617454, 878.7924330177824, 895.0220871588551, 464.9530048923895, 176.43104721109836, 571.5882891399109, 533.9800185122206, 134.83127195739132, 857.643933081008, 827.4702521467698, 821.6836024064347, 621.7901401202313, 569.1264031234983, 829.8172652063832, 730.1448872018593, 787.4199641293683, 849.3169308212767, 823.396203645775, 829.925124242138, 733.2462728637041, 812.445947375324, 608.9837612895539, 715.1051375516889, 849.6877313066584, 762.8428805103531, 853.6342776931745, 824.0927130883722, 823.9482424952237, 724.4511596010086, 662.7392536501575, 646.9572010495544, 707.9768176528445, 861.1246465747305, 841.66587440582, 716.9855447571167, 764.2657625530625, 351.6891805149902, 391.8326474167709, 549.8183780025354, 899.4459399457944, 341.1174512418705, 404.86172279303656, 410.48171724436446, 682.549563604843, 461.1187908863642, 354.741577807108, 449.1457352648097, 430.24924303040746, 675.432482016471, 491.66992231385694, 226.26663006833078, 412.2012607772125, 550.296420996758, 215.8505818266436, 437.9585475771574, 516.0551537995083, 610.7363759400655, 854.3895829193914, 615.0145857005374, 391.6310234228774, 821.8253892032659, 355.75934038947935, 270.59853990239844, 744.1362699419367, 537.7670077034047, 427.3167117817382, 148.27224065842185, 534.784597580148, 456.9016858081175, 615.0470460805674, 493.9390467487655, 429.7227897706578, 359.38036680231096, 490.82574225308343, 449.24545042352617, 895.5219907188672, 536.0302187221664, 502.9875408613933, 852.8905276412087, 446.9712293705769, 384.0485113785492, 562.3799490674498, 505.1030206198812, 836.9878401171601, 461.61590235838884, 821.0093453677915, 660.0584587404514, 649.9539810636418, 396.6278371453694, 389.5480099894846, 451.39957163263534, 401.8307370848856, 566.855251110363, 403.1444632847243, 390.2072675532413, 530.0932323887987, 839.5024401496876, 803.9432716661902, 383.89251631110267, 465.3574190604533, 571.6982757057434, 411.80075386389547, 285.0608144614446, 757.6506415091618, 688.9071303033342, 414.73290605724003, 686.5976815778433, 199.99025443106913, 423.7804871632492, 378.7544277057361, 423.37800373718954, 866.3981755345546, 815.3861166570191, 778.0117373012002, 664.4146921959434, 614.397543500919, 842.0976107789643, 405.1956216204736, 660.0626732037738, 569.7015547874876, 774.3825457268754, 860.8874195312237, 740.6722237966637, 124.58803118615874, 778.2973066177854, 806.7438530431353, 377.98237531420335, 848.665008680404, 865.7384800450085, 576.4828132530552]
Elapsed: 0.0865486209479876~0.048307324490007714
Time per graph: 0.002002637541820514~0.0011166921882044874
Speed: 592.1326525392831~200.92701894075563
Total Time: 0.0751
best val loss: 0.27116289734840393 test_score: 0.8605

Testing...
Test loss: 0.3844 score: 0.8372 time: 0.04s
test Score 0.8372
Epoch Time List: [0.32533643394708633, 0.3555040260544047, 0.32359286409337074, 0.42953334480989724, 0.3371338419383392, 0.5603312109597027, 0.3241024250164628, 0.29141403001267463, 0.3165937179001048, 0.36867188999895006, 0.30368647596333176, 0.29590074997395277, 0.5349695300683379, 0.3668816949939355, 0.3579681810224429, 0.5639187239576131, 0.22966309392359108, 0.228890509926714, 0.23579101415816694, 0.2672468830132857, 0.2734047321137041, 0.2372759459540248, 0.2542670149123296, 0.3101370359072462, 0.23653347592335194, 0.24219172890298069, 0.24256062787026167, 0.26929775124881417, 0.2537868960062042, 0.258512009982951, 0.239874763879925, 0.3056842730147764, 0.23406950500793755, 0.252486311015673, 0.2482498389435932, 0.24729201092850417, 0.2751407859614119, 0.2675415399717167, 0.38571271882392466, 0.2445296449586749, 0.24315407406538725, 0.26526179898064584, 0.2591691709822044, 0.31511766207404435, 0.3769094040617347, 0.43839398911222816, 0.3114471168955788, 0.34437842189799994, 0.5834483159705997, 0.40314651594962925, 0.4702907180180773, 0.3448340588947758, 0.30229336104821414, 0.4350032490910962, 0.432229605037719, 0.33551854104734957, 0.40343815402593464, 0.3014930260833353, 0.4267043840372935, 0.3856312888674438, 0.3161197900772095, 0.4810492020333186, 0.3198087550699711, 0.37633527792058885, 0.3638063259422779, 0.2991499430499971, 0.4186811849940568, 0.3607289999490604, 0.48439077008515596, 0.3481228419113904, 0.4068171789404005, 0.3904613188933581, 0.3556837741052732, 0.32150475098751485, 0.6359337270259857, 0.2901570020476356, 0.3528780990745872, 0.3569125881185755, 0.32398722402285784, 0.4332378430990502, 0.323668519849889, 0.2886673748726025, 0.33352580305654556, 0.2874091040575877, 0.43095716496463865, 0.37303667794913054, 0.3678984858561307, 0.5025004899362102, 0.6097620581276715, 0.27217458188533783, 0.30192183796316385, 0.27479333186056465, 0.34067132498603314, 0.34707342891488224, 0.4073260030709207, 0.3389351909281686, 0.41774243605323136, 0.32659594994038343, 0.3927706030663103, 0.3878061849391088, 0.30817993194796145, 0.3072574690449983, 0.5806553070433438, 0.34864405100233853, 0.36703596299048513, 0.3553258287720382, 0.3512214318616316, 0.3403964501339942, 0.43231775402091444, 0.3020196838770062, 0.39650763804093003, 0.36794962687417865, 0.3036185539094731, 0.3306831510271877, 0.46822535688988864, 0.5330258710309863, 0.3522365310927853, 0.40462414792273194, 0.376224507112056, 0.3248232500627637, 0.9591676300624385, 0.25217313587199897, 0.26594333292450756, 0.2980904100695625, 0.3286935108480975, 0.42900168104097247, 0.32516795094124973, 0.34681233996525407, 0.5569441351108253, 0.23226266191340983, 0.23415133694652468, 0.7514637932181358, 0.4855735308956355, 0.25370822416152805, 0.29767253797035664, 0.24606366804800928, 0.3225161678856239, 0.2756522410782054]
Total Epoch List: [33, 105]
Total Time List: [0.058135161991231143, 0.07514752901624888]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x75d296e9b3a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8791;  Loss pred: 0.8791; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9418 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8851 score: 0.4884 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.8615;  Loss pred: 0.8615; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8155 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7637 score: 0.4884 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8290;  Loss pred: 0.8290; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7435 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7100 score: 0.4884 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.9046;  Loss pred: 0.9046; Loss self: 0.0000; time: 0.16s
Val loss: 0.7198 score: 0.3864 time: 0.05s
Test loss: 0.6884 score: 0.4651 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.8292;  Loss pred: 0.8292; Loss self: 0.0000; time: 0.14s
Val loss: 0.7154 score: 0.3636 time: 0.05s
Test loss: 0.6783 score: 0.5116 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.7943;  Loss pred: 0.7943; Loss self: 0.0000; time: 0.16s
Val loss: 0.7086 score: 0.3409 time: 0.05s
Test loss: 0.6745 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.7862;  Loss pred: 0.7862; Loss self: 0.0000; time: 0.14s
Val loss: 0.7026 score: 0.3636 time: 0.04s
Test loss: 0.6741 score: 0.4651 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.6712;  Loss pred: 0.6712; Loss self: 0.0000; time: 0.18s
Val loss: 0.6984 score: 0.3864 time: 0.05s
Test loss: 0.6749 score: 0.4651 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.17s
Val loss: 0.6951 score: 0.4318 time: 0.10s
Test loss: 0.6776 score: 0.4884 time: 0.10s
Epoch 10/1000, LR 0.000240
Train loss: 0.6201;  Loss pred: 0.6201; Loss self: 0.0000; time: 0.20s
Val loss: 0.6927 score: 0.4318 time: 0.05s
Test loss: 0.6786 score: 0.4884 time: 0.13s
Epoch 11/1000, LR 0.000270
Train loss: 0.5575;  Loss pred: 0.5575; Loss self: 0.0000; time: 0.21s
Val loss: 0.6900 score: 0.4318 time: 0.05s
Test loss: 0.6796 score: 0.5116 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.21s
Val loss: 0.6863 score: 0.4318 time: 0.09s
Test loss: 0.6780 score: 0.5349 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.5578;  Loss pred: 0.5578; Loss self: 0.0000; time: 0.16s
Val loss: 0.6838 score: 0.5000 time: 0.08s
Test loss: 0.6747 score: 0.4651 time: 0.07s
Epoch 14/1000, LR 0.000270
Train loss: 0.5187;  Loss pred: 0.5187; Loss self: 0.0000; time: 0.24s
Val loss: 0.6821 score: 0.5000 time: 0.04s
Test loss: 0.6720 score: 0.5814 time: 0.29s
Epoch 15/1000, LR 0.000270
Train loss: 0.4612;  Loss pred: 0.4612; Loss self: 0.0000; time: 0.22s
Val loss: 0.6805 score: 0.5682 time: 0.06s
Test loss: 0.6712 score: 0.5814 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.4336;  Loss pred: 0.4336; Loss self: 0.0000; time: 0.18s
Val loss: 0.6783 score: 0.6136 time: 0.13s
Test loss: 0.6709 score: 0.5349 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.4646;  Loss pred: 0.4646; Loss self: 0.0000; time: 0.19s
Val loss: 0.6747 score: 0.5909 time: 0.06s
Test loss: 0.6702 score: 0.5349 time: 0.10s
Epoch 18/1000, LR 0.000270
Train loss: 0.4191;  Loss pred: 0.4191; Loss self: 0.0000; time: 0.19s
Val loss: 0.6717 score: 0.5682 time: 0.07s
Test loss: 0.6680 score: 0.5349 time: 0.12s
Epoch 19/1000, LR 0.000270
Train loss: 0.3615;  Loss pred: 0.3615; Loss self: 0.0000; time: 0.14s
Val loss: 0.6699 score: 0.5682 time: 0.13s
Test loss: 0.6678 score: 0.4884 time: 0.11s
Epoch 20/1000, LR 0.000270
Train loss: 0.4025;  Loss pred: 0.4025; Loss self: 0.0000; time: 0.17s
Val loss: 0.6703 score: 0.5227 time: 0.06s
Test loss: 0.6687 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3283;  Loss pred: 0.3283; Loss self: 0.0000; time: 0.15s
Val loss: 0.6711 score: 0.5227 time: 0.08s
Test loss: 0.6711 score: 0.4884 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3097;  Loss pred: 0.3097; Loss self: 0.0000; time: 0.19s
Val loss: 0.6706 score: 0.5227 time: 0.14s
Test loss: 0.6746 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3103;  Loss pred: 0.3103; Loss self: 0.0000; time: 0.19s
Val loss: 0.6709 score: 0.5227 time: 0.05s
Test loss: 0.6795 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2842;  Loss pred: 0.2842; Loss self: 0.0000; time: 0.18s
Val loss: 0.6724 score: 0.5000 time: 0.05s
Test loss: 0.6854 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2817;  Loss pred: 0.2817; Loss self: 0.0000; time: 0.22s
Val loss: 0.6745 score: 0.5000 time: 0.13s
Test loss: 0.6906 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2539;  Loss pred: 0.2539; Loss self: 0.0000; time: 0.19s
Val loss: 0.6761 score: 0.5000 time: 0.06s
Test loss: 0.6944 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2444;  Loss pred: 0.2444; Loss self: 0.0000; time: 0.20s
Val loss: 0.6765 score: 0.5227 time: 0.11s
Test loss: 0.6975 score: 0.5349 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2257;  Loss pred: 0.2257; Loss self: 0.0000; time: 0.18s
Val loss: 0.6760 score: 0.5227 time: 0.20s
Test loss: 0.7008 score: 0.5581 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2432;  Loss pred: 0.2432; Loss self: 0.0000; time: 0.19s
Val loss: 0.6741 score: 0.5227 time: 0.08s
Test loss: 0.7042 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2196;  Loss pred: 0.2196; Loss self: 0.0000; time: 0.15s
Val loss: 0.6719 score: 0.5227 time: 0.05s
Test loss: 0.7081 score: 0.5581 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2021;  Loss pred: 0.2021; Loss self: 0.0000; time: 0.15s
Val loss: 0.6718 score: 0.5227 time: 0.14s
Test loss: 0.7132 score: 0.5581 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2132;  Loss pred: 0.2132; Loss self: 0.0000; time: 0.14s
Val loss: 0.6696 score: 0.5455 time: 0.06s
Test loss: 0.7175 score: 0.5581 time: 0.09s
Epoch 33/1000, LR 0.000270
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.19s
Val loss: 0.6689 score: 0.5455 time: 0.12s
Test loss: 0.7219 score: 0.5581 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.1544;  Loss pred: 0.1544; Loss self: 0.0000; time: 0.17s
Val loss: 0.6670 score: 0.5455 time: 0.12s
Test loss: 0.7244 score: 0.5581 time: 0.11s
Epoch 35/1000, LR 0.000270
Train loss: 0.1554;  Loss pred: 0.1554; Loss self: 0.0000; time: 0.18s
Val loss: 0.6652 score: 0.5682 time: 0.07s
Test loss: 0.7270 score: 0.5581 time: 0.09s
Epoch 36/1000, LR 0.000270
Train loss: 0.1649;  Loss pred: 0.1649; Loss self: 0.0000; time: 0.25s
Val loss: 0.6655 score: 0.5682 time: 0.06s
Test loss: 0.7291 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1207;  Loss pred: 0.1207; Loss self: 0.0000; time: 0.27s
Val loss: 0.6665 score: 0.5682 time: 0.05s
Test loss: 0.7303 score: 0.5814 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1539;  Loss pred: 0.1539; Loss self: 0.0000; time: 0.23s
Val loss: 0.6676 score: 0.5682 time: 0.10s
Test loss: 0.7300 score: 0.6047 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1381;  Loss pred: 0.1381; Loss self: 0.0000; time: 0.17s
Val loss: 0.6667 score: 0.5682 time: 0.12s
Test loss: 0.7287 score: 0.6047 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1360;  Loss pred: 0.1360; Loss self: 0.0000; time: 0.15s
Val loss: 0.6642 score: 0.5909 time: 0.10s
Test loss: 0.7278 score: 0.6047 time: 0.12s
Epoch 41/1000, LR 0.000269
Train loss: 0.1138;  Loss pred: 0.1138; Loss self: 0.0000; time: 0.17s
Val loss: 0.6603 score: 0.5909 time: 0.25s
Test loss: 0.7266 score: 0.6047 time: 0.16s
Epoch 42/1000, LR 0.000269
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.15s
Val loss: 0.6552 score: 0.6364 time: 0.14s
Test loss: 0.7248 score: 0.6047 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.1296;  Loss pred: 0.1296; Loss self: 0.0000; time: 0.21s
Val loss: 0.6481 score: 0.6364 time: 0.09s
Test loss: 0.7217 score: 0.6047 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.1015;  Loss pred: 0.1015; Loss self: 0.0000; time: 0.19s
Val loss: 0.6420 score: 0.6818 time: 0.05s
Test loss: 0.7196 score: 0.6047 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 0.18s
Val loss: 0.6379 score: 0.6818 time: 0.04s
Test loss: 0.7199 score: 0.6047 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0908;  Loss pred: 0.0908; Loss self: 0.0000; time: 0.34s
Val loss: 0.6351 score: 0.6818 time: 0.04s
Test loss: 0.7207 score: 0.6047 time: 0.11s
Epoch 47/1000, LR 0.000269
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.20s
Val loss: 0.6352 score: 0.6818 time: 0.21s
Test loss: 0.7247 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0723;  Loss pred: 0.0723; Loss self: 0.0000; time: 0.27s
Val loss: 0.6387 score: 0.6818 time: 0.04s
Test loss: 0.7315 score: 0.6047 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0623;  Loss pred: 0.0623; Loss self: 0.0000; time: 0.35s
Val loss: 0.6448 score: 0.6364 time: 0.24s
Test loss: 0.7412 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.15s
Val loss: 0.6522 score: 0.6364 time: 0.13s
Test loss: 0.7525 score: 0.6047 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0589;  Loss pred: 0.0589; Loss self: 0.0000; time: 0.19s
Val loss: 0.6609 score: 0.6364 time: 0.04s
Test loss: 0.7651 score: 0.6047 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.18s
Val loss: 0.6670 score: 0.6364 time: 0.06s
Test loss: 0.7748 score: 0.5814 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.18s
Val loss: 0.6677 score: 0.6364 time: 0.15s
Test loss: 0.7795 score: 0.5814 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0480;  Loss pred: 0.0480; Loss self: 0.0000; time: 0.32s
Val loss: 0.6637 score: 0.6364 time: 0.07s
Test loss: 0.7801 score: 0.5814 time: 0.13s
     INFO: Early stopping counter 8 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.18s
Val loss: 0.6581 score: 0.6364 time: 0.06s
Test loss: 0.7803 score: 0.5814 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.19s
Val loss: 0.6497 score: 0.6818 time: 0.12s
Test loss: 0.7774 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0474;  Loss pred: 0.0474; Loss self: 0.0000; time: 0.19s
Val loss: 0.6518 score: 0.6818 time: 0.05s
Test loss: 0.7860 score: 0.6047 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0387;  Loss pred: 0.0387; Loss self: 0.0000; time: 0.16s
Val loss: 0.6570 score: 0.6818 time: 0.05s
Test loss: 0.7992 score: 0.5814 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0402;  Loss pred: 0.0402; Loss self: 0.0000; time: 0.16s
Val loss: 0.6611 score: 0.6591 time: 0.06s
Test loss: 0.8117 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.29s
Val loss: 0.6638 score: 0.6364 time: 0.06s
Test loss: 0.8230 score: 0.5814 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.15s
Val loss: 0.6561 score: 0.6591 time: 0.06s
Test loss: 0.8229 score: 0.5814 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.20s
Val loss: 0.6399 score: 0.6818 time: 0.11s
Test loss: 0.8136 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.27s
Val loss: 0.6197 score: 0.7045 time: 0.04s
Test loss: 0.7974 score: 0.5814 time: 0.10s
Epoch 64/1000, LR 0.000268
Train loss: 0.0285;  Loss pred: 0.0285; Loss self: 0.0000; time: 0.17s
Val loss: 0.5912 score: 0.7045 time: 0.06s
Test loss: 0.7724 score: 0.6279 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0269;  Loss pred: 0.0269; Loss self: 0.0000; time: 0.25s
Val loss: 0.5586 score: 0.7273 time: 0.05s
Test loss: 0.7413 score: 0.6744 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0245;  Loss pred: 0.0245; Loss self: 0.0000; time: 0.27s
Val loss: 0.5336 score: 0.7727 time: 0.04s
Test loss: 0.7176 score: 0.6744 time: 0.08s
Epoch 67/1000, LR 0.000268
Train loss: 0.0244;  Loss pred: 0.0244; Loss self: 0.0000; time: 0.16s
Val loss: 0.5162 score: 0.7955 time: 0.05s
Test loss: 0.7019 score: 0.7209 time: 0.07s
Epoch 68/1000, LR 0.000268
Train loss: 0.0235;  Loss pred: 0.0235; Loss self: 0.0000; time: 0.60s
Val loss: 0.5087 score: 0.7955 time: 0.05s
Test loss: 0.7026 score: 0.7209 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0178;  Loss pred: 0.0178; Loss self: 0.0000; time: 0.29s
Val loss: 0.5047 score: 0.7955 time: 0.07s
Test loss: 0.7093 score: 0.7209 time: 0.11s
Epoch 70/1000, LR 0.000268
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.20s
Val loss: 0.5082 score: 0.7955 time: 0.09s
Test loss: 0.7259 score: 0.6977 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0193;  Loss pred: 0.0193; Loss self: 0.0000; time: 0.26s
Val loss: 0.5214 score: 0.7727 time: 0.07s
Test loss: 0.7553 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.22s
Val loss: 0.5282 score: 0.7727 time: 0.06s
Test loss: 0.7756 score: 0.6744 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.18s
Val loss: 0.5280 score: 0.7727 time: 0.05s
Test loss: 0.7841 score: 0.6744 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.27s
Val loss: 0.5105 score: 0.7727 time: 0.05s
Test loss: 0.7699 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.29s
Val loss: 0.4733 score: 0.7727 time: 0.05s
Test loss: 0.7298 score: 0.6977 time: 0.08s
Epoch 76/1000, LR 0.000267
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.20s
Val loss: 0.4268 score: 0.8182 time: 0.05s
Test loss: 0.6776 score: 0.7209 time: 0.09s
Epoch 77/1000, LR 0.000267
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.19s
Val loss: 0.3875 score: 0.8409 time: 0.06s
Test loss: 0.6311 score: 0.7442 time: 0.05s
Epoch 78/1000, LR 0.000267
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.30s
Val loss: 0.3555 score: 0.8636 time: 0.06s
Test loss: 0.5901 score: 0.7442 time: 0.06s
Epoch 79/1000, LR 0.000267
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.20s
Val loss: 0.3334 score: 0.8864 time: 0.04s
Test loss: 0.5610 score: 0.7209 time: 0.11s
Epoch 80/1000, LR 0.000267
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.18s
Val loss: 0.3134 score: 0.9318 time: 0.06s
Test loss: 0.5309 score: 0.8140 time: 0.05s
Epoch 81/1000, LR 0.000267
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.48s
Val loss: 0.3076 score: 0.9091 time: 0.06s
Test loss: 0.5254 score: 0.8140 time: 0.10s
Epoch 82/1000, LR 0.000267
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.28s
Val loss: 0.3057 score: 0.9091 time: 0.08s
Test loss: 0.5278 score: 0.7907 time: 0.17s
Epoch 83/1000, LR 0.000266
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.16s
Val loss: 0.3150 score: 0.9091 time: 0.12s
Test loss: 0.5487 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.18s
Val loss: 0.3268 score: 0.9318 time: 0.05s
Test loss: 0.5705 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0074;  Loss pred: 0.0074; Loss self: 0.0000; time: 0.19s
Val loss: 0.3411 score: 0.9318 time: 0.12s
Test loss: 0.5964 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.15s
Val loss: 0.3486 score: 0.9318 time: 0.13s
Test loss: 0.6119 score: 0.7674 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.18s
Val loss: 0.3364 score: 0.9091 time: 0.05s
Test loss: 0.5898 score: 0.7907 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.20s
Val loss: 0.3175 score: 0.8864 time: 0.09s
Test loss: 0.5497 score: 0.7907 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.19s
Val loss: 0.3011 score: 0.8636 time: 0.10s
Test loss: 0.5101 score: 0.8140 time: 0.11s
Epoch 90/1000, LR 0.000266
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.18s
Val loss: 0.2953 score: 0.8182 time: 0.04s
Test loss: 0.4796 score: 0.8140 time: 0.08s
Epoch 91/1000, LR 0.000266
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.24s
Val loss: 0.2963 score: 0.8182 time: 0.08s
Test loss: 0.4577 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.29s
Val loss: 0.3016 score: 0.8182 time: 0.05s
Test loss: 0.4401 score: 0.8605 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.27s
Val loss: 0.3060 score: 0.7955 time: 0.11s
Test loss: 0.4053 score: 0.8837 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.16s
Val loss: 0.3184 score: 0.8409 time: 0.15s
Test loss: 0.3848 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.19s
Val loss: 0.3342 score: 0.8409 time: 0.09s
Test loss: 0.3757 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.24s
Val loss: 0.3526 score: 0.8182 time: 0.08s
Test loss: 0.3701 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.33s
Val loss: 0.3727 score: 0.8182 time: 0.05s
Test loss: 0.3600 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.26s
Val loss: 0.3887 score: 0.8182 time: 0.06s
Test loss: 0.3541 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.19s
Val loss: 0.4014 score: 0.8182 time: 0.12s
Test loss: 0.3532 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.17s
Val loss: 0.4092 score: 0.8182 time: 0.06s
Test loss: 0.3573 score: 0.8605 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.34s
Val loss: 0.4165 score: 0.8182 time: 0.15s
Test loss: 0.3647 score: 0.8605 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.26s
Val loss: 0.4221 score: 0.8182 time: 0.04s
Test loss: 0.3761 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.22s
Val loss: 0.4320 score: 0.8182 time: 0.04s
Test loss: 0.3868 score: 0.8837 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.4423 score: 0.7955 time: 0.04s
Test loss: 0.4022 score: 0.8837 time: 0.11s
     INFO: Early stopping counter 14 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.18s
Val loss: 0.4517 score: 0.7955 time: 0.06s
Test loss: 0.4210 score: 0.8837 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.25s
Val loss: 0.4619 score: 0.7955 time: 0.07s
Test loss: 0.4302 score: 0.8837 time: 0.30s
     INFO: Early stopping counter 16 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.16s
Val loss: 0.4717 score: 0.7955 time: 0.05s
Test loss: 0.4361 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.4811 score: 0.7955 time: 0.05s
Test loss: 0.4415 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.4874 score: 0.7955 time: 0.05s
Test loss: 0.4289 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.15s
Val loss: 0.4972 score: 0.8182 time: 0.04s
Test loss: 0.4046 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.0081,   Val_Loss: 0.2953,   Val_Precision: 0.8500,   Val_Recall: 0.7727,   Val_accuracy: 0.8095,   Val_Score: 0.8182,   Val_Loss: 0.2953,   Test_Precision: 0.7407,   Test_Recall: 0.9524,   Test_accuracy: 0.8333,   Test_Score: 0.8140,   Test_loss: 0.4796


[0.05234154895879328, 0.10435782594140619, 0.09259892394766212, 0.08181412890553474, 0.10861501900944859, 0.05076386604923755, 0.05025739793200046, 0.092634552042, 0.0892932410351932, 0.05006870604120195, 0.0491607979638502, 0.09463321999646723, 0.24938921292778105, 0.0769784840522334, 0.08240008703432977, 0.3263337900862098, 0.05130334198474884, 0.05317411699797958, 0.05354859202634543, 0.07076342508662492, 0.07731147203594446, 0.053023722022771835, 0.060262012062594295, 0.05587869498413056, 0.05180633801501244, 0.053437215043231845, 0.05301683093421161, 0.06000712397508323, 0.05415744904894382, 0.07225151604507118, 0.06152941391337663, 0.05178372992668301, 0.05767898098565638, 0.050372860045172274, 0.05217859509866685, 0.052187744062393904, 0.05935527803376317, 0.06488222896587104, 0.06646498397458345, 0.06073645199649036, 0.0499346989672631, 0.05108915700111538, 0.05997331510297954, 0.056263150996528566, 0.12226705392822623, 0.10974072804674506, 0.0782076440518722, 0.04780720896087587, 0.12605628895107657, 0.10620910197030753, 0.10475497005973011, 0.06299908796790987, 0.09325145895127207, 0.12121499900240451, 0.09573729999829084, 0.09994207008276135, 0.0636629139771685, 0.08745704800821841, 0.19004128000233322, 0.10431797301862389, 0.07813970500137657, 0.19921187905129045, 0.09818280802574009, 0.08332442701794207, 0.07040681003127247, 0.05032832897268236, 0.06991704099345952, 0.10979722603224218, 0.05232254997827113, 0.12086822499986738, 0.15890699194278568, 0.05778511508833617, 0.07996027905028313, 0.10062793898396194, 0.2900070829782635, 0.08040620503015816, 0.09411215002182871, 0.06991335097700357, 0.08705527591519058, 0.10006450908258557, 0.11965038708876818, 0.08760746696498245, 0.09571605001110584, 0.048016687971539795, 0.08021935797296464, 0.08548919507302344, 0.05041678692214191, 0.09620306000579149, 0.11196502193342894, 0.07646076299715787, 0.0851311479927972, 0.051374700968153775, 0.09315103699918836, 0.05237455607857555, 0.06514574494212866, 0.06615853006951511, 0.10841397393960506, 0.11038434004876763, 0.09525928401853889, 0.10701023100409657, 0.07585710799321532, 0.10666151694022119, 0.11019784503150731, 0.08111780602484941, 0.05122081597801298, 0.053486360935494304, 0.11201051902025938, 0.0924020940437913, 0.07521450007334352, 0.10441943001933396, 0.1508450050605461, 0.05675439001061022, 0.0624177020508796, 0.10368118702899665, 0.06262765103019774, 0.21501047699712217, 0.1014676260529086, 0.11353002593386918, 0.10156408604234457, 0.049630760098807514, 0.05273575196042657, 0.05526908906176686, 0.06471861700993031, 0.06998725899029523, 0.05106296401936561, 0.10612158104777336, 0.06514532899018377, 0.07547811593394727, 0.055528111057356, 0.0499484590254724, 0.05805536999832839, 0.34513748704921454, 0.0552488099783659, 0.053300685016438365, 0.11376191803719848, 0.050667813047766685, 0.049668578896671534, 0.07459025492426008, 0.09904669097159058, 0.0741971010575071, 0.05512486002407968, 0.05564078700263053, 0.050745132961310446, 0.05073036206886172, 0.06117196602281183, 0.052883232943713665, 0.1004608889343217, 0.13013997999951243, 0.0892504210351035, 0.093760103918612, 0.07760670001152903, 0.2990720400121063, 0.09384546591900289, 0.05489890999160707, 0.10188628791365772, 0.12704675598070025, 0.11193324695341289, 0.05508295400068164, 0.08593618299346417, 0.06972704397048801, 0.10344486602116376, 0.0951734280679375, 0.12000365101266652, 0.08141289104241878, 0.08771229104604572, 0.1255186020862311, 0.06419679801911116, 0.15987381397280842, 0.11083898099604994, 0.09759114403277636, 0.06747385498601943, 0.1139675349695608, 0.09100547898560762, 0.05371922894846648, 0.09214801993221045, 0.11842512607108802, 0.06764998298604041, 0.12269078497774899, 0.16296051105018705, 0.05254604900255799, 0.0525923470268026, 0.09678761102259159, 0.052224352955818176, 0.11142830189783126, 0.051735033048316836, 0.09994423307944089, 0.04868467990309, 0.10987045895308256, 0.08685388299636543, 0.09228482097387314, 0.07009233790449798, 0.13231639005243778, 0.11281558603513986, 0.059511535917408764, 0.09514358593150973, 0.06343883706722409, 0.050981546053662896, 0.073562100995332, 0.08155979798175395, 0.051585988025180995, 0.1079207279253751, 0.08447796502150595, 0.05260355398058891, 0.08115037903189659, 0.07712703000288457, 0.05327175196725875, 0.1189791519427672, 0.06873664900194854, 0.051980621996335685, 0.11263648292515427, 0.08332437509670854, 0.053090322064235806, 0.08572735695634037, 0.09651138505432755, 0.05344082601368427, 0.07007653592154384, 0.11189581896178424, 0.053674628026783466, 0.10166270192712545, 0.17141697194892913, 0.06827138899825513, 0.09016313299071044, 0.10308278596494347, 0.11583071399945766, 0.0750512860249728, 0.08594489202369004, 0.1140925349900499, 0.08681746397633106, 0.051546204020269215, 0.08098077995236963, 0.17339972802437842, 0.1368196519324556, 0.07863339502364397, 0.05444677406921983, 0.07708051195368171, 0.10377356200478971, 0.11034218198619783, 0.09079510194715112, 0.11427992791868746, 0.11485063598956913, 0.18824834609404206, 0.11465161095838994, 0.07894808507990092, 0.3016113620251417, 0.051183375995606184, 0.06047274696175009, 0.05192700901534408, 0.053623204003088176]
[0.0011895806581543927, 0.002371768771395595, 0.0021045209988105025, 0.0018594120205803351, 0.0024685231593056496, 0.0011537242283917624, 0.0011422135893636469, 0.0021053307282272726, 0.0020293918417089367, 0.0011379251373000443, 0.0011172908628147773, 0.00215075499991971, 0.005667936657449569, 0.0017495110011871227, 0.0018727292507802222, 0.007416677047413858, 0.0011659850451079283, 0.0012085026590449904, 0.0012170134551442143, 0.0016082596610596572, 0.0017570789099078286, 0.0012050845914266326, 0.0013695911832407794, 0.0012699703405484218, 0.0011774167730684646, 0.001214482160073451, 0.0012049279757775366, 0.0013637982721609826, 0.0012308511147487231, 0.0016420799101152543, 0.0013983957707585598, 0.0011769029528791593, 0.0013108859314921904, 0.0011714618615156344, 0.0012134556999689964, 0.0012136684665673, 0.0013803553031107715, 0.001508889045717931, 0.0015456973017344987, 0.0014124756278253573, 0.0011612720690061186, 0.0011881199302584973, 0.0013947282582088265, 0.0013084453720122922, 0.002843419858795959, 0.0025521099545754667, 0.0018187824198109812, 0.0011117955572296713, 0.0029315416035134087, 0.002469979115588547, 0.002436162094412328, 0.00146509506902116, 0.0021686385802621414, 0.002818953465172198, 0.0022264488371695544, 0.0023242341879711943, 0.001480532883189965, 0.002033884837400428, 0.004419564651217052, 0.002425999372526137, 0.0018172024418924783, 0.004632834396541639, 0.0022833211168776764, 0.0019377773725102808, 0.0016373676751458713, 0.0011704262551786595, 0.0016259776975223145, 0.0025534238612149344, 0.0012168034878667704, 0.002810888953485288, 0.0036955114405298997, 0.0013438398857752597, 0.0018595413732623986, 0.0023401846275339987, 0.00674435076693636, 0.001869911744887399, 0.002188654651670435, 0.0016258918831861297, 0.002024541300353269, 0.0023270816065717576, 0.00278256714159926, 0.0020373829526740103, 0.002225954651421066, 0.0011166671621288325, 0.00186556646448755, 0.001988120815651708, 0.001172483416793998, 0.002237280465250965, 0.0026038377193820686, 0.0017781572790036713, 0.0019797941393673766, 0.0011947604876314832, 0.0021663031860276365, 0.0012180129320598966, 0.00151501732423555, 0.0015385704667329094, 0.002521255207897792, 0.0025670776755527354, 0.002215332186477649, 0.002488610023351083, 0.0017641187905398912, 0.0024805003939586323, 0.002562740582128077, 0.001886460605229056, 0.0011911817669305344, 0.0012438688589649837, 0.0026048957911688226, 0.002148885907995146, 0.0017491744203103144, 0.0024283588376589295, 0.003508023373501072, 0.0013198695351304703, 0.0014515744662995254, 0.002411190396023178, 0.001456457000702273, 0.005000243651095864, 0.002359712233788572, 0.0026402331612527716, 0.0023619554893568504, 0.0011542037232280817, 0.00122641283628899, 0.0012853276525992294, 0.0015050841165100072, 0.0016276106741929123, 0.0011875107911480374, 0.0024679437452970548, 0.0015150076509345062, 0.001755305021719704, 0.0012913514199385115, 0.0011615920703598234, 0.0013501248836820556, 0.008026453187191036, 0.0012848560460085093, 0.001239550814335776, 0.002645626000865081, 0.0011783212336689928, 0.001155083230155152, 0.0017346570912618623, 0.0023034114179439668, 0.0017255139780815603, 0.0012819734889320856, 0.0012939717907588496, 0.001180119371193266, 0.0011797758620665517, 0.001422603860995624, 0.0012298426265979921, 0.0023362997426586442, 0.003026511162779359, 0.0020755911868628723, 0.0021804675329909766, 0.001804806977012303, 0.006955163721211775, 0.002182452695790765, 0.001276718837014118, 0.002369448556131575, 0.002954575720481401, 0.0026030987663584393, 0.0012809989302484102, 0.0019985158835689344, 0.0016215591621043723, 0.0024056945586317154, 0.0022133355364636624, 0.0027907825816899192, 0.001893323047498111, 0.0020398207220010635, 0.002919037257819328, 0.00149294879114212, 0.0037179956737862425, 0.0025776507208383707, 0.002269561489134334, 0.0015691594182795218, 0.0026504077899897863, 0.0021164064880373865, 0.0012492843941503831, 0.0021429772077258243, 0.002754072699327628, 0.0015732554182800096, 0.0028532740692499767, 0.0037897793267485363, 0.001222001139594372, 0.0012230778378326185, 0.0022508746749439904, 0.001214519836181818, 0.002591355858089099, 0.0012031403034492287, 0.0023242844902195557, 0.0011322018582113954, 0.0025551269523972687, 0.0020198577441015216, 0.002146158627299375, 0.001630054369872046, 0.0030771253500566924, 0.0026236182798869733, 0.0013839892073815993, 0.002212641533290924, 0.0014753217922610252, 0.0011856173500851836, 0.001710746534775163, 0.0018967394879477661, 0.0011996741401204883, 0.0025097843703575607, 0.0019646038377094405, 0.0012233384646648585, 0.0018872181170208509, 0.0017936518605321992, 0.0012388779527269477, 0.0027669570219248187, 0.0015985267209755473, 0.0012088516743333881, 0.0026194530912826575, 0.0019377761650397334, 0.0012346586526566466, 0.0019936594641009387, 0.00224445081521692, 0.0012428099072949832, 0.0016296868818963682, 0.0026022283479484706, 0.001248247163413569, 0.0023642488820261732, 0.003986441208114631, 0.0015877067208896543, 0.0020968170462955914, 0.002397274092207988, 0.002693737534871108, 0.0017453787447668093, 0.0019987184191555822, 0.0026533147672104626, 0.002019010790147234, 0.0011987489307039353, 0.001883273952380689, 0.004032551814520428, 0.0031818523705222234, 0.0018286836052010225, 0.0012662040481213914, 0.0017925700454344583, 0.0024133386512741792, 0.002566097255492973, 0.0021115139987709563, 0.0026576727422950573, 0.0026709450230132355, 0.004377868513814932, 0.0026663165339160452, 0.001836001978602347, 0.007014217721514924, 0.00119031106966526, 0.0014063429525988393, 0.0012076048608219555, 0.0012470512558857714]
[840.6323632997506, 421.6262614047239, 475.16750869447753, 537.8044182417909, 405.10051373440695, 866.7582559083058, 875.4929982553638, 474.9847549330259, 492.7584606617454, 878.7924330177824, 895.0220871588551, 464.9530048923895, 176.43104721109836, 571.5882891399109, 533.9800185122206, 134.83127195739132, 857.643933081008, 827.4702521467698, 821.6836024064347, 621.7901401202313, 569.1264031234983, 829.8172652063832, 730.1448872018593, 787.4199641293683, 849.3169308212767, 823.396203645775, 829.925124242138, 733.2462728637041, 812.445947375324, 608.9837612895539, 715.1051375516889, 849.6877313066584, 762.8428805103531, 853.6342776931745, 824.0927130883722, 823.9482424952237, 724.4511596010086, 662.7392536501575, 646.9572010495544, 707.9768176528445, 861.1246465747305, 841.66587440582, 716.9855447571167, 764.2657625530625, 351.6891805149902, 391.8326474167709, 549.8183780025354, 899.4459399457944, 341.1174512418705, 404.86172279303656, 410.48171724436446, 682.549563604843, 461.1187908863642, 354.741577807108, 449.1457352648097, 430.24924303040746, 675.432482016471, 491.66992231385694, 226.26663006833078, 412.2012607772125, 550.296420996758, 215.8505818266436, 437.9585475771574, 516.0551537995083, 610.7363759400655, 854.3895829193914, 615.0145857005374, 391.6310234228774, 821.8253892032659, 355.75934038947935, 270.59853990239844, 744.1362699419367, 537.7670077034047, 427.3167117817382, 148.27224065842185, 534.784597580148, 456.9016858081175, 615.0470460805674, 493.9390467487655, 429.7227897706578, 359.38036680231096, 490.82574225308343, 449.24545042352617, 895.5219907188672, 536.0302187221664, 502.9875408613933, 852.8905276412087, 446.9712293705769, 384.0485113785492, 562.3799490674498, 505.1030206198812, 836.9878401171601, 461.61590235838884, 821.0093453677915, 660.0584587404514, 649.9539810636418, 396.6278371453694, 389.5480099894846, 451.39957163263534, 401.8307370848856, 566.855251110363, 403.1444632847243, 390.2072675532413, 530.0932323887987, 839.5024401496876, 803.9432716661902, 383.89251631110267, 465.3574190604533, 571.6982757057434, 411.80075386389547, 285.0608144614446, 757.6506415091618, 688.9071303033342, 414.73290605724003, 686.5976815778433, 199.99025443106913, 423.7804871632492, 378.7544277057361, 423.37800373718954, 866.3981755345546, 815.3861166570191, 778.0117373012002, 664.4146921959434, 614.397543500919, 842.0976107789643, 405.1956216204736, 660.0626732037738, 569.7015547874876, 774.3825457268754, 860.8874195312237, 740.6722237966637, 124.58803118615874, 778.2973066177854, 806.7438530431353, 377.98237531420335, 848.665008680404, 865.7384800450085, 576.4828132530552, 434.13868326336745, 579.5374669243813, 780.0473322057727, 772.8143744258522, 847.3719052580755, 847.6186300746587, 702.9363742202553, 813.112164412624, 428.027269678174, 330.4134517322125, 481.79044424997693, 458.61723913324516, 554.0758722328361, 143.77806764637504, 458.2000800881833, 783.2578097921039, 422.0391269572979, 338.4580713460496, 384.15753290795533, 780.6407768085184, 500.3713046374231, 616.6904195479699, 415.6803682379234, 451.80677919161826, 358.32243133553743, 528.1718834624801, 490.2391613214912, 342.5787037562692, 669.8153385656252, 268.96212038397675, 387.950156286013, 440.6137506243219, 637.2838784579537, 377.300430438236, 472.4990240071192, 800.4582500849078, 466.64052067134327, 363.0986212688348, 635.6246979230292, 350.4745691194201, 263.867606470363, 818.3298424189176, 817.6094513919667, 444.2717362861988, 823.3706607409385, 385.8983693337331, 831.1582590435588, 430.2399315608471, 883.2347277540754, 391.3699861612672, 495.0843706296864, 465.94878275999235, 613.4764695477592, 324.97863630468487, 381.153008296268, 722.5489871354725, 451.94849005327666, 677.8182259935548, 843.4424478759125, 584.5401289276475, 527.2205309976331, 833.5596863824753, 398.44060382666794, 509.00847326345144, 817.435263325883, 529.8804578977828, 557.5217922742752, 807.1820132070773, 361.40785421537026, 625.5760300270244, 827.2313479248331, 381.7590791482102, 516.0554753647181, 809.9404623685051, 501.59017525641485, 445.5432897973097, 804.628281549938, 613.6148060763429, 384.28602962087245, 801.123390711588, 422.9673143137938, 250.85030677598917, 629.8392435094441, 476.91333002403894, 417.14045267095804, 371.23141622179185, 572.9415480727679, 500.3206006489296, 376.8870592957731, 495.292053356028, 834.2030381731186, 530.9901932939058, 247.98193451580613, 314.282337315315, 546.8414531392228, 789.762125214853, 557.8582563883208, 414.3637278058868, 389.69684327412216, 473.5938291586357, 376.26905076975015, 374.399319860147, 228.4216615561592, 375.04924388376736, 544.661722402526, 142.56757342057824, 840.116525406439, 711.0641100394884, 828.0854379133177, 801.8916586469472]
Elapsed: 0.08883966492430773~0.04541300762875427
Time per graph: 0.0020604068251658812~0.0010524321288495938
Speed: 569.2326599277854~196.6245388488158
Total Time: 0.0547
best val loss: 0.29528120160102844 test_score: 0.8140

Testing...
Test loss: 0.5309 score: 0.8140 time: 0.05s
test Score 0.8140
Epoch Time List: [0.32533643394708633, 0.3555040260544047, 0.32359286409337074, 0.42953334480989724, 0.3371338419383392, 0.5603312109597027, 0.3241024250164628, 0.29141403001267463, 0.3165937179001048, 0.36867188999895006, 0.30368647596333176, 0.29590074997395277, 0.5349695300683379, 0.3668816949939355, 0.3579681810224429, 0.5639187239576131, 0.22966309392359108, 0.228890509926714, 0.23579101415816694, 0.2672468830132857, 0.2734047321137041, 0.2372759459540248, 0.2542670149123296, 0.3101370359072462, 0.23653347592335194, 0.24219172890298069, 0.24256062787026167, 0.26929775124881417, 0.2537868960062042, 0.258512009982951, 0.239874763879925, 0.3056842730147764, 0.23406950500793755, 0.252486311015673, 0.2482498389435932, 0.24729201092850417, 0.2751407859614119, 0.2675415399717167, 0.38571271882392466, 0.2445296449586749, 0.24315407406538725, 0.26526179898064584, 0.2591691709822044, 0.31511766207404435, 0.3769094040617347, 0.43839398911222816, 0.3114471168955788, 0.34437842189799994, 0.5834483159705997, 0.40314651594962925, 0.4702907180180773, 0.3448340588947758, 0.30229336104821414, 0.4350032490910962, 0.432229605037719, 0.33551854104734957, 0.40343815402593464, 0.3014930260833353, 0.4267043840372935, 0.3856312888674438, 0.3161197900772095, 0.4810492020333186, 0.3198087550699711, 0.37633527792058885, 0.3638063259422779, 0.2991499430499971, 0.4186811849940568, 0.3607289999490604, 0.48439077008515596, 0.3481228419113904, 0.4068171789404005, 0.3904613188933581, 0.3556837741052732, 0.32150475098751485, 0.6359337270259857, 0.2901570020476356, 0.3528780990745872, 0.3569125881185755, 0.32398722402285784, 0.4332378430990502, 0.323668519849889, 0.2886673748726025, 0.33352580305654556, 0.2874091040575877, 0.43095716496463865, 0.37303667794913054, 0.3678984858561307, 0.5025004899362102, 0.6097620581276715, 0.27217458188533783, 0.30192183796316385, 0.27479333186056465, 0.34067132498603314, 0.34707342891488224, 0.4073260030709207, 0.3389351909281686, 0.41774243605323136, 0.32659594994038343, 0.3927706030663103, 0.3878061849391088, 0.30817993194796145, 0.3072574690449983, 0.5806553070433438, 0.34864405100233853, 0.36703596299048513, 0.3553258287720382, 0.3512214318616316, 0.3403964501339942, 0.43231775402091444, 0.3020196838770062, 0.39650763804093003, 0.36794962687417865, 0.3036185539094731, 0.3306831510271877, 0.46822535688988864, 0.5330258710309863, 0.3522365310927853, 0.40462414792273194, 0.376224507112056, 0.3248232500627637, 0.9591676300624385, 0.25217313587199897, 0.26594333292450756, 0.2980904100695625, 0.3286935108480975, 0.42900168104097247, 0.32516795094124973, 0.34681233996525407, 0.5569441351108253, 0.23226266191340983, 0.23415133694652468, 0.7514637932181358, 0.4855735308956355, 0.25370822416152805, 0.29767253797035664, 0.24606366804800928, 0.3225161678856239, 0.2756522410782054, 0.35650063503999263, 0.391267609084025, 0.23440526914782822, 0.2591081220889464, 0.23360233998391777, 0.24936665000859648, 0.23909872095100582, 0.27769301494117826, 0.368429752998054, 0.37074878392741084, 0.3499527950771153, 0.39342658896930516, 0.30896158306859434, 0.5773146328283474, 0.36629765888210386, 0.36742905306164175, 0.3434982799226418, 0.378958671935834, 0.3816591700306162, 0.28043860604520887, 0.31712018698453903, 0.3898941419320181, 0.33404698700178415, 0.3230181069811806, 0.47388049494475126, 0.32227261213120073, 0.393343135016039, 0.49813622992951423, 0.32771866396069527, 0.3621220882050693, 0.39012375788297504, 0.2906897100619972, 0.3743104441091418, 0.39570740796625614, 0.3340787620982155, 0.3587293119635433, 0.41106146783567965, 0.44356006605084985, 0.3555919338250533, 0.3740972279338166, 0.5714751810301095, 0.33764002087991685, 0.3461700538173318, 0.32947747700382024, 0.2679826971143484, 0.4879887959687039, 0.45976316393353045, 0.4117753909667954, 0.6277129530208185, 0.3858130528824404, 0.3158742052037269, 0.3319422119529918, 0.39039276202674955, 0.5196742959087715, 0.3440758138895035, 0.3686031480319798, 0.3282136939233169, 0.2696311780018732, 0.2608681219862774, 0.410760500933975, 0.28542664914857596, 0.3543239289429039, 0.42051884694956243, 0.3081675680587068, 0.35013919312041253, 0.3912699291249737, 0.2819844069890678, 0.6986626829020679, 0.4792568109696731, 0.3509883730439469, 0.3794957260834053, 0.3888175750616938, 0.30979676404967904, 0.36852950998581946, 0.418257457902655, 0.3398936170851812, 0.3016747512156144, 0.418871472007595, 0.3474623329238966, 0.2920815509278327, 0.6307086460292339, 0.521019265986979, 0.33843172201886773, 0.316757946042344, 0.39967998897191137, 0.38683304213918746, 0.2983986050821841, 0.3677326360484585, 0.39361198793631047, 0.30765419197268784, 0.3593401520047337, 0.4145962109323591, 0.5453242141520604, 0.4472351430449635, 0.3495519160060212, 0.37208482495043427, 0.4513602590886876, 0.4224275229498744, 0.4110787259414792, 0.3110016400460154, 0.5909631609683856, 0.4090296470094472, 0.4449158310890198, 0.35676632705144584, 0.31924411398358643, 0.6099107509944588, 0.2557701840996742, 0.24643996194936335, 0.23674594797194004, 0.2428569720359519]
Total Epoch List: [33, 105, 110]
Total Time List: [0.058135161991231143, 0.07514752901624888, 0.054711465956643224]
T-times Epoch Time: 0.9835495314712084 ~ 0.8190433838301161
T-times Total Epoch: 74.66666666666667 ~ 18.31413135825468
T-times Total Time: 0.08031560065379988 ~ 0.012483285763089377
T-times Inference Elapsed: 0.2177248211046822 ~ 0.1800958945432747
T-times Time Per Graph: 0.004979357148775017 ~ 0.004094462267456473
T-times Speed: 541.972392490043 ~ 29.27559839516598
T-times cross validation test micro f1 score:0.7285949400326687 ~ 0.08469036208549646
T-times cross validation test precision:0.6873517194312626 ~ 0.08377181760983779
T-times cross validation test recall:0.8073593073593073 ~ 0.055784651601955906
T-times cross validation test f1_score:0.7285949400326687 ~ 0.06527130693883977
