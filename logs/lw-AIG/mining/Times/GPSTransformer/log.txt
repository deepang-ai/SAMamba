Namespace(seed=60, model='GPSTransformer', dataset='mining/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Times/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Processing...
Loading necessary files...
This might take a while.
Processing graphs...
  0%|          | 0/130 [00:00<?, ?it/s]100%|##########| 130/130 [00:00<00:00, 94959.86it/s]
Converting graphs into PyG objects...
  0%|          | 0/130 [00:00<?, ?it/s]100%|##########| 130/130 [00:00<00:00, 25428.32it/s]
Saving...
Done!
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 68], edge_attr=[68, 2], x=[24, 14887], y=[1, 1], num_nodes=25)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72323323eb60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7192;  Loss pred: 0.7192; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4236 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1493 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7187;  Loss pred: 0.7187; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3516 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0927 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3278 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0787 score: 0.5000 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3168 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0714 score: 0.5000 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5895;  Loss pred: 0.5895; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3171 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0699 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5412;  Loss pred: 0.5412; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3231 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0674 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4830;  Loss pred: 0.4830; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3236 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0573 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4386;  Loss pred: 0.4386; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3104 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0500 score: 0.5000 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3825;  Loss pred: 0.3825; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2701 score: 0.4884 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0321 score: 0.5000 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.3440;  Loss pred: 0.3440; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2247 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0086 score: 0.5000 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.3106;  Loss pred: 0.3106; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1875 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9852 score: 0.5000 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.2755;  Loss pred: 0.2755; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1584 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9685 score: 0.5000 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.2565;  Loss pred: 0.2565; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1350 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9541 score: 0.5000 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.2280;  Loss pred: 0.2280; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1185 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9403 score: 0.5000 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.2026;  Loss pred: 0.2026; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1071 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9283 score: 0.5000 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.1898;  Loss pred: 0.1898; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1000 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9213 score: 0.5000 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.1722;  Loss pred: 0.1722; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0929 score: 0.4884 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9147 score: 0.5000 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.1634;  Loss pred: 0.1634; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0851 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9085 score: 0.5000 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.1496;  Loss pred: 0.1496; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0801 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9031 score: 0.5000 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0766 score: 0.4884 time: 0.05s
Test loss: 0.8974 score: 0.4773 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 0.1275;  Loss pred: 0.1275; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0740 score: 0.4884 time: 0.05s
Test loss: 0.8929 score: 0.4773 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.1061;  Loss pred: 0.1061; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0709 score: 0.4884 time: 0.05s
Test loss: 0.8891 score: 0.4773 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0651 score: 0.4884 time: 0.05s
Test loss: 0.8844 score: 0.4773 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0622 score: 0.4884 time: 0.05s
Test loss: 0.8820 score: 0.4773 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.1012;  Loss pred: 0.1012; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0560 score: 0.4884 time: 0.05s
Test loss: 0.8781 score: 0.4773 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.0712;  Loss pred: 0.0712; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0485 score: 0.4884 time: 0.05s
Test loss: 0.8750 score: 0.4773 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.0776;  Loss pred: 0.0776; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0390 score: 0.4884 time: 0.05s
Test loss: 0.8721 score: 0.4773 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0340 score: 0.4884 time: 0.05s
Test loss: 0.8729 score: 0.4545 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0303 score: 0.4884 time: 0.05s
Test loss: 0.8751 score: 0.5000 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 0.0606;  Loss pred: 0.0606; Loss self: 0.0000; time: 0.12s
Val loss: 1.0251 score: 0.4651 time: 0.05s
Test loss: 0.8758 score: 0.4773 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.13s
Val loss: 1.0177 score: 0.4884 time: 0.05s
Test loss: 0.8741 score: 0.5000 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.12s
Val loss: 1.0081 score: 0.5116 time: 0.05s
Test loss: 0.8718 score: 0.5227 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.12s
Val loss: 0.9914 score: 0.5581 time: 0.05s
Test loss: 0.8636 score: 0.4773 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.0450;  Loss pred: 0.0450; Loss self: 0.0000; time: 0.12s
Val loss: 0.9598 score: 0.6279 time: 0.05s
Test loss: 0.8490 score: 0.4773 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0449;  Loss pred: 0.0449; Loss self: 0.0000; time: 0.12s
Val loss: 0.9123 score: 0.6279 time: 0.05s
Test loss: 0.8309 score: 0.4545 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0452;  Loss pred: 0.0452; Loss self: 0.0000; time: 0.12s
Val loss: 0.8612 score: 0.5581 time: 0.05s
Test loss: 0.8095 score: 0.4545 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.0367;  Loss pred: 0.0367; Loss self: 0.0000; time: 0.13s
Val loss: 0.8089 score: 0.5581 time: 0.05s
Test loss: 0.7853 score: 0.4773 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0284;  Loss pred: 0.0284; Loss self: 0.0000; time: 0.12s
Val loss: 0.7680 score: 0.5814 time: 0.05s
Test loss: 0.7645 score: 0.4773 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.12s
Val loss: 0.7332 score: 0.5814 time: 0.05s
Test loss: 0.7436 score: 0.4773 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.12s
Val loss: 0.7022 score: 0.5814 time: 0.05s
Test loss: 0.7265 score: 0.5455 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.12s
Val loss: 0.6742 score: 0.6047 time: 0.05s
Test loss: 0.7122 score: 0.5682 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.12s
Val loss: 0.6520 score: 0.6047 time: 0.05s
Test loss: 0.7015 score: 0.5909 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0180;  Loss pred: 0.0180; Loss self: 0.0000; time: 0.12s
Val loss: 0.6334 score: 0.6279 time: 0.05s
Test loss: 0.6925 score: 0.5909 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0206;  Loss pred: 0.0206; Loss self: 0.0000; time: 0.12s
Val loss: 0.6192 score: 0.6512 time: 0.05s
Test loss: 0.6853 score: 0.5909 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0250;  Loss pred: 0.0250; Loss self: 0.0000; time: 0.13s
Val loss: 0.6104 score: 0.6512 time: 0.05s
Test loss: 0.6803 score: 0.6364 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.13s
Val loss: 0.6015 score: 0.6744 time: 0.05s
Test loss: 0.6752 score: 0.6364 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.13s
Val loss: 0.5926 score: 0.6744 time: 0.05s
Test loss: 0.6700 score: 0.6364 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.14s
Val loss: 0.5841 score: 0.6744 time: 0.06s
Test loss: 0.6643 score: 0.6364 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0215;  Loss pred: 0.0215; Loss self: 0.0000; time: 0.14s
Val loss: 0.5797 score: 0.6977 time: 0.05s
Test loss: 0.6601 score: 0.6591 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.12s
Val loss: 0.5781 score: 0.6977 time: 0.04s
Test loss: 0.6576 score: 0.6591 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.12s
Val loss: 0.5775 score: 0.6977 time: 0.05s
Test loss: 0.6555 score: 0.6591 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.13s
Val loss: 0.5746 score: 0.6977 time: 0.05s
Test loss: 0.6531 score: 0.6591 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.13s
Val loss: 0.5708 score: 0.6977 time: 0.05s
Test loss: 0.6488 score: 0.6591 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.0068;  Loss pred: 0.0068; Loss self: 0.0000; time: 0.13s
Val loss: 0.5657 score: 0.6744 time: 0.05s
Test loss: 0.6441 score: 0.6591 time: 0.05s
Epoch 55/1000, LR 0.000269
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.13s
Val loss: 0.5596 score: 0.6744 time: 0.05s
Test loss: 0.6385 score: 0.6818 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.13s
Val loss: 0.5524 score: 0.6744 time: 0.05s
Test loss: 0.6305 score: 0.6818 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.13s
Val loss: 0.5444 score: 0.7209 time: 0.05s
Test loss: 0.6208 score: 0.7045 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.13s
Val loss: 0.5359 score: 0.7442 time: 0.05s
Test loss: 0.6115 score: 0.7045 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.13s
Val loss: 0.5276 score: 0.7442 time: 0.05s
Test loss: 0.6050 score: 0.7273 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.13s
Val loss: 0.5212 score: 0.7442 time: 0.05s
Test loss: 0.6018 score: 0.7273 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.13s
Val loss: 0.5160 score: 0.7442 time: 0.05s
Test loss: 0.6000 score: 0.7273 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.13s
Val loss: 0.5123 score: 0.7442 time: 0.05s
Test loss: 0.6008 score: 0.7273 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.13s
Val loss: 0.5112 score: 0.7442 time: 0.05s
Test loss: 0.6051 score: 0.7273 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.13s
Val loss: 0.5114 score: 0.7442 time: 0.05s
Test loss: 0.6100 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.13s
Val loss: 0.5121 score: 0.7209 time: 0.05s
Test loss: 0.6138 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.13s
Val loss: 0.5170 score: 0.7209 time: 0.05s
Test loss: 0.6200 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.13s
Val loss: 0.5188 score: 0.6977 time: 0.05s
Test loss: 0.6232 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.13s
Val loss: 0.5163 score: 0.6977 time: 0.05s
Test loss: 0.6227 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.13s
Val loss: 0.5124 score: 0.7209 time: 0.05s
Test loss: 0.6230 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.13s
Val loss: 0.5092 score: 0.7209 time: 0.05s
Test loss: 0.6270 score: 0.7273 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.13s
Val loss: 0.5022 score: 0.7209 time: 0.05s
Test loss: 0.6249 score: 0.7500 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.13s
Val loss: 0.4938 score: 0.7674 time: 0.05s
Test loss: 0.6257 score: 0.7500 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.13s
Val loss: 0.4847 score: 0.7674 time: 0.05s
Test loss: 0.6235 score: 0.7727 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.4828 score: 0.7674 time: 0.05s
Test loss: 0.6276 score: 0.7727 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4819 score: 0.7674 time: 0.05s
Test loss: 0.6314 score: 0.7727 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.13s
Val loss: 0.4856 score: 0.7674 time: 0.05s
Test loss: 0.6393 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.13s
Val loss: 0.4916 score: 0.7674 time: 0.05s
Test loss: 0.6487 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.13s
Val loss: 0.4965 score: 0.7674 time: 0.05s
Test loss: 0.6562 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.13s
Val loss: 0.5005 score: 0.8140 time: 0.05s
Test loss: 0.6633 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.5014 score: 0.8140 time: 0.05s
Test loss: 0.6684 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.5056 score: 0.8140 time: 0.05s
Test loss: 0.6792 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.5084 score: 0.8140 time: 0.05s
Test loss: 0.6903 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.13s
Val loss: 0.5128 score: 0.8140 time: 0.05s
Test loss: 0.7027 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.5195 score: 0.8140 time: 0.05s
Test loss: 0.7172 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.13s
Val loss: 0.5266 score: 0.8140 time: 0.05s
Test loss: 0.7323 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.13s
Val loss: 0.5357 score: 0.8140 time: 0.05s
Test loss: 0.7541 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.5495 score: 0.8372 time: 0.05s
Test loss: 0.7795 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.13s
Val loss: 0.5599 score: 0.8372 time: 0.05s
Test loss: 0.8005 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.13s
Val loss: 0.5690 score: 0.8372 time: 0.05s
Test loss: 0.8219 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.13s
Val loss: 0.5757 score: 0.8372 time: 0.05s
Test loss: 0.8394 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.13s
Val loss: 0.5820 score: 0.8372 time: 0.05s
Test loss: 0.8602 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.13s
Val loss: 0.5946 score: 0.8372 time: 0.05s
Test loss: 0.8936 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.13s
Val loss: 0.5972 score: 0.8372 time: 0.05s
Test loss: 0.9118 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.13s
Val loss: 0.5988 score: 0.8372 time: 0.05s
Test loss: 0.9295 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.5969 score: 0.8372 time: 0.05s
Test loss: 0.9421 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 074,   Train_Loss: 0.0018,   Val_Loss: 0.4819,   Val_Precision: 0.7143,   Val_Recall: 0.9091,   Val_accuracy: 0.8000,   Val_Score: 0.7674,   Val_Loss: 0.4819,   Test_Precision: 0.6875,   Test_Recall: 1.0000,   Test_accuracy: 0.8148,   Test_Score: 0.7727,   Test_loss: 0.6314


[0.05307686096057296, 0.04922433500178158, 0.049118848983198404, 0.04899013799149543, 0.049107035039924085, 0.04952612495981157, 0.049405116005800664, 0.049329018918797374, 0.05674996809102595, 0.048819851013831794, 0.04885861498769373, 0.04919665795750916, 0.06028894893825054, 0.046112664975225925, 0.04640665801707655, 0.04584860894829035, 0.05028759699780494, 0.05675529397558421, 0.04955526604317129, 0.04919415700715035, 0.04943867900874466, 0.04967297101393342, 0.0495507949963212, 0.04884574702009559, 0.049391861888580024, 0.049563018954358995, 0.050564654055051506, 0.04952329699881375, 0.04919121600687504, 0.049964317004196346, 0.04935221397317946, 0.04854145203717053, 0.04905265802517533, 0.048504860955290496, 0.048776642070151865, 0.05597498896531761, 0.048435410019010305, 0.04877280304208398, 0.04849687498062849, 0.04832685005385429, 0.04869813204277307, 0.04876611800864339, 0.048553519998677075, 0.050975812948308885, 0.05038429098203778, 0.050487534957937896, 0.0506370480870828, 0.05777236900757998, 0.04935807001311332, 0.04620041896123439, 0.04792260506656021, 0.050411266973242164, 0.049848031951114535, 0.049995020031929016, 0.05090045393444598, 0.05069697101134807, 0.05113443301524967, 0.05072112998459488, 0.0509091179119423, 0.050347310956567526, 0.050873375963419676, 0.05118534294888377, 0.049919168930500746, 0.050297745037823915, 0.05003187607508153, 0.049463395960628986, 0.05041378503665328, 0.05027356592472643, 0.05056278593838215, 0.0505907719489187, 0.050825853017158806, 0.0503570280270651, 0.050593616091646254, 0.05044130492024124, 0.05050566093996167, 0.05056805908679962, 0.05052509508095682, 0.050500083016231656, 0.05083760595880449, 0.05064197408501059, 0.05068460397887975, 0.05106269498355687, 0.05118323094211519, 0.05276787199545652, 0.05081279599107802, 0.050791108049452305, 0.051066061947494745, 0.051190980011597276, 0.0508261239156127, 0.050729834008961916, 0.050810062093660235, 0.050967930001206696, 0.05065884708892554, 0.05072480603121221, 0.05102499597705901]
[0.0012062922945584762, 0.0011187348864041269, 0.0011163374768908727, 0.0011134122270794417, 0.001116068978180093, 0.0011255937490866265, 0.0011228435455863787, 0.001121114066336304, 0.0012897720020687716, 0.001109542068496177, 0.0011104230679021302, 0.0011181058626706626, 0.0013702033849602396, 0.0010480151130733166, 0.0010546967731153761, 0.0010420138397338715, 0.001142899931768294, 0.0012898930448996412, 0.0011262560464357111, 0.0011180490228897806, 0.001123606341107833, 0.0011289311594075778, 0.0011261544317345727, 0.0011101306140930815, 0.001122542315649546, 0.0011264322489627045, 0.0011491966830693525, 0.001125529477245767, 0.0011179821819744327, 0.0011355526591862806, 0.0011216412266631696, 0.001103214819026603, 0.001114833136935803, 0.0011023832035293294, 0.001108560047048906, 0.0012721588401208546, 0.0011008047731593251, 0.0011084727964109995, 0.001102201704105193, 0.001098337501223961, 0.0011067757282448424, 0.0011083208638328042, 0.0011034890908790244, 0.0011585412033706564, 0.0011450975223190405, 0.0011474439763167704, 0.0011508420019791547, 0.0013130083865359086, 0.0011217743184798483, 0.0010500095218462361, 0.0010891501151490957, 0.001145710613028231, 0.0011329098170707848, 0.001136250455271114, 0.0011568284985101359, 0.001152203886621547, 0.001162146204892038, 0.0011527529541953381, 0.0011570254070895978, 0.0011442570671947165, 0.00115621309007772, 0.0011633032488382676, 0.0011345265666022897, 0.0011431305690414526, 0.0011370880926154894, 0.001124168090014295, 0.00114576784174212, 0.0011425810437437824, 0.0011491542258723216, 0.001149790271566334, 0.0011551330231172456, 0.001144477909706025, 0.0011498549111737784, 0.0011463932936418462, 0.0011478559304536743, 0.001149274070154537, 0.0011482976154762912, 0.0011477291594598103, 0.0011554001354273748, 0.0011509539564775134, 0.0011519228177018124, 0.001160515795080838, 0.001163255248684436, 0.0011992698180785571, 0.0011548362725245004, 0.0011543433647602797, 0.001160592316988517, 0.001163431363899938, 0.0011551391799002886, 0.0011529507729309526, 0.001154774138492278, 0.0011583620454819704, 0.0011513374338392168, 0.0011528365007093685, 0.0011596589994786138]
[828.9864774159212, 893.8668241715709, 895.7864630551633, 898.1399482409764, 896.0019672176898, 888.4200012761792, 890.5960264283956, 891.9698985384302, 775.3308324231086, 901.2727217773315, 900.5576603242337, 894.3696955594528, 729.818661212122, 954.1847131073218, 948.1398118306438, 959.6801519022033, 874.9672409664078, 775.2580758180644, 887.8975639373689, 894.4151638497356, 889.9914172912535, 885.7936036815246, 887.9776803433061, 900.7949040455456, 890.8350144656789, 887.7586742752333, 870.1730650049669, 888.4707333006111, 894.4686383408472, 880.628469239449, 891.5506814732136, 906.4417761196571, 896.9952245486442, 907.1255773840303, 902.0711170875196, 786.0653626437091, 908.4262935470256, 902.1421213382849, 907.2749536454727, 910.4669547253225, 903.5254157459971, 902.2657901988706, 906.2164803128353, 863.1544541450946, 873.2880654346449, 871.50224380448, 868.9290087433855, 761.6097583643664, 891.4449043147391, 952.3723158640464, 918.1470819227782, 872.8207530144956, 882.6827916325835, 880.0876561685477, 864.4323694375499, 867.9019500031075, 860.4769312075488, 867.4885597652057, 864.2852558574472, 873.9294942277438, 864.8924740445385, 859.6210841830363, 881.4249303961445, 874.7907081502757, 879.4393385123186, 889.5466869080796, 872.7771574383843, 875.2114394646342, 870.2052148317179, 869.7238311450678, 865.7011616734815, 873.7608576969947, 869.6749392314151, 872.3009856619223, 871.1894702715555, 870.1144713597647, 870.8543730496381, 871.2856964187087, 865.5010237038848, 868.8444871074539, 868.1137178922179, 861.6858161162237, 859.6565552839182, 833.840712844902, 865.9236151406774, 866.2933668853965, 861.6290021588125, 859.5264241870703, 865.6965475678176, 867.3397195075974, 865.9702072178741, 863.2879537967948, 868.555100015665, 867.4256925285378, 862.322458972511]
Elapsed: 0.050265502870867125~0.002118630514631455
Time per graph: 0.0011423977925197075~4.815069351435125e-05
Speed: 876.797901716675~34.451165333667866
Total Time: 0.0518
best val loss: 0.48194125294685364 test_score: 0.7727

Testing...
Test loss: 0.7795 score: 0.7500 time: 0.05s
test Score 0.7500
Epoch Time List: [0.33921836188528687, 0.30092656787019223, 0.21683783002663404, 0.21632508386392146, 0.21785775595344603, 0.21745489607565105, 0.21756740601267666, 0.21772214397788048, 0.33318383200094104, 0.2189609691267833, 0.21561367099639028, 0.21693359804339707, 0.23788331693504006, 0.2219971709419042, 0.20259651821106672, 0.2014836190501228, 0.30436325387563556, 0.22718301811255515, 0.22542168491054326, 0.21684574510436505, 0.21907032513990998, 0.2194559940835461, 0.21765222016256303, 0.216711332090199, 0.21595309698022902, 0.21756397106219083, 0.2186732159461826, 0.21720195095986128, 0.21619893691968173, 0.21852869493886828, 0.2188820600276813, 0.2161154029890895, 0.21391982294153422, 0.21371191809885204, 0.21497029275633395, 0.2213594689965248, 0.22208483109716326, 0.21391148096881807, 0.2139060910558328, 0.2142273309873417, 0.21485091501381248, 0.21471423096954823, 0.21424262796062976, 0.22141538199502975, 0.2251021189149469, 0.22824719198979437, 0.22743848292157054, 0.25797588005661964, 0.232929011923261, 0.20258245093282312, 0.2057676159311086, 0.22206680907402188, 0.22126337210647762, 0.2207582158735022, 0.22360757796559483, 0.22407078696414828, 0.22469636797904968, 0.22497002594172955, 0.22411186306271702, 0.223756956984289, 0.22392920206766576, 0.22798220987897366, 0.22376286203507334, 0.22042367595713586, 0.2207005489617586, 0.21979769493918866, 0.22100241400767118, 0.22291576210409403, 0.22348509798757732, 0.2249764489242807, 0.22437805705703795, 0.22407665813807398, 0.223548712907359, 0.224047442083247, 0.22358246298972517, 0.2245779411168769, 0.22379708313383162, 0.22425135294906795, 0.22415569378063083, 0.22455150599125773, 0.22455760114826262, 0.22561579395551234, 0.22645647800527513, 0.23199929087422788, 0.22521633096039295, 0.22529056598432362, 0.22544286807533354, 0.22498517902567983, 0.22489552397746593, 0.2247548559680581, 0.2249868530780077, 0.22485308395698667, 0.22467886889353395, 0.2252987491665408, 0.22580684709828347]
Total Epoch List: [95]
Total Time List: [0.05178280302789062]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72323323e7d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7486;  Loss pred: 0.7486; Loss self: 0.0000; time: 0.13s
Val loss: 0.7410 score: 0.5227 time: 0.04s
Test loss: 0.7284 score: 0.4884 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7503;  Loss pred: 0.7503; Loss self: 0.0000; time: 0.13s
Val loss: 0.7237 score: 0.5455 time: 0.04s
Test loss: 0.7136 score: 0.4884 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7279;  Loss pred: 0.7279; Loss self: 0.0000; time: 0.13s
Val loss: 0.7140 score: 0.5000 time: 0.04s
Test loss: 0.7049 score: 0.4419 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.13s
Val loss: 0.7077 score: 0.4773 time: 0.04s
Test loss: 0.6988 score: 0.4186 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.13s
Val loss: 0.7028 score: 0.4773 time: 0.04s
Test loss: 0.6944 score: 0.3953 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5422;  Loss pred: 0.5422; Loss self: 0.0000; time: 0.13s
Val loss: 0.6984 score: 0.4773 time: 0.04s
Test loss: 0.6907 score: 0.3721 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.4701;  Loss pred: 0.4701; Loss self: 0.0000; time: 0.14s
Val loss: 0.6934 score: 0.4773 time: 0.05s
Test loss: 0.6834 score: 0.4186 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.4083;  Loss pred: 0.4083; Loss self: 0.0000; time: 0.14s
Val loss: 0.6886 score: 0.4773 time: 0.05s
Test loss: 0.6752 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.3532;  Loss pred: 0.3532; Loss self: 0.0000; time: 0.14s
Val loss: 0.6883 score: 0.5455 time: 0.05s
Test loss: 0.6683 score: 0.5349 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.2773;  Loss pred: 0.2773; Loss self: 0.0000; time: 0.14s
Val loss: 0.6860 score: 0.5682 time: 0.05s
Test loss: 0.6648 score: 0.5349 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.2384;  Loss pred: 0.2384; Loss self: 0.0000; time: 0.14s
Val loss: 0.6811 score: 0.5909 time: 0.05s
Test loss: 0.6620 score: 0.5581 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.14s
Val loss: 0.6791 score: 0.6136 time: 0.05s
Test loss: 0.6558 score: 0.5581 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.1636;  Loss pred: 0.1636; Loss self: 0.0000; time: 0.14s
Val loss: 0.6745 score: 0.6591 time: 0.05s
Test loss: 0.6546 score: 0.5814 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.1444;  Loss pred: 0.1444; Loss self: 0.0000; time: 0.14s
Val loss: 0.6716 score: 0.6136 time: 0.05s
Test loss: 0.6541 score: 0.5814 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.14s
Val loss: 0.6661 score: 0.6591 time: 0.05s
Test loss: 0.6517 score: 0.6047 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.1092;  Loss pred: 0.1092; Loss self: 0.0000; time: 0.14s
Val loss: 0.6594 score: 0.6591 time: 0.05s
Test loss: 0.6491 score: 0.6279 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.0924;  Loss pred: 0.0924; Loss self: 0.0000; time: 0.14s
Val loss: 0.6542 score: 0.6364 time: 0.05s
Test loss: 0.6455 score: 0.6512 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.0840;  Loss pred: 0.0840; Loss self: 0.0000; time: 0.14s
Val loss: 0.6506 score: 0.6364 time: 0.05s
Test loss: 0.6420 score: 0.6047 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.0741;  Loss pred: 0.0741; Loss self: 0.0000; time: 0.14s
Val loss: 0.6481 score: 0.6591 time: 0.05s
Test loss: 0.6390 score: 0.6279 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.14s
Val loss: 0.6456 score: 0.6591 time: 0.05s
Test loss: 0.6355 score: 0.6279 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.14s
Val loss: 0.6429 score: 0.6364 time: 0.05s
Test loss: 0.6330 score: 0.6279 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.14s
Val loss: 0.6401 score: 0.6364 time: 0.05s
Test loss: 0.6308 score: 0.6279 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.0451;  Loss pred: 0.0451; Loss self: 0.0000; time: 0.14s
Val loss: 0.6366 score: 0.6364 time: 0.05s
Test loss: 0.6282 score: 0.6279 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.14s
Val loss: 0.6326 score: 0.6364 time: 0.05s
Test loss: 0.6250 score: 0.6279 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.0356;  Loss pred: 0.0356; Loss self: 0.0000; time: 0.14s
Val loss: 0.6285 score: 0.6364 time: 0.05s
Test loss: 0.6219 score: 0.6279 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.14s
Val loss: 0.6246 score: 0.6136 time: 0.05s
Test loss: 0.6187 score: 0.6279 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.0260;  Loss pred: 0.0260; Loss self: 0.0000; time: 0.14s
Val loss: 0.6209 score: 0.6364 time: 0.05s
Test loss: 0.6159 score: 0.6279 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.14s
Val loss: 0.6171 score: 0.6364 time: 0.05s
Test loss: 0.6134 score: 0.6279 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.14s
Val loss: 0.6134 score: 0.6364 time: 0.05s
Test loss: 0.6111 score: 0.6279 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.14s
Val loss: 0.6102 score: 0.6364 time: 0.05s
Test loss: 0.6088 score: 0.6279 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.14s
Val loss: 0.6073 score: 0.6364 time: 0.05s
Test loss: 0.6062 score: 0.6279 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.14s
Val loss: 0.6044 score: 0.6591 time: 0.05s
Test loss: 0.6039 score: 0.6279 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.0151;  Loss pred: 0.0151; Loss self: 0.0000; time: 0.14s
Val loss: 0.6016 score: 0.6818 time: 0.05s
Test loss: 0.6026 score: 0.6279 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.15s
Val loss: 0.5984 score: 0.6818 time: 0.05s
Test loss: 0.6018 score: 0.6744 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.15s
Val loss: 0.5943 score: 0.6818 time: 0.05s
Test loss: 0.6014 score: 0.7209 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.15s
Val loss: 0.5894 score: 0.7045 time: 0.05s
Test loss: 0.6007 score: 0.7209 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.15s
Val loss: 0.5844 score: 0.7727 time: 0.05s
Test loss: 0.5997 score: 0.7442 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.0096;  Loss pred: 0.0096; Loss self: 0.0000; time: 0.15s
Val loss: 0.5794 score: 0.7727 time: 0.05s
Test loss: 0.5990 score: 0.7209 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.14s
Val loss: 0.5736 score: 0.7727 time: 0.05s
Test loss: 0.5979 score: 0.7209 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.14s
Val loss: 0.5677 score: 0.8182 time: 0.05s
Test loss: 0.5950 score: 0.7209 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.13s
Val loss: 0.5619 score: 0.8182 time: 0.05s
Test loss: 0.5907 score: 0.7442 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.14s
Val loss: 0.5558 score: 0.8182 time: 0.05s
Test loss: 0.5859 score: 0.7907 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.14s
Val loss: 0.5495 score: 0.8409 time: 0.05s
Test loss: 0.5809 score: 0.7442 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.13s
Val loss: 0.5435 score: 0.8409 time: 0.05s
Test loss: 0.5756 score: 0.7442 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.14s
Val loss: 0.5379 score: 0.8182 time: 0.05s
Test loss: 0.5705 score: 0.7442 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.14s
Val loss: 0.5326 score: 0.8409 time: 0.05s
Test loss: 0.5655 score: 0.7674 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.13s
Val loss: 0.5279 score: 0.8409 time: 0.05s
Test loss: 0.5604 score: 0.7674 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.5229 score: 0.8636 time: 0.05s
Test loss: 0.5548 score: 0.7674 time: 0.07s
Epoch 49/1000, LR 0.000269
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.14s
Val loss: 0.5174 score: 0.8636 time: 0.05s
Test loss: 0.5496 score: 0.7907 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.13s
Val loss: 0.5115 score: 0.8636 time: 0.05s
Test loss: 0.5442 score: 0.7907 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.14s
Val loss: 0.5052 score: 0.8636 time: 0.05s
Test loss: 0.5390 score: 0.7907 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.14s
Val loss: 0.4983 score: 0.8636 time: 0.05s
Test loss: 0.5335 score: 0.7907 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.14s
Val loss: 0.4912 score: 0.8636 time: 0.05s
Test loss: 0.5284 score: 0.7907 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.13s
Val loss: 0.4841 score: 0.8636 time: 0.05s
Test loss: 0.5233 score: 0.7907 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.13s
Val loss: 0.4773 score: 0.8636 time: 0.05s
Test loss: 0.5180 score: 0.7907 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.14s
Val loss: 0.4703 score: 0.8636 time: 0.05s
Test loss: 0.5128 score: 0.7907 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.13s
Val loss: 0.4632 score: 0.8636 time: 0.05s
Test loss: 0.5075 score: 0.7674 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.14s
Val loss: 0.4561 score: 0.8636 time: 0.05s
Test loss: 0.5022 score: 0.7674 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.14s
Val loss: 0.4489 score: 0.8636 time: 0.05s
Test loss: 0.4973 score: 0.7907 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.14s
Val loss: 0.4416 score: 0.8636 time: 0.05s
Test loss: 0.4922 score: 0.7907 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.14s
Val loss: 0.4343 score: 0.8636 time: 0.05s
Test loss: 0.4872 score: 0.8140 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.14s
Val loss: 0.4271 score: 0.8636 time: 0.05s
Test loss: 0.4824 score: 0.8140 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.14s
Val loss: 0.4199 score: 0.8864 time: 0.05s
Test loss: 0.4774 score: 0.8140 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.14s
Val loss: 0.4125 score: 0.8864 time: 0.05s
Test loss: 0.4723 score: 0.8140 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.14s
Val loss: 0.4050 score: 0.8864 time: 0.05s
Test loss: 0.4674 score: 0.8140 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.14s
Val loss: 0.3975 score: 0.8864 time: 0.05s
Test loss: 0.4626 score: 0.8140 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.14s
Val loss: 0.3902 score: 0.8636 time: 0.05s
Test loss: 0.4581 score: 0.8140 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.14s
Val loss: 0.3829 score: 0.8636 time: 0.05s
Test loss: 0.4539 score: 0.8140 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.14s
Val loss: 0.3760 score: 0.8409 time: 0.05s
Test loss: 0.4499 score: 0.7907 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.14s
Val loss: 0.3695 score: 0.8636 time: 0.05s
Test loss: 0.4461 score: 0.7907 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.14s
Val loss: 0.3634 score: 0.8636 time: 0.05s
Test loss: 0.4426 score: 0.7907 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.14s
Val loss: 0.3577 score: 0.8636 time: 0.05s
Test loss: 0.4390 score: 0.7907 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.14s
Val loss: 0.3525 score: 0.8636 time: 0.05s
Test loss: 0.4354 score: 0.7907 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.14s
Val loss: 0.3483 score: 0.8409 time: 0.05s
Test loss: 0.4327 score: 0.7907 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.14s
Val loss: 0.3452 score: 0.8182 time: 0.05s
Test loss: 0.4308 score: 0.7907 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.14s
Val loss: 0.3437 score: 0.8182 time: 0.05s
Test loss: 0.4304 score: 0.7907 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.14s
Val loss: 0.3449 score: 0.8182 time: 0.05s
Test loss: 0.4304 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.14s
Val loss: 0.3446 score: 0.8182 time: 0.05s
Test loss: 0.4298 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.14s
Val loss: 0.3412 score: 0.8182 time: 0.05s
Test loss: 0.4278 score: 0.7907 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.14s
Val loss: 0.3377 score: 0.8182 time: 0.05s
Test loss: 0.4260 score: 0.8140 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.14s
Val loss: 0.3345 score: 0.8182 time: 0.05s
Test loss: 0.4243 score: 0.8140 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.13s
Val loss: 0.3320 score: 0.8182 time: 0.05s
Test loss: 0.4240 score: 0.8140 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.13s
Val loss: 0.3283 score: 0.8182 time: 0.05s
Test loss: 0.4235 score: 0.8372 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.14s
Val loss: 0.3255 score: 0.8182 time: 0.05s
Test loss: 0.4247 score: 0.8372 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.14s
Val loss: 0.3265 score: 0.8409 time: 0.05s
Test loss: 0.4274 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.14s
Val loss: 0.3267 score: 0.8409 time: 0.05s
Test loss: 0.4308 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.14s
Val loss: 0.3296 score: 0.8409 time: 0.05s
Test loss: 0.4353 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.14s
Val loss: 0.3340 score: 0.8409 time: 0.05s
Test loss: 0.4410 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.14s
Val loss: 0.3401 score: 0.8182 time: 0.05s
Test loss: 0.4493 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.14s
Val loss: 0.3437 score: 0.8182 time: 0.05s
Test loss: 0.4567 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.14s
Val loss: 0.3498 score: 0.8182 time: 0.05s
Test loss: 0.4657 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.15s
Val loss: 0.3589 score: 0.8182 time: 0.05s
Test loss: 0.4773 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.14s
Val loss: 0.3704 score: 0.7955 time: 0.05s
Test loss: 0.4904 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.14s
Val loss: 0.3847 score: 0.7955 time: 0.05s
Test loss: 0.5052 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.14s
Val loss: 0.4005 score: 0.7955 time: 0.05s
Test loss: 0.5209 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.14s
Val loss: 0.4178 score: 0.7955 time: 0.05s
Test loss: 0.5379 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.14s
Val loss: 0.4359 score: 0.7955 time: 0.05s
Test loss: 0.5565 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0007;  Loss pred: 0.0007; Loss self: 0.0000; time: 0.14s
Val loss: 0.4531 score: 0.7955 time: 0.05s
Test loss: 0.5761 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0008;  Loss pred: 0.0008; Loss self: 0.0000; time: 0.14s
Val loss: 0.4701 score: 0.7727 time: 0.05s
Test loss: 0.5957 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.14s
Val loss: 0.4747 score: 0.7955 time: 0.05s
Test loss: 0.6002 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.14s
Val loss: 0.4766 score: 0.7955 time: 0.05s
Test loss: 0.6024 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.14s
Val loss: 0.4807 score: 0.7955 time: 0.05s
Test loss: 0.6024 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.14s
Val loss: 0.4828 score: 0.7955 time: 0.05s
Test loss: 0.6088 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.14s
Val loss: 0.4854 score: 0.7955 time: 0.05s
Test loss: 0.6151 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 083,   Train_Loss: 0.0011,   Val_Loss: 0.3255,   Val_Precision: 0.8889,   Val_Recall: 0.7273,   Val_accuracy: 0.8000,   Val_Score: 0.8182,   Val_Loss: 0.3255,   Test_Precision: 0.8947,   Test_Recall: 0.7727,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.4247


[0.05307686096057296, 0.04922433500178158, 0.049118848983198404, 0.04899013799149543, 0.049107035039924085, 0.04952612495981157, 0.049405116005800664, 0.049329018918797374, 0.05674996809102595, 0.048819851013831794, 0.04885861498769373, 0.04919665795750916, 0.06028894893825054, 0.046112664975225925, 0.04640665801707655, 0.04584860894829035, 0.05028759699780494, 0.05675529397558421, 0.04955526604317129, 0.04919415700715035, 0.04943867900874466, 0.04967297101393342, 0.0495507949963212, 0.04884574702009559, 0.049391861888580024, 0.049563018954358995, 0.050564654055051506, 0.04952329699881375, 0.04919121600687504, 0.049964317004196346, 0.04935221397317946, 0.04854145203717053, 0.04905265802517533, 0.048504860955290496, 0.048776642070151865, 0.05597498896531761, 0.048435410019010305, 0.04877280304208398, 0.04849687498062849, 0.04832685005385429, 0.04869813204277307, 0.04876611800864339, 0.048553519998677075, 0.050975812948308885, 0.05038429098203778, 0.050487534957937896, 0.0506370480870828, 0.05777236900757998, 0.04935807001311332, 0.04620041896123439, 0.04792260506656021, 0.050411266973242164, 0.049848031951114535, 0.049995020031929016, 0.05090045393444598, 0.05069697101134807, 0.05113443301524967, 0.05072112998459488, 0.0509091179119423, 0.050347310956567526, 0.050873375963419676, 0.05118534294888377, 0.049919168930500746, 0.050297745037823915, 0.05003187607508153, 0.049463395960628986, 0.05041378503665328, 0.05027356592472643, 0.05056278593838215, 0.0505907719489187, 0.050825853017158806, 0.0503570280270651, 0.050593616091646254, 0.05044130492024124, 0.05050566093996167, 0.05056805908679962, 0.05052509508095682, 0.050500083016231656, 0.05083760595880449, 0.05064197408501059, 0.05068460397887975, 0.05106269498355687, 0.05118323094211519, 0.05276787199545652, 0.05081279599107802, 0.050791108049452305, 0.051066061947494745, 0.051190980011597276, 0.0508261239156127, 0.050729834008961916, 0.050810062093660235, 0.050967930001206696, 0.05065884708892554, 0.05072480603121221, 0.05102499597705901, 0.04648554592859, 0.04656234197318554, 0.046967176953330636, 0.046525320038199425, 0.04673648695461452, 0.05478229990694672, 0.05007817898876965, 0.050032618921250105, 0.05001175997313112, 0.04986634699162096, 0.04961687000468373, 0.04996556101832539, 0.05043516505975276, 0.04973394796252251, 0.04982834099791944, 0.049775609979406, 0.049577030004002154, 0.04981026705354452, 0.049746673088520765, 0.049938871059566736, 0.04973764799069613, 0.049641664954833686, 0.050253069028258324, 0.04972769692540169, 0.05013095901813358, 0.05052568705286831, 0.051369393011555076, 0.04944180394522846, 0.04954435199033469, 0.050049623008817434, 0.04973616998177022, 0.05007691809441894, 0.050353365018963814, 0.050117710954509676, 0.050500529003329575, 0.05047050002031028, 0.05033360002562404, 0.04781566304154694, 0.0478766179876402, 0.04828903404995799, 0.048481485922820866, 0.0477976780384779, 0.047731546917930245, 0.04773830808699131, 0.047991055995225906, 0.048418753081932664, 0.04841118096373975, 0.07462493900675327, 0.04802734300028533, 0.04866902995854616, 0.047714304993860424, 0.047937582014128566, 0.0479600029066205, 0.04756061395164579, 0.04788145096972585, 0.04804962407797575, 0.04776886000763625, 0.04794911609496921, 0.04806716600432992, 0.04894588200841099, 0.047957631992176175, 0.04835637705400586, 0.04831575101707131, 0.04824659798759967, 0.048011775012128055, 0.04784322890918702, 0.04825019196141511, 0.048000897048041224, 0.04891721101012081, 0.04891567304730415, 0.048263162025250494, 0.04823636810760945, 0.04833623499143869, 0.0488527329871431, 0.048244166071526706, 0.04825963906478137, 0.048435285105369985, 0.048433159943670034, 0.048351228004321456, 0.04843346495181322, 0.04810633393935859, 0.04778388992417604, 0.04817489394918084, 0.0474432889604941, 0.04797547101043165, 0.048088009003549814, 0.048091702978126705, 0.04847473104018718, 0.04833519004750997, 0.04787766095250845, 0.04899243405088782, 0.04858722002245486, 0.04854466998949647, 0.04867526504676789, 0.048153440933674574, 0.04848564602434635, 0.04802861693315208, 0.04805265797767788, 0.04824495909269899, 0.04817144095432013, 0.048963691922836006, 0.048420273000374436, 0.04860317998100072, 0.04805847897659987]
[0.0012062922945584762, 0.0011187348864041269, 0.0011163374768908727, 0.0011134122270794417, 0.001116068978180093, 0.0011255937490866265, 0.0011228435455863787, 0.001121114066336304, 0.0012897720020687716, 0.001109542068496177, 0.0011104230679021302, 0.0011181058626706626, 0.0013702033849602396, 0.0010480151130733166, 0.0010546967731153761, 0.0010420138397338715, 0.001142899931768294, 0.0012898930448996412, 0.0011262560464357111, 0.0011180490228897806, 0.001123606341107833, 0.0011289311594075778, 0.0011261544317345727, 0.0011101306140930815, 0.001122542315649546, 0.0011264322489627045, 0.0011491966830693525, 0.001125529477245767, 0.0011179821819744327, 0.0011355526591862806, 0.0011216412266631696, 0.001103214819026603, 0.001114833136935803, 0.0011023832035293294, 0.001108560047048906, 0.0012721588401208546, 0.0011008047731593251, 0.0011084727964109995, 0.001102201704105193, 0.001098337501223961, 0.0011067757282448424, 0.0011083208638328042, 0.0011034890908790244, 0.0011585412033706564, 0.0011450975223190405, 0.0011474439763167704, 0.0011508420019791547, 0.0013130083865359086, 0.0011217743184798483, 0.0010500095218462361, 0.0010891501151490957, 0.001145710613028231, 0.0011329098170707848, 0.001136250455271114, 0.0011568284985101359, 0.001152203886621547, 0.001162146204892038, 0.0011527529541953381, 0.0011570254070895978, 0.0011442570671947165, 0.00115621309007772, 0.0011633032488382676, 0.0011345265666022897, 0.0011431305690414526, 0.0011370880926154894, 0.001124168090014295, 0.00114576784174212, 0.0011425810437437824, 0.0011491542258723216, 0.001149790271566334, 0.0011551330231172456, 0.001144477909706025, 0.0011498549111737784, 0.0011463932936418462, 0.0011478559304536743, 0.001149274070154537, 0.0011482976154762912, 0.0011477291594598103, 0.0011554001354273748, 0.0011509539564775134, 0.0011519228177018124, 0.001160515795080838, 0.001163255248684436, 0.0011992698180785571, 0.0011548362725245004, 0.0011543433647602797, 0.001160592316988517, 0.001163431363899938, 0.0011551391799002886, 0.0011529507729309526, 0.001154774138492278, 0.0011583620454819704, 0.0011513374338392168, 0.0011528365007093685, 0.0011596589994786138, 0.001081059207641628, 0.0010828451621671055, 0.0010922599291472242, 0.0010819841869348704, 0.0010868950454561517, 0.0012740069745801562, 0.0011646088136923174, 0.0011635492772383745, 0.001163064185421654, 0.0011596824881772315, 0.0011538806977833425, 0.0011619897911238463, 0.0011729108153430874, 0.0011566034409888955, 0.0011587986278585917, 0.001157572325102465, 0.001152954186139585, 0.0011583783035708029, 0.0011568993741516457, 0.0011613690944085287, 0.001156689488155724, 0.001154457324531016, 0.0011686760239129842, 0.0011564580680325973, 0.0011658362562356646, 0.0011750159779736816, 0.0011946370467803506, 0.0011498093940750804, 0.0011521942323333648, 0.0011639447211352892, 0.0011566551158551214, 0.0011645794905678823, 0.001171008488813112, 0.001165528161732783, 0.001174430907054176, 0.001173732558611867, 0.0011705488378052102, 0.0011119921637569056, 0.0011134097206427953, 0.0011230007918594882, 0.0011274764168097875, 0.001111573907871579, 0.0011100359748355871, 0.0011101932113253793, 0.0011160710696564164, 0.0011260175135333179, 0.0011258414177613895, 0.0017354636978314712, 0.0011169149534950077, 0.0011318379060127014, 0.0011096349998572191, 0.0011148274887006642, 0.001115348904805128, 0.001106060789573158, 0.0011135221155750196, 0.0011174331180924593, 0.0011109037211078198, 0.001115095723138819, 0.0011178410698681376, 0.0011382763257769997, 0.001115293767259911, 0.0011245669082326944, 0.001123622116676077, 0.0011220139066883644, 0.001116552907258792, 0.0011126332304462097, 0.00112209748747477, 0.001116299931349796, 0.0011376095583749025, 0.001137573791797771, 0.0011223991168662906, 0.0011217760025025454, 0.0011240984881729927, 0.001136110069468444, 0.0011219573505006212, 0.001122317187553055, 0.001126401979194651, 0.0011263525568295358, 0.0011244471628911966, 0.0011263596500421678, 0.0011187519520781068, 0.0011112532540506056, 0.0011203463709111823, 0.0011033323014068396, 0.0011157086281495731, 0.0011183257907802283, 0.0011184116971657374, 0.001127319326515981, 0.0011240741871513945, 0.0011134339756397314, 0.0011393589314159958, 0.0011299353493594152, 0.0011289458137092201, 0.0011319829080643695, 0.0011198474635738273, 0.0011275731633568918, 0.001116944579840746, 0.0011175036738994856, 0.0011219757928534648, 0.0011202660687051195, 0.0011386905098333955, 0.0011260528604738241, 0.0011303065111860633, 0.001117639045967439]
[828.9864774159212, 893.8668241715709, 895.7864630551633, 898.1399482409764, 896.0019672176898, 888.4200012761792, 890.5960264283956, 891.9698985384302, 775.3308324231086, 901.2727217773315, 900.5576603242337, 894.3696955594528, 729.818661212122, 954.1847131073218, 948.1398118306438, 959.6801519022033, 874.9672409664078, 775.2580758180644, 887.8975639373689, 894.4151638497356, 889.9914172912535, 885.7936036815246, 887.9776803433061, 900.7949040455456, 890.8350144656789, 887.7586742752333, 870.1730650049669, 888.4707333006111, 894.4686383408472, 880.628469239449, 891.5506814732136, 906.4417761196571, 896.9952245486442, 907.1255773840303, 902.0711170875196, 786.0653626437091, 908.4262935470256, 902.1421213382849, 907.2749536454727, 910.4669547253225, 903.5254157459971, 902.2657901988706, 906.2164803128353, 863.1544541450946, 873.2880654346449, 871.50224380448, 868.9290087433855, 761.6097583643664, 891.4449043147391, 952.3723158640464, 918.1470819227782, 872.8207530144956, 882.6827916325835, 880.0876561685477, 864.4323694375499, 867.9019500031075, 860.4769312075488, 867.4885597652057, 864.2852558574472, 873.9294942277438, 864.8924740445385, 859.6210841830363, 881.4249303961445, 874.7907081502757, 879.4393385123186, 889.5466869080796, 872.7771574383843, 875.2114394646342, 870.2052148317179, 869.7238311450678, 865.7011616734815, 873.7608576969947, 869.6749392314151, 872.3009856619223, 871.1894702715555, 870.1144713597647, 870.8543730496381, 871.2856964187087, 865.5010237038848, 868.8444871074539, 868.1137178922179, 861.6858161162237, 859.6565552839182, 833.840712844902, 865.9236151406774, 866.2933668853965, 861.6290021588125, 859.5264241870703, 865.6965475678176, 867.3397195075974, 865.9702072178741, 863.2879537967948, 868.555100015665, 867.4256925285378, 862.322458972511, 925.0187158403085, 923.4930670962162, 915.5329911083935, 924.2279250243744, 920.0520364690012, 784.9250592443153, 858.6574206231226, 859.4393203298183, 859.7977760251149, 862.3049931294404, 866.6407211083828, 860.5927587649682, 852.5797417150515, 864.6005748910796, 862.9627063400615, 863.876907139675, 867.3371518327899, 863.2758373645399, 864.3794113323814, 861.0527047900202, 864.5362564800719, 866.2078525996945, 855.6691328806252, 864.7092598015519, 857.7533891670786, 851.0522569442016, 837.0743253736237, 869.709366746313, 867.9092221932505, 859.1473304888735, 864.561947889449, 858.6790408891464, 853.9647744258119, 857.9801268064656, 851.4762290344512, 851.9828411189901, 854.3001092333825, 899.2869127975363, 898.1419700760996, 890.4713222367183, 886.9365115675909, 899.6252906968474, 900.8717038635751, 900.7441135459406, 896.0002881428071, 888.0856540695456, 888.2245618467197, 576.2148763178039, 895.3233161314907, 883.5187394658419, 901.1972406500099, 896.9997691441066, 896.5804293991022, 904.1094390353644, 898.0513148439829, 894.9081460079452, 900.1680172632567, 896.7839973281912, 894.5815527407323, 878.5213022131416, 896.6247542625757, 889.2312166392512, 889.9789218800903, 891.2545504462688, 895.613627888949, 898.7687700096469, 891.1881642747949, 895.8165918641869, 879.036214699637, 879.0638525696381, 890.9486696603738, 891.443566067666, 889.6017657894985, 880.1964060294558, 891.2994772517839, 891.0137090391189, 887.782530988604, 887.8214853214399, 889.3259132147961, 887.8158942950084, 893.8531889419077, 899.88487894629, 892.5811034552607, 906.3452585634606, 896.2913566945534, 894.1938102870048, 894.1251263145633, 887.0601048688967, 889.6209978223761, 898.122404990779, 877.6865414634522, 885.0063860440525, 885.7821056215613, 883.4055645857293, 892.9787605256953, 886.8604118094701, 895.2995681688881, 894.8516442102949, 891.2848266153319, 892.6450848911892, 878.2017513664116, 888.0577769494913, 884.7157740873943, 894.7432568753811]
Elapsed: 0.049600055465830406~0.0025657665582361723
Time per graph: 0.0011408067368716699~5.7374696347461474e-05
Speed: 878.3217697537825~35.396880056444225
Total Time: 0.0487
best val loss: 0.32551395893096924 test_score: 0.8372

Testing...
Test loss: 0.4774 score: 0.8140 time: 0.04s
test Score 0.8140
Epoch Time List: [0.33921836188528687, 0.30092656787019223, 0.21683783002663404, 0.21632508386392146, 0.21785775595344603, 0.21745489607565105, 0.21756740601267666, 0.21772214397788048, 0.33318383200094104, 0.2189609691267833, 0.21561367099639028, 0.21693359804339707, 0.23788331693504006, 0.2219971709419042, 0.20259651821106672, 0.2014836190501228, 0.30436325387563556, 0.22718301811255515, 0.22542168491054326, 0.21684574510436505, 0.21907032513990998, 0.2194559940835461, 0.21765222016256303, 0.216711332090199, 0.21595309698022902, 0.21756397106219083, 0.2186732159461826, 0.21720195095986128, 0.21619893691968173, 0.21852869493886828, 0.2188820600276813, 0.2161154029890895, 0.21391982294153422, 0.21371191809885204, 0.21497029275633395, 0.2213594689965248, 0.22208483109716326, 0.21391148096881807, 0.2139060910558328, 0.2142273309873417, 0.21485091501381248, 0.21471423096954823, 0.21424262796062976, 0.22141538199502975, 0.2251021189149469, 0.22824719198979437, 0.22743848292157054, 0.25797588005661964, 0.232929011923261, 0.20258245093282312, 0.2057676159311086, 0.22206680907402188, 0.22126337210647762, 0.2207582158735022, 0.22360757796559483, 0.22407078696414828, 0.22469636797904968, 0.22497002594172955, 0.22411186306271702, 0.223756956984289, 0.22392920206766576, 0.22798220987897366, 0.22376286203507334, 0.22042367595713586, 0.2207005489617586, 0.21979769493918866, 0.22100241400767118, 0.22291576210409403, 0.22348509798757732, 0.2249764489242807, 0.22437805705703795, 0.22407665813807398, 0.223548712907359, 0.224047442083247, 0.22358246298972517, 0.2245779411168769, 0.22379708313383162, 0.22425135294906795, 0.22415569378063083, 0.22455150599125773, 0.22455760114826262, 0.22561579395551234, 0.22645647800527513, 0.23199929087422788, 0.22521633096039295, 0.22529056598432362, 0.22544286807533354, 0.22498517902567983, 0.22489552397746593, 0.2247548559680581, 0.2249868530780077, 0.22485308395698667, 0.22467886889353395, 0.2252987491665408, 0.22580684709828347, 0.2210781261092052, 0.21508095390163362, 0.21571342600509524, 0.21526048495434225, 0.21691081998869777, 0.22431791690178216, 0.23494231316726655, 0.2318320000777021, 0.23146346409339458, 0.23246301908511668, 0.23255381803028286, 0.2312532929936424, 0.23241244291421026, 0.23218551196623594, 0.23214898980222642, 0.2305801911279559, 0.23074925492983311, 0.23082565411459655, 0.23196994594763964, 0.23137812200002372, 0.2315006060525775, 0.2314819429302588, 0.23156854696571827, 0.23115421191323549, 0.23142938502132893, 0.23255697102285922, 0.2346136529231444, 0.23278481094166636, 0.23137143603526056, 0.23172467399854213, 0.23088072997052222, 0.23193967004772276, 0.23495710105635226, 0.2394777740119025, 0.2409240799024701, 0.2421004199422896, 0.24333725590258837, 0.23977177205961198, 0.23205426696222275, 0.2259851860580966, 0.22328325302805752, 0.22561616299208254, 0.22462625184562057, 0.22340949508361518, 0.2246862150495872, 0.22530872595962137, 0.22408925008494407, 0.2522061479976401, 0.2245219680480659, 0.22460898896679282, 0.22451513004489243, 0.22378117102198303, 0.2242987061617896, 0.2228798579890281, 0.22381471807602793, 0.22469137713778764, 0.22298372094519436, 0.22451994707807899, 0.22580995096359402, 0.2255305090220645, 0.2249968620017171, 0.2250777909066528, 0.2248134430265054, 0.22474822402000427, 0.2248253740835935, 0.22427195496857166, 0.22522793198004365, 0.22475524188484997, 0.22535624308511615, 0.23235205514356494, 0.2267837671097368, 0.22581753006670624, 0.22525305405724794, 0.2258514161221683, 0.2253258901182562, 0.22580743802245706, 0.22560289595276117, 0.22585686901584268, 0.22518480895087123, 0.22586328501347452, 0.2244121569674462, 0.22325986286159605, 0.2231815691338852, 0.22383503196761012, 0.2244034439790994, 0.2252670299494639, 0.22525732801295817, 0.22601598710753024, 0.22577764000743628, 0.22543471003882587, 0.22803866409230977, 0.23998566903173923, 0.22764240996912122, 0.22718959604389966, 0.22579392907209694, 0.22569529386237264, 0.22431270312517881, 0.2304663179675117, 0.2247364791110158, 0.22451156191527843, 0.22573083802126348, 0.22619886393658817, 0.22581176296807826, 0.2249257629737258]
Total Epoch List: [95, 104]
Total Time List: [0.05178280302789062, 0.04873951303306967]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7232382fbb80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8144;  Loss pred: 0.8144; Loss self: 0.0000; time: 0.14s
Val loss: 0.6992 score: 0.5227 time: 0.04s
Test loss: 0.7007 score: 0.4186 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8220;  Loss pred: 0.8220; Loss self: 0.0000; time: 0.14s
Val loss: 0.6927 score: 0.5227 time: 0.04s
Test loss: 0.6973 score: 0.4186 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7790;  Loss pred: 0.7790; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7305;  Loss pred: 0.7305; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5993;  Loss pred: 0.5993; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7106 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7022 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4956;  Loss pred: 0.4956; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7242 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7135 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.4598;  Loss pred: 0.4598; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7405 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7279 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.4263;  Loss pred: 0.4263; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7628 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7470 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3954;  Loss pred: 0.3954; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7856 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7675 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.3744;  Loss pred: 0.3744; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8083 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7909 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.3605;  Loss pred: 0.3605; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8299 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8146 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3377;  Loss pred: 0.3377; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8473 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8357 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3197;  Loss pred: 0.3197; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8614 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8545 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.3063;  Loss pred: 0.3063; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8717 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8691 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2895;  Loss pred: 0.2895; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8795 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8816 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2766;  Loss pred: 0.2766; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8840 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8920 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2629;  Loss pred: 0.2629; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8855 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8987 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2522;  Loss pred: 0.2522; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8863 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9033 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2398;  Loss pred: 0.2398; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8859 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9055 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2300;  Loss pred: 0.2300; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8840 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9059 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2172;  Loss pred: 0.2172; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8814 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9055 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2088;  Loss pred: 0.2088; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8780 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9031 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 003,   Train_Loss: 0.7305,   Val_Loss: 0.6922,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6922,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6920


[0.05307686096057296, 0.04922433500178158, 0.049118848983198404, 0.04899013799149543, 0.049107035039924085, 0.04952612495981157, 0.049405116005800664, 0.049329018918797374, 0.05674996809102595, 0.048819851013831794, 0.04885861498769373, 0.04919665795750916, 0.06028894893825054, 0.046112664975225925, 0.04640665801707655, 0.04584860894829035, 0.05028759699780494, 0.05675529397558421, 0.04955526604317129, 0.04919415700715035, 0.04943867900874466, 0.04967297101393342, 0.0495507949963212, 0.04884574702009559, 0.049391861888580024, 0.049563018954358995, 0.050564654055051506, 0.04952329699881375, 0.04919121600687504, 0.049964317004196346, 0.04935221397317946, 0.04854145203717053, 0.04905265802517533, 0.048504860955290496, 0.048776642070151865, 0.05597498896531761, 0.048435410019010305, 0.04877280304208398, 0.04849687498062849, 0.04832685005385429, 0.04869813204277307, 0.04876611800864339, 0.048553519998677075, 0.050975812948308885, 0.05038429098203778, 0.050487534957937896, 0.0506370480870828, 0.05777236900757998, 0.04935807001311332, 0.04620041896123439, 0.04792260506656021, 0.050411266973242164, 0.049848031951114535, 0.049995020031929016, 0.05090045393444598, 0.05069697101134807, 0.05113443301524967, 0.05072112998459488, 0.0509091179119423, 0.050347310956567526, 0.050873375963419676, 0.05118534294888377, 0.049919168930500746, 0.050297745037823915, 0.05003187607508153, 0.049463395960628986, 0.05041378503665328, 0.05027356592472643, 0.05056278593838215, 0.0505907719489187, 0.050825853017158806, 0.0503570280270651, 0.050593616091646254, 0.05044130492024124, 0.05050566093996167, 0.05056805908679962, 0.05052509508095682, 0.050500083016231656, 0.05083760595880449, 0.05064197408501059, 0.05068460397887975, 0.05106269498355687, 0.05118323094211519, 0.05276787199545652, 0.05081279599107802, 0.050791108049452305, 0.051066061947494745, 0.051190980011597276, 0.0508261239156127, 0.050729834008961916, 0.050810062093660235, 0.050967930001206696, 0.05065884708892554, 0.05072480603121221, 0.05102499597705901, 0.04648554592859, 0.04656234197318554, 0.046967176953330636, 0.046525320038199425, 0.04673648695461452, 0.05478229990694672, 0.05007817898876965, 0.050032618921250105, 0.05001175997313112, 0.04986634699162096, 0.04961687000468373, 0.04996556101832539, 0.05043516505975276, 0.04973394796252251, 0.04982834099791944, 0.049775609979406, 0.049577030004002154, 0.04981026705354452, 0.049746673088520765, 0.049938871059566736, 0.04973764799069613, 0.049641664954833686, 0.050253069028258324, 0.04972769692540169, 0.05013095901813358, 0.05052568705286831, 0.051369393011555076, 0.04944180394522846, 0.04954435199033469, 0.050049623008817434, 0.04973616998177022, 0.05007691809441894, 0.050353365018963814, 0.050117710954509676, 0.050500529003329575, 0.05047050002031028, 0.05033360002562404, 0.04781566304154694, 0.0478766179876402, 0.04828903404995799, 0.048481485922820866, 0.0477976780384779, 0.047731546917930245, 0.04773830808699131, 0.047991055995225906, 0.048418753081932664, 0.04841118096373975, 0.07462493900675327, 0.04802734300028533, 0.04866902995854616, 0.047714304993860424, 0.047937582014128566, 0.0479600029066205, 0.04756061395164579, 0.04788145096972585, 0.04804962407797575, 0.04776886000763625, 0.04794911609496921, 0.04806716600432992, 0.04894588200841099, 0.047957631992176175, 0.04835637705400586, 0.04831575101707131, 0.04824659798759967, 0.048011775012128055, 0.04784322890918702, 0.04825019196141511, 0.048000897048041224, 0.04891721101012081, 0.04891567304730415, 0.048263162025250494, 0.04823636810760945, 0.04833623499143869, 0.0488527329871431, 0.048244166071526706, 0.04825963906478137, 0.048435285105369985, 0.048433159943670034, 0.048351228004321456, 0.04843346495181322, 0.04810633393935859, 0.04778388992417604, 0.04817489394918084, 0.0474432889604941, 0.04797547101043165, 0.048088009003549814, 0.048091702978126705, 0.04847473104018718, 0.04833519004750997, 0.04787766095250845, 0.04899243405088782, 0.04858722002245486, 0.04854466998949647, 0.04867526504676789, 0.048153440933674574, 0.04848564602434635, 0.04802861693315208, 0.04805265797767788, 0.04824495909269899, 0.04817144095432013, 0.048963691922836006, 0.048420273000374436, 0.04860317998100072, 0.04805847897659987, 0.051619036006741226, 0.051605466986075044, 0.05252698389813304, 0.056838938035070896, 0.05204669700469822, 0.06556685001123697, 0.05215402401518077, 0.05027388094458729, 0.05052021599840373, 0.050720017054118216, 0.05144635506439954, 0.051184481009840965, 0.050860174000263214, 0.05078792793210596, 0.05655855196528137, 0.056367845041677356, 0.05616743001155555, 0.05696157400961965, 0.055883163935504854, 0.0572115940740332, 0.058023515972308815, 0.05661313608288765, 0.05715184204746038, 0.05774513992946595]
[0.0012062922945584762, 0.0011187348864041269, 0.0011163374768908727, 0.0011134122270794417, 0.001116068978180093, 0.0011255937490866265, 0.0011228435455863787, 0.001121114066336304, 0.0012897720020687716, 0.001109542068496177, 0.0011104230679021302, 0.0011181058626706626, 0.0013702033849602396, 0.0010480151130733166, 0.0010546967731153761, 0.0010420138397338715, 0.001142899931768294, 0.0012898930448996412, 0.0011262560464357111, 0.0011180490228897806, 0.001123606341107833, 0.0011289311594075778, 0.0011261544317345727, 0.0011101306140930815, 0.001122542315649546, 0.0011264322489627045, 0.0011491966830693525, 0.001125529477245767, 0.0011179821819744327, 0.0011355526591862806, 0.0011216412266631696, 0.001103214819026603, 0.001114833136935803, 0.0011023832035293294, 0.001108560047048906, 0.0012721588401208546, 0.0011008047731593251, 0.0011084727964109995, 0.001102201704105193, 0.001098337501223961, 0.0011067757282448424, 0.0011083208638328042, 0.0011034890908790244, 0.0011585412033706564, 0.0011450975223190405, 0.0011474439763167704, 0.0011508420019791547, 0.0013130083865359086, 0.0011217743184798483, 0.0010500095218462361, 0.0010891501151490957, 0.001145710613028231, 0.0011329098170707848, 0.001136250455271114, 0.0011568284985101359, 0.001152203886621547, 0.001162146204892038, 0.0011527529541953381, 0.0011570254070895978, 0.0011442570671947165, 0.00115621309007772, 0.0011633032488382676, 0.0011345265666022897, 0.0011431305690414526, 0.0011370880926154894, 0.001124168090014295, 0.00114576784174212, 0.0011425810437437824, 0.0011491542258723216, 0.001149790271566334, 0.0011551330231172456, 0.001144477909706025, 0.0011498549111737784, 0.0011463932936418462, 0.0011478559304536743, 0.001149274070154537, 0.0011482976154762912, 0.0011477291594598103, 0.0011554001354273748, 0.0011509539564775134, 0.0011519228177018124, 0.001160515795080838, 0.001163255248684436, 0.0011992698180785571, 0.0011548362725245004, 0.0011543433647602797, 0.001160592316988517, 0.001163431363899938, 0.0011551391799002886, 0.0011529507729309526, 0.001154774138492278, 0.0011583620454819704, 0.0011513374338392168, 0.0011528365007093685, 0.0011596589994786138, 0.001081059207641628, 0.0010828451621671055, 0.0010922599291472242, 0.0010819841869348704, 0.0010868950454561517, 0.0012740069745801562, 0.0011646088136923174, 0.0011635492772383745, 0.001163064185421654, 0.0011596824881772315, 0.0011538806977833425, 0.0011619897911238463, 0.0011729108153430874, 0.0011566034409888955, 0.0011587986278585917, 0.001157572325102465, 0.001152954186139585, 0.0011583783035708029, 0.0011568993741516457, 0.0011613690944085287, 0.001156689488155724, 0.001154457324531016, 0.0011686760239129842, 0.0011564580680325973, 0.0011658362562356646, 0.0011750159779736816, 0.0011946370467803506, 0.0011498093940750804, 0.0011521942323333648, 0.0011639447211352892, 0.0011566551158551214, 0.0011645794905678823, 0.001171008488813112, 0.001165528161732783, 0.001174430907054176, 0.001173732558611867, 0.0011705488378052102, 0.0011119921637569056, 0.0011134097206427953, 0.0011230007918594882, 0.0011274764168097875, 0.001111573907871579, 0.0011100359748355871, 0.0011101932113253793, 0.0011160710696564164, 0.0011260175135333179, 0.0011258414177613895, 0.0017354636978314712, 0.0011169149534950077, 0.0011318379060127014, 0.0011096349998572191, 0.0011148274887006642, 0.001115348904805128, 0.001106060789573158, 0.0011135221155750196, 0.0011174331180924593, 0.0011109037211078198, 0.001115095723138819, 0.0011178410698681376, 0.0011382763257769997, 0.001115293767259911, 0.0011245669082326944, 0.001123622116676077, 0.0011220139066883644, 0.001116552907258792, 0.0011126332304462097, 0.00112209748747477, 0.001116299931349796, 0.0011376095583749025, 0.001137573791797771, 0.0011223991168662906, 0.0011217760025025454, 0.0011240984881729927, 0.001136110069468444, 0.0011219573505006212, 0.001122317187553055, 0.001126401979194651, 0.0011263525568295358, 0.0011244471628911966, 0.0011263596500421678, 0.0011187519520781068, 0.0011112532540506056, 0.0011203463709111823, 0.0011033323014068396, 0.0011157086281495731, 0.0011183257907802283, 0.0011184116971657374, 0.001127319326515981, 0.0011240741871513945, 0.0011134339756397314, 0.0011393589314159958, 0.0011299353493594152, 0.0011289458137092201, 0.0011319829080643695, 0.0011198474635738273, 0.0011275731633568918, 0.001116944579840746, 0.0011175036738994856, 0.0011219757928534648, 0.0011202660687051195, 0.0011386905098333955, 0.0011260528604738241, 0.0011303065111860633, 0.001117639045967439, 0.0012004426978311914, 0.0012001271392110475, 0.0012215577650728615, 0.0013218357682574628, 0.0012103883024348423, 0.0015248104653776038, 0.0012128842794228085, 0.0011691600219671462, 0.001174888744148924, 0.0011795352803283306, 0.00119642686196278, 0.0011903367676707201, 0.0011827947441921678, 0.0011811146030722315, 0.0013153151619832875, 0.0013108801172483106, 0.0013062193025943152, 0.0013246877676655734, 0.0012996084636163919, 0.001330502187768214, 0.0013493840923792747, 0.0013165845600671547, 0.0013291126057548926, 0.0013429102309178128]
[828.9864774159212, 893.8668241715709, 895.7864630551633, 898.1399482409764, 896.0019672176898, 888.4200012761792, 890.5960264283956, 891.9698985384302, 775.3308324231086, 901.2727217773315, 900.5576603242337, 894.3696955594528, 729.818661212122, 954.1847131073218, 948.1398118306438, 959.6801519022033, 874.9672409664078, 775.2580758180644, 887.8975639373689, 894.4151638497356, 889.9914172912535, 885.7936036815246, 887.9776803433061, 900.7949040455456, 890.8350144656789, 887.7586742752333, 870.1730650049669, 888.4707333006111, 894.4686383408472, 880.628469239449, 891.5506814732136, 906.4417761196571, 896.9952245486442, 907.1255773840303, 902.0711170875196, 786.0653626437091, 908.4262935470256, 902.1421213382849, 907.2749536454727, 910.4669547253225, 903.5254157459971, 902.2657901988706, 906.2164803128353, 863.1544541450946, 873.2880654346449, 871.50224380448, 868.9290087433855, 761.6097583643664, 891.4449043147391, 952.3723158640464, 918.1470819227782, 872.8207530144956, 882.6827916325835, 880.0876561685477, 864.4323694375499, 867.9019500031075, 860.4769312075488, 867.4885597652057, 864.2852558574472, 873.9294942277438, 864.8924740445385, 859.6210841830363, 881.4249303961445, 874.7907081502757, 879.4393385123186, 889.5466869080796, 872.7771574383843, 875.2114394646342, 870.2052148317179, 869.7238311450678, 865.7011616734815, 873.7608576969947, 869.6749392314151, 872.3009856619223, 871.1894702715555, 870.1144713597647, 870.8543730496381, 871.2856964187087, 865.5010237038848, 868.8444871074539, 868.1137178922179, 861.6858161162237, 859.6565552839182, 833.840712844902, 865.9236151406774, 866.2933668853965, 861.6290021588125, 859.5264241870703, 865.6965475678176, 867.3397195075974, 865.9702072178741, 863.2879537967948, 868.555100015665, 867.4256925285378, 862.322458972511, 925.0187158403085, 923.4930670962162, 915.5329911083935, 924.2279250243744, 920.0520364690012, 784.9250592443153, 858.6574206231226, 859.4393203298183, 859.7977760251149, 862.3049931294404, 866.6407211083828, 860.5927587649682, 852.5797417150515, 864.6005748910796, 862.9627063400615, 863.876907139675, 867.3371518327899, 863.2758373645399, 864.3794113323814, 861.0527047900202, 864.5362564800719, 866.2078525996945, 855.6691328806252, 864.7092598015519, 857.7533891670786, 851.0522569442016, 837.0743253736237, 869.709366746313, 867.9092221932505, 859.1473304888735, 864.561947889449, 858.6790408891464, 853.9647744258119, 857.9801268064656, 851.4762290344512, 851.9828411189901, 854.3001092333825, 899.2869127975363, 898.1419700760996, 890.4713222367183, 886.9365115675909, 899.6252906968474, 900.8717038635751, 900.7441135459406, 896.0002881428071, 888.0856540695456, 888.2245618467197, 576.2148763178039, 895.3233161314907, 883.5187394658419, 901.1972406500099, 896.9997691441066, 896.5804293991022, 904.1094390353644, 898.0513148439829, 894.9081460079452, 900.1680172632567, 896.7839973281912, 894.5815527407323, 878.5213022131416, 896.6247542625757, 889.2312166392512, 889.9789218800903, 891.2545504462688, 895.613627888949, 898.7687700096469, 891.1881642747949, 895.8165918641869, 879.036214699637, 879.0638525696381, 890.9486696603738, 891.443566067666, 889.6017657894985, 880.1964060294558, 891.2994772517839, 891.0137090391189, 887.782530988604, 887.8214853214399, 889.3259132147961, 887.8158942950084, 893.8531889419077, 899.88487894629, 892.5811034552607, 906.3452585634606, 896.2913566945534, 894.1938102870048, 894.1251263145633, 887.0601048688967, 889.6209978223761, 898.122404990779, 877.6865414634522, 885.0063860440525, 885.7821056215613, 883.4055645857293, 892.9787605256953, 886.8604118094701, 895.2995681688881, 894.8516442102949, 891.2848266153319, 892.6450848911892, 878.2017513664116, 888.0577769494913, 884.7157740873943, 894.7432568753811, 833.0260176572143, 833.2450515679453, 818.6268620218331, 756.52363479184, 826.1811502873741, 655.8192134078514, 824.4809640668138, 855.3149108857403, 851.1444211037946, 847.7915130454123, 835.8220897510317, 840.0983882542957, 845.4552278916203, 846.6578919597395, 760.2740612311941, 762.8462640040005, 765.5682303988884, 754.8948698773347, 769.4625173625924, 751.595908066413, 741.0788415600556, 759.541035441729, 752.381698638719, 744.6514122664397]
Elapsed: 0.05012217882838969~0.003090352040683381
Time per graph: 0.0011543141191408413~7.222031484441617e-05
Speed: 869.1413199844959~45.68842967742347
Total Time: 0.0588
best val loss: 0.6922426819801331 test_score: 0.4884

Testing...
Test loss: 0.7007 score: 0.4186 time: 0.05s
test Score 0.4186
Epoch Time List: [0.33921836188528687, 0.30092656787019223, 0.21683783002663404, 0.21632508386392146, 0.21785775595344603, 0.21745489607565105, 0.21756740601267666, 0.21772214397788048, 0.33318383200094104, 0.2189609691267833, 0.21561367099639028, 0.21693359804339707, 0.23788331693504006, 0.2219971709419042, 0.20259651821106672, 0.2014836190501228, 0.30436325387563556, 0.22718301811255515, 0.22542168491054326, 0.21684574510436505, 0.21907032513990998, 0.2194559940835461, 0.21765222016256303, 0.216711332090199, 0.21595309698022902, 0.21756397106219083, 0.2186732159461826, 0.21720195095986128, 0.21619893691968173, 0.21852869493886828, 0.2188820600276813, 0.2161154029890895, 0.21391982294153422, 0.21371191809885204, 0.21497029275633395, 0.2213594689965248, 0.22208483109716326, 0.21391148096881807, 0.2139060910558328, 0.2142273309873417, 0.21485091501381248, 0.21471423096954823, 0.21424262796062976, 0.22141538199502975, 0.2251021189149469, 0.22824719198979437, 0.22743848292157054, 0.25797588005661964, 0.232929011923261, 0.20258245093282312, 0.2057676159311086, 0.22206680907402188, 0.22126337210647762, 0.2207582158735022, 0.22360757796559483, 0.22407078696414828, 0.22469636797904968, 0.22497002594172955, 0.22411186306271702, 0.223756956984289, 0.22392920206766576, 0.22798220987897366, 0.22376286203507334, 0.22042367595713586, 0.2207005489617586, 0.21979769493918866, 0.22100241400767118, 0.22291576210409403, 0.22348509798757732, 0.2249764489242807, 0.22437805705703795, 0.22407665813807398, 0.223548712907359, 0.224047442083247, 0.22358246298972517, 0.2245779411168769, 0.22379708313383162, 0.22425135294906795, 0.22415569378063083, 0.22455150599125773, 0.22455760114826262, 0.22561579395551234, 0.22645647800527513, 0.23199929087422788, 0.22521633096039295, 0.22529056598432362, 0.22544286807533354, 0.22498517902567983, 0.22489552397746593, 0.2247548559680581, 0.2249868530780077, 0.22485308395698667, 0.22467886889353395, 0.2252987491665408, 0.22580684709828347, 0.2210781261092052, 0.21508095390163362, 0.21571342600509524, 0.21526048495434225, 0.21691081998869777, 0.22431791690178216, 0.23494231316726655, 0.2318320000777021, 0.23146346409339458, 0.23246301908511668, 0.23255381803028286, 0.2312532929936424, 0.23241244291421026, 0.23218551196623594, 0.23214898980222642, 0.2305801911279559, 0.23074925492983311, 0.23082565411459655, 0.23196994594763964, 0.23137812200002372, 0.2315006060525775, 0.2314819429302588, 0.23156854696571827, 0.23115421191323549, 0.23142938502132893, 0.23255697102285922, 0.2346136529231444, 0.23278481094166636, 0.23137143603526056, 0.23172467399854213, 0.23088072997052222, 0.23193967004772276, 0.23495710105635226, 0.2394777740119025, 0.2409240799024701, 0.2421004199422896, 0.24333725590258837, 0.23977177205961198, 0.23205426696222275, 0.2259851860580966, 0.22328325302805752, 0.22561616299208254, 0.22462625184562057, 0.22340949508361518, 0.2246862150495872, 0.22530872595962137, 0.22408925008494407, 0.2522061479976401, 0.2245219680480659, 0.22460898896679282, 0.22451513004489243, 0.22378117102198303, 0.2242987061617896, 0.2228798579890281, 0.22381471807602793, 0.22469137713778764, 0.22298372094519436, 0.22451994707807899, 0.22580995096359402, 0.2255305090220645, 0.2249968620017171, 0.2250777909066528, 0.2248134430265054, 0.22474822402000427, 0.2248253740835935, 0.22427195496857166, 0.22522793198004365, 0.22475524188484997, 0.22535624308511615, 0.23235205514356494, 0.2267837671097368, 0.22581753006670624, 0.22525305405724794, 0.2258514161221683, 0.2253258901182562, 0.22580743802245706, 0.22560289595276117, 0.22585686901584268, 0.22518480895087123, 0.22586328501347452, 0.2244121569674462, 0.22325986286159605, 0.2231815691338852, 0.22383503196761012, 0.2244034439790994, 0.2252670299494639, 0.22525732801295817, 0.22601598710753024, 0.22577764000743628, 0.22543471003882587, 0.22803866409230977, 0.23998566903173923, 0.22764240996912122, 0.22718959604389966, 0.22579392907209694, 0.22569529386237264, 0.22431270312517881, 0.2304663179675117, 0.2247364791110158, 0.22451156191527843, 0.22573083802126348, 0.22619886393658817, 0.22581176296807826, 0.2249257629737258, 0.23367742903064936, 0.22790223802439868, 0.23216166498605162, 0.2376872191671282, 0.23933163797482848, 0.2544499811483547, 0.23471747105941176, 0.22655762685462832, 0.3108030210714787, 0.2256812530104071, 0.22754795686341822, 0.22975858999416232, 0.22452161798719317, 0.23003574286121875, 0.29856773803476244, 0.2540468949591741, 0.25231184519361705, 0.25242361985147, 0.2529270510422066, 0.25642513297498226, 0.2570149650564417, 0.2531595650361851, 0.2567367880837992, 0.2648249609628692]
Total Epoch List: [95, 104, 24]
Total Time List: [0.05178280302789062, 0.04873951303306967, 0.058791145915165544]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7232382fb610>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7885;  Loss pred: 0.7885; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2715 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2039 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7915;  Loss pred: 0.7915; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0506 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0133 score: 0.5000 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7657;  Loss pred: 0.7657; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9151 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8962 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.7218;  Loss pred: 0.7218; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8289 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8236 score: 0.5000 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6624;  Loss pred: 0.6624; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7823 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7831 score: 0.5000 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7558 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7578 score: 0.5000 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.5543;  Loss pred: 0.5543; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7392 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7389 score: 0.5000 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7307 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7342 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.4690;  Loss pred: 0.4690; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7262 score: 0.5116 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7311 score: 0.5000 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.4206;  Loss pred: 0.4206; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7250 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7291 score: 0.5000 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.3845;  Loss pred: 0.3845; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7268 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7284 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.3503;  Loss pred: 0.3503; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7270 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7280 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.3202;  Loss pred: 0.3202; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7238 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7254 score: 0.5000 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.2886;  Loss pred: 0.2886; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7200 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7223 score: 0.5000 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.2606;  Loss pred: 0.2606; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7171 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7210 score: 0.5000 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.2344;  Loss pred: 0.2344; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7161 score: 0.5116 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7215 score: 0.5000 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.2190;  Loss pred: 0.2190; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7155 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7221 score: 0.5000 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 0.2012;  Loss pred: 0.2012; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7152 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7235 score: 0.5000 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 0.1829;  Loss pred: 0.1829; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7173 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7267 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1669;  Loss pred: 0.1669; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7223 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7321 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7274 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7385 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7316 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7447 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1350;  Loss pred: 0.1350; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7384 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7520 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1246;  Loss pred: 0.1246; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7498 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7592 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1090;  Loss pred: 0.1090; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7612 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7671 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1056;  Loss pred: 0.1056; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7728 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7770 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1013;  Loss pred: 0.1013; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7835 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7885 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0873;  Loss pred: 0.0873; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7940 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7997 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8038 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8101 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0779;  Loss pred: 0.0779; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8125 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8201 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0688;  Loss pred: 0.0688; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8216 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8309 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8316 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8424 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0620;  Loss pred: 0.0620; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8398 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8527 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8482 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8636 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0516;  Loss pred: 0.0516; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8551 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8746 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8608 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8850 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8623 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8911 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8614 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8958 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 017,   Train_Loss: 0.2012,   Val_Loss: 0.7152,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.7152,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.7235


[0.05444060801528394, 0.05365151504520327, 0.053719999035820365, 0.04947969189379364, 0.04838639998342842, 0.049251066986471415, 0.048772317939437926, 0.04952061700168997, 0.049464539042674005, 0.049194457940757275, 0.049516221042722464, 0.05092237296048552, 0.04891012900043279, 0.04955454193986952, 0.04934017895720899, 0.04821396991610527, 0.04829170391894877, 0.04857240803539753, 0.04820884903892875, 0.048811570974066854, 0.04857458802871406, 0.0481676779454574, 0.04823797196149826, 0.07041813700925559, 0.047882925951853395, 0.0478751779301092, 0.04834764602128416, 0.048081020009703934, 0.04803055804222822, 0.04785955999977887, 0.06562202295754105, 0.04769086593296379, 0.047225284040905535, 0.04780821199528873, 0.04838347795885056, 0.0477549210190773, 0.04811913101002574, 0.053315491997636855]
[0.0012372865458019078, 0.0012193526146637107, 0.0012209090689959173, 0.0011245384521316737, 0.0010996909087142822, 0.001119342431510714, 0.001108461771350862, 0.0011254685682202266, 0.001124194069151682, 0.001118055862289938, 0.0011253686600618742, 0.0011573266581928526, 0.001111593840918927, 0.001126239589542489, 0.0011213677035729315, 0.001095772043547847, 0.0010975387254306538, 0.0011039183644408529, 0.0010956556599756534, 0.0011093538857742467, 0.0011039679097435014, 0.00109471995330585, 0.001096317544579506, 0.0016004122047558087, 0.0010882483170875771, 0.0010880722256843, 0.0010988101368473674, 0.0010927504547659985, 0.0010916035918688233, 0.001087717272722247, 0.0014914096126713875, 0.0010838833166582679, 0.0010733019100205804, 0.0010865502726201985, 0.0010996244990647856, 0.0010853391140699387, 0.0010936166138642213, 0.0012117157272190195]
[808.220216564209, 820.1073159430534, 819.0618166366851, 889.2537183627659, 909.3464282333323, 893.3816603828337, 902.1510942874633, 888.5188162841031, 889.5261302655698, 894.4096925102268, 888.5976973492569, 864.0602831714637, 899.6091586593578, 887.9105381175854, 891.7681477839725, 912.5985700111858, 911.1295818811483, 905.8640857981479, 912.6955087533806, 901.4256071245238, 905.8234312556625, 913.4756308955424, 912.1444830872896, 624.8390239891854, 918.9079222986978, 919.0566364940429, 910.0753319123295, 915.1220167775084, 916.0834642253255, 919.3565507121942, 670.5066076440376, 922.608536021304, 931.7042955609994, 920.3439778156938, 909.4013464146036, 921.371013940589, 914.3972278059737, 825.2760755157291]
Elapsed: 0.050147837591602615~0.004591674494907612
Time per graph: 0.0011397235816273318~0.00010435623852062754
Speed: 883.1613063286045~63.2282764026923
Total Time: 0.0537
best val loss: 0.715214192867279 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2039 score: 0.5000 time: 0.04s
test Score 0.5000
Epoch Time List: [0.24115627724677324, 0.23954851005692035, 0.23943635600153357, 0.2311156109208241, 0.21503512805793434, 0.2149054129840806, 0.2175829131156206, 0.2181322230026126, 0.301933818962425, 0.2193064090097323, 0.22528714500367641, 0.22369724605232477, 0.219541679834947, 0.2219225619919598, 0.2180483820848167, 0.34888891491573304, 0.21524667378980666, 0.21465043490752578, 0.21515235886909068, 0.2190408209571615, 0.21560487407259643, 0.2171044551068917, 0.2134222030872479, 0.23291414603590965, 0.256938706850633, 0.21150638000108302, 0.21530444209929556, 0.21426704700570554, 0.21433279698248953, 0.2130253401119262, 0.22805620916187763, 0.2570701171644032, 0.20958056906238198, 0.20946724503301084, 0.21058585203718394, 0.2101260379422456, 0.21569476590957493, 0.22293096699286252]
Total Epoch List: [38]
Total Time List: [0.053687261999584734]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72323323e6e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7608;  Loss pred: 0.7608; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9774 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9407 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7641;  Loss pred: 0.7641; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8954 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8675 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7273;  Loss pred: 0.7273; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8480 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8227 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8151 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7939 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6006;  Loss pred: 0.6006; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7899 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7725 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7735 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7587 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4753;  Loss pred: 0.4753; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7613 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7496 score: 0.5116 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.4186;  Loss pred: 0.4186; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7520 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7435 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3682;  Loss pred: 0.3682; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7438 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7377 score: 0.5116 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.3167;  Loss pred: 0.3167; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7372 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7302 score: 0.5116 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.2699;  Loss pred: 0.2699; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7308 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7218 score: 0.5116 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.2323;  Loss pred: 0.2323; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7248 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7136 score: 0.5116 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.1935;  Loss pred: 0.1935; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7185 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7060 score: 0.5116 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.1678;  Loss pred: 0.1678; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7133 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7004 score: 0.5116 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.1447;  Loss pred: 0.1447; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7103 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5116 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.1236;  Loss pred: 0.1236; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7085 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5116 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.1067;  Loss pred: 0.1067; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7100 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7137 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7185 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7085 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0678;  Loss pred: 0.0678; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7238 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7151 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0600;  Loss pred: 0.0600; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7301 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7214 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7357 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7278 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7414 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7338 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0410;  Loss pred: 0.0410; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7472 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7392 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0354;  Loss pred: 0.0354; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7534 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7443 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7600 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7494 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7686 score: 0.5000 time: 0.05s
Test loss: 0.7556 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7781 score: 0.5000 time: 0.04s
Test loss: 0.7626 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7856 score: 0.5000 time: 0.04s
Test loss: 0.7673 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7928 score: 0.5000 time: 0.04s
Test loss: 0.7723 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0197;  Loss pred: 0.0197; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7981 score: 0.5000 time: 0.04s
Test loss: 0.7764 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0174;  Loss pred: 0.0174; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8006 score: 0.5000 time: 0.05s
Test loss: 0.7783 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7999 score: 0.5000 time: 0.04s
Test loss: 0.7773 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0152;  Loss pred: 0.0152; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7951 score: 0.5000 time: 0.04s
Test loss: 0.7721 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7883 score: 0.5000 time: 0.04s
Test loss: 0.7652 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7808 score: 0.5000 time: 0.04s
Test loss: 0.7571 score: 0.5581 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 015,   Train_Loss: 0.1236,   Val_Loss: 0.7085,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.7085,   Test_Precision: 0.5116,   Test_Recall: 1.0000,   Test_accuracy: 0.6769,   Test_Score: 0.5116,   Test_loss: 0.6949


[0.05444060801528394, 0.05365151504520327, 0.053719999035820365, 0.04947969189379364, 0.04838639998342842, 0.049251066986471415, 0.048772317939437926, 0.04952061700168997, 0.049464539042674005, 0.049194457940757275, 0.049516221042722464, 0.05092237296048552, 0.04891012900043279, 0.04955454193986952, 0.04934017895720899, 0.04821396991610527, 0.04829170391894877, 0.04857240803539753, 0.04820884903892875, 0.048811570974066854, 0.04857458802871406, 0.0481676779454574, 0.04823797196149826, 0.07041813700925559, 0.047882925951853395, 0.0478751779301092, 0.04834764602128416, 0.048081020009703934, 0.04803055804222822, 0.04785955999977887, 0.06562202295754105, 0.04769086593296379, 0.047225284040905535, 0.04780821199528873, 0.04838347795885056, 0.0477549210190773, 0.04811913101002574, 0.053315491997636855, 0.04697367793414742, 0.04596168000716716, 0.04605248698499054, 0.045907238963991404, 0.04578483395744115, 0.04600563703570515, 0.0455497830407694, 0.04571411502547562, 0.04592951503582299, 0.04574627801775932, 0.04595429601613432, 0.04561703896615654, 0.04577528603840619, 0.04585444298572838, 0.04581338888965547, 0.04791353491600603, 0.0459673359291628, 0.04591582901775837, 0.045733354054391384, 0.04558562801685184, 0.045641695032827556, 0.04575396596919745, 0.045806791982613504, 0.04590225103311241, 0.04567819100338966, 0.05681687104515731, 0.045770089025609195, 0.04584265104494989, 0.045695252949371934, 0.04539001302327961, 0.04567004996351898, 0.0472809070488438, 0.04580400895792991, 0.057727620005607605, 0.0461495800409466, 0.04530061094556004]
[0.0012372865458019078, 0.0012193526146637107, 0.0012209090689959173, 0.0011245384521316737, 0.0010996909087142822, 0.001119342431510714, 0.001108461771350862, 0.0011254685682202266, 0.001124194069151682, 0.001118055862289938, 0.0011253686600618742, 0.0011573266581928526, 0.001111593840918927, 0.001126239589542489, 0.0011213677035729315, 0.001095772043547847, 0.0010975387254306538, 0.0011039183644408529, 0.0010956556599756534, 0.0011093538857742467, 0.0011039679097435014, 0.00109471995330585, 0.001096317544579506, 0.0016004122047558087, 0.0010882483170875771, 0.0010880722256843, 0.0010988101368473674, 0.0010927504547659985, 0.0010916035918688233, 0.001087717272722247, 0.0014914096126713875, 0.0010838833166582679, 0.0010733019100205804, 0.0010865502726201985, 0.0010996244990647856, 0.0010853391140699387, 0.0010936166138642213, 0.0012117157272190195, 0.0010924111147476144, 0.0010688762792364457, 0.0010709880694183846, 0.0010676102084649164, 0.0010647635804056082, 0.0010698985357140733, 0.001059297280017893, 0.0010631189540808285, 0.0010681282566470463, 0.0010638669306455657, 0.0010687045585147518, 0.0010608613713059662, 0.0010645415357768881, 0.0010663823950169391, 0.001065427648596639, 0.0011142682538606054, 0.0010690078123061116, 0.0010678099771571714, 0.0010635663733579391, 0.0010601308841128336, 0.001061434768205292, 0.0010640457202138942, 0.0010652742321538024, 0.0010674942100723816, 0.0010622835117067362, 0.0013213225824455189, 0.0010644206750141673, 0.001066108163836044, 0.0010626803011481846, 0.0010555816982158047, 0.0010620941851981157, 0.0010995559778800884, 0.0010652095106495328, 0.0013425027908280838, 0.0010732460474638745, 0.0010535025801293033]
[808.220216564209, 820.1073159430534, 819.0618166366851, 889.2537183627659, 909.3464282333323, 893.3816603828337, 902.1510942874633, 888.5188162841031, 889.5261302655698, 894.4096925102268, 888.5976973492569, 864.0602831714637, 899.6091586593578, 887.9105381175854, 891.7681477839725, 912.5985700111858, 911.1295818811483, 905.8640857981479, 912.6955087533806, 901.4256071245238, 905.8234312556625, 913.4756308955424, 912.1444830872896, 624.8390239891854, 918.9079222986978, 919.0566364940429, 910.0753319123295, 915.1220167775084, 916.0834642253255, 919.3565507121942, 670.5066076440376, 922.608536021304, 931.7042955609994, 920.3439778156938, 909.4013464146036, 921.371013940589, 914.3972278059737, 825.2760755157291, 915.4062847768034, 935.5619723494589, 933.7172173571124, 936.6714481288719, 939.1756239625164, 934.6680704937862, 944.0220595894559, 940.6285121354073, 936.2171572345561, 939.9671812274393, 935.7122995617846, 942.6302314778007, 939.371519468438, 937.7499147330872, 938.5902471342667, 897.4499601288109, 935.4468587491003, 936.4962131767098, 940.2328101468228, 943.2797544020672, 942.1210138903139, 939.809240338827, 938.7254190671363, 936.7732307720853, 941.3682778464034, 756.8174594800223, 939.4781813935455, 937.9911287817409, 941.0167845583842, 947.3449584151074, 941.5360840276768, 909.4580177063569, 938.7824554722855, 744.8774086966173, 931.7527908563391, 949.2145713371328]
Elapsed: 0.04840005078900454~0.004178353445091019
Time per graph: 0.001111971798329509~9.081918847699649e-05
Speed: 904.0566486400168~58.64424465085263
Total Time: 0.0458
best val loss: 0.7084764838218689 test_score: 0.5116

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9407 score: 0.5116 time: 0.04s
test Score 0.5116
Epoch Time List: [0.24115627724677324, 0.23954851005692035, 0.23943635600153357, 0.2311156109208241, 0.21503512805793434, 0.2149054129840806, 0.2175829131156206, 0.2181322230026126, 0.301933818962425, 0.2193064090097323, 0.22528714500367641, 0.22369724605232477, 0.219541679834947, 0.2219225619919598, 0.2180483820848167, 0.34888891491573304, 0.21524667378980666, 0.21465043490752578, 0.21515235886909068, 0.2190408209571615, 0.21560487407259643, 0.2171044551068917, 0.2134222030872479, 0.23291414603590965, 0.256938706850633, 0.21150638000108302, 0.21530444209929556, 0.21426704700570554, 0.21433279698248953, 0.2130253401119262, 0.22805620916187763, 0.2570701171644032, 0.20958056906238198, 0.20946724503301084, 0.21058585203718394, 0.2101260379422456, 0.21569476590957493, 0.22293096699286252, 0.21796219400130212, 0.21431810001377016, 0.21272834809497, 0.21307848603464663, 0.21270391799043864, 0.2137491061585024, 0.2129931648960337, 0.21271206706296653, 0.21295510802883655, 0.21244102611672133, 0.21277954487595707, 0.2127766030607745, 0.21267409075517207, 0.21311840100679547, 0.21264997799880803, 0.21552057098597288, 0.21578018006403, 0.22684965294320136, 0.2141462080180645, 0.21211273584049195, 0.2124146120622754, 0.21270399203058332, 0.21247598598711193, 0.21338010707404464, 0.21754423796664923, 0.23466733191162348, 0.2231312331277877, 0.21318686101585627, 0.21299265010748059, 0.21282820194028318, 0.21306866593658924, 0.21649779716972262, 0.21351275104098022, 0.2249687899602577, 0.21497309498954564, 0.2132184711517766]
Total Epoch List: [38, 36]
Total Time List: [0.053687261999584734, 0.04576209501828998]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72323323e230>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7390;  Loss pred: 0.7390; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7702 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7452 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7341;  Loss pred: 0.7341; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7411 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7272 score: 0.4884 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7251 score: 0.5000 time: 0.04s
Test loss: 0.7211 score: 0.4651 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6342;  Loss pred: 0.6342; Loss self: 0.0000; time: 0.15s
Val loss: 0.7145 score: 0.4545 time: 0.05s
Test loss: 0.7288 score: 0.3953 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.15s
Val loss: 0.7128 score: 0.4318 time: 0.05s
Test loss: 0.7371 score: 0.3721 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.15s
Val loss: 0.7105 score: 0.4773 time: 0.05s
Test loss: 0.7439 score: 0.3721 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.4769;  Loss pred: 0.4769; Loss self: 0.0000; time: 0.15s
Val loss: 0.7104 score: 0.4545 time: 0.05s
Test loss: 0.7499 score: 0.4186 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.4221;  Loss pred: 0.4221; Loss self: 0.0000; time: 0.20s
Val loss: 0.7097 score: 0.5000 time: 0.16s
Test loss: 0.7546 score: 0.3953 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.3777;  Loss pred: 0.3777; Loss self: 0.0000; time: 0.15s
Val loss: 0.7095 score: 0.4773 time: 0.04s
Test loss: 0.7582 score: 0.4186 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.3414;  Loss pred: 0.3414; Loss self: 0.0000; time: 0.15s
Val loss: 0.7099 score: 0.5000 time: 0.04s
Test loss: 0.7633 score: 0.4186 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3048;  Loss pred: 0.3048; Loss self: 0.0000; time: 0.14s
Val loss: 0.7107 score: 0.5000 time: 0.05s
Test loss: 0.7682 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2771;  Loss pred: 0.2771; Loss self: 0.0000; time: 0.15s
Val loss: 0.7119 score: 0.5227 time: 0.04s
Test loss: 0.7721 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.14s
Val loss: 0.7121 score: 0.5455 time: 0.04s
Test loss: 0.7765 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.2177;  Loss pred: 0.2177; Loss self: 0.0000; time: 0.14s
Val loss: 0.7120 score: 0.5682 time: 0.04s
Test loss: 0.7799 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1919;  Loss pred: 0.1919; Loss self: 0.0000; time: 0.14s
Val loss: 0.7114 score: 0.5909 time: 0.04s
Test loss: 0.7805 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1744;  Loss pred: 0.1744; Loss self: 0.0000; time: 0.15s
Val loss: 0.7106 score: 0.5682 time: 0.04s
Test loss: 0.7785 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.15s
Val loss: 0.7102 score: 0.5682 time: 0.05s
Test loss: 0.7747 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1498;  Loss pred: 0.1498; Loss self: 0.0000; time: 0.14s
Val loss: 0.7095 score: 0.5682 time: 0.04s
Test loss: 0.7686 score: 0.4884 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.1320;  Loss pred: 0.1320; Loss self: 0.0000; time: 0.15s
Val loss: 0.7085 score: 0.5682 time: 0.04s
Test loss: 0.7632 score: 0.5116 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.1232;  Loss pred: 0.1232; Loss self: 0.0000; time: 0.14s
Val loss: 0.7068 score: 0.5455 time: 0.05s
Test loss: 0.7600 score: 0.4651 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.1135;  Loss pred: 0.1135; Loss self: 0.0000; time: 0.15s
Val loss: 0.7061 score: 0.5455 time: 0.04s
Test loss: 0.7578 score: 0.4884 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.1020;  Loss pred: 0.1020; Loss self: 0.0000; time: 0.15s
Val loss: 0.7053 score: 0.5682 time: 0.04s
Test loss: 0.7550 score: 0.4884 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.0948;  Loss pred: 0.0948; Loss self: 0.0000; time: 0.14s
Val loss: 0.7048 score: 0.5682 time: 0.04s
Test loss: 0.7522 score: 0.5116 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.0851;  Loss pred: 0.0851; Loss self: 0.0000; time: 0.17s
Val loss: 0.7042 score: 0.5682 time: 0.22s
Test loss: 0.7490 score: 0.5116 time: 0.13s
Epoch 25/1000, LR 0.000270
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 0.16s
Val loss: 0.7034 score: 0.5455 time: 0.05s
Test loss: 0.7459 score: 0.5349 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.18s
Val loss: 0.7017 score: 0.5455 time: 0.05s
Test loss: 0.7429 score: 0.5349 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.0635;  Loss pred: 0.0635; Loss self: 0.0000; time: 0.15s
Val loss: 0.6997 score: 0.5227 time: 0.05s
Test loss: 0.7394 score: 0.5349 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.0579;  Loss pred: 0.0579; Loss self: 0.0000; time: 0.17s
Val loss: 0.6973 score: 0.5227 time: 0.05s
Test loss: 0.7340 score: 0.5349 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.0559;  Loss pred: 0.0559; Loss self: 0.0000; time: 0.17s
Val loss: 0.6926 score: 0.5455 time: 0.10s
Test loss: 0.7253 score: 0.5581 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.0518;  Loss pred: 0.0518; Loss self: 0.0000; time: 0.19s
Val loss: 0.6879 score: 0.5227 time: 0.09s
Test loss: 0.7145 score: 0.5581 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.15s
Val loss: 0.6832 score: 0.5455 time: 0.05s
Test loss: 0.7012 score: 0.5581 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.16s
Val loss: 0.6782 score: 0.5455 time: 0.05s
Test loss: 0.6867 score: 0.5581 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.0395;  Loss pred: 0.0395; Loss self: 0.0000; time: 0.16s
Val loss: 0.6734 score: 0.5227 time: 0.06s
Test loss: 0.6724 score: 0.5116 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.0373;  Loss pred: 0.0373; Loss self: 0.0000; time: 0.17s
Val loss: 0.6688 score: 0.5227 time: 0.10s
Test loss: 0.6588 score: 0.5116 time: 0.08s
Epoch 35/1000, LR 0.000270
Train loss: 0.0352;  Loss pred: 0.0352; Loss self: 0.0000; time: 0.21s
Val loss: 0.6660 score: 0.5455 time: 0.05s
Test loss: 0.6460 score: 0.5349 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.0312;  Loss pred: 0.0312; Loss self: 0.0000; time: 0.39s
Val loss: 0.6630 score: 0.5455 time: 0.18s
Test loss: 0.6344 score: 0.5814 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.0302;  Loss pred: 0.0302; Loss self: 0.0000; time: 0.23s
Val loss: 0.6587 score: 0.5455 time: 0.05s
Test loss: 0.6246 score: 0.5814 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.15s
Val loss: 0.6539 score: 0.5455 time: 0.05s
Test loss: 0.6164 score: 0.5814 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.16s
Val loss: 0.6486 score: 0.5455 time: 0.07s
Test loss: 0.6095 score: 0.5814 time: 0.07s
Epoch 40/1000, LR 0.000269
Train loss: 0.0239;  Loss pred: 0.0239; Loss self: 0.0000; time: 0.23s
Val loss: 0.6432 score: 0.5455 time: 0.07s
Test loss: 0.6032 score: 0.6047 time: 0.08s
Epoch 41/1000, LR 0.000269
Train loss: 0.0237;  Loss pred: 0.0237; Loss self: 0.0000; time: 0.16s
Val loss: 0.6383 score: 0.5455 time: 0.07s
Test loss: 0.5966 score: 0.6047 time: 0.12s
Epoch 42/1000, LR 0.000269
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.15s
Val loss: 0.6343 score: 0.5682 time: 0.06s
Test loss: 0.5897 score: 0.6047 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.15s
Val loss: 0.6302 score: 0.5909 time: 0.07s
Test loss: 0.5826 score: 0.6047 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.0184;  Loss pred: 0.0184; Loss self: 0.0000; time: 0.15s
Val loss: 0.6262 score: 0.5909 time: 0.06s
Test loss: 0.5750 score: 0.6512 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.0167;  Loss pred: 0.0167; Loss self: 0.0000; time: 0.16s
Val loss: 0.6222 score: 0.6364 time: 0.05s
Test loss: 0.5675 score: 0.6512 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.18s
Val loss: 0.6181 score: 0.6818 time: 0.05s
Test loss: 0.5595 score: 0.6744 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.14s
Val loss: 0.6151 score: 0.6818 time: 0.04s
Test loss: 0.5529 score: 0.6744 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.26s
Val loss: 0.6121 score: 0.6818 time: 0.07s
Test loss: 0.5476 score: 0.6977 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.25s
Val loss: 0.6087 score: 0.6818 time: 0.10s
Test loss: 0.5423 score: 0.6977 time: 0.19s
Epoch 50/1000, LR 0.000269
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.62s
Val loss: 0.6054 score: 0.7273 time: 0.18s
Test loss: 0.5371 score: 0.6977 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.75s
Val loss: 0.6008 score: 0.7273 time: 0.09s
Test loss: 0.5314 score: 0.6977 time: 0.17s
Epoch 52/1000, LR 0.000269
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.37s
Val loss: 0.5942 score: 0.7500 time: 0.35s
Test loss: 0.5244 score: 0.7442 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0122;  Loss pred: 0.0122; Loss self: 0.0000; time: 0.15s
Val loss: 0.5860 score: 0.7727 time: 0.06s
Test loss: 0.5167 score: 0.7442 time: 0.06s
Epoch 54/1000, LR 0.000269
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.32s
Val loss: 0.5776 score: 0.7955 time: 0.14s
Test loss: 0.5090 score: 0.7442 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.25s
Val loss: 0.5692 score: 0.8182 time: 0.06s
Test loss: 0.5011 score: 0.7674 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.15s
Val loss: 0.5608 score: 0.8409 time: 0.04s
Test loss: 0.4928 score: 0.7674 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.17s
Val loss: 0.5525 score: 0.8182 time: 0.05s
Test loss: 0.4842 score: 0.7907 time: 0.13s
Epoch 58/1000, LR 0.000269
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.43s
Val loss: 0.5439 score: 0.8409 time: 0.24s
Test loss: 0.4747 score: 0.7907 time: 0.74s
Epoch 59/1000, LR 0.000268
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.18s
Val loss: 0.5347 score: 0.8409 time: 0.16s
Test loss: 0.4645 score: 0.7907 time: 0.37s
Epoch 60/1000, LR 0.000268
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.26s
Val loss: 0.5256 score: 0.8636 time: 0.26s
Test loss: 0.4533 score: 0.8140 time: 0.32s
Epoch 61/1000, LR 0.000268
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.53s
Val loss: 0.5169 score: 0.8409 time: 0.16s
Test loss: 0.4419 score: 0.8605 time: 0.07s
Epoch 62/1000, LR 0.000268
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.50s
Val loss: 0.5080 score: 0.8409 time: 0.06s
Test loss: 0.4301 score: 0.9070 time: 0.17s
Epoch 63/1000, LR 0.000268
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.30s
Val loss: 0.4997 score: 0.8636 time: 0.07s
Test loss: 0.4182 score: 0.8837 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.17s
Val loss: 0.4922 score: 0.8182 time: 0.12s
Test loss: 0.4073 score: 0.8837 time: 0.10s
Epoch 65/1000, LR 0.000268
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.22s
Val loss: 0.4854 score: 0.8182 time: 0.05s
Test loss: 0.3977 score: 0.8837 time: 0.09s
Epoch 66/1000, LR 0.000268
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.63s
Val loss: 0.4786 score: 0.8182 time: 0.05s
Test loss: 0.3880 score: 0.8837 time: 0.14s
Epoch 67/1000, LR 0.000268
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.22s
Val loss: 0.4720 score: 0.8182 time: 0.05s
Test loss: 0.3789 score: 0.9070 time: 0.09s
Epoch 68/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.20s
Val loss: 0.4657 score: 0.8182 time: 0.07s
Test loss: 0.3693 score: 0.9070 time: 0.07s
Epoch 69/1000, LR 0.000268
Train loss: 0.0055;  Loss pred: 0.0055; Loss self: 0.0000; time: 0.21s
Val loss: 0.4599 score: 0.7955 time: 0.12s
Test loss: 0.3600 score: 0.9070 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.26s
Val loss: 0.4545 score: 0.7955 time: 0.08s
Test loss: 0.3504 score: 0.9070 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.17s
Val loss: 0.4490 score: 0.7955 time: 0.08s
Test loss: 0.3410 score: 0.9302 time: 0.08s
Epoch 72/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.24s
Val loss: 0.4446 score: 0.7955 time: 0.07s
Test loss: 0.3316 score: 0.9302 time: 0.28s
Epoch 73/1000, LR 0.000267
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.16s
Val loss: 0.4402 score: 0.7955 time: 0.11s
Test loss: 0.3224 score: 0.9302 time: 0.07s
Epoch 74/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.22s
Val loss: 0.4362 score: 0.7955 time: 0.05s
Test loss: 0.3135 score: 0.9302 time: 0.09s
Epoch 75/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.19s
Val loss: 0.4323 score: 0.7955 time: 0.14s
Test loss: 0.3056 score: 0.9302 time: 0.10s
Epoch 76/1000, LR 0.000267
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.23s
Val loss: 0.4294 score: 0.7955 time: 0.13s
Test loss: 0.2979 score: 0.9302 time: 0.12s
Epoch 77/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.20s
Val loss: 0.4279 score: 0.7955 time: 0.05s
Test loss: 0.2910 score: 0.9302 time: 0.09s
Epoch 78/1000, LR 0.000267
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.60s
Val loss: 0.4273 score: 0.7955 time: 0.06s
Test loss: 0.2848 score: 0.9302 time: 0.11s
Epoch 79/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.19s
Val loss: 0.4270 score: 0.7955 time: 0.05s
Test loss: 0.2791 score: 0.9302 time: 0.07s
Epoch 80/1000, LR 0.000267
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.17s
Val loss: 0.4285 score: 0.7727 time: 0.14s
Test loss: 0.2737 score: 0.9302 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.28s
Val loss: 0.4322 score: 0.7727 time: 0.07s
Test loss: 0.2688 score: 0.9302 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.22s
Val loss: 0.4384 score: 0.7727 time: 0.06s
Test loss: 0.2647 score: 0.9302 time: 0.17s
     INFO: Early stopping counter 3 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.20s
Val loss: 0.4444 score: 0.7727 time: 0.11s
Test loss: 0.2613 score: 0.9070 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.20s
Val loss: 0.4515 score: 0.7727 time: 0.07s
Test loss: 0.2585 score: 0.9070 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.51s
Val loss: 0.4611 score: 0.7500 time: 0.10s
Test loss: 0.2566 score: 0.9070 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.20s
Val loss: 0.4727 score: 0.7500 time: 0.05s
Test loss: 0.2555 score: 0.9070 time: 0.18s
     INFO: Early stopping counter 7 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.18s
Val loss: 0.4823 score: 0.7500 time: 0.05s
Test loss: 0.2546 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.15s
Val loss: 0.4946 score: 0.7500 time: 0.04s
Test loss: 0.2545 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.15s
Val loss: 0.5060 score: 0.7500 time: 0.05s
Test loss: 0.2546 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.16s
Val loss: 0.5154 score: 0.7500 time: 0.05s
Test loss: 0.2547 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.16s
Val loss: 0.5255 score: 0.7500 time: 0.05s
Test loss: 0.2550 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.16s
Val loss: 0.5375 score: 0.7500 time: 0.04s
Test loss: 0.2560 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.14s
Val loss: 0.5501 score: 0.7500 time: 0.04s
Test loss: 0.2573 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.14s
Val loss: 0.5639 score: 0.7500 time: 0.04s
Test loss: 0.2592 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.14s
Val loss: 0.5784 score: 0.7500 time: 0.04s
Test loss: 0.2615 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.14s
Val loss: 0.5928 score: 0.7500 time: 0.04s
Test loss: 0.2637 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.14s
Val loss: 0.6071 score: 0.7727 time: 0.04s
Test loss: 0.2661 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.14s
Val loss: 0.6232 score: 0.7727 time: 0.04s
Test loss: 0.2696 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.14s
Val loss: 0.6378 score: 0.7727 time: 0.04s
Test loss: 0.2728 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 078,   Train_Loss: 0.0036,   Val_Loss: 0.4270,   Val_Precision: 0.9333,   Val_Recall: 0.6364,   Val_accuracy: 0.7568,   Val_Score: 0.7955,   Val_Loss: 0.4270,   Test_Precision: 1.0000,   Test_Recall: 0.8571,   Test_accuracy: 0.9231,   Test_Score: 0.9302,   Test_loss: 0.2791


[0.05444060801528394, 0.05365151504520327, 0.053719999035820365, 0.04947969189379364, 0.04838639998342842, 0.049251066986471415, 0.048772317939437926, 0.04952061700168997, 0.049464539042674005, 0.049194457940757275, 0.049516221042722464, 0.05092237296048552, 0.04891012900043279, 0.04955454193986952, 0.04934017895720899, 0.04821396991610527, 0.04829170391894877, 0.04857240803539753, 0.04820884903892875, 0.048811570974066854, 0.04857458802871406, 0.0481676779454574, 0.04823797196149826, 0.07041813700925559, 0.047882925951853395, 0.0478751779301092, 0.04834764602128416, 0.048081020009703934, 0.04803055804222822, 0.04785955999977887, 0.06562202295754105, 0.04769086593296379, 0.047225284040905535, 0.04780821199528873, 0.04838347795885056, 0.0477549210190773, 0.04811913101002574, 0.053315491997636855, 0.04697367793414742, 0.04596168000716716, 0.04605248698499054, 0.045907238963991404, 0.04578483395744115, 0.04600563703570515, 0.0455497830407694, 0.04571411502547562, 0.04592951503582299, 0.04574627801775932, 0.04595429601613432, 0.04561703896615654, 0.04577528603840619, 0.04585444298572838, 0.04581338888965547, 0.04791353491600603, 0.0459673359291628, 0.04591582901775837, 0.045733354054391384, 0.04558562801685184, 0.045641695032827556, 0.04575396596919745, 0.045806791982613504, 0.04590225103311241, 0.04567819100338966, 0.05681687104515731, 0.045770089025609195, 0.04584265104494989, 0.045695252949371934, 0.04539001302327961, 0.04567004996351898, 0.0472809070488438, 0.04580400895792991, 0.057727620005607605, 0.0461495800409466, 0.04530061094556004, 0.05357654206454754, 0.05400805198587477, 0.05445877893362194, 0.0546078709885478, 0.05533291900064796, 0.05516328208614141, 0.055205611046403646, 0.06193695601541549, 0.053696221904829144, 0.0657858649501577, 0.0545064490288496, 0.05030563601758331, 0.05148300307337195, 0.050323097966611385, 0.052777564036659896, 0.05210857803467661, 0.05785052198916674, 0.05085500795394182, 0.05036508699413389, 0.05425144801847637, 0.05111670691985637, 0.05088013003114611, 0.05049620394129306, 0.13307132094632834, 0.054817424970678985, 0.05561600090004504, 0.05802233598660678, 0.08576625003479421, 0.05817280104383826, 0.062090564984828234, 0.05499411199707538, 0.05905879510100931, 0.06376656598877162, 0.09018473397009075, 0.06473962496966124, 0.0690620499663055, 0.05640055995900184, 0.05461301899049431, 0.07394235499668866, 0.08102405292447656, 0.12203775602392852, 0.056469273986294866, 0.058324576006270945, 0.06090795004274696, 0.06772261310834438, 0.08444344892632216, 0.06155326496809721, 0.06000031600706279, 0.19148218096233904, 0.08334446896333247, 0.17179941502399743, 0.056629822938703, 0.061978978919796646, 0.2249092860147357, 0.06199305795598775, 0.052278703078627586, 0.13326592196244746, 0.7449238299159333, 0.3743276509921998, 0.33079619996715337, 0.0718526029959321, 0.1802955879829824, 0.23090545902960002, 0.10430222598370165, 0.09411191602703184, 0.14374339301139116, 0.09585104405414313, 0.07856475794687867, 0.06012420495972037, 0.05277330905664712, 0.08620081504341215, 0.28853800101205707, 0.07153720303904265, 0.09948131896089762, 0.10611669591162354, 0.12237948994152248, 0.09264597296714783, 0.109740732004866, 0.07831011503003538, 0.058782959007658064, 0.09028694895096123, 0.1810873420909047, 0.10706730396486819, 0.1133027970790863, 0.08763796102721244, 0.189759191009216, 0.05659205210395157, 0.0517794790212065, 0.053041314939036965, 0.05646310292650014, 0.05557961796876043, 0.051430388004519045, 0.05309448798652738, 0.049620277946814895, 0.04977875098120421, 0.04978584102354944, 0.0503337390255183, 0.04994185199029744, 0.049553654971532524]
[0.0012372865458019078, 0.0012193526146637107, 0.0012209090689959173, 0.0011245384521316737, 0.0010996909087142822, 0.001119342431510714, 0.001108461771350862, 0.0011254685682202266, 0.001124194069151682, 0.001118055862289938, 0.0011253686600618742, 0.0011573266581928526, 0.001111593840918927, 0.001126239589542489, 0.0011213677035729315, 0.001095772043547847, 0.0010975387254306538, 0.0011039183644408529, 0.0010956556599756534, 0.0011093538857742467, 0.0011039679097435014, 0.00109471995330585, 0.001096317544579506, 0.0016004122047558087, 0.0010882483170875771, 0.0010880722256843, 0.0010988101368473674, 0.0010927504547659985, 0.0010916035918688233, 0.001087717272722247, 0.0014914096126713875, 0.0010838833166582679, 0.0010733019100205804, 0.0010865502726201985, 0.0010996244990647856, 0.0010853391140699387, 0.0010936166138642213, 0.0012117157272190195, 0.0010924111147476144, 0.0010688762792364457, 0.0010709880694183846, 0.0010676102084649164, 0.0010647635804056082, 0.0010698985357140733, 0.001059297280017893, 0.0010631189540808285, 0.0010681282566470463, 0.0010638669306455657, 0.0010687045585147518, 0.0010608613713059662, 0.0010645415357768881, 0.0010663823950169391, 0.001065427648596639, 0.0011142682538606054, 0.0010690078123061116, 0.0010678099771571714, 0.0010635663733579391, 0.0010601308841128336, 0.001061434768205292, 0.0010640457202138942, 0.0010652742321538024, 0.0010674942100723816, 0.0010622835117067362, 0.0013213225824455189, 0.0010644206750141673, 0.001066108163836044, 0.0010626803011481846, 0.0010555816982158047, 0.0010620941851981157, 0.0010995559778800884, 0.0010652095106495328, 0.0013425027908280838, 0.0010732460474638745, 0.0010535025801293033, 0.0012459660945243615, 0.0012560012089738318, 0.0012664832310144637, 0.0012699504881057628, 0.0012868120697825107, 0.0012828670252591026, 0.0012838514196838056, 0.001440394325939895, 0.0012487493466239335, 0.0015299038360501792, 0.0012675918378802233, 0.001169898512036821, 0.001197279141241208, 0.0011703046038746834, 0.0012273852101548812, 0.00121182739615527, 0.0013453609764922498, 0.0011826746035800424, 0.0011712810928868347, 0.0012616615818250318, 0.0011887606260431714, 0.0011832588379336304, 0.0011743303242161176, 0.003094681882472752, 0.0012748238365274183, 0.0012933953697684892, 0.0013493566508513204, 0.00199456395429754, 0.0013528558382287967, 0.001443966627554145, 0.001278932837141288, 0.001373460351186263, 0.0014829433950877121, 0.002097319394653273, 0.001505572673713052, 0.0016060941852629185, 0.0013116409292791126, 0.001270070209081263, 0.0017195896510857828, 0.0018842803005692224, 0.0028380873493936867, 0.001313238929913834, 0.001356385488517929, 0.0014164639544824874, 0.0015749444908917296, 0.0019638011378214455, 0.001431471278327842, 0.0013953561862107627, 0.00445307397586835, 0.0019382434642635459, 0.003995335233116219, 0.001316972626481465, 0.0014413716027859686, 0.005230448511970597, 0.0014416990222322734, 0.001215783792526223, 0.0030992074874987784, 0.01732380999804496, 0.008705294209120924, 0.007692934882957055, 0.0016709907673472582, 0.004192920650767032, 0.00536989439603721, 0.002425633162411666, 0.002188649209930973, 0.0033428696049160734, 0.0022290940477707705, 0.0018270873941134574, 0.0013982373246446598, 0.0012272862571313283, 0.0020046701172886545, 0.006710186070047839, 0.0016636558846288988, 0.00231351904560227, 0.002467830137479617, 0.002846034649802848, 0.002154557510863903, 0.0025521100466247906, 0.001821165465814776, 0.0013670455583176294, 0.0020996964872316564, 0.004211333536997784, 0.0024899373015085627, 0.002634948769281077, 0.0020380921169119172, 0.004413004442074791, 0.001316094234975618, 0.0012041739307257326, 0.001233518952070627, 0.001313095416895352, 0.0012925492550874519, 0.001196055534988815, 0.0012347555345704044, 0.0011539599522515092, 0.0011576453716559119, 0.0011578102563616149, 0.0011705520703608906, 0.0011614384183790101, 0.0011524105807333145]
[808.220216564209, 820.1073159430534, 819.0618166366851, 889.2537183627659, 909.3464282333323, 893.3816603828337, 902.1510942874633, 888.5188162841031, 889.5261302655698, 894.4096925102268, 888.5976973492569, 864.0602831714637, 899.6091586593578, 887.9105381175854, 891.7681477839725, 912.5985700111858, 911.1295818811483, 905.8640857981479, 912.6955087533806, 901.4256071245238, 905.8234312556625, 913.4756308955424, 912.1444830872896, 624.8390239891854, 918.9079222986978, 919.0566364940429, 910.0753319123295, 915.1220167775084, 916.0834642253255, 919.3565507121942, 670.5066076440376, 922.608536021304, 931.7042955609994, 920.3439778156938, 909.4013464146036, 921.371013940589, 914.3972278059737, 825.2760755157291, 915.4062847768034, 935.5619723494589, 933.7172173571124, 936.6714481288719, 939.1756239625164, 934.6680704937862, 944.0220595894559, 940.6285121354073, 936.2171572345561, 939.9671812274393, 935.7122995617846, 942.6302314778007, 939.371519468438, 937.7499147330872, 938.5902471342667, 897.4499601288109, 935.4468587491003, 936.4962131767098, 940.2328101468228, 943.2797544020672, 942.1210138903139, 939.809240338827, 938.7254190671363, 936.7732307720853, 941.3682778464034, 756.8174594800223, 939.4781813935455, 937.9911287817409, 941.0167845583842, 947.3449584151074, 941.5360840276768, 909.4580177063569, 938.7824554722855, 744.8774086966173, 931.7527908563391, 949.2145713371328, 802.5900579435453, 796.1775775813242, 789.588030470006, 787.4322734357805, 777.1142527199127, 779.5040174160129, 778.9063318918055, 694.254331602892, 800.80121979928, 653.6358537290452, 788.8974748151475, 854.7749994646767, 835.2271125038639, 854.4783953589234, 814.740141665722, 825.2000269779932, 743.2949353171318, 845.5411124690823, 853.7660225824346, 792.6055722117413, 841.212249204899, 845.1236263287395, 851.5491590217721, 323.13499027595276, 784.4221070763549, 773.1587907099084, 741.0939126946843, 501.36271531698634, 739.1770591826198, 692.5367809184376, 781.9018880109704, 728.0879998729458, 674.3345722517296, 476.7991001033579, 664.1990901268115, 622.6284916387388, 762.4037781053443, 787.3580474920161, 581.534088303323, 530.7066043719238, 352.3499726721359, 761.4760552869182, 737.2535377775695, 705.9833727751691, 634.9430127748835, 509.21652948493346, 698.5819521074425, 716.6628921577407, 224.56397657418208, 515.9310573916779, 250.29188832798783, 759.3172248930368, 693.7836142096463, 191.18819307968772, 693.6260513318772, 822.5146659688106, 322.6631337313437, 57.72402260893261, 114.87262532175598, 129.98940134218506, 598.4473520386509, 238.49723934486212, 186.22340147656615, 412.2634928876706, 456.90282182384937, 299.14418394584857, 448.6127451643688, 547.3191940472129, 715.1861721715477, 814.8058321270624, 498.8351905761505, 149.0271640102032, 601.0858430756933, 432.2419570743899, 405.2142750073127, 351.36606649159114, 464.1324239235715, 391.83263328417877, 549.0989252602631, 731.504516375198, 476.25930989599806, 237.4544764062762, 401.616538454256, 379.51402002887573, 490.6549570071362, 226.60298966974213, 759.8240106404888, 830.4448173839133, 810.6888007852378, 761.5592798003769, 773.6649075956038, 836.0815787783229, 809.8769124755693, 866.5811998491668, 863.8223971556904, 863.6993795015005, 854.2977500280632, 861.0013102508477, 867.7462848038666]
Elapsed: 0.07358162128231946~0.07035499105812823
Time per graph: 0.0017053785435864268~0.0016381030061515067
Speed: 743.5116319119081~220.29703089254573
Total Time: 0.0503
best val loss: 0.42701151967048645 test_score: 0.9302

Testing...
Test loss: 0.4533 score: 0.8140 time: 0.04s
test Score 0.8140
Epoch Time List: [0.24115627724677324, 0.23954851005692035, 0.23943635600153357, 0.2311156109208241, 0.21503512805793434, 0.2149054129840806, 0.2175829131156206, 0.2181322230026126, 0.301933818962425, 0.2193064090097323, 0.22528714500367641, 0.22369724605232477, 0.219541679834947, 0.2219225619919598, 0.2180483820848167, 0.34888891491573304, 0.21524667378980666, 0.21465043490752578, 0.21515235886909068, 0.2190408209571615, 0.21560487407259643, 0.2171044551068917, 0.2134222030872479, 0.23291414603590965, 0.256938706850633, 0.21150638000108302, 0.21530444209929556, 0.21426704700570554, 0.21433279698248953, 0.2130253401119262, 0.22805620916187763, 0.2570701171644032, 0.20958056906238198, 0.20946724503301084, 0.21058585203718394, 0.2101260379422456, 0.21569476590957493, 0.22293096699286252, 0.21796219400130212, 0.21431810001377016, 0.21272834809497, 0.21307848603464663, 0.21270391799043864, 0.2137491061585024, 0.2129931648960337, 0.21271206706296653, 0.21295510802883655, 0.21244102611672133, 0.21277954487595707, 0.2127766030607745, 0.21267409075517207, 0.21311840100679547, 0.21264997799880803, 0.21552057098597288, 0.21578018006403, 0.22684965294320136, 0.2141462080180645, 0.21211273584049195, 0.2124146120622754, 0.21270399203058332, 0.21247598598711193, 0.21338010707404464, 0.21754423796664923, 0.23466733191162348, 0.2231312331277877, 0.21318686101585627, 0.21299265010748059, 0.21282820194028318, 0.21306866593658924, 0.21649779716972262, 0.21351275104098022, 0.2249687899602577, 0.21497309498954564, 0.2132184711517766, 0.23829538992140442, 0.2386128071229905, 0.23929194908123463, 0.24665506288874894, 0.2480000490322709, 0.24810948106460273, 0.24623649602290243, 0.4085807960946113, 0.24139046005439013, 0.2529184930026531, 0.23850268789101392, 0.23311376082710922, 0.22663106711115688, 0.23139987292233855, 0.22782078303862363, 0.23536044999491423, 0.24832491483539343, 0.22767866705544293, 0.23755740094929934, 0.2369044190272689, 0.24093209113925695, 0.23294557898771018, 0.22772934311069548, 0.5199167050886899, 0.2555044660111889, 0.27875421685166657, 0.25272002595011145, 0.30633720208425075, 0.32138555496931076, 0.33274903299752623, 0.2554565640166402, 0.25777809985447675, 0.27178330498281866, 0.3589104999555275, 0.3161415670765564, 0.6350288148969412, 0.3279098760103807, 0.24855816399212927, 0.3053188369376585, 0.3778774128295481, 0.3496714950306341, 0.25915782107040286, 0.2775459859985858, 0.2658636709675193, 0.27188387396745384, 0.3097795130452141, 0.2371406458551064, 0.3861642761621624, 0.5333755101310089, 0.8728047289187089, 1.0099452160065994, 0.766676529077813, 0.26546372403390706, 0.681421012035571, 0.3645205009961501, 0.2413223530165851, 0.3478141720406711, 1.4127929809037596, 0.702885746024549, 0.845816622953862, 0.7518428909825161, 0.7387798419222236, 0.5995326029369608, 0.38714667805470526, 0.36091156606562436, 0.8141209359746426, 0.35968789202161133, 0.34415085101500154, 0.38156084599904716, 0.3974924909416586, 0.32745618314947933, 0.5942571819759905, 0.33743108506314456, 0.3679806030122563, 0.43400862195994705, 0.47733234497718513, 0.3421858021756634, 0.7620620511006564, 0.31526663701515645, 0.3614863969851285, 0.4289787638699636, 0.45527888706419617, 0.41270619886927307, 0.37807159882504493, 0.6896685980027542, 0.4339320210274309, 0.2779293570201844, 0.23746297508478165, 0.24788112100213766, 0.2572073529008776, 0.26832654303871095, 0.2468106410233304, 0.2333494999911636, 0.22246635204646736, 0.22192178410477936, 0.2219205719884485, 0.22262051014695317, 0.22155001712962985, 0.22300142608582973]
Total Epoch List: [38, 36, 99]
Total Time List: [0.053687261999584734, 0.04576209501828998, 0.05033453495707363]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x72323323e410>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7237;  Loss pred: 0.7237; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 2.0300 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5113 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7300;  Loss pred: 0.7300; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7292 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3076 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.5594 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2051 score: 0.5000 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4569 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1479 score: 0.5000 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4051 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1150 score: 0.5000 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3803 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1010 score: 0.5000 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4792;  Loss pred: 0.4792; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3763 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1042 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.4240;  Loss pred: 0.4240; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3791 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1087 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3680;  Loss pred: 0.3680; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3882 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1209 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3442;  Loss pred: 0.3442; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3960 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1318 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3033;  Loss pred: 0.3033; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3962 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1349 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2756;  Loss pred: 0.2756; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3867 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1264 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2538;  Loss pred: 0.2538; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3740 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1174 score: 0.5000 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.2382;  Loss pred: 0.2382; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3561 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1067 score: 0.5000 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.2136;  Loss pred: 0.2136; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3382 score: 0.4884 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0967 score: 0.5000 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.2048;  Loss pred: 0.2048; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3258 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0875 score: 0.5000 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.1822;  Loss pred: 0.1822; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3122 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0772 score: 0.5000 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.1697;  Loss pred: 0.1697; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.3001 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0690 score: 0.5000 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.1614;  Loss pred: 0.1614; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2863 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0602 score: 0.5000 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.1570;  Loss pred: 0.1570; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2708 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0506 score: 0.5000 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.1509;  Loss pred: 0.1509; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2524 score: 0.4884 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0400 score: 0.5000 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.1353;  Loss pred: 0.1353; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2333 score: 0.4884 time: 0.12s
Test loss: 1.0283 score: 0.5000 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.1209;  Loss pred: 0.1209; Loss self: 0.0000; time: 0.13s
Val loss: 1.2145 score: 0.5349 time: 0.05s
Test loss: 1.0178 score: 0.5455 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 0.1204;  Loss pred: 0.1204; Loss self: 0.0000; time: 0.12s
Val loss: 1.1979 score: 0.5814 time: 0.05s
Test loss: 1.0088 score: 0.5455 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.1095;  Loss pred: 0.1095; Loss self: 0.0000; time: 0.12s
Val loss: 1.1812 score: 0.6047 time: 0.05s
Test loss: 1.0010 score: 0.5682 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.1109;  Loss pred: 0.1109; Loss self: 0.0000; time: 0.12s
Val loss: 1.1595 score: 0.5349 time: 0.05s
Test loss: 0.9916 score: 0.5000 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.1003;  Loss pred: 0.1003; Loss self: 0.0000; time: 0.12s
Val loss: 1.1392 score: 0.5581 time: 0.05s
Test loss: 0.9831 score: 0.5227 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.0949;  Loss pred: 0.0949; Loss self: 0.0000; time: 0.12s
Val loss: 1.1194 score: 0.5814 time: 0.05s
Test loss: 0.9755 score: 0.5455 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 0.0843;  Loss pred: 0.0843; Loss self: 0.0000; time: 0.13s
Val loss: 1.1022 score: 0.5814 time: 0.14s
Test loss: 0.9730 score: 0.5455 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 0.13s
Val loss: 1.0863 score: 0.5581 time: 0.05s
Test loss: 0.9711 score: 0.5455 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 0.0767;  Loss pred: 0.0767; Loss self: 0.0000; time: 0.13s
Val loss: 1.0677 score: 0.5814 time: 0.05s
Test loss: 0.9688 score: 0.5682 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.0755;  Loss pred: 0.0755; Loss self: 0.0000; time: 0.13s
Val loss: 1.0505 score: 0.5814 time: 0.05s
Test loss: 0.9663 score: 0.5227 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.0697;  Loss pred: 0.0697; Loss self: 0.0000; time: 0.13s
Val loss: 1.0367 score: 0.5814 time: 0.06s
Test loss: 0.9644 score: 0.5227 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.0688;  Loss pred: 0.0688; Loss self: 0.0000; time: 0.14s
Val loss: 1.0275 score: 0.6047 time: 0.05s
Test loss: 0.9641 score: 0.5455 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.14s
Val loss: 1.0205 score: 0.6047 time: 0.06s
Test loss: 0.9643 score: 0.5455 time: 0.10s
Epoch 36/1000, LR 0.000270
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.13s
Val loss: 1.0144 score: 0.6047 time: 0.05s
Test loss: 0.9642 score: 0.5682 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.0572;  Loss pred: 0.0572; Loss self: 0.0000; time: 0.13s
Val loss: 1.0105 score: 0.6047 time: 0.05s
Test loss: 0.9642 score: 0.5682 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 0.13s
Val loss: 1.0093 score: 0.6047 time: 0.05s
Test loss: 0.9654 score: 0.5682 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.13s
Val loss: 1.0122 score: 0.6512 time: 0.05s
Test loss: 0.9665 score: 0.5682 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0482;  Loss pred: 0.0482; Loss self: 0.0000; time: 0.13s
Val loss: 1.0166 score: 0.6512 time: 0.05s
Test loss: 0.9680 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0439;  Loss pred: 0.0439; Loss self: 0.0000; time: 0.13s
Val loss: 1.0220 score: 0.6512 time: 0.05s
Test loss: 0.9689 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0424;  Loss pred: 0.0424; Loss self: 0.0000; time: 0.13s
Val loss: 1.0242 score: 0.6512 time: 0.05s
Test loss: 0.9662 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0408;  Loss pred: 0.0408; Loss self: 0.0000; time: 0.12s
Val loss: 1.0158 score: 0.6744 time: 0.05s
Test loss: 0.9611 score: 0.6136 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.12s
Val loss: 0.9982 score: 0.6744 time: 0.05s
Test loss: 0.9503 score: 0.6136 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.12s
Val loss: 0.9745 score: 0.6512 time: 0.05s
Test loss: 0.9366 score: 0.6136 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.13s
Val loss: 0.9496 score: 0.6512 time: 0.05s
Test loss: 0.9220 score: 0.5909 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.12s
Val loss: 0.9249 score: 0.6744 time: 0.05s
Test loss: 0.9061 score: 0.6136 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0307;  Loss pred: 0.0307; Loss self: 0.0000; time: 0.12s
Val loss: 0.9026 score: 0.6744 time: 0.05s
Test loss: 0.8872 score: 0.6136 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0301;  Loss pred: 0.0301; Loss self: 0.0000; time: 0.12s
Val loss: 0.8746 score: 0.6744 time: 0.05s
Test loss: 0.8622 score: 0.6364 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 0.0277;  Loss pred: 0.0277; Loss self: 0.0000; time: 0.13s
Val loss: 0.8488 score: 0.6744 time: 0.05s
Test loss: 0.8388 score: 0.5909 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 0.0273;  Loss pred: 0.0273; Loss self: 0.0000; time: 0.14s
Val loss: 0.8291 score: 0.6977 time: 0.05s
Test loss: 0.8249 score: 0.5909 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0248;  Loss pred: 0.0248; Loss self: 0.0000; time: 0.13s
Val loss: 0.8188 score: 0.7209 time: 0.05s
Test loss: 0.8163 score: 0.5909 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.13s
Val loss: 0.8279 score: 0.7209 time: 0.05s
Test loss: 0.8118 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.13s
Val loss: 0.8320 score: 0.6977 time: 0.05s
Test loss: 0.8081 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.13s
Val loss: 0.8281 score: 0.6977 time: 0.05s
Test loss: 0.7991 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0204;  Loss pred: 0.0204; Loss self: 0.0000; time: 0.12s
Val loss: 0.8030 score: 0.6977 time: 0.05s
Test loss: 0.7785 score: 0.6364 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0186;  Loss pred: 0.0186; Loss self: 0.0000; time: 0.12s
Val loss: 0.7744 score: 0.6977 time: 0.05s
Test loss: 0.7546 score: 0.6591 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.0190;  Loss pred: 0.0190; Loss self: 0.0000; time: 0.12s
Val loss: 0.7378 score: 0.6977 time: 0.05s
Test loss: 0.7285 score: 0.6591 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.12s
Val loss: 0.6981 score: 0.7442 time: 0.05s
Test loss: 0.7036 score: 0.6818 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.13s
Val loss: 0.6685 score: 0.7442 time: 0.05s
Test loss: 0.6838 score: 0.6818 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0160;  Loss pred: 0.0160; Loss self: 0.0000; time: 0.12s
Val loss: 0.6512 score: 0.7442 time: 0.05s
Test loss: 0.6691 score: 0.6818 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.12s
Val loss: 0.6459 score: 0.7209 time: 0.05s
Test loss: 0.6546 score: 0.6818 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.13s
Val loss: 0.6415 score: 0.7209 time: 0.05s
Test loss: 0.6392 score: 0.6818 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.13s
Val loss: 0.6325 score: 0.7442 time: 0.05s
Test loss: 0.6242 score: 0.7045 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.12s
Val loss: 0.6229 score: 0.7442 time: 0.05s
Test loss: 0.6108 score: 0.7045 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.13s
Val loss: 0.6133 score: 0.7442 time: 0.05s
Test loss: 0.5907 score: 0.7045 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.12s
Val loss: 0.5953 score: 0.7442 time: 0.05s
Test loss: 0.5677 score: 0.7045 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.13s
Val loss: 0.5779 score: 0.7442 time: 0.05s
Test loss: 0.5503 score: 0.7045 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.12s
Val loss: 0.5625 score: 0.7442 time: 0.05s
Test loss: 0.5418 score: 0.7273 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.12s
Val loss: 0.5495 score: 0.7674 time: 0.05s
Test loss: 0.5375 score: 0.7045 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.13s
Val loss: 0.5334 score: 0.7674 time: 0.05s
Test loss: 0.5395 score: 0.7045 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.13s
Val loss: 0.5254 score: 0.7674 time: 0.05s
Test loss: 0.5436 score: 0.6818 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.13s
Val loss: 0.5186 score: 0.7674 time: 0.05s
Test loss: 0.5456 score: 0.6818 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.13s
Val loss: 0.5074 score: 0.7907 time: 0.05s
Test loss: 0.5420 score: 0.6818 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0084;  Loss pred: 0.0084; Loss self: 0.0000; time: 0.13s
Val loss: 0.5042 score: 0.7907 time: 0.05s
Test loss: 0.5310 score: 0.7045 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.13s
Val loss: 0.5044 score: 0.7907 time: 0.05s
Test loss: 0.5218 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.13s
Val loss: 0.5064 score: 0.7907 time: 0.05s
Test loss: 0.5123 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.13s
Val loss: 0.5139 score: 0.8140 time: 0.05s
Test loss: 0.5044 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.13s
Val loss: 0.5307 score: 0.8140 time: 0.05s
Test loss: 0.5020 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.13s
Val loss: 0.5482 score: 0.8140 time: 0.05s
Test loss: 0.4989 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0064;  Loss pred: 0.0064; Loss self: 0.0000; time: 0.13s
Val loss: 0.5640 score: 0.8140 time: 0.05s
Test loss: 0.4951 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.13s
Val loss: 0.5788 score: 0.8140 time: 0.05s
Test loss: 0.4951 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.12s
Val loss: 0.5698 score: 0.8140 time: 0.05s
Test loss: 0.5115 score: 0.7273 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.13s
Val loss: 0.5745 score: 0.8140 time: 0.05s
Test loss: 0.5240 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.13s
Val loss: 0.5773 score: 0.7907 time: 0.05s
Test loss: 0.5363 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.13s
Val loss: 0.5802 score: 0.7907 time: 0.05s
Test loss: 0.5358 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.13s
Val loss: 0.5808 score: 0.8140 time: 0.05s
Test loss: 0.5274 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.13s
Val loss: 0.5683 score: 0.8372 time: 0.05s
Test loss: 0.5116 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.13s
Val loss: 0.5504 score: 0.8372 time: 0.05s
Test loss: 0.5041 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.13s
Val loss: 0.5308 score: 0.7907 time: 0.05s
Test loss: 0.4966 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.5103 score: 0.7907 time: 0.05s
Test loss: 0.4879 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.13s
Val loss: 0.4983 score: 0.7907 time: 0.05s
Test loss: 0.4845 score: 0.7727 time: 0.05s
Epoch 93/1000, LR 0.000265
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.13s
Val loss: 0.4947 score: 0.7907 time: 0.05s
Test loss: 0.4791 score: 0.7727 time: 0.05s
Epoch 94/1000, LR 0.000265
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.13s
Val loss: 0.4996 score: 0.7907 time: 0.05s
Test loss: 0.4780 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.13s
Val loss: 0.5034 score: 0.7907 time: 0.05s
Test loss: 0.4767 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.13s
Val loss: 0.5056 score: 0.7907 time: 0.05s
Test loss: 0.4781 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0044;  Loss pred: 0.0044; Loss self: 0.0000; time: 0.13s
Val loss: 0.5022 score: 0.7907 time: 0.05s
Test loss: 0.4884 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.13s
Val loss: 0.4965 score: 0.7907 time: 0.05s
Test loss: 0.5018 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.13s
Val loss: 0.4936 score: 0.7907 time: 0.05s
Test loss: 0.5186 score: 0.7955 time: 0.05s
Epoch 100/1000, LR 0.000265
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.13s
Val loss: 0.4932 score: 0.7907 time: 0.05s
Test loss: 0.5378 score: 0.7727 time: 0.05s
Epoch 101/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.13s
Val loss: 0.4941 score: 0.7907 time: 0.05s
Test loss: 0.5574 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.13s
Val loss: 0.4987 score: 0.7907 time: 0.05s
Test loss: 0.5795 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.13s
Val loss: 0.5007 score: 0.7907 time: 0.05s
Test loss: 0.6005 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.13s
Val loss: 0.5090 score: 0.8140 time: 0.05s
Test loss: 0.6183 score: 0.7500 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.13s
Val loss: 0.5140 score: 0.8140 time: 0.05s
Test loss: 0.6341 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.13s
Val loss: 0.5242 score: 0.8140 time: 0.05s
Test loss: 0.6555 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.13s
Val loss: 0.5286 score: 0.8372 time: 0.05s
Test loss: 0.6728 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.12s
Val loss: 0.5313 score: 0.8372 time: 0.05s
Test loss: 0.6868 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.13s
Val loss: 0.5357 score: 0.8372 time: 0.05s
Test loss: 0.7009 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.13s
Val loss: 0.5440 score: 0.8372 time: 0.05s
Test loss: 0.7224 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.13s
Val loss: 0.5465 score: 0.8372 time: 0.05s
Test loss: 0.7349 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.12s
Val loss: 0.5495 score: 0.8372 time: 0.05s
Test loss: 0.7430 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.5528 score: 0.8372 time: 0.05s
Test loss: 0.7522 score: 0.7273 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 114/1000, LR 0.000263
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.12s
Val loss: 0.5571 score: 0.8372 time: 0.05s
Test loss: 0.7637 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 115/1000, LR 0.000263
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.13s
Val loss: 0.5628 score: 0.8372 time: 0.05s
Test loss: 0.7691 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 116/1000, LR 0.000263
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.13s
Val loss: 0.5747 score: 0.8372 time: 0.05s
Test loss: 0.7779 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 117/1000, LR 0.000262
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.5874 score: 0.8372 time: 0.05s
Test loss: 0.7864 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 118/1000, LR 0.000262
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.5996 score: 0.8372 time: 0.05s
Test loss: 0.7983 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 119/1000, LR 0.000262
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.6137 score: 0.8372 time: 0.05s
Test loss: 0.8133 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 120/1000, LR 0.000262
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.12s
Val loss: 0.6261 score: 0.8372 time: 0.05s
Test loss: 0.8295 score: 0.7500 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 099,   Train_Loss: 0.0035,   Val_Loss: 0.4932,   Val_Precision: 0.8095,   Val_Recall: 0.7727,   Val_accuracy: 0.7907,   Val_Score: 0.7907,   Val_Loss: 0.4932,   Test_Precision: 0.7308,   Test_Recall: 0.8636,   Test_accuracy: 0.7917,   Test_Score: 0.7727,   Test_loss: 0.5378


[0.048780576908029616, 0.04871664091479033, 0.04879215406253934, 0.04903556895442307, 0.04866315796971321, 0.04854537209030241, 0.05110683396924287, 0.049236071994528174, 0.055589318042621017, 0.0499450369970873, 0.049111898988485336, 0.050849321065470576, 0.04983690904919058, 0.049495535087771714, 0.052554802037775517, 0.052524460945278406, 0.05200210807379335, 0.05266693397425115, 0.053621052065864205, 0.05322651704773307, 0.05330949102062732, 0.053375329938717186, 0.04804471705574542, 0.0479902719380334, 0.04915372608229518, 0.04924426192883402, 0.048587735975161195, 0.0492762359790504, 0.05167092499323189, 0.04904033802449703, 0.04969378293026239, 0.0502032550284639, 0.05501824303064495, 0.05447902996093035, 0.10931648104451597, 0.04994028201326728, 0.050110921962186694, 0.0500894810538739, 0.05064785899594426, 0.05009997601155192, 0.049800312030129135, 0.049888701061718166, 0.0500828200019896, 0.04940100002568215, 0.050849311985075474, 0.05024723301175982, 0.04992049699649215, 0.04976472200360149, 0.05024197499733418, 0.05039370502345264, 0.04983890301082283, 0.049953669076785445, 0.049931479967199266, 0.04950387997087091, 0.04967596393544227, 0.049600790021941066, 0.05041738704312593, 0.04942926694639027, 0.04951858799904585, 0.04964416695293039, 0.0493299700319767, 0.04957074404228479, 0.04966315801721066, 0.04936539102345705, 0.04958181595429778, 0.049508066033013165, 0.05017594003584236, 0.049640623037703335, 0.04958463506773114, 0.04995737399440259, 0.050089098047465086, 0.05156403000000864, 0.04975597595330328, 0.04979188402649015, 0.05006332090124488, 0.05003855796530843, 0.049960224074311554, 0.049703386961482465, 0.049703135970048606, 0.05016912904102355, 0.0501471150200814, 0.05005458800587803, 0.04945104196667671, 0.05006946297362447, 0.05018197896424681, 0.04998596804216504, 0.04989417491015047, 0.04964996897615492, 0.04992587200831622, 0.04987053503282368, 0.050076504005119205, 0.05012852395884693, 0.049879543017596006, 0.0507942580152303, 0.0533343399874866, 0.051047367975115776, 0.04981123108882457, 0.049746529082767665, 0.04996520408894867, 0.049791760044172406, 0.0506338820559904, 0.05006676400080323, 0.04989221994765103, 0.049409330007620156, 0.04995954898186028, 0.049611212918534875, 0.05008299509063363, 0.049592501018196344, 0.04972410399932414, 0.04970510501880199, 0.050007533049210906, 0.050038516987115145, 0.049550950061529875, 0.05008700001053512, 0.049891895963810384, 0.0507116480730474, 0.04995475499890745, 0.049806399969384074, 0.049904173938557506, 0.049332261085510254]
[0.0011086494751824912, 0.001107196384427053, 0.0011089125923304396, 0.0011144447489641607, 0.0011059808629480276, 0.0011033039111432365, 0.001161518953846429, 0.0011190016362392767, 0.0012633935918777504, 0.0011351144772065295, 0.001116179522465576, 0.001155666387851604, 0.0011326570238452405, 0.0011248985247220844, 0.0011944273190403526, 0.0011937377487563274, 0.0011818660925862125, 0.0011969757721420717, 0.0012186602742241864, 0.0012096935692666607, 0.0012115793413778936, 0.0012130756804253906, 0.0010919253876305777, 0.0010906879985916682, 0.0011171301382339814, 0.0011191877711098641, 0.001104266726708209, 0.0011199144540693271, 0.001174339204391634, 0.001114553136920387, 0.0011294041575059634, 0.001140983068828725, 0.0012504146143328398, 0.0012381597718393261, 0.0024844654782844536, 0.0011350064093924382, 0.0011388845900496976, 0.0011383972966789522, 0.0011510877044532788, 0.0011386358184443618, 0.0011318252734120258, 0.0011338341150390493, 0.0011382459091361272, 0.0011227500005836853, 0.001155666181478988, 0.0011419825684490868, 0.0011345567499202762, 0.001131016409172761, 0.0011418630681212314, 0.0011453114778057418, 0.0011327023411550645, 0.0011353106608360329, 0.0011348063628908924, 0.0011250881811561571, 0.0011289991803509606, 0.0011272906823168423, 0.0011458497055255893, 0.0011233924305997789, 0.0011254224545237694, 0.001128276521657509, 0.001121135682544925, 0.001126607819142836, 0.0011287081367547878, 0.0011219407050785694, 0.0011268594535067677, 0.0011251833189321173, 0.001140362273541872, 0.0011281959781296212, 0.0011269235242666168, 0.0011353948635091497, 0.0011383885919878428, 0.001171909772727469, 0.0011308176353023473, 0.0011316337278747762, 0.0011378027477555654, 0.0011372399537570097, 0.0011354596380525354, 0.0011296224309427832, 0.0011296167265920137, 0.0011402074782050806, 0.0011397071595473046, 0.0011376042728608643, 0.0011238873174244707, 0.001137942340309647, 0.0011404995219147002, 0.0011360447282310236, 0.001133958520685238, 0.0011284083858217027, 0.001134678909279914, 0.0011334212507459927, 0.0011381023637527092, 0.0011392846354283392, 0.0011336259776726365, 0.0011544149548915977, 0.0012121440906246955, 0.001160167453979904, 0.001132073433836922, 0.0011306029336992651, 0.0011355728202033788, 0.0011316309100948274, 0.0011507700467270545, 0.0011378810000182552, 0.0011339140897193415, 0.0011229393183550035, 0.001135444295042279, 0.001127527566330338, 0.0011382498884234917, 0.0011271022958680987, 0.0011300932727119123, 0.0011296614777000452, 0.0011365348420275207, 0.0011372390224344351, 0.0011261579559438608, 0.0011383409093303437, 0.001133906726450236, 0.0011525374562056227, 0.0011353353408842602, 0.00113196363566782, 0.0011341857713308525, 0.001121187751943415]
[901.9983523966339, 903.1821401019797, 901.7843308086584, 897.3078305851113, 904.1747768894191, 906.3685806785605, 860.9416115754714, 893.653742420596, 791.5189743156168, 880.9684133894224, 895.9132288962425, 865.3016220875045, 882.8797941014085, 888.9690741189819, 837.2213060259181, 837.7049322950796, 846.119544568501, 835.4387977380948, 820.5732320573195, 826.6556303231562, 825.3689757228192, 824.350876154181, 915.8134899399572, 916.8524832869092, 895.1508564444006, 893.5051166689667, 905.5783134759249, 892.925344758606, 851.5427197357766, 897.220569279534, 885.4226304676233, 876.437194661047, 799.7347348131813, 807.6502102103252, 402.5010646114951, 881.052293383342, 878.0520947749086, 878.427946831305, 868.743533730091, 878.2439334872056, 883.5286006517398, 881.9632314252249, 878.5447783941089, 890.6702288845504, 865.301776608388, 875.6701088336999, 881.4014813011943, 884.1604700778938, 875.761751052474, 873.124926605871, 882.8444717261358, 880.8161805365315, 881.2076074833803, 888.8192203498088, 885.7402356033067, 887.0826448638512, 872.714802977857, 890.1608847997136, 888.5552229568383, 886.3075503254622, 891.9527007917965, 887.6203262647655, 885.968628590874, 891.312700816903, 887.4221154093501, 888.7440678991529, 876.9143132857964, 886.3708250917975, 887.3716614007001, 880.7508578199072, 878.4346637327153, 853.3080133572304, 884.3158868252259, 883.6781507723474, 878.8869617098431, 879.3219027315907, 880.7006136432408, 885.2515430004339, 885.2560133532551, 877.0333637648159, 877.4183715729251, 879.0402988599761, 889.7689158835121, 878.7791477447695, 876.808784909593, 880.247031784687, 881.8664719726358, 886.2039777130899, 881.3065897511186, 882.2844986732181, 878.6555865701404, 877.7437778962304, 882.1251627040392, 866.2396443867123, 824.9844286124729, 861.9445378936838, 883.3349234339973, 884.4838184950219, 880.6128345172104, 883.6803511457662, 868.9833410629127, 878.826520509576, 881.901026776652, 890.520069655146, 880.7125143578834, 886.8962762964718, 878.5417070279957, 887.2309138806216, 884.8827120263053, 885.2209442743575, 879.8674383057634, 879.3226228373223, 887.9749014976104, 878.4714594754167, 881.9067535921238, 867.6507601689531, 880.7970332546384, 883.4206051239748, 881.6897771752162, 891.9112773633553]
Elapsed: 0.0506723517649031~0.005525051406478992
Time per graph: 0.0011516443582932523~0.00012556935014724983
Speed: 873.4374966059512~48.108935898921914
Total Time: 0.0499
best val loss: 0.49318933486938477 test_score: 0.7727

Testing...
Test loss: 0.5116 score: 0.7273 time: 0.05s
test Score 0.7273
Epoch Time List: [0.214531535981223, 0.212832995923236, 0.2126648281700909, 0.21185074106324464, 0.21194179111626, 0.21217657602392137, 0.2269525210140273, 0.2207354559795931, 0.2322369300527498, 0.22783999401144683, 0.21460950304754078, 0.21712373895570636, 0.21771800506394356, 0.2160906509961933, 0.3209781660698354, 0.23115364089608192, 0.23115328303538263, 0.2325881840661168, 0.23477065784391016, 0.23623829998541623, 0.2346863349666819, 0.3108152929926291, 0.22269384993705899, 0.21088385407347232, 0.21324257692322135, 0.21357741602696478, 0.21342324616853148, 0.21416953403968364, 0.31626988807693124, 0.22279651800636202, 0.2187746912240982, 0.22217686800286174, 0.23385163384955376, 0.24437908094841987, 0.3003551500150934, 0.21977241907734424, 0.22037961101159453, 0.22077110304962844, 0.22184824291616678, 0.2252388729248196, 0.21911599289160222, 0.2204102820251137, 0.21827815694268793, 0.2174042658880353, 0.2184164640493691, 0.22063102095853537, 0.21873964206315577, 0.2184863950824365, 0.21852185297757387, 0.22231229790486395, 0.23211363703012466, 0.21918448095675558, 0.21957972494419664, 0.21817811497021466, 0.2192366721574217, 0.2176317070843652, 0.21858846594113857, 0.21811859612353146, 0.21817104890942574, 0.2191662269178778, 0.21770129713695496, 0.2179600519593805, 0.21874859102535993, 0.21840309700928628, 0.21689834515564144, 0.21803944907151163, 0.21825957496184856, 0.21868057700339705, 0.21736592811066657, 0.21878843486774713, 0.22021179413422942, 0.22239815699867904, 0.22984811279457062, 0.22111741488333791, 0.21890791889745742, 0.2189196809194982, 0.219581050099805, 0.21896657010074705, 0.21838076401036233, 0.21899972099345177, 0.218926944071427, 0.22017264901660383, 0.21825834386982024, 0.21857412299141288, 0.21871946891769767, 0.21886095497757196, 0.22037637291941792, 0.22126975806895643, 0.22026909212581813, 0.2191331370268017, 0.21986144012771547, 0.22030817810446024, 0.22006625006906688, 0.22110882797278464, 0.2277552931336686, 0.22027957206591964, 0.22214685298968107, 0.21950416406616569, 0.2201563878916204, 0.2196123821195215, 0.2201759790768847, 0.22179190895985812, 0.21982307895086706, 0.21851181297097355, 0.21897952107246965, 0.21952412102837116, 0.21956160804256797, 0.2185752559453249, 0.2185803969623521, 0.21882079902570695, 0.21904479898512363, 0.2187215780140832, 0.2183542789425701, 0.21849072386976331, 0.21999394590966403, 0.2218747790902853, 0.22236309596337378, 0.22837858297862113, 0.2190575290005654, 0.21788158197887242]
Total Epoch List: [120]
Total Time List: [0.049916084040887654]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7232382fbe50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7979;  Loss pred: 0.7979; Loss self: 0.0000; time: 0.14s
Val loss: 0.8405 score: 0.4773 time: 0.05s
Test loss: 0.8179 score: 0.3953 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7883;  Loss pred: 0.7883; Loss self: 0.0000; time: 0.14s
Val loss: 0.7576 score: 0.5455 time: 0.05s
Test loss: 0.7432 score: 0.4884 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7358;  Loss pred: 0.7358; Loss self: 0.0000; time: 0.14s
Val loss: 0.7237 score: 0.5227 time: 0.05s
Test loss: 0.7164 score: 0.4651 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.14s
Val loss: 0.7119 score: 0.5000 time: 0.05s
Test loss: 0.7121 score: 0.4419 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6297;  Loss pred: 0.6297; Loss self: 0.0000; time: 0.14s
Val loss: 0.7006 score: 0.5000 time: 0.05s
Test loss: 0.7037 score: 0.4419 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5564;  Loss pred: 0.5564; Loss self: 0.0000; time: 0.14s
Val loss: 0.6952 score: 0.5000 time: 0.05s
Test loss: 0.6964 score: 0.4419 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4896;  Loss pred: 0.4896; Loss self: 0.0000; time: 0.14s
Val loss: 0.6951 score: 0.5227 time: 0.05s
Test loss: 0.6936 score: 0.4884 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.4438;  Loss pred: 0.4438; Loss self: 0.0000; time: 0.14s
Val loss: 0.6946 score: 0.5455 time: 0.05s
Test loss: 0.6955 score: 0.5349 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3962;  Loss pred: 0.3962; Loss self: 0.0000; time: 0.14s
Val loss: 0.6984 score: 0.5227 time: 0.05s
Test loss: 0.6922 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3583;  Loss pred: 0.3583; Loss self: 0.0000; time: 0.14s
Val loss: 0.6987 score: 0.5000 time: 0.05s
Test loss: 0.6875 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3205;  Loss pred: 0.3205; Loss self: 0.0000; time: 0.14s
Val loss: 0.7002 score: 0.5000 time: 0.05s
Test loss: 0.6837 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2878;  Loss pred: 0.2878; Loss self: 0.0000; time: 0.14s
Val loss: 0.7032 score: 0.5000 time: 0.05s
Test loss: 0.6855 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2623;  Loss pred: 0.2623; Loss self: 0.0000; time: 0.14s
Val loss: 0.7091 score: 0.5000 time: 0.05s
Test loss: 0.6888 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.2310;  Loss pred: 0.2310; Loss self: 0.0000; time: 0.14s
Val loss: 0.7162 score: 0.5000 time: 0.05s
Test loss: 0.6929 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2097;  Loss pred: 0.2097; Loss self: 0.0000; time: 0.15s
Val loss: 0.7243 score: 0.5000 time: 0.05s
Test loss: 0.6992 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1850;  Loss pred: 0.1850; Loss self: 0.0000; time: 0.24s
Val loss: 0.7346 score: 0.5000 time: 0.05s
Test loss: 0.7080 score: 0.4651 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1718;  Loss pred: 0.1718; Loss self: 0.0000; time: 0.14s
Val loss: 0.7459 score: 0.4773 time: 0.05s
Test loss: 0.7183 score: 0.4651 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1470;  Loss pred: 0.1470; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7584 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7305 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7707 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7439 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1194;  Loss pred: 0.1194; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7827 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7600 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1061;  Loss pred: 0.1061; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7938 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7759 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0957;  Loss pred: 0.0957; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8033 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7910 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8148 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8058 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0787;  Loss pred: 0.0787; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8266 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8198 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0656;  Loss pred: 0.0656; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8389 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8335 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0655;  Loss pred: 0.0655; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8500 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8458 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8604 score: 0.5000 time: 0.05s
Test loss: 0.8568 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0519;  Loss pred: 0.0519; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8684 score: 0.5000 time: 0.05s
Test loss: 0.8655 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 007,   Train_Loss: 0.4438,   Val_Loss: 0.6946,   Val_Precision: 0.5263,   Val_Recall: 0.9091,   Val_accuracy: 0.6667,   Val_Score: 0.5455,   Val_Loss: 0.6946,   Test_Precision: 0.5294,   Test_Recall: 0.8182,   Test_accuracy: 0.6429,   Test_Score: 0.5349,   Test_loss: 0.6955


[0.048780576908029616, 0.04871664091479033, 0.04879215406253934, 0.04903556895442307, 0.04866315796971321, 0.04854537209030241, 0.05110683396924287, 0.049236071994528174, 0.055589318042621017, 0.0499450369970873, 0.049111898988485336, 0.050849321065470576, 0.04983690904919058, 0.049495535087771714, 0.052554802037775517, 0.052524460945278406, 0.05200210807379335, 0.05266693397425115, 0.053621052065864205, 0.05322651704773307, 0.05330949102062732, 0.053375329938717186, 0.04804471705574542, 0.0479902719380334, 0.04915372608229518, 0.04924426192883402, 0.048587735975161195, 0.0492762359790504, 0.05167092499323189, 0.04904033802449703, 0.04969378293026239, 0.0502032550284639, 0.05501824303064495, 0.05447902996093035, 0.10931648104451597, 0.04994028201326728, 0.050110921962186694, 0.0500894810538739, 0.05064785899594426, 0.05009997601155192, 0.049800312030129135, 0.049888701061718166, 0.0500828200019896, 0.04940100002568215, 0.050849311985075474, 0.05024723301175982, 0.04992049699649215, 0.04976472200360149, 0.05024197499733418, 0.05039370502345264, 0.04983890301082283, 0.049953669076785445, 0.049931479967199266, 0.04950387997087091, 0.04967596393544227, 0.049600790021941066, 0.05041738704312593, 0.04942926694639027, 0.04951858799904585, 0.04964416695293039, 0.0493299700319767, 0.04957074404228479, 0.04966315801721066, 0.04936539102345705, 0.04958181595429778, 0.049508066033013165, 0.05017594003584236, 0.049640623037703335, 0.04958463506773114, 0.04995737399440259, 0.050089098047465086, 0.05156403000000864, 0.04975597595330328, 0.04979188402649015, 0.05006332090124488, 0.05003855796530843, 0.049960224074311554, 0.049703386961482465, 0.049703135970048606, 0.05016912904102355, 0.0501471150200814, 0.05005458800587803, 0.04945104196667671, 0.05006946297362447, 0.05018197896424681, 0.04998596804216504, 0.04989417491015047, 0.04964996897615492, 0.04992587200831622, 0.04987053503282368, 0.050076504005119205, 0.05012852395884693, 0.049879543017596006, 0.0507942580152303, 0.0533343399874866, 0.051047367975115776, 0.04981123108882457, 0.049746529082767665, 0.04996520408894867, 0.049791760044172406, 0.0506338820559904, 0.05006676400080323, 0.04989221994765103, 0.049409330007620156, 0.04995954898186028, 0.049611212918534875, 0.05008299509063363, 0.049592501018196344, 0.04972410399932414, 0.04970510501880199, 0.050007533049210906, 0.050038516987115145, 0.049550950061529875, 0.05008700001053512, 0.049891895963810384, 0.0507116480730474, 0.04995475499890745, 0.049806399969384074, 0.049904173938557506, 0.049332261085510254, 0.04829074000008404, 0.04831506102345884, 0.04872284294106066, 0.04841685690917075, 0.048769342014566064, 0.048339304979890585, 0.04876797692850232, 0.04861664597410709, 0.04847087897360325, 0.04883138998411596, 0.048369549913331866, 0.04847744002472609, 0.05113254301249981, 0.051083847996778786, 0.050246250932104886, 0.0488781169988215, 0.04862667003180832, 0.048782840953208506, 0.04833956004586071, 0.048193333903327584, 0.0484591550193727, 0.048607649048790336, 0.04843555006664246, 0.04864162695594132, 0.048674904042854905, 0.04800173197872937, 0.04856814397498965, 0.04875177110079676]
[0.0011086494751824912, 0.001107196384427053, 0.0011089125923304396, 0.0011144447489641607, 0.0011059808629480276, 0.0011033039111432365, 0.001161518953846429, 0.0011190016362392767, 0.0012633935918777504, 0.0011351144772065295, 0.001116179522465576, 0.001155666387851604, 0.0011326570238452405, 0.0011248985247220844, 0.0011944273190403526, 0.0011937377487563274, 0.0011818660925862125, 0.0011969757721420717, 0.0012186602742241864, 0.0012096935692666607, 0.0012115793413778936, 0.0012130756804253906, 0.0010919253876305777, 0.0010906879985916682, 0.0011171301382339814, 0.0011191877711098641, 0.001104266726708209, 0.0011199144540693271, 0.001174339204391634, 0.001114553136920387, 0.0011294041575059634, 0.001140983068828725, 0.0012504146143328398, 0.0012381597718393261, 0.0024844654782844536, 0.0011350064093924382, 0.0011388845900496976, 0.0011383972966789522, 0.0011510877044532788, 0.0011386358184443618, 0.0011318252734120258, 0.0011338341150390493, 0.0011382459091361272, 0.0011227500005836853, 0.001155666181478988, 0.0011419825684490868, 0.0011345567499202762, 0.001131016409172761, 0.0011418630681212314, 0.0011453114778057418, 0.0011327023411550645, 0.0011353106608360329, 0.0011348063628908924, 0.0011250881811561571, 0.0011289991803509606, 0.0011272906823168423, 0.0011458497055255893, 0.0011233924305997789, 0.0011254224545237694, 0.001128276521657509, 0.001121135682544925, 0.001126607819142836, 0.0011287081367547878, 0.0011219407050785694, 0.0011268594535067677, 0.0011251833189321173, 0.001140362273541872, 0.0011281959781296212, 0.0011269235242666168, 0.0011353948635091497, 0.0011383885919878428, 0.001171909772727469, 0.0011308176353023473, 0.0011316337278747762, 0.0011378027477555654, 0.0011372399537570097, 0.0011354596380525354, 0.0011296224309427832, 0.0011296167265920137, 0.0011402074782050806, 0.0011397071595473046, 0.0011376042728608643, 0.0011238873174244707, 0.001137942340309647, 0.0011404995219147002, 0.0011360447282310236, 0.001133958520685238, 0.0011284083858217027, 0.001134678909279914, 0.0011334212507459927, 0.0011381023637527092, 0.0011392846354283392, 0.0011336259776726365, 0.0011544149548915977, 0.0012121440906246955, 0.001160167453979904, 0.001132073433836922, 0.0011306029336992651, 0.0011355728202033788, 0.0011316309100948274, 0.0011507700467270545, 0.0011378810000182552, 0.0011339140897193415, 0.0011229393183550035, 0.001135444295042279, 0.001127527566330338, 0.0011382498884234917, 0.0011271022958680987, 0.0011300932727119123, 0.0011296614777000452, 0.0011365348420275207, 0.0011372390224344351, 0.0011261579559438608, 0.0011383409093303437, 0.001133906726450236, 0.0011525374562056227, 0.0011353353408842602, 0.00113196363566782, 0.0011341857713308525, 0.001121187751943415, 0.0011230404651182335, 0.0011236060703129962, 0.001133089370722341, 0.001125973416492343, 0.0011341707445247922, 0.0011241698832532695, 0.0011341389983372634, 0.0011306196738164439, 0.0011272297435721687, 0.0011356137205608362, 0.0011248732537984154, 0.0011273823261564208, 0.0011891289072674374, 0.0011879964650413672, 0.0011685174635373229, 0.0011367003953214301, 0.0011308527914374027, 0.0011344846733304303, 0.0011241758150200164, 0.0011207752070541298, 0.0011269570934737837, 0.0011304104429951242, 0.0011264081410847086, 0.0011312006268823563, 0.0011319745126245327, 0.0011163193483425433, 0.0011294917203485966, 0.0011337621186231805]
[901.9983523966339, 903.1821401019797, 901.7843308086584, 897.3078305851113, 904.1747768894191, 906.3685806785605, 860.9416115754714, 893.653742420596, 791.5189743156168, 880.9684133894224, 895.9132288962425, 865.3016220875045, 882.8797941014085, 888.9690741189819, 837.2213060259181, 837.7049322950796, 846.119544568501, 835.4387977380948, 820.5732320573195, 826.6556303231562, 825.3689757228192, 824.350876154181, 915.8134899399572, 916.8524832869092, 895.1508564444006, 893.5051166689667, 905.5783134759249, 892.925344758606, 851.5427197357766, 897.220569279534, 885.4226304676233, 876.437194661047, 799.7347348131813, 807.6502102103252, 402.5010646114951, 881.052293383342, 878.0520947749086, 878.427946831305, 868.743533730091, 878.2439334872056, 883.5286006517398, 881.9632314252249, 878.5447783941089, 890.6702288845504, 865.301776608388, 875.6701088336999, 881.4014813011943, 884.1604700778938, 875.761751052474, 873.124926605871, 882.8444717261358, 880.8161805365315, 881.2076074833803, 888.8192203498088, 885.7402356033067, 887.0826448638512, 872.714802977857, 890.1608847997136, 888.5552229568383, 886.3075503254622, 891.9527007917965, 887.6203262647655, 885.968628590874, 891.312700816903, 887.4221154093501, 888.7440678991529, 876.9143132857964, 886.3708250917975, 887.3716614007001, 880.7508578199072, 878.4346637327153, 853.3080133572304, 884.3158868252259, 883.6781507723474, 878.8869617098431, 879.3219027315907, 880.7006136432408, 885.2515430004339, 885.2560133532551, 877.0333637648159, 877.4183715729251, 879.0402988599761, 889.7689158835121, 878.7791477447695, 876.808784909593, 880.247031784687, 881.8664719726358, 886.2039777130899, 881.3065897511186, 882.2844986732181, 878.6555865701404, 877.7437778962304, 882.1251627040392, 866.2396443867123, 824.9844286124729, 861.9445378936838, 883.3349234339973, 884.4838184950219, 880.6128345172104, 883.6803511457662, 868.9833410629127, 878.826520509576, 881.901026776652, 890.520069655146, 880.7125143578834, 886.8962762964718, 878.5417070279957, 887.2309138806216, 884.8827120263053, 885.2209442743575, 879.8674383057634, 879.3226228373223, 887.9749014976104, 878.4714594754167, 881.9067535921238, 867.6507601689531, 880.7970332546384, 883.4206051239748, 881.6897771752162, 891.9112773633553, 890.439864866953, 889.9916317837585, 882.5429183600081, 888.1204345971346, 881.7014588213448, 889.5452679323428, 881.7261389177855, 884.4707227006471, 887.1306011062305, 880.5811182927044, 888.9890453197729, 887.0105347573572, 840.9517201107769, 841.7533464337195, 855.7852417308435, 879.739291123608, 884.2883950694603, 881.4574788960059, 889.5405742047516, 892.2395799853763, 887.3452288387969, 884.6344318532746, 887.7776744733219, 884.0164832263649, 883.4121164808349, 895.801010243844, 885.3539888644501, 882.0192380517893]
Elapsed: 0.05031414822646971~0.005040507858420168
Time per graph: 0.0011483809890827036~0.00011352303931001506
Speed: 875.0058454713338~43.81115635483132
Total Time: 0.0494
best val loss: 0.6945876479148865 test_score: 0.5349

Testing...
Test loss: 0.7432 score: 0.4884 time: 0.04s
test Score 0.4884
Epoch Time List: [0.214531535981223, 0.212832995923236, 0.2126648281700909, 0.21185074106324464, 0.21194179111626, 0.21217657602392137, 0.2269525210140273, 0.2207354559795931, 0.2322369300527498, 0.22783999401144683, 0.21460950304754078, 0.21712373895570636, 0.21771800506394356, 0.2160906509961933, 0.3209781660698354, 0.23115364089608192, 0.23115328303538263, 0.2325881840661168, 0.23477065784391016, 0.23623829998541623, 0.2346863349666819, 0.3108152929926291, 0.22269384993705899, 0.21088385407347232, 0.21324257692322135, 0.21357741602696478, 0.21342324616853148, 0.21416953403968364, 0.31626988807693124, 0.22279651800636202, 0.2187746912240982, 0.22217686800286174, 0.23385163384955376, 0.24437908094841987, 0.3003551500150934, 0.21977241907734424, 0.22037961101159453, 0.22077110304962844, 0.22184824291616678, 0.2252388729248196, 0.21911599289160222, 0.2204102820251137, 0.21827815694268793, 0.2174042658880353, 0.2184164640493691, 0.22063102095853537, 0.21873964206315577, 0.2184863950824365, 0.21852185297757387, 0.22231229790486395, 0.23211363703012466, 0.21918448095675558, 0.21957972494419664, 0.21817811497021466, 0.2192366721574217, 0.2176317070843652, 0.21858846594113857, 0.21811859612353146, 0.21817104890942574, 0.2191662269178778, 0.21770129713695496, 0.2179600519593805, 0.21874859102535993, 0.21840309700928628, 0.21689834515564144, 0.21803944907151163, 0.21825957496184856, 0.21868057700339705, 0.21736592811066657, 0.21878843486774713, 0.22021179413422942, 0.22239815699867904, 0.22984811279457062, 0.22111741488333791, 0.21890791889745742, 0.2189196809194982, 0.219581050099805, 0.21896657010074705, 0.21838076401036233, 0.21899972099345177, 0.218926944071427, 0.22017264901660383, 0.21825834386982024, 0.21857412299141288, 0.21871946891769767, 0.21886095497757196, 0.22037637291941792, 0.22126975806895643, 0.22026909212581813, 0.2191331370268017, 0.21986144012771547, 0.22030817810446024, 0.22006625006906688, 0.22110882797278464, 0.2277552931336686, 0.22027957206591964, 0.22214685298968107, 0.21950416406616569, 0.2201563878916204, 0.2196123821195215, 0.2201759790768847, 0.22179190895985812, 0.21982307895086706, 0.21851181297097355, 0.21897952107246965, 0.21952412102837116, 0.21956160804256797, 0.2185752559453249, 0.2185803969623521, 0.21882079902570695, 0.21904479898512363, 0.2187215780140832, 0.2183542789425701, 0.21849072386976331, 0.21999394590966403, 0.2218747790902853, 0.22236309596337378, 0.22837858297862113, 0.2190575290005654, 0.21788158197887242, 0.22677540685981512, 0.2254800561349839, 0.2262055620085448, 0.22598331607878208, 0.22602262801956385, 0.22541887999977916, 0.22506065107882023, 0.22523827012628317, 0.22494681493844837, 0.226086272043176, 0.2252407429041341, 0.22623500286135823, 0.23704679403454065, 0.23990674782544374, 0.2422623458551243, 0.3286102389683947, 0.2283573701279238, 0.22752144001424313, 0.22613897500559688, 0.22401471121702343, 0.22378816397394985, 0.32009095104876906, 0.22588480007834733, 0.2266131518408656, 0.22520914406049997, 0.22372444905340672, 0.2243928329553455, 0.22618340817280114]
Total Epoch List: [120, 28]
Total Time List: [0.049916084040887654, 0.049372291075997055]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7232382fbeb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8399;  Loss pred: 0.8399; Loss self: 0.0000; time: 0.14s
Val loss: 0.7470 score: 0.5000 time: 0.04s
Test loss: 0.7054 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8364;  Loss pred: 0.8364; Loss self: 0.0000; time: 0.14s
Val loss: 0.7155 score: 0.4091 time: 0.04s
Test loss: 0.7001 score: 0.4186 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.8028;  Loss pred: 0.8028; Loss self: 0.0000; time: 0.16s
Val loss: 0.7124 score: 0.4773 time: 0.04s
Test loss: 0.7076 score: 0.5581 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.7358;  Loss pred: 0.7358; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7142 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7180 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7184 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7352 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5749;  Loss pred: 0.5749; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7253 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7533 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.5014;  Loss pred: 0.5014; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7290 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7740 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4264;  Loss pred: 0.4264; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7312 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7913 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3651;  Loss pred: 0.3651; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7323 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8042 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3188;  Loss pred: 0.3188; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7320 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8119 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2779;  Loss pred: 0.2779; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7311 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8138 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2484;  Loss pred: 0.2484; Loss self: 0.0000; time: 0.17s
Val loss: 0.7254 score: 0.5455 time: 0.05s
Test loss: 0.8134 score: 0.5349 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2208;  Loss pred: 0.2208; Loss self: 0.0000; time: 0.16s
Val loss: 0.7160 score: 0.5455 time: 0.12s
Test loss: 0.8111 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.2006;  Loss pred: 0.2006; Loss self: 0.0000; time: 0.16s
Val loss: 0.7059 score: 0.6591 time: 0.05s
Test loss: 0.8084 score: 0.5581 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.1764;  Loss pred: 0.1764; Loss self: 0.0000; time: 0.16s
Val loss: 0.6982 score: 0.6136 time: 0.05s
Test loss: 0.8036 score: 0.5116 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.1545;  Loss pred: 0.1545; Loss self: 0.0000; time: 0.15s
Val loss: 0.6924 score: 0.6136 time: 0.05s
Test loss: 0.7958 score: 0.4884 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.1430;  Loss pred: 0.1430; Loss self: 0.0000; time: 0.17s
Val loss: 0.6877 score: 0.6136 time: 0.06s
Test loss: 0.7877 score: 0.4651 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.1249;  Loss pred: 0.1249; Loss self: 0.0000; time: 0.21s
Val loss: 0.6840 score: 0.6364 time: 0.05s
Test loss: 0.7801 score: 0.5349 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 0.27s
Val loss: 0.6828 score: 0.6818 time: 0.06s
Test loss: 0.7730 score: 0.5349 time: 0.19s
Epoch 20/1000, LR 0.000270
Train loss: 0.1020;  Loss pred: 0.1020; Loss self: 0.0000; time: 0.15s
Val loss: 0.6852 score: 0.6364 time: 0.05s
Test loss: 0.7658 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0952;  Loss pred: 0.0952; Loss self: 0.0000; time: 0.16s
Val loss: 0.6906 score: 0.6364 time: 0.05s
Test loss: 0.7583 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0821;  Loss pred: 0.0821; Loss self: 0.0000; time: 0.15s
Val loss: 0.6970 score: 0.5909 time: 0.04s
Test loss: 0.7509 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.15s
Val loss: 0.7070 score: 0.5682 time: 0.07s
Test loss: 0.7463 score: 0.5349 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0698;  Loss pred: 0.0698; Loss self: 0.0000; time: 0.18s
Val loss: 0.7166 score: 0.5227 time: 0.05s
Test loss: 0.7450 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.21s
Val loss: 0.7254 score: 0.5227 time: 0.06s
Test loss: 0.7454 score: 0.5814 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0571;  Loss pred: 0.0571; Loss self: 0.0000; time: 0.15s
Val loss: 0.7330 score: 0.5227 time: 0.06s
Test loss: 0.7468 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0536;  Loss pred: 0.0536; Loss self: 0.0000; time: 0.15s
Val loss: 0.7386 score: 0.5682 time: 0.05s
Test loss: 0.7484 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.16s
Val loss: 0.7419 score: 0.5682 time: 0.05s
Test loss: 0.7506 score: 0.5349 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.20s
Val loss: 0.7436 score: 0.5682 time: 0.06s
Test loss: 0.7519 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0399;  Loss pred: 0.0399; Loss self: 0.0000; time: 0.21s
Val loss: 0.7422 score: 0.5682 time: 0.05s
Test loss: 0.7521 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.16s
Val loss: 0.7407 score: 0.5682 time: 0.04s
Test loss: 0.7522 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0323;  Loss pred: 0.0323; Loss self: 0.0000; time: 0.27s
Val loss: 0.7374 score: 0.5682 time: 0.05s
Test loss: 0.7526 score: 0.5581 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0304;  Loss pred: 0.0304; Loss self: 0.0000; time: 0.15s
Val loss: 0.7336 score: 0.5682 time: 0.05s
Test loss: 0.7533 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.16s
Val loss: 0.7301 score: 0.5455 time: 0.04s
Test loss: 0.7541 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0247;  Loss pred: 0.0247; Loss self: 0.0000; time: 0.18s
Val loss: 0.7279 score: 0.5455 time: 0.05s
Test loss: 0.7545 score: 0.5581 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.16s
Val loss: 0.7264 score: 0.5455 time: 0.05s
Test loss: 0.7542 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.17s
Val loss: 0.7248 score: 0.5682 time: 0.06s
Test loss: 0.7535 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0177;  Loss pred: 0.0177; Loss self: 0.0000; time: 0.20s
Val loss: 0.7230 score: 0.5682 time: 0.10s
Test loss: 0.7519 score: 0.5581 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.16s
Val loss: 0.7207 score: 0.5682 time: 0.04s
Test loss: 0.7491 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 018,   Train_Loss: 0.1154,   Val_Loss: 0.6828,   Val_Precision: 0.6818,   Val_Recall: 0.6818,   Val_accuracy: 0.6818,   Val_Score: 0.6818,   Val_Loss: 0.6828,   Test_Precision: 0.5333,   Test_Recall: 0.3810,   Test_accuracy: 0.4444,   Test_Score: 0.5349,   Test_loss: 0.7730


[0.048780576908029616, 0.04871664091479033, 0.04879215406253934, 0.04903556895442307, 0.04866315796971321, 0.04854537209030241, 0.05110683396924287, 0.049236071994528174, 0.055589318042621017, 0.0499450369970873, 0.049111898988485336, 0.050849321065470576, 0.04983690904919058, 0.049495535087771714, 0.052554802037775517, 0.052524460945278406, 0.05200210807379335, 0.05266693397425115, 0.053621052065864205, 0.05322651704773307, 0.05330949102062732, 0.053375329938717186, 0.04804471705574542, 0.0479902719380334, 0.04915372608229518, 0.04924426192883402, 0.048587735975161195, 0.0492762359790504, 0.05167092499323189, 0.04904033802449703, 0.04969378293026239, 0.0502032550284639, 0.05501824303064495, 0.05447902996093035, 0.10931648104451597, 0.04994028201326728, 0.050110921962186694, 0.0500894810538739, 0.05064785899594426, 0.05009997601155192, 0.049800312030129135, 0.049888701061718166, 0.0500828200019896, 0.04940100002568215, 0.050849311985075474, 0.05024723301175982, 0.04992049699649215, 0.04976472200360149, 0.05024197499733418, 0.05039370502345264, 0.04983890301082283, 0.049953669076785445, 0.049931479967199266, 0.04950387997087091, 0.04967596393544227, 0.049600790021941066, 0.05041738704312593, 0.04942926694639027, 0.04951858799904585, 0.04964416695293039, 0.0493299700319767, 0.04957074404228479, 0.04966315801721066, 0.04936539102345705, 0.04958181595429778, 0.049508066033013165, 0.05017594003584236, 0.049640623037703335, 0.04958463506773114, 0.04995737399440259, 0.050089098047465086, 0.05156403000000864, 0.04975597595330328, 0.04979188402649015, 0.05006332090124488, 0.05003855796530843, 0.049960224074311554, 0.049703386961482465, 0.049703135970048606, 0.05016912904102355, 0.0501471150200814, 0.05005458800587803, 0.04945104196667671, 0.05006946297362447, 0.05018197896424681, 0.04998596804216504, 0.04989417491015047, 0.04964996897615492, 0.04992587200831622, 0.04987053503282368, 0.050076504005119205, 0.05012852395884693, 0.049879543017596006, 0.0507942580152303, 0.0533343399874866, 0.051047367975115776, 0.04981123108882457, 0.049746529082767665, 0.04996520408894867, 0.049791760044172406, 0.0506338820559904, 0.05006676400080323, 0.04989221994765103, 0.049409330007620156, 0.04995954898186028, 0.049611212918534875, 0.05008299509063363, 0.049592501018196344, 0.04972410399932414, 0.04970510501880199, 0.050007533049210906, 0.050038516987115145, 0.049550950061529875, 0.05008700001053512, 0.049891895963810384, 0.0507116480730474, 0.04995475499890745, 0.049806399969384074, 0.049904173938557506, 0.049332261085510254, 0.04829074000008404, 0.04831506102345884, 0.04872284294106066, 0.04841685690917075, 0.048769342014566064, 0.048339304979890585, 0.04876797692850232, 0.04861664597410709, 0.04847087897360325, 0.04883138998411596, 0.048369549913331866, 0.04847744002472609, 0.05113254301249981, 0.051083847996778786, 0.050246250932104886, 0.0488781169988215, 0.04862667003180832, 0.048782840953208506, 0.04833956004586071, 0.048193333903327584, 0.0484591550193727, 0.048607649048790336, 0.04843555006664246, 0.04864162695594132, 0.048674904042854905, 0.04800173197872937, 0.04856814397498965, 0.04875177110079676, 0.052499509998597205, 0.05695091304369271, 0.052418419043533504, 0.05190910503733903, 0.05193510593380779, 0.12596995290368795, 0.06429724500048906, 0.056794675067067146, 0.062115462962538004, 0.06924917199648917, 0.05654406303074211, 0.06311433797236532, 0.06409551005344838, 0.056326815974898636, 0.058851519017480314, 0.05588500096928328, 0.0587649189401418, 0.06172892404720187, 0.1943750149803236, 0.05885997600853443, 0.05297313700430095, 0.056603141012601554, 0.06588742102030665, 0.05587073205970228, 0.14474297605920583, 0.06496790808159858, 0.0647672259947285, 0.0677546450169757, 0.08345634199213237, 0.05814511305652559, 0.052864020923152566, 0.0748402209719643, 0.06464186089579016, 0.05677867692429572, 0.1535509870154783, 0.055856833001598716, 0.061070877010934055, 0.09026441094465554, 0.051074306946247816]
[0.0011086494751824912, 0.001107196384427053, 0.0011089125923304396, 0.0011144447489641607, 0.0011059808629480276, 0.0011033039111432365, 0.001161518953846429, 0.0011190016362392767, 0.0012633935918777504, 0.0011351144772065295, 0.001116179522465576, 0.001155666387851604, 0.0011326570238452405, 0.0011248985247220844, 0.0011944273190403526, 0.0011937377487563274, 0.0011818660925862125, 0.0011969757721420717, 0.0012186602742241864, 0.0012096935692666607, 0.0012115793413778936, 0.0012130756804253906, 0.0010919253876305777, 0.0010906879985916682, 0.0011171301382339814, 0.0011191877711098641, 0.001104266726708209, 0.0011199144540693271, 0.001174339204391634, 0.001114553136920387, 0.0011294041575059634, 0.001140983068828725, 0.0012504146143328398, 0.0012381597718393261, 0.0024844654782844536, 0.0011350064093924382, 0.0011388845900496976, 0.0011383972966789522, 0.0011510877044532788, 0.0011386358184443618, 0.0011318252734120258, 0.0011338341150390493, 0.0011382459091361272, 0.0011227500005836853, 0.001155666181478988, 0.0011419825684490868, 0.0011345567499202762, 0.001131016409172761, 0.0011418630681212314, 0.0011453114778057418, 0.0011327023411550645, 0.0011353106608360329, 0.0011348063628908924, 0.0011250881811561571, 0.0011289991803509606, 0.0011272906823168423, 0.0011458497055255893, 0.0011233924305997789, 0.0011254224545237694, 0.001128276521657509, 0.001121135682544925, 0.001126607819142836, 0.0011287081367547878, 0.0011219407050785694, 0.0011268594535067677, 0.0011251833189321173, 0.001140362273541872, 0.0011281959781296212, 0.0011269235242666168, 0.0011353948635091497, 0.0011383885919878428, 0.001171909772727469, 0.0011308176353023473, 0.0011316337278747762, 0.0011378027477555654, 0.0011372399537570097, 0.0011354596380525354, 0.0011296224309427832, 0.0011296167265920137, 0.0011402074782050806, 0.0011397071595473046, 0.0011376042728608643, 0.0011238873174244707, 0.001137942340309647, 0.0011404995219147002, 0.0011360447282310236, 0.001133958520685238, 0.0011284083858217027, 0.001134678909279914, 0.0011334212507459927, 0.0011381023637527092, 0.0011392846354283392, 0.0011336259776726365, 0.0011544149548915977, 0.0012121440906246955, 0.001160167453979904, 0.001132073433836922, 0.0011306029336992651, 0.0011355728202033788, 0.0011316309100948274, 0.0011507700467270545, 0.0011378810000182552, 0.0011339140897193415, 0.0011229393183550035, 0.001135444295042279, 0.001127527566330338, 0.0011382498884234917, 0.0011271022958680987, 0.0011300932727119123, 0.0011296614777000452, 0.0011365348420275207, 0.0011372390224344351, 0.0011261579559438608, 0.0011383409093303437, 0.001133906726450236, 0.0011525374562056227, 0.0011353353408842602, 0.00113196363566782, 0.0011341857713308525, 0.001121187751943415, 0.0011230404651182335, 0.0011236060703129962, 0.001133089370722341, 0.001125973416492343, 0.0011341707445247922, 0.0011241698832532695, 0.0011341389983372634, 0.0011306196738164439, 0.0011272297435721687, 0.0011356137205608362, 0.0011248732537984154, 0.0011273823261564208, 0.0011891289072674374, 0.0011879964650413672, 0.0011685174635373229, 0.0011367003953214301, 0.0011308527914374027, 0.0011344846733304303, 0.0011241758150200164, 0.0011207752070541298, 0.0011269570934737837, 0.0011304104429951242, 0.0011264081410847086, 0.0011312006268823563, 0.0011319745126245327, 0.0011163193483425433, 0.0011294917203485966, 0.0011337621186231805, 0.0012209188371766792, 0.0013244398382254118, 0.001219033001012407, 0.0012071884892404425, 0.001207793161251344, 0.0029295337884578596, 0.001495284767453234, 0.0013208063969085383, 0.0014445456502915814, 0.001610445860383469, 0.0013149782100172584, 0.0014677753016829145, 0.001490593257056939, 0.0013099259529046196, 0.001368639977150705, 0.0012996511853321693, 0.0013666260218637627, 0.0014355563731907413, 0.004520349185588921, 0.001368836651361266, 0.0012319334187046733, 0.0013163521165721291, 0.0015322656051234105, 0.0012993193502256343, 0.0033661157223071124, 0.0015108815832929902, 0.001506214558016942, 0.0015756894189994349, 0.0019408451626077294, 0.0013522119315471067, 0.0012293958354221528, 0.0017404702551619604, 0.0015032990905997712, 0.0013204343470766447, 0.0035709531864064723, 0.0012989961163162491, 0.0014202529537426524, 0.0020991723475501287, 0.001187774580145298]
[901.9983523966339, 903.1821401019797, 901.7843308086584, 897.3078305851113, 904.1747768894191, 906.3685806785605, 860.9416115754714, 893.653742420596, 791.5189743156168, 880.9684133894224, 895.9132288962425, 865.3016220875045, 882.8797941014085, 888.9690741189819, 837.2213060259181, 837.7049322950796, 846.119544568501, 835.4387977380948, 820.5732320573195, 826.6556303231562, 825.3689757228192, 824.350876154181, 915.8134899399572, 916.8524832869092, 895.1508564444006, 893.5051166689667, 905.5783134759249, 892.925344758606, 851.5427197357766, 897.220569279534, 885.4226304676233, 876.437194661047, 799.7347348131813, 807.6502102103252, 402.5010646114951, 881.052293383342, 878.0520947749086, 878.427946831305, 868.743533730091, 878.2439334872056, 883.5286006517398, 881.9632314252249, 878.5447783941089, 890.6702288845504, 865.301776608388, 875.6701088336999, 881.4014813011943, 884.1604700778938, 875.761751052474, 873.124926605871, 882.8444717261358, 880.8161805365315, 881.2076074833803, 888.8192203498088, 885.7402356033067, 887.0826448638512, 872.714802977857, 890.1608847997136, 888.5552229568383, 886.3075503254622, 891.9527007917965, 887.6203262647655, 885.968628590874, 891.312700816903, 887.4221154093501, 888.7440678991529, 876.9143132857964, 886.3708250917975, 887.3716614007001, 880.7508578199072, 878.4346637327153, 853.3080133572304, 884.3158868252259, 883.6781507723474, 878.8869617098431, 879.3219027315907, 880.7006136432408, 885.2515430004339, 885.2560133532551, 877.0333637648159, 877.4183715729251, 879.0402988599761, 889.7689158835121, 878.7791477447695, 876.808784909593, 880.247031784687, 881.8664719726358, 886.2039777130899, 881.3065897511186, 882.2844986732181, 878.6555865701404, 877.7437778962304, 882.1251627040392, 866.2396443867123, 824.9844286124729, 861.9445378936838, 883.3349234339973, 884.4838184950219, 880.6128345172104, 883.6803511457662, 868.9833410629127, 878.826520509576, 881.901026776652, 890.520069655146, 880.7125143578834, 886.8962762964718, 878.5417070279957, 887.2309138806216, 884.8827120263053, 885.2209442743575, 879.8674383057634, 879.3226228373223, 887.9749014976104, 878.4714594754167, 881.9067535921238, 867.6507601689531, 880.7970332546384, 883.4206051239748, 881.6897771752162, 891.9112773633553, 890.439864866953, 889.9916317837585, 882.5429183600081, 888.1204345971346, 881.7014588213448, 889.5452679323428, 881.7261389177855, 884.4707227006471, 887.1306011062305, 880.5811182927044, 888.9890453197729, 887.0105347573572, 840.9517201107769, 841.7533464337195, 855.7852417308435, 879.739291123608, 884.2883950694603, 881.4574788960059, 889.5405742047516, 892.2395799853763, 887.3452288387969, 884.6344318532746, 887.7776744733219, 884.0164832263649, 883.4121164808349, 895.801010243844, 885.3539888644501, 882.0192380517893, 819.0552635852976, 755.036182949524, 820.3223367780034, 828.3710529986873, 827.9563356394085, 341.3512429656637, 668.7689340293341, 757.1132319926573, 692.2591887616359, 620.9460526427672, 760.4688749837729, 681.3031932431518, 670.873825079836, 763.4019295385422, 730.6523385951681, 769.437223838192, 731.7290787689163, 696.5940304924068, 221.2218479023801, 730.5473585950017, 811.7321803409297, 759.6751563738648, 652.6283672075631, 769.6337315582533, 297.07831889826025, 661.8652388498139, 663.9160368470904, 634.6428350296351, 515.2394530310676, 739.529046202003, 813.4076683744563, 574.5573628932511, 665.2036219891745, 757.3265586539267, 280.03727514734567, 769.8252423077632, 704.0999262594728, 476.378226479148, 841.910592056678]
Elapsed: 0.054520269601237285~0.016820967957512718
Time per graph: 0.0012507266624096735~0.00039470330971301326
Speed: 833.0318796344249~114.92110038063093
Total Time: 0.0518
best val loss: 0.682796835899353 test_score: 0.5349

Testing...
Test loss: 0.7730 score: 0.5349 time: 0.10s
test Score 0.5349
Epoch Time List: [0.214531535981223, 0.212832995923236, 0.2126648281700909, 0.21185074106324464, 0.21194179111626, 0.21217657602392137, 0.2269525210140273, 0.2207354559795931, 0.2322369300527498, 0.22783999401144683, 0.21460950304754078, 0.21712373895570636, 0.21771800506394356, 0.2160906509961933, 0.3209781660698354, 0.23115364089608192, 0.23115328303538263, 0.2325881840661168, 0.23477065784391016, 0.23623829998541623, 0.2346863349666819, 0.3108152929926291, 0.22269384993705899, 0.21088385407347232, 0.21324257692322135, 0.21357741602696478, 0.21342324616853148, 0.21416953403968364, 0.31626988807693124, 0.22279651800636202, 0.2187746912240982, 0.22217686800286174, 0.23385163384955376, 0.24437908094841987, 0.3003551500150934, 0.21977241907734424, 0.22037961101159453, 0.22077110304962844, 0.22184824291616678, 0.2252388729248196, 0.21911599289160222, 0.2204102820251137, 0.21827815694268793, 0.2174042658880353, 0.2184164640493691, 0.22063102095853537, 0.21873964206315577, 0.2184863950824365, 0.21852185297757387, 0.22231229790486395, 0.23211363703012466, 0.21918448095675558, 0.21957972494419664, 0.21817811497021466, 0.2192366721574217, 0.2176317070843652, 0.21858846594113857, 0.21811859612353146, 0.21817104890942574, 0.2191662269178778, 0.21770129713695496, 0.2179600519593805, 0.21874859102535993, 0.21840309700928628, 0.21689834515564144, 0.21803944907151163, 0.21825957496184856, 0.21868057700339705, 0.21736592811066657, 0.21878843486774713, 0.22021179413422942, 0.22239815699867904, 0.22984811279457062, 0.22111741488333791, 0.21890791889745742, 0.2189196809194982, 0.219581050099805, 0.21896657010074705, 0.21838076401036233, 0.21899972099345177, 0.218926944071427, 0.22017264901660383, 0.21825834386982024, 0.21857412299141288, 0.21871946891769767, 0.21886095497757196, 0.22037637291941792, 0.22126975806895643, 0.22026909212581813, 0.2191331370268017, 0.21986144012771547, 0.22030817810446024, 0.22006625006906688, 0.22110882797278464, 0.2277552931336686, 0.22027957206591964, 0.22214685298968107, 0.21950416406616569, 0.2201563878916204, 0.2196123821195215, 0.2201759790768847, 0.22179190895985812, 0.21982307895086706, 0.21851181297097355, 0.21897952107246965, 0.21952412102837116, 0.21956160804256797, 0.2185752559453249, 0.2185803969623521, 0.21882079902570695, 0.21904479898512363, 0.2187215780140832, 0.2183542789425701, 0.21849072386976331, 0.21999394590966403, 0.2218747790902853, 0.22236309596337378, 0.22837858297862113, 0.2190575290005654, 0.21788158197887242, 0.22677540685981512, 0.2254800561349839, 0.2262055620085448, 0.22598331607878208, 0.22602262801956385, 0.22541887999977916, 0.22506065107882023, 0.22523827012628317, 0.22494681493844837, 0.226086272043176, 0.2252407429041341, 0.22623500286135823, 0.23704679403454065, 0.23990674782544374, 0.2422623458551243, 0.3286102389683947, 0.2283573701279238, 0.22752144001424313, 0.22613897500559688, 0.22401471121702343, 0.22378816397394985, 0.32009095104876906, 0.22588480007834733, 0.2266131518408656, 0.22520914406049997, 0.22372444905340672, 0.2243928329553455, 0.22618340817280114, 0.23294420295860618, 0.23771505802869797, 0.2492300608428195, 0.2296339760068804, 0.22873036714736372, 0.4632890878710896, 0.5881717189913616, 0.25538522005081177, 0.2551885839784518, 0.28359787294175476, 0.2839617379941046, 0.2814242790918797, 0.3409182650502771, 0.2581347430823371, 0.25899914710316807, 0.25164275511633605, 0.28423090605065227, 0.31626206601504236, 0.5187718669185415, 0.2601769028697163, 0.2566540570696816, 0.24221701500937343, 0.27788585296366364, 0.28062181710265577, 0.404961952008307, 0.27360438695177436, 0.2637140820734203, 0.2675529100233689, 0.3296115449629724, 0.316994939930737, 0.2501481210347265, 0.39334800199139863, 0.2580422580940649, 0.2528097110334784, 0.3762126669753343, 0.26178325701039284, 0.29305972100701183, 0.3827924579381943, 0.25044107297435403]
Total Epoch List: [120, 28, 39]
Total Time List: [0.049916084040887654, 0.049372291075997055, 0.0517617940204218]
T-times Epoch Time: 0.25957312995213133 ~ 0.03509915374106554
T-times Total Epoch: 64.77777777777779 ~ 7.020252888413916
T-times Total Time: 0.051127502565375633 ~ 0.0014085198026863835
T-times Inference Elapsed: 0.05940802323731548 ~ 0.010181812601154973
T-times Time Per Graph: 0.001370139775045647 ~ 0.0002402951221233935
T-times Speed: 815.2282771769429 ~ 52.81054709463484
T-times cross validation test micro f1 score:0.7162186697925179 ~ 0.03509957113219966
T-times cross validation test precision:0.6528612412126344 ~ 0.039726490138223935
T-times cross validation test recall:0.8547378547378548 ~ 0.11874819624210768
T-times cross validation test f1_score:0.7162186697925179 ~ 0.06373089917064646
