Namespace(seed=15, model='Ethident', dataset='mining/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Times/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbce500>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 1.1040;  Loss pred: 1.0702; Loss self: 3.3746; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1086 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.1510 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 1.1040;  Loss pred: 1.0702; Loss self: 3.3746; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0173 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0305 score: 0.5000 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.9583;  Loss pred: 0.9251; Loss self: 3.3264; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9305 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8857 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.8239;  Loss pred: 0.7912; Loss self: 3.2706; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8512 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7919 score: 0.5000 time: 0.07s
Epoch 5/1000, LR 0.000090
Train loss: 0.7742;  Loss pred: 0.7416; Loss self: 3.2549; time: 0.34s
Val loss: 0.7921 score: 0.5116 time: 0.07s
Test loss: 0.7334 score: 0.5227 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.7482;  Loss pred: 0.7154; Loss self: 3.2787; time: 0.19s
Val loss: 0.7622 score: 0.5581 time: 0.05s
Test loss: 0.7075 score: 0.6136 time: 0.20s
Epoch 7/1000, LR 0.000150
Train loss: 0.7284;  Loss pred: 0.6953; Loss self: 3.3087; time: 0.19s
Val loss: 0.7499 score: 0.4884 time: 0.05s
Test loss: 0.7074 score: 0.6136 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.7157;  Loss pred: 0.6823; Loss self: 3.3381; time: 0.15s
Val loss: 0.7447 score: 0.4651 time: 0.05s
Test loss: 0.7052 score: 0.5455 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.7151;  Loss pred: 0.6815; Loss self: 3.3645; time: 0.15s
Val loss: 0.7425 score: 0.4651 time: 0.05s
Test loss: 0.7033 score: 0.5000 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.7189;  Loss pred: 0.6850; Loss self: 3.3925; time: 0.14s
Val loss: 0.7378 score: 0.4884 time: 0.05s
Test loss: 0.7022 score: 0.4773 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.7189;  Loss pred: 0.6847; Loss self: 3.4203; time: 0.15s
Val loss: 0.7302 score: 0.5116 time: 0.05s
Test loss: 0.7000 score: 0.5000 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.7179;  Loss pred: 0.6835; Loss self: 3.4437; time: 0.15s
Val loss: 0.7192 score: 0.5116 time: 0.06s
Test loss: 0.6935 score: 0.5000 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.7118;  Loss pred: 0.6772; Loss self: 3.4631; time: 0.14s
Val loss: 0.7058 score: 0.5116 time: 0.05s
Test loss: 0.6849 score: 0.4773 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.7006;  Loss pred: 0.6658; Loss self: 3.4781; time: 0.14s
Val loss: 0.6920 score: 0.5116 time: 0.05s
Test loss: 0.6751 score: 0.5227 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6517; Loss self: 3.4903; time: 0.14s
Val loss: 0.6786 score: 0.5116 time: 0.05s
Test loss: 0.6660 score: 0.5455 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.6737;  Loss pred: 0.6387; Loss self: 3.5000; time: 0.14s
Val loss: 0.6712 score: 0.6047 time: 0.05s
Test loss: 0.6601 score: 0.6364 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.6638;  Loss pred: 0.6287; Loss self: 3.5079; time: 0.14s
Val loss: 0.6680 score: 0.6512 time: 0.07s
Test loss: 0.6566 score: 0.7045 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.6565;  Loss pred: 0.6213; Loss self: 3.5151; time: 0.16s
Val loss: 0.6653 score: 0.6512 time: 0.06s
Test loss: 0.6542 score: 0.6818 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.6496;  Loss pred: 0.6144; Loss self: 3.5211; time: 0.14s
Val loss: 0.6616 score: 0.6744 time: 0.05s
Test loss: 0.6508 score: 0.7045 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.6425;  Loss pred: 0.6072; Loss self: 3.5252; time: 0.14s
Val loss: 0.6568 score: 0.6744 time: 0.05s
Test loss: 0.6464 score: 0.7045 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.6337;  Loss pred: 0.5984; Loss self: 3.5256; time: 0.14s
Val loss: 0.6525 score: 0.6977 time: 0.06s
Test loss: 0.6418 score: 0.7045 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.6248;  Loss pred: 0.5895; Loss self: 3.5236; time: 0.14s
Val loss: 0.6490 score: 0.6512 time: 0.05s
Test loss: 0.6376 score: 0.7045 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.6146;  Loss pred: 0.5794; Loss self: 3.5198; time: 0.14s
Val loss: 0.6453 score: 0.6279 time: 0.05s
Test loss: 0.6324 score: 0.6818 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.6041;  Loss pred: 0.5690; Loss self: 3.5149; time: 0.15s
Val loss: 0.6406 score: 0.6744 time: 0.05s
Test loss: 0.6270 score: 0.7045 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.5926;  Loss pred: 0.5575; Loss self: 3.5086; time: 0.14s
Val loss: 0.6351 score: 0.6744 time: 0.05s
Test loss: 0.6208 score: 0.6818 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.5801;  Loss pred: 0.5451; Loss self: 3.5010; time: 0.14s
Val loss: 0.6279 score: 0.6744 time: 0.05s
Test loss: 0.6133 score: 0.6818 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.5666;  Loss pred: 0.5317; Loss self: 3.4920; time: 0.15s
Val loss: 0.6182 score: 0.6977 time: 0.05s
Test loss: 0.6044 score: 0.7045 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.5516;  Loss pred: 0.5168; Loss self: 3.4813; time: 0.13s
Val loss: 0.6063 score: 0.6977 time: 0.05s
Test loss: 0.5949 score: 0.7045 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.5346;  Loss pred: 0.4999; Loss self: 3.4682; time: 0.14s
Val loss: 0.5927 score: 0.6977 time: 0.05s
Test loss: 0.5848 score: 0.7045 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.5167;  Loss pred: 0.4822; Loss self: 3.4524; time: 0.14s
Val loss: 0.5788 score: 0.7209 time: 0.05s
Test loss: 0.5740 score: 0.7273 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.4984;  Loss pred: 0.4640; Loss self: 3.4336; time: 0.14s
Val loss: 0.5645 score: 0.7442 time: 0.05s
Test loss: 0.5630 score: 0.7273 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.4789;  Loss pred: 0.4448; Loss self: 3.4118; time: 0.14s
Val loss: 0.5507 score: 0.7674 time: 0.05s
Test loss: 0.5513 score: 0.7045 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.4589;  Loss pred: 0.4250; Loss self: 3.3872; time: 0.14s
Val loss: 0.5380 score: 0.8140 time: 0.05s
Test loss: 0.5396 score: 0.7273 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.4387;  Loss pred: 0.4051; Loss self: 3.3601; time: 0.14s
Val loss: 0.5248 score: 0.7907 time: 0.05s
Test loss: 0.5282 score: 0.7727 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 0.4177;  Loss pred: 0.3844; Loss self: 3.3308; time: 0.14s
Val loss: 0.5110 score: 0.8372 time: 0.05s
Test loss: 0.5169 score: 0.7955 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.3963;  Loss pred: 0.3633; Loss self: 3.3003; time: 0.14s
Val loss: 0.4963 score: 0.8372 time: 0.05s
Test loss: 0.5046 score: 0.7955 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.3749;  Loss pred: 0.3422; Loss self: 3.2688; time: 0.14s
Val loss: 0.4801 score: 0.8372 time: 0.05s
Test loss: 0.4915 score: 0.7955 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.3531;  Loss pred: 0.3208; Loss self: 3.2368; time: 0.14s
Val loss: 0.4617 score: 0.8372 time: 0.05s
Test loss: 0.4765 score: 0.7955 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.3317;  Loss pred: 0.2997; Loss self: 3.2048; time: 0.14s
Val loss: 0.4428 score: 0.8605 time: 0.05s
Test loss: 0.4604 score: 0.7955 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.3111;  Loss pred: 0.2794; Loss self: 3.1723; time: 0.14s
Val loss: 0.4239 score: 0.8605 time: 0.05s
Test loss: 0.4458 score: 0.8182 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 0.2911;  Loss pred: 0.2597; Loss self: 3.1391; time: 0.15s
Val loss: 0.4059 score: 0.8605 time: 0.05s
Test loss: 0.4332 score: 0.8182 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.2719;  Loss pred: 0.2409; Loss self: 3.1050; time: 0.14s
Val loss: 0.3881 score: 0.8605 time: 0.05s
Test loss: 0.4210 score: 0.8182 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.2530;  Loss pred: 0.2223; Loss self: 3.0704; time: 0.14s
Val loss: 0.3717 score: 0.8837 time: 0.05s
Test loss: 0.4089 score: 0.8182 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.2353;  Loss pred: 0.2049; Loss self: 3.0349; time: 0.14s
Val loss: 0.3569 score: 0.8837 time: 0.05s
Test loss: 0.3967 score: 0.8182 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.2206;  Loss pred: 0.1906; Loss self: 3.0004; time: 0.14s
Val loss: 0.3442 score: 0.9070 time: 0.05s
Test loss: 0.3875 score: 0.8182 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.2076;  Loss pred: 0.1779; Loss self: 2.9704; time: 0.14s
Val loss: 0.3328 score: 0.9070 time: 0.05s
Test loss: 0.3800 score: 0.8182 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.1952;  Loss pred: 0.1658; Loss self: 2.9437; time: 0.14s
Val loss: 0.3223 score: 0.9070 time: 0.05s
Test loss: 0.3739 score: 0.8409 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.1836;  Loss pred: 0.1544; Loss self: 2.9204; time: 0.14s
Val loss: 0.3126 score: 0.8837 time: 0.07s
Test loss: 0.3697 score: 0.8182 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.1731;  Loss pred: 0.1441; Loss self: 2.8983; time: 0.15s
Val loss: 0.3031 score: 0.8837 time: 0.05s
Test loss: 0.3674 score: 0.8182 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 0.1633;  Loss pred: 0.1345; Loss self: 2.8781; time: 0.14s
Val loss: 0.2948 score: 0.8837 time: 0.05s
Test loss: 0.3663 score: 0.8182 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 0.1542;  Loss pred: 0.1256; Loss self: 2.8592; time: 0.14s
Val loss: 0.2871 score: 0.8837 time: 0.05s
Test loss: 0.3653 score: 0.7955 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.1459;  Loss pred: 0.1175; Loss self: 2.8421; time: 0.14s
Val loss: 0.2800 score: 0.8837 time: 0.05s
Test loss: 0.3630 score: 0.7955 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.1385;  Loss pred: 0.1103; Loss self: 2.8285; time: 0.14s
Val loss: 0.2740 score: 0.8837 time: 0.05s
Test loss: 0.3602 score: 0.7955 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.1320;  Loss pred: 0.1038; Loss self: 2.8171; time: 0.14s
Val loss: 0.2687 score: 0.8837 time: 0.06s
Test loss: 0.3580 score: 0.7955 time: 0.06s
Epoch 55/1000, LR 0.000269
Train loss: 0.1263;  Loss pred: 0.0982; Loss self: 2.8071; time: 0.14s
Val loss: 0.2643 score: 0.8837 time: 0.05s
Test loss: 0.3579 score: 0.7955 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.1212;  Loss pred: 0.0932; Loss self: 2.7990; time: 0.14s
Val loss: 0.2606 score: 0.8837 time: 0.05s
Test loss: 0.3595 score: 0.7955 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.1165;  Loss pred: 0.0886; Loss self: 2.7926; time: 0.14s
Val loss: 0.2572 score: 0.8837 time: 0.05s
Test loss: 0.3616 score: 0.7955 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.1123;  Loss pred: 0.0845; Loss self: 2.7872; time: 0.14s
Val loss: 0.2543 score: 0.8837 time: 0.05s
Test loss: 0.3641 score: 0.7955 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.1086;  Loss pred: 0.0807; Loss self: 2.7830; time: 0.15s
Val loss: 0.2518 score: 0.8837 time: 0.05s
Test loss: 0.3658 score: 0.7955 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.1052;  Loss pred: 0.0774; Loss self: 2.7796; time: 0.14s
Val loss: 0.2498 score: 0.8837 time: 0.06s
Test loss: 0.3663 score: 0.7955 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.1021;  Loss pred: 0.0743; Loss self: 2.7768; time: 0.14s
Val loss: 0.2481 score: 0.8837 time: 0.05s
Test loss: 0.3662 score: 0.7955 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.0990;  Loss pred: 0.0713; Loss self: 2.7745; time: 0.16s
Val loss: 0.2465 score: 0.8837 time: 0.06s
Test loss: 0.3653 score: 0.7955 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 0.0962;  Loss pred: 0.0685; Loss self: 2.7729; time: 0.15s
Val loss: 0.2446 score: 0.8837 time: 0.06s
Test loss: 0.3636 score: 0.7955 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 0.0937;  Loss pred: 0.0660; Loss self: 2.7714; time: 0.17s
Val loss: 0.2424 score: 0.8837 time: 0.05s
Test loss: 0.3614 score: 0.7955 time: 0.06s
Epoch 65/1000, LR 0.000268
Train loss: 0.0912;  Loss pred: 0.0635; Loss self: 2.7697; time: 0.21s
Val loss: 0.2395 score: 0.8837 time: 0.05s
Test loss: 0.3588 score: 0.7955 time: 0.06s
Epoch 66/1000, LR 0.000268
Train loss: 0.0888;  Loss pred: 0.0611; Loss self: 2.7674; time: 0.15s
Val loss: 0.2364 score: 0.8837 time: 0.05s
Test loss: 0.3559 score: 0.7955 time: 0.07s
Epoch 67/1000, LR 0.000268
Train loss: 0.0865;  Loss pred: 0.0589; Loss self: 2.7647; time: 0.20s
Val loss: 0.2333 score: 0.9070 time: 0.06s
Test loss: 0.3526 score: 0.7955 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0844;  Loss pred: 0.0567; Loss self: 2.7619; time: 0.16s
Val loss: 0.2307 score: 0.9070 time: 0.06s
Test loss: 0.3497 score: 0.7955 time: 0.06s
Epoch 69/1000, LR 0.000268
Train loss: 0.0823;  Loss pred: 0.0547; Loss self: 2.7592; time: 0.16s
Val loss: 0.2287 score: 0.9070 time: 0.05s
Test loss: 0.3468 score: 0.7955 time: 0.08s
Epoch 70/1000, LR 0.000268
Train loss: 0.0804;  Loss pred: 0.0528; Loss self: 2.7573; time: 0.18s
Val loss: 0.2274 score: 0.9070 time: 0.05s
Test loss: 0.3440 score: 0.8182 time: 0.10s
Epoch 71/1000, LR 0.000268
Train loss: 0.0785;  Loss pred: 0.0509; Loss self: 2.7557; time: 0.18s
Val loss: 0.2267 score: 0.9070 time: 0.05s
Test loss: 0.3416 score: 0.8182 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0764;  Loss pred: 0.0489; Loss self: 2.7544; time: 0.16s
Val loss: 0.2262 score: 0.9070 time: 0.05s
Test loss: 0.3395 score: 0.8182 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 0.0742;  Loss pred: 0.0467; Loss self: 2.7534; time: 0.17s
Val loss: 0.2259 score: 0.9070 time: 0.05s
Test loss: 0.3381 score: 0.8182 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0721;  Loss pred: 0.0446; Loss self: 2.7531; time: 0.17s
Val loss: 0.2260 score: 0.9070 time: 0.05s
Test loss: 0.3373 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0700;  Loss pred: 0.0425; Loss self: 2.7530; time: 0.33s
Val loss: 0.2271 score: 0.9070 time: 0.06s
Test loss: 0.3373 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0679;  Loss pred: 0.0404; Loss self: 2.7534; time: 0.23s
Val loss: 0.2286 score: 0.9070 time: 0.05s
Test loss: 0.3378 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0657;  Loss pred: 0.0382; Loss self: 2.7540; time: 0.17s
Val loss: 0.2307 score: 0.9070 time: 0.06s
Test loss: 0.3397 score: 0.8409 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0638;  Loss pred: 0.0363; Loss self: 2.7543; time: 0.16s
Val loss: 0.2327 score: 0.9070 time: 0.06s
Test loss: 0.3429 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0619;  Loss pred: 0.0344; Loss self: 2.7534; time: 0.15s
Val loss: 0.2355 score: 0.9070 time: 0.05s
Test loss: 0.3470 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0600;  Loss pred: 0.0324; Loss self: 2.7517; time: 0.15s
Val loss: 0.2393 score: 0.9070 time: 0.07s
Test loss: 0.3516 score: 0.8409 time: 0.15s
     INFO: Early stopping counter 7 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0581;  Loss pred: 0.0306; Loss self: 2.7498; time: 0.20s
Val loss: 0.2441 score: 0.9070 time: 0.05s
Test loss: 0.3565 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0563;  Loss pred: 0.0288; Loss self: 2.7485; time: 0.15s
Val loss: 0.2499 score: 0.9070 time: 0.05s
Test loss: 0.3614 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0544;  Loss pred: 0.0269; Loss self: 2.7475; time: 0.15s
Val loss: 0.2567 score: 0.9070 time: 0.06s
Test loss: 0.3662 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0526;  Loss pred: 0.0251; Loss self: 2.7468; time: 0.15s
Val loss: 0.2641 score: 0.9070 time: 0.06s
Test loss: 0.3710 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0508;  Loss pred: 0.0233; Loss self: 2.7457; time: 0.18s
Val loss: 0.2721 score: 0.9070 time: 0.06s
Test loss: 0.3764 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0492;  Loss pred: 0.0218; Loss self: 2.7440; time: 0.17s
Val loss: 0.2804 score: 0.9070 time: 0.06s
Test loss: 0.3822 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0477;  Loss pred: 0.0203; Loss self: 2.7418; time: 0.17s
Val loss: 0.2882 score: 0.9070 time: 0.05s
Test loss: 0.3878 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0462;  Loss pred: 0.0189; Loss self: 2.7396; time: 0.17s
Val loss: 0.2959 score: 0.9070 time: 0.06s
Test loss: 0.3932 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0450;  Loss pred: 0.0176; Loss self: 2.7373; time: 0.16s
Val loss: 0.3033 score: 0.9070 time: 0.06s
Test loss: 0.3981 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0439;  Loss pred: 0.0165; Loss self: 2.7350; time: 0.16s
Val loss: 0.3108 score: 0.9070 time: 0.05s
Test loss: 0.4024 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0428;  Loss pred: 0.0155; Loss self: 2.7329; time: 0.15s
Val loss: 0.3176 score: 0.9070 time: 0.05s
Test loss: 0.4064 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0418;  Loss pred: 0.0145; Loss self: 2.7314; time: 0.15s
Val loss: 0.3238 score: 0.9070 time: 0.05s
Test loss: 0.4097 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0409;  Loss pred: 0.0136; Loss self: 2.7302; time: 0.18s
Val loss: 0.3294 score: 0.9070 time: 0.06s
Test loss: 0.4121 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 072,   Train_Loss: 0.0742,   Val_Loss: 0.2259,   Val_Precision: 0.9091,   Val_Recall: 0.9091,   Val_accuracy: 0.9091,   Val_Score: 0.9070,   Val_Loss: 0.2259,   Test_Precision: 0.7917,   Test_Recall: 0.8636,   Test_accuracy: 0.8261,   Test_Score: 0.8182,   Test_loss: 0.3381


[0.053479971014894545, 0.05104726902209222, 0.059244206990115345, 0.07292524306103587, 0.05774662701878697, 0.2024409860605374, 0.0541916589718312, 0.055830162949860096, 0.08719803905114532, 0.051699987961910665, 0.058047899045050144, 0.05822045402601361, 0.059791274019517004, 0.06570984702557325, 0.05201531003694981, 0.05492900998797268, 0.06307178596034646, 0.05618452001363039, 0.061370031093247235, 0.05400642903987318, 0.051611586939543486, 0.051086443942040205, 0.051583499065600336, 0.05475352599751204, 0.050839767907746136, 0.051151714054867625, 0.05115750408731401, 0.05567310191690922, 0.05023603793233633, 0.05526092299260199, 0.05644303001463413, 0.05114851496182382, 0.05180432496126741, 0.055911062983796, 0.055436752969399095, 0.059701831080019474, 0.05577641993295401, 0.052019037888385355, 0.05350944492965937, 0.05687929596751928, 0.050654642982408404, 0.04978737002238631, 0.050601797993294895, 0.05033637804444879, 0.05111703707370907, 0.0508030760101974, 0.05063315900042653, 0.057281719986349344, 0.05737010296434164, 0.0522556760115549, 0.05124917905777693, 0.050806573941372335, 0.05649829900357872, 0.06069887208286673, 0.05274957907386124, 0.058241468970663846, 0.05134840798564255, 0.05228742107283324, 0.05039729899726808, 0.05146392295137048, 0.050891074002720416, 0.06682134792208672, 0.06368735793512315, 0.06199471291620284, 0.06676666194107383, 0.07153436203952879, 0.061840542941354215, 0.06111174798570573, 0.08222242898773402, 0.10726988397073, 0.058109108940698206, 0.05986333009786904, 0.05918944603763521, 0.05887664796318859, 0.05695899797137827, 0.0638948290143162, 0.07092560594901443, 0.06920232798438519, 0.05974148097448051, 0.15689063805621117, 0.05514732399024069, 0.05391760100610554, 0.05758287804201245, 0.056776094948872924, 0.06323726300615817, 0.0590920140966773, 0.06230867700651288, 0.06289607496000826, 0.05751894402783364, 0.055577951949089766, 0.05543449800461531, 0.05382215708959848, 0.06657404103316367]
[0.0012154538867021488, 0.0011601652050475504, 0.0013464592497753488, 0.0016573918877508152, 0.0013124233413360673, 0.00460093150137585, 0.0012316286129961636, 0.0012688673397695477, 0.001981773614798757, 0.0011749997264070605, 0.0013192704328420487, 0.0013231921369548547, 0.0013588925913526591, 0.0014934056142175739, 0.0011821661372034048, 0.0012483865906357426, 0.001433449680916965, 0.0012769209094006908, 0.0013947734339374372, 0.0012274188418152996, 0.001172990612262352, 0.0011610555441372774, 0.0011723522514909166, 0.0012443983181252736, 0.001155449270630594, 0.0011625389557924461, 0.0011626705474389548, 0.001265297770838846, 0.0011417281348258257, 0.0012559300680136816, 0.0012827961366962302, 0.0011624662491323595, 0.0011773710218469866, 0.0012707059769044545, 0.0012599262038499794, 0.0013568597972731698, 0.0012676459075671366, 0.0011822508610996672, 0.0012161237484013493, 0.0012927112719890747, 0.0011512418859638274, 0.0011315311368724163, 0.0011500408634839748, 0.0011440085919192907, 0.001161750842584297, 0.0011546153638681228, 0.0011507536136460576, 0.0013018572724170306, 0.00130386597646231, 0.0011876290002626113, 0.0011647540694949303, 0.0011546948623039168, 0.0012840522500813347, 0.001379519820065153, 0.0011988540698604827, 0.0013236697493332692, 0.0011670092724009671, 0.0011883504789280282, 0.00114539315902882, 0.0011696346125311472, 0.0011566153182436458, 0.0015186669982292435, 0.0014474399530709807, 0.0014089707480955192, 0.0015174241350244054, 0.001625780955443836, 0.0014054668850307776, 0.0013889033633114939, 0.001868691567903046, 0.002437951908425682, 0.00132066156683405, 0.0013605302294970236, 0.0013452146826735275, 0.0013381056355270134, 0.0012945226811676878, 0.0014521552048708227, 0.0016119455897503278, 0.0015727801814632999, 0.0013577609312381935, 0.003565696319459345, 0.0012533482725054703, 0.001225400022866035, 0.0013087017736821012, 0.0012903657942925665, 0.0014372105228672312, 0.0013430003203790295, 0.0014161062956025655, 0.0014294562490910969, 0.00130724872790531, 0.001263135271570222, 0.001259874954650348, 0.00122323084294542, 0.001513046387117356]
[822.73791785986, 861.9462087375858, 742.6886481464968, 603.3576050363458, 761.9492647715214, 217.34729145629808, 811.9330693100054, 788.104452417871, 504.5985033469864, 851.0640279532843, 757.9947030615566, 755.7481427461946, 735.8933342955289, 669.6104464050248, 845.9047916612238, 801.0339164975719, 697.6177910621243, 783.133859456761, 716.9623221005909, 814.7178175308463, 852.5217419014938, 861.285237428542, 852.985950876342, 803.6012146870564, 865.4642184803522, 860.1862286140329, 860.0888722972527, 790.3277971769735, 875.8652515404232, 796.2226763004024, 779.5470935665928, 860.2400291160101, 849.3499342554414, 786.9641114273211, 793.697279209117, 736.9958207986282, 788.8638254819896, 845.8441714052574, 822.2847397845377, 773.5679433361138, 868.627186164959, 883.7582700234199, 869.5343198245706, 874.1193091236412, 860.7697652067247, 866.089289380197, 868.9957503862113, 768.1333593070447, 766.949991833694, 842.0137936837831, 858.5503379555719, 866.029660861866, 778.784508135598, 724.8899112973754, 834.1298788069973, 755.4754503558752, 856.8912207035273, 841.5025850808485, 873.0626615998833, 854.9678585827334, 864.5916963286716, 658.4722004007422, 690.874946403363, 709.7379426447868, 659.0115294191736, 615.0890110082519, 711.5073365660277, 719.9924965375195, 535.1337894258019, 410.1803635026395, 757.1962606568808, 735.0075568476648, 743.375769592824, 747.3251538965006, 772.4854995186164, 688.6316260450656, 620.3683339925194, 635.8167605275961, 736.5066831670155, 280.4501310284401, 797.8628302578487, 816.0600467928369, 764.1160271269805, 774.9740456722527, 695.7922893613405, 744.6014604953884, 706.1616794624102, 699.5667063163621, 764.9653647797885, 791.6808456760814, 793.7295652309631, 817.5071825299124, 660.9182696012329]
Elapsed: 0.06048811364403215~0.019866689984798565
Time per graph: 0.0013747298555461852~0.00045151568147269456
Speed: 761.647470544791~115.6725054209831
Total Time: 0.0671
best val loss: 0.2258637696504593 test_score: 0.8182

Testing...
Test loss: 0.3875 score: 0.8182 time: 0.05s
test Score 0.8182
Epoch Time List: [0.46641524706501514, 0.25219002191442996, 0.28882176987826824, 0.3776581770507619, 0.46515969920437783, 0.4401430869475007, 0.2936167569132522, 0.2511678319424391, 0.2859513961011544, 0.2375560860382393, 0.24988426105119288, 0.2707818109774962, 0.25040851696394384, 0.2446277568815276, 0.236584790982306, 0.2407670581014827, 0.26387072599027306, 0.27344838599674404, 0.25110868900083005, 0.24327741714660078, 0.24625904206186533, 0.24485349492169917, 0.23820248700212687, 0.24892146897036582, 0.23287157400045544, 0.23151225305628031, 0.24809013109188527, 0.23380675492808223, 0.23224733700044453, 0.23901306395418942, 0.24298432702198625, 0.23653675499372184, 0.22964637191034853, 0.23838811297900975, 0.24136708211153746, 0.24371832190081477, 0.2390701969852671, 0.23505068896338344, 0.24423011194448918, 0.24452908511739224, 0.2458609399618581, 0.2331586549989879, 0.24197144410572946, 0.2362105471547693, 0.23241896578110754, 0.23569558595772833, 0.2345382209168747, 0.25657204899471253, 0.25269452191423625, 0.23530512291472405, 0.2336516350042075, 0.23301243293099105, 0.24276639602612704, 0.24967077490873635, 0.23964230401907116, 0.2467238570097834, 0.23413122992496938, 0.23462586395908147, 0.24390040000434965, 0.2462419499643147, 0.2362316041253507, 0.28699155896902084, 0.2638365312013775, 0.277920680004172, 0.32201675593387336, 0.26626937789842486, 0.3198564890772104, 0.27086996904108673, 0.2892608599504456, 0.33854899602010846, 0.28701814101077616, 0.2704275579890236, 0.274724252987653, 0.2782532050041482, 0.4392861109226942, 0.3426497569307685, 0.29169259301852435, 0.2776578238699585, 0.2564922060118988, 0.3740790809970349, 0.3039485542103648, 0.25244893797207624, 0.26115182298235595, 0.26283224287908524, 0.2980102519504726, 0.27779098495375365, 0.280817475868389, 0.28652364399749786, 0.27876294404268265, 0.26148250000551343, 0.25350258499383926, 0.2546298940433189, 0.29925261600874364]
Total Epoch List: [93]
Total Time List: [0.06712407304439694]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbcfeb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7444;  Loss pred: 0.7123; Loss self: 3.2059; time: 0.21s
Val loss: 0.7199 score: 0.5455 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.5116 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7444;  Loss pred: 0.7123; Loss self: 3.2059; time: 0.20s
Val loss: 0.7047 score: 0.5227 time: 0.07s
Test loss: 0.6881 score: 0.5349 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7215;  Loss pred: 0.6894; Loss self: 3.2072; time: 0.20s
Val loss: 0.6754 score: 0.6136 time: 0.08s
Test loss: 0.6743 score: 0.6279 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6884;  Loss pred: 0.6565; Loss self: 3.1920; time: 0.20s
Val loss: 0.6592 score: 0.6136 time: 0.06s
Test loss: 0.6553 score: 0.6279 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6615;  Loss pred: 0.6298; Loss self: 3.1741; time: 0.19s
Val loss: 0.6530 score: 0.6136 time: 0.06s
Test loss: 0.6503 score: 0.5581 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6281;  Loss pred: 0.5961; Loss self: 3.1939; time: 0.20s
Val loss: 0.6492 score: 0.6591 time: 0.06s
Test loss: 0.6676 score: 0.5814 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.5938;  Loss pred: 0.5616; Loss self: 3.2200; time: 0.19s
Val loss: 0.6348 score: 0.6591 time: 0.05s
Test loss: 0.6697 score: 0.5581 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.5640;  Loss pred: 0.5317; Loss self: 3.2327; time: 0.21s
Val loss: 0.6200 score: 0.6591 time: 0.05s
Test loss: 0.6688 score: 0.5814 time: 0.11s
Epoch 9/1000, LR 0.000210
Train loss: 0.5351;  Loss pred: 0.5030; Loss self: 3.2092; time: 0.18s
Val loss: 0.5990 score: 0.6818 time: 0.06s
Test loss: 0.6693 score: 0.4884 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.5037;  Loss pred: 0.4721; Loss self: 3.1665; time: 0.20s
Val loss: 0.5714 score: 0.7727 time: 0.06s
Test loss: 0.6606 score: 0.5349 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.4607;  Loss pred: 0.4300; Loss self: 3.0775; time: 0.19s
Val loss: 0.5462 score: 0.7273 time: 0.06s
Test loss: 0.6457 score: 0.5814 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.4214;  Loss pred: 0.3916; Loss self: 2.9804; time: 0.21s
Val loss: 0.5278 score: 0.7727 time: 0.05s
Test loss: 0.6384 score: 0.5581 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.3846;  Loss pred: 0.3560; Loss self: 2.8564; time: 0.20s
Val loss: 0.5196 score: 0.7727 time: 0.06s
Test loss: 0.6325 score: 0.6047 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.3560;  Loss pred: 0.3285; Loss self: 2.7470; time: 0.20s
Val loss: 0.4982 score: 0.7955 time: 0.06s
Test loss: 0.6033 score: 0.7209 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.3245;  Loss pred: 0.2979; Loss self: 2.6617; time: 0.17s
Val loss: 0.4784 score: 0.7500 time: 0.09s
Test loss: 0.5761 score: 0.6744 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.2984;  Loss pred: 0.2724; Loss self: 2.5975; time: 0.17s
Val loss: 0.4712 score: 0.7955 time: 0.05s
Test loss: 0.5657 score: 0.6744 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.2730;  Loss pred: 0.2475; Loss self: 2.5486; time: 0.16s
Val loss: 0.4763 score: 0.7955 time: 0.05s
Test loss: 0.5633 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.2568;  Loss pred: 0.2316; Loss self: 2.5183; time: 0.19s
Val loss: 0.4672 score: 0.7955 time: 0.05s
Test loss: 0.5516 score: 0.7209 time: 0.10s
Epoch 19/1000, LR 0.000270
Train loss: 0.2413;  Loss pred: 0.2163; Loss self: 2.4974; time: 0.18s
Val loss: 0.4554 score: 0.7727 time: 0.06s
Test loss: 0.5343 score: 0.6977 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.2223;  Loss pred: 0.1974; Loss self: 2.4899; time: 0.17s
Val loss: 0.4535 score: 0.7727 time: 0.06s
Test loss: 0.5303 score: 0.7209 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.2040;  Loss pred: 0.1791; Loss self: 2.4883; time: 0.17s
Val loss: 0.4577 score: 0.7727 time: 0.06s
Test loss: 0.5422 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1887;  Loss pred: 0.1637; Loss self: 2.4922; time: 0.19s
Val loss: 0.4618 score: 0.7727 time: 0.05s
Test loss: 0.5521 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1774;  Loss pred: 0.1524; Loss self: 2.4999; time: 0.18s
Val loss: 0.4522 score: 0.7500 time: 0.06s
Test loss: 0.5440 score: 0.7209 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.1668;  Loss pred: 0.1418; Loss self: 2.4990; time: 0.17s
Val loss: 0.4397 score: 0.7500 time: 0.05s
Test loss: 0.5313 score: 0.7442 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.1564;  Loss pred: 0.1314; Loss self: 2.4955; time: 0.16s
Val loss: 0.4349 score: 0.7727 time: 0.05s
Test loss: 0.5257 score: 0.7209 time: 0.06s
Epoch 26/1000, LR 0.000270
Train loss: 0.1477;  Loss pred: 0.1227; Loss self: 2.4921; time: 0.17s
Val loss: 0.4449 score: 0.7955 time: 0.07s
Test loss: 0.5338 score: 0.7209 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1398;  Loss pred: 0.1149; Loss self: 2.4902; time: 0.18s
Val loss: 0.4645 score: 0.7955 time: 0.14s
Test loss: 0.5518 score: 0.7442 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1333;  Loss pred: 0.1084; Loss self: 2.4906; time: 0.24s
Val loss: 0.4801 score: 0.7955 time: 0.05s
Test loss: 0.5661 score: 0.7209 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1280;  Loss pred: 0.1031; Loss self: 2.4923; time: 0.18s
Val loss: 0.4853 score: 0.7955 time: 0.24s
Test loss: 0.5699 score: 0.7209 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1226;  Loss pred: 0.0976; Loss self: 2.4950; time: 0.18s
Val loss: 0.4814 score: 0.7955 time: 0.09s
Test loss: 0.5611 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1169;  Loss pred: 0.0920; Loss self: 2.4976; time: 0.19s
Val loss: 0.4718 score: 0.7955 time: 0.06s
Test loss: 0.5492 score: 0.7442 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1116;  Loss pred: 0.0866; Loss self: 2.5008; time: 0.25s
Val loss: 0.4644 score: 0.7955 time: 0.14s
Test loss: 0.5430 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1068;  Loss pred: 0.0818; Loss self: 2.5058; time: 0.22s
Val loss: 0.4602 score: 0.7955 time: 0.10s
Test loss: 0.5426 score: 0.7674 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1021;  Loss pred: 0.0770; Loss self: 2.5123; time: 0.35s
Val loss: 0.4573 score: 0.8182 time: 0.05s
Test loss: 0.5456 score: 0.7674 time: 0.11s
     INFO: Early stopping counter 9 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0974;  Loss pred: 0.0722; Loss self: 2.5193; time: 0.24s
Val loss: 0.4528 score: 0.8182 time: 0.07s
Test loss: 0.5512 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0928;  Loss pred: 0.0676; Loss self: 2.5251; time: 0.18s
Val loss: 0.4481 score: 0.8409 time: 0.08s
Test loss: 0.5563 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0883;  Loss pred: 0.0630; Loss self: 2.5300; time: 0.24s
Val loss: 0.4429 score: 0.8409 time: 0.05s
Test loss: 0.5607 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0835;  Loss pred: 0.0581; Loss self: 2.5336; time: 0.21s
Val loss: 0.4385 score: 0.8409 time: 0.05s
Test loss: 0.5652 score: 0.7907 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0787;  Loss pred: 0.0533; Loss self: 2.5367; time: 0.31s
Val loss: 0.4326 score: 0.8409 time: 0.05s
Test loss: 0.5679 score: 0.7907 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.0740;  Loss pred: 0.0486; Loss self: 2.5390; time: 0.21s
Val loss: 0.4258 score: 0.8409 time: 0.11s
Test loss: 0.5679 score: 0.8140 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 0.0696;  Loss pred: 0.0442; Loss self: 2.5407; time: 0.17s
Val loss: 0.4184 score: 0.8409 time: 0.07s
Test loss: 0.5688 score: 0.8140 time: 0.07s
Epoch 42/1000, LR 0.000269
Train loss: 0.0654;  Loss pred: 0.0400; Loss self: 2.5422; time: 0.24s
Val loss: 0.4112 score: 0.8409 time: 0.09s
Test loss: 0.5690 score: 0.8140 time: 0.08s
Epoch 43/1000, LR 0.000269
Train loss: 0.0615;  Loss pred: 0.0361; Loss self: 2.5435; time: 0.19s
Val loss: 0.4072 score: 0.8636 time: 0.15s
Test loss: 0.5700 score: 0.8140 time: 0.10s
Epoch 44/1000, LR 0.000269
Train loss: 0.0581;  Loss pred: 0.0326; Loss self: 2.5457; time: 0.55s
Val loss: 0.4052 score: 0.8636 time: 0.11s
Test loss: 0.5738 score: 0.8140 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.0549;  Loss pred: 0.0294; Loss self: 2.5485; time: 0.22s
Val loss: 0.4048 score: 0.8864 time: 0.18s
Test loss: 0.5795 score: 0.7674 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0524;  Loss pred: 0.0269; Loss self: 2.5521; time: 0.29s
Val loss: 0.4064 score: 0.8636 time: 0.05s
Test loss: 0.5865 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0500;  Loss pred: 0.0244; Loss self: 2.5563; time: 0.23s
Val loss: 0.4096 score: 0.8636 time: 0.11s
Test loss: 0.5931 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0480;  Loss pred: 0.0224; Loss self: 2.5603; time: 0.39s
Val loss: 0.4141 score: 0.8636 time: 0.11s
Test loss: 0.6011 score: 0.7442 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0461;  Loss pred: 0.0205; Loss self: 2.5637; time: 0.24s
Val loss: 0.4189 score: 0.8409 time: 0.07s
Test loss: 0.6090 score: 0.7442 time: 0.18s
     INFO: Early stopping counter 4 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0445;  Loss pred: 0.0188; Loss self: 2.5676; time: 0.16s
Val loss: 0.4246 score: 0.8409 time: 0.11s
Test loss: 0.6160 score: 0.7442 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0430;  Loss pred: 0.0173; Loss self: 2.5706; time: 0.18s
Val loss: 0.4308 score: 0.8409 time: 0.06s
Test loss: 0.6225 score: 0.7442 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0417;  Loss pred: 0.0159; Loss self: 2.5729; time: 0.17s
Val loss: 0.4381 score: 0.8636 time: 0.07s
Test loss: 0.6294 score: 0.7442 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0406;  Loss pred: 0.0148; Loss self: 2.5753; time: 0.16s
Val loss: 0.4456 score: 0.8636 time: 0.06s
Test loss: 0.6369 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0396;  Loss pred: 0.0138; Loss self: 2.5774; time: 0.16s
Val loss: 0.4543 score: 0.8636 time: 0.05s
Test loss: 0.6454 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0388;  Loss pred: 0.0130; Loss self: 2.5793; time: 0.17s
Val loss: 0.4636 score: 0.8636 time: 0.05s
Test loss: 0.6557 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0381;  Loss pred: 0.0123; Loss self: 2.5807; time: 0.16s
Val loss: 0.4735 score: 0.8636 time: 0.06s
Test loss: 0.6679 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0374;  Loss pred: 0.0116; Loss self: 2.5821; time: 0.16s
Val loss: 0.4836 score: 0.8636 time: 0.08s
Test loss: 0.6801 score: 0.7442 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0368;  Loss pred: 0.0110; Loss self: 2.5830; time: 0.18s
Val loss: 0.4929 score: 0.8636 time: 0.08s
Test loss: 0.6923 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0363;  Loss pred: 0.0104; Loss self: 2.5832; time: 0.17s
Val loss: 0.5010 score: 0.8636 time: 0.05s
Test loss: 0.7031 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0357;  Loss pred: 0.0099; Loss self: 2.5830; time: 0.17s
Val loss: 0.5073 score: 0.8636 time: 0.05s
Test loss: 0.7121 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0352;  Loss pred: 0.0094; Loss self: 2.5825; time: 0.19s
Val loss: 0.5122 score: 0.8636 time: 0.06s
Test loss: 0.7191 score: 0.7674 time: 0.15s
     INFO: Early stopping counter 16 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0348;  Loss pred: 0.0090; Loss self: 2.5819; time: 0.19s
Val loss: 0.5154 score: 0.8636 time: 0.07s
Test loss: 0.7236 score: 0.7674 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0343;  Loss pred: 0.0085; Loss self: 2.5814; time: 0.21s
Val loss: 0.5170 score: 0.8636 time: 0.07s
Test loss: 0.7267 score: 0.7674 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0339;  Loss pred: 0.0081; Loss self: 2.5808; time: 0.25s
Val loss: 0.5173 score: 0.8636 time: 0.05s
Test loss: 0.7288 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0335;  Loss pred: 0.0077; Loss self: 2.5802; time: 0.19s
Val loss: 0.5171 score: 0.8636 time: 0.05s
Test loss: 0.7306 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 044,   Train_Loss: 0.0549,   Val_Loss: 0.4048,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.4048,   Test_Precision: 0.8000,   Test_Recall: 0.7273,   Test_accuracy: 0.7619,   Test_Score: 0.7674,   Test_loss: 0.5795


[0.053479971014894545, 0.05104726902209222, 0.059244206990115345, 0.07292524306103587, 0.05774662701878697, 0.2024409860605374, 0.0541916589718312, 0.055830162949860096, 0.08719803905114532, 0.051699987961910665, 0.058047899045050144, 0.05822045402601361, 0.059791274019517004, 0.06570984702557325, 0.05201531003694981, 0.05492900998797268, 0.06307178596034646, 0.05618452001363039, 0.061370031093247235, 0.05400642903987318, 0.051611586939543486, 0.051086443942040205, 0.051583499065600336, 0.05475352599751204, 0.050839767907746136, 0.051151714054867625, 0.05115750408731401, 0.05567310191690922, 0.05023603793233633, 0.05526092299260199, 0.05644303001463413, 0.05114851496182382, 0.05180432496126741, 0.055911062983796, 0.055436752969399095, 0.059701831080019474, 0.05577641993295401, 0.052019037888385355, 0.05350944492965937, 0.05687929596751928, 0.050654642982408404, 0.04978737002238631, 0.050601797993294895, 0.05033637804444879, 0.05111703707370907, 0.0508030760101974, 0.05063315900042653, 0.057281719986349344, 0.05737010296434164, 0.0522556760115549, 0.05124917905777693, 0.050806573941372335, 0.05649829900357872, 0.06069887208286673, 0.05274957907386124, 0.058241468970663846, 0.05134840798564255, 0.05228742107283324, 0.05039729899726808, 0.05146392295137048, 0.050891074002720416, 0.06682134792208672, 0.06368735793512315, 0.06199471291620284, 0.06676666194107383, 0.07153436203952879, 0.061840542941354215, 0.06111174798570573, 0.08222242898773402, 0.10726988397073, 0.058109108940698206, 0.05986333009786904, 0.05918944603763521, 0.05887664796318859, 0.05695899797137827, 0.0638948290143162, 0.07092560594901443, 0.06920232798438519, 0.05974148097448051, 0.15689063805621117, 0.05514732399024069, 0.05391760100610554, 0.05758287804201245, 0.056776094948872924, 0.06323726300615817, 0.0590920140966773, 0.06230867700651288, 0.06289607496000826, 0.05751894402783364, 0.055577951949089766, 0.05543449800461531, 0.05382215708959848, 0.06657404103316367, 0.06537244596984237, 0.0560759479412809, 0.053938722936436534, 0.05481782800052315, 0.060830753995105624, 0.06409896502736956, 0.06885762501042336, 0.11347280105110258, 0.06722240790259093, 0.06444386800285429, 0.07329016190487891, 0.06028739199973643, 0.05763973400462419, 0.057401316007599235, 0.05353778298012912, 0.05444031406659633, 0.056779478094540536, 0.10769528802484274, 0.048740084981545806, 0.05636125605087727, 0.05682537110988051, 0.054205835913307965, 0.05700614408124238, 0.0676392619498074, 0.059614853002130985, 0.05979486997239292, 0.06664267496671528, 0.08256603707559407, 0.07401768898125738, 0.1246201959438622, 0.0729629339184612, 0.05594870098866522, 0.12482702592387795, 0.11368855403270572, 0.08149380306713283, 0.06652342702727765, 0.11355167010333389, 0.08141914091538638, 0.06264513800852001, 0.053056357079185545, 0.07343619305174798, 0.09177262405864894, 0.10529648908413947, 0.09340312995482236, 0.05245809396728873, 0.05516630597412586, 0.053822992951609194, 0.07075224095024168, 0.18700411694590002, 0.09693706093821675, 0.06760723190382123, 0.08894331892952323, 0.0556069080485031, 0.05207931203767657, 0.0553061889950186, 0.0533777839737013, 0.16294966801069677, 0.05482175410725176, 0.0792473410256207, 0.08297482295893133, 0.15581621194723994, 0.09931345202494413, 0.10443944903090596, 0.06283816799987108, 0.06145069794729352]
[0.0012154538867021488, 0.0011601652050475504, 0.0013464592497753488, 0.0016573918877508152, 0.0013124233413360673, 0.00460093150137585, 0.0012316286129961636, 0.0012688673397695477, 0.001981773614798757, 0.0011749997264070605, 0.0013192704328420487, 0.0013231921369548547, 0.0013588925913526591, 0.0014934056142175739, 0.0011821661372034048, 0.0012483865906357426, 0.001433449680916965, 0.0012769209094006908, 0.0013947734339374372, 0.0012274188418152996, 0.001172990612262352, 0.0011610555441372774, 0.0011723522514909166, 0.0012443983181252736, 0.001155449270630594, 0.0011625389557924461, 0.0011626705474389548, 0.001265297770838846, 0.0011417281348258257, 0.0012559300680136816, 0.0012827961366962302, 0.0011624662491323595, 0.0011773710218469866, 0.0012707059769044545, 0.0012599262038499794, 0.0013568597972731698, 0.0012676459075671366, 0.0011822508610996672, 0.0012161237484013493, 0.0012927112719890747, 0.0011512418859638274, 0.0011315311368724163, 0.0011500408634839748, 0.0011440085919192907, 0.001161750842584297, 0.0011546153638681228, 0.0011507536136460576, 0.0013018572724170306, 0.00130386597646231, 0.0011876290002626113, 0.0011647540694949303, 0.0011546948623039168, 0.0012840522500813347, 0.001379519820065153, 0.0011988540698604827, 0.0013236697493332692, 0.0011670092724009671, 0.0011883504789280282, 0.00114539315902882, 0.0011696346125311472, 0.0011566153182436458, 0.0015186669982292435, 0.0014474399530709807, 0.0014089707480955192, 0.0015174241350244054, 0.001625780955443836, 0.0014054668850307776, 0.0013889033633114939, 0.001868691567903046, 0.002437951908425682, 0.00132066156683405, 0.0013605302294970236, 0.0013452146826735275, 0.0013381056355270134, 0.0012945226811676878, 0.0014521552048708227, 0.0016119455897503278, 0.0015727801814632999, 0.0013577609312381935, 0.003565696319459345, 0.0012533482725054703, 0.001225400022866035, 0.0013087017736821012, 0.0012903657942925665, 0.0014372105228672312, 0.0013430003203790295, 0.0014161062956025655, 0.0014294562490910969, 0.00130724872790531, 0.001263135271570222, 0.001259874954650348, 0.00122323084294542, 0.001513046387117356, 0.001520289441159125, 0.001304091812587928, 0.001254388905498524, 0.001274833209314492, 0.0014146686975605959, 0.0014906736052876643, 0.0016013401165214736, 0.0026389023500256415, 0.001563311811688161, 0.0014986946047175415, 0.001704422369880905, 0.0014020323720868937, 0.0013404589303400974, 0.0013349143257581216, 0.0012450647204681191, 0.0012660538155022402, 0.001320452978942803, 0.002504541581973087, 0.001133490348408042, 0.0013107268849041225, 0.001321520258369314, 0.0012606008351932084, 0.0013257242809591252, 0.0015730060918559862, 0.001386391930282116, 0.0013905783714509982, 0.0015498296503887274, 0.0019201403971068388, 0.001721341604215288, 0.0028981440917177255, 0.0016968124167084, 0.0013011325811317495, 0.002902954091252976, 0.0026439198612257147, 0.0018952047224914611, 0.001547056442494829, 0.0026407365140310207, 0.0018934683933810785, 0.0014568636746167443, 0.0012338687692833848, 0.0017078184430639064, 0.002134247071131371, 0.0024487555600962666, 0.0021721658129028455, 0.0012199556736578775, 0.0012829373482354852, 0.0012516975105025393, 0.001645400952331202, 0.004348932952230233, 0.0022543502543771335, 0.00157226120706561, 0.0020684492774307728, 0.0012931839081047232, 0.0012111467915738737, 0.0012861904417446187, 0.0012413438133418906, 0.0037895271630394595, 0.001274924514122134, 0.0018429614192004814, 0.0019296470455565425, 0.003623632835982324, 0.0023096151633707936, 0.0024288243960675806, 0.0014613527441830483, 0.001429085998774268]
[822.73791785986, 861.9462087375858, 742.6886481464968, 603.3576050363458, 761.9492647715214, 217.34729145629808, 811.9330693100054, 788.104452417871, 504.5985033469864, 851.0640279532843, 757.9947030615566, 755.7481427461946, 735.8933342955289, 669.6104464050248, 845.9047916612238, 801.0339164975719, 697.6177910621243, 783.133859456761, 716.9623221005909, 814.7178175308463, 852.5217419014938, 861.285237428542, 852.985950876342, 803.6012146870564, 865.4642184803522, 860.1862286140329, 860.0888722972527, 790.3277971769735, 875.8652515404232, 796.2226763004024, 779.5470935665928, 860.2400291160101, 849.3499342554414, 786.9641114273211, 793.697279209117, 736.9958207986282, 788.8638254819896, 845.8441714052574, 822.2847397845377, 773.5679433361138, 868.627186164959, 883.7582700234199, 869.5343198245706, 874.1193091236412, 860.7697652067247, 866.089289380197, 868.9957503862113, 768.1333593070447, 766.949991833694, 842.0137936837831, 858.5503379555719, 866.029660861866, 778.784508135598, 724.8899112973754, 834.1298788069973, 755.4754503558752, 856.8912207035273, 841.5025850808485, 873.0626615998833, 854.9678585827334, 864.5916963286716, 658.4722004007422, 690.874946403363, 709.7379426447868, 659.0115294191736, 615.0890110082519, 711.5073365660277, 719.9924965375195, 535.1337894258019, 410.1803635026395, 757.1962606568808, 735.0075568476648, 743.375769592824, 747.3251538965006, 772.4854995186164, 688.6316260450656, 620.3683339925194, 635.8167605275961, 736.5066831670155, 280.4501310284401, 797.8628302578487, 816.0600467928369, 764.1160271269805, 774.9740456722527, 695.7922893613405, 744.6014604953884, 706.1616794624102, 699.5667063163621, 764.9653647797885, 791.6808456760814, 793.7295652309631, 817.5071825299124, 660.9182696012329, 657.7694831831253, 766.8171752535831, 797.20092836964, 784.4163398737658, 706.879286807126, 670.8376645650903, 624.4769550720179, 378.94543539676005, 639.6676546057296, 667.2473476932745, 586.7090327322295, 713.250292867006, 746.0131581549334, 749.1117450043711, 803.171099108824, 789.8558400562954, 757.3158726186766, 399.27466455246326, 882.2307145398055, 762.9354456044047, 756.7042530501552, 793.2725190101378, 754.3046577351117, 635.72544644128, 721.2967546605026, 719.1252363263429, 645.2322032612943, 520.7952509653693, 580.9422124877254, 345.0484062741344, 589.3403361226415, 768.5611862322143, 344.4766842896847, 378.22628993618684, 527.6474821598096, 646.3888275384254, 378.6822330386624, 528.1313400823905, 686.4060223500795, 810.4589603810033, 585.5423356395853, 468.54931349157107, 408.3706909319636, 460.37001137754623, 819.7019134323388, 779.4612896532874, 798.9150666270109, 607.7546014442263, 229.94146172963576, 443.5867931606286, 636.0266319019282, 483.4539627880569, 773.2852177735411, 825.6637485704846, 777.4898394079004, 805.5785909206286, 263.8851648177478, 784.3601632278309, 542.6049561220998, 518.2294877722487, 275.96614923843737, 432.97256437325206, 411.721819666775, 684.2974798388175, 699.7479513883025]
Elapsed: 0.06687702521345802~0.024985391259832984
Time per graph: 0.0015364615995231928~0.0005812219069825991
Speed: 704.8454709136943~153.68528697775156
Total Time: 0.0621
best val loss: 0.4047783613204956 test_score: 0.7674

Testing...
Test loss: 0.5795 score: 0.7674 time: 0.05s
test Score 0.7674
Epoch Time List: [0.46641524706501514, 0.25219002191442996, 0.28882176987826824, 0.3776581770507619, 0.46515969920437783, 0.4401430869475007, 0.2936167569132522, 0.2511678319424391, 0.2859513961011544, 0.2375560860382393, 0.24988426105119288, 0.2707818109774962, 0.25040851696394384, 0.2446277568815276, 0.236584790982306, 0.2407670581014827, 0.26387072599027306, 0.27344838599674404, 0.25110868900083005, 0.24327741714660078, 0.24625904206186533, 0.24485349492169917, 0.23820248700212687, 0.24892146897036582, 0.23287157400045544, 0.23151225305628031, 0.24809013109188527, 0.23380675492808223, 0.23224733700044453, 0.23901306395418942, 0.24298432702198625, 0.23653675499372184, 0.22964637191034853, 0.23838811297900975, 0.24136708211153746, 0.24371832190081477, 0.2390701969852671, 0.23505068896338344, 0.24423011194448918, 0.24452908511739224, 0.2458609399618581, 0.2331586549989879, 0.24197144410572946, 0.2362105471547693, 0.23241896578110754, 0.23569558595772833, 0.2345382209168747, 0.25657204899471253, 0.25269452191423625, 0.23530512291472405, 0.2336516350042075, 0.23301243293099105, 0.24276639602612704, 0.24967077490873635, 0.23964230401907116, 0.2467238570097834, 0.23413122992496938, 0.23462586395908147, 0.24390040000434965, 0.2462419499643147, 0.2362316041253507, 0.28699155896902084, 0.2638365312013775, 0.277920680004172, 0.32201675593387336, 0.26626937789842486, 0.3198564890772104, 0.27086996904108673, 0.2892608599504456, 0.33854899602010846, 0.28701814101077616, 0.2704275579890236, 0.274724252987653, 0.2782532050041482, 0.4392861109226942, 0.3426497569307685, 0.29169259301852435, 0.2776578238699585, 0.2564922060118988, 0.3740790809970349, 0.3039485542103648, 0.25244893797207624, 0.26115182298235595, 0.26283224287908524, 0.2980102519504726, 0.27779098495375365, 0.280817475868389, 0.28652364399749786, 0.27876294404268265, 0.26148250000551343, 0.25350258499383926, 0.2546298940433189, 0.29925261600874364, 0.36205579712986946, 0.3228044629795477, 0.32285961089655757, 0.31096854305360466, 0.3058903091587126, 0.31543029088061303, 0.31061533105093986, 0.3651299389312044, 0.3036766959121451, 0.31256263912655413, 0.3231566270114854, 0.3194344377843663, 0.30642396199982613, 0.3094292739406228, 0.31613732303958386, 0.2675779319833964, 0.2659426929894835, 0.3452928850892931, 0.2810627321014181, 0.28243021795060486, 0.27800912491511554, 0.29410009493585676, 0.28697767516132444, 0.2855175700969994, 0.2699646418914199, 0.29788875789381564, 0.3750634618336335, 0.3746301119681448, 0.48848223488312215, 0.394437070004642, 0.3156374648679048, 0.44672213797457516, 0.43897390796337277, 0.5129031769465655, 0.3859038329683244, 0.3179394629551098, 0.3987551759928465, 0.3385371210752055, 0.4209076319821179, 0.36939337314106524, 0.30868458503391594, 0.4161502799252048, 0.4482192441355437, 0.7464608898153529, 0.4433628829428926, 0.39555891102645546, 0.3887913479702547, 0.5704187850933522, 0.4868862939765677, 0.35962873196695, 0.30472967587411404, 0.32671312510501593, 0.27729998412542045, 0.2636644438607618, 0.2695827040588483, 0.2724816290428862, 0.3949749340536073, 0.3084097420796752, 0.30231600091792643, 0.30124477099161595, 0.40229804103728384, 0.3482721949694678, 0.3773647010093555, 0.3640767838805914, 0.2975949359824881]
Total Epoch List: [93, 65]
Total Time List: [0.06712407304439694, 0.06213423900771886]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbce350>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8299;  Loss pred: 0.7979; Loss self: 3.2049; time: 0.20s
Val loss: 0.7990 score: 0.5227 time: 0.05s
Test loss: 0.7808 score: 0.4419 time: 0.10s
Epoch 2/1000, LR 0.000000
Train loss: 0.8299;  Loss pred: 0.7979; Loss self: 3.2049; time: 0.19s
Val loss: 0.7684 score: 0.5227 time: 0.05s
Test loss: 0.7566 score: 0.4419 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8059;  Loss pred: 0.7738; Loss self: 3.2107; time: 0.18s
Val loss: 0.7289 score: 0.5455 time: 0.05s
Test loss: 0.7289 score: 0.4651 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.7762;  Loss pred: 0.7439; Loss self: 3.2302; time: 0.17s
Val loss: 0.6951 score: 0.5682 time: 0.05s
Test loss: 0.7077 score: 0.5349 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.7459;  Loss pred: 0.7133; Loss self: 3.2589; time: 0.17s
Val loss: 0.6701 score: 0.5909 time: 0.05s
Test loss: 0.6839 score: 0.6047 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.7133;  Loss pred: 0.6805; Loss self: 3.2839; time: 0.17s
Val loss: 0.6666 score: 0.5909 time: 0.06s
Test loss: 0.6584 score: 0.6744 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6852;  Loss pred: 0.6522; Loss self: 3.2987; time: 0.19s
Val loss: 0.6728 score: 0.5682 time: 0.08s
Test loss: 0.6492 score: 0.6512 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6653;  Loss pred: 0.6324; Loss self: 3.2892; time: 0.19s
Val loss: 0.6753 score: 0.5455 time: 0.06s
Test loss: 0.6372 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6508;  Loss pred: 0.6181; Loss self: 3.2627; time: 0.18s
Val loss: 0.6653 score: 0.6364 time: 0.06s
Test loss: 0.6134 score: 0.6512 time: 0.07s
Epoch 10/1000, LR 0.000240
Train loss: 0.6328;  Loss pred: 0.6003; Loss self: 3.2423; time: 0.18s
Val loss: 0.6762 score: 0.5227 time: 0.09s
Test loss: 0.6022 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6187;  Loss pred: 0.5866; Loss self: 3.2134; time: 0.19s
Val loss: 0.6749 score: 0.5682 time: 0.05s
Test loss: 0.5859 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.6038;  Loss pred: 0.5717; Loss self: 3.2063; time: 0.19s
Val loss: 0.6619 score: 0.5682 time: 0.05s
Test loss: 0.5542 score: 0.7209 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.5780;  Loss pred: 0.5459; Loss self: 3.2068; time: 0.23s
Val loss: 0.6515 score: 0.5909 time: 0.06s
Test loss: 0.5245 score: 0.7907 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.5486;  Loss pred: 0.5165; Loss self: 3.2081; time: 0.21s
Val loss: 0.6403 score: 0.5909 time: 0.05s
Test loss: 0.5002 score: 0.7907 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.5240;  Loss pred: 0.4920; Loss self: 3.2035; time: 0.21s
Val loss: 0.6283 score: 0.6364 time: 0.04s
Test loss: 0.4757 score: 0.8605 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.5041;  Loss pred: 0.4722; Loss self: 3.1880; time: 0.19s
Val loss: 0.6226 score: 0.6364 time: 0.05s
Test loss: 0.4564 score: 0.8837 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.4802;  Loss pred: 0.4487; Loss self: 3.1514; time: 0.21s
Val loss: 0.6210 score: 0.6136 time: 0.06s
Test loss: 0.4391 score: 0.8837 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.4548;  Loss pred: 0.4239; Loss self: 3.0936; time: 0.18s
Val loss: 0.6132 score: 0.6136 time: 0.06s
Test loss: 0.4178 score: 0.9070 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.4289;  Loss pred: 0.3987; Loss self: 3.0263; time: 0.18s
Val loss: 0.5982 score: 0.6136 time: 0.05s
Test loss: 0.3887 score: 0.9070 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.4035;  Loss pred: 0.3739; Loss self: 2.9587; time: 0.18s
Val loss: 0.5843 score: 0.6364 time: 0.06s
Test loss: 0.3646 score: 0.9302 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.3769;  Loss pred: 0.3480; Loss self: 2.8926; time: 0.19s
Val loss: 0.5723 score: 0.6818 time: 0.05s
Test loss: 0.3489 score: 0.9535 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.3503;  Loss pred: 0.3220; Loss self: 2.8266; time: 0.17s
Val loss: 0.5596 score: 0.6818 time: 0.05s
Test loss: 0.3327 score: 0.9302 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.3268;  Loss pred: 0.2990; Loss self: 2.7748; time: 0.17s
Val loss: 0.5453 score: 0.7045 time: 0.05s
Test loss: 0.3131 score: 0.9302 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.3051;  Loss pred: 0.2776; Loss self: 2.7468; time: 0.17s
Val loss: 0.5326 score: 0.7273 time: 0.05s
Test loss: 0.3041 score: 0.9302 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.2862;  Loss pred: 0.2590; Loss self: 2.7217; time: 0.19s
Val loss: 0.5206 score: 0.7045 time: 0.05s
Test loss: 0.2981 score: 0.9070 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.2678;  Loss pred: 0.2409; Loss self: 2.6879; time: 0.17s
Val loss: 0.5106 score: 0.7045 time: 0.05s
Test loss: 0.2866 score: 0.9070 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.2494;  Loss pred: 0.2229; Loss self: 2.6450; time: 0.19s
Val loss: 0.5027 score: 0.7045 time: 0.05s
Test loss: 0.2691 score: 0.9302 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.2323;  Loss pred: 0.2063; Loss self: 2.6015; time: 0.18s
Val loss: 0.4957 score: 0.7045 time: 0.05s
Test loss: 0.2620 score: 0.9302 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.2181;  Loss pred: 0.1924; Loss self: 2.5649; time: 0.19s
Val loss: 0.4894 score: 0.7045 time: 0.09s
Test loss: 0.2567 score: 0.9302 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.2049;  Loss pred: 0.1796; Loss self: 2.5369; time: 0.21s
Val loss: 0.4834 score: 0.7045 time: 0.07s
Test loss: 0.2500 score: 0.9302 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.1934;  Loss pred: 0.1682; Loss self: 2.5239; time: 0.17s
Val loss: 0.4768 score: 0.7500 time: 0.05s
Test loss: 0.2374 score: 0.9302 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.1819;  Loss pred: 0.1566; Loss self: 2.5266; time: 0.22s
Val loss: 0.4707 score: 0.7727 time: 0.06s
Test loss: 0.2254 score: 0.9302 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.1709;  Loss pred: 0.1455; Loss self: 2.5382; time: 0.18s
Val loss: 0.4658 score: 0.7727 time: 0.06s
Test loss: 0.2229 score: 0.9302 time: 0.07s
Epoch 34/1000, LR 0.000270
Train loss: 0.1606;  Loss pred: 0.1351; Loss self: 2.5520; time: 0.19s
Val loss: 0.4624 score: 0.7727 time: 0.05s
Test loss: 0.2247 score: 0.9302 time: 0.11s
Epoch 35/1000, LR 0.000270
Train loss: 0.1517;  Loss pred: 0.1261; Loss self: 2.5672; time: 0.20s
Val loss: 0.4574 score: 0.7727 time: 0.05s
Test loss: 0.2189 score: 0.9302 time: 0.10s
Epoch 36/1000, LR 0.000270
Train loss: 0.1435;  Loss pred: 0.1177; Loss self: 2.5836; time: 0.22s
Val loss: 0.4501 score: 0.8409 time: 0.05s
Test loss: 0.2090 score: 0.9302 time: 0.10s
Epoch 37/1000, LR 0.000270
Train loss: 0.1355;  Loss pred: 0.1096; Loss self: 2.5956; time: 0.19s
Val loss: 0.4423 score: 0.8409 time: 0.05s
Test loss: 0.1964 score: 0.9302 time: 0.06s
Epoch 38/1000, LR 0.000270
Train loss: 0.1279;  Loss pred: 0.1019; Loss self: 2.6019; time: 0.18s
Val loss: 0.4354 score: 0.8409 time: 0.06s
Test loss: 0.1909 score: 0.9302 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.1208;  Loss pred: 0.0948; Loss self: 2.6019; time: 0.18s
Val loss: 0.4295 score: 0.8409 time: 0.05s
Test loss: 0.1908 score: 0.9302 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.1141;  Loss pred: 0.0881; Loss self: 2.5982; time: 0.19s
Val loss: 0.4251 score: 0.8409 time: 0.06s
Test loss: 0.1930 score: 0.9302 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.1078;  Loss pred: 0.0819; Loss self: 2.5928; time: 0.18s
Val loss: 0.4207 score: 0.8409 time: 0.06s
Test loss: 0.1905 score: 0.9302 time: 0.09s
Epoch 42/1000, LR 0.000269
Train loss: 0.1014;  Loss pred: 0.0756; Loss self: 2.5883; time: 0.20s
Val loss: 0.4161 score: 0.8409 time: 0.06s
Test loss: 0.1818 score: 0.9302 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0951;  Loss pred: 0.0693; Loss self: 2.5856; time: 0.19s
Val loss: 0.4133 score: 0.8409 time: 0.06s
Test loss: 0.1719 score: 0.9535 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.0894;  Loss pred: 0.0635; Loss self: 2.5845; time: 0.19s
Val loss: 0.4135 score: 0.8409 time: 0.05s
Test loss: 0.1658 score: 0.9535 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0843;  Loss pred: 0.0585; Loss self: 2.5847; time: 0.21s
Val loss: 0.4154 score: 0.8409 time: 0.05s
Test loss: 0.1621 score: 0.9535 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0797;  Loss pred: 0.0538; Loss self: 2.5855; time: 0.19s
Val loss: 0.4175 score: 0.8409 time: 0.05s
Test loss: 0.1590 score: 0.9535 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0755;  Loss pred: 0.0497; Loss self: 2.5877; time: 0.20s
Val loss: 0.4199 score: 0.8409 time: 0.07s
Test loss: 0.1551 score: 0.9535 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0716;  Loss pred: 0.0457; Loss self: 2.5916; time: 0.23s
Val loss: 0.4234 score: 0.8409 time: 0.09s
Test loss: 0.1512 score: 0.9535 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0679;  Loss pred: 0.0420; Loss self: 2.5972; time: 0.17s
Val loss: 0.4288 score: 0.8409 time: 0.10s
Test loss: 0.1484 score: 0.9535 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0644;  Loss pred: 0.0383; Loss self: 2.6039; time: 0.19s
Val loss: 0.4347 score: 0.8182 time: 0.10s
Test loss: 0.1476 score: 0.9535 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0611;  Loss pred: 0.0350; Loss self: 2.6114; time: 0.16s
Val loss: 0.4408 score: 0.8182 time: 0.08s
Test loss: 0.1487 score: 0.9535 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0582;  Loss pred: 0.0320; Loss self: 2.6196; time: 0.17s
Val loss: 0.4462 score: 0.8182 time: 0.05s
Test loss: 0.1511 score: 0.9535 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0555;  Loss pred: 0.0293; Loss self: 2.6277; time: 0.20s
Val loss: 0.4512 score: 0.8182 time: 0.05s
Test loss: 0.1542 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 10 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0532;  Loss pred: 0.0268; Loss self: 2.6356; time: 0.23s
Val loss: 0.4569 score: 0.8182 time: 0.04s
Test loss: 0.1574 score: 0.9535 time: 0.17s
     INFO: Early stopping counter 11 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0510;  Loss pred: 0.0246; Loss self: 2.6423; time: 0.17s
Val loss: 0.4648 score: 0.8182 time: 0.05s
Test loss: 0.1597 score: 0.9535 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0491;  Loss pred: 0.0226; Loss self: 2.6487; time: 0.19s
Val loss: 0.4723 score: 0.8182 time: 0.06s
Test loss: 0.1613 score: 0.9535 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0473;  Loss pred: 0.0208; Loss self: 2.6546; time: 0.17s
Val loss: 0.4799 score: 0.8409 time: 0.05s
Test loss: 0.1627 score: 0.9535 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0458;  Loss pred: 0.0192; Loss self: 2.6600; time: 0.18s
Val loss: 0.4868 score: 0.8409 time: 0.05s
Test loss: 0.1639 score: 0.9535 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0445;  Loss pred: 0.0178; Loss self: 2.6648; time: 0.17s
Val loss: 0.4934 score: 0.8409 time: 0.05s
Test loss: 0.1647 score: 0.9535 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0433;  Loss pred: 0.0166; Loss self: 2.6695; time: 0.19s
Val loss: 0.5005 score: 0.8409 time: 0.05s
Test loss: 0.1655 score: 0.9535 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0423;  Loss pred: 0.0156; Loss self: 2.6740; time: 0.19s
Val loss: 0.5069 score: 0.8409 time: 0.06s
Test loss: 0.1670 score: 0.9535 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0415;  Loss pred: 0.0147; Loss self: 2.6781; time: 0.19s
Val loss: 0.5130 score: 0.8409 time: 0.06s
Test loss: 0.1692 score: 0.9535 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0407;  Loss pred: 0.0139; Loss self: 2.6822; time: 0.20s
Val loss: 0.5185 score: 0.8409 time: 0.06s
Test loss: 0.1720 score: 0.9535 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 042,   Train_Loss: 0.0951,   Val_Loss: 0.4133,   Val_Precision: 0.8261,   Val_Recall: 0.8636,   Val_accuracy: 0.8444,   Val_Score: 0.8409,   Val_Loss: 0.4133,   Test_Precision: 0.9524,   Test_Recall: 0.9524,   Test_accuracy: 0.9524,   Test_Score: 0.9535,   Test_loss: 0.1719


[0.053479971014894545, 0.05104726902209222, 0.059244206990115345, 0.07292524306103587, 0.05774662701878697, 0.2024409860605374, 0.0541916589718312, 0.055830162949860096, 0.08719803905114532, 0.051699987961910665, 0.058047899045050144, 0.05822045402601361, 0.059791274019517004, 0.06570984702557325, 0.05201531003694981, 0.05492900998797268, 0.06307178596034646, 0.05618452001363039, 0.061370031093247235, 0.05400642903987318, 0.051611586939543486, 0.051086443942040205, 0.051583499065600336, 0.05475352599751204, 0.050839767907746136, 0.051151714054867625, 0.05115750408731401, 0.05567310191690922, 0.05023603793233633, 0.05526092299260199, 0.05644303001463413, 0.05114851496182382, 0.05180432496126741, 0.055911062983796, 0.055436752969399095, 0.059701831080019474, 0.05577641993295401, 0.052019037888385355, 0.05350944492965937, 0.05687929596751928, 0.050654642982408404, 0.04978737002238631, 0.050601797993294895, 0.05033637804444879, 0.05111703707370907, 0.0508030760101974, 0.05063315900042653, 0.057281719986349344, 0.05737010296434164, 0.0522556760115549, 0.05124917905777693, 0.050806573941372335, 0.05649829900357872, 0.06069887208286673, 0.05274957907386124, 0.058241468970663846, 0.05134840798564255, 0.05228742107283324, 0.05039729899726808, 0.05146392295137048, 0.050891074002720416, 0.06682134792208672, 0.06368735793512315, 0.06199471291620284, 0.06676666194107383, 0.07153436203952879, 0.061840542941354215, 0.06111174798570573, 0.08222242898773402, 0.10726988397073, 0.058109108940698206, 0.05986333009786904, 0.05918944603763521, 0.05887664796318859, 0.05695899797137827, 0.0638948290143162, 0.07092560594901443, 0.06920232798438519, 0.05974148097448051, 0.15689063805621117, 0.05514732399024069, 0.05391760100610554, 0.05758287804201245, 0.056776094948872924, 0.06323726300615817, 0.0590920140966773, 0.06230867700651288, 0.06289607496000826, 0.05751894402783364, 0.055577951949089766, 0.05543449800461531, 0.05382215708959848, 0.06657404103316367, 0.06537244596984237, 0.0560759479412809, 0.053938722936436534, 0.05481782800052315, 0.060830753995105624, 0.06409896502736956, 0.06885762501042336, 0.11347280105110258, 0.06722240790259093, 0.06444386800285429, 0.07329016190487891, 0.06028739199973643, 0.05763973400462419, 0.057401316007599235, 0.05353778298012912, 0.05444031406659633, 0.056779478094540536, 0.10769528802484274, 0.048740084981545806, 0.05636125605087727, 0.05682537110988051, 0.054205835913307965, 0.05700614408124238, 0.0676392619498074, 0.059614853002130985, 0.05979486997239292, 0.06664267496671528, 0.08256603707559407, 0.07401768898125738, 0.1246201959438622, 0.0729629339184612, 0.05594870098866522, 0.12482702592387795, 0.11368855403270572, 0.08149380306713283, 0.06652342702727765, 0.11355167010333389, 0.08141914091538638, 0.06264513800852001, 0.053056357079185545, 0.07343619305174798, 0.09177262405864894, 0.10529648908413947, 0.09340312995482236, 0.05245809396728873, 0.05516630597412586, 0.053822992951609194, 0.07075224095024168, 0.18700411694590002, 0.09693706093821675, 0.06760723190382123, 0.08894331892952323, 0.0556069080485031, 0.05207931203767657, 0.0553061889950186, 0.0533777839737013, 0.16294966801069677, 0.05482175410725176, 0.0792473410256207, 0.08297482295893133, 0.15581621194723994, 0.09931345202494413, 0.10443944903090596, 0.06283816799987108, 0.06145069794729352, 0.10489930608309805, 0.07697644201107323, 0.07288834289647639, 0.061393255018629134, 0.058412168989889324, 0.07677510299254209, 0.12283545499667525, 0.07340277195908129, 0.07142099796328694, 0.05637242493685335, 0.055202414048835635, 0.05830161308404058, 0.05298083508387208, 0.05842699494678527, 0.07124770304653794, 0.0820322580402717, 0.07691261498257518, 0.06932122504804283, 0.05965075199492276, 0.0655207330128178, 0.05765950004570186, 0.05600687291007489, 0.05798088607843965, 0.057691717986017466, 0.058424211922101676, 0.058747963048517704, 0.06649074598681182, 0.10003620502538979, 0.0609743440290913, 0.06601833400782198, 0.08502044097986072, 0.05940548307262361, 0.07186078501399606, 0.11702596500981599, 0.1019548230106011, 0.1019634740659967, 0.06061298691201955, 0.09608728298917413, 0.05975583603139967, 0.07005720201414078, 0.09447768202517182, 0.056131910998374224, 0.05491798894945532, 0.05828040593769401, 0.06839975295588374, 0.07063995499629527, 0.06659189402125776, 0.06601406703703105, 0.09074433404020965, 0.10772475297562778, 0.08433574403170496, 0.05542238196358085, 0.17922195699065924, 0.17538658494595438, 0.056682436959818006, 0.05934195895679295, 0.05839379003737122, 0.05763856694102287, 0.07782818307168782, 0.06439302593935281, 0.06317951797973365, 0.0632131980964914, 0.06279258197173476]
[0.0012154538867021488, 0.0011601652050475504, 0.0013464592497753488, 0.0016573918877508152, 0.0013124233413360673, 0.00460093150137585, 0.0012316286129961636, 0.0012688673397695477, 0.001981773614798757, 0.0011749997264070605, 0.0013192704328420487, 0.0013231921369548547, 0.0013588925913526591, 0.0014934056142175739, 0.0011821661372034048, 0.0012483865906357426, 0.001433449680916965, 0.0012769209094006908, 0.0013947734339374372, 0.0012274188418152996, 0.001172990612262352, 0.0011610555441372774, 0.0011723522514909166, 0.0012443983181252736, 0.001155449270630594, 0.0011625389557924461, 0.0011626705474389548, 0.001265297770838846, 0.0011417281348258257, 0.0012559300680136816, 0.0012827961366962302, 0.0011624662491323595, 0.0011773710218469866, 0.0012707059769044545, 0.0012599262038499794, 0.0013568597972731698, 0.0012676459075671366, 0.0011822508610996672, 0.0012161237484013493, 0.0012927112719890747, 0.0011512418859638274, 0.0011315311368724163, 0.0011500408634839748, 0.0011440085919192907, 0.001161750842584297, 0.0011546153638681228, 0.0011507536136460576, 0.0013018572724170306, 0.00130386597646231, 0.0011876290002626113, 0.0011647540694949303, 0.0011546948623039168, 0.0012840522500813347, 0.001379519820065153, 0.0011988540698604827, 0.0013236697493332692, 0.0011670092724009671, 0.0011883504789280282, 0.00114539315902882, 0.0011696346125311472, 0.0011566153182436458, 0.0015186669982292435, 0.0014474399530709807, 0.0014089707480955192, 0.0015174241350244054, 0.001625780955443836, 0.0014054668850307776, 0.0013889033633114939, 0.001868691567903046, 0.002437951908425682, 0.00132066156683405, 0.0013605302294970236, 0.0013452146826735275, 0.0013381056355270134, 0.0012945226811676878, 0.0014521552048708227, 0.0016119455897503278, 0.0015727801814632999, 0.0013577609312381935, 0.003565696319459345, 0.0012533482725054703, 0.001225400022866035, 0.0013087017736821012, 0.0012903657942925665, 0.0014372105228672312, 0.0013430003203790295, 0.0014161062956025655, 0.0014294562490910969, 0.00130724872790531, 0.001263135271570222, 0.001259874954650348, 0.00122323084294542, 0.001513046387117356, 0.001520289441159125, 0.001304091812587928, 0.001254388905498524, 0.001274833209314492, 0.0014146686975605959, 0.0014906736052876643, 0.0016013401165214736, 0.0026389023500256415, 0.001563311811688161, 0.0014986946047175415, 0.001704422369880905, 0.0014020323720868937, 0.0013404589303400974, 0.0013349143257581216, 0.0012450647204681191, 0.0012660538155022402, 0.001320452978942803, 0.002504541581973087, 0.001133490348408042, 0.0013107268849041225, 0.001321520258369314, 0.0012606008351932084, 0.0013257242809591252, 0.0015730060918559862, 0.001386391930282116, 0.0013905783714509982, 0.0015498296503887274, 0.0019201403971068388, 0.001721341604215288, 0.0028981440917177255, 0.0016968124167084, 0.0013011325811317495, 0.002902954091252976, 0.0026439198612257147, 0.0018952047224914611, 0.001547056442494829, 0.0026407365140310207, 0.0018934683933810785, 0.0014568636746167443, 0.0012338687692833848, 0.0017078184430639064, 0.002134247071131371, 0.0024487555600962666, 0.0021721658129028455, 0.0012199556736578775, 0.0012829373482354852, 0.0012516975105025393, 0.001645400952331202, 0.004348932952230233, 0.0022543502543771335, 0.00157226120706561, 0.0020684492774307728, 0.0012931839081047232, 0.0012111467915738737, 0.0012861904417446187, 0.0012413438133418906, 0.0037895271630394595, 0.001274924514122134, 0.0018429614192004814, 0.0019296470455565425, 0.003623632835982324, 0.0023096151633707936, 0.0024288243960675806, 0.0014613527441830483, 0.001429085998774268, 0.002439518746118559, 0.0017901498142110055, 0.0016950777417785207, 0.0014277501167123054, 0.0013584225346485888, 0.001785467511454467, 0.0028566384882947734, 0.0017070412083507278, 0.001660953441006673, 0.00131098662643845, 0.0012837770709031543, 0.0013558514670707112, 0.0012321124438109787, 0.0013587673243438435, 0.001656923326663673, 0.0019077269311691093, 0.0017886654647110507, 0.0016121215127451822, 0.001387226790579599, 0.0015237379770422744, 0.0013409186057139967, 0.0013024854165133695, 0.0013483926994985965, 0.001341667860139941, 0.0013587026028395739, 0.0013662316988027373, 0.0015462964182979492, 0.0023264233726834837, 0.0014180080006765418, 0.0015353100932051624, 0.0019772195576711797, 0.0013815228621540374, 0.0016711810468371178, 0.002721534069995721, 0.0023710423955953745, 0.002371243582930156, 0.0014096043467911523, 0.0022345879764924218, 0.0013896706053813876, 0.001629237256142809, 0.0021971553959342282, 0.0013053932790319587, 0.0012771625337082632, 0.001355358277620791, 0.0015906919292065986, 0.0016427896510766342, 0.001548648698168785, 0.0015352108613263036, 0.0021103333497723173, 0.0025052268133866924, 0.001961296372830348, 0.0012888926038042057, 0.004167952488154866, 0.0040787577894408, 0.0013181962083678605, 0.0013800455571347198, 0.0013579951171481678, 0.0013404317893261134, 0.001809957745853205, 0.0014975122311477397, 0.0014692911158077592, 0.0014700743743370091, 0.0014602926039938317]
[822.73791785986, 861.9462087375858, 742.6886481464968, 603.3576050363458, 761.9492647715214, 217.34729145629808, 811.9330693100054, 788.104452417871, 504.5985033469864, 851.0640279532843, 757.9947030615566, 755.7481427461946, 735.8933342955289, 669.6104464050248, 845.9047916612238, 801.0339164975719, 697.6177910621243, 783.133859456761, 716.9623221005909, 814.7178175308463, 852.5217419014938, 861.285237428542, 852.985950876342, 803.6012146870564, 865.4642184803522, 860.1862286140329, 860.0888722972527, 790.3277971769735, 875.8652515404232, 796.2226763004024, 779.5470935665928, 860.2400291160101, 849.3499342554414, 786.9641114273211, 793.697279209117, 736.9958207986282, 788.8638254819896, 845.8441714052574, 822.2847397845377, 773.5679433361138, 868.627186164959, 883.7582700234199, 869.5343198245706, 874.1193091236412, 860.7697652067247, 866.089289380197, 868.9957503862113, 768.1333593070447, 766.949991833694, 842.0137936837831, 858.5503379555719, 866.029660861866, 778.784508135598, 724.8899112973754, 834.1298788069973, 755.4754503558752, 856.8912207035273, 841.5025850808485, 873.0626615998833, 854.9678585827334, 864.5916963286716, 658.4722004007422, 690.874946403363, 709.7379426447868, 659.0115294191736, 615.0890110082519, 711.5073365660277, 719.9924965375195, 535.1337894258019, 410.1803635026395, 757.1962606568808, 735.0075568476648, 743.375769592824, 747.3251538965006, 772.4854995186164, 688.6316260450656, 620.3683339925194, 635.8167605275961, 736.5066831670155, 280.4501310284401, 797.8628302578487, 816.0600467928369, 764.1160271269805, 774.9740456722527, 695.7922893613405, 744.6014604953884, 706.1616794624102, 699.5667063163621, 764.9653647797885, 791.6808456760814, 793.7295652309631, 817.5071825299124, 660.9182696012329, 657.7694831831253, 766.8171752535831, 797.20092836964, 784.4163398737658, 706.879286807126, 670.8376645650903, 624.4769550720179, 378.94543539676005, 639.6676546057296, 667.2473476932745, 586.7090327322295, 713.250292867006, 746.0131581549334, 749.1117450043711, 803.171099108824, 789.8558400562954, 757.3158726186766, 399.27466455246326, 882.2307145398055, 762.9354456044047, 756.7042530501552, 793.2725190101378, 754.3046577351117, 635.72544644128, 721.2967546605026, 719.1252363263429, 645.2322032612943, 520.7952509653693, 580.9422124877254, 345.0484062741344, 589.3403361226415, 768.5611862322143, 344.4766842896847, 378.22628993618684, 527.6474821598096, 646.3888275384254, 378.6822330386624, 528.1313400823905, 686.4060223500795, 810.4589603810033, 585.5423356395853, 468.54931349157107, 408.3706909319636, 460.37001137754623, 819.7019134323388, 779.4612896532874, 798.9150666270109, 607.7546014442263, 229.94146172963576, 443.5867931606286, 636.0266319019282, 483.4539627880569, 773.2852177735411, 825.6637485704846, 777.4898394079004, 805.5785909206286, 263.8851648177478, 784.3601632278309, 542.6049561220998, 518.2294877722487, 275.96614923843737, 432.97256437325206, 411.721819666775, 684.2974798388175, 699.7479513883025, 409.9169156175037, 558.6124647566115, 589.9434435088348, 700.402674315804, 736.1479764164031, 560.0773991039388, 350.06179609269867, 585.8089395312012, 602.0638359338469, 762.7842876755305, 778.9514415431073, 737.5439156034394, 811.6142362031143, 735.961177520151, 603.5282284386493, 524.1840347597188, 559.0760372630914, 620.3006362077272, 720.8626641229937, 656.280814068242, 745.7574201288165, 767.7629149022686, 741.6237127150367, 745.3409518922933, 735.9962348714754, 731.9402710948112, 646.7065358016721, 429.8443747349906, 705.2146387911018, 651.3342186869678, 505.76072653146616, 723.8389080589111, 598.3792132471841, 367.4398241141888, 421.75542784796875, 421.71964415578753, 709.4189247333248, 447.509791746788, 719.5949861266263, 613.7841472932449, 455.1339435756209, 766.0526647889367, 782.985699632515, 737.8122939975765, 628.657241316851, 608.7206596076561, 645.7242376417969, 651.3763191696529, 473.85878638931115, 399.1654546632244, 509.8668481994383, 775.8598327342942, 239.92595953095798, 245.17268531826713, 758.6124081165134, 724.613759908203, 736.3796727782285, 746.0282634021523, 552.4990858439102, 667.7741785344678, 680.6003175553395, 680.2376923623278, 684.7942647008189]
Elapsed: 0.06899127206708225~0.025193132063754013
Time per graph: 0.0015909945543785523~0.000587193902248636
Speed: 681.2320473316257~153.91580456021867
Total Time: 0.0636
best val loss: 0.4132780134677887 test_score: 0.9535

Testing...
Test loss: 0.2090 score: 0.9302 time: 0.06s
test Score 0.9302
Epoch Time List: [0.46641524706501514, 0.25219002191442996, 0.28882176987826824, 0.3776581770507619, 0.46515969920437783, 0.4401430869475007, 0.2936167569132522, 0.2511678319424391, 0.2859513961011544, 0.2375560860382393, 0.24988426105119288, 0.2707818109774962, 0.25040851696394384, 0.2446277568815276, 0.236584790982306, 0.2407670581014827, 0.26387072599027306, 0.27344838599674404, 0.25110868900083005, 0.24327741714660078, 0.24625904206186533, 0.24485349492169917, 0.23820248700212687, 0.24892146897036582, 0.23287157400045544, 0.23151225305628031, 0.24809013109188527, 0.23380675492808223, 0.23224733700044453, 0.23901306395418942, 0.24298432702198625, 0.23653675499372184, 0.22964637191034853, 0.23838811297900975, 0.24136708211153746, 0.24371832190081477, 0.2390701969852671, 0.23505068896338344, 0.24423011194448918, 0.24452908511739224, 0.2458609399618581, 0.2331586549989879, 0.24197144410572946, 0.2362105471547693, 0.23241896578110754, 0.23569558595772833, 0.2345382209168747, 0.25657204899471253, 0.25269452191423625, 0.23530512291472405, 0.2336516350042075, 0.23301243293099105, 0.24276639602612704, 0.24967077490873635, 0.23964230401907116, 0.2467238570097834, 0.23413122992496938, 0.23462586395908147, 0.24390040000434965, 0.2462419499643147, 0.2362316041253507, 0.28699155896902084, 0.2638365312013775, 0.277920680004172, 0.32201675593387336, 0.26626937789842486, 0.3198564890772104, 0.27086996904108673, 0.2892608599504456, 0.33854899602010846, 0.28701814101077616, 0.2704275579890236, 0.274724252987653, 0.2782532050041482, 0.4392861109226942, 0.3426497569307685, 0.29169259301852435, 0.2776578238699585, 0.2564922060118988, 0.3740790809970349, 0.3039485542103648, 0.25244893797207624, 0.26115182298235595, 0.26283224287908524, 0.2980102519504726, 0.27779098495375365, 0.280817475868389, 0.28652364399749786, 0.27876294404268265, 0.26148250000551343, 0.25350258499383926, 0.2546298940433189, 0.29925261600874364, 0.36205579712986946, 0.3228044629795477, 0.32285961089655757, 0.31096854305360466, 0.3058903091587126, 0.31543029088061303, 0.31061533105093986, 0.3651299389312044, 0.3036766959121451, 0.31256263912655413, 0.3231566270114854, 0.3194344377843663, 0.30642396199982613, 0.3094292739406228, 0.31613732303958386, 0.2675779319833964, 0.2659426929894835, 0.3452928850892931, 0.2810627321014181, 0.28243021795060486, 0.27800912491511554, 0.29410009493585676, 0.28697767516132444, 0.2855175700969994, 0.2699646418914199, 0.29788875789381564, 0.3750634618336335, 0.3746301119681448, 0.48848223488312215, 0.394437070004642, 0.3156374648679048, 0.44672213797457516, 0.43897390796337277, 0.5129031769465655, 0.3859038329683244, 0.3179394629551098, 0.3987551759928465, 0.3385371210752055, 0.4209076319821179, 0.36939337314106524, 0.30868458503391594, 0.4161502799252048, 0.4482192441355437, 0.7464608898153529, 0.4433628829428926, 0.39555891102645546, 0.3887913479702547, 0.5704187850933522, 0.4868862939765677, 0.35962873196695, 0.30472967587411404, 0.32671312510501593, 0.27729998412542045, 0.2636644438607618, 0.2695827040588483, 0.2724816290428862, 0.3949749340536073, 0.3084097420796752, 0.30231600091792643, 0.30124477099161595, 0.40229804103728384, 0.3482721949694678, 0.3773647010093555, 0.3640767838805914, 0.2975949359824881, 0.34418397513218224, 0.3152769618900493, 0.3016161289997399, 0.27286415197886527, 0.2782950180117041, 0.3081957520917058, 0.3898321510059759, 0.314026664942503, 0.3025200602132827, 0.3178338260622695, 0.2891406189883128, 0.3010450159199536, 0.34182299894746393, 0.31184963101986796, 0.31509038305375725, 0.3196807081112638, 0.3397084168391302, 0.3054276950424537, 0.28252172190696, 0.2989803799428046, 0.2883263248950243, 0.2665383471176028, 0.267825971939601, 0.2709685460431501, 0.29131302796304226, 0.27985413500573486, 0.3052379790460691, 0.3281500729499385, 0.33882538694888353, 0.34121121698990464, 0.2969120249617845, 0.32829841400962323, 0.2968541618902236, 0.3462340630358085, 0.3429505149833858, 0.3654173630056903, 0.30054886417929083, 0.3251477931626141, 0.2835646689636633, 0.31976496207062155, 0.33143350202590227, 0.30512732616625726, 0.29936434293631464, 0.29815228504594415, 0.3202130129793659, 0.3073107150848955, 0.330803660210222, 0.3735833269311115, 0.35723126900848, 0.3995476249838248, 0.32512983004562557, 0.2723866750020534, 0.4295179439941421, 0.4464226000709459, 0.2743048390839249, 0.2998131619533524, 0.28141457098536193, 0.28300104290246964, 0.2911897999001667, 0.2995568229816854, 0.3130004049744457, 0.3118473159847781, 0.32217760500498116]
Total Epoch List: [93, 65, 63]
Total Time List: [0.06712407304439694, 0.06213423900771886, 0.06358535494655371]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbcdb10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7766;  Loss pred: 0.7446; Loss self: 3.1998; time: 0.18s
Val loss: 0.7179 score: 0.4651 time: 0.07s
Test loss: 0.7350 score: 0.4545 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7766;  Loss pred: 0.7446; Loss self: 3.1998; time: 0.16s
Val loss: 0.7073 score: 0.4419 time: 0.05s
Test loss: 0.7267 score: 0.5000 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.7518;  Loss pred: 0.7199; Loss self: 3.1904; time: 0.15s
Val loss: 0.6732 score: 0.5581 time: 0.06s
Test loss: 0.7168 score: 0.5227 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.7210;  Loss pred: 0.6894; Loss self: 3.1534; time: 0.16s
Val loss: 0.6518 score: 0.6512 time: 0.05s
Test loss: 0.7050 score: 0.4773 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6890;  Loss pred: 0.6577; Loss self: 3.1261; time: 0.16s
Val loss: 0.6406 score: 0.5814 time: 0.07s
Test loss: 0.6948 score: 0.5000 time: 0.07s
Epoch 6/1000, LR 0.000120
Train loss: 0.6595;  Loss pred: 0.6283; Loss self: 3.1243; time: 0.16s
Val loss: 0.6315 score: 0.6279 time: 0.05s
Test loss: 0.6893 score: 0.4773 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.6272;  Loss pred: 0.5959; Loss self: 3.1257; time: 0.18s
Val loss: 0.6242 score: 0.7442 time: 0.06s
Test loss: 0.6837 score: 0.6136 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6063;  Loss pred: 0.5752; Loss self: 3.1123; time: 0.18s
Val loss: 0.6178 score: 0.6512 time: 0.05s
Test loss: 0.6762 score: 0.6136 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.5685;  Loss pred: 0.5376; Loss self: 3.0933; time: 0.18s
Val loss: 0.6395 score: 0.5581 time: 0.06s
Test loss: 0.6884 score: 0.5227 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5427;  Loss pred: 0.5119; Loss self: 3.0747; time: 0.17s
Val loss: 0.5881 score: 0.6977 time: 0.06s
Test loss: 0.6600 score: 0.6591 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.5182;  Loss pred: 0.4875; Loss self: 3.0686; time: 0.21s
Val loss: 0.5752 score: 0.7209 time: 0.16s
Test loss: 0.6413 score: 0.6136 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.4641;  Loss pred: 0.4337; Loss self: 3.0416; time: 0.23s
Val loss: 0.5610 score: 0.6744 time: 0.10s
Test loss: 0.6424 score: 0.6364 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.4371;  Loss pred: 0.4069; Loss self: 3.0175; time: 0.18s
Val loss: 0.5144 score: 0.7442 time: 0.05s
Test loss: 0.6163 score: 0.7500 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.3978;  Loss pred: 0.3680; Loss self: 2.9824; time: 0.18s
Val loss: 0.5007 score: 0.7442 time: 0.06s
Test loss: 0.6041 score: 0.7500 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.3693;  Loss pred: 0.3400; Loss self: 2.9368; time: 0.18s
Val loss: 0.4991 score: 0.7907 time: 0.06s
Test loss: 0.5885 score: 0.7273 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.3290;  Loss pred: 0.3002; Loss self: 2.8764; time: 0.17s
Val loss: 0.5060 score: 0.7674 time: 0.06s
Test loss: 0.5860 score: 0.6818 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3081;  Loss pred: 0.2800; Loss self: 2.8146; time: 0.16s
Val loss: 0.4849 score: 0.7907 time: 0.08s
Test loss: 0.5628 score: 0.7273 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.2855;  Loss pred: 0.2582; Loss self: 2.7306; time: 0.16s
Val loss: 0.4726 score: 0.7907 time: 0.07s
Test loss: 0.5518 score: 0.7727 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.2728;  Loss pred: 0.2463; Loss self: 2.6504; time: 0.17s
Val loss: 0.4587 score: 0.7907 time: 0.06s
Test loss: 0.5521 score: 0.7273 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.2537;  Loss pred: 0.2278; Loss self: 2.5838; time: 0.15s
Val loss: 0.4468 score: 0.7907 time: 0.06s
Test loss: 0.5590 score: 0.7500 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.2389;  Loss pred: 0.2135; Loss self: 2.5329; time: 0.15s
Val loss: 0.4338 score: 0.8140 time: 0.05s
Test loss: 0.5475 score: 0.7500 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.2258;  Loss pred: 0.2007; Loss self: 2.5129; time: 0.25s
Val loss: 0.4230 score: 0.8140 time: 0.13s
Test loss: 0.5327 score: 0.7500 time: 0.14s
Epoch 23/1000, LR 0.000270
Train loss: 0.2122;  Loss pred: 0.1871; Loss self: 2.5074; time: 0.20s
Val loss: 0.4154 score: 0.8372 time: 0.06s
Test loss: 0.5187 score: 0.7727 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.1984;  Loss pred: 0.1733; Loss self: 2.5101; time: 0.17s
Val loss: 0.4084 score: 0.8372 time: 0.05s
Test loss: 0.5111 score: 0.7727 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.1854;  Loss pred: 0.1603; Loss self: 2.5120; time: 0.16s
Val loss: 0.3994 score: 0.8372 time: 0.06s
Test loss: 0.5057 score: 0.7500 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1741;  Loss pred: 0.1490; Loss self: 2.5091; time: 0.17s
Val loss: 0.3904 score: 0.8372 time: 0.06s
Test loss: 0.4973 score: 0.7500 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.1647;  Loss pred: 0.1397; Loss self: 2.5029; time: 0.18s
Val loss: 0.3817 score: 0.8372 time: 0.07s
Test loss: 0.4875 score: 0.7500 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.1561;  Loss pred: 0.1312; Loss self: 2.4934; time: 0.18s
Val loss: 0.3716 score: 0.8372 time: 0.07s
Test loss: 0.4787 score: 0.7500 time: 0.09s
Epoch 29/1000, LR 0.000270
Train loss: 0.1478;  Loss pred: 0.1230; Loss self: 2.4796; time: 0.18s
Val loss: 0.3632 score: 0.8372 time: 0.07s
Test loss: 0.4692 score: 0.7500 time: 0.05s
Epoch 30/1000, LR 0.000270
Train loss: 0.1399;  Loss pred: 0.1152; Loss self: 2.4657; time: 0.16s
Val loss: 0.3574 score: 0.8372 time: 0.08s
Test loss: 0.4596 score: 0.7273 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.1332;  Loss pred: 0.1086; Loss self: 2.4559; time: 0.16s
Val loss: 0.3505 score: 0.8605 time: 0.06s
Test loss: 0.4527 score: 0.7727 time: 0.05s
Epoch 32/1000, LR 0.000270
Train loss: 0.1275;  Loss pred: 0.1030; Loss self: 2.4499; time: 0.19s
Val loss: 0.3438 score: 0.8605 time: 0.06s
Test loss: 0.4470 score: 0.7955 time: 0.06s
Epoch 33/1000, LR 0.000270
Train loss: 0.1223;  Loss pred: 0.0979; Loss self: 2.4463; time: 0.18s
Val loss: 0.3386 score: 0.8605 time: 0.11s
Test loss: 0.4407 score: 0.7955 time: 0.11s
Epoch 34/1000, LR 0.000270
Train loss: 0.1170;  Loss pred: 0.0925; Loss self: 2.4437; time: 0.20s
Val loss: 0.3348 score: 0.8605 time: 0.11s
Test loss: 0.4340 score: 0.7955 time: 0.11s
Epoch 35/1000, LR 0.000270
Train loss: 0.1117;  Loss pred: 0.0873; Loss self: 2.4415; time: 0.20s
Val loss: 0.3313 score: 0.8605 time: 0.08s
Test loss: 0.4266 score: 0.8182 time: 0.10s
Epoch 36/1000, LR 0.000270
Train loss: 0.1067;  Loss pred: 0.0823; Loss self: 2.4369; time: 0.17s
Val loss: 0.3270 score: 0.8605 time: 0.06s
Test loss: 0.4200 score: 0.8182 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.1021;  Loss pred: 0.0778; Loss self: 2.4296; time: 0.16s
Val loss: 0.3209 score: 0.8605 time: 0.07s
Test loss: 0.4136 score: 0.7955 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.0978;  Loss pred: 0.0736; Loss self: 2.4196; time: 0.20s
Val loss: 0.3147 score: 0.8605 time: 0.07s
Test loss: 0.4074 score: 0.7955 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.0937;  Loss pred: 0.0696; Loss self: 2.4108; time: 0.18s
Val loss: 0.3079 score: 0.8605 time: 0.18s
Test loss: 0.4007 score: 0.7955 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.0899;  Loss pred: 0.0659; Loss self: 2.4041; time: 0.16s
Val loss: 0.2990 score: 0.8605 time: 0.13s
Test loss: 0.3958 score: 0.7955 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.0865;  Loss pred: 0.0625; Loss self: 2.3991; time: 0.16s
Val loss: 0.2876 score: 0.8605 time: 0.07s
Test loss: 0.3922 score: 0.7955 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.0832;  Loss pred: 0.0593; Loss self: 2.3940; time: 0.16s
Val loss: 0.2780 score: 0.8605 time: 0.08s
Test loss: 0.3872 score: 0.8182 time: 0.06s
Epoch 43/1000, LR 0.000269
Train loss: 0.0799;  Loss pred: 0.0560; Loss self: 2.3899; time: 0.17s
Val loss: 0.2708 score: 0.8837 time: 0.07s
Test loss: 0.3826 score: 0.8182 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.0768;  Loss pred: 0.0529; Loss self: 2.3877; time: 0.16s
Val loss: 0.2652 score: 0.8837 time: 0.09s
Test loss: 0.3768 score: 0.8182 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0738;  Loss pred: 0.0499; Loss self: 2.3865; time: 0.21s
Val loss: 0.2592 score: 0.8837 time: 0.07s
Test loss: 0.3707 score: 0.8182 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0706;  Loss pred: 0.0467; Loss self: 2.3858; time: 0.17s
Val loss: 0.2542 score: 0.9070 time: 0.08s
Test loss: 0.3641 score: 0.8182 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0675;  Loss pred: 0.0436; Loss self: 2.3841; time: 0.17s
Val loss: 0.2517 score: 0.9070 time: 0.06s
Test loss: 0.3553 score: 0.8182 time: 0.06s
Epoch 48/1000, LR 0.000269
Train loss: 0.0645;  Loss pred: 0.0406; Loss self: 2.3822; time: 0.23s
Val loss: 0.2516 score: 0.9070 time: 0.05s
Test loss: 0.3450 score: 0.8182 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0615;  Loss pred: 0.0377; Loss self: 2.3793; time: 0.20s
Val loss: 0.2531 score: 0.9070 time: 0.05s
Test loss: 0.3339 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0587;  Loss pred: 0.0349; Loss self: 2.3758; time: 0.18s
Val loss: 0.2547 score: 0.9070 time: 0.05s
Test loss: 0.3233 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0560;  Loss pred: 0.0323; Loss self: 2.3727; time: 0.18s
Val loss: 0.2554 score: 0.9070 time: 0.06s
Test loss: 0.3135 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0537;  Loss pred: 0.0300; Loss self: 2.3690; time: 0.17s
Val loss: 0.2549 score: 0.9070 time: 0.05s
Test loss: 0.3051 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0514;  Loss pred: 0.0277; Loss self: 2.3646; time: 0.22s
Val loss: 0.2526 score: 0.9070 time: 0.07s
Test loss: 0.2980 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0492;  Loss pred: 0.0256; Loss self: 2.3599; time: 0.16s
Val loss: 0.2491 score: 0.9302 time: 0.05s
Test loss: 0.2938 score: 0.8409 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0472;  Loss pred: 0.0237; Loss self: 2.3551; time: 0.15s
Val loss: 0.2475 score: 0.9302 time: 0.06s
Test loss: 0.2906 score: 0.8636 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0454;  Loss pred: 0.0219; Loss self: 2.3511; time: 0.15s
Val loss: 0.2481 score: 0.9302 time: 0.06s
Test loss: 0.2879 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0437;  Loss pred: 0.0202; Loss self: 2.3484; time: 0.18s
Val loss: 0.2496 score: 0.9302 time: 0.06s
Test loss: 0.2863 score: 0.8864 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0421;  Loss pred: 0.0186; Loss self: 2.3465; time: 0.15s
Val loss: 0.2518 score: 0.9302 time: 0.10s
Test loss: 0.2850 score: 0.8864 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0407;  Loss pred: 0.0172; Loss self: 2.3455; time: 0.19s
Val loss: 0.2537 score: 0.9302 time: 0.06s
Test loss: 0.2840 score: 0.9091 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0394;  Loss pred: 0.0159; Loss self: 2.3448; time: 0.16s
Val loss: 0.2543 score: 0.9302 time: 0.06s
Test loss: 0.2855 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0382;  Loss pred: 0.0148; Loss self: 2.3438; time: 0.17s
Val loss: 0.2529 score: 0.9302 time: 0.05s
Test loss: 0.2900 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0371;  Loss pred: 0.0136; Loss self: 2.3417; time: 0.20s
Val loss: 0.2512 score: 0.9302 time: 0.07s
Test loss: 0.2975 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0361;  Loss pred: 0.0128; Loss self: 2.3391; time: 0.17s
Val loss: 0.2505 score: 0.9070 time: 0.06s
Test loss: 0.3053 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0353;  Loss pred: 0.0120; Loss self: 2.3375; time: 0.16s
Val loss: 0.2507 score: 0.9070 time: 0.06s
Test loss: 0.3130 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0346;  Loss pred: 0.0112; Loss self: 2.3363; time: 0.27s
Val loss: 0.2522 score: 0.9070 time: 0.15s
Test loss: 0.3207 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0339;  Loss pred: 0.0105; Loss self: 2.3353; time: 0.17s
Val loss: 0.2549 score: 0.9302 time: 0.37s
Test loss: 0.3291 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0332;  Loss pred: 0.0099; Loss self: 2.3340; time: 0.50s
Val loss: 0.2588 score: 0.9070 time: 0.12s
Test loss: 0.3371 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0326;  Loss pred: 0.0093; Loss self: 2.3331; time: 0.20s
Val loss: 0.2630 score: 0.9070 time: 0.06s
Test loss: 0.3447 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 13 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0320;  Loss pred: 0.0087; Loss self: 2.3323; time: 0.20s
Val loss: 0.2673 score: 0.9070 time: 0.06s
Test loss: 0.3517 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0315;  Loss pred: 0.0082; Loss self: 2.3318; time: 0.21s
Val loss: 0.2723 score: 0.9070 time: 0.25s
Test loss: 0.3587 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0310;  Loss pred: 0.0077; Loss self: 2.3311; time: 0.17s
Val loss: 0.2773 score: 0.9070 time: 0.06s
Test loss: 0.3651 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 16 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0306;  Loss pred: 0.0073; Loss self: 2.3298; time: 0.18s
Val loss: 0.2816 score: 0.9070 time: 0.09s
Test loss: 0.3701 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0302;  Loss pred: 0.0070; Loss self: 2.3282; time: 0.17s
Val loss: 0.2855 score: 0.9070 time: 0.07s
Test loss: 0.3736 score: 0.8409 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0299;  Loss pred: 0.0066; Loss self: 2.3264; time: 0.24s
Val loss: 0.2887 score: 0.9070 time: 0.07s
Test loss: 0.3753 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 19 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0295;  Loss pred: 0.0063; Loss self: 2.3240; time: 0.51s
Val loss: 0.2911 score: 0.9070 time: 0.09s
Test loss: 0.3753 score: 0.8409 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 054,   Train_Loss: 0.0472,   Val_Loss: 0.2475,   Val_Precision: 0.9524,   Val_Recall: 0.9091,   Val_accuracy: 0.9302,   Val_Score: 0.9302,   Val_Loss: 0.2475,   Test_Precision: 0.8333,   Test_Recall: 0.9091,   Test_accuracy: 0.8696,   Test_Score: 0.8636,   Test_loss: 0.2906


[0.08681390306446701, 0.07521008094772696, 0.07460255397018045, 0.05631180899217725, 0.0706074129557237, 0.07972352590877563, 0.07565028197132051, 0.06912055506836623, 0.07099193893373013, 0.06554836104623973, 0.0574045330286026, 0.05809263000264764, 0.06845613394398242, 0.0614512279862538, 0.0655731949955225, 0.080146394087933, 0.07589714403729886, 0.05338877602480352, 0.06408609298523515, 0.054745213012211025, 0.0547851889859885, 0.1464262290392071, 0.05821526702493429, 0.06311097589787096, 0.07920319202821702, 0.06940238899551332, 0.06309182802215219, 0.09555561398155987, 0.05538188689388335, 0.0556610330240801, 0.05681280395947397, 0.064236911945045, 0.11772111197933555, 0.12003226997330785, 0.10373539803549647, 0.056221570004709065, 0.07215154101140797, 0.05677173798903823, 0.05593049596063793, 0.09617582196369767, 0.08070451801177114, 0.06575674598570913, 0.0622683959081769, 0.05847220099531114, 0.0619994601001963, 0.06485317891929299, 0.0632767240749672, 0.057153822970576584, 0.0788000039756298, 0.06862757203634828, 0.10064123501069844, 0.05893886205740273, 0.055383804021403193, 0.08457379706669599, 0.0553865929832682, 0.05661593296099454, 0.0747232639696449, 0.0796633290592581, 0.05671337491367012, 0.06135561701375991, 0.09368045500013977, 0.09568561194464564, 0.06537210196256638, 0.09354120609350502, 0.06325742695480585, 0.06876797403674573, 0.10816523805260658, 0.10647109197452664, 0.09366843290627003, 0.09606273693498224, 0.11629460810218006, 0.11785413499455899, 0.08066802099347115, 0.11079071601852775, 0.0810510030714795]
[0.0019730432514651593, 0.0017093200215392492, 0.001695512590231374, 0.001279813840731301, 0.0016047139308119024, 0.001811898316108537, 0.0017193245902572844, 0.0015709217060992325, 0.0016134531575847757, 0.0014897354783236303, 0.0013046484779227865, 0.001320287045514719, 0.0015558212259996005, 0.0013966188178694044, 0.001490299886261875, 0.0018215089565439318, 0.0017249350917567922, 0.0012133812732909892, 0.001456502113300799, 0.0012442093866411597, 0.0012451179314997387, 0.0033278688418001616, 0.0013230742505666885, 0.001434340361315249, 0.0018000725460958413, 0.0015773270226253028, 0.0014339051823216405, 0.002171718499580906, 0.0012586792475882578, 0.0012650234778200022, 0.0012912000899880447, 0.0014599298169328408, 0.0026754798177121716, 0.0027280061357569966, 0.00235762268262492, 0.0012777629546524788, 0.001639807750259272, 0.0012902667724781415, 0.0012711476354690437, 0.0021858141355385833, 0.0018341935911766168, 0.0014944714996752075, 0.0014151908160949295, 0.001328913658984344, 0.0014090786386408251, 0.0014739358845293862, 0.0014381073653401638, 0.0012989505220585588, 0.0017909091812643137, 0.0015597175462806429, 0.0022873007956976917, 0.0013395195922136984, 0.0012587228186682544, 0.001922131751515818, 0.0012587862041651863, 0.0012867257491135124, 0.0016982559993101115, 0.0018105302058922296, 0.0012889403389470483, 0.0013944458412218162, 0.0021291012500031766, 0.0021746729987419462, 0.0014857295900583267, 0.002125936502125114, 0.0014376687944274056, 0.00156290850083513, 0.002458300864831968, 0.0024197975448756056, 0.002128828020597046, 0.0021832440212495962, 0.002643059275049547, 0.002678503068058159, 0.0018333641134879806, 0.0025179708186029034, 0.001842068251624534]
[506.83126143200934, 585.027956964721, 589.7921405959819, 781.36363912785, 623.1640299240449, 551.9073510414905, 581.6237408960442, 636.5689621051246, 619.7886782761817, 671.2601092948931, 766.489990922431, 757.4110519354116, 642.7473692277912, 716.0149836198959, 671.0058889612505, 548.9953790275978, 579.7319590626053, 824.1432614892378, 686.576415418821, 803.7232404262581, 803.1367750005052, 300.4926117998884, 755.8154801755745, 697.1845922839601, 555.5331656876217, 633.9839397004689, 697.3961823479128, 460.4648347347863, 794.4835842142385, 790.4991626900762, 774.4733041408471, 684.9644334964643, 373.76473310686737, 366.56809047920603, 424.1560820438936, 782.6177745714785, 609.8275848750494, 775.033521230154, 786.6906817877277, 457.49544013887555, 545.1987210131456, 669.1328675169311, 706.6184917447351, 752.4943349323953, 709.6835993231607, 678.4555627528471, 695.3583745560362, 769.8522638223462, 558.3756063465139, 641.1417261956404, 437.1965427026276, 746.5362998889727, 794.4560829190444, 520.2557000639457, 794.4160785136578, 777.1663858354808, 588.8393742793982, 552.3243946693503, 775.8310992244317, 717.1307557730589, 469.681749516848, 459.8392496612145, 673.069989782422, 470.38093517863155, 695.5704984876435, 639.832721791235, 406.7850336408489, 413.25771328171464, 469.74203191836153, 458.0340036509718, 378.3494412857063, 373.34286151293173, 545.4453878763325, 397.145190330224, 542.868050148572]
Elapsed: 0.07522245627672722~0.019898035375823648
Time per graph: 0.0017096012790165278~0.0004522280767232648
Speed: 619.9007800585681~136.5624112222731
Total Time: 0.0815
best val loss: 0.24750569462776184 test_score: 0.8636

Testing...
Test loss: 0.2938 score: 0.8409 time: 0.05s
test Score 0.8409
Epoch Time List: [0.3286124839214608, 0.28020759113132954, 0.2858091179514304, 0.2673150320770219, 0.3011973351240158, 0.2907096421113238, 0.3158476899843663, 0.2980767620028928, 0.3097978890873492, 0.2903372129658237, 0.4149516400648281, 0.38076281105168164, 0.30136585293803364, 0.2949411738663912, 0.29735954804345965, 0.30908367200754583, 0.3123958990909159, 0.2856757799163461, 0.28516202594619244, 0.2542874999344349, 0.25327751808799803, 0.5179129838943481, 0.32125673594418913, 0.28310569503810257, 0.29865630192216486, 0.30209794524125755, 0.30132712912745774, 0.34051386499777436, 0.3077063320670277, 0.287279192940332, 0.2702582450583577, 0.30723408109042794, 0.3959537842310965, 0.4211943191476166, 0.37816344399470836, 0.2822132520377636, 0.2938724079867825, 0.32488973205909133, 0.40487976593431085, 0.3846206170273945, 0.307109356042929, 0.299369316897355, 0.2964651361107826, 0.305572974961251, 0.33631014288403094, 0.31083561899140477, 0.29398246307391673, 0.3333615829469636, 0.3274947649333626, 0.3006463859928772, 0.32901980716269463, 0.27905503718648106, 0.3354557609418407, 0.2885711860144511, 0.25454988097772, 0.2562272670911625, 0.30915243993513286, 0.32046370790340006, 0.2988464059308171, 0.2798852891428396, 0.3102245010668412, 0.35606049292255193, 0.2897188710048795, 0.31455157487653196, 0.48273555701598525, 0.6106308560119942, 0.731329380068928, 0.35676511307246983, 0.3479168730555102, 0.5488161979010329, 0.33972382987849414, 0.3881970070069656, 0.31277544901240617, 0.4185002100421116, 0.6769095989875495]
Total Epoch List: [75]
Total Time List: [0.08151599008124322]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbce380>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7573;  Loss pred: 0.7248; Loss self: 3.2496; time: 0.20s
Val loss: 0.7545 score: 0.4091 time: 0.05s
Test loss: 0.7106 score: 0.5581 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7573;  Loss pred: 0.7248; Loss self: 3.2496; time: 0.22s
Val loss: 0.7290 score: 0.5909 time: 0.11s
Test loss: 0.6968 score: 0.6047 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7260;  Loss pred: 0.6931; Loss self: 3.2828; time: 0.23s
Val loss: 0.7060 score: 0.5682 time: 0.11s
Test loss: 0.6931 score: 0.5814 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.7067;  Loss pred: 0.6735; Loss self: 3.3164; time: 0.23s
Val loss: 0.6847 score: 0.6136 time: 0.08s
Test loss: 0.6759 score: 0.6279 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6811;  Loss pred: 0.6476; Loss self: 3.3425; time: 0.19s
Val loss: 0.6672 score: 0.6364 time: 0.08s
Test loss: 0.6627 score: 0.6977 time: 0.17s
Epoch 6/1000, LR 0.000120
Train loss: 0.6610;  Loss pred: 0.6274; Loss self: 3.3551; time: 0.23s
Val loss: 0.6601 score: 0.6364 time: 0.06s
Test loss: 0.6570 score: 0.6279 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6375;  Loss pred: 0.6039; Loss self: 3.3540; time: 0.35s
Val loss: 0.6460 score: 0.6364 time: 0.05s
Test loss: 0.6478 score: 0.6512 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.6028;  Loss pred: 0.5694; Loss self: 3.3379; time: 0.16s
Val loss: 0.6359 score: 0.7045 time: 0.06s
Test loss: 0.6420 score: 0.6279 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.5730;  Loss pred: 0.5400; Loss self: 3.3027; time: 0.16s
Val loss: 0.6203 score: 0.7045 time: 0.05s
Test loss: 0.6340 score: 0.6047 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.5333;  Loss pred: 0.5009; Loss self: 3.2409; time: 0.17s
Val loss: 0.6017 score: 0.7045 time: 0.07s
Test loss: 0.6225 score: 0.6279 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.4926;  Loss pred: 0.4614; Loss self: 3.1250; time: 0.16s
Val loss: 0.5822 score: 0.7045 time: 0.05s
Test loss: 0.6211 score: 0.5581 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.4505;  Loss pred: 0.4210; Loss self: 2.9488; time: 0.21s
Val loss: 0.5567 score: 0.7273 time: 0.06s
Test loss: 0.6116 score: 0.6512 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.4069;  Loss pred: 0.3790; Loss self: 2.7869; time: 0.18s
Val loss: 0.5281 score: 0.7727 time: 0.06s
Test loss: 0.5970 score: 0.6279 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.3718;  Loss pred: 0.3446; Loss self: 2.7196; time: 0.20s
Val loss: 0.5069 score: 0.7955 time: 0.08s
Test loss: 0.5779 score: 0.6047 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.3426;  Loss pred: 0.3157; Loss self: 2.6908; time: 0.22s
Val loss: 0.4845 score: 0.7727 time: 0.06s
Test loss: 0.5488 score: 0.6744 time: 0.10s
Epoch 16/1000, LR 0.000270
Train loss: 0.3132;  Loss pred: 0.2864; Loss self: 2.6792; time: 0.20s
Val loss: 0.4692 score: 0.7955 time: 0.10s
Test loss: 0.5253 score: 0.7209 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.2865;  Loss pred: 0.2599; Loss self: 2.6579; time: 0.19s
Val loss: 0.4562 score: 0.8182 time: 0.05s
Test loss: 0.5157 score: 0.6977 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.2621;  Loss pred: 0.2359; Loss self: 2.6191; time: 0.16s
Val loss: 0.4543 score: 0.7955 time: 0.05s
Test loss: 0.5098 score: 0.6744 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.2435;  Loss pred: 0.2178; Loss self: 2.5696; time: 0.18s
Val loss: 0.4453 score: 0.8182 time: 0.06s
Test loss: 0.4968 score: 0.7209 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.2238;  Loss pred: 0.1986; Loss self: 2.5294; time: 0.17s
Val loss: 0.4282 score: 0.7955 time: 0.09s
Test loss: 0.4771 score: 0.7674 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.2035;  Loss pred: 0.1785; Loss self: 2.5038; time: 0.21s
Val loss: 0.4130 score: 0.8182 time: 0.06s
Test loss: 0.4670 score: 0.7907 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.1857;  Loss pred: 0.1608; Loss self: 2.4940; time: 0.18s
Val loss: 0.4111 score: 0.8409 time: 0.06s
Test loss: 0.4740 score: 0.7907 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.1699;  Loss pred: 0.1449; Loss self: 2.5005; time: 0.17s
Val loss: 0.4257 score: 0.8182 time: 0.05s
Test loss: 0.4971 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1574;  Loss pred: 0.1323; Loss self: 2.5112; time: 0.16s
Val loss: 0.4414 score: 0.7955 time: 0.06s
Test loss: 0.5186 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1469;  Loss pred: 0.1217; Loss self: 2.5265; time: 0.17s
Val loss: 0.4418 score: 0.7955 time: 0.05s
Test loss: 0.5262 score: 0.7442 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1366;  Loss pred: 0.1112; Loss self: 2.5370; time: 0.23s
Val loss: 0.4279 score: 0.8182 time: 0.10s
Test loss: 0.5181 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1277;  Loss pred: 0.1023; Loss self: 2.5408; time: 0.17s
Val loss: 0.4194 score: 0.8182 time: 0.14s
Test loss: 0.5139 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1201;  Loss pred: 0.0947; Loss self: 2.5420; time: 0.22s
Val loss: 0.4200 score: 0.8182 time: 0.07s
Test loss: 0.5169 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1136;  Loss pred: 0.0882; Loss self: 2.5421; time: 0.21s
Val loss: 0.4216 score: 0.8182 time: 0.05s
Test loss: 0.5182 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1079;  Loss pred: 0.0825; Loss self: 2.5428; time: 0.19s
Val loss: 0.4209 score: 0.8182 time: 0.09s
Test loss: 0.5171 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1031;  Loss pred: 0.0777; Loss self: 2.5434; time: 0.22s
Val loss: 0.4176 score: 0.8182 time: 0.08s
Test loss: 0.5139 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0989;  Loss pred: 0.0735; Loss self: 2.5432; time: 0.21s
Val loss: 0.4151 score: 0.8409 time: 0.08s
Test loss: 0.5104 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0953;  Loss pred: 0.0698; Loss self: 2.5463; time: 0.19s
Val loss: 0.4145 score: 0.8409 time: 0.06s
Test loss: 0.5082 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0921;  Loss pred: 0.0666; Loss self: 2.5507; time: 0.18s
Val loss: 0.4149 score: 0.8409 time: 0.06s
Test loss: 0.5082 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0887;  Loss pred: 0.0631; Loss self: 2.5560; time: 0.19s
Val loss: 0.4158 score: 0.8409 time: 0.05s
Test loss: 0.5116 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0853;  Loss pred: 0.0596; Loss self: 2.5638; time: 0.18s
Val loss: 0.4179 score: 0.8409 time: 0.05s
Test loss: 0.5187 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0817;  Loss pred: 0.0559; Loss self: 2.5734; time: 0.19s
Val loss: 0.4189 score: 0.8409 time: 0.05s
Test loss: 0.5245 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0784;  Loss pred: 0.0526; Loss self: 2.5819; time: 0.20s
Val loss: 0.4184 score: 0.8409 time: 0.05s
Test loss: 0.5282 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0751;  Loss pred: 0.0492; Loss self: 2.5874; time: 0.19s
Val loss: 0.4171 score: 0.8409 time: 0.05s
Test loss: 0.5295 score: 0.7674 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0719;  Loss pred: 0.0460; Loss self: 2.5915; time: 0.23s
Val loss: 0.4155 score: 0.8636 time: 0.13s
Test loss: 0.5314 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0691;  Loss pred: 0.0432; Loss self: 2.5949; time: 0.19s
Val loss: 0.4137 score: 0.8636 time: 0.08s
Test loss: 0.5340 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0665;  Loss pred: 0.0405; Loss self: 2.5983; time: 0.15s
Val loss: 0.4125 score: 0.8864 time: 0.07s
Test loss: 0.5380 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 021,   Train_Loss: 0.1857,   Val_Loss: 0.4111,   Val_Precision: 0.8571,   Val_Recall: 0.8182,   Val_accuracy: 0.8372,   Val_Score: 0.8409,   Val_Loss: 0.4111,   Test_Precision: 0.8095,   Test_Recall: 0.7727,   Test_accuracy: 0.7907,   Test_Score: 0.7907,   Test_loss: 0.4740


[0.08681390306446701, 0.07521008094772696, 0.07460255397018045, 0.05631180899217725, 0.0706074129557237, 0.07972352590877563, 0.07565028197132051, 0.06912055506836623, 0.07099193893373013, 0.06554836104623973, 0.0574045330286026, 0.05809263000264764, 0.06845613394398242, 0.0614512279862538, 0.0655731949955225, 0.080146394087933, 0.07589714403729886, 0.05338877602480352, 0.06408609298523515, 0.054745213012211025, 0.0547851889859885, 0.1464262290392071, 0.05821526702493429, 0.06311097589787096, 0.07920319202821702, 0.06940238899551332, 0.06309182802215219, 0.09555561398155987, 0.05538188689388335, 0.0556610330240801, 0.05681280395947397, 0.064236911945045, 0.11772111197933555, 0.12003226997330785, 0.10373539803549647, 0.056221570004709065, 0.07215154101140797, 0.05677173798903823, 0.05593049596063793, 0.09617582196369767, 0.08070451801177114, 0.06575674598570913, 0.0622683959081769, 0.05847220099531114, 0.0619994601001963, 0.06485317891929299, 0.0632767240749672, 0.057153822970576584, 0.0788000039756298, 0.06862757203634828, 0.10064123501069844, 0.05893886205740273, 0.055383804021403193, 0.08457379706669599, 0.0553865929832682, 0.05661593296099454, 0.0747232639696449, 0.0796633290592581, 0.05671337491367012, 0.06135561701375991, 0.09368045500013977, 0.09568561194464564, 0.06537210196256638, 0.09354120609350502, 0.06325742695480585, 0.06876797403674573, 0.10816523805260658, 0.10647109197452664, 0.09366843290627003, 0.09606273693498224, 0.11629460810218006, 0.11785413499455899, 0.08066802099347115, 0.11079071601852775, 0.0810510030714795, 0.06051734206266701, 0.048950041993521154, 0.05175352597143501, 0.053730306099168956, 0.17563226900529116, 0.0660934840561822, 0.052736708894371986, 0.05529725702945143, 0.05402538098860532, 0.056219882098957896, 0.052085050963796675, 0.061942324973642826, 0.059336959035135806, 0.057800928014330566, 0.10567965300288051, 0.07378567196428776, 0.061317118001170456, 0.05583361303433776, 0.05514026107266545, 0.06156978604849428, 0.056834199000149965, 0.05558981094509363, 0.05174317199271172, 0.052591526065953076, 0.08128368202596903, 0.04873460892122239, 0.05496159510221332, 0.04941926698666066, 0.04854727501515299, 0.053735528024844825, 0.055289268027991056, 0.05152015294879675, 0.05145593301858753, 0.056651529972441494, 0.06245367298834026, 0.06861511606257409, 0.06524303799960762, 0.0574223279254511, 0.08974991296418011, 0.08251157798804343, 0.052137734019197524, 0.05335650697816163]
[0.0019730432514651593, 0.0017093200215392492, 0.001695512590231374, 0.001279813840731301, 0.0016047139308119024, 0.001811898316108537, 0.0017193245902572844, 0.0015709217060992325, 0.0016134531575847757, 0.0014897354783236303, 0.0013046484779227865, 0.001320287045514719, 0.0015558212259996005, 0.0013966188178694044, 0.001490299886261875, 0.0018215089565439318, 0.0017249350917567922, 0.0012133812732909892, 0.001456502113300799, 0.0012442093866411597, 0.0012451179314997387, 0.0033278688418001616, 0.0013230742505666885, 0.001434340361315249, 0.0018000725460958413, 0.0015773270226253028, 0.0014339051823216405, 0.002171718499580906, 0.0012586792475882578, 0.0012650234778200022, 0.0012912000899880447, 0.0014599298169328408, 0.0026754798177121716, 0.0027280061357569966, 0.00235762268262492, 0.0012777629546524788, 0.001639807750259272, 0.0012902667724781415, 0.0012711476354690437, 0.0021858141355385833, 0.0018341935911766168, 0.0014944714996752075, 0.0014151908160949295, 0.001328913658984344, 0.0014090786386408251, 0.0014739358845293862, 0.0014381073653401638, 0.0012989505220585588, 0.0017909091812643137, 0.0015597175462806429, 0.0022873007956976917, 0.0013395195922136984, 0.0012587228186682544, 0.001922131751515818, 0.0012587862041651863, 0.0012867257491135124, 0.0016982559993101115, 0.0018105302058922296, 0.0012889403389470483, 0.0013944458412218162, 0.0021291012500031766, 0.0021746729987419462, 0.0014857295900583267, 0.002125936502125114, 0.0014376687944274056, 0.00156290850083513, 0.002458300864831968, 0.0024197975448756056, 0.002128828020597046, 0.0021832440212495962, 0.002643059275049547, 0.002678503068058159, 0.0018333641134879806, 0.0025179708186029034, 0.001842068251624534, 0.0014073800479690003, 0.001138373069616771, 0.0012035703714287213, 0.0012495420023062548, 0.004084471372216073, 0.0015370577687484233, 0.0012264350905667903, 0.0012859827216151495, 0.001256404209037333, 0.0013074391185804162, 0.0012112802549720156, 0.001440519185433554, 0.0013799292798868793, 0.0013442076282402457, 0.002457666348904198, 0.001715945859634599, 0.001425979488399313, 0.0012984561170776223, 0.001282331652852685, 0.001431855489499867, 0.0013217255581430224, 0.0012927863010486892, 0.001203329581225854, 0.001223058745719839, 0.0018903181866504425, 0.0011333629981679625, 0.0012781766302840307, 0.0011492852787595503, 0.0011290063957012323, 0.0012496634424382516, 0.001285796930883513, 0.0011981430918324826, 0.001196649605083431, 0.0013174774412195696, 0.0014524109997288432, 0.0015957003735482347, 0.001517279953479247, 0.0013354029750104906, 0.002087207278236747, 0.0019188739066986844, 0.0012125054423069192, 0.001240848999492131]
[506.83126143200934, 585.027956964721, 589.7921405959819, 781.36363912785, 623.1640299240449, 551.9073510414905, 581.6237408960442, 636.5689621051246, 619.7886782761817, 671.2601092948931, 766.489990922431, 757.4110519354116, 642.7473692277912, 716.0149836198959, 671.0058889612505, 548.9953790275978, 579.7319590626053, 824.1432614892378, 686.576415418821, 803.7232404262581, 803.1367750005052, 300.4926117998884, 755.8154801755745, 697.1845922839601, 555.5331656876217, 633.9839397004689, 697.3961823479128, 460.4648347347863, 794.4835842142385, 790.4991626900762, 774.4733041408471, 684.9644334964643, 373.76473310686737, 366.56809047920603, 424.1560820438936, 782.6177745714785, 609.8275848750494, 775.033521230154, 786.6906817877277, 457.49544013887555, 545.1987210131456, 669.1328675169311, 706.6184917447351, 752.4943349323953, 709.6835993231607, 678.4555627528471, 695.3583745560362, 769.8522638223462, 558.3756063465139, 641.1417261956404, 437.1965427026276, 746.5362998889727, 794.4560829190444, 520.2557000639457, 794.4160785136578, 777.1663858354808, 588.8393742793982, 552.3243946693503, 775.8310992244317, 717.1307557730589, 469.681749516848, 459.8392496612145, 673.069989782422, 470.38093517863155, 695.5704984876435, 639.832721791235, 406.7850336408489, 413.25771328171464, 469.74203191836153, 458.0340036509718, 378.3494412857063, 373.34286151293173, 545.4453878763325, 397.145190330224, 542.868050148572, 710.5401284060456, 878.4466416942262, 830.8612639017783, 800.2932259614482, 244.82972430712357, 650.5936343656542, 815.3713210683293, 777.6154245245494, 795.9221982917488, 764.8539697097134, 825.5727738442356, 694.1941559070797, 724.6748181776212, 743.9326923840915, 406.89005667749444, 582.7689693035795, 701.2723591995826, 770.1453956338977, 779.8294596997525, 698.3945009347907, 756.5867163868455, 773.5230480001334, 831.0275219705657, 817.622214386312, 529.0114685781838, 882.3298463214886, 782.364484146283, 870.1059854167129, 885.7345749391386, 800.215454849886, 777.7277857654143, 834.6248514195116, 835.6665106911388, 759.026279094627, 688.5103460292536, 626.6840671199304, 659.0741528660668, 748.8376308223696, 479.10909971758565, 521.1389849583417, 824.7385662017275, 805.8998318161932]
Elapsed: 0.07060666000032718~0.021263138700777354
Time per graph: 0.0016165293428964504~0.0004829552072772095
Speed: 659.6506037596844~145.17106026138418
Total Time: 0.0561
best val loss: 0.4111403524875641 test_score: 0.7907

Testing...
Test loss: 0.5380 score: 0.7907 time: 0.05s
test Score 0.7907
Epoch Time List: [0.3286124839214608, 0.28020759113132954, 0.2858091179514304, 0.2673150320770219, 0.3011973351240158, 0.2907096421113238, 0.3158476899843663, 0.2980767620028928, 0.3097978890873492, 0.2903372129658237, 0.4149516400648281, 0.38076281105168164, 0.30136585293803364, 0.2949411738663912, 0.29735954804345965, 0.30908367200754583, 0.3123958990909159, 0.2856757799163461, 0.28516202594619244, 0.2542874999344349, 0.25327751808799803, 0.5179129838943481, 0.32125673594418913, 0.28310569503810257, 0.29865630192216486, 0.30209794524125755, 0.30132712912745774, 0.34051386499777436, 0.3077063320670277, 0.287279192940332, 0.2702582450583577, 0.30723408109042794, 0.3959537842310965, 0.4211943191476166, 0.37816344399470836, 0.2822132520377636, 0.2938724079867825, 0.32488973205909133, 0.40487976593431085, 0.3846206170273945, 0.307109356042929, 0.299369316897355, 0.2964651361107826, 0.305572974961251, 0.33631014288403094, 0.31083561899140477, 0.29398246307391673, 0.3333615829469636, 0.3274947649333626, 0.3006463859928772, 0.32901980716269463, 0.27905503718648106, 0.3354557609418407, 0.2885711860144511, 0.25454988097772, 0.2562272670911625, 0.30915243993513286, 0.32046370790340006, 0.2988464059308171, 0.2798852891428396, 0.3102245010668412, 0.35606049292255193, 0.2897188710048795, 0.31455157487653196, 0.48273555701598525, 0.6106308560119942, 0.731329380068928, 0.35676511307246983, 0.3479168730555102, 0.5488161979010329, 0.33972382987849414, 0.3881970070069656, 0.31277544901240617, 0.4185002100421116, 0.6769095989875495, 0.3090495078358799, 0.3712238841690123, 0.3846489619463682, 0.35477577010169625, 0.4392420470248908, 0.3481142270611599, 0.4488870290806517, 0.26408471004106104, 0.25959562498610467, 0.2929458608850837, 0.2634700941853225, 0.32667102897539735, 0.2927327679935843, 0.3248758749105036, 0.38097588694654405, 0.3623865708941594, 0.29504070698749274, 0.2618860750226304, 0.2932987109525129, 0.3238520090235397, 0.3170923900324851, 0.29244918597396463, 0.2694429300026968, 0.2586708349408582, 0.2952964570140466, 0.3778236979851499, 0.36417091090697795, 0.3292334759607911, 0.3035678109154105, 0.325554738054052, 0.35418178199324757, 0.33468638896010816, 0.29711933492217213, 0.29616963188163936, 0.2971392588224262, 0.29970601585227996, 0.3037424589274451, 0.29879168793559074, 0.32751789095345885, 0.43421168404165655, 0.313715560012497, 0.26902202295605093]
Total Epoch List: [75, 42]
Total Time List: [0.08151599008124322, 0.05605657596606761]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbcebc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7204;  Loss pred: 0.6927; Loss self: 2.7716; time: 0.19s
Val loss: 0.6937 score: 0.5227 time: 0.06s
Test loss: 0.6602 score: 0.6047 time: 0.08s
Epoch 2/1000, LR 0.000000
Train loss: 0.7204;  Loss pred: 0.6927; Loss self: 2.7716; time: 0.19s
Val loss: 0.6909 score: 0.5227 time: 0.05s
Test loss: 0.6480 score: 0.5814 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.6897;  Loss pred: 0.6626; Loss self: 2.7070; time: 0.24s
Val loss: 0.6897 score: 0.5227 time: 0.05s
Test loss: 0.6605 score: 0.6279 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6705;  Loss pred: 0.6439; Loss self: 2.6601; time: 0.17s
Val loss: 0.6950 score: 0.5000 time: 0.05s
Test loss: 0.6436 score: 0.5349 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6393;  Loss pred: 0.6123; Loss self: 2.6974; time: 0.18s
Val loss: 0.6846 score: 0.5909 time: 0.06s
Test loss: 0.5999 score: 0.5814 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.6162;  Loss pred: 0.5886; Loss self: 2.7579; time: 0.19s
Val loss: 0.6791 score: 0.5909 time: 0.06s
Test loss: 0.5738 score: 0.6047 time: 0.07s
Epoch 7/1000, LR 0.000150
Train loss: 0.5998;  Loss pred: 0.5718; Loss self: 2.7983; time: 0.20s
Val loss: 0.6698 score: 0.5455 time: 0.06s
Test loss: 0.5552 score: 0.6279 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.5742;  Loss pred: 0.5463; Loss self: 2.7882; time: 0.19s
Val loss: 0.6590 score: 0.5682 time: 0.05s
Test loss: 0.5463 score: 0.6279 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.5437;  Loss pred: 0.5163; Loss self: 2.7447; time: 0.19s
Val loss: 0.6503 score: 0.5682 time: 0.06s
Test loss: 0.5122 score: 0.7209 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.5135;  Loss pred: 0.4864; Loss self: 2.7107; time: 0.17s
Val loss: 0.6420 score: 0.5909 time: 0.08s
Test loss: 0.4913 score: 0.6977 time: 0.08s
Epoch 11/1000, LR 0.000270
Train loss: 0.4757;  Loss pred: 0.4490; Loss self: 2.6752; time: 0.23s
Val loss: 0.6339 score: 0.5455 time: 0.07s
Test loss: 0.4869 score: 0.7674 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.4364;  Loss pred: 0.4098; Loss self: 2.6574; time: 0.18s
Val loss: 0.6278 score: 0.5909 time: 0.05s
Test loss: 0.4759 score: 0.6977 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.4075;  Loss pred: 0.3811; Loss self: 2.6438; time: 0.24s
Val loss: 0.6140 score: 0.5909 time: 0.05s
Test loss: 0.4429 score: 0.7907 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.3862;  Loss pred: 0.3599; Loss self: 2.6247; time: 0.20s
Val loss: 0.6034 score: 0.5909 time: 0.05s
Test loss: 0.4321 score: 0.7442 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.3610;  Loss pred: 0.3351; Loss self: 2.5902; time: 0.20s
Val loss: 0.5975 score: 0.6364 time: 0.05s
Test loss: 0.4345 score: 0.7209 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.3322;  Loss pred: 0.3068; Loss self: 2.5436; time: 0.19s
Val loss: 0.5905 score: 0.6136 time: 0.06s
Test loss: 0.4095 score: 0.7209 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.3012;  Loss pred: 0.2763; Loss self: 2.4931; time: 0.18s
Val loss: 0.5901 score: 0.6364 time: 0.05s
Test loss: 0.3835 score: 0.7907 time: 0.10s
Epoch 18/1000, LR 0.000270
Train loss: 0.2732;  Loss pred: 0.2488; Loss self: 2.4304; time: 0.18s
Val loss: 0.5900 score: 0.6364 time: 0.05s
Test loss: 0.3881 score: 0.8372 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.2530;  Loss pred: 0.2292; Loss self: 2.3789; time: 0.18s
Val loss: 0.5810 score: 0.6591 time: 0.06s
Test loss: 0.3842 score: 0.8140 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.2371;  Loss pred: 0.2137; Loss self: 2.3406; time: 0.17s
Val loss: 0.5779 score: 0.6591 time: 0.16s
Test loss: 0.3766 score: 0.8372 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.2200;  Loss pred: 0.1968; Loss self: 2.3168; time: 0.26s
Val loss: 0.5734 score: 0.6591 time: 0.05s
Test loss: 0.3747 score: 0.8372 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.2019;  Loss pred: 0.1788; Loss self: 2.3103; time: 0.24s
Val loss: 0.5667 score: 0.7045 time: 0.22s
Test loss: 0.3675 score: 0.8372 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.1835;  Loss pred: 0.1603; Loss self: 2.3168; time: 0.17s
Val loss: 0.5585 score: 0.6818 time: 0.11s
Test loss: 0.3533 score: 0.8140 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.1679;  Loss pred: 0.1446; Loss self: 2.3305; time: 0.18s
Val loss: 0.5517 score: 0.6818 time: 0.05s
Test loss: 0.3400 score: 0.8140 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.1556;  Loss pred: 0.1321; Loss self: 2.3457; time: 0.17s
Val loss: 0.5468 score: 0.6818 time: 0.05s
Test loss: 0.3428 score: 0.8140 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.1444;  Loss pred: 0.1209; Loss self: 2.3545; time: 0.17s
Val loss: 0.5446 score: 0.7045 time: 0.05s
Test loss: 0.3476 score: 0.8372 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.1340;  Loss pred: 0.1104; Loss self: 2.3586; time: 0.16s
Val loss: 0.5477 score: 0.7045 time: 0.05s
Test loss: 0.3566 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1251;  Loss pred: 0.1014; Loss self: 2.3648; time: 0.19s
Val loss: 0.5551 score: 0.7045 time: 0.05s
Test loss: 0.3625 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1172;  Loss pred: 0.0935; Loss self: 2.3712; time: 0.22s
Val loss: 0.5625 score: 0.7045 time: 0.06s
Test loss: 0.3786 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1098;  Loss pred: 0.0860; Loss self: 2.3797; time: 0.23s
Val loss: 0.5699 score: 0.7045 time: 0.05s
Test loss: 0.3976 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1031;  Loss pred: 0.0792; Loss self: 2.3871; time: 0.17s
Val loss: 0.5790 score: 0.7045 time: 0.04s
Test loss: 0.4095 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0970;  Loss pred: 0.0731; Loss self: 2.3935; time: 0.16s
Val loss: 0.5882 score: 0.7045 time: 0.05s
Test loss: 0.4117 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0916;  Loss pred: 0.0676; Loss self: 2.3987; time: 0.18s
Val loss: 0.5988 score: 0.7500 time: 0.06s
Test loss: 0.4069 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0867;  Loss pred: 0.0627; Loss self: 2.4041; time: 0.18s
Val loss: 0.6098 score: 0.7500 time: 0.05s
Test loss: 0.4057 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0823;  Loss pred: 0.0582; Loss self: 2.4100; time: 0.18s
Val loss: 0.6179 score: 0.7500 time: 0.05s
Test loss: 0.4121 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0780;  Loss pred: 0.0539; Loss self: 2.4154; time: 0.20s
Val loss: 0.6197 score: 0.7500 time: 0.05s
Test loss: 0.4254 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0740;  Loss pred: 0.0498; Loss self: 2.4198; time: 0.17s
Val loss: 0.6200 score: 0.7273 time: 0.05s
Test loss: 0.4379 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0703;  Loss pred: 0.0461; Loss self: 2.4228; time: 0.16s
Val loss: 0.6220 score: 0.7500 time: 0.05s
Test loss: 0.4417 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0669;  Loss pred: 0.0427; Loss self: 2.4257; time: 0.17s
Val loss: 0.6267 score: 0.7500 time: 0.05s
Test loss: 0.4371 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0638;  Loss pred: 0.0395; Loss self: 2.4279; time: 0.23s
Val loss: 0.6353 score: 0.7500 time: 0.05s
Test loss: 0.4253 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0609;  Loss pred: 0.0366; Loss self: 2.4295; time: 0.18s
Val loss: 0.6446 score: 0.7500 time: 0.06s
Test loss: 0.4149 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0581;  Loss pred: 0.0338; Loss self: 2.4303; time: 0.18s
Val loss: 0.6532 score: 0.7727 time: 0.06s
Test loss: 0.4097 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0554;  Loss pred: 0.0310; Loss self: 2.4310; time: 0.19s
Val loss: 0.6582 score: 0.7727 time: 0.06s
Test loss: 0.4100 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0528;  Loss pred: 0.0285; Loss self: 2.4326; time: 0.18s
Val loss: 0.6616 score: 0.7727 time: 0.05s
Test loss: 0.4144 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0503;  Loss pred: 0.0260; Loss self: 2.4356; time: 0.16s
Val loss: 0.6647 score: 0.7727 time: 0.05s
Test loss: 0.4213 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0481;  Loss pred: 0.0237; Loss self: 2.4404; time: 0.16s
Val loss: 0.6687 score: 0.7727 time: 0.05s
Test loss: 0.4292 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 025,   Train_Loss: 0.1444,   Val_Loss: 0.5446,   Val_Precision: 0.7143,   Val_Recall: 0.6818,   Val_accuracy: 0.6977,   Val_Score: 0.7045,   Val_Loss: 0.5446,   Test_Precision: 0.7692,   Test_Recall: 0.9524,   Test_accuracy: 0.8511,   Test_Score: 0.8372,   Test_loss: 0.3476


[0.08681390306446701, 0.07521008094772696, 0.07460255397018045, 0.05631180899217725, 0.0706074129557237, 0.07972352590877563, 0.07565028197132051, 0.06912055506836623, 0.07099193893373013, 0.06554836104623973, 0.0574045330286026, 0.05809263000264764, 0.06845613394398242, 0.0614512279862538, 0.0655731949955225, 0.080146394087933, 0.07589714403729886, 0.05338877602480352, 0.06408609298523515, 0.054745213012211025, 0.0547851889859885, 0.1464262290392071, 0.05821526702493429, 0.06311097589787096, 0.07920319202821702, 0.06940238899551332, 0.06309182802215219, 0.09555561398155987, 0.05538188689388335, 0.0556610330240801, 0.05681280395947397, 0.064236911945045, 0.11772111197933555, 0.12003226997330785, 0.10373539803549647, 0.056221570004709065, 0.07215154101140797, 0.05677173798903823, 0.05593049596063793, 0.09617582196369767, 0.08070451801177114, 0.06575674598570913, 0.0622683959081769, 0.05847220099531114, 0.0619994601001963, 0.06485317891929299, 0.0632767240749672, 0.057153822970576584, 0.0788000039756298, 0.06862757203634828, 0.10064123501069844, 0.05893886205740273, 0.055383804021403193, 0.08457379706669599, 0.0553865929832682, 0.05661593296099454, 0.0747232639696449, 0.0796633290592581, 0.05671337491367012, 0.06135561701375991, 0.09368045500013977, 0.09568561194464564, 0.06537210196256638, 0.09354120609350502, 0.06325742695480585, 0.06876797403674573, 0.10816523805260658, 0.10647109197452664, 0.09366843290627003, 0.09606273693498224, 0.11629460810218006, 0.11785413499455899, 0.08066802099347115, 0.11079071601852775, 0.0810510030714795, 0.06051734206266701, 0.048950041993521154, 0.05175352597143501, 0.053730306099168956, 0.17563226900529116, 0.0660934840561822, 0.052736708894371986, 0.05529725702945143, 0.05402538098860532, 0.056219882098957896, 0.052085050963796675, 0.061942324973642826, 0.059336959035135806, 0.057800928014330566, 0.10567965300288051, 0.07378567196428776, 0.061317118001170456, 0.05583361303433776, 0.05514026107266545, 0.06156978604849428, 0.056834199000149965, 0.05558981094509363, 0.05174317199271172, 0.052591526065953076, 0.08128368202596903, 0.04873460892122239, 0.05496159510221332, 0.04941926698666066, 0.04854727501515299, 0.053735528024844825, 0.055289268027991056, 0.05152015294879675, 0.05145593301858753, 0.056651529972441494, 0.06245367298834026, 0.06861511606257409, 0.06524303799960762, 0.0574223279254511, 0.08974991296418011, 0.08251157798804343, 0.052137734019197524, 0.05335650697816163, 0.08552349091041833, 0.06124899908900261, 0.05819043796509504, 0.06184836395550519, 0.05949444801080972, 0.07075168692972511, 0.07421624497510493, 0.06960874504875392, 0.05940900195855647, 0.08046527102123946, 0.05807827797252685, 0.06369294202886522, 0.06453141896054149, 0.06818521302193403, 0.08177429705392569, 0.06864936603233218, 0.10464574594516307, 0.06600265705492347, 0.06506917998194695, 0.05790749401785433, 0.06983292906079441, 0.05598726694006473, 0.06636416004039347, 0.0523988869972527, 0.05388538003899157, 0.057215515058487654, 0.05559221399016678, 0.060076034045778215, 0.05551189510151744, 0.06741850101388991, 0.09605489997193217, 0.06719669897574931, 0.06013549107592553, 0.05917605198919773, 0.10800034995190799, 0.06146736803930253, 0.056357711902819574, 0.056374312029220164, 0.05734451801981777, 0.06280062301084399, 0.05952540296129882, 0.05751446890644729, 0.06144254305399954, 0.05597166798543185, 0.05444307695142925, 0.056526091997511685]
[0.0019730432514651593, 0.0017093200215392492, 0.001695512590231374, 0.001279813840731301, 0.0016047139308119024, 0.001811898316108537, 0.0017193245902572844, 0.0015709217060992325, 0.0016134531575847757, 0.0014897354783236303, 0.0013046484779227865, 0.001320287045514719, 0.0015558212259996005, 0.0013966188178694044, 0.001490299886261875, 0.0018215089565439318, 0.0017249350917567922, 0.0012133812732909892, 0.001456502113300799, 0.0012442093866411597, 0.0012451179314997387, 0.0033278688418001616, 0.0013230742505666885, 0.001434340361315249, 0.0018000725460958413, 0.0015773270226253028, 0.0014339051823216405, 0.002171718499580906, 0.0012586792475882578, 0.0012650234778200022, 0.0012912000899880447, 0.0014599298169328408, 0.0026754798177121716, 0.0027280061357569966, 0.00235762268262492, 0.0012777629546524788, 0.001639807750259272, 0.0012902667724781415, 0.0012711476354690437, 0.0021858141355385833, 0.0018341935911766168, 0.0014944714996752075, 0.0014151908160949295, 0.001328913658984344, 0.0014090786386408251, 0.0014739358845293862, 0.0014381073653401638, 0.0012989505220585588, 0.0017909091812643137, 0.0015597175462806429, 0.0022873007956976917, 0.0013395195922136984, 0.0012587228186682544, 0.001922131751515818, 0.0012587862041651863, 0.0012867257491135124, 0.0016982559993101115, 0.0018105302058922296, 0.0012889403389470483, 0.0013944458412218162, 0.0021291012500031766, 0.0021746729987419462, 0.0014857295900583267, 0.002125936502125114, 0.0014376687944274056, 0.00156290850083513, 0.002458300864831968, 0.0024197975448756056, 0.002128828020597046, 0.0021832440212495962, 0.002643059275049547, 0.002678503068058159, 0.0018333641134879806, 0.0025179708186029034, 0.001842068251624534, 0.0014073800479690003, 0.001138373069616771, 0.0012035703714287213, 0.0012495420023062548, 0.004084471372216073, 0.0015370577687484233, 0.0012264350905667903, 0.0012859827216151495, 0.001256404209037333, 0.0013074391185804162, 0.0012112802549720156, 0.001440519185433554, 0.0013799292798868793, 0.0013442076282402457, 0.002457666348904198, 0.001715945859634599, 0.001425979488399313, 0.0012984561170776223, 0.001282331652852685, 0.001431855489499867, 0.0013217255581430224, 0.0012927863010486892, 0.001203329581225854, 0.001223058745719839, 0.0018903181866504425, 0.0011333629981679625, 0.0012781766302840307, 0.0011492852787595503, 0.0011290063957012323, 0.0012496634424382516, 0.001285796930883513, 0.0011981430918324826, 0.001196649605083431, 0.0013174774412195696, 0.0014524109997288432, 0.0015957003735482347, 0.001517279953479247, 0.0013354029750104906, 0.002087207278236747, 0.0019188739066986844, 0.0012125054423069192, 0.001240848999492131, 0.0019889183932655426, 0.0014243953276512234, 0.0013532659991882569, 0.001438334045476865, 0.0013835918142048771, 0.0016453880681331422, 0.0017259591854675565, 0.001618808024389626, 0.0013816046967106157, 0.0018712853725869642, 0.0013506576272680662, 0.0014812312099736096, 0.0015007306735009649, 0.0015857026284170706, 0.001901727838463388, 0.0015964968844728415, 0.0024336219987247227, 0.001534945512905197, 0.001513236743766208, 0.0013466859073919613, 0.0016240216060649864, 0.0013020294637224356, 0.001543352559078918, 0.0012185787673779699, 0.001253148372999804, 0.0013305933734532012, 0.0012928421858178323, 0.0013971170708320516, 0.0012909743046864521, 0.001567872116602091, 0.0022338348830681903, 0.0015627139296685887, 0.0013984997924633843, 0.001376187255562738, 0.002511636045393209, 0.001429473675332617, 0.001310644462856269, 0.0013110305123074457, 0.0013335934423213435, 0.0014604796049033488, 0.0013843116967743913, 0.00133754578852203, 0.0014288963500930126, 0.0013016666973356245, 0.0012661180686378895, 0.0013145602790118997]
[506.83126143200934, 585.027956964721, 589.7921405959819, 781.36363912785, 623.1640299240449, 551.9073510414905, 581.6237408960442, 636.5689621051246, 619.7886782761817, 671.2601092948931, 766.489990922431, 757.4110519354116, 642.7473692277912, 716.0149836198959, 671.0058889612505, 548.9953790275978, 579.7319590626053, 824.1432614892378, 686.576415418821, 803.7232404262581, 803.1367750005052, 300.4926117998884, 755.8154801755745, 697.1845922839601, 555.5331656876217, 633.9839397004689, 697.3961823479128, 460.4648347347863, 794.4835842142385, 790.4991626900762, 774.4733041408471, 684.9644334964643, 373.76473310686737, 366.56809047920603, 424.1560820438936, 782.6177745714785, 609.8275848750494, 775.033521230154, 786.6906817877277, 457.49544013887555, 545.1987210131456, 669.1328675169311, 706.6184917447351, 752.4943349323953, 709.6835993231607, 678.4555627528471, 695.3583745560362, 769.8522638223462, 558.3756063465139, 641.1417261956404, 437.1965427026276, 746.5362998889727, 794.4560829190444, 520.2557000639457, 794.4160785136578, 777.1663858354808, 588.8393742793982, 552.3243946693503, 775.8310992244317, 717.1307557730589, 469.681749516848, 459.8392496612145, 673.069989782422, 470.38093517863155, 695.5704984876435, 639.832721791235, 406.7850336408489, 413.25771328171464, 469.74203191836153, 458.0340036509718, 378.3494412857063, 373.34286151293173, 545.4453878763325, 397.145190330224, 542.868050148572, 710.5401284060456, 878.4466416942262, 830.8612639017783, 800.2932259614482, 244.82972430712357, 650.5936343656542, 815.3713210683293, 777.6154245245494, 795.9221982917488, 764.8539697097134, 825.5727738442356, 694.1941559070797, 724.6748181776212, 743.9326923840915, 406.89005667749444, 582.7689693035795, 701.2723591995826, 770.1453956338977, 779.8294596997525, 698.3945009347907, 756.5867163868455, 773.5230480001334, 831.0275219705657, 817.622214386312, 529.0114685781838, 882.3298463214886, 782.364484146283, 870.1059854167129, 885.7345749391386, 800.215454849886, 777.7277857654143, 834.6248514195116, 835.6665106911388, 759.026279094627, 688.5103460292536, 626.6840671199304, 659.0741528660668, 748.8376308223696, 479.10909971758565, 521.1389849583417, 824.7385662017275, 805.8998318161932, 502.78583746119995, 702.0522888466392, 738.9530222438457, 695.2487867089736, 722.7565165776007, 607.7593604617543, 579.387976506005, 617.7384748120776, 723.7960339747276, 534.3920358964527, 740.3800784234783, 675.1140492224819, 666.3420809992241, 630.6352667134387, 525.8375987218067, 626.3714071262951, 410.91015799660937, 651.4889236083021, 660.8351298099975, 742.5636479233935, 615.7553546488865, 768.0317749039651, 647.940092571464, 820.6281175830035, 797.9901036029645, 751.5444011304264, 773.4896114697984, 715.7596316566736, 774.6087558596893, 637.8071204985841, 447.660660857123, 639.9123864033606, 715.0519473717992, 726.6452991465097, 398.14685803469786, 699.5581781296638, 762.9834240635435, 762.7587539819921, 749.8537172313415, 684.7065831269707, 722.380662050402, 747.637956458288, 699.8408246580698, 768.2458205675042, 789.8157563423894, 760.7106467203303]
Elapsed: 0.0691097335035747~0.019322038661153857
Time per graph: 0.0015889094685627677~0.0004389522865194462
Speed: 664.5026610612688~134.03344867326743
Total Time: 0.0572
best val loss: 0.5445767045021057 test_score: 0.8372

Testing...
Test loss: 0.4098 score: 0.8837 time: 0.05s
test Score 0.8837
Epoch Time List: [0.3286124839214608, 0.28020759113132954, 0.2858091179514304, 0.2673150320770219, 0.3011973351240158, 0.2907096421113238, 0.3158476899843663, 0.2980767620028928, 0.3097978890873492, 0.2903372129658237, 0.4149516400648281, 0.38076281105168164, 0.30136585293803364, 0.2949411738663912, 0.29735954804345965, 0.30908367200754583, 0.3123958990909159, 0.2856757799163461, 0.28516202594619244, 0.2542874999344349, 0.25327751808799803, 0.5179129838943481, 0.32125673594418913, 0.28310569503810257, 0.29865630192216486, 0.30209794524125755, 0.30132712912745774, 0.34051386499777436, 0.3077063320670277, 0.287279192940332, 0.2702582450583577, 0.30723408109042794, 0.3959537842310965, 0.4211943191476166, 0.37816344399470836, 0.2822132520377636, 0.2938724079867825, 0.32488973205909133, 0.40487976593431085, 0.3846206170273945, 0.307109356042929, 0.299369316897355, 0.2964651361107826, 0.305572974961251, 0.33631014288403094, 0.31083561899140477, 0.29398246307391673, 0.3333615829469636, 0.3274947649333626, 0.3006463859928772, 0.32901980716269463, 0.27905503718648106, 0.3354557609418407, 0.2885711860144511, 0.25454988097772, 0.2562272670911625, 0.30915243993513286, 0.32046370790340006, 0.2988464059308171, 0.2798852891428396, 0.3102245010668412, 0.35606049292255193, 0.2897188710048795, 0.31455157487653196, 0.48273555701598525, 0.6106308560119942, 0.731329380068928, 0.35676511307246983, 0.3479168730555102, 0.5488161979010329, 0.33972382987849414, 0.3881970070069656, 0.31277544901240617, 0.4185002100421116, 0.6769095989875495, 0.3090495078358799, 0.3712238841690123, 0.3846489619463682, 0.35477577010169625, 0.4392420470248908, 0.3481142270611599, 0.4488870290806517, 0.26408471004106104, 0.25959562498610467, 0.2929458608850837, 0.2634700941853225, 0.32667102897539735, 0.2927327679935843, 0.3248758749105036, 0.38097588694654405, 0.3623865708941594, 0.29504070698749274, 0.2618860750226304, 0.2932987109525129, 0.3238520090235397, 0.3170923900324851, 0.29244918597396463, 0.2694429300026968, 0.2586708349408582, 0.2952964570140466, 0.3778236979851499, 0.36417091090697795, 0.3292334759607911, 0.3035678109154105, 0.325554738054052, 0.35418178199324757, 0.33468638896010816, 0.29711933492217213, 0.29616963188163936, 0.2971392588224262, 0.29970601585227996, 0.3037424589274451, 0.29879168793559074, 0.32751789095345885, 0.43421168404165655, 0.313715560012497, 0.26902202295605093, 0.3293399640824646, 0.3033683419926092, 0.33954300289042294, 0.27735499606933445, 0.29882468294817954, 0.31526804994791746, 0.33231422503013164, 0.3035507118329406, 0.30778267013374716, 0.328941871994175, 0.3543986992444843, 0.2817135180812329, 0.35504075908102095, 0.3048410869669169, 0.3248845419147983, 0.30917292402591556, 0.33417440694756806, 0.29306238994468004, 0.29918690701015294, 0.3822851590812206, 0.368113057105802, 0.5120320459827781, 0.35014460794627666, 0.278972411993891, 0.275116040953435, 0.2682870429707691, 0.26051385689061135, 0.2884785550413653, 0.3258222379954532, 0.34018856193870306, 0.3082498840522021, 0.27478310698643327, 0.29415830387733877, 0.28366990108042955, 0.33222807687707245, 0.3097846779273823, 0.27279168798122555, 0.26193759695161134, 0.2648381369654089, 0.3348660130286589, 0.29207542398944497, 0.28652831295039505, 0.30147773609496653, 0.27321962686255574, 0.2575142061104998, 0.26330592995509505]
Total Epoch List: [75, 42, 46]
Total Time List: [0.08151599008124322, 0.05605657596606761, 0.05718563206028193]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbce050>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9507;  Loss pred: 0.9160; Loss self: 3.4684; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9009 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9131 score: 0.5000 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.9507;  Loss pred: 0.9160; Loss self: 3.4684; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8589 score: 0.5116 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8728 score: 0.5000 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.9165;  Loss pred: 0.8819; Loss self: 3.4666; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7922 score: 0.5116 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8087 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.8648;  Loss pred: 0.8304; Loss self: 3.4454; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7303 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7535 score: 0.5000 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.8140;  Loss pred: 0.7800; Loss self: 3.4005; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7006 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7217 score: 0.5000 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.7743;  Loss pred: 0.7405; Loss self: 3.3768; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7095 score: 0.5000 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.7503;  Loss pred: 0.7163; Loss self: 3.3931; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7045 score: 0.5000 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.7415;  Loss pred: 0.7072; Loss self: 3.4304; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5116 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5000 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.7332;  Loss pred: 0.6985; Loss self: 3.4694; time: 0.16s
Val loss: 0.6828 score: 0.4884 time: 0.06s
Test loss: 0.6938 score: 0.4773 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.7231;  Loss pred: 0.6880; Loss self: 3.5063; time: 0.16s
Val loss: 0.6796 score: 0.5116 time: 0.05s
Test loss: 0.6894 score: 0.4318 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.7107;  Loss pred: 0.6754; Loss self: 3.5384; time: 0.20s
Val loss: 0.6753 score: 0.6047 time: 0.07s
Test loss: 0.6851 score: 0.5682 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.7018;  Loss pred: 0.6662; Loss self: 3.5637; time: 0.26s
Val loss: 0.6713 score: 0.6512 time: 0.11s
Test loss: 0.6811 score: 0.6818 time: 0.10s
Epoch 13/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6587; Loss self: 3.5815; time: 0.20s
Val loss: 0.6678 score: 0.6744 time: 0.10s
Test loss: 0.6763 score: 0.7045 time: 0.17s
Epoch 14/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6496; Loss self: 3.5919; time: 0.19s
Val loss: 0.6631 score: 0.6512 time: 0.08s
Test loss: 0.6704 score: 0.7273 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6391; Loss self: 3.5965; time: 0.16s
Val loss: 0.6577 score: 0.6512 time: 0.06s
Test loss: 0.6634 score: 0.7045 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.6647;  Loss pred: 0.6287; Loss self: 3.5966; time: 0.15s
Val loss: 0.6537 score: 0.7209 time: 0.05s
Test loss: 0.6574 score: 0.7273 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.6557;  Loss pred: 0.6197; Loss self: 3.5942; time: 0.22s
Val loss: 0.6500 score: 0.7442 time: 0.06s
Test loss: 0.6525 score: 0.6818 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.6467;  Loss pred: 0.6108; Loss self: 3.5902; time: 0.17s
Val loss: 0.6462 score: 0.7442 time: 0.06s
Test loss: 0.6470 score: 0.6818 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.6367;  Loss pred: 0.6009; Loss self: 3.5836; time: 0.17s
Val loss: 0.6405 score: 0.7442 time: 0.07s
Test loss: 0.6400 score: 0.7500 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.6253;  Loss pred: 0.5895; Loss self: 3.5754; time: 0.20s
Val loss: 0.6333 score: 0.7209 time: 0.07s
Test loss: 0.6320 score: 0.7500 time: 0.10s
Epoch 21/1000, LR 0.000270
Train loss: 0.6127;  Loss pred: 0.5770; Loss self: 3.5647; time: 0.18s
Val loss: 0.6256 score: 0.7209 time: 0.06s
Test loss: 0.6246 score: 0.7273 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.5995;  Loss pred: 0.5640; Loss self: 3.5507; time: 0.15s
Val loss: 0.6173 score: 0.7442 time: 0.05s
Test loss: 0.6162 score: 0.7273 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.5850;  Loss pred: 0.5496; Loss self: 3.5316; time: 0.15s
Val loss: 0.6085 score: 0.7442 time: 0.05s
Test loss: 0.6067 score: 0.7500 time: 0.06s
Epoch 24/1000, LR 0.000270
Train loss: 0.5680;  Loss pred: 0.5329; Loss self: 3.5050; time: 0.16s
Val loss: 0.5995 score: 0.7674 time: 0.06s
Test loss: 0.5966 score: 0.7727 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.5489;  Loss pred: 0.5142; Loss self: 3.4701; time: 0.15s
Val loss: 0.5884 score: 0.7674 time: 0.06s
Test loss: 0.5857 score: 0.7955 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.5281;  Loss pred: 0.4938; Loss self: 3.4276; time: 0.18s
Val loss: 0.5732 score: 0.7907 time: 0.07s
Test loss: 0.5715 score: 0.7955 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.5048;  Loss pred: 0.4710; Loss self: 3.3779; time: 0.25s
Val loss: 0.5528 score: 0.8140 time: 0.06s
Test loss: 0.5523 score: 0.7955 time: 0.06s
Epoch 28/1000, LR 0.000270
Train loss: 0.4767;  Loss pred: 0.4435; Loss self: 3.3194; time: 0.17s
Val loss: 0.5302 score: 0.8372 time: 0.05s
Test loss: 0.5287 score: 0.8182 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.4456;  Loss pred: 0.4131; Loss self: 3.2508; time: 0.15s
Val loss: 0.5106 score: 0.8372 time: 0.09s
Test loss: 0.5071 score: 0.8182 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.4164;  Loss pred: 0.3846; Loss self: 3.1735; time: 0.14s
Val loss: 0.4961 score: 0.8372 time: 0.05s
Test loss: 0.4892 score: 0.7955 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.3850;  Loss pred: 0.3540; Loss self: 3.0997; time: 0.15s
Val loss: 0.4836 score: 0.8140 time: 0.07s
Test loss: 0.4800 score: 0.8182 time: 0.06s
Epoch 32/1000, LR 0.000270
Train loss: 0.3550;  Loss pred: 0.3248; Loss self: 3.0289; time: 0.17s
Val loss: 0.4618 score: 0.8140 time: 0.05s
Test loss: 0.4592 score: 0.8409 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.3248;  Loss pred: 0.2951; Loss self: 2.9645; time: 0.17s
Val loss: 0.4313 score: 0.8372 time: 0.07s
Test loss: 0.4285 score: 0.8409 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.2956;  Loss pred: 0.2666; Loss self: 2.9082; time: 0.19s
Val loss: 0.4098 score: 0.8372 time: 0.06s
Test loss: 0.4069 score: 0.8409 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.2721;  Loss pred: 0.2436; Loss self: 2.8581; time: 0.16s
Val loss: 0.3900 score: 0.8372 time: 0.13s
Test loss: 0.3897 score: 0.8409 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.2500;  Loss pred: 0.2219; Loss self: 2.8079; time: 0.15s
Val loss: 0.3774 score: 0.8837 time: 0.06s
Test loss: 0.3838 score: 0.8409 time: 0.09s
Epoch 37/1000, LR 0.000270
Train loss: 0.2293;  Loss pred: 0.2017; Loss self: 2.7593; time: 0.16s
Val loss: 0.3679 score: 0.8837 time: 0.07s
Test loss: 0.3813 score: 0.8409 time: 0.07s
Epoch 38/1000, LR 0.000270
Train loss: 0.2111;  Loss pred: 0.1840; Loss self: 2.7182; time: 0.17s
Val loss: 0.3505 score: 0.8837 time: 0.07s
Test loss: 0.3600 score: 0.8409 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.1936;  Loss pred: 0.1667; Loss self: 2.6886; time: 0.16s
Val loss: 0.3369 score: 0.8605 time: 0.08s
Test loss: 0.3383 score: 0.8636 time: 0.09s
Epoch 40/1000, LR 0.000269
Train loss: 0.1811;  Loss pred: 0.1544; Loss self: 2.6706; time: 0.16s
Val loss: 0.3294 score: 0.8605 time: 0.13s
Test loss: 0.3269 score: 0.8636 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 0.1709;  Loss pred: 0.1442; Loss self: 2.6613; time: 0.17s
Val loss: 0.3192 score: 0.8605 time: 0.06s
Test loss: 0.3229 score: 0.8636 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.1603;  Loss pred: 0.1337; Loss self: 2.6591; time: 0.17s
Val loss: 0.3111 score: 0.8605 time: 0.07s
Test loss: 0.3240 score: 0.8636 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.1505;  Loss pred: 0.1239; Loss self: 2.6620; time: 0.17s
Val loss: 0.3044 score: 0.8837 time: 0.07s
Test loss: 0.3220 score: 0.8409 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.1416;  Loss pred: 0.1150; Loss self: 2.6643; time: 0.16s
Val loss: 0.2963 score: 0.8605 time: 0.13s
Test loss: 0.3171 score: 0.8409 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.1339;  Loss pred: 0.1073; Loss self: 2.6623; time: 0.16s
Val loss: 0.2917 score: 0.8605 time: 0.09s
Test loss: 0.3141 score: 0.8409 time: 0.18s
Epoch 46/1000, LR 0.000269
Train loss: 0.1276;  Loss pred: 0.1010; Loss self: 2.6601; time: 0.28s
Val loss: 0.2904 score: 0.8605 time: 0.08s
Test loss: 0.3140 score: 0.8409 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.1225;  Loss pred: 0.0960; Loss self: 2.6574; time: 0.23s
Val loss: 0.2901 score: 0.8605 time: 0.05s
Test loss: 0.3150 score: 0.8409 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.1183;  Loss pred: 0.0917; Loss self: 2.6558; time: 0.15s
Val loss: 0.2903 score: 0.8605 time: 0.09s
Test loss: 0.3157 score: 0.8636 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1144;  Loss pred: 0.0878; Loss self: 2.6556; time: 0.15s
Val loss: 0.2902 score: 0.8605 time: 0.06s
Test loss: 0.3168 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.1106;  Loss pred: 0.0840; Loss self: 2.6557; time: 0.15s
Val loss: 0.2910 score: 0.8605 time: 0.05s
Test loss: 0.3179 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.1070;  Loss pred: 0.0805; Loss self: 2.6564; time: 0.15s
Val loss: 0.2907 score: 0.8605 time: 0.06s
Test loss: 0.3224 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.1037;  Loss pred: 0.0771; Loss self: 2.6571; time: 0.16s
Val loss: 0.2890 score: 0.8605 time: 0.06s
Test loss: 0.3295 score: 0.8409 time: 0.07s
Epoch 53/1000, LR 0.000269
Train loss: 0.1005;  Loss pred: 0.0739; Loss self: 2.6576; time: 0.15s
Val loss: 0.2867 score: 0.8837 time: 0.06s
Test loss: 0.3394 score: 0.8409 time: 0.08s
Epoch 54/1000, LR 0.000269
Train loss: 0.0975;  Loss pred: 0.0709; Loss self: 2.6578; time: 0.19s
Val loss: 0.2856 score: 0.8837 time: 0.06s
Test loss: 0.3488 score: 0.8409 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0950;  Loss pred: 0.0684; Loss self: 2.6581; time: 0.18s
Val loss: 0.2848 score: 0.8837 time: 0.06s
Test loss: 0.3573 score: 0.8409 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0928;  Loss pred: 0.0663; Loss self: 2.6582; time: 0.19s
Val loss: 0.2856 score: 0.8837 time: 0.07s
Test loss: 0.3638 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0911;  Loss pred: 0.0645; Loss self: 2.6578; time: 0.16s
Val loss: 0.2877 score: 0.8837 time: 0.06s
Test loss: 0.3681 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0895;  Loss pred: 0.0629; Loss self: 2.6572; time: 0.16s
Val loss: 0.2901 score: 0.8605 time: 0.06s
Test loss: 0.3700 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0881;  Loss pred: 0.0615; Loss self: 2.6570; time: 0.16s
Val loss: 0.2916 score: 0.8605 time: 0.06s
Test loss: 0.3700 score: 0.8409 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0868;  Loss pred: 0.0602; Loss self: 2.6568; time: 0.16s
Val loss: 0.2924 score: 0.8605 time: 0.07s
Test loss: 0.3675 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0856;  Loss pred: 0.0590; Loss self: 2.6564; time: 0.17s
Val loss: 0.2929 score: 0.8605 time: 0.06s
Test loss: 0.3639 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0845;  Loss pred: 0.0579; Loss self: 2.6562; time: 0.18s
Val loss: 0.2934 score: 0.8605 time: 0.06s
Test loss: 0.3597 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0834;  Loss pred: 0.0569; Loss self: 2.6559; time: 0.26s
Val loss: 0.2942 score: 0.8605 time: 0.12s
Test loss: 0.3558 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 8 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0824;  Loss pred: 0.0559; Loss self: 2.6555; time: 0.16s
Val loss: 0.2940 score: 0.8605 time: 0.05s
Test loss: 0.3522 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0815;  Loss pred: 0.0549; Loss self: 2.6550; time: 0.16s
Val loss: 0.2936 score: 0.8837 time: 0.28s
Test loss: 0.3490 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 10 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0806;  Loss pred: 0.0540; Loss self: 2.6542; time: 0.26s
Val loss: 0.2933 score: 0.8837 time: 0.13s
Test loss: 0.3454 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0798;  Loss pred: 0.0532; Loss self: 2.6531; time: 0.16s
Val loss: 0.2931 score: 0.8837 time: 0.05s
Test loss: 0.3416 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0790;  Loss pred: 0.0525; Loss self: 2.6520; time: 0.34s
Val loss: 0.2930 score: 0.8837 time: 0.05s
Test loss: 0.3382 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0782;  Loss pred: 0.0517; Loss self: 2.6509; time: 0.20s
Val loss: 0.2931 score: 0.8837 time: 0.14s
Test loss: 0.3346 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0773;  Loss pred: 0.0508; Loss self: 2.6495; time: 0.31s
Val loss: 0.2932 score: 0.8837 time: 0.07s
Test loss: 0.3308 score: 0.8409 time: 0.21s
     INFO: Early stopping counter 15 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0763;  Loss pred: 0.0499; Loss self: 2.6479; time: 0.27s
Val loss: 0.2930 score: 0.9070 time: 0.09s
Test loss: 0.3277 score: 0.8409 time: 0.14s
     INFO: Early stopping counter 16 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0754;  Loss pred: 0.0489; Loss self: 2.6463; time: 0.16s
Val loss: 0.2928 score: 0.9070 time: 0.12s
Test loss: 0.3251 score: 0.8409 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0743;  Loss pred: 0.0479; Loss self: 2.6450; time: 0.16s
Val loss: 0.2925 score: 0.9070 time: 0.07s
Test loss: 0.3224 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0729;  Loss pred: 0.0464; Loss self: 2.6445; time: 0.20s
Val loss: 0.2925 score: 0.9070 time: 0.06s
Test loss: 0.3186 score: 0.8409 time: 0.10s
     INFO: Early stopping counter 19 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0712;  Loss pred: 0.0447; Loss self: 2.6454; time: 0.20s
Val loss: 0.2932 score: 0.9070 time: 0.07s
Test loss: 0.3147 score: 0.8636 time: 0.26s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 054,   Train_Loss: 0.0950,   Val_Loss: 0.2848,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8837,   Val_Loss: 0.2848,   Test_Precision: 0.8571,   Test_Recall: 0.8182,   Test_accuracy: 0.8372,   Test_Score: 0.8409,   Test_loss: 0.3573


[0.09606563299894333, 0.08854309399612248, 0.049770776997320354, 0.050327257020398974, 0.05540606798604131, 0.05867616296745837, 0.05348561704158783, 0.05658952007070184, 0.05386015505064279, 0.10040831798687577, 0.0657052380265668, 0.10157998895738274, 0.1749052699888125, 0.059011874021962285, 0.08106245298404247, 0.05915443098638207, 0.06653371499851346, 0.06127986405044794, 0.08926146104931831, 0.1094474500278011, 0.05587126698810607, 0.05250150500796735, 0.06439994496759027, 0.06231321208178997, 0.054105940042063594, 0.08244907297194004, 0.06033465499058366, 0.06931448599789292, 0.06755068805068731, 0.053143564029596746, 0.06526663806289434, 0.0559076169738546, 0.05418161698617041, 0.04843488708138466, 0.05273674090858549, 0.09587509802076966, 0.0784628790570423, 0.06309647695161402, 0.08952838799450547, 0.05525418499018997, 0.05465786694549024, 0.05772610497660935, 0.05422304000239819, 0.05006633803714067, 0.1845979579957202, 0.05472040502354503, 0.055079350946471095, 0.07205719605553895, 0.0551165530923754, 0.058669661986641586, 0.057289661024697125, 0.07161983603145927, 0.08916899503674358, 0.07028578699100763, 0.05867060599848628, 0.07019799307454377, 0.06337361398618668, 0.059662251034751534, 0.07086935197003186, 0.06294333399273455, 0.06158216996118426, 0.06969642301555723, 0.10453590808901936, 0.08953887003008276, 0.12751902197487652, 0.09313221299089491, 0.057385199004784226, 0.05416493897791952, 0.05322748201433569, 0.21813076303806156, 0.1409108330262825, 0.08583790203556418, 0.11551490996498615, 0.10235814901534468, 0.26469211210496724]
[0.0021833098408850756, 0.00201234304536642, 0.0011311540226663717, 0.0011438012959181585, 0.0012592288178645751, 0.0013335491583513265, 0.0012155822054906325, 0.0012861254561523144, 0.0012240944329691542, 0.002282007226974449, 0.0014933008642401546, 0.0023086361126677894, 0.003975119772473012, 0.0013411789550445974, 0.001842328476910056, 0.0013444188860541378, 0.0015121298863298514, 0.001392724182964726, 0.002028669569302689, 0.0024874420460863885, 0.0012698015224569563, 0.001193216022908349, 0.001463635112899779, 0.0014162093654952266, 0.0012296804555014453, 0.001873842567544092, 0.0013712421588769014, 0.001575329227224839, 0.0015352429102428935, 0.001207808273399926, 0.0014833326832475987, 0.0012706276584966954, 0.0012314003860493276, 0.0011007928882132876, 0.001198562293376943, 0.0021789795004720377, 0.001783247251296416, 0.0014340108398094096, 0.0020347360907842153, 0.0012557769315952266, 0.0012422242487611418, 0.001311956931286576, 0.0012323418182363225, 0.0011378713190259243, 0.004195408136266368, 0.0012436455687169325, 0.001251803430601616, 0.0016376635467167944, 0.0012526489339176226, 0.0013334014087873088, 0.0013020377505612983, 0.0016277235461695288, 0.0020265680690168997, 0.001597404249795628, 0.001333422863601961, 0.0015954089335123585, 0.00144030940877697, 0.0013559602507898076, 0.0016106670902279968, 0.0014305303180166943, 0.0013995947718450968, 0.001584009613989937, 0.002375816092932258, 0.0020349743188655175, 0.0028981595903381026, 0.0021166412043385208, 0.0013042090682905507, 0.0012310213404072617, 0.0012097155003258113, 0.004957517341774126, 0.0032025189324155112, 0.001950861409899186, 0.002625338862840594, 0.002326321568530561, 0.006015729820567437]
[458.02019542705744, 496.9331656958686, 884.0529052292865, 874.2777295048214, 794.1368445615941, 749.8786180753209, 822.6510683383858, 777.5291245627682, 816.9304369552661, 438.21070686346104, 669.6574173007233, 433.1561801848584, 251.56474703600628, 745.6126538808892, 542.7913711007686, 743.815793108199, 661.3188516676556, 718.017258716134, 492.93389871457885, 402.01941652202424, 787.5246503603857, 838.0712132599397, 683.230397512658, 706.110285925355, 813.2193981990345, 533.6627619206168, 729.2657927167566, 634.7879431918107, 651.3627213831512, 827.9459762144573, 674.1575988271267, 787.0126179868614, 812.0835524571144, 908.4361015659484, 834.332938326055, 458.9304304071551, 560.7747323165662, 697.3447984067591, 491.46422699692044, 796.3197721188344, 805.0076312689035, 762.2201431714269, 811.4631713392306, 878.8339975525975, 238.3558327390605, 804.0876155991122, 798.8474672252662, 610.6260361017444, 798.3082673231752, 749.9617095121206, 768.0268867541727, 614.3549390517013, 493.44505881073417, 626.0156125964609, 749.9496426052802, 626.7985461247599, 694.2952631609509, 737.4847451594019, 620.8607638828988, 699.0414585455363, 714.4925232049075, 631.3092996204207, 420.90799998150914, 491.40669281639504, 345.04656104301654, 472.44662815326495, 766.74823409311, 812.333602331514, 826.6406437965549, 201.71386826498406, 312.2542039886542, 512.5940750715228, 380.90320992620684, 429.8631855232541, 166.23086970778792]
Elapsed: 0.07761368011745313~0.03796836960714072
Time per graph: 0.0017639472753966624~0.0008629174910713801
Speed: 644.9656357278242~179.2711237109869
Total Time: 0.2652
best val loss: 0.2847553491592407 test_score: 0.8409

Testing...
Test loss: 0.3277 score: 0.8409 time: 0.06s
test Score 0.8409
Epoch Time List: [0.3288679119432345, 0.4660455980338156, 0.38897013396490365, 0.28381930908653885, 0.2940787529805675, 0.2817123760469258, 0.3252614460652694, 0.29265848791692406, 0.2645999869564548, 0.3049295919481665, 0.3312269210582599, 0.4712867710040882, 0.4626472679665312, 0.3166475840844214, 0.2955576250096783, 0.2605921638896689, 0.3448535749921575, 0.28413123288191855, 0.32972387899644673, 0.3689081050688401, 0.29426623007748276, 0.2525339029962197, 0.2592078309971839, 0.2842382341623306, 0.26103993807919323, 0.3277807709528133, 0.3677016559522599, 0.2839946490712464, 0.2998153731459752, 0.2429948019562289, 0.2763243360677734, 0.2717857201350853, 0.286811585072428, 0.291601498844102, 0.3317037670640275, 0.3029760620556772, 0.30135926604270935, 0.293776721926406, 0.32928699685726315, 0.3374282290460542, 0.28827611613087356, 0.28390934399794787, 0.28974784596357495, 0.3376118381274864, 0.4308742618886754, 0.4108435860835016, 0.3303012599935755, 0.3040991530288011, 0.25519975810311735, 0.26168006903026253, 0.2613855559611693, 0.2829981150571257, 0.29468201904091984, 0.3099397229962051, 0.2946053908672184, 0.32457137014716864, 0.281981089967303, 0.2683602829929441, 0.2903908089501783, 0.2925841851392761, 0.2928704209625721, 0.2979354120325297, 0.4723472020123154, 0.29933381790760905, 0.566951964981854, 0.4801859809085727, 0.2682512679602951, 0.44570807192940265, 0.3938619199907407, 0.5903075560927391, 0.4989373900461942, 0.3579790609655902, 0.3373327270383015, 0.35147908807266504, 0.52275140886195]
Total Epoch List: [75]
Total Time List: [0.26516593410633504]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbceef0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8420;  Loss pred: 0.8101; Loss self: 3.1875; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8131 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8026 score: 0.5116 time: 0.16s
Epoch 2/1000, LR 0.000000
Train loss: 0.8420;  Loss pred: 0.8101; Loss self: 3.1875; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7760 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7759 score: 0.5116 time: 0.07s
Epoch 3/1000, LR 0.000030
Train loss: 0.8113;  Loss pred: 0.7796; Loss self: 3.1687; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7218 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7373 score: 0.5116 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.7671;  Loss pred: 0.7357; Loss self: 3.1415; time: 0.16s
Val loss: 0.6834 score: 0.5455 time: 0.09s
Test loss: 0.7015 score: 0.4186 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7334;  Loss pred: 0.7017; Loss self: 3.1748; time: 0.25s
Val loss: 0.6727 score: 0.6818 time: 0.05s
Test loss: 0.6778 score: 0.5581 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7078;  Loss pred: 0.6754; Loss self: 3.2465; time: 0.18s
Val loss: 0.6751 score: 0.5455 time: 0.05s
Test loss: 0.6701 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6950;  Loss pred: 0.6620; Loss self: 3.3045; time: 0.17s
Val loss: 0.6769 score: 0.5227 time: 0.05s
Test loss: 0.6692 score: 0.6047 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6850;  Loss pred: 0.6517; Loss self: 3.3293; time: 0.18s
Val loss: 0.6764 score: 0.4545 time: 0.06s
Test loss: 0.6726 score: 0.5814 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6778;  Loss pred: 0.6445; Loss self: 3.3375; time: 0.19s
Val loss: 0.6742 score: 0.5227 time: 0.05s
Test loss: 0.6782 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6703;  Loss pred: 0.6368; Loss self: 3.3450; time: 0.16s
Val loss: 0.6708 score: 0.5455 time: 0.07s
Test loss: 0.6807 score: 0.5581 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.6603;  Loss pred: 0.6268; Loss self: 3.3492; time: 0.21s
Val loss: 0.6673 score: 0.6364 time: 0.06s
Test loss: 0.6847 score: 0.5814 time: 0.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.6500;  Loss pred: 0.6165; Loss self: 3.3492; time: 0.21s
Val loss: 0.6655 score: 0.6591 time: 0.06s
Test loss: 0.6896 score: 0.5349 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.6393;  Loss pred: 0.6058; Loss self: 3.3463; time: 0.22s
Val loss: 0.6649 score: 0.7045 time: 0.12s
Test loss: 0.6930 score: 0.5581 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.6265;  Loss pred: 0.5931; Loss self: 3.3388; time: 0.17s
Val loss: 0.6616 score: 0.7045 time: 0.06s
Test loss: 0.6945 score: 0.5581 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.6113;  Loss pred: 0.5781; Loss self: 3.3239; time: 0.18s
Val loss: 0.6559 score: 0.7045 time: 0.07s
Test loss: 0.6938 score: 0.5814 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.5957;  Loss pred: 0.5627; Loss self: 3.3001; time: 0.17s
Val loss: 0.6476 score: 0.7045 time: 0.06s
Test loss: 0.6904 score: 0.5814 time: 0.08s
Epoch 17/1000, LR 0.000270
Train loss: 0.5778;  Loss pred: 0.5451; Loss self: 3.2682; time: 0.22s
Val loss: 0.6383 score: 0.7045 time: 0.05s
Test loss: 0.6850 score: 0.5814 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.5569;  Loss pred: 0.5246; Loss self: 3.2301; time: 0.16s
Val loss: 0.6282 score: 0.6818 time: 0.07s
Test loss: 0.6761 score: 0.5814 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.5343;  Loss pred: 0.5024; Loss self: 3.1859; time: 0.28s
Val loss: 0.6164 score: 0.6818 time: 0.05s
Test loss: 0.6638 score: 0.5814 time: 0.06s
Epoch 20/1000, LR 0.000270
Train loss: 0.5100;  Loss pred: 0.4786; Loss self: 3.1367; time: 0.22s
Val loss: 0.6032 score: 0.6591 time: 0.06s
Test loss: 0.6522 score: 0.5581 time: 0.08s
Epoch 21/1000, LR 0.000270
Train loss: 0.4850;  Loss pred: 0.4542; Loss self: 3.0833; time: 0.20s
Val loss: 0.5872 score: 0.6818 time: 0.09s
Test loss: 0.6414 score: 0.5581 time: 0.09s
Epoch 22/1000, LR 0.000270
Train loss: 0.4605;  Loss pred: 0.4302; Loss self: 3.0280; time: 0.20s
Val loss: 0.5706 score: 0.6818 time: 0.09s
Test loss: 0.6322 score: 0.5581 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.4374;  Loss pred: 0.4077; Loss self: 2.9720; time: 0.23s
Val loss: 0.5544 score: 0.6818 time: 0.07s
Test loss: 0.6239 score: 0.5581 time: 0.07s
Epoch 24/1000, LR 0.000270
Train loss: 0.4150;  Loss pred: 0.3859; Loss self: 2.9146; time: 0.20s
Val loss: 0.5396 score: 0.6818 time: 0.08s
Test loss: 0.6149 score: 0.5581 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.3925;  Loss pred: 0.3639; Loss self: 2.8600; time: 0.20s
Val loss: 0.5290 score: 0.6818 time: 0.07s
Test loss: 0.6044 score: 0.6047 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.3704;  Loss pred: 0.3423; Loss self: 2.8088; time: 0.25s
Val loss: 0.5206 score: 0.6818 time: 0.06s
Test loss: 0.5945 score: 0.6279 time: 0.32s
Epoch 27/1000, LR 0.000270
Train loss: 0.3523;  Loss pred: 0.3246; Loss self: 2.7673; time: 0.24s
Val loss: 0.5111 score: 0.7045 time: 0.18s
Test loss: 0.5854 score: 0.6512 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.3352;  Loss pred: 0.3079; Loss self: 2.7291; time: 0.21s
Val loss: 0.4986 score: 0.7045 time: 0.06s
Test loss: 0.5776 score: 0.6744 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.3193;  Loss pred: 0.2922; Loss self: 2.7069; time: 0.19s
Val loss: 0.4883 score: 0.7273 time: 0.05s
Test loss: 0.5706 score: 0.6744 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.3050;  Loss pred: 0.2781; Loss self: 2.6937; time: 0.24s
Val loss: 0.4809 score: 0.7500 time: 0.11s
Test loss: 0.5599 score: 0.6977 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.2909;  Loss pred: 0.2641; Loss self: 2.6841; time: 0.19s
Val loss: 0.4715 score: 0.7500 time: 0.12s
Test loss: 0.5460 score: 0.7209 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.2773;  Loss pred: 0.2505; Loss self: 2.6783; time: 0.21s
Val loss: 0.4609 score: 0.7500 time: 0.07s
Test loss: 0.5353 score: 0.7209 time: 0.12s
Epoch 33/1000, LR 0.000270
Train loss: 0.2635;  Loss pred: 0.2368; Loss self: 2.6732; time: 0.18s
Val loss: 0.4476 score: 0.7500 time: 0.06s
Test loss: 0.5292 score: 0.7209 time: 0.06s
Epoch 34/1000, LR 0.000270
Train loss: 0.2499;  Loss pred: 0.2232; Loss self: 2.6670; time: 0.17s
Val loss: 0.4358 score: 0.7273 time: 0.05s
Test loss: 0.5265 score: 0.6744 time: 0.05s
Epoch 35/1000, LR 0.000270
Train loss: 0.2373;  Loss pred: 0.2107; Loss self: 2.6617; time: 0.18s
Val loss: 0.4291 score: 0.7500 time: 0.06s
Test loss: 0.5255 score: 0.6744 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.2252;  Loss pred: 0.1987; Loss self: 2.6532; time: 0.18s
Val loss: 0.4252 score: 0.7500 time: 0.06s
Test loss: 0.5241 score: 0.6512 time: 0.06s
Epoch 37/1000, LR 0.000270
Train loss: 0.2135;  Loss pred: 0.1871; Loss self: 2.6435; time: 0.18s
Val loss: 0.4222 score: 0.7500 time: 0.06s
Test loss: 0.5195 score: 0.6744 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.2030;  Loss pred: 0.1767; Loss self: 2.6323; time: 0.28s
Val loss: 0.4210 score: 0.7727 time: 0.08s
Test loss: 0.5113 score: 0.6977 time: 0.07s
Epoch 39/1000, LR 0.000269
Train loss: 0.1929;  Loss pred: 0.1667; Loss self: 2.6212; time: 0.19s
Val loss: 0.4202 score: 0.7955 time: 0.06s
Test loss: 0.5031 score: 0.7209 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.1824;  Loss pred: 0.1562; Loss self: 2.6139; time: 0.23s
Val loss: 0.4201 score: 0.7500 time: 0.06s
Test loss: 0.4958 score: 0.7209 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.1721;  Loss pred: 0.1460; Loss self: 2.6078; time: 0.16s
Val loss: 0.4198 score: 0.7727 time: 0.05s
Test loss: 0.4902 score: 0.7209 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.1635;  Loss pred: 0.1375; Loss self: 2.6027; time: 0.17s
Val loss: 0.4183 score: 0.7500 time: 0.06s
Test loss: 0.4879 score: 0.7674 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.1557;  Loss pred: 0.1297; Loss self: 2.6002; time: 0.20s
Val loss: 0.4148 score: 0.7955 time: 0.05s
Test loss: 0.4832 score: 0.7674 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.1482;  Loss pred: 0.1222; Loss self: 2.5970; time: 0.18s
Val loss: 0.4096 score: 0.8182 time: 0.05s
Test loss: 0.4784 score: 0.7674 time: 0.06s
Epoch 45/1000, LR 0.000269
Train loss: 0.1409;  Loss pred: 0.1150; Loss self: 2.5939; time: 0.18s
Val loss: 0.4065 score: 0.7955 time: 0.06s
Test loss: 0.4746 score: 0.7674 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.1339;  Loss pred: 0.1080; Loss self: 2.5901; time: 0.18s
Val loss: 0.4043 score: 0.8182 time: 0.06s
Test loss: 0.4723 score: 0.8140 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.1278;  Loss pred: 0.1019; Loss self: 2.5869; time: 0.21s
Val loss: 0.4041 score: 0.8182 time: 0.06s
Test loss: 0.4716 score: 0.8372 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.1220;  Loss pred: 0.0962; Loss self: 2.5841; time: 0.17s
Val loss: 0.4052 score: 0.8182 time: 0.05s
Test loss: 0.4732 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.1167;  Loss pred: 0.0909; Loss self: 2.5811; time: 0.16s
Val loss: 0.4069 score: 0.8182 time: 0.05s
Test loss: 0.4770 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.1118;  Loss pred: 0.0860; Loss self: 2.5789; time: 0.18s
Val loss: 0.4086 score: 0.8409 time: 0.08s
Test loss: 0.4814 score: 0.8140 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.1073;  Loss pred: 0.0815; Loss self: 2.5781; time: 0.18s
Val loss: 0.4109 score: 0.8409 time: 0.06s
Test loss: 0.4865 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.1031;  Loss pred: 0.0773; Loss self: 2.5788; time: 0.18s
Val loss: 0.4126 score: 0.8409 time: 0.07s
Test loss: 0.4895 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0989;  Loss pred: 0.0731; Loss self: 2.5804; time: 0.17s
Val loss: 0.4125 score: 0.8409 time: 0.06s
Test loss: 0.4905 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0949;  Loss pred: 0.0690; Loss self: 2.5822; time: 0.18s
Val loss: 0.4107 score: 0.8409 time: 0.06s
Test loss: 0.4891 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0904;  Loss pred: 0.0646; Loss self: 2.5845; time: 0.18s
Val loss: 0.4074 score: 0.8409 time: 0.06s
Test loss: 0.4854 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0860;  Loss pred: 0.0602; Loss self: 2.5864; time: 0.19s
Val loss: 0.4034 score: 0.8409 time: 0.06s
Test loss: 0.4820 score: 0.7907 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0818;  Loss pred: 0.0559; Loss self: 2.5882; time: 0.21s
Val loss: 0.4001 score: 0.8864 time: 0.07s
Test loss: 0.4794 score: 0.8140 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.0779;  Loss pred: 0.0520; Loss self: 2.5903; time: 0.20s
Val loss: 0.3982 score: 0.8864 time: 0.05s
Test loss: 0.4793 score: 0.8140 time: 0.06s
Epoch 59/1000, LR 0.000268
Train loss: 0.0744;  Loss pred: 0.0485; Loss self: 2.5935; time: 0.19s
Val loss: 0.3968 score: 0.8636 time: 0.05s
Test loss: 0.4810 score: 0.7907 time: 0.06s
Epoch 60/1000, LR 0.000268
Train loss: 0.0711;  Loss pred: 0.0451; Loss self: 2.5976; time: 0.18s
Val loss: 0.3956 score: 0.8636 time: 0.06s
Test loss: 0.4835 score: 0.7907 time: 0.07s
Epoch 61/1000, LR 0.000268
Train loss: 0.0680;  Loss pred: 0.0420; Loss self: 2.6023; time: 0.20s
Val loss: 0.3952 score: 0.8409 time: 0.06s
Test loss: 0.4875 score: 0.7907 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0652;  Loss pred: 0.0392; Loss self: 2.6076; time: 0.18s
Val loss: 0.3951 score: 0.8409 time: 0.06s
Test loss: 0.4923 score: 0.7907 time: 0.06s
Epoch 63/1000, LR 0.000268
Train loss: 0.0627;  Loss pred: 0.0365; Loss self: 2.6133; time: 0.18s
Val loss: 0.3957 score: 0.8409 time: 0.06s
Test loss: 0.4976 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0601;  Loss pred: 0.0339; Loss self: 2.6194; time: 0.17s
Val loss: 0.3967 score: 0.8409 time: 0.05s
Test loss: 0.5032 score: 0.8140 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0577;  Loss pred: 0.0315; Loss self: 2.6258; time: 0.19s
Val loss: 0.3981 score: 0.8409 time: 0.07s
Test loss: 0.5105 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0556;  Loss pred: 0.0293; Loss self: 2.6323; time: 0.18s
Val loss: 0.4002 score: 0.8409 time: 0.06s
Test loss: 0.5184 score: 0.8140 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0536;  Loss pred: 0.0272; Loss self: 2.6385; time: 0.18s
Val loss: 0.4029 score: 0.8409 time: 0.05s
Test loss: 0.5268 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0518;  Loss pred: 0.0253; Loss self: 2.6440; time: 0.17s
Val loss: 0.4065 score: 0.8409 time: 0.07s
Test loss: 0.5352 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0501;  Loss pred: 0.0236; Loss self: 2.6494; time: 0.17s
Val loss: 0.4111 score: 0.8409 time: 0.06s
Test loss: 0.5439 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0486;  Loss pred: 0.0220; Loss self: 2.6543; time: 0.17s
Val loss: 0.4156 score: 0.8409 time: 0.07s
Test loss: 0.5528 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0471;  Loss pred: 0.0206; Loss self: 2.6590; time: 0.19s
Val loss: 0.4195 score: 0.8409 time: 0.06s
Test loss: 0.5613 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0458;  Loss pred: 0.0192; Loss self: 2.6633; time: 0.20s
Val loss: 0.4227 score: 0.8409 time: 0.06s
Test loss: 0.5697 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0447;  Loss pred: 0.0180; Loss self: 2.6677; time: 0.19s
Val loss: 0.4254 score: 0.8409 time: 0.05s
Test loss: 0.5778 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0436;  Loss pred: 0.0169; Loss self: 2.6722; time: 0.20s
Val loss: 0.4278 score: 0.8409 time: 0.05s
Test loss: 0.5856 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0427;  Loss pred: 0.0159; Loss self: 2.6766; time: 0.17s
Val loss: 0.4302 score: 0.8409 time: 0.06s
Test loss: 0.5934 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0418;  Loss pred: 0.0150; Loss self: 2.6811; time: 0.18s
Val loss: 0.4320 score: 0.8409 time: 0.06s
Test loss: 0.6009 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0409;  Loss pred: 0.0141; Loss self: 2.6856; time: 0.18s
Val loss: 0.4334 score: 0.8409 time: 0.06s
Test loss: 0.6077 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0401;  Loss pred: 0.0132; Loss self: 2.6901; time: 0.21s
Val loss: 0.4348 score: 0.8409 time: 0.06s
Test loss: 0.6132 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0394;  Loss pred: 0.0125; Loss self: 2.6945; time: 0.19s
Val loss: 0.4364 score: 0.8409 time: 0.06s
Test loss: 0.6187 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0388;  Loss pred: 0.0118; Loss self: 2.6987; time: 0.19s
Val loss: 0.4384 score: 0.8409 time: 0.06s
Test loss: 0.6242 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0382;  Loss pred: 0.0112; Loss self: 2.7027; time: 0.20s
Val loss: 0.4405 score: 0.8409 time: 0.06s
Test loss: 0.6297 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0377;  Loss pred: 0.0106; Loss self: 2.7066; time: 0.18s
Val loss: 0.4424 score: 0.8409 time: 0.05s
Test loss: 0.6346 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 061,   Train_Loss: 0.0652,   Val_Loss: 0.3951,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3951,   Test_Precision: 0.8421,   Test_Recall: 0.7273,   Test_accuracy: 0.7805,   Test_Score: 0.7907,   Test_loss: 0.4923


[0.09606563299894333, 0.08854309399612248, 0.049770776997320354, 0.050327257020398974, 0.05540606798604131, 0.05867616296745837, 0.05348561704158783, 0.05658952007070184, 0.05386015505064279, 0.10040831798687577, 0.0657052380265668, 0.10157998895738274, 0.1749052699888125, 0.059011874021962285, 0.08106245298404247, 0.05915443098638207, 0.06653371499851346, 0.06127986405044794, 0.08926146104931831, 0.1094474500278011, 0.05587126698810607, 0.05250150500796735, 0.06439994496759027, 0.06231321208178997, 0.054105940042063594, 0.08244907297194004, 0.06033465499058366, 0.06931448599789292, 0.06755068805068731, 0.053143564029596746, 0.06526663806289434, 0.0559076169738546, 0.05418161698617041, 0.04843488708138466, 0.05273674090858549, 0.09587509802076966, 0.0784628790570423, 0.06309647695161402, 0.08952838799450547, 0.05525418499018997, 0.05465786694549024, 0.05772610497660935, 0.05422304000239819, 0.05006633803714067, 0.1845979579957202, 0.05472040502354503, 0.055079350946471095, 0.07205719605553895, 0.0551165530923754, 0.058669661986641586, 0.057289661024697125, 0.07161983603145927, 0.08916899503674358, 0.07028578699100763, 0.05867060599848628, 0.07019799307454377, 0.06337361398618668, 0.059662251034751534, 0.07086935197003186, 0.06294333399273455, 0.06158216996118426, 0.06969642301555723, 0.10453590808901936, 0.08953887003008276, 0.12751902197487652, 0.09313221299089491, 0.057385199004784226, 0.05416493897791952, 0.05322748201433569, 0.21813076303806156, 0.1409108330262825, 0.08583790203556418, 0.11551490996498615, 0.10235814901534468, 0.26469211210496724, 0.1595605700276792, 0.07900628901552409, 0.07137339701876044, 0.09831511811353266, 0.08259711798746139, 0.05662894994020462, 0.11063014389947057, 0.09152446303050965, 0.05044538201764226, 0.05042748700361699, 0.1061776620335877, 0.05384348810184747, 0.06537774100434035, 0.054581934004090726, 0.052074930048547685, 0.0876909070648253, 0.052753797033801675, 0.06874799798242748, 0.06799923093058169, 0.08369376801420003, 0.09752725798171014, 0.055329455994069576, 0.08276798203587532, 0.08618082595057786, 0.07799265207722783, 0.32262772298417985, 0.05227535299491137, 0.05185305094346404, 0.07376122893765569, 0.06774325401056558, 0.09453555394429713, 0.12538466602563858, 0.06159613898489624, 0.05797191592864692, 0.057970541063696146, 0.06763410498388112, 0.05957451194990426, 0.07189204404130578, 0.060775225050747395, 0.06739445705898106, 0.05604912503622472, 0.0543121270602569, 0.06357618805486709, 0.060917446040548384, 0.05593392497394234, 0.06014202407095581, 0.05462468299083412, 0.054236855008639395, 0.059678885038010776, 0.08692099596373737, 0.06380074203480035, 0.06188802293036133, 0.06448649894446135, 0.0680591108975932, 0.05586791504174471, 0.053181080031208694, 0.057239271933212876, 0.06706278899218887, 0.06402530404739082, 0.07024848693981767, 0.06328373006545007, 0.0633432730101049, 0.051991246989928186, 0.07939420000184327, 0.0571343710180372, 0.06697018607519567, 0.05574237392283976, 0.05637404206208885, 0.07580773194786161, 0.07003694702871144, 0.058840414974838495, 0.06313284102361649, 0.07293612300418317, 0.07017405505757779, 0.0603686620015651, 0.059039001003839076, 0.05650870897807181, 0.06676067493390292, 0.06349029194097966, 0.06974090496078134, 0.05941038893070072, 0.06805636291392148]
[0.0021833098408850756, 0.00201234304536642, 0.0011311540226663717, 0.0011438012959181585, 0.0012592288178645751, 0.0013335491583513265, 0.0012155822054906325, 0.0012861254561523144, 0.0012240944329691542, 0.002282007226974449, 0.0014933008642401546, 0.0023086361126677894, 0.003975119772473012, 0.0013411789550445974, 0.001842328476910056, 0.0013444188860541378, 0.0015121298863298514, 0.001392724182964726, 0.002028669569302689, 0.0024874420460863885, 0.0012698015224569563, 0.001193216022908349, 0.001463635112899779, 0.0014162093654952266, 0.0012296804555014453, 0.001873842567544092, 0.0013712421588769014, 0.001575329227224839, 0.0015352429102428935, 0.001207808273399926, 0.0014833326832475987, 0.0012706276584966954, 0.0012314003860493276, 0.0011007928882132876, 0.001198562293376943, 0.0021789795004720377, 0.001783247251296416, 0.0014340108398094096, 0.0020347360907842153, 0.0012557769315952266, 0.0012422242487611418, 0.001311956931286576, 0.0012323418182363225, 0.0011378713190259243, 0.004195408136266368, 0.0012436455687169325, 0.001251803430601616, 0.0016376635467167944, 0.0012526489339176226, 0.0013334014087873088, 0.0013020377505612983, 0.0016277235461695288, 0.0020265680690168997, 0.001597404249795628, 0.001333422863601961, 0.0015954089335123585, 0.00144030940877697, 0.0013559602507898076, 0.0016106670902279968, 0.0014305303180166943, 0.0013995947718450968, 0.001584009613989937, 0.002375816092932258, 0.0020349743188655175, 0.0028981595903381026, 0.0021166412043385208, 0.0013042090682905507, 0.0012310213404072617, 0.0012097155003258113, 0.004957517341774126, 0.0032025189324155112, 0.001950861409899186, 0.002625338862840594, 0.002326321568530561, 0.006015729820567437, 0.0037107109308762605, 0.0018373555585005602, 0.0016598464422967545, 0.0022863980956635502, 0.00192086320901073, 0.0013169523241908052, 0.0025727940441737343, 0.002128475884430457, 0.0011731484190149362, 0.0011727322558980696, 0.002469247954269481, 0.0012521741419034295, 0.0015204125814962872, 0.0012693473024207147, 0.0012110448848499462, 0.0020393234201122163, 0.0012268324891581786, 0.0015987906507541273, 0.0015813774635018998, 0.0019463666980046518, 0.0022680757670165146, 0.001286731534745804, 0.0019248367915319841, 0.0020042052546646013, 0.0018137826064471589, 0.007502970301957671, 0.00121570588360259, 0.0012058849056619545, 0.0017153774171547835, 0.0015754245118736181, 0.002198501254518538, 0.002915922465712525, 0.0014324683484859592, 0.0013481840913638818, 0.0013481521177603755, 0.00157288616241584, 0.0013854537662768432, 0.0016719080009605996, 0.0014133773267615674, 0.0015673129548600248, 0.0013034680240982493, 0.0012630727223315557, 0.0014785160012759787, 0.00141668479164066, 0.00130078895288238, 0.0013986517225803678, 0.001270341464903119, 0.0012613222095032418, 0.0013878810473955995, 0.00202141851078459, 0.0014837381868558221, 0.0014392563472177054, 0.0014996860219642174, 0.0015827700208742605, 0.0012992538381801095, 0.001236769303051365, 0.0013311458589119274, 0.0015595997440043924, 0.001488960559241647, 0.0016336857427864573, 0.0014717146526848854, 0.001473099372328021, 0.0012090987672076323, 0.0018463767442289134, 0.001328706302745051, 0.0015574461877952482, 0.0012963342772753432, 0.0013110242340020662, 0.0017629705104153864, 0.0016287662099700334, 0.0013683817436008952, 0.0014682056052003835, 0.0016961889070740274, 0.0016319547687808787, 0.001403922372129421, 0.0013730000233450948, 0.0013141560227458561, 0.0015525738356721608, 0.001476518417232085, 0.001621881510715845, 0.001381636951876761, 0.0015827061142772436]
[458.02019542705744, 496.9331656958686, 884.0529052292865, 874.2777295048214, 794.1368445615941, 749.8786180753209, 822.6510683383858, 777.5291245627682, 816.9304369552661, 438.21070686346104, 669.6574173007233, 433.1561801848584, 251.56474703600628, 745.6126538808892, 542.7913711007686, 743.815793108199, 661.3188516676556, 718.017258716134, 492.93389871457885, 402.01941652202424, 787.5246503603857, 838.0712132599397, 683.230397512658, 706.110285925355, 813.2193981990345, 533.6627619206168, 729.2657927167566, 634.7879431918107, 651.3627213831512, 827.9459762144573, 674.1575988271267, 787.0126179868614, 812.0835524571144, 908.4361015659484, 834.332938326055, 458.9304304071551, 560.7747323165662, 697.3447984067591, 491.46422699692044, 796.3197721188344, 805.0076312689035, 762.2201431714269, 811.4631713392306, 878.8339975525975, 238.3558327390605, 804.0876155991122, 798.8474672252662, 610.6260361017444, 798.3082673231752, 749.9617095121206, 768.0268867541727, 614.3549390517013, 493.44505881073417, 626.0156125964609, 749.9496426052802, 626.7985461247599, 694.2952631609509, 737.4847451594019, 620.8607638828988, 699.0414585455363, 714.4925232049075, 631.3092996204207, 420.90799998150914, 491.40669281639504, 345.04656104301654, 472.44662815326495, 766.74823409311, 812.333602331514, 826.6406437965549, 201.71386826498406, 312.2542039886542, 512.5940750715228, 380.90320992620684, 429.8631855232541, 166.23086970778792, 269.49013777364127, 544.2604700943599, 602.4653693966322, 437.36915364679027, 520.5992781313216, 759.3289306159546, 388.6824918086885, 469.81974628647606, 852.4070644357815, 852.7095549479941, 404.98160513647025, 798.6109651489052, 657.7162095145699, 787.8064561944122, 825.7332263319905, 490.3587092355237, 815.1072039885207, 625.4727593811697, 632.3600930707197, 513.7777999516563, 440.9023783695841, 777.1628914010813, 519.5245666538286, 498.9508922165497, 551.3339892253141, 133.2805488699696, 822.5673770999832, 829.2665372165541, 582.9620875262852, 634.7495500185672, 454.85532380057504, 342.9446467657168, 698.0957038645533, 741.738466138075, 741.7560576630292, 635.7739192416011, 721.7851828338663, 598.1190349142698, 707.5251463749405, 638.034667485607, 767.1841437704686, 791.7200508883293, 676.3538569328887, 705.8733219278099, 768.7642163504921, 714.9742740495135, 787.1899230466069, 792.8188312753482, 720.5228444300252, 494.702108774032, 673.9733524814726, 694.8032585947232, 666.806241676006, 631.8037281547947, 769.6725386632066, 808.5582311372006, 751.2324763699415, 641.1901539765729, 671.6094619116823, 612.112827950848, 679.4795432495526, 678.8408296038064, 827.0622939344005, 541.6012756473607, 752.6117682546114, 642.0767586298567, 771.4059695326551, 762.76240672331, 567.2244624014628, 613.9616563008133, 730.7902233250358, 681.1035160593315, 589.5569743614393, 612.7620808676157, 712.2900951305695, 728.3321070626496, 760.9446539768966, 644.0917507585503, 677.2688971090673, 616.5678524559004, 723.7791365102386, 631.8292391614716]
Elapsed: 0.07450988747707456~0.03559719188544142
Time per graph: 0.0017131915698779374~0.0008161957550197033
Speed: 648.286740164184~158.61300674630473
Total Time: 0.0689
best val loss: 0.3950978219509125 test_score: 0.7907

Testing...
Test loss: 0.4794 score: 0.8140 time: 0.10s
test Score 0.8140
Epoch Time List: [0.3288679119432345, 0.4660455980338156, 0.38897013396490365, 0.28381930908653885, 0.2940787529805675, 0.2817123760469258, 0.3252614460652694, 0.29265848791692406, 0.2645999869564548, 0.3049295919481665, 0.3312269210582599, 0.4712867710040882, 0.4626472679665312, 0.3166475840844214, 0.2955576250096783, 0.2605921638896689, 0.3448535749921575, 0.28413123288191855, 0.32972387899644673, 0.3689081050688401, 0.29426623007748276, 0.2525339029962197, 0.2592078309971839, 0.2842382341623306, 0.26103993807919323, 0.3277807709528133, 0.3677016559522599, 0.2839946490712464, 0.2998153731459752, 0.2429948019562289, 0.2763243360677734, 0.2717857201350853, 0.286811585072428, 0.291601498844102, 0.3317037670640275, 0.3029760620556772, 0.30135926604270935, 0.293776721926406, 0.32928699685726315, 0.3374282290460542, 0.28827611613087356, 0.28390934399794787, 0.28974784596357495, 0.3376118381274864, 0.4308742618886754, 0.4108435860835016, 0.3303012599935755, 0.3040991530288011, 0.25519975810311735, 0.26168006903026253, 0.2613855559611693, 0.2829981150571257, 0.29468201904091984, 0.3099397229962051, 0.2946053908672184, 0.32457137014716864, 0.281981089967303, 0.2683602829929441, 0.2903908089501783, 0.2925841851392761, 0.2928704209625721, 0.2979354120325297, 0.4723472020123154, 0.29933381790760905, 0.566951964981854, 0.4801859809085727, 0.2682512679602951, 0.44570807192940265, 0.3938619199907407, 0.5903075560927391, 0.4989373900461942, 0.3579790609655902, 0.3373327270383015, 0.35147908807266504, 0.52275140886195, 0.5434526210883632, 0.32357407093513757, 0.3119497940642759, 0.343512047897093, 0.37684068409726024, 0.29147976415697485, 0.3273377420846373, 0.32244240888394415, 0.2833342249505222, 0.2714722231030464, 0.3696827990934253, 0.3111739590531215, 0.3966994290240109, 0.27875840791966766, 0.29539934906642884, 0.31489696307107806, 0.3198281889781356, 0.2963068970711902, 0.39586834295187145, 0.36149473499972373, 0.38008829311002046, 0.3460551301250234, 0.36803865199908614, 0.3589352349517867, 0.35028093005530536, 0.6191439511021599, 0.4721750250319019, 0.324119680095464, 0.3149337860522792, 0.41858164104633033, 0.40508626005612314, 0.39312728797085583, 0.29266368213575333, 0.27450285491067916, 0.28496602503582835, 0.3109124780166894, 0.2937601748853922, 0.42538453615270555, 0.3059932349715382, 0.357514234026894, 0.2637308550765738, 0.27158298494759947, 0.3156238959636539, 0.28711505292449147, 0.2965253960574046, 0.30267358396667987, 0.3184525810647756, 0.27407831395976245, 0.2671594660496339, 0.3427204169565812, 0.3057041859719902, 0.30424070404842496, 0.2884243020089343, 0.308404030976817, 0.2888137889094651, 0.2928337298799306, 0.3283633909886703, 0.3048158041201532, 0.2977478419197723, 0.30082057893741876, 0.31586113199591637, 0.30419717507902533, 0.2907313600881025, 0.29450040694791824, 0.30893636809196323, 0.29663104109931737, 0.2880605610553175, 0.29153854807373136, 0.3058122581569478, 0.30686920799780637, 0.30746007012203336, 0.31139945704489946, 0.3154189460910857, 0.3157516779610887, 0.2848281830083579, 0.2914524241350591, 0.28319944080431014, 0.32444968493655324, 0.30855557695031166, 0.3176498948596418, 0.313905430957675, 0.29866791889071465]
Total Epoch List: [75, 82]
Total Time List: [0.26516593410633504, 0.06886991788633168]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x71e93cbce230>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7225;  Loss pred: 0.6888; Loss self: 3.3756; time: 0.17s
Val loss: 0.6748 score: 0.6364 time: 0.06s
Test loss: 0.6851 score: 0.5814 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7225;  Loss pred: 0.6888; Loss self: 3.3756; time: 0.17s
Val loss: 0.6649 score: 0.6364 time: 0.06s
Test loss: 0.6770 score: 0.5814 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.7044;  Loss pred: 0.6702; Loss self: 3.4168; time: 0.18s
Val loss: 0.6615 score: 0.6364 time: 0.11s
Test loss: 0.6687 score: 0.5814 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.6881;  Loss pred: 0.6539; Loss self: 3.4240; time: 0.18s
Val loss: 0.6642 score: 0.5909 time: 0.06s
Test loss: 0.6553 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6773;  Loss pred: 0.6434; Loss self: 3.3957; time: 0.18s
Val loss: 0.6663 score: 0.5682 time: 0.06s
Test loss: 0.6429 score: 0.5349 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6678;  Loss pred: 0.6344; Loss self: 3.3372; time: 0.17s
Val loss: 0.6609 score: 0.6136 time: 0.08s
Test loss: 0.6221 score: 0.6744 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6501;  Loss pred: 0.6171; Loss self: 3.3032; time: 0.22s
Val loss: 0.6543 score: 0.5909 time: 0.07s
Test loss: 0.6054 score: 0.7674 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.6408;  Loss pred: 0.6080; Loss self: 3.2804; time: 0.23s
Val loss: 0.6505 score: 0.5909 time: 0.07s
Test loss: 0.5922 score: 0.7442 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.6268;  Loss pred: 0.5944; Loss self: 3.2439; time: 0.22s
Val loss: 0.6426 score: 0.6364 time: 0.06s
Test loss: 0.5832 score: 0.7442 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6071;  Loss pred: 0.5749; Loss self: 3.2194; time: 0.21s
Val loss: 0.6356 score: 0.6136 time: 0.05s
Test loss: 0.5778 score: 0.7209 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.5898;  Loss pred: 0.5575; Loss self: 3.2284; time: 0.21s
Val loss: 0.6230 score: 0.6364 time: 0.05s
Test loss: 0.5523 score: 0.7907 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.5673;  Loss pred: 0.5350; Loss self: 3.2312; time: 0.20s
Val loss: 0.6161 score: 0.6364 time: 0.09s
Test loss: 0.5264 score: 0.7907 time: 0.15s
Epoch 13/1000, LR 0.000270
Train loss: 0.5433;  Loss pred: 0.5113; Loss self: 3.1957; time: 0.23s
Val loss: 0.6062 score: 0.6364 time: 0.06s
Test loss: 0.5042 score: 0.8140 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.5169;  Loss pred: 0.4856; Loss self: 3.1248; time: 0.19s
Val loss: 0.5973 score: 0.6136 time: 0.07s
Test loss: 0.4889 score: 0.7907 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.4907;  Loss pred: 0.4599; Loss self: 3.0811; time: 0.19s
Val loss: 0.5840 score: 0.6364 time: 0.05s
Test loss: 0.4539 score: 0.8140 time: 0.06s
Epoch 16/1000, LR 0.000270
Train loss: 0.4528;  Loss pred: 0.4223; Loss self: 3.0451; time: 0.19s
Val loss: 0.5832 score: 0.6136 time: 0.06s
Test loss: 0.4314 score: 0.8140 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.4310;  Loss pred: 0.4009; Loss self: 3.0086; time: 0.20s
Val loss: 0.5548 score: 0.6364 time: 0.07s
Test loss: 0.4160 score: 0.8605 time: 0.07s
Epoch 18/1000, LR 0.000270
Train loss: 0.3989;  Loss pred: 0.3692; Loss self: 2.9690; time: 0.20s
Val loss: 0.5316 score: 0.7500 time: 0.06s
Test loss: 0.4172 score: 0.7907 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.3756;  Loss pred: 0.3465; Loss self: 2.9152; time: 0.21s
Val loss: 0.5143 score: 0.7273 time: 0.06s
Test loss: 0.3974 score: 0.8140 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.3445;  Loss pred: 0.3165; Loss self: 2.8082; time: 0.21s
Val loss: 0.5086 score: 0.7500 time: 0.05s
Test loss: 0.3784 score: 0.8372 time: 0.07s
Epoch 21/1000, LR 0.000270
Train loss: 0.3221;  Loss pred: 0.2951; Loss self: 2.6970; time: 0.19s
Val loss: 0.4991 score: 0.7273 time: 0.05s
Test loss: 0.3738 score: 0.8837 time: 0.08s
Epoch 22/1000, LR 0.000270
Train loss: 0.2958;  Loss pred: 0.2694; Loss self: 2.6368; time: 0.21s
Val loss: 0.4970 score: 0.6818 time: 0.06s
Test loss: 0.3735 score: 0.8372 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.2731;  Loss pred: 0.2471; Loss self: 2.5997; time: 0.19s
Val loss: 0.4990 score: 0.7045 time: 0.06s
Test loss: 0.3549 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2482;  Loss pred: 0.2225; Loss self: 2.5695; time: 0.21s
Val loss: 0.5093 score: 0.7045 time: 0.06s
Test loss: 0.3344 score: 0.8837 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2291;  Loss pred: 0.2036; Loss self: 2.5492; time: 0.21s
Val loss: 0.5122 score: 0.7045 time: 0.05s
Test loss: 0.3271 score: 0.8837 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2153;  Loss pred: 0.1899; Loss self: 2.5400; time: 0.28s
Val loss: 0.5029 score: 0.7273 time: 0.11s
Test loss: 0.3244 score: 0.8837 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2026;  Loss pred: 0.1774; Loss self: 2.5282; time: 0.19s
Val loss: 0.4910 score: 0.7273 time: 0.06s
Test loss: 0.3176 score: 0.8837 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.1897;  Loss pred: 0.1647; Loss self: 2.5024; time: 0.18s
Val loss: 0.4790 score: 0.7500 time: 0.05s
Test loss: 0.3060 score: 0.9070 time: 0.06s
Epoch 29/1000, LR 0.000270
Train loss: 0.1772;  Loss pred: 0.1525; Loss self: 2.4760; time: 0.19s
Val loss: 0.4688 score: 0.7500 time: 0.05s
Test loss: 0.2967 score: 0.9070 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.1663;  Loss pred: 0.1417; Loss self: 2.4587; time: 0.22s
Val loss: 0.4556 score: 0.7500 time: 0.05s
Test loss: 0.2904 score: 0.9070 time: 0.06s
Epoch 31/1000, LR 0.000270
Train loss: 0.1561;  Loss pred: 0.1316; Loss self: 2.4501; time: 0.22s
Val loss: 0.4471 score: 0.7727 time: 0.06s
Test loss: 0.2862 score: 0.9070 time: 0.09s
Epoch 32/1000, LR 0.000270
Train loss: 0.1474;  Loss pred: 0.1229; Loss self: 2.4489; time: 0.19s
Val loss: 0.4473 score: 0.7727 time: 0.11s
Test loss: 0.2816 score: 0.9070 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1396;  Loss pred: 0.1151; Loss self: 2.4499; time: 0.20s
Val loss: 0.4587 score: 0.7727 time: 0.07s
Test loss: 0.2769 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1310;  Loss pred: 0.1065; Loss self: 2.4514; time: 0.21s
Val loss: 0.4765 score: 0.7500 time: 0.06s
Test loss: 0.2759 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1232;  Loss pred: 0.0987; Loss self: 2.4542; time: 0.20s
Val loss: 0.4925 score: 0.7500 time: 0.06s
Test loss: 0.2773 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1162;  Loss pred: 0.0917; Loss self: 2.4581; time: 0.19s
Val loss: 0.5005 score: 0.7500 time: 0.05s
Test loss: 0.2798 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1095;  Loss pred: 0.0849; Loss self: 2.4635; time: 0.20s
Val loss: 0.5040 score: 0.7500 time: 0.06s
Test loss: 0.2830 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1033;  Loss pred: 0.0786; Loss self: 2.4684; time: 0.18s
Val loss: 0.5107 score: 0.7727 time: 0.06s
Test loss: 0.2858 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0981;  Loss pred: 0.0734; Loss self: 2.4711; time: 0.18s
Val loss: 0.5216 score: 0.7727 time: 0.05s
Test loss: 0.2866 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0929;  Loss pred: 0.0682; Loss self: 2.4724; time: 0.18s
Val loss: 0.5435 score: 0.7500 time: 0.06s
Test loss: 0.2867 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0880;  Loss pred: 0.0632; Loss self: 2.4743; time: 0.19s
Val loss: 0.5665 score: 0.7500 time: 0.13s
Test loss: 0.2889 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0837;  Loss pred: 0.0589; Loss self: 2.4765; time: 0.19s
Val loss: 0.5845 score: 0.7500 time: 0.05s
Test loss: 0.2911 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0796;  Loss pred: 0.0549; Loss self: 2.4789; time: 0.17s
Val loss: 0.5996 score: 0.7500 time: 0.05s
Test loss: 0.2923 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0760;  Loss pred: 0.0512; Loss self: 2.4828; time: 0.18s
Val loss: 0.6137 score: 0.7500 time: 0.05s
Test loss: 0.2938 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0728;  Loss pred: 0.0480; Loss self: 2.4861; time: 0.17s
Val loss: 0.6330 score: 0.7500 time: 0.05s
Test loss: 0.2970 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0698;  Loss pred: 0.0449; Loss self: 2.4906; time: 0.27s
Val loss: 0.6540 score: 0.7500 time: 0.11s
Test loss: 0.3016 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0667;  Loss pred: 0.0417; Loss self: 2.4969; time: 0.16s
Val loss: 0.6780 score: 0.7500 time: 0.05s
Test loss: 0.3081 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0639;  Loss pred: 0.0388; Loss self: 2.5050; time: 0.17s
Val loss: 0.6985 score: 0.7500 time: 0.05s
Test loss: 0.3138 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0614;  Loss pred: 0.0363; Loss self: 2.5121; time: 0.17s
Val loss: 0.7118 score: 0.7500 time: 0.05s
Test loss: 0.3169 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0591;  Loss pred: 0.0340; Loss self: 2.5172; time: 0.17s
Val loss: 0.7189 score: 0.7500 time: 0.10s
Test loss: 0.3181 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0569;  Loss pred: 0.0317; Loss self: 2.5213; time: 0.22s
Val loss: 0.7238 score: 0.7500 time: 0.07s
Test loss: 0.3188 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 030,   Train_Loss: 0.1561,   Val_Loss: 0.4471,   Val_Precision: 0.8750,   Val_Recall: 0.6364,   Val_accuracy: 0.7368,   Val_Score: 0.7727,   Val_Loss: 0.4471,   Test_Precision: 0.8696,   Test_Recall: 0.9524,   Test_accuracy: 0.9091,   Test_Score: 0.9070,   Test_loss: 0.2862


[0.09606563299894333, 0.08854309399612248, 0.049770776997320354, 0.050327257020398974, 0.05540606798604131, 0.05867616296745837, 0.05348561704158783, 0.05658952007070184, 0.05386015505064279, 0.10040831798687577, 0.0657052380265668, 0.10157998895738274, 0.1749052699888125, 0.059011874021962285, 0.08106245298404247, 0.05915443098638207, 0.06653371499851346, 0.06127986405044794, 0.08926146104931831, 0.1094474500278011, 0.05587126698810607, 0.05250150500796735, 0.06439994496759027, 0.06231321208178997, 0.054105940042063594, 0.08244907297194004, 0.06033465499058366, 0.06931448599789292, 0.06755068805068731, 0.053143564029596746, 0.06526663806289434, 0.0559076169738546, 0.05418161698617041, 0.04843488708138466, 0.05273674090858549, 0.09587509802076966, 0.0784628790570423, 0.06309647695161402, 0.08952838799450547, 0.05525418499018997, 0.05465786694549024, 0.05772610497660935, 0.05422304000239819, 0.05006633803714067, 0.1845979579957202, 0.05472040502354503, 0.055079350946471095, 0.07205719605553895, 0.0551165530923754, 0.058669661986641586, 0.057289661024697125, 0.07161983603145927, 0.08916899503674358, 0.07028578699100763, 0.05867060599848628, 0.07019799307454377, 0.06337361398618668, 0.059662251034751534, 0.07086935197003186, 0.06294333399273455, 0.06158216996118426, 0.06969642301555723, 0.10453590808901936, 0.08953887003008276, 0.12751902197487652, 0.09313221299089491, 0.057385199004784226, 0.05416493897791952, 0.05322748201433569, 0.21813076303806156, 0.1409108330262825, 0.08583790203556418, 0.11551490996498615, 0.10235814901534468, 0.26469211210496724, 0.1595605700276792, 0.07900628901552409, 0.07137339701876044, 0.09831511811353266, 0.08259711798746139, 0.05662894994020462, 0.11063014389947057, 0.09152446303050965, 0.05044538201764226, 0.05042748700361699, 0.1061776620335877, 0.05384348810184747, 0.06537774100434035, 0.054581934004090726, 0.052074930048547685, 0.0876909070648253, 0.052753797033801675, 0.06874799798242748, 0.06799923093058169, 0.08369376801420003, 0.09752725798171014, 0.055329455994069576, 0.08276798203587532, 0.08618082595057786, 0.07799265207722783, 0.32262772298417985, 0.05227535299491137, 0.05185305094346404, 0.07376122893765569, 0.06774325401056558, 0.09453555394429713, 0.12538466602563858, 0.06159613898489624, 0.05797191592864692, 0.057970541063696146, 0.06763410498388112, 0.05957451194990426, 0.07189204404130578, 0.060775225050747395, 0.06739445705898106, 0.05604912503622472, 0.0543121270602569, 0.06357618805486709, 0.060917446040548384, 0.05593392497394234, 0.06014202407095581, 0.05462468299083412, 0.054236855008639395, 0.059678885038010776, 0.08692099596373737, 0.06380074203480035, 0.06188802293036133, 0.06448649894446135, 0.0680591108975932, 0.05586791504174471, 0.053181080031208694, 0.057239271933212876, 0.06706278899218887, 0.06402530404739082, 0.07024848693981767, 0.06328373006545007, 0.0633432730101049, 0.051991246989928186, 0.07939420000184327, 0.0571343710180372, 0.06697018607519567, 0.05574237392283976, 0.05637404206208885, 0.07580773194786161, 0.07003694702871144, 0.058840414974838495, 0.06313284102361649, 0.07293612300418317, 0.07017405505757779, 0.0603686620015651, 0.059039001003839076, 0.05650870897807181, 0.06676067493390292, 0.06349029194097966, 0.06974090496078134, 0.05941038893070072, 0.06805636291392148, 0.06573443405795842, 0.06442197598516941, 0.060065582976676524, 0.06150970293674618, 0.09412923501804471, 0.0666618850082159, 0.06768773903604597, 0.05760036804713309, 0.06167200894560665, 0.07476819306612015, 0.0688815169269219, 0.15236493200063705, 0.06028627196792513, 0.06410140497609973, 0.06893175502773374, 0.06072783702984452, 0.07330645399633795, 0.0639476099750027, 0.07049877196550369, 0.07261514605488628, 0.08067234500776976, 0.06400673894677311, 0.07517873903270811, 0.07105715700890869, 0.10970114090014249, 0.10790290404111147, 0.08114124101120979, 0.06366894498933107, 0.07303895510267466, 0.0697858560597524, 0.09600484906695783, 0.09174998803064227, 0.0620938140200451, 0.056758755003102124, 0.05991280800662935, 0.056698323925957084, 0.056106869014911354, 0.05659145209938288, 0.055243699927814305, 0.056277906987816095, 0.05579567397944629, 0.055644663982093334, 0.05616492300760001, 0.0643643889343366, 0.05513217393308878, 0.06821682408917695, 0.0552232920890674, 0.05642834701575339, 0.058751694043166935, 0.0572813410544768, 0.06868528004270047]
[0.0021833098408850756, 0.00201234304536642, 0.0011311540226663717, 0.0011438012959181585, 0.0012592288178645751, 0.0013335491583513265, 0.0012155822054906325, 0.0012861254561523144, 0.0012240944329691542, 0.002282007226974449, 0.0014933008642401546, 0.0023086361126677894, 0.003975119772473012, 0.0013411789550445974, 0.001842328476910056, 0.0013444188860541378, 0.0015121298863298514, 0.001392724182964726, 0.002028669569302689, 0.0024874420460863885, 0.0012698015224569563, 0.001193216022908349, 0.001463635112899779, 0.0014162093654952266, 0.0012296804555014453, 0.001873842567544092, 0.0013712421588769014, 0.001575329227224839, 0.0015352429102428935, 0.001207808273399926, 0.0014833326832475987, 0.0012706276584966954, 0.0012314003860493276, 0.0011007928882132876, 0.001198562293376943, 0.0021789795004720377, 0.001783247251296416, 0.0014340108398094096, 0.0020347360907842153, 0.0012557769315952266, 0.0012422242487611418, 0.001311956931286576, 0.0012323418182363225, 0.0011378713190259243, 0.004195408136266368, 0.0012436455687169325, 0.001251803430601616, 0.0016376635467167944, 0.0012526489339176226, 0.0013334014087873088, 0.0013020377505612983, 0.0016277235461695288, 0.0020265680690168997, 0.001597404249795628, 0.001333422863601961, 0.0015954089335123585, 0.00144030940877697, 0.0013559602507898076, 0.0016106670902279968, 0.0014305303180166943, 0.0013995947718450968, 0.001584009613989937, 0.002375816092932258, 0.0020349743188655175, 0.0028981595903381026, 0.0021166412043385208, 0.0013042090682905507, 0.0012310213404072617, 0.0012097155003258113, 0.004957517341774126, 0.0032025189324155112, 0.001950861409899186, 0.002625338862840594, 0.002326321568530561, 0.006015729820567437, 0.0037107109308762605, 0.0018373555585005602, 0.0016598464422967545, 0.0022863980956635502, 0.00192086320901073, 0.0013169523241908052, 0.0025727940441737343, 0.002128475884430457, 0.0011731484190149362, 0.0011727322558980696, 0.002469247954269481, 0.0012521741419034295, 0.0015204125814962872, 0.0012693473024207147, 0.0012110448848499462, 0.0020393234201122163, 0.0012268324891581786, 0.0015987906507541273, 0.0015813774635018998, 0.0019463666980046518, 0.0022680757670165146, 0.001286731534745804, 0.0019248367915319841, 0.0020042052546646013, 0.0018137826064471589, 0.007502970301957671, 0.00121570588360259, 0.0012058849056619545, 0.0017153774171547835, 0.0015754245118736181, 0.002198501254518538, 0.002915922465712525, 0.0014324683484859592, 0.0013481840913638818, 0.0013481521177603755, 0.00157288616241584, 0.0013854537662768432, 0.0016719080009605996, 0.0014133773267615674, 0.0015673129548600248, 0.0013034680240982493, 0.0012630727223315557, 0.0014785160012759787, 0.00141668479164066, 0.00130078895288238, 0.0013986517225803678, 0.001270341464903119, 0.0012613222095032418, 0.0013878810473955995, 0.00202141851078459, 0.0014837381868558221, 0.0014392563472177054, 0.0014996860219642174, 0.0015827700208742605, 0.0012992538381801095, 0.001236769303051365, 0.0013311458589119274, 0.0015595997440043924, 0.001488960559241647, 0.0016336857427864573, 0.0014717146526848854, 0.001473099372328021, 0.0012090987672076323, 0.0018463767442289134, 0.001328706302745051, 0.0015574461877952482, 0.0012963342772753432, 0.0013110242340020662, 0.0017629705104153864, 0.0016287662099700334, 0.0013683817436008952, 0.0014682056052003835, 0.0016961889070740274, 0.0016319547687808787, 0.001403922372129421, 0.0013730000233450948, 0.0013141560227458561, 0.0015525738356721608, 0.001476518417232085, 0.001621881510715845, 0.001381636951876761, 0.0015827061142772436, 0.0015287077687897307, 0.0014981854880271957, 0.0013968740227134075, 0.0014304582078313065, 0.0021890519771638303, 0.0015502763955399048, 0.0015741334659545574, 0.0013395434429565834, 0.0014342327661768987, 0.0017387951875841895, 0.0016018957424865558, 0.0035433705116427222, 0.0014020063248354681, 0.001490730348281389, 0.0016030640704124126, 0.0014122752797638262, 0.0017048012557287896, 0.0014871537203489, 0.0016395063247791556, 0.0016887243268578205, 0.0018761010466923202, 0.0014885288127156537, 0.0017483427682025141, 0.0016524920234629928, 0.0025511893232591275, 0.002509369861421197, 0.0018870056049118554, 0.001480673139286769, 0.0016985803512249921, 0.0016229268851105211, 0.002232670908533903, 0.0021337206518754017, 0.0014440421865126768, 0.0013199710465837703, 0.0013933211164332408, 0.0013185656726966764, 0.00130481090732352, 0.0013160802813809971, 0.0012847372076235884, 0.0013087885346003744, 0.001297573813475495, 0.001294061953071938, 0.0013061610001767444, 0.0014968462542868978, 0.001282143579839274, 0.001586437769515743, 0.0012842626067224978, 0.0013122871399012415, 0.0013663184661201614, 0.001332124210569228, 0.00159733209401629]
[458.02019542705744, 496.9331656958686, 884.0529052292865, 874.2777295048214, 794.1368445615941, 749.8786180753209, 822.6510683383858, 777.5291245627682, 816.9304369552661, 438.21070686346104, 669.6574173007233, 433.1561801848584, 251.56474703600628, 745.6126538808892, 542.7913711007686, 743.815793108199, 661.3188516676556, 718.017258716134, 492.93389871457885, 402.01941652202424, 787.5246503603857, 838.0712132599397, 683.230397512658, 706.110285925355, 813.2193981990345, 533.6627619206168, 729.2657927167566, 634.7879431918107, 651.3627213831512, 827.9459762144573, 674.1575988271267, 787.0126179868614, 812.0835524571144, 908.4361015659484, 834.332938326055, 458.9304304071551, 560.7747323165662, 697.3447984067591, 491.46422699692044, 796.3197721188344, 805.0076312689035, 762.2201431714269, 811.4631713392306, 878.8339975525975, 238.3558327390605, 804.0876155991122, 798.8474672252662, 610.6260361017444, 798.3082673231752, 749.9617095121206, 768.0268867541727, 614.3549390517013, 493.44505881073417, 626.0156125964609, 749.9496426052802, 626.7985461247599, 694.2952631609509, 737.4847451594019, 620.8607638828988, 699.0414585455363, 714.4925232049075, 631.3092996204207, 420.90799998150914, 491.40669281639504, 345.04656104301654, 472.44662815326495, 766.74823409311, 812.333602331514, 826.6406437965549, 201.71386826498406, 312.2542039886542, 512.5940750715228, 380.90320992620684, 429.8631855232541, 166.23086970778792, 269.49013777364127, 544.2604700943599, 602.4653693966322, 437.36915364679027, 520.5992781313216, 759.3289306159546, 388.6824918086885, 469.81974628647606, 852.4070644357815, 852.7095549479941, 404.98160513647025, 798.6109651489052, 657.7162095145699, 787.8064561944122, 825.7332263319905, 490.3587092355237, 815.1072039885207, 625.4727593811697, 632.3600930707197, 513.7777999516563, 440.9023783695841, 777.1628914010813, 519.5245666538286, 498.9508922165497, 551.3339892253141, 133.2805488699696, 822.5673770999832, 829.2665372165541, 582.9620875262852, 634.7495500185672, 454.85532380057504, 342.9446467657168, 698.0957038645533, 741.738466138075, 741.7560576630292, 635.7739192416011, 721.7851828338663, 598.1190349142698, 707.5251463749405, 638.034667485607, 767.1841437704686, 791.7200508883293, 676.3538569328887, 705.8733219278099, 768.7642163504921, 714.9742740495135, 787.1899230466069, 792.8188312753482, 720.5228444300252, 494.702108774032, 673.9733524814726, 694.8032585947232, 666.806241676006, 631.8037281547947, 769.6725386632066, 808.5582311372006, 751.2324763699415, 641.1901539765729, 671.6094619116823, 612.112827950848, 679.4795432495526, 678.8408296038064, 827.0622939344005, 541.6012756473607, 752.6117682546114, 642.0767586298567, 771.4059695326551, 762.76240672331, 567.2244624014628, 613.9616563008133, 730.7902233250358, 681.1035160593315, 589.5569743614393, 612.7620808676157, 712.2900951305695, 728.3321070626496, 760.9446539768966, 644.0917507585503, 677.2688971090673, 616.5678524559004, 723.7791365102386, 631.8292391614716, 654.1472611156378, 667.4740931557119, 715.8841697531997, 699.0766976101197, 456.8187555307003, 645.0462658639245, 635.2701480707026, 746.5230077143616, 697.2368945841387, 575.1108624756196, 624.260351955079, 282.2171705482743, 713.2635440267037, 670.8121298750408, 623.8053852349987, 708.0772525928722, 586.5786387941788, 672.4254435280508, 609.9397025105724, 592.1629623590976, 533.0203305216745, 671.8042616693541, 571.9702212788104, 605.1466426472556, 391.974045549276, 398.5064200275538, 529.9401323435451, 675.3685019785621, 588.726932628659, 616.1707031749, 447.89404303953427, 468.6649112764902, 692.5005441946078, 757.5923749146692, 717.70964223947, 758.3998436383082, 766.3945744071376, 759.8320665899455, 778.369299235698, 764.065373101194, 770.6690668498799, 772.76052945234, 765.6024026629826, 668.071284633306, 779.9438500681471, 630.3430359611572, 778.656946613162, 762.0283469936745, 731.8937896225833, 750.6807488865409, 626.0438914024611]
Elapsed: 0.07318868389064356~0.03218783646407786
Time per graph: 0.0016872708188281653~0.0007382708931487963
Speed: 648.499488945571~148.76415656375534
Total Time: 0.0698
best val loss: 0.44708335399627686 test_score: 0.9070

Testing...
Test loss: 0.2862 score: 0.9070 time: 0.08s
test Score 0.9070
Epoch Time List: [0.3288679119432345, 0.4660455980338156, 0.38897013396490365, 0.28381930908653885, 0.2940787529805675, 0.2817123760469258, 0.3252614460652694, 0.29265848791692406, 0.2645999869564548, 0.3049295919481665, 0.3312269210582599, 0.4712867710040882, 0.4626472679665312, 0.3166475840844214, 0.2955576250096783, 0.2605921638896689, 0.3448535749921575, 0.28413123288191855, 0.32972387899644673, 0.3689081050688401, 0.29426623007748276, 0.2525339029962197, 0.2592078309971839, 0.2842382341623306, 0.26103993807919323, 0.3277807709528133, 0.3677016559522599, 0.2839946490712464, 0.2998153731459752, 0.2429948019562289, 0.2763243360677734, 0.2717857201350853, 0.286811585072428, 0.291601498844102, 0.3317037670640275, 0.3029760620556772, 0.30135926604270935, 0.293776721926406, 0.32928699685726315, 0.3374282290460542, 0.28827611613087356, 0.28390934399794787, 0.28974784596357495, 0.3376118381274864, 0.4308742618886754, 0.4108435860835016, 0.3303012599935755, 0.3040991530288011, 0.25519975810311735, 0.26168006903026253, 0.2613855559611693, 0.2829981150571257, 0.29468201904091984, 0.3099397229962051, 0.2946053908672184, 0.32457137014716864, 0.281981089967303, 0.2683602829929441, 0.2903908089501783, 0.2925841851392761, 0.2928704209625721, 0.2979354120325297, 0.4723472020123154, 0.29933381790760905, 0.566951964981854, 0.4801859809085727, 0.2682512679602951, 0.44570807192940265, 0.3938619199907407, 0.5903075560927391, 0.4989373900461942, 0.3579790609655902, 0.3373327270383015, 0.35147908807266504, 0.52275140886195, 0.5434526210883632, 0.32357407093513757, 0.3119497940642759, 0.343512047897093, 0.37684068409726024, 0.29147976415697485, 0.3273377420846373, 0.32244240888394415, 0.2833342249505222, 0.2714722231030464, 0.3696827990934253, 0.3111739590531215, 0.3966994290240109, 0.27875840791966766, 0.29539934906642884, 0.31489696307107806, 0.3198281889781356, 0.2963068970711902, 0.39586834295187145, 0.36149473499972373, 0.38008829311002046, 0.3460551301250234, 0.36803865199908614, 0.3589352349517867, 0.35028093005530536, 0.6191439511021599, 0.4721750250319019, 0.324119680095464, 0.3149337860522792, 0.41858164104633033, 0.40508626005612314, 0.39312728797085583, 0.29266368213575333, 0.27450285491067916, 0.28496602503582835, 0.3109124780166894, 0.2937601748853922, 0.42538453615270555, 0.3059932349715382, 0.357514234026894, 0.2637308550765738, 0.27158298494759947, 0.3156238959636539, 0.28711505292449147, 0.2965253960574046, 0.30267358396667987, 0.3184525810647756, 0.27407831395976245, 0.2671594660496339, 0.3427204169565812, 0.3057041859719902, 0.30424070404842496, 0.2884243020089343, 0.308404030976817, 0.2888137889094651, 0.2928337298799306, 0.3283633909886703, 0.3048158041201532, 0.2977478419197723, 0.30082057893741876, 0.31586113199591637, 0.30419717507902533, 0.2907313600881025, 0.29450040694791824, 0.30893636809196323, 0.29663104109931737, 0.2880605610553175, 0.29153854807373136, 0.3058122581569478, 0.30686920799780637, 0.30746007012203336, 0.31139945704489946, 0.3154189460910857, 0.3157516779610887, 0.2848281830083579, 0.2914524241350591, 0.28319944080431014, 0.32444968493655324, 0.30855557695031166, 0.3176498948596418, 0.313905430957675, 0.29866791889071465, 0.2901284128893167, 0.28545012092217803, 0.34434518206398934, 0.2961806289386004, 0.3233450660482049, 0.311237134039402, 0.34866226906888187, 0.3474224549718201, 0.3335734698921442, 0.33416980097536, 0.3254289770266041, 0.43838146480266005, 0.34464982396457344, 0.31428695004433393, 0.30514162208419293, 0.3057150288950652, 0.332826248020865, 0.3257346039172262, 0.33360025414731354, 0.3266530839027837, 0.31339460297022015, 0.327792540891096, 0.3179824079852551, 0.3269706849241629, 0.3608603199245408, 0.49971234798431396, 0.32624403294175863, 0.2883927058428526, 0.30259689991362393, 0.33517516998108476, 0.36857942782808095, 0.3900329259922728, 0.3264212211361155, 0.322822995018214, 0.3146838911343366, 0.2969167820410803, 0.3037776770070195, 0.29484961507841945, 0.28979435411747545, 0.2942948630079627, 0.3693252089433372, 0.293836479075253, 0.2752302639419213, 0.28430615400429815, 0.273387108114548, 0.4429568479536101, 0.2611939850030467, 0.27389088401105255, 0.2716133809881285, 0.321937715052627, 0.34865564585197717]
Total Epoch List: [75, 82, 51]
Total Time List: [0.26516593410633504, 0.06886991788633168, 0.06981001305393875]
T-times Epoch Time: 0.3201503122444725 ~ 0.009340572850907747
T-times Total Epoch: 65.77777777777777 ~ 8.283555030940356
T-times Total Time: 0.08793863668365198 ~ 0.03300640519141083
T-times Inference Elapsed: 0.07042989648710017 ~ 0.00195135666164957
T-times Time Per Graph: 0.0016223916139231616 ~ 4.5884422320608046e-05
T-times Speed: 664.7447324461551 ~ 13.364107241429839
T-times cross validation test micro f1 score:0.8420541565221261 ~ 0.007433523559403239
T-times cross validation test precision:0.8361054298697318 ~ 0.022930265759436
T-times cross validation test recall:0.8528138528138528 ~ 0.018897259529161384
T-times cross validation test f1_score:0.8420541565221261 ~ 0.0039554020530186175
