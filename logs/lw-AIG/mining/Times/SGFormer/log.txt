Namespace(seed=60, model='SGFormer', dataset='mining/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=32, abs_pe='lap', abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Times/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 68], edge_attr=[68, 2], x=[24, 14887], y=[1, 1], num_nodes=25)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f8513a90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.45s
Val loss: 0.6897 score: 0.5116 time: 0.06s
Test loss: 0.6865 score: 0.5227 time: 0.06s
Epoch 2/1000, LR 0.000015
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.37s
Val loss: 0.6889 score: 0.5116 time: 0.06s
Test loss: 0.6856 score: 0.5227 time: 0.06s
Epoch 3/1000, LR 0.000045
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.17s
Val loss: 0.6872 score: 0.5349 time: 0.07s
Test loss: 0.6835 score: 0.5455 time: 0.07s
Epoch 4/1000, LR 0.000075
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.17s
Val loss: 0.6849 score: 0.6512 time: 0.06s
Test loss: 0.6804 score: 0.5909 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.17s
Val loss: 0.6825 score: 0.6512 time: 0.06s
Test loss: 0.6769 score: 0.7727 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.17s
Val loss: 0.6809 score: 0.6047 time: 0.06s
Test loss: 0.6738 score: 0.6364 time: 0.06s
Epoch 7/1000, LR 0.000165
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.16s
Val loss: 0.6807 score: 0.5116 time: 0.06s
Test loss: 0.6722 score: 0.5455 time: 0.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6244;  Loss pred: 0.6244; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.4884 time: 0.06s
Test loss: 0.6711 score: 0.5227 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.4884 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6711 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6165;  Loss pred: 0.6165; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6731 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6313;  Loss pred: 0.6313; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6751 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6062;  Loss pred: 0.6062; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6774 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5927;  Loss pred: 0.5927; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.6116;  Loss pred: 0.6116; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6790 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4884 time: 0.06s
Test loss: 0.6743 score: 0.5227 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5784;  Loss pred: 0.5784; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.4884 time: 0.06s
Test loss: 0.6711 score: 0.5227 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5476;  Loss pred: 0.5476; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.4884 time: 0.06s
Test loss: 0.6668 score: 0.5227 time: 0.07s
Epoch 20/1000, LR 0.000285
Train loss: 0.5632;  Loss pred: 0.5632; Loss self: 0.0000; time: 0.17s
Val loss: 0.6757 score: 0.5116 time: 0.06s
Test loss: 0.6628 score: 0.5227 time: 0.06s
Epoch 21/1000, LR 0.000285
Train loss: 0.5531;  Loss pred: 0.5531; Loss self: 0.0000; time: 0.17s
Val loss: 0.6714 score: 0.5116 time: 0.19s
Test loss: 0.6588 score: 0.5455 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.5307;  Loss pred: 0.5307; Loss self: 0.0000; time: 0.17s
Val loss: 0.6664 score: 0.5349 time: 0.06s
Test loss: 0.6546 score: 0.5455 time: 0.06s
Epoch 23/1000, LR 0.000285
Train loss: 0.5316;  Loss pred: 0.5316; Loss self: 0.0000; time: 0.38s
Val loss: 0.6569 score: 0.5581 time: 0.07s
Test loss: 0.6460 score: 0.5682 time: 0.07s
Epoch 24/1000, LR 0.000285
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.18s
Val loss: 0.6505 score: 0.5581 time: 0.07s
Test loss: 0.6402 score: 0.5682 time: 0.06s
Epoch 25/1000, LR 0.000285
Train loss: 0.4851;  Loss pred: 0.4851; Loss self: 0.0000; time: 0.17s
Val loss: 0.6460 score: 0.5581 time: 0.07s
Test loss: 0.6359 score: 0.5682 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4863;  Loss pred: 0.4863; Loss self: 0.0000; time: 0.18s
Val loss: 0.6411 score: 0.5581 time: 0.07s
Test loss: 0.6313 score: 0.5682 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.5095;  Loss pred: 0.5095; Loss self: 0.0000; time: 0.18s
Val loss: 0.6351 score: 0.5581 time: 0.07s
Test loss: 0.6255 score: 0.5682 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.4966;  Loss pred: 0.4966; Loss self: 0.0000; time: 0.18s
Val loss: 0.6318 score: 0.5581 time: 0.07s
Test loss: 0.6219 score: 0.5682 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.4672;  Loss pred: 0.4672; Loss self: 0.0000; time: 0.17s
Val loss: 0.6259 score: 0.5581 time: 0.07s
Test loss: 0.6161 score: 0.5682 time: 0.19s
Epoch 30/1000, LR 0.000285
Train loss: 0.4719;  Loss pred: 0.4719; Loss self: 0.0000; time: 0.17s
Val loss: 0.6196 score: 0.5581 time: 0.06s
Test loss: 0.6103 score: 0.5909 time: 0.06s
Epoch 31/1000, LR 0.000285
Train loss: 0.4532;  Loss pred: 0.4532; Loss self: 0.0000; time: 0.17s
Val loss: 0.6136 score: 0.5814 time: 0.06s
Test loss: 0.6049 score: 0.6136 time: 0.25s
Epoch 32/1000, LR 0.000285
Train loss: 0.4256;  Loss pred: 0.4256; Loss self: 0.0000; time: 0.17s
Val loss: 0.6077 score: 0.5814 time: 0.06s
Test loss: 0.5993 score: 0.6136 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.4466;  Loss pred: 0.4466; Loss self: 0.0000; time: 0.18s
Val loss: 0.6040 score: 0.5814 time: 0.07s
Test loss: 0.5957 score: 0.6136 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.4453;  Loss pred: 0.4453; Loss self: 0.0000; time: 0.17s
Val loss: 0.5989 score: 0.6047 time: 0.07s
Test loss: 0.5908 score: 0.6364 time: 0.06s
Epoch 35/1000, LR 0.000285
Train loss: 0.4399;  Loss pred: 0.4399; Loss self: 0.0000; time: 0.18s
Val loss: 0.5909 score: 0.6047 time: 0.06s
Test loss: 0.5830 score: 0.6364 time: 0.20s
Epoch 36/1000, LR 0.000285
Train loss: 0.4043;  Loss pred: 0.4043; Loss self: 0.0000; time: 0.17s
Val loss: 0.5868 score: 0.6512 time: 0.06s
Test loss: 0.5785 score: 0.6364 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.4473;  Loss pred: 0.4473; Loss self: 0.0000; time: 0.18s
Val loss: 0.5799 score: 0.6512 time: 0.08s
Test loss: 0.5709 score: 0.6364 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.4314;  Loss pred: 0.4314; Loss self: 0.0000; time: 0.17s
Val loss: 0.5732 score: 0.6512 time: 0.06s
Test loss: 0.5636 score: 0.6591 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.4176;  Loss pred: 0.4176; Loss self: 0.0000; time: 0.17s
Val loss: 0.5664 score: 0.6744 time: 0.06s
Test loss: 0.5562 score: 0.6818 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.4020;  Loss pred: 0.4020; Loss self: 0.0000; time: 0.17s
Val loss: 0.5592 score: 0.6744 time: 0.06s
Test loss: 0.5484 score: 0.7045 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.4192;  Loss pred: 0.4192; Loss self: 0.0000; time: 0.17s
Val loss: 0.5513 score: 0.6744 time: 0.06s
Test loss: 0.5396 score: 0.7045 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3896;  Loss pred: 0.3896; Loss self: 0.0000; time: 0.36s
Val loss: 0.5431 score: 0.6977 time: 0.06s
Test loss: 0.5308 score: 0.7045 time: 0.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.4203;  Loss pred: 0.4203; Loss self: 0.0000; time: 0.17s
Val loss: 0.5362 score: 0.7209 time: 0.06s
Test loss: 0.5224 score: 0.7045 time: 0.20s
Epoch 44/1000, LR 0.000284
Train loss: 0.3895;  Loss pred: 0.3895; Loss self: 0.0000; time: 0.19s
Val loss: 0.5262 score: 0.7442 time: 0.06s
Test loss: 0.5117 score: 0.7045 time: 0.06s
Epoch 45/1000, LR 0.000284
Train loss: 0.3809;  Loss pred: 0.3809; Loss self: 0.0000; time: 0.18s
Val loss: 0.5166 score: 0.7442 time: 0.06s
Test loss: 0.5008 score: 0.7500 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.3853;  Loss pred: 0.3853; Loss self: 0.0000; time: 0.18s
Val loss: 0.5089 score: 0.7442 time: 0.06s
Test loss: 0.4920 score: 0.7727 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3580;  Loss pred: 0.3580; Loss self: 0.0000; time: 0.17s
Val loss: 0.5031 score: 0.7674 time: 0.06s
Test loss: 0.4849 score: 0.7955 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3455;  Loss pred: 0.3455; Loss self: 0.0000; time: 0.17s
Val loss: 0.4990 score: 0.7674 time: 0.06s
Test loss: 0.4791 score: 0.7955 time: 0.06s
Epoch 49/1000, LR 0.000284
Train loss: 0.3614;  Loss pred: 0.3614; Loss self: 0.0000; time: 0.17s
Val loss: 0.4943 score: 0.7907 time: 0.30s
Test loss: 0.4731 score: 0.7955 time: 0.17s
Epoch 50/1000, LR 0.000284
Train loss: 0.3539;  Loss pred: 0.3539; Loss self: 0.0000; time: 0.22s
Val loss: 0.4897 score: 0.7907 time: 0.07s
Test loss: 0.4667 score: 0.7955 time: 0.07s
Epoch 51/1000, LR 0.000284
Train loss: 0.3575;  Loss pred: 0.3575; Loss self: 0.0000; time: 0.17s
Val loss: 0.4828 score: 0.7907 time: 0.06s
Test loss: 0.4596 score: 0.7955 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.3502;  Loss pred: 0.3502; Loss self: 0.0000; time: 0.17s
Val loss: 0.4769 score: 0.7907 time: 0.06s
Test loss: 0.4534 score: 0.7955 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.3451;  Loss pred: 0.3451; Loss self: 0.0000; time: 0.17s
Val loss: 0.4709 score: 0.7907 time: 0.06s
Test loss: 0.4472 score: 0.7955 time: 0.06s
Epoch 54/1000, LR 0.000284
Train loss: 0.3412;  Loss pred: 0.3412; Loss self: 0.0000; time: 0.17s
Val loss: 0.4681 score: 0.7907 time: 0.06s
Test loss: 0.4425 score: 0.8182 time: 0.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.3341;  Loss pred: 0.3341; Loss self: 0.0000; time: 0.17s
Val loss: 0.4653 score: 0.7907 time: 0.06s
Test loss: 0.4372 score: 0.8182 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.3493;  Loss pred: 0.3493; Loss self: 0.0000; time: 0.17s
Val loss: 0.4635 score: 0.7907 time: 0.19s
Test loss: 0.4330 score: 0.8182 time: 0.06s
Epoch 57/1000, LR 0.000283
Train loss: 0.3593;  Loss pred: 0.3593; Loss self: 0.0000; time: 0.36s
Val loss: 0.4636 score: 0.7907 time: 0.06s
Test loss: 0.4298 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.3311;  Loss pred: 0.3311; Loss self: 0.0000; time: 0.16s
Val loss: 0.4640 score: 0.7907 time: 0.06s
Test loss: 0.4266 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.3301;  Loss pred: 0.3301; Loss self: 0.0000; time: 0.16s
Val loss: 0.4629 score: 0.7907 time: 0.06s
Test loss: 0.4241 score: 0.8182 time: 0.06s
Epoch 60/1000, LR 0.000283
Train loss: 0.3114;  Loss pred: 0.3114; Loss self: 0.0000; time: 0.19s
Val loss: 0.4593 score: 0.7907 time: 0.07s
Test loss: 0.4205 score: 0.8182 time: 0.06s
Epoch 61/1000, LR 0.000283
Train loss: 0.3159;  Loss pred: 0.3159; Loss self: 0.0000; time: 0.16s
Val loss: 0.4554 score: 0.7907 time: 0.06s
Test loss: 0.4175 score: 0.8182 time: 0.06s
Epoch 62/1000, LR 0.000283
Train loss: 0.3092;  Loss pred: 0.3092; Loss self: 0.0000; time: 0.16s
Val loss: 0.4516 score: 0.7907 time: 0.06s
Test loss: 0.4146 score: 0.8409 time: 0.06s
Epoch 63/1000, LR 0.000283
Train loss: 0.3231;  Loss pred: 0.3231; Loss self: 0.0000; time: 0.16s
Val loss: 0.4446 score: 0.7907 time: 0.06s
Test loss: 0.4097 score: 0.8409 time: 0.06s
Epoch 64/1000, LR 0.000283
Train loss: 0.2969;  Loss pred: 0.2969; Loss self: 0.0000; time: 0.29s
Val loss: 0.4391 score: 0.7907 time: 0.06s
Test loss: 0.4054 score: 0.8409 time: 0.06s
Epoch 65/1000, LR 0.000283
Train loss: 0.2965;  Loss pred: 0.2965; Loss self: 0.0000; time: 0.38s
Val loss: 0.4339 score: 0.8140 time: 0.06s
Test loss: 0.4010 score: 0.8409 time: 0.06s
Epoch 66/1000, LR 0.000283
Train loss: 0.3006;  Loss pred: 0.3006; Loss self: 0.0000; time: 0.17s
Val loss: 0.4289 score: 0.8140 time: 0.06s
Test loss: 0.3976 score: 0.8409 time: 0.06s
Epoch 67/1000, LR 0.000283
Train loss: 0.2862;  Loss pred: 0.2862; Loss self: 0.0000; time: 0.16s
Val loss: 0.4241 score: 0.8140 time: 0.06s
Test loss: 0.3947 score: 0.8636 time: 0.06s
Epoch 68/1000, LR 0.000283
Train loss: 0.2878;  Loss pred: 0.2878; Loss self: 0.0000; time: 0.16s
Val loss: 0.4202 score: 0.8140 time: 0.06s
Test loss: 0.3922 score: 0.8636 time: 0.06s
Epoch 69/1000, LR 0.000283
Train loss: 0.2931;  Loss pred: 0.2931; Loss self: 0.0000; time: 0.15s
Val loss: 0.4176 score: 0.8140 time: 0.06s
Test loss: 0.3900 score: 0.8636 time: 0.06s
Epoch 70/1000, LR 0.000283
Train loss: 0.2892;  Loss pred: 0.2892; Loss self: 0.0000; time: 0.16s
Val loss: 0.4169 score: 0.8140 time: 0.07s
Test loss: 0.3882 score: 0.8636 time: 0.32s
Epoch 71/1000, LR 0.000282
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 0.17s
Val loss: 0.4166 score: 0.8140 time: 0.07s
Test loss: 0.3872 score: 0.8636 time: 0.06s
Epoch 72/1000, LR 0.000282
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.17s
Val loss: 0.4156 score: 0.8140 time: 0.07s
Test loss: 0.3851 score: 0.8636 time: 0.06s
Epoch 73/1000, LR 0.000282
Train loss: 0.2678;  Loss pred: 0.2678; Loss self: 0.0000; time: 0.17s
Val loss: 0.4149 score: 0.8140 time: 0.06s
Test loss: 0.3842 score: 0.8636 time: 0.06s
Epoch 74/1000, LR 0.000282
Train loss: 0.2707;  Loss pred: 0.2707; Loss self: 0.0000; time: 0.17s
Val loss: 0.4157 score: 0.8140 time: 0.06s
Test loss: 0.3841 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.2657;  Loss pred: 0.2657; Loss self: 0.0000; time: 0.17s
Val loss: 0.4159 score: 0.8140 time: 0.07s
Test loss: 0.3828 score: 0.8636 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.2659;  Loss pred: 0.2659; Loss self: 0.0000; time: 0.16s
Val loss: 0.4150 score: 0.7907 time: 0.07s
Test loss: 0.3818 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.2754;  Loss pred: 0.2754; Loss self: 0.0000; time: 0.16s
Val loss: 0.4153 score: 0.7907 time: 0.06s
Test loss: 0.3803 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.2618;  Loss pred: 0.2618; Loss self: 0.0000; time: 0.16s
Val loss: 0.4154 score: 0.7907 time: 0.20s
Test loss: 0.3790 score: 0.8636 time: 0.34s
     INFO: Early stopping counter 5 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.2460;  Loss pred: 0.2460; Loss self: 0.0000; time: 0.18s
Val loss: 0.4138 score: 0.7907 time: 0.06s
Test loss: 0.3777 score: 0.8636 time: 0.06s
Epoch 80/1000, LR 0.000282
Train loss: 0.2588;  Loss pred: 0.2588; Loss self: 0.0000; time: 0.16s
Val loss: 0.4126 score: 0.7907 time: 0.06s
Test loss: 0.3768 score: 0.8636 time: 0.07s
Epoch 81/1000, LR 0.000281
Train loss: 0.2414;  Loss pred: 0.2414; Loss self: 0.0000; time: 0.18s
Val loss: 0.4110 score: 0.8140 time: 0.06s
Test loss: 0.3762 score: 0.8636 time: 0.06s
Epoch 82/1000, LR 0.000281
Train loss: 0.2375;  Loss pred: 0.2375; Loss self: 0.0000; time: 0.17s
Val loss: 0.4095 score: 0.8140 time: 0.06s
Test loss: 0.3736 score: 0.8636 time: 0.06s
Epoch 83/1000, LR 0.000281
Train loss: 0.2577;  Loss pred: 0.2577; Loss self: 0.0000; time: 0.15s
Val loss: 0.4112 score: 0.8140 time: 0.06s
Test loss: 0.3731 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2386;  Loss pred: 0.2386; Loss self: 0.0000; time: 0.16s
Val loss: 0.4120 score: 0.8140 time: 0.06s
Test loss: 0.3733 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.2421;  Loss pred: 0.2421; Loss self: 0.0000; time: 0.17s
Val loss: 0.4114 score: 0.8140 time: 0.06s
Test loss: 0.3716 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.2689;  Loss pred: 0.2689; Loss self: 0.0000; time: 0.18s
Val loss: 0.4094 score: 0.8372 time: 0.07s
Test loss: 0.3690 score: 0.8636 time: 0.29s
Epoch 87/1000, LR 0.000281
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.19s
Val loss: 0.4067 score: 0.8372 time: 0.07s
Test loss: 0.3672 score: 0.8636 time: 0.06s
Epoch 88/1000, LR 0.000281
Train loss: 0.2319;  Loss pred: 0.2319; Loss self: 0.0000; time: 0.17s
Val loss: 0.4045 score: 0.8372 time: 0.06s
Test loss: 0.3654 score: 0.8636 time: 0.06s
Epoch 89/1000, LR 0.000281
Train loss: 0.2263;  Loss pred: 0.2263; Loss self: 0.0000; time: 0.16s
Val loss: 0.4030 score: 0.8605 time: 0.18s
Test loss: 0.3655 score: 0.8636 time: 0.06s
Epoch 90/1000, LR 0.000281
Train loss: 0.2157;  Loss pred: 0.2157; Loss self: 0.0000; time: 0.17s
Val loss: 0.4020 score: 0.8605 time: 0.06s
Test loss: 0.3647 score: 0.8636 time: 0.06s
Epoch 91/1000, LR 0.000280
Train loss: 0.2390;  Loss pred: 0.2390; Loss self: 0.0000; time: 0.16s
Val loss: 0.4027 score: 0.8605 time: 0.06s
Test loss: 0.3644 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.2298;  Loss pred: 0.2298; Loss self: 0.0000; time: 0.16s
Val loss: 0.4004 score: 0.8605 time: 0.06s
Test loss: 0.3628 score: 0.8636 time: 0.06s
Epoch 93/1000, LR 0.000280
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.15s
Val loss: 0.3998 score: 0.8605 time: 0.06s
Test loss: 0.3625 score: 0.8636 time: 0.06s
Epoch 94/1000, LR 0.000280
Train loss: 0.1989;  Loss pred: 0.1989; Loss self: 0.0000; time: 0.52s
Val loss: 0.4008 score: 0.8605 time: 0.06s
Test loss: 0.3619 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.2151;  Loss pred: 0.2151; Loss self: 0.0000; time: 0.16s
Val loss: 0.4039 score: 0.8605 time: 0.06s
Test loss: 0.3624 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.2144;  Loss pred: 0.2144; Loss self: 0.0000; time: 0.16s
Val loss: 0.4068 score: 0.8605 time: 0.06s
Test loss: 0.3622 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.2360;  Loss pred: 0.2360; Loss self: 0.0000; time: 0.16s
Val loss: 0.4040 score: 0.8605 time: 0.06s
Test loss: 0.3589 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.2007;  Loss pred: 0.2007; Loss self: 0.0000; time: 0.17s
Val loss: 0.4029 score: 0.8605 time: 0.06s
Test loss: 0.3572 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 99/1000, LR 0.000279
Train loss: 0.2147;  Loss pred: 0.2147; Loss self: 0.0000; time: 0.16s
Val loss: 0.4032 score: 0.8605 time: 0.06s
Test loss: 0.3563 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.2019;  Loss pred: 0.2019; Loss self: 0.0000; time: 0.16s
Val loss: 0.4015 score: 0.8605 time: 0.06s
Test loss: 0.3551 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1880;  Loss pred: 0.1880; Loss self: 0.0000; time: 0.16s
Val loss: 0.4010 score: 0.8372 time: 0.06s
Test loss: 0.3535 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1820;  Loss pred: 0.1820; Loss self: 0.0000; time: 0.38s
Val loss: 0.4022 score: 0.8372 time: 0.06s
Test loss: 0.3532 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1855;  Loss pred: 0.1855; Loss self: 0.0000; time: 0.17s
Val loss: 0.4043 score: 0.8372 time: 0.06s
Test loss: 0.3531 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1759;  Loss pred: 0.1759; Loss self: 0.0000; time: 0.17s
Val loss: 0.4042 score: 0.8140 time: 0.06s
Test loss: 0.3532 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 0.16s
Val loss: 0.4020 score: 0.8372 time: 0.06s
Test loss: 0.3524 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1835;  Loss pred: 0.1835; Loss self: 0.0000; time: 0.16s
Val loss: 0.3998 score: 0.8605 time: 0.06s
Test loss: 0.3513 score: 0.8864 time: 0.06s
Epoch 107/1000, LR 0.000278
Train loss: 0.1804;  Loss pred: 0.1804; Loss self: 0.0000; time: 0.16s
Val loss: 0.4004 score: 0.8140 time: 0.06s
Test loss: 0.3506 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1682;  Loss pred: 0.1682; Loss self: 0.0000; time: 0.17s
Val loss: 0.4043 score: 0.7907 time: 0.06s
Test loss: 0.3472 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1758;  Loss pred: 0.1758; Loss self: 0.0000; time: 0.29s
Val loss: 0.4079 score: 0.7674 time: 0.06s
Test loss: 0.3451 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1690;  Loss pred: 0.1690; Loss self: 0.0000; time: 0.20s
Val loss: 0.4049 score: 0.7674 time: 0.12s
Test loss: 0.3443 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1786;  Loss pred: 0.1786; Loss self: 0.0000; time: 0.18s
Val loss: 0.4006 score: 0.7907 time: 0.06s
Test loss: 0.3454 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.18s
Val loss: 0.4006 score: 0.7907 time: 0.06s
Test loss: 0.3423 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1549;  Loss pred: 0.1549; Loss self: 0.0000; time: 0.18s
Val loss: 0.4025 score: 0.7674 time: 0.06s
Test loss: 0.3393 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1489;  Loss pred: 0.1489; Loss self: 0.0000; time: 0.19s
Val loss: 0.4064 score: 0.7674 time: 0.07s
Test loss: 0.3389 score: 0.8864 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1636;  Loss pred: 0.1636; Loss self: 0.0000; time: 0.19s
Val loss: 0.4058 score: 0.7674 time: 0.07s
Test loss: 0.3370 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1577;  Loss pred: 0.1577; Loss self: 0.0000; time: 0.25s
Val loss: 0.3985 score: 0.7907 time: 0.07s
Test loss: 0.3336 score: 0.8636 time: 0.06s
Epoch 117/1000, LR 0.000277
Train loss: 0.1614;  Loss pred: 0.1614; Loss self: 0.0000; time: 0.18s
Val loss: 0.3978 score: 0.7674 time: 0.06s
Test loss: 0.3305 score: 0.8636 time: 0.06s
Epoch 118/1000, LR 0.000277
Train loss: 0.1457;  Loss pred: 0.1457; Loss self: 0.0000; time: 0.36s
Val loss: 0.3945 score: 0.8140 time: 0.06s
Test loss: 0.3267 score: 0.8636 time: 0.06s
Epoch 119/1000, LR 0.000277
Train loss: 0.1465;  Loss pred: 0.1465; Loss self: 0.0000; time: 0.17s
Val loss: 0.3946 score: 0.7907 time: 0.06s
Test loss: 0.3225 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 120/1000, LR 0.000277
Train loss: 0.1477;  Loss pred: 0.1477; Loss self: 0.0000; time: 0.17s
Val loss: 0.3970 score: 0.7674 time: 0.06s
Test loss: 0.3226 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 121/1000, LR 0.000276
Train loss: 0.1478;  Loss pred: 0.1478; Loss self: 0.0000; time: 0.16s
Val loss: 0.3985 score: 0.7674 time: 0.06s
Test loss: 0.3204 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 122/1000, LR 0.000276
Train loss: 0.1383;  Loss pred: 0.1383; Loss self: 0.0000; time: 0.16s
Val loss: 0.4035 score: 0.7674 time: 0.06s
Test loss: 0.3223 score: 0.8864 time: 0.19s
     INFO: Early stopping counter 4 of 20
Epoch 123/1000, LR 0.000276
Train loss: 0.1351;  Loss pred: 0.1351; Loss self: 0.0000; time: 0.17s
Val loss: 0.4068 score: 0.7674 time: 0.07s
Test loss: 0.3233 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 124/1000, LR 0.000276
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 0.17s
Val loss: 0.4008 score: 0.7674 time: 0.06s
Test loss: 0.3231 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 125/1000, LR 0.000276
Train loss: 0.1195;  Loss pred: 0.1195; Loss self: 0.0000; time: 0.17s
Val loss: 0.3982 score: 0.7907 time: 0.06s
Test loss: 0.3272 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 126/1000, LR 0.000276
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 0.39s
Val loss: 0.3985 score: 0.7674 time: 0.06s
Test loss: 0.3204 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 127/1000, LR 0.000275
Train loss: 0.1216;  Loss pred: 0.1216; Loss self: 0.0000; time: 0.16s
Val loss: 0.4047 score: 0.7674 time: 0.06s
Test loss: 0.3133 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1298;  Loss pred: 0.1298; Loss self: 0.0000; time: 0.17s
Val loss: 0.4066 score: 0.7674 time: 0.06s
Test loss: 0.3090 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 129/1000, LR 0.000275
Train loss: 0.1237;  Loss pred: 0.1237; Loss self: 0.0000; time: 0.16s
Val loss: 0.3896 score: 0.8140 time: 0.06s
Test loss: 0.2995 score: 0.8864 time: 0.06s
Epoch 130/1000, LR 0.000275
Train loss: 0.1119;  Loss pred: 0.1119; Loss self: 0.0000; time: 0.15s
Val loss: 0.3888 score: 0.8140 time: 0.06s
Test loss: 0.2950 score: 0.8864 time: 0.06s
Epoch 131/1000, LR 0.000275
Train loss: 0.1121;  Loss pred: 0.1121; Loss self: 0.0000; time: 0.28s
Val loss: 0.3898 score: 0.8140 time: 0.06s
Test loss: 0.2957 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000275
Train loss: 0.1053;  Loss pred: 0.1053; Loss self: 0.0000; time: 0.30s
Val loss: 0.3885 score: 0.8140 time: 0.07s
Test loss: 0.2967 score: 0.8636 time: 0.06s
Epoch 133/1000, LR 0.000274
Train loss: 0.1063;  Loss pred: 0.1063; Loss self: 0.0000; time: 0.19s
Val loss: 0.3846 score: 0.8140 time: 0.07s
Test loss: 0.2987 score: 0.8636 time: 0.07s
Epoch 134/1000, LR 0.000274
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.42s
Val loss: 0.3858 score: 0.8140 time: 0.07s
Test loss: 0.2995 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 135/1000, LR 0.000274
Train loss: 0.1125;  Loss pred: 0.1125; Loss self: 0.0000; time: 0.17s
Val loss: 0.3961 score: 0.7907 time: 0.06s
Test loss: 0.2984 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 136/1000, LR 0.000274
Train loss: 0.1051;  Loss pred: 0.1051; Loss self: 0.0000; time: 0.15s
Val loss: 0.3927 score: 0.8140 time: 0.06s
Test loss: 0.2985 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 137/1000, LR 0.000274
Train loss: 0.0990;  Loss pred: 0.0990; Loss self: 0.0000; time: 0.33s
Val loss: 0.3839 score: 0.8140 time: 0.06s
Test loss: 0.2978 score: 0.8864 time: 0.06s
Epoch 138/1000, LR 0.000274
Train loss: 0.0976;  Loss pred: 0.0976; Loss self: 0.0000; time: 0.29s
Val loss: 0.3824 score: 0.8140 time: 0.07s
Test loss: 0.2983 score: 0.8864 time: 0.06s
Epoch 139/1000, LR 0.000273
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.18s
Val loss: 0.3853 score: 0.8140 time: 0.06s
Test loss: 0.2982 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.1005;  Loss pred: 0.1005; Loss self: 0.0000; time: 0.16s
Val loss: 0.3929 score: 0.8140 time: 0.06s
Test loss: 0.3006 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0849;  Loss pred: 0.0849; Loss self: 0.0000; time: 0.16s
Val loss: 0.3955 score: 0.8140 time: 0.29s
Test loss: 0.3033 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0915;  Loss pred: 0.0915; Loss self: 0.0000; time: 0.15s
Val loss: 0.3928 score: 0.8140 time: 0.06s
Test loss: 0.3024 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 143/1000, LR 0.000273
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.16s
Val loss: 0.3840 score: 0.8140 time: 0.06s
Test loss: 0.2986 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 144/1000, LR 0.000272
Train loss: 0.0866;  Loss pred: 0.0866; Loss self: 0.0000; time: 0.16s
Val loss: 0.3813 score: 0.8140 time: 0.06s
Test loss: 0.2966 score: 0.8864 time: 0.06s
Epoch 145/1000, LR 0.000272
Train loss: 0.0852;  Loss pred: 0.0852; Loss self: 0.0000; time: 0.17s
Val loss: 0.3876 score: 0.8140 time: 0.06s
Test loss: 0.2944 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.17s
Val loss: 0.3865 score: 0.8140 time: 0.06s
Test loss: 0.2952 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 147/1000, LR 0.000272
Train loss: 0.0781;  Loss pred: 0.0781; Loss self: 0.0000; time: 0.17s
Val loss: 0.3829 score: 0.8140 time: 0.06s
Test loss: 0.2954 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 148/1000, LR 0.000272
Train loss: 0.0763;  Loss pred: 0.0763; Loss self: 0.0000; time: 0.17s
Val loss: 0.3884 score: 0.8140 time: 0.06s
Test loss: 0.2954 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 149/1000, LR 0.000272
Train loss: 0.0801;  Loss pred: 0.0801; Loss self: 0.0000; time: 0.17s
Val loss: 0.3887 score: 0.8140 time: 0.06s
Test loss: 0.2942 score: 0.8864 time: 0.28s
     INFO: Early stopping counter 5 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0832;  Loss pred: 0.0832; Loss self: 0.0000; time: 0.18s
Val loss: 0.3799 score: 0.8140 time: 0.06s
Test loss: 0.2931 score: 0.8636 time: 0.06s
Epoch 151/1000, LR 0.000271
Train loss: 0.0785;  Loss pred: 0.0785; Loss self: 0.0000; time: 0.17s
Val loss: 0.3784 score: 0.8140 time: 0.06s
Test loss: 0.2926 score: 0.8636 time: 0.06s
Epoch 152/1000, LR 0.000271
Train loss: 0.0719;  Loss pred: 0.0719; Loss self: 0.0000; time: 0.17s
Val loss: 0.3770 score: 0.7907 time: 0.06s
Test loss: 0.2928 score: 0.8636 time: 0.06s
Epoch 153/1000, LR 0.000271
Train loss: 0.0723;  Loss pred: 0.0723; Loss self: 0.0000; time: 0.17s
Val loss: 0.3750 score: 0.7907 time: 0.06s
Test loss: 0.2924 score: 0.8636 time: 0.06s
Epoch 154/1000, LR 0.000271
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.16s
Val loss: 0.3811 score: 0.7907 time: 0.06s
Test loss: 0.2940 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 155/1000, LR 0.000270
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.17s
Val loss: 0.3902 score: 0.7907 time: 0.06s
Test loss: 0.2989 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0707;  Loss pred: 0.0707; Loss self: 0.0000; time: 0.17s
Val loss: 0.3954 score: 0.8140 time: 0.07s
Test loss: 0.3004 score: 0.8864 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 157/1000, LR 0.000270
Train loss: 0.0708;  Loss pred: 0.0708; Loss self: 0.0000; time: 0.18s
Val loss: 0.3896 score: 0.7907 time: 0.07s
Test loss: 0.2976 score: 0.8636 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 158/1000, LR 0.000270
Train loss: 0.0731;  Loss pred: 0.0731; Loss self: 0.0000; time: 0.33s
Val loss: 0.3751 score: 0.7907 time: 0.07s
Test loss: 0.2932 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 159/1000, LR 0.000270
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.17s
Val loss: 0.3724 score: 0.7907 time: 0.07s
Test loss: 0.2913 score: 0.8636 time: 0.06s
Epoch 160/1000, LR 0.000269
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 0.17s
Val loss: 0.3762 score: 0.7907 time: 0.07s
Test loss: 0.2894 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 161/1000, LR 0.000269
Train loss: 0.0626;  Loss pred: 0.0626; Loss self: 0.0000; time: 0.17s
Val loss: 0.3866 score: 0.7907 time: 0.07s
Test loss: 0.2939 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 162/1000, LR 0.000269
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.17s
Val loss: 0.3846 score: 0.7907 time: 0.07s
Test loss: 0.2935 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 163/1000, LR 0.000269
Train loss: 0.0742;  Loss pred: 0.0742; Loss self: 0.0000; time: 0.18s
Val loss: 0.3697 score: 0.7907 time: 0.07s
Test loss: 0.2891 score: 0.8636 time: 0.06s
Epoch 164/1000, LR 0.000269
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.17s
Val loss: 0.3730 score: 0.7907 time: 0.06s
Test loss: 0.2905 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 165/1000, LR 0.000268
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.17s
Val loss: 0.3801 score: 0.7907 time: 0.06s
Test loss: 0.2930 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 166/1000, LR 0.000268
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.17s
Val loss: 0.3825 score: 0.7907 time: 0.07s
Test loss: 0.2933 score: 0.8636 time: 0.26s
     INFO: Early stopping counter 3 of 20
Epoch 167/1000, LR 0.000268
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.17s
Val loss: 0.3838 score: 0.7907 time: 0.06s
Test loss: 0.2936 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 168/1000, LR 0.000268
Train loss: 0.0580;  Loss pred: 0.0580; Loss self: 0.0000; time: 0.17s
Val loss: 0.3938 score: 0.7907 time: 0.06s
Test loss: 0.2973 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 169/1000, LR 0.000267
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.17s
Val loss: 0.3963 score: 0.7907 time: 0.06s
Test loss: 0.2999 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 170/1000, LR 0.000267
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.17s
Val loss: 0.3765 score: 0.7907 time: 0.06s
Test loss: 0.2958 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 171/1000, LR 0.000267
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.17s
Val loss: 0.3681 score: 0.8140 time: 0.06s
Test loss: 0.2969 score: 0.8636 time: 0.06s
Epoch 172/1000, LR 0.000267
Train loss: 0.0541;  Loss pred: 0.0541; Loss self: 0.0000; time: 0.17s
Val loss: 0.3705 score: 0.8140 time: 0.07s
Test loss: 0.2969 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 173/1000, LR 0.000267
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.17s
Val loss: 0.3821 score: 0.7907 time: 0.06s
Test loss: 0.2988 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 174/1000, LR 0.000266
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.36s
Val loss: 0.3834 score: 0.7907 time: 0.06s
Test loss: 0.2971 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 175/1000, LR 0.000266
Train loss: 0.0631;  Loss pred: 0.0631; Loss self: 0.0000; time: 0.17s
Val loss: 0.3759 score: 0.7907 time: 0.06s
Test loss: 0.2959 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 176/1000, LR 0.000266
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.17s
Val loss: 0.3732 score: 0.7907 time: 0.06s
Test loss: 0.2957 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 177/1000, LR 0.000266
Train loss: 0.0576;  Loss pred: 0.0576; Loss self: 0.0000; time: 0.16s
Val loss: 0.3718 score: 0.8140 time: 0.06s
Test loss: 0.2951 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 178/1000, LR 0.000265
Train loss: 0.0611;  Loss pred: 0.0611; Loss self: 0.0000; time: 0.17s
Val loss: 0.3750 score: 0.7907 time: 0.06s
Test loss: 0.2956 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 179/1000, LR 0.000265
Train loss: 0.0505;  Loss pred: 0.0505; Loss self: 0.0000; time: 0.17s
Val loss: 0.3919 score: 0.8140 time: 0.06s
Test loss: 0.2978 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.17s
Val loss: 0.3966 score: 0.7907 time: 0.06s
Test loss: 0.2981 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 181/1000, LR 0.000265
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.17s
Val loss: 0.3970 score: 0.7907 time: 0.06s
Test loss: 0.2966 score: 0.8636 time: 0.26s
     INFO: Early stopping counter 10 of 20
Epoch 182/1000, LR 0.000265
Train loss: 0.0507;  Loss pred: 0.0507; Loss self: 0.0000; time: 0.17s
Val loss: 0.3959 score: 0.7907 time: 0.06s
Test loss: 0.2961 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 183/1000, LR 0.000264
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.17s
Val loss: 0.3880 score: 0.8140 time: 0.06s
Test loss: 0.2960 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 184/1000, LR 0.000264
Train loss: 0.0554;  Loss pred: 0.0554; Loss self: 0.0000; time: 0.17s
Val loss: 0.3838 score: 0.7907 time: 0.06s
Test loss: 0.2973 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 185/1000, LR 0.000264
Train loss: 0.0467;  Loss pred: 0.0467; Loss self: 0.0000; time: 0.17s
Val loss: 0.3853 score: 0.7907 time: 0.06s
Test loss: 0.2996 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 186/1000, LR 0.000264
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.17s
Val loss: 0.3827 score: 0.7907 time: 0.06s
Test loss: 0.3009 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.17s
Val loss: 0.3827 score: 0.7907 time: 0.06s
Test loss: 0.3021 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 188/1000, LR 0.000263
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.16s
Val loss: 0.3847 score: 0.7907 time: 0.06s
Test loss: 0.3022 score: 0.8636 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 189/1000, LR 0.000263
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.15s
Val loss: 0.3864 score: 0.7907 time: 0.06s
Test loss: 0.3011 score: 0.8636 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 190/1000, LR 0.000263
Train loss: 0.0475;  Loss pred: 0.0475; Loss self: 0.0000; time: 0.36s
Val loss: 0.3781 score: 0.7907 time: 0.06s
Test loss: 0.3013 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 191/1000, LR 0.000262
Train loss: 0.0478;  Loss pred: 0.0478; Loss self: 0.0000; time: 0.17s
Val loss: 0.3813 score: 0.8140 time: 0.06s
Test loss: 0.3013 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 170,   Train_Loss: 0.0633,   Val_Loss: 0.3681,   Val_Precision: 0.8500,   Val_Recall: 0.7727,   Val_accuracy: 0.8095,   Val_Score: 0.8140,   Val_Loss: 0.3681,   Test_Precision: 0.8636,   Test_Recall: 0.8636,   Test_accuracy: 0.8636,   Test_Score: 0.8636,   Test_loss: 0.2969


[0.06350060299973848, 0.06373522700050671, 0.07073915399996622, 0.06606672099951538, 0.06469852999998693, 0.06363127500026167, 0.062068326999906276, 0.06356700800006365, 0.0658538640000188, 0.06851071199980652, 0.06652169700009836, 0.06886783199934143, 0.06842447800045193, 0.06421562400009861, 0.07551755200074695, 0.061776715000632976, 0.06244954199974018, 0.06524099100079184, 0.07655509899996105, 0.0658303509999314, 0.06636043499929656, 0.06723987800069153, 0.06994144099917321, 0.06739384299999074, 0.06884786299997359, 0.06819067700052983, 0.06794365099995048, 0.06782897799985221, 0.19556002100034675, 0.06712780299949372, 0.2582174090002809, 0.06798611999965942, 0.06821748599941202, 0.06824019399937242, 0.2014430600002015, 0.06769217299915908, 0.06511741099984647, 0.06403170800058433, 0.06244684400007827, 0.06116209399988293, 0.06185214699962671, 0.06329495800036966, 0.20426789399971312, 0.06495283799995377, 0.06463266099945031, 0.06729733599968313, 0.06661839099979261, 0.06791740999960894, 0.17394717000024684, 0.07020363400079077, 0.06224347500028671, 0.0617642919996797, 0.06267032300002029, 0.06280726000022696, 0.0634187280002152, 0.06321506699987367, 0.06216300899995986, 0.06183282399979362, 0.061743704000036814, 0.06394455600002402, 0.0632358770008068, 0.06161817300016992, 0.06198944300012954, 0.06991618299980473, 0.06651037999927212, 0.0610227790002682, 0.0633783149996816, 0.059931950999271066, 0.060488209999675746, 0.32696698200015817, 0.06912919900059933, 0.06957099400005973, 0.06691877899993415, 0.06816951599921595, 0.071528939000018, 0.06328540999948018, 0.06492626700037363, 0.3450811650000105, 0.06513603200073703, 0.07806837999942218, 0.06495670599997538, 0.06049580099988816, 0.06201394099934987, 0.06784448200050974, 0.06975755699932051, 0.29286755500015715, 0.0688028190006662, 0.06177164099972288, 0.06794669099963357, 0.06324412799949641, 0.06341912599964417, 0.06104711399984808, 0.06364352099990356, 0.0631757959999959, 0.062210632999267546, 0.06160685399936483, 0.06147956899985729, 0.05983518700031709, 0.06256287199994404, 0.06136330400022416, 0.06226615500054322, 0.06412958500004606, 0.06285159700018994, 0.06311834700045438, 0.0625745850002204, 0.06311157599975559, 0.06302812200010521, 0.06895593499939423, 0.06557479799994326, 0.06818578200000047, 0.06407179300003918, 0.0680360809992635, 0.0691134789994976, 0.07238891199995123, 0.06952343899956759, 0.06825794199994561, 0.0662621860001309, 0.06543172000056074, 0.06647999199958576, 0.0667790839997906, 0.06386670599931676, 0.1934934600003544, 0.06623523899997963, 0.06694045800031745, 0.06738868899992667, 0.061274583999875176, 0.06104577599944605, 0.061762522000208264, 0.06124599900067551, 0.06070841900054802, 0.06580629099971702, 0.06793537199973798, 0.07109902799948031, 0.06814388600014354, 0.06034620000082214, 0.061514279000220995, 0.06503680700006953, 0.06738740599939774, 0.06212860100004036, 0.06252250699981232, 0.06020272999921872, 0.06043923600009293, 0.06170038000072964, 0.06748890900053084, 0.06565391599997383, 0.0670236260002639, 0.06738802699965163, 0.0663078219995441, 0.2866026100000454, 0.06763153700012481, 0.06380279699988023, 0.06170751799982099, 0.0631392949999281, 0.061892814000202634, 0.06729150400042272, 0.06896494499960681, 0.07280482900023344, 0.06901519800067035, 0.06899085299937724, 0.0674443389998487, 0.06824306500038801, 0.06868973200016626, 0.06905440199989243, 0.0666594439999244, 0.06847605799976009, 0.26471962799951143, 0.06865426500007743, 0.06697241499932716, 0.06564353199973993, 0.06546640200031106, 0.06769317300040711, 0.067694979999942, 0.06735809899964806, 0.06603790600001958, 0.0651524969998718, 0.06487727700005053, 0.06441163300041808, 0.06699942599971109, 0.06519279100029962, 0.06489685300039127, 0.26541176000046107, 0.06678862900025706, 0.06516621600076178, 0.06503236899970943, 0.06482019899976876, 0.06478466799944727, 0.06575518000045122, 0.059186753000176395, 0.06008414099960646, 0.06061053199937305, 0.06012160899990704]
[0.001443195522721329, 0.0014485278863751525, 0.0016077080454537777, 0.0015015163863526223, 0.0014704211363633394, 0.0014461653409150379, 0.0014106437954524154, 0.0014447047272741738, 0.0014966787272731547, 0.001557061636359239, 0.0015118567500022355, 0.0015651779999850326, 0.0015551017727375438, 0.0014594460000022411, 0.001716308000016976, 0.0014040162500143858, 0.0014193077727213677, 0.0014827497954725418, 0.0017398886136354783, 0.0014961443409075318, 0.0015081917045294672, 0.001528179045470262, 0.001589578204526664, 0.0015316782499997896, 0.0015647241590903088, 0.0015497881136484052, 0.0015441738863625108, 0.0015415676818148231, 0.004444545931826063, 0.00152563188635213, 0.005868577477279111, 0.0015451390909013505, 0.001550397409077546, 0.001550913499985737, 0.004578251363640943, 0.0015384584772536155, 0.0014799411590874197, 0.0014552660909223712, 0.0014192464545472335, 0.00139004759090643, 0.0014057306136278798, 0.0014385217727356742, 0.004642452136357117, 0.001476200863635313, 0.0014689241136238707, 0.0015294849090837074, 0.0015140543409043776, 0.001543577499991112, 0.003953344772732883, 0.0015955371363816084, 0.001414624431824698, 0.0014037339090836294, 0.0014243255227277339, 0.0014274377272778854, 0.0014413347272776182, 0.0014367060681789471, 0.0014127956590899967, 0.001405291454540764, 0.0014032660000008366, 0.0014532853636369096, 0.001437179022745609, 0.0014004130227311346, 0.0014088509772756713, 0.0015890041590864712, 0.0015115995454380027, 0.0013868813409151865, 0.0014404162499927638, 0.0013620897954379789, 0.001374732045447176, 0.0074310677727308675, 0.0015711181591045302, 0.001581158954546812, 0.0015208813409075942, 0.0015493071818003625, 0.0016256577045458637, 0.0014383047727154587, 0.0014755969772812189, 0.007842753750000238, 0.0014803643636531144, 0.0017742813636232313, 0.0014762887727267132, 0.0013749045681792763, 0.0014094077499852244, 0.0015419200454661304, 0.00158539902271183, 0.006656080795458117, 0.001563700431833323, 0.0014039009318118835, 0.0015442429772643993, 0.0014373665454431002, 0.0014413437727191856, 0.0013874344090874563, 0.0014464436590887171, 0.0014358135454544522, 0.001413878022710626, 0.001400155772712837, 0.0013972629318149384, 0.0013598906136435703, 0.0014218834545441825, 0.00139462054545964, 0.0014151398863759823, 0.001457490568182865, 0.0014284453863679532, 0.0014345078863739632, 0.001422149659095918, 0.001434353999994445, 0.0014324573181842095, 0.0015671803408953235, 0.0014903363181805287, 0.0015496768636363743, 0.001456177113637254, 0.0015462745681650797, 0.0015707608863522182, 0.001645202545453437, 0.0015800781590810816, 0.0015513168636351277, 0.0015059587727302476, 0.0014870845454672897, 0.0015109089090814946, 0.0015177064545406954, 0.0014515160454390173, 0.004397578636371691, 0.001505346340908628, 0.0015213740454617603, 0.001531561113634697, 0.0013926041818153449, 0.0013874039999874103, 0.001403693681822915, 0.0013919545227426252, 0.0013797367954670005, 0.0014955975227208414, 0.0015439857272667723, 0.001615886999988189, 0.0015487246818214442, 0.0013715045454732303, 0.001398051795459568, 0.0014781092500015802, 0.0015315319545317668, 0.0014120136590918264, 0.0014209660681775529, 0.0013682438636186073, 0.0013736190000021122, 0.0014022813636529463, 0.0015338388409211555, 0.0014921344545448599, 0.001523264227278725, 0.0015315460681739007, 0.0015069959545350932, 0.006513695681819213, 0.001537080386366473, 0.0014500635681790961, 0.0014024435909050226, 0.0014349839772710932, 0.0014066548636409689, 0.0015293523636459709, 0.0015673851136274277, 0.00165465520455076, 0.0015685272272879627, 0.001567973931804028, 0.0015328258863601977, 0.0015509787500088185, 0.0015611302727310513, 0.0015694182272702826, 0.0015149873636346456, 0.001556274045449093, 0.006016355181807078, 0.0015603242045472143, 0.001522100340893799, 0.001491898454539544, 0.0014878727727343423, 0.0015384812045547071, 0.0015385222727259547, 0.0015308658863556377, 0.001500861500000445, 0.0014807385681789046, 0.0014744835681829666, 0.0014639007500095017, 0.0015227142272661613, 0.0014816543409159005, 0.00147492847728162, 0.006032085454555933, 0.0015179233863694787, 0.0014810503636536769, 0.0014780083863570326, 0.0014731863409038356, 0.001472378818169256, 0.001494435909101164, 0.0013451534772767363, 0.001365548659081965, 0.001377512090894842, 0.0013664002045433417]
[692.9068059429485, 690.3560569361459, 622.0034805621369, 665.9933978004259, 680.0772753261765, 691.4838654391109, 708.8961814625105, 692.1829638411791, 668.1460635322392, 642.2353339449204, 661.4383274066947, 638.9049680033598, 643.0447302748789, 685.1915041724492, 582.6460052566957, 712.242468696323, 704.5688181377385, 674.4226187408153, 574.7494363507046, 668.3847090538201, 663.0456837792943, 654.3735846687212, 629.0977047573288, 652.8786316578808, 639.0902793891639, 645.2494964914064, 647.5954611275168, 648.6902987112067, 224.9948623186228, 655.4661114163359, 170.39904540267514, 647.1909266217931, 644.9959179143489, 644.7812853580787, 218.42400527452267, 650.0012933629209, 675.7025398338355, 687.1595553815068, 704.5992588503728, 719.3998295755574, 711.3738509394921, 695.1580566613699, 215.40340549093727, 677.414588105162, 680.7703616036204, 653.8148850380522, 660.478275438039, 647.8456702081742, 252.95036418205345, 626.748182287891, 706.9014061280693, 712.3857260474738, 702.0866958031436, 700.5559548345367, 693.8013641625025, 696.0365952010765, 707.8164443427829, 711.5961580559034, 712.623266009013, 688.0961062577942, 695.8075397521351, 714.0750505517044, 709.7982796830107, 629.3249733058638, 661.5508737204859, 721.0422193293855, 694.2437646097256, 734.1659876971994, 727.4144829254473, 134.57016280615977, 636.4893653638101, 632.4474823510819, 657.5134910940814, 645.4497931378311, 615.1356446093647, 695.262936597257, 677.6918192408443, 127.50623465641382, 675.5093709039895, 563.6084673503622, 677.3742498583083, 727.3232071112067, 709.5178808336223, 648.542058286618, 630.7560340799863, 150.2385609084502, 639.5086805901653, 712.3009731957322, 647.5664870896699, 695.7167628329124, 693.7970100730633, 720.7547927672633, 691.3508132283673, 696.4692617407283, 707.2745908326965, 714.2062472538144, 715.6849131473601, 735.3532629515611, 703.2925214820598, 717.040920740501, 706.643922362255, 686.1107864641319, 700.0617661293003, 697.103173498559, 703.1608759346151, 697.1779630439017, 698.1010793868577, 638.0886576389188, 670.9894859308308, 645.2958184156295, 686.7296502842225, 646.7156742975291, 636.6341361620624, 607.8278949686334, 632.8800852367741, 644.6136333854756, 664.028802187617, 672.4567228191911, 661.8532685785245, 658.8889419348425, 688.9348575527084, 227.39786657347184, 664.2989542170072, 657.3005520785553, 652.9285649116557, 718.0791304938053, 720.7705902599923, 712.4061417027567, 718.4142755107106, 724.7759161641618, 668.6290828970928, 647.6743808831974, 618.8551550989081, 645.6925570682499, 729.1262747182185, 715.2810813216543, 676.5399783533801, 652.9409961320256, 708.2084465409323, 703.7465724164272, 730.8638661497745, 728.0039079238584, 713.1236468799653, 651.958975950462, 670.1808921804076, 656.4849236868616, 652.934979090981, 663.5717879604388, 153.52267727077933, 650.5840611003532, 689.6249391712811, 713.0411565107454, 696.8718925361791, 710.9064389907356, 653.871549664332, 638.0052938525631, 604.3555160312088, 637.5407341376116, 637.7657049753708, 652.389817329201, 644.7541592651183, 640.5615325430808, 637.1787855040515, 660.0715121483746, 642.5603529945337, 166.21359108317122, 640.8924485601932, 656.986910214333, 670.2869065633811, 672.1004768184898, 649.9916911818475, 649.9743407862398, 653.225085824199, 666.2839975572053, 675.338659699974, 678.203556538998, 683.106419607688, 656.7220441588519, 674.9212501087395, 677.9989778508168, 165.78014478304792, 658.7947777731843, 675.1964852383887, 676.5861474336971, 678.8007546869296, 679.1730413803372, 669.14880317715, 743.4095936952119, 732.3063834812616, 725.9464411309759, 731.8500075416816]
Elapsed: 0.07700857694761198~0.047081377203167254
Time per graph: 0.0017501949306275456~0.001070031300071983
Speed: 644.0150631546635~123.86397389697898
Total Time: 0.0610
best val loss: 0.36810695639876434 test_score: 0.8636

Testing...
Test loss: 0.3655 score: 0.8636 time: 0.06s
test Score 0.8636
Epoch Time List: [0.5700324429999455, 0.4866280430005645, 0.30414171000120405, 0.2897876550014189, 0.2873983439994845, 0.2875784620009654, 0.27489089600112493, 0.28859704000115016, 0.3065372729988667, 0.29033887599962327, 0.49570806399970024, 0.2980611189996125, 0.3043050509995737, 0.4414936459997989, 0.288208357998883, 0.27317378200041276, 0.5084014069998375, 0.2916986829995949, 0.28853621700000076, 0.2945799330009322, 0.41461351300040405, 0.2958135429998947, 0.5077828389994465, 0.3043055819998699, 0.3001781340008165, 0.3092997350004225, 0.3077048189998095, 0.30293055699985416, 0.42937507199894753, 0.2954339980005898, 0.4868299999998271, 0.2988134750003155, 0.30390727799931483, 0.30285702499986655, 0.43846086099893, 0.2950884820002102, 0.3172638840005675, 0.2936941880006998, 0.2910829920010656, 0.28717959799996606, 0.2913839510001708, 0.47304545900078665, 0.4332024059995092, 0.30994950600052107, 0.30053262999899744, 0.30754917200010823, 0.2934067980004329, 0.2963256270004422, 0.6340499089992591, 0.3565279339991321, 0.28431488200021704, 0.28350378299910517, 0.2841179609995379, 0.2892710390005959, 0.28752186500059906, 0.4158761329990739, 0.47978541500015126, 0.27314411900078994, 0.2805995589988015, 0.31251994899866986, 0.2751492830002462, 0.27412828100023034, 0.2733636800003296, 0.421865343000718, 0.5032315789994755, 0.28205012700072984, 0.27598566399956326, 0.270181799999591, 0.2666669420004837, 0.543753519999882, 0.3023085949998858, 0.3028060810001989, 0.2998941859987099, 0.2995304500000202, 0.3065309410003465, 0.28818615099953604, 0.28258505999929184, 0.6976713960002598, 0.3020038149998072, 0.291246005001085, 0.3013309609996213, 0.281013604000691, 0.2707161369999085, 0.2912344149999626, 0.3037727659993834, 0.5308232469997165, 0.31773888699990493, 0.2856947089994719, 0.406455036000807, 0.2902667769994878, 0.27539824699942983, 0.27156221399945935, 0.2713888410007712, 0.633021173999623, 0.2720852319998812, 0.2727071049994265, 0.2758389190003072, 0.28202960199996596, 0.2777938469998844, 0.27482530900033453, 0.27364117500019347, 0.4981661340016217, 0.2921308360000694, 0.2852605140005835, 0.28021839299981366, 0.276468054000361, 0.2745185190005941, 0.2903616269995837, 0.41322249899985763, 0.37979863099917566, 0.3074161060012557, 0.3113480879992494, 0.31183114599934925, 0.3180695180008115, 0.319006810999781, 0.3761736230007955, 0.3076056920008341, 0.4872973639994598, 0.2942673569996259, 0.2911014390001583, 0.27777745300045353, 0.40543582099871855, 0.2972243840004012, 0.2958638879999853, 0.29811787200014805, 0.5062063140003374, 0.27157074599927, 0.2808315499996752, 0.27175447100034944, 0.2666125389987428, 0.3981828839996524, 0.42980992999946466, 0.3239768240000558, 0.5493599060009728, 0.28840962200047215, 0.26634609699885914, 0.4452717220001432, 0.42240187599963974, 0.29156495400002314, 0.27971969500049454, 0.5040559949993622, 0.2679832069998156, 0.2727228260000629, 0.28302529500069795, 0.29343686799984425, 0.28970173000016075, 0.2936274349995074, 0.29234915300003195, 0.5099962809990757, 0.30915104799896653, 0.28916843200022413, 0.28845488900060445, 0.2879303419995267, 0.2753992190000645, 0.29651099999955477, 0.30014128099992377, 0.3171970000003057, 0.4620925070003068, 0.3031702759990367, 0.3005791859995952, 0.2985902150003312, 0.30386245299996517, 0.3076246889995673, 0.2991574600009699, 0.298300516999916, 0.49704628200015577, 0.29868907899981423, 0.299690205000843, 0.29501909600003273, 0.2911140180003713, 0.2941526399990835, 0.298582204000013, 0.2969782109994412, 0.48930144700079836, 0.28815188400039915, 0.2883717219992832, 0.28689890400073637, 0.29165983099937876, 0.28901122200022655, 0.28838628600078664, 0.490149829000984, 0.2939032670001325, 0.2896611969999867, 0.2922746989997904, 0.2875826799991046, 0.2892925460000697, 0.2887165910005933, 0.2672805870006414, 0.2659578290003992, 0.47540582200053905, 0.2810636959993644]
Total Epoch List: [191]
Total Time List: [0.061039842999889515]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f8513610>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7522;  Loss pred: 0.7522; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7221 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7106 score: 0.5116 time: 0.06s
Epoch 2/1000, LR 0.000015
Train loss: 0.7248;  Loss pred: 0.7248; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7223 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7110 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7192 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7086 score: 0.5116 time: 0.06s
Epoch 4/1000, LR 0.000075
Train loss: 0.7142;  Loss pred: 0.7142; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7137 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7040 score: 0.5116 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.7114;  Loss pred: 0.7114; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7065 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5116 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5116 time: 0.06s
Epoch 7/1000, LR 0.000165
Train loss: 0.6992;  Loss pred: 0.6992; Loss self: 0.0000; time: 0.17s
Val loss: 0.6882 score: 0.4545 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6830 score: 0.5116 time: 0.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.18s
Val loss: 0.6797 score: 0.5000 time: 0.07s
Test loss: 0.6770 score: 0.5349 time: 0.06s
Epoch 9/1000, LR 0.000225
Train loss: 0.6256;  Loss pred: 0.6256; Loss self: 0.0000; time: 0.20s
Val loss: 0.6738 score: 0.7500 time: 0.07s
Test loss: 0.6741 score: 0.8372 time: 0.06s
Epoch 10/1000, LR 0.000255
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.20s
Val loss: 0.6706 score: 0.6591 time: 0.06s
Test loss: 0.6746 score: 0.5581 time: 0.06s
Epoch 11/1000, LR 0.000285
Train loss: 0.5861;  Loss pred: 0.5861; Loss self: 0.0000; time: 0.18s
Val loss: 0.6714 score: 0.5682 time: 0.07s
Test loss: 0.6792 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5658;  Loss pred: 0.5658; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6746 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6790 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5328;  Loss pred: 0.5328; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7008 score: 0.4884 time: 0.28s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5650;  Loss pred: 0.5650; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7067 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7120 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7162 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7209 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5097;  Loss pred: 0.5097; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7247 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.4980;  Loss pred: 0.4980; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7265 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4928;  Loss pred: 0.4928; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7281 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.4763;  Loss pred: 0.4763; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7303 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.4958;  Loss pred: 0.4958; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.32s
Test loss: 0.7304 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.5000 time: 0.07s
Test loss: 0.7293 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.4599;  Loss pred: 0.4599; Loss self: 0.0000; time: 0.18s
Val loss: 0.6772 score: 0.5227 time: 0.07s
Test loss: 0.7271 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.4664;  Loss pred: 0.4664; Loss self: 0.0000; time: 0.18s
Val loss: 0.6714 score: 0.5227 time: 0.06s
Test loss: 0.7241 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.4507;  Loss pred: 0.4507; Loss self: 0.0000; time: 0.17s
Val loss: 0.6654 score: 0.5227 time: 0.06s
Test loss: 0.7211 score: 0.5116 time: 0.05s
Epoch 28/1000, LR 0.000285
Train loss: 0.4387;  Loss pred: 0.4387; Loss self: 0.0000; time: 0.16s
Val loss: 0.6592 score: 0.5227 time: 0.06s
Test loss: 0.7181 score: 0.5349 time: 0.05s
Epoch 29/1000, LR 0.000285
Train loss: 0.4391;  Loss pred: 0.4391; Loss self: 0.0000; time: 0.18s
Val loss: 0.6526 score: 0.5455 time: 0.19s
Test loss: 0.7139 score: 0.5349 time: 0.07s
Epoch 30/1000, LR 0.000285
Train loss: 0.4477;  Loss pred: 0.4477; Loss self: 0.0000; time: 0.19s
Val loss: 0.6448 score: 0.5682 time: 0.07s
Test loss: 0.7089 score: 0.5349 time: 0.06s
Epoch 31/1000, LR 0.000285
Train loss: 0.4132;  Loss pred: 0.4132; Loss self: 0.0000; time: 0.18s
Val loss: 0.6390 score: 0.6136 time: 0.27s
Test loss: 0.7064 score: 0.5349 time: 0.18s
Epoch 32/1000, LR 0.000285
Train loss: 0.4171;  Loss pred: 0.4171; Loss self: 0.0000; time: 0.17s
Val loss: 0.6333 score: 0.6364 time: 0.06s
Test loss: 0.7052 score: 0.5349 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.4048;  Loss pred: 0.4048; Loss self: 0.0000; time: 0.18s
Val loss: 0.6274 score: 0.6364 time: 0.07s
Test loss: 0.7032 score: 0.5349 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.3937;  Loss pred: 0.3937; Loss self: 0.0000; time: 0.17s
Val loss: 0.6234 score: 0.6364 time: 0.07s
Test loss: 0.7034 score: 0.5349 time: 0.06s
Epoch 35/1000, LR 0.000285
Train loss: 0.3986;  Loss pred: 0.3986; Loss self: 0.0000; time: 0.19s
Val loss: 0.6192 score: 0.6591 time: 0.06s
Test loss: 0.7033 score: 0.5349 time: 0.06s
Epoch 36/1000, LR 0.000285
Train loss: 0.3919;  Loss pred: 0.3919; Loss self: 0.0000; time: 0.18s
Val loss: 0.6149 score: 0.6591 time: 0.06s
Test loss: 0.7027 score: 0.5349 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.3860;  Loss pred: 0.3860; Loss self: 0.0000; time: 0.36s
Val loss: 0.6120 score: 0.6591 time: 0.07s
Test loss: 0.7033 score: 0.5349 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3713;  Loss pred: 0.3713; Loss self: 0.0000; time: 0.18s
Val loss: 0.6086 score: 0.6591 time: 0.07s
Test loss: 0.7036 score: 0.5349 time: 0.19s
Epoch 39/1000, LR 0.000284
Train loss: 0.3733;  Loss pred: 0.3733; Loss self: 0.0000; time: 0.19s
Val loss: 0.6041 score: 0.6591 time: 0.07s
Test loss: 0.7024 score: 0.5349 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.3895;  Loss pred: 0.3895; Loss self: 0.0000; time: 0.19s
Val loss: 0.6004 score: 0.6591 time: 0.07s
Test loss: 0.7017 score: 0.5349 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3561;  Loss pred: 0.3561; Loss self: 0.0000; time: 0.19s
Val loss: 0.5956 score: 0.6591 time: 0.07s
Test loss: 0.6993 score: 0.5349 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3772;  Loss pred: 0.3772; Loss self: 0.0000; time: 0.19s
Val loss: 0.5914 score: 0.6591 time: 0.07s
Test loss: 0.6980 score: 0.5349 time: 0.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.3650;  Loss pred: 0.3650; Loss self: 0.0000; time: 0.19s
Val loss: 0.5858 score: 0.6591 time: 0.07s
Test loss: 0.6952 score: 0.5581 time: 0.06s
Epoch 44/1000, LR 0.000284
Train loss: 0.3338;  Loss pred: 0.3338; Loss self: 0.0000; time: 0.20s
Val loss: 0.5801 score: 0.6591 time: 0.27s
Test loss: 0.6920 score: 0.5581 time: 0.20s
Epoch 45/1000, LR 0.000284
Train loss: 0.3451;  Loss pred: 0.3451; Loss self: 0.0000; time: 0.19s
Val loss: 0.5756 score: 0.6591 time: 0.06s
Test loss: 0.6911 score: 0.5581 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.3423;  Loss pred: 0.3423; Loss self: 0.0000; time: 0.18s
Val loss: 0.5711 score: 0.6591 time: 0.06s
Test loss: 0.6904 score: 0.5581 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3134;  Loss pred: 0.3134; Loss self: 0.0000; time: 0.18s
Val loss: 0.5673 score: 0.6591 time: 0.07s
Test loss: 0.6902 score: 0.5581 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3288;  Loss pred: 0.3288; Loss self: 0.0000; time: 0.19s
Val loss: 0.5612 score: 0.6818 time: 0.06s
Test loss: 0.6863 score: 0.5581 time: 0.06s
Epoch 49/1000, LR 0.000284
Train loss: 0.3321;  Loss pred: 0.3321; Loss self: 0.0000; time: 0.20s
Val loss: 0.5567 score: 0.6818 time: 0.07s
Test loss: 0.6833 score: 0.5581 time: 0.06s
Epoch 50/1000, LR 0.000284
Train loss: 0.3280;  Loss pred: 0.3280; Loss self: 0.0000; time: 0.18s
Val loss: 0.5519 score: 0.6818 time: 0.07s
Test loss: 0.6797 score: 0.5814 time: 0.06s
Epoch 51/1000, LR 0.000284
Train loss: 0.3061;  Loss pred: 0.3061; Loss self: 0.0000; time: 0.27s
Val loss: 0.5484 score: 0.7273 time: 0.07s
Test loss: 0.6775 score: 0.5814 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.2987;  Loss pred: 0.2987; Loss self: 0.0000; time: 0.37s
Val loss: 0.5447 score: 0.7273 time: 0.07s
Test loss: 0.6755 score: 0.5814 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.2913;  Loss pred: 0.2913; Loss self: 0.0000; time: 0.19s
Val loss: 0.5416 score: 0.7500 time: 0.07s
Test loss: 0.6739 score: 0.5814 time: 0.06s
Epoch 54/1000, LR 0.000284
Train loss: 0.3199;  Loss pred: 0.3199; Loss self: 0.0000; time: 0.19s
Val loss: 0.5384 score: 0.7500 time: 0.07s
Test loss: 0.6719 score: 0.5814 time: 0.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.2740;  Loss pred: 0.2740; Loss self: 0.0000; time: 0.19s
Val loss: 0.5372 score: 0.7500 time: 0.07s
Test loss: 0.6721 score: 0.5814 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.2859;  Loss pred: 0.2859; Loss self: 0.0000; time: 0.22s
Val loss: 0.5353 score: 0.7500 time: 0.08s
Test loss: 0.6709 score: 0.5814 time: 0.08s
Epoch 57/1000, LR 0.000283
Train loss: 0.2866;  Loss pred: 0.2866; Loss self: 0.0000; time: 0.23s
Val loss: 0.5326 score: 0.7727 time: 0.08s
Test loss: 0.6688 score: 0.6279 time: 0.17s
Epoch 58/1000, LR 0.000283
Train loss: 0.2936;  Loss pred: 0.2936; Loss self: 0.0000; time: 0.24s
Val loss: 0.5300 score: 0.7955 time: 0.06s
Test loss: 0.6680 score: 0.6279 time: 0.06s
Epoch 59/1000, LR 0.000283
Train loss: 0.2783;  Loss pred: 0.2783; Loss self: 0.0000; time: 0.17s
Val loss: 0.5283 score: 0.8182 time: 0.29s
Test loss: 0.6681 score: 0.6279 time: 0.06s
Epoch 60/1000, LR 0.000283
Train loss: 0.3016;  Loss pred: 0.3016; Loss self: 0.0000; time: 0.19s
Val loss: 0.5244 score: 0.8182 time: 0.07s
Test loss: 0.6662 score: 0.6279 time: 0.06s
Epoch 61/1000, LR 0.000283
Train loss: 0.2790;  Loss pred: 0.2790; Loss self: 0.0000; time: 0.18s
Val loss: 0.5204 score: 0.8182 time: 0.07s
Test loss: 0.6627 score: 0.6279 time: 0.06s
Epoch 62/1000, LR 0.000283
Train loss: 0.2940;  Loss pred: 0.2940; Loss self: 0.0000; time: 0.19s
Val loss: 0.5175 score: 0.8182 time: 0.07s
Test loss: 0.6594 score: 0.6279 time: 0.06s
Epoch 63/1000, LR 0.000283
Train loss: 0.2672;  Loss pred: 0.2672; Loss self: 0.0000; time: 0.31s
Val loss: 0.5149 score: 0.8182 time: 0.07s
Test loss: 0.6570 score: 0.6279 time: 0.06s
Epoch 64/1000, LR 0.000283
Train loss: 0.2688;  Loss pred: 0.2688; Loss self: 0.0000; time: 0.19s
Val loss: 0.5121 score: 0.8182 time: 0.07s
Test loss: 0.6546 score: 0.6279 time: 0.06s
Epoch 65/1000, LR 0.000283
Train loss: 0.2718;  Loss pred: 0.2718; Loss self: 0.0000; time: 0.19s
Val loss: 0.5107 score: 0.8182 time: 0.16s
Test loss: 0.6527 score: 0.6279 time: 0.06s
Epoch 66/1000, LR 0.000283
Train loss: 0.2758;  Loss pred: 0.2758; Loss self: 0.0000; time: 0.18s
Val loss: 0.5118 score: 0.8182 time: 0.07s
Test loss: 0.6541 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.2610;  Loss pred: 0.2610; Loss self: 0.0000; time: 0.40s
Val loss: 0.5145 score: 0.8182 time: 0.07s
Test loss: 0.6572 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.2381;  Loss pred: 0.2381; Loss self: 0.0000; time: 0.18s
Val loss: 0.5183 score: 0.8182 time: 0.06s
Test loss: 0.6612 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.2510;  Loss pred: 0.2510; Loss self: 0.0000; time: 0.18s
Val loss: 0.5190 score: 0.8182 time: 0.06s
Test loss: 0.6612 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.2799;  Loss pred: 0.2799; Loss self: 0.0000; time: 0.17s
Val loss: 0.5203 score: 0.8182 time: 0.06s
Test loss: 0.6626 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.2604;  Loss pred: 0.2604; Loss self: 0.0000; time: 0.18s
Val loss: 0.5215 score: 0.8182 time: 0.19s
Test loss: 0.6646 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.2445;  Loss pred: 0.2445; Loss self: 0.0000; time: 0.18s
Val loss: 0.5193 score: 0.8182 time: 0.07s
Test loss: 0.6636 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.2554;  Loss pred: 0.2554; Loss self: 0.0000; time: 0.17s
Val loss: 0.5162 score: 0.8182 time: 0.06s
Test loss: 0.6628 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.2581;  Loss pred: 0.2581; Loss self: 0.0000; time: 0.37s
Val loss: 0.5134 score: 0.8182 time: 0.08s
Test loss: 0.6601 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.2530;  Loss pred: 0.2530; Loss self: 0.0000; time: 0.17s
Val loss: 0.5093 score: 0.8182 time: 0.06s
Test loss: 0.6540 score: 0.6744 time: 0.05s
Epoch 76/1000, LR 0.000282
Train loss: 0.2501;  Loss pred: 0.2501; Loss self: 0.0000; time: 0.17s
Val loss: 0.5075 score: 0.8182 time: 0.07s
Test loss: 0.6522 score: 0.6744 time: 0.07s
Epoch 77/1000, LR 0.000282
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.19s
Val loss: 0.5051 score: 0.8182 time: 0.07s
Test loss: 0.6498 score: 0.6744 time: 0.06s
Epoch 78/1000, LR 0.000282
Train loss: 0.2256;  Loss pred: 0.2256; Loss self: 0.0000; time: 0.19s
Val loss: 0.5019 score: 0.8182 time: 0.07s
Test loss: 0.6462 score: 0.6744 time: 0.06s
Epoch 79/1000, LR 0.000282
Train loss: 0.2383;  Loss pred: 0.2383; Loss self: 0.0000; time: 0.29s
Val loss: 0.5015 score: 0.8182 time: 0.07s
Test loss: 0.6465 score: 0.6744 time: 0.07s
Epoch 80/1000, LR 0.000282
Train loss: 0.2189;  Loss pred: 0.2189; Loss self: 0.0000; time: 0.20s
Val loss: 0.5012 score: 0.8182 time: 0.08s
Test loss: 0.6466 score: 0.6744 time: 0.06s
Epoch 81/1000, LR 0.000281
Train loss: 0.2263;  Loss pred: 0.2263; Loss self: 0.0000; time: 0.19s
Val loss: 0.5029 score: 0.8182 time: 0.08s
Test loss: 0.6490 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.2129;  Loss pred: 0.2129; Loss self: 0.0000; time: 0.20s
Val loss: 0.5036 score: 0.8182 time: 0.28s
Test loss: 0.6507 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.2218;  Loss pred: 0.2218; Loss self: 0.0000; time: 0.19s
Val loss: 0.5060 score: 0.8182 time: 0.07s
Test loss: 0.6529 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2217;  Loss pred: 0.2217; Loss self: 0.0000; time: 0.19s
Val loss: 0.5093 score: 0.8182 time: 0.17s
Test loss: 0.6561 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.2203;  Loss pred: 0.2203; Loss self: 0.0000; time: 0.18s
Val loss: 0.5118 score: 0.8182 time: 0.07s
Test loss: 0.6584 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.2151;  Loss pred: 0.2151; Loss self: 0.0000; time: 0.29s
Val loss: 0.5116 score: 0.8182 time: 0.07s
Test loss: 0.6566 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.2245;  Loss pred: 0.2245; Loss self: 0.0000; time: 0.18s
Val loss: 0.5114 score: 0.8182 time: 0.07s
Test loss: 0.6555 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.2243;  Loss pred: 0.2243; Loss self: 0.0000; time: 0.24s
Val loss: 0.5103 score: 0.8182 time: 0.07s
Test loss: 0.6540 score: 0.6744 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.2078;  Loss pred: 0.2078; Loss self: 0.0000; time: 0.22s
Val loss: 0.5086 score: 0.8182 time: 0.28s
Test loss: 0.6509 score: 0.6744 time: 0.15s
     INFO: Early stopping counter 9 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.2011;  Loss pred: 0.2011; Loss self: 0.0000; time: 0.50s
Val loss: 0.5077 score: 0.8182 time: 0.11s
Test loss: 0.6502 score: 0.6744 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.1920;  Loss pred: 0.1920; Loss self: 0.0000; time: 0.24s
Val loss: 0.5056 score: 0.8182 time: 0.09s
Test loss: 0.6483 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.2042;  Loss pred: 0.2042; Loss self: 0.0000; time: 0.23s
Val loss: 0.5068 score: 0.8182 time: 0.08s
Test loss: 0.6501 score: 0.6744 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1966;  Loss pred: 0.1966; Loss self: 0.0000; time: 0.22s
Val loss: 0.5063 score: 0.8182 time: 0.09s
Test loss: 0.6510 score: 0.6744 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.2008;  Loss pred: 0.2008; Loss self: 0.0000; time: 0.27s
Val loss: 0.5054 score: 0.8182 time: 0.07s
Test loss: 0.6504 score: 0.6744 time: 0.30s
     INFO: Early stopping counter 14 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1828;  Loss pred: 0.1828; Loss self: 0.0000; time: 0.34s
Val loss: 0.5057 score: 0.8182 time: 0.09s
Test loss: 0.6503 score: 0.6977 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.2108;  Loss pred: 0.2108; Loss self: 0.0000; time: 0.21s
Val loss: 0.5082 score: 0.8182 time: 0.10s
Test loss: 0.6524 score: 0.6977 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.1858;  Loss pred: 0.1858; Loss self: 0.0000; time: 0.23s
Val loss: 0.5102 score: 0.8182 time: 0.08s
Test loss: 0.6538 score: 0.6977 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.1892;  Loss pred: 0.1892; Loss self: 0.0000; time: 0.20s
Val loss: 0.5091 score: 0.8182 time: 0.08s
Test loss: 0.6532 score: 0.6977 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 99/1000, LR 0.000279
Train loss: 0.1892;  Loss pred: 0.1892; Loss self: 0.0000; time: 0.19s
Val loss: 0.5088 score: 0.8182 time: 0.09s
Test loss: 0.6529 score: 0.6977 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 0.22s
Val loss: 0.5063 score: 0.8182 time: 0.09s
Test loss: 0.6514 score: 0.6977 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 079,   Train_Loss: 0.2189,   Val_Loss: 0.5012,   Val_Precision: 1.0000,   Val_Recall: 0.6364,   Val_accuracy: 0.7778,   Val_Score: 0.8182,   Val_Loss: 0.5012,   Test_Precision: 1.0000,   Test_Recall: 0.3636,   Test_accuracy: 0.5333,   Test_Score: 0.6744,   Test_loss: 0.6466


[0.06350060299973848, 0.06373522700050671, 0.07073915399996622, 0.06606672099951538, 0.06469852999998693, 0.06363127500026167, 0.062068326999906276, 0.06356700800006365, 0.0658538640000188, 0.06851071199980652, 0.06652169700009836, 0.06886783199934143, 0.06842447800045193, 0.06421562400009861, 0.07551755200074695, 0.061776715000632976, 0.06244954199974018, 0.06524099100079184, 0.07655509899996105, 0.0658303509999314, 0.06636043499929656, 0.06723987800069153, 0.06994144099917321, 0.06739384299999074, 0.06884786299997359, 0.06819067700052983, 0.06794365099995048, 0.06782897799985221, 0.19556002100034675, 0.06712780299949372, 0.2582174090002809, 0.06798611999965942, 0.06821748599941202, 0.06824019399937242, 0.2014430600002015, 0.06769217299915908, 0.06511741099984647, 0.06403170800058433, 0.06244684400007827, 0.06116209399988293, 0.06185214699962671, 0.06329495800036966, 0.20426789399971312, 0.06495283799995377, 0.06463266099945031, 0.06729733599968313, 0.06661839099979261, 0.06791740999960894, 0.17394717000024684, 0.07020363400079077, 0.06224347500028671, 0.0617642919996797, 0.06267032300002029, 0.06280726000022696, 0.0634187280002152, 0.06321506699987367, 0.06216300899995986, 0.06183282399979362, 0.061743704000036814, 0.06394455600002402, 0.0632358770008068, 0.06161817300016992, 0.06198944300012954, 0.06991618299980473, 0.06651037999927212, 0.0610227790002682, 0.0633783149996816, 0.059931950999271066, 0.060488209999675746, 0.32696698200015817, 0.06912919900059933, 0.06957099400005973, 0.06691877899993415, 0.06816951599921595, 0.071528939000018, 0.06328540999948018, 0.06492626700037363, 0.3450811650000105, 0.06513603200073703, 0.07806837999942218, 0.06495670599997538, 0.06049580099988816, 0.06201394099934987, 0.06784448200050974, 0.06975755699932051, 0.29286755500015715, 0.0688028190006662, 0.06177164099972288, 0.06794669099963357, 0.06324412799949641, 0.06341912599964417, 0.06104711399984808, 0.06364352099990356, 0.0631757959999959, 0.062210632999267546, 0.06160685399936483, 0.06147956899985729, 0.05983518700031709, 0.06256287199994404, 0.06136330400022416, 0.06226615500054322, 0.06412958500004606, 0.06285159700018994, 0.06311834700045438, 0.0625745850002204, 0.06311157599975559, 0.06302812200010521, 0.06895593499939423, 0.06557479799994326, 0.06818578200000047, 0.06407179300003918, 0.0680360809992635, 0.0691134789994976, 0.07238891199995123, 0.06952343899956759, 0.06825794199994561, 0.0662621860001309, 0.06543172000056074, 0.06647999199958576, 0.0667790839997906, 0.06386670599931676, 0.1934934600003544, 0.06623523899997963, 0.06694045800031745, 0.06738868899992667, 0.061274583999875176, 0.06104577599944605, 0.061762522000208264, 0.06124599900067551, 0.06070841900054802, 0.06580629099971702, 0.06793537199973798, 0.07109902799948031, 0.06814388600014354, 0.06034620000082214, 0.061514279000220995, 0.06503680700006953, 0.06738740599939774, 0.06212860100004036, 0.06252250699981232, 0.06020272999921872, 0.06043923600009293, 0.06170038000072964, 0.06748890900053084, 0.06565391599997383, 0.0670236260002639, 0.06738802699965163, 0.0663078219995441, 0.2866026100000454, 0.06763153700012481, 0.06380279699988023, 0.06170751799982099, 0.0631392949999281, 0.061892814000202634, 0.06729150400042272, 0.06896494499960681, 0.07280482900023344, 0.06901519800067035, 0.06899085299937724, 0.0674443389998487, 0.06824306500038801, 0.06868973200016626, 0.06905440199989243, 0.0666594439999244, 0.06847605799976009, 0.26471962799951143, 0.06865426500007743, 0.06697241499932716, 0.06564353199973993, 0.06546640200031106, 0.06769317300040711, 0.067694979999942, 0.06735809899964806, 0.06603790600001958, 0.0651524969998718, 0.06487727700005053, 0.06441163300041808, 0.06699942599971109, 0.06519279100029962, 0.06489685300039127, 0.26541176000046107, 0.06678862900025706, 0.06516621600076178, 0.06503236899970943, 0.06482019899976876, 0.06478466799944727, 0.06575518000045122, 0.059186753000176395, 0.06008414099960646, 0.06061053199937305, 0.06012160899990704, 0.0639704619998156, 0.06555788000059692, 0.0654392269998425, 0.06554908700036322, 0.06283538699972269, 0.06079457300074864, 0.06604558599974553, 0.06615381400024489, 0.06522387800032448, 0.06449269600034313, 0.0641019229997255, 0.06424544199944648, 0.06512294499952986, 0.28685643100016023, 0.06622143799995683, 0.0668290679996062, 0.06596541199996864, 0.0658539189998919, 0.06673868899997615, 0.06581860099959158, 0.06031796499974007, 0.0604634849996728, 0.06609640299939201, 0.06556461099989974, 0.06491379900035099, 0.061017769000500266, 0.058422424999662326, 0.06044197899973369, 0.07645124600003328, 0.06302439999944909, 0.18280944500020269, 0.061585634000039136, 0.06150392400013516, 0.06647463700028311, 0.062337504000424815, 0.06077699900015432, 0.06331354800022382, 0.1918663389997164, 0.06758570400052122, 0.06578674199954548, 0.06803769599991938, 0.06857279599989852, 0.06978991600044537, 0.20362328800001706, 0.06184811200000695, 0.0668049340001744, 0.0683256150005036, 0.06805200500002684, 0.06589992899989738, 0.06766682299985405, 0.06874118399991858, 0.06680323599994153, 0.06809698500001105, 0.0693087680001554, 0.0686166499999672, 0.08456416300032288, 0.1767682399995465, 0.06368135499997152, 0.06735585799924593, 0.06596119299956626, 0.06887563399959618, 0.06869848699989234, 0.06564882299971941, 0.06559135699990293, 0.06747929599987401, 0.06455229699986376, 0.06264930900033505, 0.06351718099995196, 0.060732633999577956, 0.061178977999588824, 0.06694362199959869, 0.060285233999820775, 0.06263337699965632, 0.07179830799941556, 0.059079753999867535, 0.07315766400006396, 0.0680984009995882, 0.06841575999987981, 0.0774685149999641, 0.06802117000006547, 0.07209002699983103, 0.0713979610000024, 0.06647994200011453, 0.07512810299976991, 0.06436836699958803, 0.06226407099984499, 0.07045170099991083, 0.06644873799996276, 0.16070369999943068, 0.08546210599979531, 0.07449653600087913, 0.07898156999999628, 0.08194162200015853, 0.30429256000024907, 0.08206915499977185, 0.07182921200001147, 0.07045482399917091, 0.09102877700024692, 0.07590608099962992, 0.07389263000004576]
[0.001443195522721329, 0.0014485278863751525, 0.0016077080454537777, 0.0015015163863526223, 0.0014704211363633394, 0.0014461653409150379, 0.0014106437954524154, 0.0014447047272741738, 0.0014966787272731547, 0.001557061636359239, 0.0015118567500022355, 0.0015651779999850326, 0.0015551017727375438, 0.0014594460000022411, 0.001716308000016976, 0.0014040162500143858, 0.0014193077727213677, 0.0014827497954725418, 0.0017398886136354783, 0.0014961443409075318, 0.0015081917045294672, 0.001528179045470262, 0.001589578204526664, 0.0015316782499997896, 0.0015647241590903088, 0.0015497881136484052, 0.0015441738863625108, 0.0015415676818148231, 0.004444545931826063, 0.00152563188635213, 0.005868577477279111, 0.0015451390909013505, 0.001550397409077546, 0.001550913499985737, 0.004578251363640943, 0.0015384584772536155, 0.0014799411590874197, 0.0014552660909223712, 0.0014192464545472335, 0.00139004759090643, 0.0014057306136278798, 0.0014385217727356742, 0.004642452136357117, 0.001476200863635313, 0.0014689241136238707, 0.0015294849090837074, 0.0015140543409043776, 0.001543577499991112, 0.003953344772732883, 0.0015955371363816084, 0.001414624431824698, 0.0014037339090836294, 0.0014243255227277339, 0.0014274377272778854, 0.0014413347272776182, 0.0014367060681789471, 0.0014127956590899967, 0.001405291454540764, 0.0014032660000008366, 0.0014532853636369096, 0.001437179022745609, 0.0014004130227311346, 0.0014088509772756713, 0.0015890041590864712, 0.0015115995454380027, 0.0013868813409151865, 0.0014404162499927638, 0.0013620897954379789, 0.001374732045447176, 0.0074310677727308675, 0.0015711181591045302, 0.001581158954546812, 0.0015208813409075942, 0.0015493071818003625, 0.0016256577045458637, 0.0014383047727154587, 0.0014755969772812189, 0.007842753750000238, 0.0014803643636531144, 0.0017742813636232313, 0.0014762887727267132, 0.0013749045681792763, 0.0014094077499852244, 0.0015419200454661304, 0.00158539902271183, 0.006656080795458117, 0.001563700431833323, 0.0014039009318118835, 0.0015442429772643993, 0.0014373665454431002, 0.0014413437727191856, 0.0013874344090874563, 0.0014464436590887171, 0.0014358135454544522, 0.001413878022710626, 0.001400155772712837, 0.0013972629318149384, 0.0013598906136435703, 0.0014218834545441825, 0.00139462054545964, 0.0014151398863759823, 0.001457490568182865, 0.0014284453863679532, 0.0014345078863739632, 0.001422149659095918, 0.001434353999994445, 0.0014324573181842095, 0.0015671803408953235, 0.0014903363181805287, 0.0015496768636363743, 0.001456177113637254, 0.0015462745681650797, 0.0015707608863522182, 0.001645202545453437, 0.0015800781590810816, 0.0015513168636351277, 0.0015059587727302476, 0.0014870845454672897, 0.0015109089090814946, 0.0015177064545406954, 0.0014515160454390173, 0.004397578636371691, 0.001505346340908628, 0.0015213740454617603, 0.001531561113634697, 0.0013926041818153449, 0.0013874039999874103, 0.001403693681822915, 0.0013919545227426252, 0.0013797367954670005, 0.0014955975227208414, 0.0015439857272667723, 0.001615886999988189, 0.0015487246818214442, 0.0013715045454732303, 0.001398051795459568, 0.0014781092500015802, 0.0015315319545317668, 0.0014120136590918264, 0.0014209660681775529, 0.0013682438636186073, 0.0013736190000021122, 0.0014022813636529463, 0.0015338388409211555, 0.0014921344545448599, 0.001523264227278725, 0.0015315460681739007, 0.0015069959545350932, 0.006513695681819213, 0.001537080386366473, 0.0014500635681790961, 0.0014024435909050226, 0.0014349839772710932, 0.0014066548636409689, 0.0015293523636459709, 0.0015673851136274277, 0.00165465520455076, 0.0015685272272879627, 0.001567973931804028, 0.0015328258863601977, 0.0015509787500088185, 0.0015611302727310513, 0.0015694182272702826, 0.0015149873636346456, 0.001556274045449093, 0.006016355181807078, 0.0015603242045472143, 0.001522100340893799, 0.001491898454539544, 0.0014878727727343423, 0.0015384812045547071, 0.0015385222727259547, 0.0015308658863556377, 0.001500861500000445, 0.0014807385681789046, 0.0014744835681829666, 0.0014639007500095017, 0.0015227142272661613, 0.0014816543409159005, 0.00147492847728162, 0.006032085454555933, 0.0015179233863694787, 0.0014810503636536769, 0.0014780083863570326, 0.0014731863409038356, 0.001472378818169256, 0.001494435909101164, 0.0013451534772767363, 0.001365548659081965, 0.001377512090894842, 0.0013664002045433417, 0.0014876851627864092, 0.0015246018604789982, 0.0015218424883684304, 0.0015243973721014701, 0.0014612880697609928, 0.0014138272790871777, 0.0015359438604591984, 0.0015384607907033695, 0.0015168343721005692, 0.0014998301395428635, 0.0014907423953424536, 0.0014940800464987554, 0.0015144870930123224, 0.006671079790701401, 0.0015400334418594612, 0.001554164372083865, 0.00153407934883648, 0.0015314864883695792, 0.0015520625348831663, 0.0015306651395253857, 0.0014027433720869783, 0.0014061275581319255, 0.0015371256511486514, 0.0015247583953465057, 0.001509623232566302, 0.0014190178837325644, 0.001358661046503775, 0.001405627418598458, 0.001777935953489146, 0.0014656837209174206, 0.004251382441865179, 0.0014322240465125381, 0.0014303238139566315, 0.0015459217907042584, 0.0014497093953587167, 0.0014134185813989377, 0.001472408093028461, 0.004462007883714335, 0.0015717605581516561, 0.0015299242325475693, 0.001582271999998125, 0.0015947161860441516, 0.001623021302335939, 0.004735425302325978, 0.0014383281860466733, 0.0015536031162831255, 0.0015889677907093861, 0.0015826047674424846, 0.0015325564883697067, 0.0015736470465082337, 0.001598632186044618, 0.001553563627905617, 0.0015836508139537451, 0.0016118318139571022, 0.001595736046510865, 0.001966608441867974, 0.0041108893023150345, 0.0014809617441853841, 0.0015664153023080447, 0.0015339812325480527, 0.001601758930223167, 0.0015976392325556359, 0.001526716813946963, 0.0015253803953465797, 0.0015692859534854422, 0.001501216209299157, 0.0014569606744263967, 0.0014771437441849294, 0.0014123868371994873, 0.0014227669302229959, 0.0015568284185953185, 0.0014019821860423436, 0.001456590162782705, 0.001669728093009664, 0.0013739477674387798, 0.0017013410232573013, 0.0015836837441764696, 0.0015910641860437166, 0.0018015933720921884, 0.001581887674420127, 0.0016765122558100238, 0.0016604176976744744, 0.0015460451627933613, 0.0017471651860411607, 0.0014969387674322798, 0.0014480016511591858, 0.001638411651160717, 0.001545319488371227, 0.0037372953488239696, 0.001987490837204542, 0.0017324775814157938, 0.001836780697674332, 0.0019056191162827565, 0.00707657116279649, 0.0019085849999946943, 0.001670446790697941, 0.0016384842790504864, 0.0021169483023313236, 0.0017652576976658122, 0.0017184332558150175]
[692.9068059429485, 690.3560569361459, 622.0034805621369, 665.9933978004259, 680.0772753261765, 691.4838654391109, 708.8961814625105, 692.1829638411791, 668.1460635322392, 642.2353339449204, 661.4383274066947, 638.9049680033598, 643.0447302748789, 685.1915041724492, 582.6460052566957, 712.242468696323, 704.5688181377385, 674.4226187408153, 574.7494363507046, 668.3847090538201, 663.0456837792943, 654.3735846687212, 629.0977047573288, 652.8786316578808, 639.0902793891639, 645.2494964914064, 647.5954611275168, 648.6902987112067, 224.9948623186228, 655.4661114163359, 170.39904540267514, 647.1909266217931, 644.9959179143489, 644.7812853580787, 218.42400527452267, 650.0012933629209, 675.7025398338355, 687.1595553815068, 704.5992588503728, 719.3998295755574, 711.3738509394921, 695.1580566613699, 215.40340549093727, 677.414588105162, 680.7703616036204, 653.8148850380522, 660.478275438039, 647.8456702081742, 252.95036418205345, 626.748182287891, 706.9014061280693, 712.3857260474738, 702.0866958031436, 700.5559548345367, 693.8013641625025, 696.0365952010765, 707.8164443427829, 711.5961580559034, 712.623266009013, 688.0961062577942, 695.8075397521351, 714.0750505517044, 709.7982796830107, 629.3249733058638, 661.5508737204859, 721.0422193293855, 694.2437646097256, 734.1659876971994, 727.4144829254473, 134.57016280615977, 636.4893653638101, 632.4474823510819, 657.5134910940814, 645.4497931378311, 615.1356446093647, 695.262936597257, 677.6918192408443, 127.50623465641382, 675.5093709039895, 563.6084673503622, 677.3742498583083, 727.3232071112067, 709.5178808336223, 648.542058286618, 630.7560340799863, 150.2385609084502, 639.5086805901653, 712.3009731957322, 647.5664870896699, 695.7167628329124, 693.7970100730633, 720.7547927672633, 691.3508132283673, 696.4692617407283, 707.2745908326965, 714.2062472538144, 715.6849131473601, 735.3532629515611, 703.2925214820598, 717.040920740501, 706.643922362255, 686.1107864641319, 700.0617661293003, 697.103173498559, 703.1608759346151, 697.1779630439017, 698.1010793868577, 638.0886576389188, 670.9894859308308, 645.2958184156295, 686.7296502842225, 646.7156742975291, 636.6341361620624, 607.8278949686334, 632.8800852367741, 644.6136333854756, 664.028802187617, 672.4567228191911, 661.8532685785245, 658.8889419348425, 688.9348575527084, 227.39786657347184, 664.2989542170072, 657.3005520785553, 652.9285649116557, 718.0791304938053, 720.7705902599923, 712.4061417027567, 718.4142755107106, 724.7759161641618, 668.6290828970928, 647.6743808831974, 618.8551550989081, 645.6925570682499, 729.1262747182185, 715.2810813216543, 676.5399783533801, 652.9409961320256, 708.2084465409323, 703.7465724164272, 730.8638661497745, 728.0039079238584, 713.1236468799653, 651.958975950462, 670.1808921804076, 656.4849236868616, 652.934979090981, 663.5717879604388, 153.52267727077933, 650.5840611003532, 689.6249391712811, 713.0411565107454, 696.8718925361791, 710.9064389907356, 653.871549664332, 638.0052938525631, 604.3555160312088, 637.5407341376116, 637.7657049753708, 652.389817329201, 644.7541592651183, 640.5615325430808, 637.1787855040515, 660.0715121483746, 642.5603529945337, 166.21359108317122, 640.8924485601932, 656.986910214333, 670.2869065633811, 672.1004768184898, 649.9916911818475, 649.9743407862398, 653.225085824199, 666.2839975572053, 675.338659699974, 678.203556538998, 683.106419607688, 656.7220441588519, 674.9212501087395, 677.9989778508168, 165.78014478304792, 658.7947777731843, 675.1964852383887, 676.5861474336971, 678.8007546869296, 679.1730413803372, 669.14880317715, 743.4095936952119, 732.3063834812616, 725.9464411309759, 731.8500075416816, 672.1852344934441, 655.9089464090125, 657.098226421646, 655.9969324936857, 684.3277658207114, 707.2999755993103, 651.0654625755865, 650.00031592798, 659.2677608005167, 666.742168753051, 670.8067088749293, 669.3081822111283, 660.2895492565705, 149.90077039610097, 649.33654868727, 643.432585357219, 651.8567639662501, 652.9603803848117, 644.3039359076317, 653.3107563356873, 712.8887720297809, 711.1730327855348, 650.5649029100045, 655.8416094326519, 662.4169384966592, 704.7127534218345, 736.0187462305533, 711.4260769024366, 562.4499566688721, 682.275436186237, 235.21760596096294, 698.2147817130968, 699.142383173885, 646.8632540229872, 689.7934187372495, 707.5044952431892, 679.1595378582795, 224.11435077240705, 636.2292238558081, 653.6271396491574, 632.0025886833521, 627.0708284968231, 616.1348582182789, 211.17427393666483, 695.2516189984129, 643.6650322847074, 629.339377328445, 631.8697002385608, 652.5044966295329, 635.4665121501677, 625.5347594834988, 643.6813929199124, 631.452332287443, 620.412124479021, 626.6700574863471, 508.4896305286652, 243.2563677734774, 675.2368884113605, 638.4003006907194, 651.8984579354523, 624.3136723830683, 625.9235374436614, 655.0003188965594, 655.5741787757744, 637.2324927645998, 666.1265671164382, 686.360323619372, 676.982185340255, 708.0213250803318, 702.8558077627416, 642.3315428056428, 713.275824725634, 686.5349125313161, 598.8999072283156, 727.8297062661502, 587.7716379785228, 631.4392022253214, 628.5101561405668, 555.0642089889025, 632.1561360964331, 596.4764030411694, 602.2580952976872, 646.8116353038623, 572.3557268593844, 668.0299967882548, 690.6069473052453, 610.3472221352672, 647.1153748627114, 267.5731797099403, 503.1469737020404, 577.2080462840925, 544.4308083518981, 524.7638373562683, 141.31137481627815, 523.9483701290642, 598.6422348611194, 610.3201677220286, 472.3780920387776, 566.4895280288498, 581.9254234146676]
Elapsed: 0.07727230736421715~0.04495996083835451
Time per graph: 0.0017703152090815422~0.0010288729947579422
Speed: 632.9888836096354~123.87518396576866
Total Time: 0.0754
best val loss: 0.5011792860247872 test_score: 0.6744

Testing...
Test loss: 0.6681 score: 0.6279 time: 0.12s
test Score 0.6279
Epoch Time List: [0.5700324429999455, 0.4866280430005645, 0.30414171000120405, 0.2897876550014189, 0.2873983439994845, 0.2875784620009654, 0.27489089600112493, 0.28859704000115016, 0.3065372729988667, 0.29033887599962327, 0.49570806399970024, 0.2980611189996125, 0.3043050509995737, 0.4414936459997989, 0.288208357998883, 0.27317378200041276, 0.5084014069998375, 0.2916986829995949, 0.28853621700000076, 0.2945799330009322, 0.41461351300040405, 0.2958135429998947, 0.5077828389994465, 0.3043055819998699, 0.3001781340008165, 0.3092997350004225, 0.3077048189998095, 0.30293055699985416, 0.42937507199894753, 0.2954339980005898, 0.4868299999998271, 0.2988134750003155, 0.30390727799931483, 0.30285702499986655, 0.43846086099893, 0.2950884820002102, 0.3172638840005675, 0.2936941880006998, 0.2910829920010656, 0.28717959799996606, 0.2913839510001708, 0.47304545900078665, 0.4332024059995092, 0.30994950600052107, 0.30053262999899744, 0.30754917200010823, 0.2934067980004329, 0.2963256270004422, 0.6340499089992591, 0.3565279339991321, 0.28431488200021704, 0.28350378299910517, 0.2841179609995379, 0.2892710390005959, 0.28752186500059906, 0.4158761329990739, 0.47978541500015126, 0.27314411900078994, 0.2805995589988015, 0.31251994899866986, 0.2751492830002462, 0.27412828100023034, 0.2733636800003296, 0.421865343000718, 0.5032315789994755, 0.28205012700072984, 0.27598566399956326, 0.270181799999591, 0.2666669420004837, 0.543753519999882, 0.3023085949998858, 0.3028060810001989, 0.2998941859987099, 0.2995304500000202, 0.3065309410003465, 0.28818615099953604, 0.28258505999929184, 0.6976713960002598, 0.3020038149998072, 0.291246005001085, 0.3013309609996213, 0.281013604000691, 0.2707161369999085, 0.2912344149999626, 0.3037727659993834, 0.5308232469997165, 0.31773888699990493, 0.2856947089994719, 0.406455036000807, 0.2902667769994878, 0.27539824699942983, 0.27156221399945935, 0.2713888410007712, 0.633021173999623, 0.2720852319998812, 0.2727071049994265, 0.2758389190003072, 0.28202960199996596, 0.2777938469998844, 0.27482530900033453, 0.27364117500019347, 0.4981661340016217, 0.2921308360000694, 0.2852605140005835, 0.28021839299981366, 0.276468054000361, 0.2745185190005941, 0.2903616269995837, 0.41322249899985763, 0.37979863099917566, 0.3074161060012557, 0.3113480879992494, 0.31183114599934925, 0.3180695180008115, 0.319006810999781, 0.3761736230007955, 0.3076056920008341, 0.4872973639994598, 0.2942673569996259, 0.2911014390001583, 0.27777745300045353, 0.40543582099871855, 0.2972243840004012, 0.2958638879999853, 0.29811787200014805, 0.5062063140003374, 0.27157074599927, 0.2808315499996752, 0.27175447100034944, 0.2666125389987428, 0.3981828839996524, 0.42980992999946466, 0.3239768240000558, 0.5493599060009728, 0.28840962200047215, 0.26634609699885914, 0.4452717220001432, 0.42240187599963974, 0.29156495400002314, 0.27971969500049454, 0.5040559949993622, 0.2679832069998156, 0.2727228260000629, 0.28302529500069795, 0.29343686799984425, 0.28970173000016075, 0.2936274349995074, 0.29234915300003195, 0.5099962809990757, 0.30915104799896653, 0.28916843200022413, 0.28845488900060445, 0.2879303419995267, 0.2753992190000645, 0.29651099999955477, 0.30014128099992377, 0.3171970000003057, 0.4620925070003068, 0.3031702759990367, 0.3005791859995952, 0.2985902150003312, 0.30386245299996517, 0.3076246889995673, 0.2991574600009699, 0.298300516999916, 0.49704628200015577, 0.29868907899981423, 0.299690205000843, 0.29501909600003273, 0.2911140180003713, 0.2941526399990835, 0.298582204000013, 0.2969782109994412, 0.48930144700079836, 0.28815188400039915, 0.2883717219992832, 0.28689890400073637, 0.29165983099937876, 0.28901122200022655, 0.28838628600078664, 0.490149829000984, 0.2939032670001325, 0.2896611969999867, 0.2922746989997904, 0.2875826799991046, 0.2892925460000697, 0.2887165910005933, 0.2672805870006414, 0.2659578290003992, 0.47540582200053905, 0.2810636959993644, 0.8695857659995454, 0.30671411999992415, 0.30604987900005654, 0.3072637320001377, 0.2972074419994897, 0.6538482289997773, 0.30335605800064513, 0.3120745859996532, 0.3253839020007945, 0.3193181690012352, 0.3100546010000471, 0.44626248700024007, 0.3043062110000392, 0.5254482480004299, 0.31372097700023005, 0.3183855479992417, 0.3153853200001322, 0.43753393899987714, 0.32299895100095455, 0.2953367759992034, 0.290857238001081, 0.29117158000008203, 0.6587000450008418, 0.33065672200064, 0.3105658200001926, 0.29872012800024095, 0.28205959600018105, 0.2756244579995837, 0.43968942500032426, 0.32172500799970294, 0.6259675450000941, 0.289710383000056, 0.2980106909999449, 0.29608510300022317, 0.31022992900034296, 0.2969187700000475, 0.48055257000032725, 0.4295261040006153, 0.3229038870003933, 0.3190814580002552, 0.3234032069985915, 0.31897699199998897, 0.3320430750009109, 0.6645303659997808, 0.30869206900115387, 0.3001996490002057, 0.3129033530012748, 0.31992983400050434, 0.3312264719997984, 0.3161496900002021, 0.409454143999028, 0.5021023389999755, 0.3190571950008234, 0.32533369599968864, 0.330675436999627, 0.3759601929996279, 0.48115442100061045, 0.36555791900082113, 0.5245346299998346, 0.3227611579995937, 0.3177268990002631, 0.32695252200028335, 0.44206792899967695, 0.31160212900067563, 0.4117193349984518, 0.31718680600079097, 0.5202701470007014, 0.3035992629993416, 0.29906497899992246, 0.2898964860005435, 0.42757612699915626, 0.30236352300016733, 0.2885739900002591, 0.510297888000423, 0.28610564800055727, 0.3047274730006393, 0.32049998300044535, 0.3202511190002042, 0.43653434999941965, 0.34224797200113244, 0.3346979219995774, 0.5472961860004943, 0.3219485359995815, 0.43230777799999487, 0.3069696570000815, 0.41108422499928565, 0.32214168599966797, 0.367175682999914, 0.653302404999522, 0.6894922069996028, 0.4006255990016143, 0.3833845730005123, 0.38857483199990384, 0.6433651610004745, 0.5070092149999255, 0.3772786819999965, 0.36926399799995124, 0.36435477399936644, 0.35874919299931207, 0.3783413629998904]
Total Epoch List: [191, 100]
Total Time List: [0.061039842999889515, 0.07541468199997325]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f84c3670>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7349;  Loss pred: 0.7349; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7393 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7461 score: 0.4884 time: 0.07s
Epoch 2/1000, LR 0.000015
Train loss: 0.7424;  Loss pred: 0.7424; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7357 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7423 score: 0.4884 time: 0.07s
Epoch 3/1000, LR 0.000045
Train loss: 0.7316;  Loss pred: 0.7316; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7282 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7345 score: 0.4884 time: 0.08s
Epoch 4/1000, LR 0.000075
Train loss: 0.7140;  Loss pred: 0.7140; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7184 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7242 score: 0.4884 time: 0.07s
Epoch 5/1000, LR 0.000105
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7059 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7109 score: 0.4884 time: 0.11s
Epoch 6/1000, LR 0.000135
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4884 time: 0.08s
Epoch 7/1000, LR 0.000165
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.4884 time: 0.13s
Epoch 8/1000, LR 0.000195
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.30s
Val loss: 0.6763 score: 0.5455 time: 0.08s
Test loss: 0.6755 score: 0.4884 time: 0.07s
Epoch 9/1000, LR 0.000225
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.27s
Val loss: 0.6724 score: 0.7500 time: 0.08s
Test loss: 0.6692 score: 0.8837 time: 0.18s
Epoch 10/1000, LR 0.000255
Train loss: 0.6158;  Loss pred: 0.6158; Loss self: 0.0000; time: 0.44s
Val loss: 0.6724 score: 0.5682 time: 0.13s
Test loss: 0.6662 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6765 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6669 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6705 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5616;  Loss pred: 0.5616; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6760 score: 0.5116 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5662;  Loss pred: 0.5662; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7001 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5536;  Loss pred: 0.5536; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7076 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5465;  Loss pred: 0.5465; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7152 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7213 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.5116 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5022;  Loss pred: 0.5022; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7270 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5090;  Loss pred: 0.5090; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7322 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7047 score: 0.5116 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5017;  Loss pred: 0.5017; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7355 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7063 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.4985;  Loss pred: 0.4985; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7367 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7055 score: 0.5116 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.4927;  Loss pred: 0.4927; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7370 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7036 score: 0.5116 time: 0.29s
     INFO: Early stopping counter 13 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.4884;  Loss pred: 0.4884; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7348 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6993 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 14 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7336 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.4726;  Loss pred: 0.4726; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7317 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.4770;  Loss pred: 0.4770; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7277 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.5116 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.4891;  Loss pred: 0.4891; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7225 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 18 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.4632;  Loss pred: 0.4632; Loss self: 0.0000; time: 0.28s
Val loss: 0.7168 score: 0.5227 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6673 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 19 of 20
Epoch 29/1000, LR 0.000285
Train loss: 0.4505;  Loss pred: 0.4505; Loss self: 0.0000; time: 0.65s
Val loss: 0.7132 score: 0.5227 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6612 score: 0.5116 time: 0.19s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 0.6500,   Val_Loss: 0.6724,   Val_Precision: 1.0000,   Val_Recall: 0.5000,   Val_accuracy: 0.6667,   Val_Score: 0.7500,   Val_Loss: 0.6724,   Test_Precision: 1.0000,   Test_Recall: 0.7619,   Test_accuracy: 0.8649,   Test_Score: 0.8837,   Test_loss: 0.6692


[0.06350060299973848, 0.06373522700050671, 0.07073915399996622, 0.06606672099951538, 0.06469852999998693, 0.06363127500026167, 0.062068326999906276, 0.06356700800006365, 0.0658538640000188, 0.06851071199980652, 0.06652169700009836, 0.06886783199934143, 0.06842447800045193, 0.06421562400009861, 0.07551755200074695, 0.061776715000632976, 0.06244954199974018, 0.06524099100079184, 0.07655509899996105, 0.0658303509999314, 0.06636043499929656, 0.06723987800069153, 0.06994144099917321, 0.06739384299999074, 0.06884786299997359, 0.06819067700052983, 0.06794365099995048, 0.06782897799985221, 0.19556002100034675, 0.06712780299949372, 0.2582174090002809, 0.06798611999965942, 0.06821748599941202, 0.06824019399937242, 0.2014430600002015, 0.06769217299915908, 0.06511741099984647, 0.06403170800058433, 0.06244684400007827, 0.06116209399988293, 0.06185214699962671, 0.06329495800036966, 0.20426789399971312, 0.06495283799995377, 0.06463266099945031, 0.06729733599968313, 0.06661839099979261, 0.06791740999960894, 0.17394717000024684, 0.07020363400079077, 0.06224347500028671, 0.0617642919996797, 0.06267032300002029, 0.06280726000022696, 0.0634187280002152, 0.06321506699987367, 0.06216300899995986, 0.06183282399979362, 0.061743704000036814, 0.06394455600002402, 0.0632358770008068, 0.06161817300016992, 0.06198944300012954, 0.06991618299980473, 0.06651037999927212, 0.0610227790002682, 0.0633783149996816, 0.059931950999271066, 0.060488209999675746, 0.32696698200015817, 0.06912919900059933, 0.06957099400005973, 0.06691877899993415, 0.06816951599921595, 0.071528939000018, 0.06328540999948018, 0.06492626700037363, 0.3450811650000105, 0.06513603200073703, 0.07806837999942218, 0.06495670599997538, 0.06049580099988816, 0.06201394099934987, 0.06784448200050974, 0.06975755699932051, 0.29286755500015715, 0.0688028190006662, 0.06177164099972288, 0.06794669099963357, 0.06324412799949641, 0.06341912599964417, 0.06104711399984808, 0.06364352099990356, 0.0631757959999959, 0.062210632999267546, 0.06160685399936483, 0.06147956899985729, 0.05983518700031709, 0.06256287199994404, 0.06136330400022416, 0.06226615500054322, 0.06412958500004606, 0.06285159700018994, 0.06311834700045438, 0.0625745850002204, 0.06311157599975559, 0.06302812200010521, 0.06895593499939423, 0.06557479799994326, 0.06818578200000047, 0.06407179300003918, 0.0680360809992635, 0.0691134789994976, 0.07238891199995123, 0.06952343899956759, 0.06825794199994561, 0.0662621860001309, 0.06543172000056074, 0.06647999199958576, 0.0667790839997906, 0.06386670599931676, 0.1934934600003544, 0.06623523899997963, 0.06694045800031745, 0.06738868899992667, 0.061274583999875176, 0.06104577599944605, 0.061762522000208264, 0.06124599900067551, 0.06070841900054802, 0.06580629099971702, 0.06793537199973798, 0.07109902799948031, 0.06814388600014354, 0.06034620000082214, 0.061514279000220995, 0.06503680700006953, 0.06738740599939774, 0.06212860100004036, 0.06252250699981232, 0.06020272999921872, 0.06043923600009293, 0.06170038000072964, 0.06748890900053084, 0.06565391599997383, 0.0670236260002639, 0.06738802699965163, 0.0663078219995441, 0.2866026100000454, 0.06763153700012481, 0.06380279699988023, 0.06170751799982099, 0.0631392949999281, 0.061892814000202634, 0.06729150400042272, 0.06896494499960681, 0.07280482900023344, 0.06901519800067035, 0.06899085299937724, 0.0674443389998487, 0.06824306500038801, 0.06868973200016626, 0.06905440199989243, 0.0666594439999244, 0.06847605799976009, 0.26471962799951143, 0.06865426500007743, 0.06697241499932716, 0.06564353199973993, 0.06546640200031106, 0.06769317300040711, 0.067694979999942, 0.06735809899964806, 0.06603790600001958, 0.0651524969998718, 0.06487727700005053, 0.06441163300041808, 0.06699942599971109, 0.06519279100029962, 0.06489685300039127, 0.26541176000046107, 0.06678862900025706, 0.06516621600076178, 0.06503236899970943, 0.06482019899976876, 0.06478466799944727, 0.06575518000045122, 0.059186753000176395, 0.06008414099960646, 0.06061053199937305, 0.06012160899990704, 0.0639704619998156, 0.06555788000059692, 0.0654392269998425, 0.06554908700036322, 0.06283538699972269, 0.06079457300074864, 0.06604558599974553, 0.06615381400024489, 0.06522387800032448, 0.06449269600034313, 0.0641019229997255, 0.06424544199944648, 0.06512294499952986, 0.28685643100016023, 0.06622143799995683, 0.0668290679996062, 0.06596541199996864, 0.0658539189998919, 0.06673868899997615, 0.06581860099959158, 0.06031796499974007, 0.0604634849996728, 0.06609640299939201, 0.06556461099989974, 0.06491379900035099, 0.061017769000500266, 0.058422424999662326, 0.06044197899973369, 0.07645124600003328, 0.06302439999944909, 0.18280944500020269, 0.061585634000039136, 0.06150392400013516, 0.06647463700028311, 0.062337504000424815, 0.06077699900015432, 0.06331354800022382, 0.1918663389997164, 0.06758570400052122, 0.06578674199954548, 0.06803769599991938, 0.06857279599989852, 0.06978991600044537, 0.20362328800001706, 0.06184811200000695, 0.0668049340001744, 0.0683256150005036, 0.06805200500002684, 0.06589992899989738, 0.06766682299985405, 0.06874118399991858, 0.06680323599994153, 0.06809698500001105, 0.0693087680001554, 0.0686166499999672, 0.08456416300032288, 0.1767682399995465, 0.06368135499997152, 0.06735585799924593, 0.06596119299956626, 0.06887563399959618, 0.06869848699989234, 0.06564882299971941, 0.06559135699990293, 0.06747929599987401, 0.06455229699986376, 0.06264930900033505, 0.06351718099995196, 0.060732633999577956, 0.061178977999588824, 0.06694362199959869, 0.060285233999820775, 0.06263337699965632, 0.07179830799941556, 0.059079753999867535, 0.07315766400006396, 0.0680984009995882, 0.06841575999987981, 0.0774685149999641, 0.06802117000006547, 0.07209002699983103, 0.0713979610000024, 0.06647994200011453, 0.07512810299976991, 0.06436836699958803, 0.06226407099984499, 0.07045170099991083, 0.06644873799996276, 0.16070369999943068, 0.08546210599979531, 0.07449653600087913, 0.07898156999999628, 0.08194162200015853, 0.30429256000024907, 0.08206915499977185, 0.07182921200001147, 0.07045482399917091, 0.09102877700024692, 0.07590608099962992, 0.07389263000004576, 0.07576947100005782, 0.0740987919998588, 0.08553179699993052, 0.0788423239991971, 0.11666030600008526, 0.08241128400004527, 0.13230544999987615, 0.07246172600025602, 0.1890758919998916, 0.09191333200033114, 0.08857850300046266, 0.08631618400067964, 0.07983466099994985, 0.13176870999996027, 0.08912715699989349, 0.08539228399968124, 0.07840928399946279, 0.13183391599977767, 0.07742268499987404, 0.10001550299966766, 0.16123193699968397, 0.29413248599939834, 0.08980317599980481, 0.08782031999999163, 0.08482613100022718, 0.0770853520007222, 0.1036349149999296, 0.12249781300033646, 0.19272506899960717]
[0.001443195522721329, 0.0014485278863751525, 0.0016077080454537777, 0.0015015163863526223, 0.0014704211363633394, 0.0014461653409150379, 0.0014106437954524154, 0.0014447047272741738, 0.0014966787272731547, 0.001557061636359239, 0.0015118567500022355, 0.0015651779999850326, 0.0015551017727375438, 0.0014594460000022411, 0.001716308000016976, 0.0014040162500143858, 0.0014193077727213677, 0.0014827497954725418, 0.0017398886136354783, 0.0014961443409075318, 0.0015081917045294672, 0.001528179045470262, 0.001589578204526664, 0.0015316782499997896, 0.0015647241590903088, 0.0015497881136484052, 0.0015441738863625108, 0.0015415676818148231, 0.004444545931826063, 0.00152563188635213, 0.005868577477279111, 0.0015451390909013505, 0.001550397409077546, 0.001550913499985737, 0.004578251363640943, 0.0015384584772536155, 0.0014799411590874197, 0.0014552660909223712, 0.0014192464545472335, 0.00139004759090643, 0.0014057306136278798, 0.0014385217727356742, 0.004642452136357117, 0.001476200863635313, 0.0014689241136238707, 0.0015294849090837074, 0.0015140543409043776, 0.001543577499991112, 0.003953344772732883, 0.0015955371363816084, 0.001414624431824698, 0.0014037339090836294, 0.0014243255227277339, 0.0014274377272778854, 0.0014413347272776182, 0.0014367060681789471, 0.0014127956590899967, 0.001405291454540764, 0.0014032660000008366, 0.0014532853636369096, 0.001437179022745609, 0.0014004130227311346, 0.0014088509772756713, 0.0015890041590864712, 0.0015115995454380027, 0.0013868813409151865, 0.0014404162499927638, 0.0013620897954379789, 0.001374732045447176, 0.0074310677727308675, 0.0015711181591045302, 0.001581158954546812, 0.0015208813409075942, 0.0015493071818003625, 0.0016256577045458637, 0.0014383047727154587, 0.0014755969772812189, 0.007842753750000238, 0.0014803643636531144, 0.0017742813636232313, 0.0014762887727267132, 0.0013749045681792763, 0.0014094077499852244, 0.0015419200454661304, 0.00158539902271183, 0.006656080795458117, 0.001563700431833323, 0.0014039009318118835, 0.0015442429772643993, 0.0014373665454431002, 0.0014413437727191856, 0.0013874344090874563, 0.0014464436590887171, 0.0014358135454544522, 0.001413878022710626, 0.001400155772712837, 0.0013972629318149384, 0.0013598906136435703, 0.0014218834545441825, 0.00139462054545964, 0.0014151398863759823, 0.001457490568182865, 0.0014284453863679532, 0.0014345078863739632, 0.001422149659095918, 0.001434353999994445, 0.0014324573181842095, 0.0015671803408953235, 0.0014903363181805287, 0.0015496768636363743, 0.001456177113637254, 0.0015462745681650797, 0.0015707608863522182, 0.001645202545453437, 0.0015800781590810816, 0.0015513168636351277, 0.0015059587727302476, 0.0014870845454672897, 0.0015109089090814946, 0.0015177064545406954, 0.0014515160454390173, 0.004397578636371691, 0.001505346340908628, 0.0015213740454617603, 0.001531561113634697, 0.0013926041818153449, 0.0013874039999874103, 0.001403693681822915, 0.0013919545227426252, 0.0013797367954670005, 0.0014955975227208414, 0.0015439857272667723, 0.001615886999988189, 0.0015487246818214442, 0.0013715045454732303, 0.001398051795459568, 0.0014781092500015802, 0.0015315319545317668, 0.0014120136590918264, 0.0014209660681775529, 0.0013682438636186073, 0.0013736190000021122, 0.0014022813636529463, 0.0015338388409211555, 0.0014921344545448599, 0.001523264227278725, 0.0015315460681739007, 0.0015069959545350932, 0.006513695681819213, 0.001537080386366473, 0.0014500635681790961, 0.0014024435909050226, 0.0014349839772710932, 0.0014066548636409689, 0.0015293523636459709, 0.0015673851136274277, 0.00165465520455076, 0.0015685272272879627, 0.001567973931804028, 0.0015328258863601977, 0.0015509787500088185, 0.0015611302727310513, 0.0015694182272702826, 0.0015149873636346456, 0.001556274045449093, 0.006016355181807078, 0.0015603242045472143, 0.001522100340893799, 0.001491898454539544, 0.0014878727727343423, 0.0015384812045547071, 0.0015385222727259547, 0.0015308658863556377, 0.001500861500000445, 0.0014807385681789046, 0.0014744835681829666, 0.0014639007500095017, 0.0015227142272661613, 0.0014816543409159005, 0.00147492847728162, 0.006032085454555933, 0.0015179233863694787, 0.0014810503636536769, 0.0014780083863570326, 0.0014731863409038356, 0.001472378818169256, 0.001494435909101164, 0.0013451534772767363, 0.001365548659081965, 0.001377512090894842, 0.0013664002045433417, 0.0014876851627864092, 0.0015246018604789982, 0.0015218424883684304, 0.0015243973721014701, 0.0014612880697609928, 0.0014138272790871777, 0.0015359438604591984, 0.0015384607907033695, 0.0015168343721005692, 0.0014998301395428635, 0.0014907423953424536, 0.0014940800464987554, 0.0015144870930123224, 0.006671079790701401, 0.0015400334418594612, 0.001554164372083865, 0.00153407934883648, 0.0015314864883695792, 0.0015520625348831663, 0.0015306651395253857, 0.0014027433720869783, 0.0014061275581319255, 0.0015371256511486514, 0.0015247583953465057, 0.001509623232566302, 0.0014190178837325644, 0.001358661046503775, 0.001405627418598458, 0.001777935953489146, 0.0014656837209174206, 0.004251382441865179, 0.0014322240465125381, 0.0014303238139566315, 0.0015459217907042584, 0.0014497093953587167, 0.0014134185813989377, 0.001472408093028461, 0.004462007883714335, 0.0015717605581516561, 0.0015299242325475693, 0.001582271999998125, 0.0015947161860441516, 0.001623021302335939, 0.004735425302325978, 0.0014383281860466733, 0.0015536031162831255, 0.0015889677907093861, 0.0015826047674424846, 0.0015325564883697067, 0.0015736470465082337, 0.001598632186044618, 0.001553563627905617, 0.0015836508139537451, 0.0016118318139571022, 0.001595736046510865, 0.001966608441867974, 0.0041108893023150345, 0.0014809617441853841, 0.0015664153023080447, 0.0015339812325480527, 0.001601758930223167, 0.0015976392325556359, 0.001526716813946963, 0.0015253803953465797, 0.0015692859534854422, 0.001501216209299157, 0.0014569606744263967, 0.0014771437441849294, 0.0014123868371994873, 0.0014227669302229959, 0.0015568284185953185, 0.0014019821860423436, 0.001456590162782705, 0.001669728093009664, 0.0013739477674387798, 0.0017013410232573013, 0.0015836837441764696, 0.0015910641860437166, 0.0018015933720921884, 0.001581887674420127, 0.0016765122558100238, 0.0016604176976744744, 0.0015460451627933613, 0.0017471651860411607, 0.0014969387674322798, 0.0014480016511591858, 0.001638411651160717, 0.001545319488371227, 0.0037372953488239696, 0.001987490837204542, 0.0017324775814157938, 0.001836780697674332, 0.0019056191162827565, 0.00707657116279649, 0.0019085849999946943, 0.001670446790697941, 0.0016384842790504864, 0.0021169483023313236, 0.0017652576976658122, 0.0017184332558150175, 0.0017620807209315773, 0.001723227720926949, 0.001989111558137919, 0.001833542418585979, 0.002713030372095006, 0.0019165414883731459, 0.003076870930229678, 0.001685156418610605, 0.00439711376743934, 0.00213751934884491, 0.002059965186057271, 0.0020073531162948756, 0.0018566200232546476, 0.0030643886046502387, 0.002072724581392872, 0.001985867069760029, 0.0018234717209177393, 0.0030659050232506434, 0.0018005275581366054, 0.0023259419302248292, 0.0037495799302252083, 0.006840290372079031, 0.0020884459534838327, 0.002042333023255619, 0.0019727007209355157, 0.0017926826046679582, 0.002410114302323944, 0.002848786348845034, 0.004481978348828074]
[692.9068059429485, 690.3560569361459, 622.0034805621369, 665.9933978004259, 680.0772753261765, 691.4838654391109, 708.8961814625105, 692.1829638411791, 668.1460635322392, 642.2353339449204, 661.4383274066947, 638.9049680033598, 643.0447302748789, 685.1915041724492, 582.6460052566957, 712.242468696323, 704.5688181377385, 674.4226187408153, 574.7494363507046, 668.3847090538201, 663.0456837792943, 654.3735846687212, 629.0977047573288, 652.8786316578808, 639.0902793891639, 645.2494964914064, 647.5954611275168, 648.6902987112067, 224.9948623186228, 655.4661114163359, 170.39904540267514, 647.1909266217931, 644.9959179143489, 644.7812853580787, 218.42400527452267, 650.0012933629209, 675.7025398338355, 687.1595553815068, 704.5992588503728, 719.3998295755574, 711.3738509394921, 695.1580566613699, 215.40340549093727, 677.414588105162, 680.7703616036204, 653.8148850380522, 660.478275438039, 647.8456702081742, 252.95036418205345, 626.748182287891, 706.9014061280693, 712.3857260474738, 702.0866958031436, 700.5559548345367, 693.8013641625025, 696.0365952010765, 707.8164443427829, 711.5961580559034, 712.623266009013, 688.0961062577942, 695.8075397521351, 714.0750505517044, 709.7982796830107, 629.3249733058638, 661.5508737204859, 721.0422193293855, 694.2437646097256, 734.1659876971994, 727.4144829254473, 134.57016280615977, 636.4893653638101, 632.4474823510819, 657.5134910940814, 645.4497931378311, 615.1356446093647, 695.262936597257, 677.6918192408443, 127.50623465641382, 675.5093709039895, 563.6084673503622, 677.3742498583083, 727.3232071112067, 709.5178808336223, 648.542058286618, 630.7560340799863, 150.2385609084502, 639.5086805901653, 712.3009731957322, 647.5664870896699, 695.7167628329124, 693.7970100730633, 720.7547927672633, 691.3508132283673, 696.4692617407283, 707.2745908326965, 714.2062472538144, 715.6849131473601, 735.3532629515611, 703.2925214820598, 717.040920740501, 706.643922362255, 686.1107864641319, 700.0617661293003, 697.103173498559, 703.1608759346151, 697.1779630439017, 698.1010793868577, 638.0886576389188, 670.9894859308308, 645.2958184156295, 686.7296502842225, 646.7156742975291, 636.6341361620624, 607.8278949686334, 632.8800852367741, 644.6136333854756, 664.028802187617, 672.4567228191911, 661.8532685785245, 658.8889419348425, 688.9348575527084, 227.39786657347184, 664.2989542170072, 657.3005520785553, 652.9285649116557, 718.0791304938053, 720.7705902599923, 712.4061417027567, 718.4142755107106, 724.7759161641618, 668.6290828970928, 647.6743808831974, 618.8551550989081, 645.6925570682499, 729.1262747182185, 715.2810813216543, 676.5399783533801, 652.9409961320256, 708.2084465409323, 703.7465724164272, 730.8638661497745, 728.0039079238584, 713.1236468799653, 651.958975950462, 670.1808921804076, 656.4849236868616, 652.934979090981, 663.5717879604388, 153.52267727077933, 650.5840611003532, 689.6249391712811, 713.0411565107454, 696.8718925361791, 710.9064389907356, 653.871549664332, 638.0052938525631, 604.3555160312088, 637.5407341376116, 637.7657049753708, 652.389817329201, 644.7541592651183, 640.5615325430808, 637.1787855040515, 660.0715121483746, 642.5603529945337, 166.21359108317122, 640.8924485601932, 656.986910214333, 670.2869065633811, 672.1004768184898, 649.9916911818475, 649.9743407862398, 653.225085824199, 666.2839975572053, 675.338659699974, 678.203556538998, 683.106419607688, 656.7220441588519, 674.9212501087395, 677.9989778508168, 165.78014478304792, 658.7947777731843, 675.1964852383887, 676.5861474336971, 678.8007546869296, 679.1730413803372, 669.14880317715, 743.4095936952119, 732.3063834812616, 725.9464411309759, 731.8500075416816, 672.1852344934441, 655.9089464090125, 657.098226421646, 655.9969324936857, 684.3277658207114, 707.2999755993103, 651.0654625755865, 650.00031592798, 659.2677608005167, 666.742168753051, 670.8067088749293, 669.3081822111283, 660.2895492565705, 149.90077039610097, 649.33654868727, 643.432585357219, 651.8567639662501, 652.9603803848117, 644.3039359076317, 653.3107563356873, 712.8887720297809, 711.1730327855348, 650.5649029100045, 655.8416094326519, 662.4169384966592, 704.7127534218345, 736.0187462305533, 711.4260769024366, 562.4499566688721, 682.275436186237, 235.21760596096294, 698.2147817130968, 699.142383173885, 646.8632540229872, 689.7934187372495, 707.5044952431892, 679.1595378582795, 224.11435077240705, 636.2292238558081, 653.6271396491574, 632.0025886833521, 627.0708284968231, 616.1348582182789, 211.17427393666483, 695.2516189984129, 643.6650322847074, 629.339377328445, 631.8697002385608, 652.5044966295329, 635.4665121501677, 625.5347594834988, 643.6813929199124, 631.452332287443, 620.412124479021, 626.6700574863471, 508.4896305286652, 243.2563677734774, 675.2368884113605, 638.4003006907194, 651.8984579354523, 624.3136723830683, 625.9235374436614, 655.0003188965594, 655.5741787757744, 637.2324927645998, 666.1265671164382, 686.360323619372, 676.982185340255, 708.0213250803318, 702.8558077627416, 642.3315428056428, 713.275824725634, 686.5349125313161, 598.8999072283156, 727.8297062661502, 587.7716379785228, 631.4392022253214, 628.5101561405668, 555.0642089889025, 632.1561360964331, 596.4764030411694, 602.2580952976872, 646.8116353038623, 572.3557268593844, 668.0299967882548, 690.6069473052453, 610.3472221352672, 647.1153748627114, 267.5731797099403, 503.1469737020404, 577.2080462840925, 544.4308083518981, 524.7638373562683, 141.31137481627815, 523.9483701290642, 598.6422348611194, 610.3201677220286, 472.3780920387776, 566.4895280288498, 581.9254234146676, 567.5108910284881, 580.3063564124221, 502.73701136005513, 545.3923453656427, 368.5915241810576, 521.7732076589946, 325.0055080878396, 593.416723193263, 227.42190738957166, 467.8320224518145, 485.44509721253024, 498.16845470904303, 538.6131720409887, 326.32936908931543, 482.4567667972556, 503.5583777119782, 548.4044465996477, 326.1679642442884, 555.3927766786951, 429.93334743457604, 266.69654164164933, 146.19262423154427, 478.8249359921687, 489.63611155145077, 506.91926524250925, 557.8232294975722, 414.9180804560819, 351.0266750630225, 223.11575875003393]
Elapsed: 0.08014927469683072~0.04611012326044892
Time per graph: 0.0018396424906421491~0.0010590939655511504
Speed: 615.7167988202419~135.0348513025921
Total Time: 0.1939
best val loss: 0.6723858172243292 test_score: 0.8837

Testing...
Test loss: 0.6692 score: 0.8837 time: 0.13s
test Score 0.8837
Epoch Time List: [0.5700324429999455, 0.4866280430005645, 0.30414171000120405, 0.2897876550014189, 0.2873983439994845, 0.2875784620009654, 0.27489089600112493, 0.28859704000115016, 0.3065372729988667, 0.29033887599962327, 0.49570806399970024, 0.2980611189996125, 0.3043050509995737, 0.4414936459997989, 0.288208357998883, 0.27317378200041276, 0.5084014069998375, 0.2916986829995949, 0.28853621700000076, 0.2945799330009322, 0.41461351300040405, 0.2958135429998947, 0.5077828389994465, 0.3043055819998699, 0.3001781340008165, 0.3092997350004225, 0.3077048189998095, 0.30293055699985416, 0.42937507199894753, 0.2954339980005898, 0.4868299999998271, 0.2988134750003155, 0.30390727799931483, 0.30285702499986655, 0.43846086099893, 0.2950884820002102, 0.3172638840005675, 0.2936941880006998, 0.2910829920010656, 0.28717959799996606, 0.2913839510001708, 0.47304545900078665, 0.4332024059995092, 0.30994950600052107, 0.30053262999899744, 0.30754917200010823, 0.2934067980004329, 0.2963256270004422, 0.6340499089992591, 0.3565279339991321, 0.28431488200021704, 0.28350378299910517, 0.2841179609995379, 0.2892710390005959, 0.28752186500059906, 0.4158761329990739, 0.47978541500015126, 0.27314411900078994, 0.2805995589988015, 0.31251994899866986, 0.2751492830002462, 0.27412828100023034, 0.2733636800003296, 0.421865343000718, 0.5032315789994755, 0.28205012700072984, 0.27598566399956326, 0.270181799999591, 0.2666669420004837, 0.543753519999882, 0.3023085949998858, 0.3028060810001989, 0.2998941859987099, 0.2995304500000202, 0.3065309410003465, 0.28818615099953604, 0.28258505999929184, 0.6976713960002598, 0.3020038149998072, 0.291246005001085, 0.3013309609996213, 0.281013604000691, 0.2707161369999085, 0.2912344149999626, 0.3037727659993834, 0.5308232469997165, 0.31773888699990493, 0.2856947089994719, 0.406455036000807, 0.2902667769994878, 0.27539824699942983, 0.27156221399945935, 0.2713888410007712, 0.633021173999623, 0.2720852319998812, 0.2727071049994265, 0.2758389190003072, 0.28202960199996596, 0.2777938469998844, 0.27482530900033453, 0.27364117500019347, 0.4981661340016217, 0.2921308360000694, 0.2852605140005835, 0.28021839299981366, 0.276468054000361, 0.2745185190005941, 0.2903616269995837, 0.41322249899985763, 0.37979863099917566, 0.3074161060012557, 0.3113480879992494, 0.31183114599934925, 0.3180695180008115, 0.319006810999781, 0.3761736230007955, 0.3076056920008341, 0.4872973639994598, 0.2942673569996259, 0.2911014390001583, 0.27777745300045353, 0.40543582099871855, 0.2972243840004012, 0.2958638879999853, 0.29811787200014805, 0.5062063140003374, 0.27157074599927, 0.2808315499996752, 0.27175447100034944, 0.2666125389987428, 0.3981828839996524, 0.42980992999946466, 0.3239768240000558, 0.5493599060009728, 0.28840962200047215, 0.26634609699885914, 0.4452717220001432, 0.42240187599963974, 0.29156495400002314, 0.27971969500049454, 0.5040559949993622, 0.2679832069998156, 0.2727228260000629, 0.28302529500069795, 0.29343686799984425, 0.28970173000016075, 0.2936274349995074, 0.29234915300003195, 0.5099962809990757, 0.30915104799896653, 0.28916843200022413, 0.28845488900060445, 0.2879303419995267, 0.2753992190000645, 0.29651099999955477, 0.30014128099992377, 0.3171970000003057, 0.4620925070003068, 0.3031702759990367, 0.3005791859995952, 0.2985902150003312, 0.30386245299996517, 0.3076246889995673, 0.2991574600009699, 0.298300516999916, 0.49704628200015577, 0.29868907899981423, 0.299690205000843, 0.29501909600003273, 0.2911140180003713, 0.2941526399990835, 0.298582204000013, 0.2969782109994412, 0.48930144700079836, 0.28815188400039915, 0.2883717219992832, 0.28689890400073637, 0.29165983099937876, 0.28901122200022655, 0.28838628600078664, 0.490149829000984, 0.2939032670001325, 0.2896611969999867, 0.2922746989997904, 0.2875826799991046, 0.2892925460000697, 0.2887165910005933, 0.2672805870006414, 0.2659578290003992, 0.47540582200053905, 0.2810636959993644, 0.8695857659995454, 0.30671411999992415, 0.30604987900005654, 0.3072637320001377, 0.2972074419994897, 0.6538482289997773, 0.30335605800064513, 0.3120745859996532, 0.3253839020007945, 0.3193181690012352, 0.3100546010000471, 0.44626248700024007, 0.3043062110000392, 0.5254482480004299, 0.31372097700023005, 0.3183855479992417, 0.3153853200001322, 0.43753393899987714, 0.32299895100095455, 0.2953367759992034, 0.290857238001081, 0.29117158000008203, 0.6587000450008418, 0.33065672200064, 0.3105658200001926, 0.29872012800024095, 0.28205959600018105, 0.2756244579995837, 0.43968942500032426, 0.32172500799970294, 0.6259675450000941, 0.289710383000056, 0.2980106909999449, 0.29608510300022317, 0.31022992900034296, 0.2969187700000475, 0.48055257000032725, 0.4295261040006153, 0.3229038870003933, 0.3190814580002552, 0.3234032069985915, 0.31897699199998897, 0.3320430750009109, 0.6645303659997808, 0.30869206900115387, 0.3001996490002057, 0.3129033530012748, 0.31992983400050434, 0.3312264719997984, 0.3161496900002021, 0.409454143999028, 0.5021023389999755, 0.3190571950008234, 0.32533369599968864, 0.330675436999627, 0.3759601929996279, 0.48115442100061045, 0.36555791900082113, 0.5245346299998346, 0.3227611579995937, 0.3177268990002631, 0.32695252200028335, 0.44206792899967695, 0.31160212900067563, 0.4117193349984518, 0.31718680600079097, 0.5202701470007014, 0.3035992629993416, 0.29906497899992246, 0.2898964860005435, 0.42757612699915626, 0.30236352300016733, 0.2885739900002591, 0.510297888000423, 0.28610564800055727, 0.3047274730006393, 0.32049998300044535, 0.3202511190002042, 0.43653434999941965, 0.34224797200113244, 0.3346979219995774, 0.5472961860004943, 0.3219485359995815, 0.43230777799999487, 0.3069696570000815, 0.41108422499928565, 0.32214168599966797, 0.367175682999914, 0.653302404999522, 0.6894922069996028, 0.4006255990016143, 0.3833845730005123, 0.38857483199990384, 0.6433651610004745, 0.5070092149999255, 0.3772786819999965, 0.36926399799995124, 0.36435477399936644, 0.35874919299931207, 0.3783413629998904, 0.3423976310004946, 0.333770870999615, 0.5525751570003195, 0.30219384399879345, 0.371706523001194, 0.41812180599936255, 0.5416154039985486, 0.45080186700124614, 0.5281967250002708, 0.6378934559998015, 0.5190679790011927, 0.37542106499950023, 0.42744165000021894, 0.4223373239992725, 0.39023315600115893, 0.4012331809999523, 0.5989723659995434, 0.41135953800039715, 0.4266902620001929, 0.40400007799962623, 0.5224737989992718, 0.6276168519998464, 0.48023559500052215, 0.3785951079998995, 0.36832937199960725, 0.2988299779999579, 0.5324837110001681, 0.4750538830012374, 0.9715520579993608]
Total Epoch List: [191, 100, 29]
Total Time List: [0.061039842999889515, 0.07541468199997325, 0.19388643399997818]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f8513520>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.40s
Val loss: 0.6976 score: 0.4186 time: 0.12s
Test loss: 0.6996 score: 0.3409 time: 0.14s
Epoch 2/1000, LR 0.000015
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.47s
Val loss: 0.6967 score: 0.4651 time: 0.16s
Test loss: 0.6986 score: 0.3409 time: 0.11s
Epoch 3/1000, LR 0.000045
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.32s
Val loss: 0.6954 score: 0.4651 time: 0.32s
Test loss: 0.6969 score: 0.3864 time: 0.12s
Epoch 4/1000, LR 0.000075
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.47s
Val loss: 0.6936 score: 0.5581 time: 0.15s
Test loss: 0.6948 score: 0.5227 time: 0.16s
Epoch 5/1000, LR 0.000105
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.40s
Val loss: 0.6919 score: 0.5581 time: 0.21s
Test loss: 0.6926 score: 0.5455 time: 0.15s
Epoch 6/1000, LR 0.000135
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.4884 time: 0.20s
Test loss: 0.6904 score: 0.5227 time: 0.08s
Epoch 7/1000, LR 0.000165
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.4884 time: 0.14s
Test loss: 0.6882 score: 0.5227 time: 0.13s
Epoch 8/1000, LR 0.000195
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.4884 time: 0.16s
Test loss: 0.6867 score: 0.5227 time: 0.12s
Epoch 9/1000, LR 0.000225
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.4884 time: 0.11s
Test loss: 0.6852 score: 0.5227 time: 0.10s
Epoch 10/1000, LR 0.000255
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.4884 time: 0.08s
Test loss: 0.6843 score: 0.5227 time: 0.37s
Epoch 11/1000, LR 0.000285
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.4884 time: 0.10s
Test loss: 0.6838 score: 0.5227 time: 0.24s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.4884 time: 0.07s
Test loss: 0.6833 score: 0.5227 time: 0.16s
Epoch 13/1000, LR 0.000285
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.4884 time: 0.09s
Test loss: 0.6826 score: 0.5227 time: 0.10s
Epoch 14/1000, LR 0.000285
Train loss: 0.5704;  Loss pred: 0.5704; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6848 score: 0.4884 time: 0.12s
Test loss: 0.6822 score: 0.5227 time: 0.34s
Epoch 15/1000, LR 0.000285
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.4884 time: 0.22s
Test loss: 0.6806 score: 0.5227 time: 0.18s
Epoch 16/1000, LR 0.000285
Train loss: 0.5631;  Loss pred: 0.5631; Loss self: 0.0000; time: 0.62s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6805 score: 0.4884 time: 0.20s
Test loss: 0.6781 score: 0.5227 time: 0.12s
Epoch 17/1000, LR 0.000285
Train loss: 0.5910;  Loss pred: 0.5910; Loss self: 0.0000; time: 0.39s
Val loss: 0.6762 score: 0.5116 time: 0.41s
Test loss: 0.6741 score: 0.5227 time: 0.12s
Epoch 18/1000, LR 0.000285
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.40s
Val loss: 0.6708 score: 0.5349 time: 0.31s
Test loss: 0.6688 score: 0.5227 time: 0.15s
Epoch 19/1000, LR 0.000285
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.35s
Val loss: 0.6653 score: 0.5349 time: 0.10s
Test loss: 0.6633 score: 0.5227 time: 0.11s
Epoch 20/1000, LR 0.000285
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.30s
Val loss: 0.6587 score: 0.5349 time: 0.13s
Test loss: 0.6572 score: 0.5227 time: 0.36s
Epoch 21/1000, LR 0.000285
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.45s
Val loss: 0.6516 score: 0.5349 time: 0.16s
Test loss: 0.6508 score: 0.5455 time: 0.34s
Epoch 22/1000, LR 0.000285
Train loss: 0.5444;  Loss pred: 0.5444; Loss self: 0.0000; time: 0.34s
Val loss: 0.6438 score: 0.5349 time: 0.24s
Test loss: 0.6437 score: 0.5682 time: 0.13s
Epoch 23/1000, LR 0.000285
Train loss: 0.4934;  Loss pred: 0.4934; Loss self: 0.0000; time: 0.57s
Val loss: 0.6347 score: 0.5349 time: 0.13s
Test loss: 0.6355 score: 0.5682 time: 0.14s
Epoch 24/1000, LR 0.000285
Train loss: 0.5114;  Loss pred: 0.5114; Loss self: 0.0000; time: 0.30s
Val loss: 0.6257 score: 0.5581 time: 0.32s
Test loss: 0.6270 score: 0.5682 time: 0.13s
Epoch 25/1000, LR 0.000285
Train loss: 0.4980;  Loss pred: 0.4980; Loss self: 0.0000; time: 0.32s
Val loss: 0.6158 score: 0.5581 time: 0.12s
Test loss: 0.6174 score: 0.5682 time: 0.13s
Epoch 26/1000, LR 0.000285
Train loss: 0.4882;  Loss pred: 0.4882; Loss self: 0.0000; time: 0.32s
Val loss: 0.6067 score: 0.5814 time: 0.11s
Test loss: 0.6083 score: 0.6591 time: 0.13s
Epoch 27/1000, LR 0.000285
Train loss: 0.4592;  Loss pred: 0.4592; Loss self: 0.0000; time: 0.47s
Val loss: 0.5962 score: 0.6047 time: 0.09s
Test loss: 0.5984 score: 0.6591 time: 0.07s
Epoch 28/1000, LR 0.000285
Train loss: 0.4699;  Loss pred: 0.4699; Loss self: 0.0000; time: 0.18s
Val loss: 0.5872 score: 0.6279 time: 0.07s
Test loss: 0.5900 score: 0.6591 time: 0.11s
Epoch 29/1000, LR 0.000285
Train loss: 0.4488;  Loss pred: 0.4488; Loss self: 0.0000; time: 0.25s
Val loss: 0.5783 score: 0.6744 time: 0.27s
Test loss: 0.5815 score: 0.6591 time: 0.10s
Epoch 30/1000, LR 0.000285
Train loss: 0.4537;  Loss pred: 0.4537; Loss self: 0.0000; time: 0.21s
Val loss: 0.5692 score: 0.6744 time: 0.09s
Test loss: 0.5730 score: 0.7045 time: 0.11s
Epoch 31/1000, LR 0.000285
Train loss: 0.4762;  Loss pred: 0.4762; Loss self: 0.0000; time: 0.44s
Val loss: 0.5597 score: 0.6977 time: 0.19s
Test loss: 0.5640 score: 0.7273 time: 0.12s
Epoch 32/1000, LR 0.000285
Train loss: 0.4454;  Loss pred: 0.4454; Loss self: 0.0000; time: 0.49s
Val loss: 0.5523 score: 0.7442 time: 0.33s
Test loss: 0.5571 score: 0.7500 time: 0.12s
Epoch 33/1000, LR 0.000285
Train loss: 0.4455;  Loss pred: 0.4455; Loss self: 0.0000; time: 0.31s
Val loss: 0.5448 score: 0.7674 time: 0.17s
Test loss: 0.5501 score: 0.7727 time: 0.22s
Epoch 34/1000, LR 0.000285
Train loss: 0.4319;  Loss pred: 0.4319; Loss self: 0.0000; time: 0.43s
Val loss: 0.5389 score: 0.7907 time: 0.13s
Test loss: 0.5447 score: 0.7727 time: 0.12s
Epoch 35/1000, LR 0.000285
Train loss: 0.4045;  Loss pred: 0.4045; Loss self: 0.0000; time: 0.36s
Val loss: 0.5336 score: 0.8140 time: 0.47s
Test loss: 0.5396 score: 0.7727 time: 0.13s
Epoch 36/1000, LR 0.000285
Train loss: 0.4151;  Loss pred: 0.4151; Loss self: 0.0000; time: 0.40s
Val loss: 0.5289 score: 0.8140 time: 0.25s
Test loss: 0.5350 score: 0.7955 time: 0.13s
Epoch 37/1000, LR 0.000285
Train loss: 0.4112;  Loss pred: 0.4112; Loss self: 0.0000; time: 0.29s
Val loss: 0.5253 score: 0.8140 time: 0.22s
Test loss: 0.5310 score: 0.7955 time: 0.20s
Epoch 38/1000, LR 0.000284
Train loss: 0.4057;  Loss pred: 0.4057; Loss self: 0.0000; time: 0.29s
Val loss: 0.5214 score: 0.8140 time: 0.13s
Test loss: 0.5262 score: 0.7955 time: 0.13s
Epoch 39/1000, LR 0.000284
Train loss: 0.4027;  Loss pred: 0.4027; Loss self: 0.0000; time: 0.66s
Val loss: 0.5167 score: 0.8140 time: 0.27s
Test loss: 0.5206 score: 0.7955 time: 0.12s
Epoch 40/1000, LR 0.000284
Train loss: 0.3704;  Loss pred: 0.3704; Loss self: 0.0000; time: 0.38s
Val loss: 0.5124 score: 0.8140 time: 0.14s
Test loss: 0.5155 score: 0.7955 time: 0.23s
Epoch 41/1000, LR 0.000284
Train loss: 0.3708;  Loss pred: 0.3708; Loss self: 0.0000; time: 0.41s
Val loss: 0.5071 score: 0.8140 time: 0.14s
Test loss: 0.5093 score: 0.7955 time: 0.13s
Epoch 42/1000, LR 0.000284
Train loss: 0.3664;  Loss pred: 0.3664; Loss self: 0.0000; time: 0.30s
Val loss: 0.5025 score: 0.8140 time: 0.29s
Test loss: 0.5036 score: 0.7955 time: 0.14s
Epoch 43/1000, LR 0.000284
Train loss: 0.4390;  Loss pred: 0.4390; Loss self: 0.0000; time: 0.39s
Val loss: 0.4972 score: 0.8140 time: 0.13s
Test loss: 0.4980 score: 0.7955 time: 0.08s
Epoch 44/1000, LR 0.000284
Train loss: 0.3767;  Loss pred: 0.3767; Loss self: 0.0000; time: 0.36s
Val loss: 0.4913 score: 0.8372 time: 0.10s
Test loss: 0.4920 score: 0.7955 time: 0.09s
Epoch 45/1000, LR 0.000284
Train loss: 0.3585;  Loss pred: 0.3585; Loss self: 0.0000; time: 0.36s
Val loss: 0.4859 score: 0.8372 time: 0.22s
Test loss: 0.4861 score: 0.7955 time: 0.08s
Epoch 46/1000, LR 0.000284
Train loss: 0.3810;  Loss pred: 0.3810; Loss self: 0.0000; time: 0.26s
Val loss: 0.4805 score: 0.8372 time: 0.09s
Test loss: 0.4797 score: 0.7955 time: 0.11s
Epoch 47/1000, LR 0.000284
Train loss: 0.3344;  Loss pred: 0.3344; Loss self: 0.0000; time: 0.26s
Val loss: 0.4755 score: 0.8372 time: 0.39s
Test loss: 0.4739 score: 0.7727 time: 0.14s
Epoch 48/1000, LR 0.000284
Train loss: 0.3525;  Loss pred: 0.3525; Loss self: 0.0000; time: 0.23s
Val loss: 0.4696 score: 0.8372 time: 0.17s
Test loss: 0.4669 score: 0.7955 time: 0.11s
Epoch 49/1000, LR 0.000284
Train loss: 0.3707;  Loss pred: 0.3707; Loss self: 0.0000; time: 0.36s
Val loss: 0.4651 score: 0.8372 time: 0.11s
Test loss: 0.4608 score: 0.7727 time: 0.30s
Epoch 50/1000, LR 0.000284
Train loss: 0.3413;  Loss pred: 0.3413; Loss self: 0.0000; time: 0.48s
Val loss: 0.4605 score: 0.8372 time: 0.11s
Test loss: 0.4543 score: 0.7955 time: 0.12s
Epoch 51/1000, LR 0.000284
Train loss: 0.3580;  Loss pred: 0.3580; Loss self: 0.0000; time: 0.64s
Val loss: 0.4576 score: 0.8372 time: 0.14s
Test loss: 0.4493 score: 0.7955 time: 0.13s
Epoch 52/1000, LR 0.000284
Train loss: 0.3381;  Loss pred: 0.3381; Loss self: 0.0000; time: 0.31s
Val loss: 0.4550 score: 0.8372 time: 0.28s
Test loss: 0.4447 score: 0.7955 time: 0.13s
Epoch 53/1000, LR 0.000284
Train loss: 0.3428;  Loss pred: 0.3428; Loss self: 0.0000; time: 0.36s
Val loss: 0.4524 score: 0.8372 time: 0.13s
Test loss: 0.4398 score: 0.7955 time: 0.13s
Epoch 54/1000, LR 0.000284
Train loss: 0.3085;  Loss pred: 0.3085; Loss self: 0.0000; time: 0.46s
Val loss: 0.4515 score: 0.8140 time: 0.14s
Test loss: 0.4369 score: 0.7955 time: 0.17s
Epoch 55/1000, LR 0.000284
Train loss: 0.3246;  Loss pred: 0.3246; Loss self: 0.0000; time: 0.40s
Val loss: 0.4523 score: 0.8140 time: 0.29s
Test loss: 0.4353 score: 0.7955 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.32s
Val loss: 0.4529 score: 0.8140 time: 0.12s
Test loss: 0.4341 score: 0.7727 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.30s
Val loss: 0.4524 score: 0.8140 time: 0.12s
Test loss: 0.4316 score: 0.7727 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.3289;  Loss pred: 0.3289; Loss self: 0.0000; time: 0.63s
Val loss: 0.4508 score: 0.8372 time: 0.14s
Test loss: 0.4289 score: 0.7955 time: 0.13s
Epoch 59/1000, LR 0.000283
Train loss: 0.2936;  Loss pred: 0.2936; Loss self: 0.0000; time: 0.39s
Val loss: 0.4480 score: 0.8372 time: 0.16s
Test loss: 0.4247 score: 0.7955 time: 0.14s
Epoch 60/1000, LR 0.000283
Train loss: 0.3199;  Loss pred: 0.3199; Loss self: 0.0000; time: 0.48s
Val loss: 0.4448 score: 0.8372 time: 0.20s
Test loss: 0.4207 score: 0.7955 time: 0.38s
Epoch 61/1000, LR 0.000283
Train loss: 0.2976;  Loss pred: 0.2976; Loss self: 0.0000; time: 0.25s
Val loss: 0.4410 score: 0.8372 time: 0.13s
Test loss: 0.4171 score: 0.7955 time: 0.11s
Epoch 62/1000, LR 0.000283
Train loss: 0.2875;  Loss pred: 0.2875; Loss self: 0.0000; time: 0.27s
Val loss: 0.4401 score: 0.8372 time: 0.20s
Test loss: 0.4155 score: 0.7955 time: 0.11s
Epoch 63/1000, LR 0.000283
Train loss: 0.3140;  Loss pred: 0.3140; Loss self: 0.0000; time: 0.19s
Val loss: 0.4374 score: 0.8372 time: 0.08s
Test loss: 0.4123 score: 0.7955 time: 0.09s
Epoch 64/1000, LR 0.000283
Train loss: 0.3013;  Loss pred: 0.3013; Loss self: 0.0000; time: 0.23s
Val loss: 0.4347 score: 0.8372 time: 0.11s
Test loss: 0.4088 score: 0.8182 time: 0.11s
Epoch 65/1000, LR 0.000283
Train loss: 0.2959;  Loss pred: 0.2959; Loss self: 0.0000; time: 0.43s
Val loss: 0.4320 score: 0.8372 time: 0.09s
Test loss: 0.4062 score: 0.8182 time: 0.08s
Epoch 66/1000, LR 0.000283
Train loss: 0.2640;  Loss pred: 0.2640; Loss self: 0.0000; time: 0.39s
Val loss: 0.4299 score: 0.8372 time: 0.21s
Test loss: 0.4048 score: 0.8409 time: 0.14s
Epoch 67/1000, LR 0.000283
Train loss: 0.2771;  Loss pred: 0.2771; Loss self: 0.0000; time: 0.26s
Val loss: 0.4289 score: 0.8372 time: 0.15s
Test loss: 0.4037 score: 0.8409 time: 0.13s
Epoch 68/1000, LR 0.000283
Train loss: 0.2671;  Loss pred: 0.2671; Loss self: 0.0000; time: 0.35s
Val loss: 0.4280 score: 0.8372 time: 0.13s
Test loss: 0.4026 score: 0.8409 time: 0.13s
Epoch 69/1000, LR 0.000283
Train loss: 0.2786;  Loss pred: 0.2786; Loss self: 0.0000; time: 0.34s
Val loss: 0.4262 score: 0.8372 time: 0.44s
Test loss: 0.4005 score: 0.8409 time: 0.14s
Epoch 70/1000, LR 0.000283
Train loss: 0.2834;  Loss pred: 0.2834; Loss self: 0.0000; time: 0.33s
Val loss: 0.4249 score: 0.8372 time: 0.10s
Test loss: 0.3978 score: 0.8409 time: 0.11s
Epoch 71/1000, LR 0.000282
Train loss: 0.2740;  Loss pred: 0.2740; Loss self: 0.0000; time: 0.32s
Val loss: 0.4234 score: 0.8372 time: 0.17s
Test loss: 0.3960 score: 0.8409 time: 0.12s
Epoch 72/1000, LR 0.000282
Train loss: 0.2573;  Loss pred: 0.2573; Loss self: 0.0000; time: 0.36s
Val loss: 0.4234 score: 0.8372 time: 0.12s
Test loss: 0.3953 score: 0.8409 time: 0.22s
Epoch 73/1000, LR 0.000282
Train loss: 0.2401;  Loss pred: 0.2401; Loss self: 0.0000; time: 0.31s
Val loss: 0.4234 score: 0.8372 time: 0.46s
Test loss: 0.3954 score: 0.8409 time: 0.21s
Epoch 74/1000, LR 0.000282
Train loss: 0.2602;  Loss pred: 0.2602; Loss self: 0.0000; time: 0.30s
Val loss: 0.4219 score: 0.8372 time: 0.15s
Test loss: 0.3938 score: 0.8409 time: 0.14s
Epoch 75/1000, LR 0.000282
Train loss: 0.2627;  Loss pred: 0.2627; Loss self: 0.0000; time: 0.38s
Val loss: 0.4202 score: 0.8140 time: 0.26s
Test loss: 0.3926 score: 0.8409 time: 0.22s
Epoch 76/1000, LR 0.000282
Train loss: 0.2577;  Loss pred: 0.2577; Loss self: 0.0000; time: 0.53s
Val loss: 0.4202 score: 0.8372 time: 0.35s
Test loss: 0.3925 score: 0.8409 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.2483;  Loss pred: 0.2483; Loss self: 0.0000; time: 0.33s
Val loss: 0.4196 score: 0.8372 time: 0.22s
Test loss: 0.3919 score: 0.8636 time: 0.14s
Epoch 78/1000, LR 0.000282
Train loss: 0.2626;  Loss pred: 0.2626; Loss self: 0.0000; time: 0.26s
Val loss: 0.4147 score: 0.8372 time: 0.14s
Test loss: 0.3885 score: 0.8636 time: 0.12s
Epoch 79/1000, LR 0.000282
Train loss: 0.2368;  Loss pred: 0.2368; Loss self: 0.0000; time: 0.32s
Val loss: 0.4119 score: 0.8372 time: 0.11s
Test loss: 0.3867 score: 0.8636 time: 0.29s
Epoch 80/1000, LR 0.000282
Train loss: 0.2272;  Loss pred: 0.2272; Loss self: 0.0000; time: 0.24s
Val loss: 0.4086 score: 0.8372 time: 0.10s
Test loss: 0.3840 score: 0.8636 time: 0.09s
Epoch 81/1000, LR 0.000281
Train loss: 0.2484;  Loss pred: 0.2484; Loss self: 0.0000; time: 0.28s
Val loss: 0.4065 score: 0.8605 time: 0.10s
Test loss: 0.3819 score: 0.8636 time: 0.09s
Epoch 82/1000, LR 0.000281
Train loss: 0.2449;  Loss pred: 0.2449; Loss self: 0.0000; time: 0.27s
Val loss: 0.4057 score: 0.8605 time: 0.11s
Test loss: 0.3806 score: 0.8636 time: 0.10s
Epoch 83/1000, LR 0.000281
Train loss: 0.2447;  Loss pred: 0.2447; Loss self: 0.0000; time: 0.29s
Val loss: 0.4061 score: 0.8605 time: 0.09s
Test loss: 0.3796 score: 0.8636 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2323;  Loss pred: 0.2323; Loss self: 0.0000; time: 0.67s
Val loss: 0.4039 score: 0.8605 time: 0.11s
Test loss: 0.3777 score: 0.8636 time: 0.36s
Epoch 85/1000, LR 0.000281
Train loss: 0.2118;  Loss pred: 0.2118; Loss self: 0.0000; time: 0.25s
Val loss: 0.4033 score: 0.8605 time: 0.14s
Test loss: 0.3761 score: 0.8636 time: 0.11s
Epoch 86/1000, LR 0.000281
Train loss: 0.2376;  Loss pred: 0.2376; Loss self: 0.0000; time: 0.40s
Val loss: 0.4037 score: 0.8605 time: 0.15s
Test loss: 0.3759 score: 0.8636 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.2148;  Loss pred: 0.2148; Loss self: 0.0000; time: 0.57s
Val loss: 0.4048 score: 0.8605 time: 0.10s
Test loss: 0.3756 score: 0.8636 time: 0.49s
     INFO: Early stopping counter 2 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.2126;  Loss pred: 0.2126; Loss self: 0.0000; time: 0.26s
Val loss: 0.4058 score: 0.8605 time: 0.14s
Test loss: 0.3747 score: 0.8409 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.2177;  Loss pred: 0.2177; Loss self: 0.0000; time: 0.32s
Val loss: 0.4100 score: 0.8605 time: 0.13s
Test loss: 0.3741 score: 0.8636 time: 0.17s
     INFO: Early stopping counter 4 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.42s
Val loss: 0.4128 score: 0.8605 time: 0.37s
Test loss: 0.3736 score: 0.8636 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.2090;  Loss pred: 0.2090; Loss self: 0.0000; time: 0.55s
Val loss: 0.4124 score: 0.8605 time: 0.13s
Test loss: 0.3723 score: 0.8636 time: 0.15s
     INFO: Early stopping counter 6 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.2071;  Loss pred: 0.2071; Loss self: 0.0000; time: 0.31s
Val loss: 0.4125 score: 0.8605 time: 0.26s
Test loss: 0.3722 score: 0.8636 time: 0.13s
     INFO: Early stopping counter 7 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.2312;  Loss pred: 0.2312; Loss self: 0.0000; time: 0.54s
Val loss: 0.4131 score: 0.8605 time: 0.19s
Test loss: 0.3727 score: 0.8409 time: 0.13s
     INFO: Early stopping counter 8 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.2127;  Loss pred: 0.2127; Loss self: 0.0000; time: 0.34s
Val loss: 0.4123 score: 0.8605 time: 0.14s
Test loss: 0.3732 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.55s
Val loss: 0.4123 score: 0.8605 time: 0.21s
Test loss: 0.3735 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.1896;  Loss pred: 0.1896; Loss self: 0.0000; time: 0.23s
Val loss: 0.4164 score: 0.8605 time: 0.09s
Test loss: 0.3736 score: 0.8409 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.1838;  Loss pred: 0.1838; Loss self: 0.0000; time: 0.40s
Val loss: 0.4171 score: 0.8605 time: 0.10s
Test loss: 0.3742 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.2195;  Loss pred: 0.2195; Loss self: 0.0000; time: 0.29s
Val loss: 0.4163 score: 0.8605 time: 0.08s
Test loss: 0.3739 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 99/1000, LR 0.000279
Train loss: 0.2163;  Loss pred: 0.2163; Loss self: 0.0000; time: 0.22s
Val loss: 0.4154 score: 0.8605 time: 0.30s
Test loss: 0.3735 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 14 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.1881;  Loss pred: 0.1881; Loss self: 0.0000; time: 0.25s
Val loss: 0.4165 score: 0.8372 time: 0.08s
Test loss: 0.3737 score: 0.8409 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.2039;  Loss pred: 0.2039; Loss self: 0.0000; time: 0.41s
Val loss: 0.4151 score: 0.8372 time: 0.07s
Test loss: 0.3718 score: 0.8409 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1938;  Loss pred: 0.1938; Loss self: 0.0000; time: 0.32s
Val loss: 0.4123 score: 0.8372 time: 0.18s
Test loss: 0.3696 score: 0.8409 time: 0.13s
     INFO: Early stopping counter 17 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1756;  Loss pred: 0.1756; Loss self: 0.0000; time: 0.29s
Val loss: 0.4091 score: 0.8605 time: 0.25s
Test loss: 0.3677 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 18 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1790;  Loss pred: 0.1790; Loss self: 0.0000; time: 0.51s
Val loss: 0.4062 score: 0.8605 time: 0.31s
Test loss: 0.3673 score: 0.8409 time: 0.11s
     INFO: Early stopping counter 19 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1776;  Loss pred: 0.1776; Loss self: 0.0000; time: 0.38s
Val loss: 0.4064 score: 0.8605 time: 0.13s
Test loss: 0.3667 score: 0.8182 time: 0.15s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.2118,   Val_Loss: 0.4033,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8605,   Val_Loss: 0.4033,   Test_Precision: 0.9444,   Test_Recall: 0.7727,   Test_accuracy: 0.8500,   Test_Score: 0.8636,   Test_loss: 0.3761


[0.14325995600029273, 0.11921512600019923, 0.12388083499990898, 0.16605801800051267, 0.1590702510002302, 0.08966273600071872, 0.13911126099992543, 0.12162514999999985, 0.10829779800042161, 0.37977016600052593, 0.25003066699991905, 0.16428116200040677, 0.10591250599918567, 0.34125094299997727, 0.19040753100034635, 0.12241671300034795, 0.12389926000014384, 0.154451641000378, 0.11077352099982818, 0.36320420900028694, 0.342821747999551, 0.13636582199978875, 0.1431244780005727, 0.13617141000031552, 0.1382212549997348, 0.13866753999991488, 0.07711421699968923, 0.11841612499938492, 0.10266007900008844, 0.11487066600057005, 0.12030713699914486, 0.12196998000035819, 0.228251853999609, 0.12406542000007903, 0.13701121499980218, 0.1339023669997914, 0.2017187470000863, 0.1311638409997613, 0.12489465799990285, 0.23314120900067792, 0.13389955799993913, 0.14120742399973096, 0.08694541800014122, 0.09969470200030628, 0.08220909300052881, 0.11285818599935737, 0.1457144710002467, 0.11734438999974373, 0.30157984099969326, 0.12795172599999205, 0.12997847699989507, 0.13777742899947043, 0.13456023700018704, 0.17037855600028706, 0.152811964000648, 0.1331401689994891, 0.12267166999936308, 0.13232543199956126, 0.14370235600017622, 0.38343813299979956, 0.11711021500013885, 0.11927531399942382, 0.09922414199991181, 0.11434888999974646, 0.08437333399979252, 0.14478408000013587, 0.1405869199998051, 0.13849298799959797, 0.14314806699985638, 0.11538301800010231, 0.12983058100053313, 0.22176234099970316, 0.21117629099990154, 0.14731260999997176, 0.22258827299992845, 0.13181460999931005, 0.14329327399991598, 0.1299918840004466, 0.2950968289997036, 0.0957611989997531, 0.09740188500018121, 0.10718828900007793, 0.09135461599998962, 0.36809142800029804, 0.11660664600003656, 0.17304783500003396, 0.4997527390005416, 0.1353521589999218, 0.17503565600054571, 0.1289236519996848, 0.1570707199998651, 0.13802424799996516, 0.1359188960004758, 0.12130880099994101, 0.11385062600038509, 0.15586095599974215, 0.11645838699951128, 0.09185944499949983, 0.12881321999975626, 0.0974821569998312, 0.07741325499955565, 0.1363815810000233, 0.12963872300042567, 0.116529518999414, 0.15681607399983477]
[0.0032559080909157437, 0.00270943468182271, 0.002815473522725204, 0.0037740458636480153, 0.003615232977277959, 0.002037789454561789, 0.0031616195681801237, 0.0027642079545454512, 0.002461313590918673, 0.008631140136375589, 0.005682515159089069, 0.0037336627727365176, 0.0024071024090724018, 0.007755703249999483, 0.004327443886371508, 0.002782198022735181, 0.0028158922727305417, 0.003510264568190409, 0.0025175800227233676, 0.008254641113642885, 0.007791403363626159, 0.003099223227267926, 0.0032528290454675616, 0.0030948047727344438, 0.003141392159084882, 0.0031515349999980654, 0.001752595840902028, 0.002691275568167839, 0.0023331836136383736, 0.00261069695455841, 0.0027342531136169287, 0.0027720450000081405, 0.00518754213635475, 0.0028196686363654326, 0.003113891249995504, 0.003043235613631623, 0.0045845169772746885, 0.0029809963863582116, 0.0028385149545432464, 0.005298663840924498, 0.0030431717727258892, 0.0032092596363575217, 0.0019760322272759367, 0.0022657886818251427, 0.0018683884772847457, 0.0025649587727126677, 0.0033116925227328797, 0.00266691795453963, 0.006854087295447574, 0.002907993772727092, 0.0029540562954521606, 0.003131305204533419, 0.0030581872045497057, 0.003872239909097433, 0.003472999181832909, 0.0030259129318065707, 0.0027879924999855243, 0.0030073961818082103, 0.0032659626363676416, 0.008714503022722718, 0.002661595795457701, 0.0027108025908959958, 0.002255094136361632, 0.002598838409085147, 0.0019175757727225573, 0.003290547272730361, 0.003195157272722843, 0.003147567909081772, 0.0032533651590876452, 0.0026223413181841434, 0.0029506950227393895, 0.005040053204538708, 0.004799461159088672, 0.003348013863635722, 0.00505882438636201, 0.0029957865908934104, 0.003256665318179909, 0.00295436100001015, 0.006706746113629627, 0.002176390886358025, 0.002213679204549573, 0.0024360974772744985, 0.0020762412727270366, 0.008365714272734047, 0.0026501510454553763, 0.003932905340909862, 0.011358016795466856, 0.0030761854318164046, 0.003978083090921494, 0.0029300829999928365, 0.003569789090906025, 0.003136914727271935, 0.003089065818192632, 0.002757018204544114, 0.002587514227281479, 0.0035422944545395944, 0.0026467815227161655, 0.0020877146590795414, 0.0029275731818126424, 0.002215503568177982, 0.0017593921590808102, 0.0030995813863641656, 0.002946334613646038, 0.0026483981590775907, 0.0035640016818144268]
[307.1339767821099, 369.0806819256012, 355.1800405610144, 264.96763317905044, 276.60734627203374, 490.72783145550346, 316.2935889138665, 361.7672825069491, 406.28711582694143, 115.85954858797167, 175.97841308008128, 267.83350850592996, 415.43724780091884, 128.93737263607483, 231.08329680468344, 359.42804639653195, 355.1272219055135, 284.8788120023426, 397.2068379054977, 121.14397055339532, 128.34658319301735, 322.6614950487239, 307.4247020123555, 323.12215904864354, 318.33020182087347, 317.3056938922188, 570.5822053562097, 371.57101704036126, 428.5989298718745, 383.03947850168856, 365.7305883716005, 360.744504507345, 192.76951853401098, 354.65160235601485, 321.14159413930844, 328.597626657851, 218.12548736474739, 335.45830668438623, 352.29689327492474, 188.72682435078244, 328.6045201136512, 311.5983476908681, 506.06462090881587, 441.3474248597967, 535.220599012289, 389.86981414224164, 301.9604003498424, 374.96466597249423, 145.89834603714363, 343.8796910016105, 338.5175839538074, 319.35564714427295, 326.9911006469083, 258.24846173673325, 287.9355702791269, 330.4787753436669, 358.6810222786439, 332.5135564276555, 306.1884385524343, 114.75123680519016, 375.7144498449416, 368.89443862803455, 443.44046834931675, 384.78729439435364, 521.4917784345016, 303.9008156142492, 312.9736393688758, 317.7056155372121, 307.37404229177713, 381.3386125847478, 338.9032049376666, 198.41060389986998, 208.35672315137174, 298.68454574261114, 197.67438511917535, 333.80214833720106, 307.0625631739407, 338.4826701938472, 149.10360151665404, 459.4762853806106, 451.7366373342584, 410.49260521331774, 481.63959224572733, 119.53551931115433, 377.3369830051214, 254.26495512059637, 88.043539467129, 325.07793244750104, 251.37735365109162, 341.2872604641045, 280.1285943047679, 318.784566027928, 323.7224646074669, 362.71069895432726, 386.47130495225537, 282.3029007987914, 377.8173572005995, 478.9926610185766, 341.57984716229635, 451.36465332908244, 568.3781156115003, 322.62421125615555, 339.4047625712537, 377.5867297643378, 280.58348151252886]
Elapsed: 0.15412537926664907~0.0736082015093165
Time per graph: 0.0035028495287874784~0.001672913670666284
Speed: 326.5150249782759~97.90370677710493
Total Time: 0.1575
best val loss: 0.4033236385777939 test_score: 0.8636

Testing...
Test loss: 0.3819 score: 0.8636 time: 0.11s
test Score 0.8636
Epoch Time List: [0.6591209390007862, 0.7469383420002487, 0.7564135829998122, 0.7843028839997714, 0.763782168999569, 0.7146352580002713, 1.037606305000736, 0.9628248450007959, 0.4766330639995431, 0.6860566949990243, 0.5737096459997701, 0.4309524059999603, 0.4986140510000041, 0.7488338199991631, 0.7028720979997161, 0.9397899570003574, 0.915337421998629, 0.8537459719991602, 0.5638187200001994, 0.7864816629999041, 0.9457079809999414, 0.710854855999969, 0.8399142110019966, 0.7508033539988901, 0.5656874329988568, 0.5700495099999898, 0.6276187240000581, 0.36652619899905403, 0.617627087000983, 0.411494037999546, 0.7438824000009845, 0.9335121049989539, 0.6989594409997153, 0.6809088669988341, 0.9616808219998347, 0.7794582759988771, 0.7079063539995332, 0.5363676620008846, 1.0437001249993045, 0.7480708989996856, 0.6832800610009144, 0.7273809689995687, 0.6036545269998896, 0.551956098998744, 0.6511637040002825, 0.4591628859998309, 0.7891358549995857, 0.5141963920004855, 0.7616654079993168, 0.7129683260000093, 0.9043226100002357, 0.7235869870009992, 0.615709453000818, 0.7638940890001322, 0.828580256999885, 0.5710702659989693, 0.5403754879998814, 0.8898024430000078, 0.6971760589995029, 1.0530343030004587, 0.48715270299999247, 0.5748706270005641, 0.3563839549997283, 0.4495671209988359, 0.6020067160006874, 0.7359857859992189, 0.5422037729986187, 0.6156179939998765, 0.9129159259991866, 0.5417944440005158, 0.6102547730006336, 0.6919471689989223, 0.9765997510003217, 0.5890723420006907, 0.8569402319999426, 1.004507895999268, 0.6900182370000039, 0.529685029000575, 0.71918204999929, 0.4308176460008326, 0.468032095000126, 0.4884422880004422, 0.4665299700000105, 1.1457956490003198, 0.5042514060005487, 0.712683217000631, 1.166385387999071, 0.5362926529996912, 0.6193782079999437, 0.9168944079992798, 0.8300678109999353, 0.7060676190003505, 0.8570891790013775, 0.5955233249997036, 0.8669288319997577, 0.4709742349996304, 0.6130862799991519, 0.4542953950003721, 0.6382253950005179, 0.4162797099988893, 0.548656320000191, 0.6358789089999846, 0.6546853810004905, 0.9299482120004541, 0.6625302710008327]
Total Epoch List: [105]
Total Time List: [0.15748658400025306]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f85134c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7419;  Loss pred: 0.7419; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4884 time: 0.13s
Epoch 2/1000, LR 0.000015
Train loss: 0.7343;  Loss pred: 0.7343; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4884 time: 0.22s
Epoch 3/1000, LR 0.000045
Train loss: 0.7174;  Loss pred: 0.7174; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6980 score: 0.4884 time: 0.24s
Epoch 4/1000, LR 0.000075
Train loss: 0.7275;  Loss pred: 0.7275; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.4884 time: 0.10s
Epoch 5/1000, LR 0.000105
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4884 time: 0.10s
Epoch 6/1000, LR 0.000135
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4884 time: 0.12s
Epoch 7/1000, LR 0.000165
Train loss: 0.6708;  Loss pred: 0.6708; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4884 time: 0.30s
Epoch 8/1000, LR 0.000195
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.10s
Epoch 9/1000, LR 0.000225
Train loss: 0.6115;  Loss pred: 0.6115; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.13s
Epoch 10/1000, LR 0.000255
Train loss: 0.6338;  Loss pred: 0.6338; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.12s
Epoch 11/1000, LR 0.000285
Train loss: 0.6253;  Loss pred: 0.6253; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4884 time: 0.16s
Epoch 12/1000, LR 0.000285
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6757 score: 0.5000 time: 0.21s
Test loss: 0.6959 score: 0.5116 time: 0.11s
Epoch 13/1000, LR 0.000285
Train loss: 0.5947;  Loss pred: 0.5947; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6717 score: 0.5000 time: 0.20s
Test loss: 0.6949 score: 0.5116 time: 0.32s
Epoch 14/1000, LR 0.000285
Train loss: 0.6074;  Loss pred: 0.6074; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6674 score: 0.5000 time: 0.12s
Test loss: 0.6935 score: 0.5116 time: 0.16s
Epoch 15/1000, LR 0.000285
Train loss: 0.5527;  Loss pred: 0.5527; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6638 score: 0.5000 time: 0.09s
Test loss: 0.6928 score: 0.5116 time: 0.12s
Epoch 16/1000, LR 0.000285
Train loss: 0.5502;  Loss pred: 0.5502; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6587 score: 0.5000 time: 0.28s
Test loss: 0.6907 score: 0.5116 time: 0.12s
Epoch 17/1000, LR 0.000285
Train loss: 0.5182;  Loss pred: 0.5182; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6532 score: 0.5000 time: 0.15s
Test loss: 0.6879 score: 0.5116 time: 0.13s
Epoch 18/1000, LR 0.000285
Train loss: 0.5276;  Loss pred: 0.5276; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6484 score: 0.5000 time: 0.14s
Test loss: 0.6854 score: 0.5116 time: 0.41s
Epoch 19/1000, LR 0.000285
Train loss: 0.5116;  Loss pred: 0.5116; Loss self: 0.0000; time: 0.42s
Val loss: 0.6420 score: 0.5455 time: 0.28s
Test loss: 0.6813 score: 0.5349 time: 0.12s
Epoch 20/1000, LR 0.000285
Train loss: 0.4932;  Loss pred: 0.4932; Loss self: 0.0000; time: 0.33s
Val loss: 0.6341 score: 0.5909 time: 0.16s
Test loss: 0.6762 score: 0.5349 time: 0.12s
Epoch 21/1000, LR 0.000285
Train loss: 0.4920;  Loss pred: 0.4920; Loss self: 0.0000; time: 0.30s
Val loss: 0.6259 score: 0.6364 time: 0.13s
Test loss: 0.6711 score: 0.5349 time: 0.13s
Epoch 22/1000, LR 0.000285
Train loss: 0.4553;  Loss pred: 0.4553; Loss self: 0.0000; time: 0.32s
Val loss: 0.6182 score: 0.6591 time: 0.12s
Test loss: 0.6664 score: 0.5581 time: 0.38s
Epoch 23/1000, LR 0.000285
Train loss: 0.4394;  Loss pred: 0.4394; Loss self: 0.0000; time: 0.26s
Val loss: 0.6092 score: 0.6591 time: 0.18s
Test loss: 0.6602 score: 0.5581 time: 0.09s
Epoch 24/1000, LR 0.000285
Train loss: 0.4540;  Loss pred: 0.4540; Loss self: 0.0000; time: 0.31s
Val loss: 0.6000 score: 0.7273 time: 0.08s
Test loss: 0.6545 score: 0.5581 time: 0.16s
Epoch 25/1000, LR 0.000285
Train loss: 0.4524;  Loss pred: 0.4524; Loss self: 0.0000; time: 0.28s
Val loss: 0.5915 score: 0.7273 time: 0.10s
Test loss: 0.6498 score: 0.5581 time: 0.10s
Epoch 26/1000, LR 0.000285
Train loss: 0.4481;  Loss pred: 0.4481; Loss self: 0.0000; time: 0.26s
Val loss: 0.5828 score: 0.7727 time: 0.07s
Test loss: 0.6454 score: 0.5581 time: 0.12s
Epoch 27/1000, LR 0.000285
Train loss: 0.4127;  Loss pred: 0.4127; Loss self: 0.0000; time: 0.72s
Val loss: 0.5758 score: 0.7955 time: 0.13s
Test loss: 0.6434 score: 0.5581 time: 0.13s
Epoch 28/1000, LR 0.000285
Train loss: 0.4233;  Loss pred: 0.4233; Loss self: 0.0000; time: 0.29s
Val loss: 0.5674 score: 0.7955 time: 0.27s
Test loss: 0.6399 score: 0.5814 time: 0.17s
Epoch 29/1000, LR 0.000285
Train loss: 0.3923;  Loss pred: 0.3923; Loss self: 0.0000; time: 0.32s
Val loss: 0.5613 score: 0.7955 time: 0.22s
Test loss: 0.6391 score: 0.6047 time: 0.13s
Epoch 30/1000, LR 0.000285
Train loss: 0.3906;  Loss pred: 0.3906; Loss self: 0.0000; time: 0.44s
Val loss: 0.5547 score: 0.7955 time: 0.36s
Test loss: 0.6369 score: 0.6047 time: 0.27s
Epoch 31/1000, LR 0.000285
Train loss: 0.4063;  Loss pred: 0.4063; Loss self: 0.0000; time: 0.31s
Val loss: 0.5483 score: 0.7955 time: 0.21s
Test loss: 0.6348 score: 0.6047 time: 0.11s
Epoch 32/1000, LR 0.000285
Train loss: 0.4074;  Loss pred: 0.4074; Loss self: 0.0000; time: 0.35s
Val loss: 0.5414 score: 0.7955 time: 0.13s
Test loss: 0.6308 score: 0.6047 time: 0.11s
Epoch 33/1000, LR 0.000285
Train loss: 0.3777;  Loss pred: 0.3777; Loss self: 0.0000; time: 0.32s
Val loss: 0.5357 score: 0.7955 time: 0.13s
Test loss: 0.6272 score: 0.6279 time: 0.29s
Epoch 34/1000, LR 0.000285
Train loss: 0.3788;  Loss pred: 0.3788; Loss self: 0.0000; time: 0.30s
Val loss: 0.5304 score: 0.7955 time: 0.15s
Test loss: 0.6235 score: 0.6279 time: 0.12s
Epoch 35/1000, LR 0.000285
Train loss: 0.3557;  Loss pred: 0.3557; Loss self: 0.0000; time: 0.36s
Val loss: 0.5259 score: 0.7955 time: 0.14s
Test loss: 0.6211 score: 0.6279 time: 0.20s
Epoch 36/1000, LR 0.000285
Train loss: 0.3553;  Loss pred: 0.3553; Loss self: 0.0000; time: 0.42s
Val loss: 0.5230 score: 0.7955 time: 0.14s
Test loss: 0.6206 score: 0.6279 time: 0.14s
Epoch 37/1000, LR 0.000285
Train loss: 0.3460;  Loss pred: 0.3460; Loss self: 0.0000; time: 0.48s
Val loss: 0.5198 score: 0.7955 time: 0.14s
Test loss: 0.6196 score: 0.6279 time: 0.13s
Epoch 38/1000, LR 0.000284
Train loss: 0.3439;  Loss pred: 0.3439; Loss self: 0.0000; time: 0.36s
Val loss: 0.5161 score: 0.7955 time: 0.19s
Test loss: 0.6178 score: 0.6279 time: 0.16s
Epoch 39/1000, LR 0.000284
Train loss: 0.3367;  Loss pred: 0.3367; Loss self: 0.0000; time: 0.37s
Val loss: 0.5120 score: 0.7955 time: 0.13s
Test loss: 0.6157 score: 0.6279 time: 0.08s
Epoch 40/1000, LR 0.000284
Train loss: 0.3300;  Loss pred: 0.3300; Loss self: 0.0000; time: 0.33s
Val loss: 0.5079 score: 0.7955 time: 0.11s
Test loss: 0.6138 score: 0.6279 time: 0.21s
Epoch 41/1000, LR 0.000284
Train loss: 0.3376;  Loss pred: 0.3376; Loss self: 0.0000; time: 0.48s
Val loss: 0.5051 score: 0.7955 time: 0.10s
Test loss: 0.6140 score: 0.6279 time: 0.13s
Epoch 42/1000, LR 0.000284
Train loss: 0.3227;  Loss pred: 0.3227; Loss self: 0.0000; time: 0.28s
Val loss: 0.5011 score: 0.7955 time: 0.16s
Test loss: 0.6123 score: 0.6279 time: 0.10s
Epoch 43/1000, LR 0.000284
Train loss: 0.3202;  Loss pred: 0.3202; Loss self: 0.0000; time: 0.30s
Val loss: 0.4978 score: 0.7955 time: 0.13s
Test loss: 0.6119 score: 0.6279 time: 0.10s
Epoch 44/1000, LR 0.000284
Train loss: 0.3100;  Loss pred: 0.3100; Loss self: 0.0000; time: 0.29s
Val loss: 0.4961 score: 0.7955 time: 0.13s
Test loss: 0.6134 score: 0.6279 time: 0.20s
Epoch 45/1000, LR 0.000284
Train loss: 0.3001;  Loss pred: 0.3001; Loss self: 0.0000; time: 0.30s
Val loss: 0.4943 score: 0.7955 time: 0.14s
Test loss: 0.6146 score: 0.6512 time: 0.31s
Epoch 46/1000, LR 0.000284
Train loss: 0.3034;  Loss pred: 0.3034; Loss self: 0.0000; time: 0.44s
Val loss: 0.4921 score: 0.7955 time: 0.14s
Test loss: 0.6157 score: 0.6512 time: 0.13s
Epoch 47/1000, LR 0.000284
Train loss: 0.2911;  Loss pred: 0.2911; Loss self: 0.0000; time: 0.39s
Val loss: 0.4922 score: 0.7727 time: 0.14s
Test loss: 0.6201 score: 0.6512 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.2946;  Loss pred: 0.2946; Loss self: 0.0000; time: 0.33s
Val loss: 0.4920 score: 0.7727 time: 0.15s
Test loss: 0.6240 score: 0.6512 time: 0.15s
Epoch 49/1000, LR 0.000284
Train loss: 0.3091;  Loss pred: 0.3091; Loss self: 0.0000; time: 0.36s
Val loss: 0.4909 score: 0.7727 time: 0.45s
Test loss: 0.6267 score: 0.6512 time: 0.12s
Epoch 50/1000, LR 0.000284
Train loss: 0.2892;  Loss pred: 0.2892; Loss self: 0.0000; time: 0.34s
Val loss: 0.4909 score: 0.7727 time: 0.13s
Test loss: 0.6305 score: 0.6744 time: 0.21s
Epoch 51/1000, LR 0.000284
Train loss: 0.2646;  Loss pred: 0.2646; Loss self: 0.0000; time: 0.42s
Val loss: 0.4915 score: 0.7727 time: 0.13s
Test loss: 0.6345 score: 0.6744 time: 0.15s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.2860;  Loss pred: 0.2860; Loss self: 0.0000; time: 0.35s
Val loss: 0.4934 score: 0.7727 time: 0.14s
Test loss: 0.6396 score: 0.6744 time: 0.26s
     INFO: Early stopping counter 2 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.2885;  Loss pred: 0.2885; Loss self: 0.0000; time: 0.66s
Val loss: 0.4914 score: 0.7727 time: 0.16s
Test loss: 0.6402 score: 0.6744 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.2746;  Loss pred: 0.2746; Loss self: 0.0000; time: 0.32s
Val loss: 0.4898 score: 0.7727 time: 0.13s
Test loss: 0.6410 score: 0.6977 time: 0.10s
Epoch 55/1000, LR 0.000284
Train loss: 0.2650;  Loss pred: 0.2650; Loss self: 0.0000; time: 0.39s
Val loss: 0.4882 score: 0.7955 time: 0.12s
Test loss: 0.6423 score: 0.6977 time: 0.19s
Epoch 56/1000, LR 0.000284
Train loss: 0.2612;  Loss pred: 0.2612; Loss self: 0.0000; time: 0.37s
Val loss: 0.4861 score: 0.7955 time: 0.13s
Test loss: 0.6425 score: 0.6977 time: 0.23s
Epoch 57/1000, LR 0.000283
Train loss: 0.2664;  Loss pred: 0.2664; Loss self: 0.0000; time: 0.48s
Val loss: 0.4837 score: 0.7955 time: 0.12s
Test loss: 0.6434 score: 0.6977 time: 0.11s
Epoch 58/1000, LR 0.000283
Train loss: 0.2604;  Loss pred: 0.2604; Loss self: 0.0000; time: 0.33s
Val loss: 0.4818 score: 0.7955 time: 0.11s
Test loss: 0.6434 score: 0.6977 time: 0.15s
Epoch 59/1000, LR 0.000283
Train loss: 0.2727;  Loss pred: 0.2727; Loss self: 0.0000; time: 0.22s
Val loss: 0.4777 score: 0.7955 time: 0.08s
Test loss: 0.6412 score: 0.7209 time: 0.09s
Epoch 60/1000, LR 0.000283
Train loss: 0.2499;  Loss pred: 0.2499; Loss self: 0.0000; time: 0.36s
Val loss: 0.4770 score: 0.7955 time: 0.10s
Test loss: 0.6423 score: 0.7209 time: 0.10s
Epoch 61/1000, LR 0.000283
Train loss: 0.2376;  Loss pred: 0.2376; Loss self: 0.0000; time: 0.45s
Val loss: 0.4785 score: 0.7955 time: 0.10s
Test loss: 0.6450 score: 0.7209 time: 0.16s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.2502;  Loss pred: 0.2502; Loss self: 0.0000; time: 0.35s
Val loss: 0.4806 score: 0.7955 time: 0.13s
Test loss: 0.6491 score: 0.7209 time: 0.34s
     INFO: Early stopping counter 2 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.2537;  Loss pred: 0.2537; Loss self: 0.0000; time: 0.35s
Val loss: 0.4803 score: 0.7955 time: 0.13s
Test loss: 0.6513 score: 0.7209 time: 0.18s
     INFO: Early stopping counter 3 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.2543;  Loss pred: 0.2543; Loss self: 0.0000; time: 0.35s
Val loss: 0.4801 score: 0.7955 time: 0.19s
Test loss: 0.6521 score: 0.7209 time: 0.15s
     INFO: Early stopping counter 4 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.2441;  Loss pred: 0.2441; Loss self: 0.0000; time: 0.58s
Val loss: 0.4784 score: 0.7955 time: 0.29s
Test loss: 0.6503 score: 0.7209 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.2336;  Loss pred: 0.2336; Loss self: 0.0000; time: 0.30s
Val loss: 0.4808 score: 0.7955 time: 0.18s
Test loss: 0.6532 score: 0.7209 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.2255;  Loss pred: 0.2255; Loss self: 0.0000; time: 0.45s
Val loss: 0.4848 score: 0.7955 time: 0.12s
Test loss: 0.6577 score: 0.7209 time: 0.11s
     INFO: Early stopping counter 7 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.2472;  Loss pred: 0.2472; Loss self: 0.0000; time: 0.67s
Val loss: 0.4867 score: 0.7955 time: 0.30s
Test loss: 0.6626 score: 0.7209 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.2175;  Loss pred: 0.2175; Loss self: 0.0000; time: 0.30s
Val loss: 0.4870 score: 0.7955 time: 0.17s
Test loss: 0.6646 score: 0.7209 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.2361;  Loss pred: 0.2361; Loss self: 0.0000; time: 0.32s
Val loss: 0.4835 score: 0.7955 time: 0.24s
Test loss: 0.6618 score: 0.7209 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.2277;  Loss pred: 0.2277; Loss self: 0.0000; time: 0.78s
Val loss: 0.4820 score: 0.7955 time: 0.13s
Test loss: 0.6608 score: 0.7209 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.2240;  Loss pred: 0.2240; Loss self: 0.0000; time: 0.45s
Val loss: 0.4790 score: 0.7955 time: 0.25s
Test loss: 0.6574 score: 0.7209 time: 0.12s
     INFO: Early stopping counter 12 of 20
Epoch 73/1000, LR 0.000282
Train loss: 0.2346;  Loss pred: 0.2346; Loss self: 0.0000; time: 0.44s
Val loss: 0.4743 score: 0.7955 time: 0.17s
Test loss: 0.6516 score: 0.7209 time: 0.11s
Epoch 74/1000, LR 0.000282
Train loss: 0.2117;  Loss pred: 0.2117; Loss self: 0.0000; time: 0.34s
Val loss: 0.4743 score: 0.7955 time: 0.45s
Test loss: 0.6505 score: 0.7209 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.2293;  Loss pred: 0.2293; Loss self: 0.0000; time: 0.31s
Val loss: 0.4711 score: 0.7955 time: 0.11s
Test loss: 0.6450 score: 0.7209 time: 0.12s
Epoch 76/1000, LR 0.000282
Train loss: 0.2002;  Loss pred: 0.2002; Loss self: 0.0000; time: 0.22s
Val loss: 0.4690 score: 0.7955 time: 0.09s
Test loss: 0.6415 score: 0.7442 time: 0.10s
Epoch 77/1000, LR 0.000282
Train loss: 0.2055;  Loss pred: 0.2055; Loss self: 0.0000; time: 0.31s
Val loss: 0.4680 score: 0.7955 time: 0.11s
Test loss: 0.6406 score: 0.7442 time: 0.11s
Epoch 78/1000, LR 0.000282
Train loss: 0.2006;  Loss pred: 0.2006; Loss self: 0.0000; time: 0.37s
Val loss: 0.4690 score: 0.7955 time: 0.08s
Test loss: 0.6432 score: 0.7674 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.2073;  Loss pred: 0.2073; Loss self: 0.0000; time: 0.41s
Val loss: 0.4667 score: 0.7955 time: 0.16s
Test loss: 0.6422 score: 0.7674 time: 0.12s
Epoch 80/1000, LR 0.000282
Train loss: 0.1982;  Loss pred: 0.1982; Loss self: 0.0000; time: 0.62s
Val loss: 0.4643 score: 0.7955 time: 0.22s
Test loss: 0.6411 score: 0.7674 time: 0.11s
Epoch 81/1000, LR 0.000281
Train loss: 0.1921;  Loss pred: 0.1921; Loss self: 0.0000; time: 0.46s
Val loss: 0.4635 score: 0.7955 time: 0.11s
Test loss: 0.6421 score: 0.7674 time: 0.21s
Epoch 82/1000, LR 0.000281
Train loss: 0.1873;  Loss pred: 0.1873; Loss self: 0.0000; time: 0.33s
Val loss: 0.4628 score: 0.7955 time: 0.23s
Test loss: 0.6430 score: 0.7674 time: 0.20s
Epoch 83/1000, LR 0.000281
Train loss: 0.2009;  Loss pred: 0.2009; Loss self: 0.0000; time: 0.43s
Val loss: 0.4637 score: 0.7955 time: 0.37s
Test loss: 0.6453 score: 0.7674 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2146;  Loss pred: 0.2146; Loss self: 0.0000; time: 0.65s
Val loss: 0.4636 score: 0.8182 time: 0.20s
Test loss: 0.6469 score: 0.7674 time: 0.19s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.1855;  Loss pred: 0.1855; Loss self: 0.0000; time: 0.27s
Val loss: 0.4665 score: 0.7955 time: 0.10s
Test loss: 0.6526 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.1914;  Loss pred: 0.1914; Loss self: 0.0000; time: 0.59s
Val loss: 0.4671 score: 0.7955 time: 0.19s
Test loss: 0.6555 score: 0.7442 time: 0.38s
     INFO: Early stopping counter 4 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.1918;  Loss pred: 0.1918; Loss self: 0.0000; time: 0.46s
Val loss: 0.4681 score: 0.7955 time: 0.13s
Test loss: 0.6585 score: 0.7442 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.1883;  Loss pred: 0.1883; Loss self: 0.0000; time: 0.29s
Val loss: 0.4704 score: 0.7955 time: 0.12s
Test loss: 0.6631 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.1790;  Loss pred: 0.1790; Loss self: 0.0000; time: 0.41s
Val loss: 0.4719 score: 0.7955 time: 0.13s
Test loss: 0.6682 score: 0.7442 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.1963;  Loss pred: 0.1963; Loss self: 0.0000; time: 0.64s
Val loss: 0.4688 score: 0.8182 time: 0.13s
Test loss: 0.6658 score: 0.7442 time: 0.13s
     INFO: Early stopping counter 8 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.1641;  Loss pred: 0.1641; Loss self: 0.0000; time: 0.36s
Val loss: 0.4644 score: 0.8182 time: 0.26s
Test loss: 0.6633 score: 0.7674 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.1842;  Loss pred: 0.1842; Loss self: 0.0000; time: 0.27s
Val loss: 0.4616 score: 0.8182 time: 0.20s
Test loss: 0.6609 score: 0.7674 time: 0.10s
Epoch 93/1000, LR 0.000280
Train loss: 0.1858;  Loss pred: 0.1858; Loss self: 0.0000; time: 0.69s
Val loss: 0.4605 score: 0.8182 time: 0.08s
Test loss: 0.6619 score: 0.7674 time: 0.06s
Epoch 94/1000, LR 0.000280
Train loss: 0.1748;  Loss pred: 0.1748; Loss self: 0.0000; time: 0.32s
Val loss: 0.4613 score: 0.8182 time: 0.15s
Test loss: 0.6638 score: 0.7674 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1809;  Loss pred: 0.1809; Loss self: 0.0000; time: 0.25s
Val loss: 0.4598 score: 0.8182 time: 0.14s
Test loss: 0.6617 score: 0.7674 time: 0.07s
Epoch 96/1000, LR 0.000280
Train loss: 0.1807;  Loss pred: 0.1807; Loss self: 0.0000; time: 0.36s
Val loss: 0.4584 score: 0.8182 time: 0.13s
Test loss: 0.6619 score: 0.7674 time: 0.13s
Epoch 97/1000, LR 0.000280
Train loss: 0.1713;  Loss pred: 0.1713; Loss self: 0.0000; time: 0.35s
Val loss: 0.4579 score: 0.8182 time: 0.12s
Test loss: 0.6636 score: 0.7674 time: 0.57s
Epoch 98/1000, LR 0.000280
Train loss: 0.1628;  Loss pred: 0.1628; Loss self: 0.0000; time: 0.62s
Val loss: 0.4573 score: 0.8182 time: 0.13s
Test loss: 0.6657 score: 0.7674 time: 0.12s
Epoch 99/1000, LR 0.000279
Train loss: 0.1707;  Loss pred: 0.1707; Loss self: 0.0000; time: 0.37s
Val loss: 0.4590 score: 0.8182 time: 0.14s
Test loss: 0.6718 score: 0.7674 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.1561;  Loss pred: 0.1561; Loss self: 0.0000; time: 0.59s
Val loss: 0.4585 score: 0.8182 time: 0.14s
Test loss: 0.6750 score: 0.7674 time: 0.33s
     INFO: Early stopping counter 2 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 0.40s
Val loss: 0.4563 score: 0.8182 time: 0.15s
Test loss: 0.6749 score: 0.7674 time: 0.14s
Epoch 102/1000, LR 0.000279
Train loss: 0.1622;  Loss pred: 0.1622; Loss self: 0.0000; time: 0.33s
Val loss: 0.4528 score: 0.8182 time: 0.14s
Test loss: 0.6703 score: 0.7674 time: 0.12s
Epoch 103/1000, LR 0.000279
Train loss: 0.1637;  Loss pred: 0.1637; Loss self: 0.0000; time: 0.36s
Val loss: 0.4468 score: 0.8182 time: 0.23s
Test loss: 0.6610 score: 0.7674 time: 0.21s
Epoch 104/1000, LR 0.000279
Train loss: 0.1390;  Loss pred: 0.1390; Loss self: 0.0000; time: 0.67s
Val loss: 0.4416 score: 0.8182 time: 0.14s
Test loss: 0.6532 score: 0.7907 time: 0.15s
Epoch 105/1000, LR 0.000279
Train loss: 0.1503;  Loss pred: 0.1503; Loss self: 0.0000; time: 0.36s
Val loss: 0.4363 score: 0.8182 time: 0.14s
Test loss: 0.6442 score: 0.7907 time: 0.11s
Epoch 106/1000, LR 0.000279
Train loss: 0.1464;  Loss pred: 0.1464; Loss self: 0.0000; time: 0.51s
Val loss: 0.4330 score: 0.8182 time: 0.13s
Test loss: 0.6387 score: 0.8140 time: 0.19s
Epoch 107/1000, LR 0.000278
Train loss: 0.1477;  Loss pred: 0.1477; Loss self: 0.0000; time: 0.29s
Val loss: 0.4318 score: 0.8182 time: 0.28s
Test loss: 0.6370 score: 0.8140 time: 0.16s
Epoch 108/1000, LR 0.000278
Train loss: 0.1443;  Loss pred: 0.1443; Loss self: 0.0000; time: 0.31s
Val loss: 0.4331 score: 0.8182 time: 0.13s
Test loss: 0.6411 score: 0.8140 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1365;  Loss pred: 0.1365; Loss self: 0.0000; time: 0.43s
Val loss: 0.4371 score: 0.8182 time: 0.11s
Test loss: 0.6495 score: 0.7907 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1411;  Loss pred: 0.1411; Loss self: 0.0000; time: 0.28s
Val loss: 0.4378 score: 0.8182 time: 0.08s
Test loss: 0.6532 score: 0.7907 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1479;  Loss pred: 0.1479; Loss self: 0.0000; time: 0.35s
Val loss: 0.4397 score: 0.8182 time: 0.33s
Test loss: 0.6568 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1239;  Loss pred: 0.1239; Loss self: 0.0000; time: 0.23s
Val loss: 0.4380 score: 0.8182 time: 0.07s
Test loss: 0.6562 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1487;  Loss pred: 0.1487; Loss self: 0.0000; time: 0.48s
Val loss: 0.4348 score: 0.8182 time: 0.16s
Test loss: 0.6516 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1297;  Loss pred: 0.1297; Loss self: 0.0000; time: 0.36s
Val loss: 0.4276 score: 0.8182 time: 0.19s
Test loss: 0.6403 score: 0.8140 time: 0.11s
Epoch 115/1000, LR 0.000277
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.41s
Val loss: 0.4206 score: 0.8182 time: 0.34s
Test loss: 0.6320 score: 0.8140 time: 0.13s
Epoch 116/1000, LR 0.000277
Train loss: 0.1290;  Loss pred: 0.1290; Loss self: 0.0000; time: 0.33s
Val loss: 0.4132 score: 0.8182 time: 0.13s
Test loss: 0.6241 score: 0.8140 time: 0.18s
Epoch 117/1000, LR 0.000277
Train loss: 0.1161;  Loss pred: 0.1161; Loss self: 0.0000; time: 0.37s
Val loss: 0.4064 score: 0.8182 time: 0.11s
Test loss: 0.6160 score: 0.8140 time: 0.13s
Epoch 118/1000, LR 0.000277
Train loss: 0.1306;  Loss pred: 0.1306; Loss self: 0.0000; time: 0.51s
Val loss: 0.4015 score: 0.8409 time: 0.12s
Test loss: 0.6111 score: 0.7907 time: 0.23s
Epoch 119/1000, LR 0.000277
Train loss: 0.1139;  Loss pred: 0.1139; Loss self: 0.0000; time: 0.70s
Val loss: 0.3956 score: 0.8409 time: 0.14s
Test loss: 0.6018 score: 0.7907 time: 0.12s
Epoch 120/1000, LR 0.000277
Train loss: 0.1167;  Loss pred: 0.1167; Loss self: 0.0000; time: 0.38s
Val loss: 0.3926 score: 0.8409 time: 0.13s
Test loss: 0.5987 score: 0.7907 time: 0.13s
Epoch 121/1000, LR 0.000276
Train loss: 0.1250;  Loss pred: 0.1250; Loss self: 0.0000; time: 0.37s
Val loss: 0.3881 score: 0.8409 time: 0.13s
Test loss: 0.5968 score: 0.7907 time: 0.22s
Epoch 122/1000, LR 0.000276
Train loss: 0.1052;  Loss pred: 0.1052; Loss self: 0.0000; time: 0.35s
Val loss: 0.3830 score: 0.8409 time: 0.20s
Test loss: 0.5976 score: 0.7907 time: 0.12s
Epoch 123/1000, LR 0.000276
Train loss: 0.1073;  Loss pred: 0.1073; Loss self: 0.0000; time: 0.53s
Val loss: 0.3831 score: 0.8409 time: 0.13s
Test loss: 0.6006 score: 0.7907 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 124/1000, LR 0.000276
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.32s
Val loss: 0.3839 score: 0.8409 time: 0.14s
Test loss: 0.6034 score: 0.7907 time: 0.18s
     INFO: Early stopping counter 2 of 20
Epoch 125/1000, LR 0.000276
Train loss: 0.0995;  Loss pred: 0.0995; Loss self: 0.0000; time: 0.29s
Val loss: 0.3887 score: 0.8409 time: 0.25s
Test loss: 0.6110 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 126/1000, LR 0.000276
Train loss: 0.0956;  Loss pred: 0.0956; Loss self: 0.0000; time: 0.33s
Val loss: 0.3867 score: 0.8409 time: 0.09s
Test loss: 0.6075 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 127/1000, LR 0.000275
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.53s
Val loss: 0.3852 score: 0.8409 time: 0.10s
Test loss: 0.6066 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.0898;  Loss pred: 0.0898; Loss self: 0.0000; time: 0.24s
Val loss: 0.3933 score: 0.8409 time: 0.06s
Test loss: 0.6164 score: 0.7907 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 129/1000, LR 0.000275
Train loss: 0.0852;  Loss pred: 0.0852; Loss self: 0.0000; time: 0.29s
Val loss: 0.4036 score: 0.8409 time: 0.13s
Test loss: 0.6270 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 130/1000, LR 0.000275
Train loss: 0.0795;  Loss pred: 0.0795; Loss self: 0.0000; time: 0.33s
Val loss: 0.4134 score: 0.8182 time: 0.12s
Test loss: 0.6400 score: 0.8140 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 131/1000, LR 0.000275
Train loss: 0.0847;  Loss pred: 0.0847; Loss self: 0.0000; time: 0.34s
Val loss: 0.4095 score: 0.8409 time: 0.08s
Test loss: 0.6300 score: 0.7907 time: 0.23s
     INFO: Early stopping counter 9 of 20
Epoch 132/1000, LR 0.000275
Train loss: 0.0807;  Loss pred: 0.0807; Loss self: 0.0000; time: 0.59s
Val loss: 0.4087 score: 0.8409 time: 0.11s
Test loss: 0.6322 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 133/1000, LR 0.000274
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.39s
Val loss: 0.4065 score: 0.8409 time: 0.12s
Test loss: 0.6291 score: 0.7907 time: 0.18s
     INFO: Early stopping counter 11 of 20
Epoch 134/1000, LR 0.000274
Train loss: 0.0722;  Loss pred: 0.0722; Loss self: 0.0000; time: 0.29s
Val loss: 0.3989 score: 0.8409 time: 0.12s
Test loss: 0.6220 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 135/1000, LR 0.000274
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 0.37s
Val loss: 0.3959 score: 0.8409 time: 0.18s
Test loss: 0.6255 score: 0.7907 time: 0.13s
     INFO: Early stopping counter 13 of 20
Epoch 136/1000, LR 0.000274
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.30s
Val loss: 0.3924 score: 0.8409 time: 0.33s
Test loss: 0.6253 score: 0.7907 time: 0.19s
     INFO: Early stopping counter 14 of 20
Epoch 137/1000, LR 0.000274
Train loss: 0.0745;  Loss pred: 0.0745; Loss self: 0.0000; time: 0.40s
Val loss: 0.3904 score: 0.8409 time: 0.14s
Test loss: 0.6244 score: 0.7907 time: 0.23s
     INFO: Early stopping counter 15 of 20
Epoch 138/1000, LR 0.000274
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.34s
Val loss: 0.3946 score: 0.8409 time: 0.13s
Test loss: 0.6358 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 16 of 20
Epoch 139/1000, LR 0.000273
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.36s
Val loss: 0.3989 score: 0.8409 time: 0.13s
Test loss: 0.6463 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 17 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.40s
Val loss: 0.3940 score: 0.8409 time: 0.12s
Test loss: 0.6410 score: 0.7907 time: 0.34s
     INFO: Early stopping counter 18 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.31s
Val loss: 0.3873 score: 0.8409 time: 0.35s
Test loss: 0.6303 score: 0.7907 time: 0.13s
     INFO: Early stopping counter 19 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.34s
Val loss: 0.3910 score: 0.8409 time: 0.14s
Test loss: 0.6364 score: 0.7907 time: 0.20s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 121,   Train_Loss: 0.1052,   Val_Loss: 0.3830,   Val_Precision: 1.0000,   Val_Recall: 0.6818,   Val_accuracy: 0.8108,   Val_Score: 0.8409,   Val_Loss: 0.3830,   Test_Precision: 0.9333,   Test_Recall: 0.6364,   Test_accuracy: 0.7568,   Test_Score: 0.7907,   Test_loss: 0.5976


[0.14325995600029273, 0.11921512600019923, 0.12388083499990898, 0.16605801800051267, 0.1590702510002302, 0.08966273600071872, 0.13911126099992543, 0.12162514999999985, 0.10829779800042161, 0.37977016600052593, 0.25003066699991905, 0.16428116200040677, 0.10591250599918567, 0.34125094299997727, 0.19040753100034635, 0.12241671300034795, 0.12389926000014384, 0.154451641000378, 0.11077352099982818, 0.36320420900028694, 0.342821747999551, 0.13636582199978875, 0.1431244780005727, 0.13617141000031552, 0.1382212549997348, 0.13866753999991488, 0.07711421699968923, 0.11841612499938492, 0.10266007900008844, 0.11487066600057005, 0.12030713699914486, 0.12196998000035819, 0.228251853999609, 0.12406542000007903, 0.13701121499980218, 0.1339023669997914, 0.2017187470000863, 0.1311638409997613, 0.12489465799990285, 0.23314120900067792, 0.13389955799993913, 0.14120742399973096, 0.08694541800014122, 0.09969470200030628, 0.08220909300052881, 0.11285818599935737, 0.1457144710002467, 0.11734438999974373, 0.30157984099969326, 0.12795172599999205, 0.12997847699989507, 0.13777742899947043, 0.13456023700018704, 0.17037855600028706, 0.152811964000648, 0.1331401689994891, 0.12267166999936308, 0.13232543199956126, 0.14370235600017622, 0.38343813299979956, 0.11711021500013885, 0.11927531399942382, 0.09922414199991181, 0.11434888999974646, 0.08437333399979252, 0.14478408000013587, 0.1405869199998051, 0.13849298799959797, 0.14314806699985638, 0.11538301800010231, 0.12983058100053313, 0.22176234099970316, 0.21117629099990154, 0.14731260999997176, 0.22258827299992845, 0.13181460999931005, 0.14329327399991598, 0.1299918840004466, 0.2950968289997036, 0.0957611989997531, 0.09740188500018121, 0.10718828900007793, 0.09135461599998962, 0.36809142800029804, 0.11660664600003656, 0.17304783500003396, 0.4997527390005416, 0.1353521589999218, 0.17503565600054571, 0.1289236519996848, 0.1570707199998651, 0.13802424799996516, 0.1359188960004758, 0.12130880099994101, 0.11385062600038509, 0.15586095599974215, 0.11645838699951128, 0.09185944499949983, 0.12881321999975626, 0.0974821569998312, 0.07741325499955565, 0.1363815810000233, 0.12963872300042567, 0.116529518999414, 0.15681607399983477, 0.1327583769998455, 0.22403057599967724, 0.24614654800006974, 0.10605968299933011, 0.10669969500031584, 0.12880392399983975, 0.3037896460000411, 0.10226154399970255, 0.1353642759995637, 0.12218087099972763, 0.1615457530006097, 0.11846357499962323, 0.3255610619999061, 0.17172724500051118, 0.12574780500017368, 0.1274916279999161, 0.13893959800043376, 0.417139170000155, 0.13040258399996674, 0.12939675000052375, 0.13589675099956366, 0.3814669719995436, 0.09485699800006842, 0.167689117000009, 0.10734938900077395, 0.12683278100030293, 0.1308064419999937, 0.17855357400003413, 0.13524696999957087, 0.27591882799970335, 0.11537621000024956, 0.11323194699980377, 0.2946524099997987, 0.12602699999933975, 0.20518017999984295, 0.14121414900000673, 0.1363322569995944, 0.16565463400002045, 0.08334398199986026, 0.21422826800062467, 0.1341107660000489, 0.10140826199949515, 0.10258503699969879, 0.20562904700000217, 0.31794796099984524, 0.13883807499951217, 0.15687276700009534, 0.1594079740007146, 0.1207428819998313, 0.21897738900042896, 0.1499791890000779, 0.26972336699964217, 0.13184340099996916, 0.10922606699932658, 0.19325850599943806, 0.23983448700073495, 0.11067222099973151, 0.15762250299940206, 0.09765095900002052, 0.10630853900056536, 0.16611585899954662, 0.3433312460001616, 0.18252796399974613, 0.15692070400018565, 0.1318747360000998, 0.11750317599944538, 0.1184664120000889, 0.12145284100006393, 0.12970959099948232, 0.11521425699993415, 0.1243095939998966, 0.12333640800079593, 0.1169623550003962, 0.09868338600062998, 0.12387604000014107, 0.10930602999997063, 0.11395596799957275, 0.07919201100048667, 0.12119189799977903, 0.11348222400010854, 0.2193604369995228, 0.20117800800016994, 0.11991902899990237, 0.1970362349993593, 0.12170690499988268, 0.3848852450000777, 0.13584938799976953, 0.12548418599999422, 0.1969166070002757, 0.13393962499958434, 0.12267197599976498, 0.10522404799939977, 0.06815666300008161, 0.11033271399992373, 0.07368076799957635, 0.1335441160008486, 0.5770064289999937, 0.12861590400007117, 0.14432276400020783, 0.33435338899926137, 0.14399835800031724, 0.12139116099933744, 0.21386312400045426, 0.15640015699955256, 0.11880146299972694, 0.20017386200015608, 0.16322385399962513, 0.12986015000024054, 0.14656033599931106, 0.0755562899994402, 0.12446154200006276, 0.11666824800067843, 0.12971447099971556, 0.12126529299985123, 0.13440647600054945, 0.18429755299985118, 0.1392319509996014, 0.236484458000632, 0.12455798299924936, 0.12982908400044835, 0.22299103299974377, 0.1258206730008169, 0.14303683700018155, 0.186388249000629, 0.10038375299973268, 0.10366182899997511, 0.09544742300022335, 0.06677399299951503, 0.10823251700003311, 0.1214779419997285, 0.23804526700041606, 0.11847392199979367, 0.18442734899963398, 0.11192002499956288, 0.13628551199963113, 0.19499466300021595, 0.23376740399999107, 0.12283514099999593, 0.12599423899973772, 0.34836354599974584, 0.13958765099960146, 0.20623931600039214]
[0.0032559080909157437, 0.00270943468182271, 0.002815473522725204, 0.0037740458636480153, 0.003615232977277959, 0.002037789454561789, 0.0031616195681801237, 0.0027642079545454512, 0.002461313590918673, 0.008631140136375589, 0.005682515159089069, 0.0037336627727365176, 0.0024071024090724018, 0.007755703249999483, 0.004327443886371508, 0.002782198022735181, 0.0028158922727305417, 0.003510264568190409, 0.0025175800227233676, 0.008254641113642885, 0.007791403363626159, 0.003099223227267926, 0.0032528290454675616, 0.0030948047727344438, 0.003141392159084882, 0.0031515349999980654, 0.001752595840902028, 0.002691275568167839, 0.0023331836136383736, 0.00261069695455841, 0.0027342531136169287, 0.0027720450000081405, 0.00518754213635475, 0.0028196686363654326, 0.003113891249995504, 0.003043235613631623, 0.0045845169772746885, 0.0029809963863582116, 0.0028385149545432464, 0.005298663840924498, 0.0030431717727258892, 0.0032092596363575217, 0.0019760322272759367, 0.0022657886818251427, 0.0018683884772847457, 0.0025649587727126677, 0.0033116925227328797, 0.00266691795453963, 0.006854087295447574, 0.002907993772727092, 0.0029540562954521606, 0.003131305204533419, 0.0030581872045497057, 0.003872239909097433, 0.003472999181832909, 0.0030259129318065707, 0.0027879924999855243, 0.0030073961818082103, 0.0032659626363676416, 0.008714503022722718, 0.002661595795457701, 0.0027108025908959958, 0.002255094136361632, 0.002598838409085147, 0.0019175757727225573, 0.003290547272730361, 0.003195157272722843, 0.003147567909081772, 0.0032533651590876452, 0.0026223413181841434, 0.0029506950227393895, 0.005040053204538708, 0.004799461159088672, 0.003348013863635722, 0.00505882438636201, 0.0029957865908934104, 0.003256665318179909, 0.00295436100001015, 0.006706746113629627, 0.002176390886358025, 0.002213679204549573, 0.0024360974772744985, 0.0020762412727270366, 0.008365714272734047, 0.0026501510454553763, 0.003932905340909862, 0.011358016795466856, 0.0030761854318164046, 0.003978083090921494, 0.0029300829999928365, 0.003569789090906025, 0.003136914727271935, 0.003089065818192632, 0.002757018204544114, 0.002587514227281479, 0.0035422944545395944, 0.0026467815227161655, 0.0020877146590795414, 0.0029275731818126424, 0.002215503568177982, 0.0017593921590808102, 0.0030995813863641656, 0.002946334613646038, 0.0026483981590775907, 0.0035640016818144268, 0.003087404116275477, 0.005210013395341331, 0.005724338325583017, 0.0024665042557983747, 0.0024813882558212988, 0.002995440093019529, 0.007064875488373049, 0.002378175441853548, 0.003148006418594504, 0.0028414156046448285, 0.0037568779767583648, 0.0027549668604563544, 0.007571187488369909, 0.003993656860477004, 0.002924367558143574, 0.0029649215813933975, 0.003231153441870553, 0.009700910930236162, 0.003032618232557366, 0.0030092267441982267, 0.0031603895581293878, 0.008871324930221944, 0.0022059766976760095, 0.003899746906976953, 0.0024964974186226503, 0.00294959955814658, 0.0030420102790696207, 0.0041524086976752125, 0.0031452783720830432, 0.00641671693022566, 0.0026831676744244084, 0.0026333010930186924, 0.0068523816279022955, 0.0029308604651009243, 0.004771632093019604, 0.0032840499767443423, 0.00317051760464173, 0.003852433348837685, 0.001938232139531634, 0.004982052744200573, 0.0031188550232569513, 0.002358331674406864, 0.002385698534876716, 0.004782070860465167, 0.0073941386279033775, 0.0032287924418491205, 0.003648203883723147, 0.00370716218606313, 0.002807973999996077, 0.005092497418614627, 0.0034878881162808814, 0.006272636441852143, 0.0030661256046504454, 0.0025401410930075948, 0.0044943838604520475, 0.005577546209319418, 0.002573772581389105, 0.003665639604637257, 0.0022709525348841983, 0.0024722916046643105, 0.003863159511617363, 0.007984447581399108, 0.004244836372087119, 0.003649318697678736, 0.003066854325583716, 0.002732631999987102, 0.00275503283721137, 0.0028244846744200913, 0.0030165021162670306, 0.002679401325579864, 0.00289092079069527, 0.0028682885581580446, 0.0027200547674510744, 0.0022949624651309296, 0.0028808381395381646, 0.0025420006976737356, 0.002650138790687738, 0.0018416746744299227, 0.0028184162325530005, 0.002639121488374617, 0.005101405511616809, 0.004678558325585347, 0.0027888146279047062, 0.004582238023240914, 0.0028303931395321554, 0.008950819651164598, 0.003159288093017896, 0.002918236883720796, 0.004579455976750598, 0.0031148749999903333, 0.002852836651157325, 0.0024470708837069713, 0.0015850386744205025, 0.002565877069765668, 0.001713506232548287, 0.0031056771162988046, 0.01341875416279055, 0.002991067534885376, 0.0033563433488420424, 0.0077756602092851484, 0.0033487990232631917, 0.0028230502557985452, 0.004973561023266378, 0.0036372129534779666, 0.0027628247209238823, 0.004655206093026886, 0.003795903581386631, 0.003020003488377687, 0.0034083799069607222, 0.0017571230232427953, 0.002894454465117739, 0.0027132150697832193, 0.003016615604644548, 0.0028201230930197958, 0.003125732000012778, 0.004285989604647702, 0.0032379523488279393, 0.005499638558154232, 0.002896697279052311, 0.0030192810232662405, 0.005185837976738227, 0.002926062162809695, 0.003326438069771664, 0.004334610441875093, 0.0023345058837147136, 0.0024107402093017467, 0.0022197075116331014, 0.0015528835581282566, 0.0025170352790705374, 0.002825068418598337, 0.005535936441870141, 0.002755207488367295, 0.004289008116270558, 0.002602791279059602, 0.0031694305116193285, 0.004534759604656185, 0.005436451255813746, 0.002856631186046417, 0.002930098581389249, 0.008101477813947577, 0.003246224441851197, 0.004796263162799817]
[307.1339767821099, 369.0806819256012, 355.1800405610144, 264.96763317905044, 276.60734627203374, 490.72783145550346, 316.2935889138665, 361.7672825069491, 406.28711582694143, 115.85954858797167, 175.97841308008128, 267.83350850592996, 415.43724780091884, 128.93737263607483, 231.08329680468344, 359.42804639653195, 355.1272219055135, 284.8788120023426, 397.2068379054977, 121.14397055339532, 128.34658319301735, 322.6614950487239, 307.4247020123555, 323.12215904864354, 318.33020182087347, 317.3056938922188, 570.5822053562097, 371.57101704036126, 428.5989298718745, 383.03947850168856, 365.7305883716005, 360.744504507345, 192.76951853401098, 354.65160235601485, 321.14159413930844, 328.597626657851, 218.12548736474739, 335.45830668438623, 352.29689327492474, 188.72682435078244, 328.6045201136512, 311.5983476908681, 506.06462090881587, 441.3474248597967, 535.220599012289, 389.86981414224164, 301.9604003498424, 374.96466597249423, 145.89834603714363, 343.8796910016105, 338.5175839538074, 319.35564714427295, 326.9911006469083, 258.24846173673325, 287.9355702791269, 330.4787753436669, 358.6810222786439, 332.5135564276555, 306.1884385524343, 114.75123680519016, 375.7144498449416, 368.89443862803455, 443.44046834931675, 384.78729439435364, 521.4917784345016, 303.9008156142492, 312.9736393688758, 317.7056155372121, 307.37404229177713, 381.3386125847478, 338.9032049376666, 198.41060389986998, 208.35672315137174, 298.68454574261114, 197.67438511917535, 333.80214833720106, 307.0625631739407, 338.4826701938472, 149.10360151665404, 459.4762853806106, 451.7366373342584, 410.49260521331774, 481.63959224572733, 119.53551931115433, 377.3369830051214, 254.26495512059637, 88.043539467129, 325.07793244750104, 251.37735365109162, 341.2872604641045, 280.1285943047679, 318.784566027928, 323.7224646074669, 362.71069895432726, 386.47130495225537, 282.3029007987914, 377.8173572005995, 478.9926610185766, 341.57984716229635, 451.36465332908244, 568.3781156115003, 322.62421125615555, 339.4047625712537, 377.5867297643378, 280.58348151252886, 323.8966984362128, 191.93808616580066, 174.69267941952944, 405.43209996461707, 403.00021476043315, 333.8407609387234, 141.5453112578899, 420.4904240456713, 317.6613599302862, 351.93725210958644, 266.17846152747643, 362.980772782999, 132.07967726807698, 250.39707589714143, 341.9542790424104, 337.2770484978692, 309.48700456054115, 103.08310293656676, 329.74806695556714, 332.31128293273173, 316.41668902105, 112.72273396201506, 453.3139452712702, 256.4268973996547, 400.5611992788332, 339.02907167112653, 328.72998716685584, 240.82407893998123, 317.93688243172056, 155.84293508250997, 372.6938161680557, 379.75148480026155, 145.93466247240053, 341.1967276871248, 209.57189919627186, 304.5020651577764, 315.40591307109315, 259.57619754841687, 515.9340718814238, 200.72047634663517, 320.6304854002871, 424.0285668263802, 419.1644440321863, 209.11442535645048, 135.24225745866926, 309.71331171331127, 274.1074873752553, 269.7481118466962, 356.12865361338714, 196.36730621496162, 286.70644431860256, 159.42259833964272, 326.14449926098354, 393.67891915640536, 222.49990900853302, 179.2903119886517, 388.534716404619, 272.8036871750674, 440.34385775966587, 404.48303028387323, 258.8554774900653, 125.24347987826239, 235.5803409939959, 274.0237515117771, 326.0670034627971, 365.9475553256787, 362.97208022108185, 354.0468847490967, 331.50979560309935, 373.2176999589953, 345.9105497523849, 348.63995714649405, 367.63965636511284, 435.73697400011685, 347.1212027761868, 393.3909227149824, 377.3387278862062, 542.9840643866936, 354.8091968992713, 378.91396982102566, 196.02440890511878, 213.7410566266452, 358.57528499530304, 218.23397102639433, 353.3078094463207, 111.7216119833103, 316.52700562826936, 342.67266155754464, 218.36654944973634, 321.04017015228646, 350.52830648201495, 408.6518321386503, 630.8994323848941, 389.7302843473036, 583.598694305782, 321.99097412668306, 74.52256654145611, 334.32879342803676, 297.94329604121276, 128.6064428080165, 298.6145161454221, 354.2267793306194, 201.06318095263896, 274.93578539133443, 361.9484046261184, 214.813260684187, 263.4418863807661, 331.1254453342334, 293.3945238785624, 569.1121149585109, 345.4882472850796, 368.5664329145489, 331.49732384210466, 354.59445102773765, 319.9250607524612, 233.3183447098439, 308.83715764439086, 181.83013109421796, 345.22074751530886, 331.20467829728744, 192.83286606439196, 341.7562390539814, 300.6218600873104, 230.7012391100626, 428.3561703467543, 414.81035415659437, 450.50980580061736, 643.9632867291956, 397.29280249471464, 353.9737280048431, 180.6379120317685, 362.94907161151366, 233.15414027929958, 384.20291632500926, 315.5140951454648, 220.5188559440336, 183.9435236231722, 350.06269093631295, 341.2854455995366, 123.43427001409434, 308.0501727199548, 208.49564881178253]
Elapsed: 0.1573994519230432~0.07464881715190984
Time per graph: 0.0036258229379972674~0.0017218242198674676
Speed: 317.37020263648304~100.90856895854444
Total Time: 0.2071
best val loss: 0.3829653784632683 test_score: 0.7907

Testing...
Test loss: 0.6111 score: 0.7907 time: 0.19s
test Score 0.7907
Epoch Time List: [0.6591209390007862, 0.7469383420002487, 0.7564135829998122, 0.7843028839997714, 0.763782168999569, 0.7146352580002713, 1.037606305000736, 0.9628248450007959, 0.4766330639995431, 0.6860566949990243, 0.5737096459997701, 0.4309524059999603, 0.4986140510000041, 0.7488338199991631, 0.7028720979997161, 0.9397899570003574, 0.915337421998629, 0.8537459719991602, 0.5638187200001994, 0.7864816629999041, 0.9457079809999414, 0.710854855999969, 0.8399142110019966, 0.7508033539988901, 0.5656874329988568, 0.5700495099999898, 0.6276187240000581, 0.36652619899905403, 0.617627087000983, 0.411494037999546, 0.7438824000009845, 0.9335121049989539, 0.6989594409997153, 0.6809088669988341, 0.9616808219998347, 0.7794582759988771, 0.7079063539995332, 0.5363676620008846, 1.0437001249993045, 0.7480708989996856, 0.6832800610009144, 0.7273809689995687, 0.6036545269998896, 0.551956098998744, 0.6511637040002825, 0.4591628859998309, 0.7891358549995857, 0.5141963920004855, 0.7616654079993168, 0.7129683260000093, 0.9043226100002357, 0.7235869870009992, 0.615709453000818, 0.7638940890001322, 0.828580256999885, 0.5710702659989693, 0.5403754879998814, 0.8898024430000078, 0.6971760589995029, 1.0530343030004587, 0.48715270299999247, 0.5748706270005641, 0.3563839549997283, 0.4495671209988359, 0.6020067160006874, 0.7359857859992189, 0.5422037729986187, 0.6156179939998765, 0.9129159259991866, 0.5417944440005158, 0.6102547730006336, 0.6919471689989223, 0.9765997510003217, 0.5890723420006907, 0.8569402319999426, 1.004507895999268, 0.6900182370000039, 0.529685029000575, 0.71918204999929, 0.4308176460008326, 0.468032095000126, 0.4884422880004422, 0.4665299700000105, 1.1457956490003198, 0.5042514060005487, 0.712683217000631, 1.166385387999071, 0.5362926529996912, 0.6193782079999437, 0.9168944079992798, 0.8300678109999353, 0.7060676190003505, 0.8570891790013775, 0.5955233249997036, 0.8669288319997577, 0.4709742349996304, 0.6130862799991519, 0.4542953950003721, 0.6382253950005179, 0.4162797099988893, 0.548656320000191, 0.6358789089999846, 0.6546853810004905, 0.9299482120004541, 0.6625302710008327, 1.0147791359995608, 0.6791533709993018, 0.8153370810005072, 0.8262575599992488, 0.5235213879996081, 0.552325975999338, 0.7190860339997016, 0.4905170989995895, 0.523346832000243, 0.5700461390015334, 0.7719431850000547, 1.1209817540002405, 0.8275421939997614, 0.5943990789983218, 0.755979861999549, 0.8475440670008538, 0.6808355289995234, 0.9091629069998817, 0.8189999239993995, 0.6141615009992165, 0.5578556119999121, 0.8153116099992985, 0.5363101679995452, 0.5551445909995891, 0.4893550619990492, 0.44955539599959593, 0.9793937200001892, 0.7275783170007344, 0.6724765700000717, 1.077069307999409, 0.6335819360010646, 0.5894009929997992, 0.7463299989985899, 0.5757175930002632, 0.7048896060005063, 0.6966170139994574, 0.7540513380008633, 0.7092690190002031, 0.578936975999568, 0.6485942050003359, 0.7072486420001951, 0.538353633000952, 0.5223413779995099, 0.6177196670005287, 0.7533422100004827, 0.7172287489993323, 0.677761713000109, 0.6308615420002752, 0.9215227300001061, 0.6799469699999463, 0.7008373649987334, 0.7591818490000151, 0.9393872529990404, 0.5554260459994111, 0.703267949000292, 0.7334165199999916, 0.7071817249998276, 0.5901488290000998, 0.3959178730010535, 0.5629749580002681, 0.7054362589997254, 0.81779778300006, 0.6644461730011244, 0.6886592870014283, 0.9996088530006091, 0.5925122269991334, 0.6840484239992293, 1.0881844259993159, 0.5918394080008511, 0.672049670000888, 1.0238414100003865, 0.820349472000089, 0.7240935840009115, 0.8916660369995952, 0.5407496560001164, 0.41179664799983584, 0.5298989469993103, 0.5223505939984534, 0.68668378700022, 0.9372951320001448, 0.7795443750001141, 0.7486668840001585, 0.9203434809996907, 1.0430403550008123, 0.4851859490008792, 1.1650015219993293, 0.7233047600002465, 0.5295723190001809, 0.7312043060001088, 0.8995461710001109, 0.7353265480005575, 0.5695828349998919, 0.8292970029997377, 0.5730359279996264, 0.46068595799988543, 0.6193649609995191, 1.0364393390009354, 0.8656225810009346, 0.6503617869993832, 1.0557705900000656, 0.6925231849991178, 0.5859904879989699, 0.8028223460014488, 0.9658262129996729, 0.605744799000604, 0.8289523589992314, 0.7228263980005067, 0.5627481940000507, 0.6725089760011542, 0.42749133800043637, 0.8050089200005459, 0.41856826300136163, 0.761000944999978, 0.6657579690008788, 0.8804332700001396, 0.6316537889997562, 0.6146444639998663, 0.8592132829999173, 0.9631716640005834, 0.6388677580007425, 0.708773583999573, 0.6743730849993881, 0.7948492490004355, 0.6412495649992707, 0.6352686150003137, 0.513404571999672, 0.7207414769991374, 0.36446164200060593, 0.5192260300000271, 0.5630828909997945, 0.6521392940003352, 0.8156695890002084, 0.6969412299995383, 0.5115284630001042, 0.6808779570001207, 0.8099008280005364, 0.7671617460009656, 0.5859309870002107, 0.6111885199998142, 0.8667482470000323, 0.7988274020008248, 0.677208778000022]
Total Epoch List: [105, 142]
Total Time List: [0.15748658400025306, 0.2071282719998635]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f85139a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7096 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7046 score: 0.5116 time: 0.29s
Epoch 2/1000, LR 0.000015
Train loss: 0.7115;  Loss pred: 0.7115; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7090 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7036 score: 0.5116 time: 0.19s
Epoch 3/1000, LR 0.000045
Train loss: 0.7337;  Loss pred: 0.7337; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7084 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7024 score: 0.5116 time: 0.16s
Epoch 4/1000, LR 0.000075
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7080 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000105
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7080 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.5116 time: 0.12s
Epoch 6/1000, LR 0.000135
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7089 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6989 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7101 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7114 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5116 time: 0.49s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7153 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6350;  Loss pred: 0.6350; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7209 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 5 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5886;  Loss pred: 0.5886; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7267 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7043 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6297;  Loss pred: 0.6297; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7301 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7058 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.6177;  Loss pred: 0.6177; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7335 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7071 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7356 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7071 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.6148;  Loss pred: 0.6148; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7350 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7046 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7335 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5654;  Loss pred: 0.5654; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7320 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5809;  Loss pred: 0.5809; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7282 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 13 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5486;  Loss pred: 0.5486; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7251 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7210 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6806 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 15 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7170 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6744 score: 0.5116 time: 0.28s
     INFO: Early stopping counter 16 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7138 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6689 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 17 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5024;  Loss pred: 0.5024; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7083 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6610 score: 0.5116 time: 0.18s
     INFO: Early stopping counter 18 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.5528;  Loss pred: 0.5528; Loss self: 0.0000; time: 0.41s
Val loss: 0.7030 score: 0.5227 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6530 score: 0.5116 time: 0.11s
Epoch 25/1000, LR 0.000285
Train loss: 0.5056;  Loss pred: 0.5056; Loss self: 0.0000; time: 0.29s
Val loss: 0.6951 score: 0.5227 time: 0.17s
Test loss: 0.6432 score: 0.5581 time: 0.13s
Epoch 26/1000, LR 0.000285
Train loss: 0.5155;  Loss pred: 0.5155; Loss self: 0.0000; time: 0.55s
Val loss: 0.6887 score: 0.5227 time: 0.11s
Test loss: 0.6349 score: 0.5581 time: 0.17s
Epoch 27/1000, LR 0.000285
Train loss: 0.4826;  Loss pred: 0.4826; Loss self: 0.0000; time: 0.29s
Val loss: 0.6827 score: 0.5227 time: 0.30s
Test loss: 0.6269 score: 0.5581 time: 0.17s
Epoch 28/1000, LR 0.000285
Train loss: 0.4840;  Loss pred: 0.4840; Loss self: 0.0000; time: 0.27s
Val loss: 0.6780 score: 0.5455 time: 0.16s
Test loss: 0.6201 score: 0.5581 time: 0.11s
Epoch 29/1000, LR 0.000285
Train loss: 0.4692;  Loss pred: 0.4692; Loss self: 0.0000; time: 0.55s
Val loss: 0.6733 score: 0.5455 time: 0.11s
Test loss: 0.6133 score: 0.5581 time: 0.21s
Epoch 30/1000, LR 0.000285
Train loss: 0.4708;  Loss pred: 0.4708; Loss self: 0.0000; time: 0.33s
Val loss: 0.6688 score: 0.5682 time: 0.12s
Test loss: 0.6068 score: 0.5814 time: 0.11s
Epoch 31/1000, LR 0.000285
Train loss: 0.4835;  Loss pred: 0.4835; Loss self: 0.0000; time: 0.54s
Val loss: 0.6650 score: 0.5909 time: 0.13s
Test loss: 0.6005 score: 0.5814 time: 0.17s
Epoch 32/1000, LR 0.000285
Train loss: 0.4388;  Loss pred: 0.4388; Loss self: 0.0000; time: 0.30s
Val loss: 0.6623 score: 0.5909 time: 0.08s
Test loss: 0.5960 score: 0.5814 time: 0.11s
Epoch 33/1000, LR 0.000285
Train loss: 0.4408;  Loss pred: 0.4408; Loss self: 0.0000; time: 0.64s
Val loss: 0.6590 score: 0.5909 time: 0.09s
Test loss: 0.5913 score: 0.5814 time: 0.09s
Epoch 34/1000, LR 0.000285
Train loss: 0.4482;  Loss pred: 0.4482; Loss self: 0.0000; time: 0.32s
Val loss: 0.6553 score: 0.5909 time: 0.15s
Test loss: 0.5859 score: 0.5814 time: 0.11s
Epoch 35/1000, LR 0.000285
Train loss: 0.4428;  Loss pred: 0.4428; Loss self: 0.0000; time: 0.31s
Val loss: 0.6519 score: 0.6136 time: 0.09s
Test loss: 0.5804 score: 0.5814 time: 0.11s
Epoch 36/1000, LR 0.000285
Train loss: 0.4307;  Loss pred: 0.4307; Loss self: 0.0000; time: 0.23s
Val loss: 0.6487 score: 0.6136 time: 0.19s
Test loss: 0.5747 score: 0.5814 time: 0.13s
Epoch 37/1000, LR 0.000285
Train loss: 0.4086;  Loss pred: 0.4086; Loss self: 0.0000; time: 0.31s
Val loss: 0.6448 score: 0.6364 time: 0.12s
Test loss: 0.5692 score: 0.5814 time: 0.40s
Epoch 38/1000, LR 0.000284
Train loss: 0.4034;  Loss pred: 0.4034; Loss self: 0.0000; time: 0.51s
Val loss: 0.6429 score: 0.6364 time: 0.20s
Test loss: 0.5647 score: 0.6047 time: 0.14s
Epoch 39/1000, LR 0.000284
Train loss: 0.4113;  Loss pred: 0.4113; Loss self: 0.0000; time: 0.32s
Val loss: 0.6405 score: 0.6364 time: 0.14s
Test loss: 0.5603 score: 0.6279 time: 0.12s
Epoch 40/1000, LR 0.000284
Train loss: 0.4080;  Loss pred: 0.4080; Loss self: 0.0000; time: 0.37s
Val loss: 0.6396 score: 0.6364 time: 0.12s
Test loss: 0.5568 score: 0.6279 time: 0.16s
Epoch 41/1000, LR 0.000284
Train loss: 0.3763;  Loss pred: 0.3763; Loss self: 0.0000; time: 0.48s
Val loss: 0.6382 score: 0.6364 time: 0.11s
Test loss: 0.5527 score: 0.6279 time: 0.32s
Epoch 42/1000, LR 0.000284
Train loss: 0.3926;  Loss pred: 0.3926; Loss self: 0.0000; time: 0.37s
Val loss: 0.6361 score: 0.6364 time: 0.11s
Test loss: 0.5472 score: 0.6279 time: 0.14s
Epoch 43/1000, LR 0.000284
Train loss: 0.3933;  Loss pred: 0.3933; Loss self: 0.0000; time: 0.45s
Val loss: 0.6350 score: 0.6364 time: 0.15s
Test loss: 0.5420 score: 0.6512 time: 0.11s
Epoch 44/1000, LR 0.000284
Train loss: 0.3757;  Loss pred: 0.3757; Loss self: 0.0000; time: 0.36s
Val loss: 0.6355 score: 0.6364 time: 0.28s
Test loss: 0.5385 score: 0.6512 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 45/1000, LR 0.000284
Train loss: 0.3628;  Loss pred: 0.3628; Loss self: 0.0000; time: 0.34s
Val loss: 0.6367 score: 0.6364 time: 0.46s
Test loss: 0.5354 score: 0.6512 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.3591;  Loss pred: 0.3591; Loss self: 0.0000; time: 0.46s
Val loss: 0.6393 score: 0.6364 time: 0.11s
Test loss: 0.5329 score: 0.6512 time: 0.21s
     INFO: Early stopping counter 3 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.3604;  Loss pred: 0.3604; Loss self: 0.0000; time: 0.43s
Val loss: 0.6398 score: 0.6364 time: 0.11s
Test loss: 0.5288 score: 0.6512 time: 0.48s
     INFO: Early stopping counter 4 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.3459;  Loss pred: 0.3459; Loss self: 0.0000; time: 0.35s
Val loss: 0.6445 score: 0.6364 time: 0.35s
Test loss: 0.5277 score: 0.6744 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.3554;  Loss pred: 0.3554; Loss self: 0.0000; time: 0.30s
Val loss: 0.6461 score: 0.6364 time: 0.11s
Test loss: 0.5254 score: 0.6744 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.3288;  Loss pred: 0.3288; Loss self: 0.0000; time: 0.30s
Val loss: 0.6505 score: 0.6364 time: 0.10s
Test loss: 0.5253 score: 0.6977 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.3250;  Loss pred: 0.3250; Loss self: 0.0000; time: 0.22s
Val loss: 0.6556 score: 0.6364 time: 0.11s
Test loss: 0.5250 score: 0.6977 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.3376;  Loss pred: 0.3376; Loss self: 0.0000; time: 0.33s
Val loss: 0.6617 score: 0.6364 time: 0.08s
Test loss: 0.5255 score: 0.6977 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.3312;  Loss pred: 0.3312; Loss self: 0.0000; time: 0.27s
Val loss: 0.6660 score: 0.6364 time: 0.12s
Test loss: 0.5243 score: 0.6977 time: 0.13s
     INFO: Early stopping counter 10 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.3244;  Loss pred: 0.3244; Loss self: 0.0000; time: 0.57s
Val loss: 0.6702 score: 0.6364 time: 0.10s
Test loss: 0.5232 score: 0.6977 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.3337;  Loss pred: 0.3337; Loss self: 0.0000; time: 0.36s
Val loss: 0.6751 score: 0.6364 time: 0.11s
Test loss: 0.5232 score: 0.6977 time: 0.13s
     INFO: Early stopping counter 12 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.3142;  Loss pred: 0.3142; Loss self: 0.0000; time: 0.32s
Val loss: 0.6768 score: 0.6364 time: 0.12s
Test loss: 0.5211 score: 0.6977 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.3010;  Loss pred: 0.3010; Loss self: 0.0000; time: 0.31s
Val loss: 0.6768 score: 0.6364 time: 0.10s
Test loss: 0.5174 score: 0.6977 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.3202;  Loss pred: 0.3202; Loss self: 0.0000; time: 0.41s
Val loss: 0.6782 score: 0.6364 time: 0.32s
Test loss: 0.5155 score: 0.7442 time: 0.16s
     INFO: Early stopping counter 15 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.3007;  Loss pred: 0.3007; Loss self: 0.0000; time: 0.40s
Val loss: 0.6780 score: 0.6364 time: 0.11s
Test loss: 0.5132 score: 0.7442 time: 0.25s
     INFO: Early stopping counter 16 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.3259;  Loss pred: 0.3259; Loss self: 0.0000; time: 0.30s
Val loss: 0.6783 score: 0.6364 time: 0.11s
Test loss: 0.5111 score: 0.7442 time: 0.13s
     INFO: Early stopping counter 17 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.3031;  Loss pred: 0.3031; Loss self: 0.0000; time: 0.30s
Val loss: 0.6778 score: 0.6364 time: 0.13s
Test loss: 0.5100 score: 0.7442 time: 0.19s
     INFO: Early stopping counter 18 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.2932;  Loss pred: 0.2932; Loss self: 0.0000; time: 0.58s
Val loss: 0.6746 score: 0.6591 time: 0.10s
Test loss: 0.5054 score: 0.7442 time: 0.17s
     INFO: Early stopping counter 19 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.2950;  Loss pred: 0.2950; Loss self: 0.0000; time: 0.41s
Val loss: 0.6750 score: 0.6364 time: 0.11s
Test loss: 0.5030 score: 0.7674 time: 0.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 042,   Train_Loss: 0.3933,   Val_Loss: 0.6350,   Val_Precision: 1.0000,   Val_Recall: 0.2727,   Val_accuracy: 0.4286,   Val_Score: 0.6364,   Val_Loss: 0.6350,   Test_Precision: 1.0000,   Test_Recall: 0.2857,   Test_accuracy: 0.4444,   Test_Score: 0.6512,   Test_loss: 0.5420


[0.14325995600029273, 0.11921512600019923, 0.12388083499990898, 0.16605801800051267, 0.1590702510002302, 0.08966273600071872, 0.13911126099992543, 0.12162514999999985, 0.10829779800042161, 0.37977016600052593, 0.25003066699991905, 0.16428116200040677, 0.10591250599918567, 0.34125094299997727, 0.19040753100034635, 0.12241671300034795, 0.12389926000014384, 0.154451641000378, 0.11077352099982818, 0.36320420900028694, 0.342821747999551, 0.13636582199978875, 0.1431244780005727, 0.13617141000031552, 0.1382212549997348, 0.13866753999991488, 0.07711421699968923, 0.11841612499938492, 0.10266007900008844, 0.11487066600057005, 0.12030713699914486, 0.12196998000035819, 0.228251853999609, 0.12406542000007903, 0.13701121499980218, 0.1339023669997914, 0.2017187470000863, 0.1311638409997613, 0.12489465799990285, 0.23314120900067792, 0.13389955799993913, 0.14120742399973096, 0.08694541800014122, 0.09969470200030628, 0.08220909300052881, 0.11285818599935737, 0.1457144710002467, 0.11734438999974373, 0.30157984099969326, 0.12795172599999205, 0.12997847699989507, 0.13777742899947043, 0.13456023700018704, 0.17037855600028706, 0.152811964000648, 0.1331401689994891, 0.12267166999936308, 0.13232543199956126, 0.14370235600017622, 0.38343813299979956, 0.11711021500013885, 0.11927531399942382, 0.09922414199991181, 0.11434888999974646, 0.08437333399979252, 0.14478408000013587, 0.1405869199998051, 0.13849298799959797, 0.14314806699985638, 0.11538301800010231, 0.12983058100053313, 0.22176234099970316, 0.21117629099990154, 0.14731260999997176, 0.22258827299992845, 0.13181460999931005, 0.14329327399991598, 0.1299918840004466, 0.2950968289997036, 0.0957611989997531, 0.09740188500018121, 0.10718828900007793, 0.09135461599998962, 0.36809142800029804, 0.11660664600003656, 0.17304783500003396, 0.4997527390005416, 0.1353521589999218, 0.17503565600054571, 0.1289236519996848, 0.1570707199998651, 0.13802424799996516, 0.1359188960004758, 0.12130880099994101, 0.11385062600038509, 0.15586095599974215, 0.11645838699951128, 0.09185944499949983, 0.12881321999975626, 0.0974821569998312, 0.07741325499955565, 0.1363815810000233, 0.12963872300042567, 0.116529518999414, 0.15681607399983477, 0.1327583769998455, 0.22403057599967724, 0.24614654800006974, 0.10605968299933011, 0.10669969500031584, 0.12880392399983975, 0.3037896460000411, 0.10226154399970255, 0.1353642759995637, 0.12218087099972763, 0.1615457530006097, 0.11846357499962323, 0.3255610619999061, 0.17172724500051118, 0.12574780500017368, 0.1274916279999161, 0.13893959800043376, 0.417139170000155, 0.13040258399996674, 0.12939675000052375, 0.13589675099956366, 0.3814669719995436, 0.09485699800006842, 0.167689117000009, 0.10734938900077395, 0.12683278100030293, 0.1308064419999937, 0.17855357400003413, 0.13524696999957087, 0.27591882799970335, 0.11537621000024956, 0.11323194699980377, 0.2946524099997987, 0.12602699999933975, 0.20518017999984295, 0.14121414900000673, 0.1363322569995944, 0.16565463400002045, 0.08334398199986026, 0.21422826800062467, 0.1341107660000489, 0.10140826199949515, 0.10258503699969879, 0.20562904700000217, 0.31794796099984524, 0.13883807499951217, 0.15687276700009534, 0.1594079740007146, 0.1207428819998313, 0.21897738900042896, 0.1499791890000779, 0.26972336699964217, 0.13184340099996916, 0.10922606699932658, 0.19325850599943806, 0.23983448700073495, 0.11067222099973151, 0.15762250299940206, 0.09765095900002052, 0.10630853900056536, 0.16611585899954662, 0.3433312460001616, 0.18252796399974613, 0.15692070400018565, 0.1318747360000998, 0.11750317599944538, 0.1184664120000889, 0.12145284100006393, 0.12970959099948232, 0.11521425699993415, 0.1243095939998966, 0.12333640800079593, 0.1169623550003962, 0.09868338600062998, 0.12387604000014107, 0.10930602999997063, 0.11395596799957275, 0.07919201100048667, 0.12119189799977903, 0.11348222400010854, 0.2193604369995228, 0.20117800800016994, 0.11991902899990237, 0.1970362349993593, 0.12170690499988268, 0.3848852450000777, 0.13584938799976953, 0.12548418599999422, 0.1969166070002757, 0.13393962499958434, 0.12267197599976498, 0.10522404799939977, 0.06815666300008161, 0.11033271399992373, 0.07368076799957635, 0.1335441160008486, 0.5770064289999937, 0.12861590400007117, 0.14432276400020783, 0.33435338899926137, 0.14399835800031724, 0.12139116099933744, 0.21386312400045426, 0.15640015699955256, 0.11880146299972694, 0.20017386200015608, 0.16322385399962513, 0.12986015000024054, 0.14656033599931106, 0.0755562899994402, 0.12446154200006276, 0.11666824800067843, 0.12971447099971556, 0.12126529299985123, 0.13440647600054945, 0.18429755299985118, 0.1392319509996014, 0.236484458000632, 0.12455798299924936, 0.12982908400044835, 0.22299103299974377, 0.1258206730008169, 0.14303683700018155, 0.186388249000629, 0.10038375299973268, 0.10366182899997511, 0.09544742300022335, 0.06677399299951503, 0.10823251700003311, 0.1214779419997285, 0.23804526700041606, 0.11847392199979367, 0.18442734899963398, 0.11192002499956288, 0.13628551199963113, 0.19499466300021595, 0.23376740399999107, 0.12283514099999593, 0.12599423899973772, 0.34836354599974584, 0.13958765099960146, 0.20623931600039214, 0.2938632309997047, 0.19547774300008314, 0.17504688000008173, 0.25200328800019633, 0.12185342499924445, 0.13371616699987499, 0.14558056599980773, 0.4961702480004533, 0.1461139250004635, 0.11787108299995452, 0.148883921999186, 0.12747206999938498, 0.12764038700061064, 0.13657380800032115, 0.11073941599988757, 0.0839564630005043, 0.1199105210007474, 0.11216382699967653, 0.13864678600020852, 0.13116749599976174, 0.2861192870004743, 0.13075476199992409, 0.1868992480003726, 0.11903886599975522, 0.13595778299986705, 0.17130486800033395, 0.17667542500021227, 0.11635918499996478, 0.21218907200000103, 0.12206601500020042, 0.17952441499983252, 0.11521687100048439, 0.10037194800042926, 0.11753439300082391, 0.11095250299968029, 0.13292253500003426, 0.4125209840003663, 0.14763805999973556, 0.12782013499963796, 0.16531658800067817, 0.32776473600006284, 0.14478660199984006, 0.11942999500024598, 0.14926306800043676, 0.12824548199932906, 0.2131732709995049, 0.48862019399985, 0.12555298200004472, 0.11756827299996075, 0.20746340000005148, 0.11281115200017666, 0.10005372200066631, 0.1419290470003034, 0.12640002500029368, 0.13633364099950995, 0.15638996500001667, 0.2009496360005869, 0.16156134999982896, 0.2557545600002413, 0.13284871199994086, 0.19701032400007534, 0.177065448000576, 0.12873076600044442]
[0.0032559080909157437, 0.00270943468182271, 0.002815473522725204, 0.0037740458636480153, 0.003615232977277959, 0.002037789454561789, 0.0031616195681801237, 0.0027642079545454512, 0.002461313590918673, 0.008631140136375589, 0.005682515159089069, 0.0037336627727365176, 0.0024071024090724018, 0.007755703249999483, 0.004327443886371508, 0.002782198022735181, 0.0028158922727305417, 0.003510264568190409, 0.0025175800227233676, 0.008254641113642885, 0.007791403363626159, 0.003099223227267926, 0.0032528290454675616, 0.0030948047727344438, 0.003141392159084882, 0.0031515349999980654, 0.001752595840902028, 0.002691275568167839, 0.0023331836136383736, 0.00261069695455841, 0.0027342531136169287, 0.0027720450000081405, 0.00518754213635475, 0.0028196686363654326, 0.003113891249995504, 0.003043235613631623, 0.0045845169772746885, 0.0029809963863582116, 0.0028385149545432464, 0.005298663840924498, 0.0030431717727258892, 0.0032092596363575217, 0.0019760322272759367, 0.0022657886818251427, 0.0018683884772847457, 0.0025649587727126677, 0.0033116925227328797, 0.00266691795453963, 0.006854087295447574, 0.002907993772727092, 0.0029540562954521606, 0.003131305204533419, 0.0030581872045497057, 0.003872239909097433, 0.003472999181832909, 0.0030259129318065707, 0.0027879924999855243, 0.0030073961818082103, 0.0032659626363676416, 0.008714503022722718, 0.002661595795457701, 0.0027108025908959958, 0.002255094136361632, 0.002598838409085147, 0.0019175757727225573, 0.003290547272730361, 0.003195157272722843, 0.003147567909081772, 0.0032533651590876452, 0.0026223413181841434, 0.0029506950227393895, 0.005040053204538708, 0.004799461159088672, 0.003348013863635722, 0.00505882438636201, 0.0029957865908934104, 0.003256665318179909, 0.00295436100001015, 0.006706746113629627, 0.002176390886358025, 0.002213679204549573, 0.0024360974772744985, 0.0020762412727270366, 0.008365714272734047, 0.0026501510454553763, 0.003932905340909862, 0.011358016795466856, 0.0030761854318164046, 0.003978083090921494, 0.0029300829999928365, 0.003569789090906025, 0.003136914727271935, 0.003089065818192632, 0.002757018204544114, 0.002587514227281479, 0.0035422944545395944, 0.0026467815227161655, 0.0020877146590795414, 0.0029275731818126424, 0.002215503568177982, 0.0017593921590808102, 0.0030995813863641656, 0.002946334613646038, 0.0026483981590775907, 0.0035640016818144268, 0.003087404116275477, 0.005210013395341331, 0.005724338325583017, 0.0024665042557983747, 0.0024813882558212988, 0.002995440093019529, 0.007064875488373049, 0.002378175441853548, 0.003148006418594504, 0.0028414156046448285, 0.0037568779767583648, 0.0027549668604563544, 0.007571187488369909, 0.003993656860477004, 0.002924367558143574, 0.0029649215813933975, 0.003231153441870553, 0.009700910930236162, 0.003032618232557366, 0.0030092267441982267, 0.0031603895581293878, 0.008871324930221944, 0.0022059766976760095, 0.003899746906976953, 0.0024964974186226503, 0.00294959955814658, 0.0030420102790696207, 0.0041524086976752125, 0.0031452783720830432, 0.00641671693022566, 0.0026831676744244084, 0.0026333010930186924, 0.0068523816279022955, 0.0029308604651009243, 0.004771632093019604, 0.0032840499767443423, 0.00317051760464173, 0.003852433348837685, 0.001938232139531634, 0.004982052744200573, 0.0031188550232569513, 0.002358331674406864, 0.002385698534876716, 0.004782070860465167, 0.0073941386279033775, 0.0032287924418491205, 0.003648203883723147, 0.00370716218606313, 0.002807973999996077, 0.005092497418614627, 0.0034878881162808814, 0.006272636441852143, 0.0030661256046504454, 0.0025401410930075948, 0.0044943838604520475, 0.005577546209319418, 0.002573772581389105, 0.003665639604637257, 0.0022709525348841983, 0.0024722916046643105, 0.003863159511617363, 0.007984447581399108, 0.004244836372087119, 0.003649318697678736, 0.003066854325583716, 0.002732631999987102, 0.00275503283721137, 0.0028244846744200913, 0.0030165021162670306, 0.002679401325579864, 0.00289092079069527, 0.0028682885581580446, 0.0027200547674510744, 0.0022949624651309296, 0.0028808381395381646, 0.0025420006976737356, 0.002650138790687738, 0.0018416746744299227, 0.0028184162325530005, 0.002639121488374617, 0.005101405511616809, 0.004678558325585347, 0.0027888146279047062, 0.004582238023240914, 0.0028303931395321554, 0.008950819651164598, 0.003159288093017896, 0.002918236883720796, 0.004579455976750598, 0.0031148749999903333, 0.002852836651157325, 0.0024470708837069713, 0.0015850386744205025, 0.002565877069765668, 0.001713506232548287, 0.0031056771162988046, 0.01341875416279055, 0.002991067534885376, 0.0033563433488420424, 0.0077756602092851484, 0.0033487990232631917, 0.0028230502557985452, 0.004973561023266378, 0.0036372129534779666, 0.0027628247209238823, 0.004655206093026886, 0.003795903581386631, 0.003020003488377687, 0.0034083799069607222, 0.0017571230232427953, 0.002894454465117739, 0.0027132150697832193, 0.003016615604644548, 0.0028201230930197958, 0.003125732000012778, 0.004285989604647702, 0.0032379523488279393, 0.005499638558154232, 0.002896697279052311, 0.0030192810232662405, 0.005185837976738227, 0.002926062162809695, 0.003326438069771664, 0.004334610441875093, 0.0023345058837147136, 0.0024107402093017467, 0.0022197075116331014, 0.0015528835581282566, 0.0025170352790705374, 0.002825068418598337, 0.005535936441870141, 0.002755207488367295, 0.004289008116270558, 0.002602791279059602, 0.0031694305116193285, 0.004534759604656185, 0.005436451255813746, 0.002856631186046417, 0.002930098581389249, 0.008101477813947577, 0.003246224441851197, 0.004796263162799817, 0.006834028627900109, 0.0045459940232577475, 0.004070857674420505, 0.005860541581399915, 0.002833800581377778, 0.003109678302322674, 0.0033855945581350634, 0.011538842976754728, 0.0033979982558247327, 0.0027411879767431286, 0.0034624167906787437, 0.002964466744171744, 0.002968381093037457, 0.0031761350697749103, 0.002575335255811339, 0.0019524758837326582, 0.002788616767459242, 0.0026084610930157334, 0.0032243438604699654, 0.0030504068837153894, 0.006653936906987774, 0.0030408084186028857, 0.004346494139543549, 0.00276834572092454, 0.0031618089069736523, 0.00398383413954265, 0.004108730813958425, 0.002706027558138716, 0.004934629581395373, 0.002838744534888382, 0.004174986395344942, 0.0026794621162903346, 0.002334231348847192, 0.002733357976763347, 0.0025802907674344253, 0.003091221744186843, 0.009593511255822472, 0.0034334432558078038, 0.0029725612790613477, 0.00384457181396926, 0.007622435720931694, 0.003367130279066048, 0.002777441744191767, 0.003471234139545041, 0.0029824530697518388, 0.004957517930221044, 0.011363260325577907, 0.0029198367906987143, 0.0027341458837200176, 0.0048247302325593365, 0.002623515162794806, 0.002326830744201542, 0.0033006755116349626, 0.002939535465123109, 0.003170549790686278, 0.003636975930232946, 0.004673247348850858, 0.003757240697670441, 0.005947780465121891, 0.0030895049302311827, 0.0045816354418622175, 0.004117801116292465, 0.0029937387441963817]
[307.1339767821099, 369.0806819256012, 355.1800405610144, 264.96763317905044, 276.60734627203374, 490.72783145550346, 316.2935889138665, 361.7672825069491, 406.28711582694143, 115.85954858797167, 175.97841308008128, 267.83350850592996, 415.43724780091884, 128.93737263607483, 231.08329680468344, 359.42804639653195, 355.1272219055135, 284.8788120023426, 397.2068379054977, 121.14397055339532, 128.34658319301735, 322.6614950487239, 307.4247020123555, 323.12215904864354, 318.33020182087347, 317.3056938922188, 570.5822053562097, 371.57101704036126, 428.5989298718745, 383.03947850168856, 365.7305883716005, 360.744504507345, 192.76951853401098, 354.65160235601485, 321.14159413930844, 328.597626657851, 218.12548736474739, 335.45830668438623, 352.29689327492474, 188.72682435078244, 328.6045201136512, 311.5983476908681, 506.06462090881587, 441.3474248597967, 535.220599012289, 389.86981414224164, 301.9604003498424, 374.96466597249423, 145.89834603714363, 343.8796910016105, 338.5175839538074, 319.35564714427295, 326.9911006469083, 258.24846173673325, 287.9355702791269, 330.4787753436669, 358.6810222786439, 332.5135564276555, 306.1884385524343, 114.75123680519016, 375.7144498449416, 368.89443862803455, 443.44046834931675, 384.78729439435364, 521.4917784345016, 303.9008156142492, 312.9736393688758, 317.7056155372121, 307.37404229177713, 381.3386125847478, 338.9032049376666, 198.41060389986998, 208.35672315137174, 298.68454574261114, 197.67438511917535, 333.80214833720106, 307.0625631739407, 338.4826701938472, 149.10360151665404, 459.4762853806106, 451.7366373342584, 410.49260521331774, 481.63959224572733, 119.53551931115433, 377.3369830051214, 254.26495512059637, 88.043539467129, 325.07793244750104, 251.37735365109162, 341.2872604641045, 280.1285943047679, 318.784566027928, 323.7224646074669, 362.71069895432726, 386.47130495225537, 282.3029007987914, 377.8173572005995, 478.9926610185766, 341.57984716229635, 451.36465332908244, 568.3781156115003, 322.62421125615555, 339.4047625712537, 377.5867297643378, 280.58348151252886, 323.8966984362128, 191.93808616580066, 174.69267941952944, 405.43209996461707, 403.00021476043315, 333.8407609387234, 141.5453112578899, 420.4904240456713, 317.6613599302862, 351.93725210958644, 266.17846152747643, 362.980772782999, 132.07967726807698, 250.39707589714143, 341.9542790424104, 337.2770484978692, 309.48700456054115, 103.08310293656676, 329.74806695556714, 332.31128293273173, 316.41668902105, 112.72273396201506, 453.3139452712702, 256.4268973996547, 400.5611992788332, 339.02907167112653, 328.72998716685584, 240.82407893998123, 317.93688243172056, 155.84293508250997, 372.6938161680557, 379.75148480026155, 145.93466247240053, 341.1967276871248, 209.57189919627186, 304.5020651577764, 315.40591307109315, 259.57619754841687, 515.9340718814238, 200.72047634663517, 320.6304854002871, 424.0285668263802, 419.1644440321863, 209.11442535645048, 135.24225745866926, 309.71331171331127, 274.1074873752553, 269.7481118466962, 356.12865361338714, 196.36730621496162, 286.70644431860256, 159.42259833964272, 326.14449926098354, 393.67891915640536, 222.49990900853302, 179.2903119886517, 388.534716404619, 272.8036871750674, 440.34385775966587, 404.48303028387323, 258.8554774900653, 125.24347987826239, 235.5803409939959, 274.0237515117771, 326.0670034627971, 365.9475553256787, 362.97208022108185, 354.0468847490967, 331.50979560309935, 373.2176999589953, 345.9105497523849, 348.63995714649405, 367.63965636511284, 435.73697400011685, 347.1212027761868, 393.3909227149824, 377.3387278862062, 542.9840643866936, 354.8091968992713, 378.91396982102566, 196.02440890511878, 213.7410566266452, 358.57528499530304, 218.23397102639433, 353.3078094463207, 111.7216119833103, 316.52700562826936, 342.67266155754464, 218.36654944973634, 321.04017015228646, 350.52830648201495, 408.6518321386503, 630.8994323848941, 389.7302843473036, 583.598694305782, 321.99097412668306, 74.52256654145611, 334.32879342803676, 297.94329604121276, 128.6064428080165, 298.6145161454221, 354.2267793306194, 201.06318095263896, 274.93578539133443, 361.9484046261184, 214.813260684187, 263.4418863807661, 331.1254453342334, 293.3945238785624, 569.1121149585109, 345.4882472850796, 368.5664329145489, 331.49732384210466, 354.59445102773765, 319.9250607524612, 233.3183447098439, 308.83715764439086, 181.83013109421796, 345.22074751530886, 331.20467829728744, 192.83286606439196, 341.7562390539814, 300.6218600873104, 230.7012391100626, 428.3561703467543, 414.81035415659437, 450.50980580061736, 643.9632867291956, 397.29280249471464, 353.9737280048431, 180.6379120317685, 362.94907161151366, 233.15414027929958, 384.20291632500926, 315.5140951454648, 220.5188559440336, 183.9435236231722, 350.06269093631295, 341.2854455995366, 123.43427001409434, 308.0501727199548, 208.49564881178253, 146.32657462356428, 219.9738923729118, 245.64847999564415, 170.6326942843956, 352.8829821588242, 321.5766721763734, 295.3690948011343, 86.66380173597332, 294.2909103281128, 364.80533567352217, 288.81560495320036, 337.32879681178366, 336.88396760967424, 314.8480710144575, 388.29895942420046, 512.1702185064861, 358.6007269514906, 383.36780359789265, 310.14061876584236, 327.8251191139465, 150.28696754696136, 328.8599156337034, 230.07048161003982, 361.22655940025857, 316.2746482857995, 251.01446620837518, 243.38416053023977, 369.5453865546842, 202.64945595313122, 352.26840165077397, 239.52173858937303, 373.2092325248029, 428.40655040207156, 365.8503600703376, 387.5532217612423, 323.4966892558054, 104.2371216683654, 291.25281109814784, 336.41022206807867, 260.10698938344655, 131.1916605939931, 296.9888056358108, 360.04355522171323, 288.08197885812035, 335.2944628507456, 201.7138443219735, 88.00291213509117, 342.4848961371916, 365.7449318832331, 207.26547429565568, 381.16798949037116, 429.7691194307958, 302.96828527220424, 340.1898061325549, 315.4027112072402, 274.95370307164796, 213.98396561352524, 266.1527648787629, 168.12994458421161, 323.6764538599301, 218.2626733814394, 242.84805695044534, 334.03048343432954]
Elapsed: 0.1597077586161181~0.07642766071808693
Time per graph: 0.0036865420833063715~0.0017672448521544074
Speed: 312.2285943082173~98.5980802168566
Total Time: 0.1300
best val loss: 0.6349600120024248 test_score: 0.6512

Testing...
Test loss: 0.5054 score: 0.7442 time: 0.14s
test Score 0.7442
Epoch Time List: [0.6591209390007862, 0.7469383420002487, 0.7564135829998122, 0.7843028839997714, 0.763782168999569, 0.7146352580002713, 1.037606305000736, 0.9628248450007959, 0.4766330639995431, 0.6860566949990243, 0.5737096459997701, 0.4309524059999603, 0.4986140510000041, 0.7488338199991631, 0.7028720979997161, 0.9397899570003574, 0.915337421998629, 0.8537459719991602, 0.5638187200001994, 0.7864816629999041, 0.9457079809999414, 0.710854855999969, 0.8399142110019966, 0.7508033539988901, 0.5656874329988568, 0.5700495099999898, 0.6276187240000581, 0.36652619899905403, 0.617627087000983, 0.411494037999546, 0.7438824000009845, 0.9335121049989539, 0.6989594409997153, 0.6809088669988341, 0.9616808219998347, 0.7794582759988771, 0.7079063539995332, 0.5363676620008846, 1.0437001249993045, 0.7480708989996856, 0.6832800610009144, 0.7273809689995687, 0.6036545269998896, 0.551956098998744, 0.6511637040002825, 0.4591628859998309, 0.7891358549995857, 0.5141963920004855, 0.7616654079993168, 0.7129683260000093, 0.9043226100002357, 0.7235869870009992, 0.615709453000818, 0.7638940890001322, 0.828580256999885, 0.5710702659989693, 0.5403754879998814, 0.8898024430000078, 0.6971760589995029, 1.0530343030004587, 0.48715270299999247, 0.5748706270005641, 0.3563839549997283, 0.4495671209988359, 0.6020067160006874, 0.7359857859992189, 0.5422037729986187, 0.6156179939998765, 0.9129159259991866, 0.5417944440005158, 0.6102547730006336, 0.6919471689989223, 0.9765997510003217, 0.5890723420006907, 0.8569402319999426, 1.004507895999268, 0.6900182370000039, 0.529685029000575, 0.71918204999929, 0.4308176460008326, 0.468032095000126, 0.4884422880004422, 0.4665299700000105, 1.1457956490003198, 0.5042514060005487, 0.712683217000631, 1.166385387999071, 0.5362926529996912, 0.6193782079999437, 0.9168944079992798, 0.8300678109999353, 0.7060676190003505, 0.8570891790013775, 0.5955233249997036, 0.8669288319997577, 0.4709742349996304, 0.6130862799991519, 0.4542953950003721, 0.6382253950005179, 0.4162797099988893, 0.548656320000191, 0.6358789089999846, 0.6546853810004905, 0.9299482120004541, 0.6625302710008327, 1.0147791359995608, 0.6791533709993018, 0.8153370810005072, 0.8262575599992488, 0.5235213879996081, 0.552325975999338, 0.7190860339997016, 0.4905170989995895, 0.523346832000243, 0.5700461390015334, 0.7719431850000547, 1.1209817540002405, 0.8275421939997614, 0.5943990789983218, 0.755979861999549, 0.8475440670008538, 0.6808355289995234, 0.9091629069998817, 0.8189999239993995, 0.6141615009992165, 0.5578556119999121, 0.8153116099992985, 0.5363101679995452, 0.5551445909995891, 0.4893550619990492, 0.44955539599959593, 0.9793937200001892, 0.7275783170007344, 0.6724765700000717, 1.077069307999409, 0.6335819360010646, 0.5894009929997992, 0.7463299989985899, 0.5757175930002632, 0.7048896060005063, 0.6966170139994574, 0.7540513380008633, 0.7092690190002031, 0.578936975999568, 0.6485942050003359, 0.7072486420001951, 0.538353633000952, 0.5223413779995099, 0.6177196670005287, 0.7533422100004827, 0.7172287489993323, 0.677761713000109, 0.6308615420002752, 0.9215227300001061, 0.6799469699999463, 0.7008373649987334, 0.7591818490000151, 0.9393872529990404, 0.5554260459994111, 0.703267949000292, 0.7334165199999916, 0.7071817249998276, 0.5901488290000998, 0.3959178730010535, 0.5629749580002681, 0.7054362589997254, 0.81779778300006, 0.6644461730011244, 0.6886592870014283, 0.9996088530006091, 0.5925122269991334, 0.6840484239992293, 1.0881844259993159, 0.5918394080008511, 0.672049670000888, 1.0238414100003865, 0.820349472000089, 0.7240935840009115, 0.8916660369995952, 0.5407496560001164, 0.41179664799983584, 0.5298989469993103, 0.5223505939984534, 0.68668378700022, 0.9372951320001448, 0.7795443750001141, 0.7486668840001585, 0.9203434809996907, 1.0430403550008123, 0.4851859490008792, 1.1650015219993293, 0.7233047600002465, 0.5295723190001809, 0.7312043060001088, 0.8995461710001109, 0.7353265480005575, 0.5695828349998919, 0.8292970029997377, 0.5730359279996264, 0.46068595799988543, 0.6193649609995191, 1.0364393390009354, 0.8656225810009346, 0.6503617869993832, 1.0557705900000656, 0.6925231849991178, 0.5859904879989699, 0.8028223460014488, 0.9658262129996729, 0.605744799000604, 0.8289523589992314, 0.7228263980005067, 0.5627481940000507, 0.6725089760011542, 0.42749133800043637, 0.8050089200005459, 0.41856826300136163, 0.761000944999978, 0.6657579690008788, 0.8804332700001396, 0.6316537889997562, 0.6146444639998663, 0.8592132829999173, 0.9631716640005834, 0.6388677580007425, 0.708773583999573, 0.6743730849993881, 0.7948492490004355, 0.6412495649992707, 0.6352686150003137, 0.513404571999672, 0.7207414769991374, 0.36446164200060593, 0.5192260300000271, 0.5630828909997945, 0.6521392940003352, 0.8156695890002084, 0.6969412299995383, 0.5115284630001042, 0.6808779570001207, 0.8099008280005364, 0.7671617460009656, 0.5859309870002107, 0.6111885199998142, 0.8667482470000323, 0.7988274020008248, 0.677208778000022, 0.6380091969986097, 0.8875593150005443, 0.72152624499995, 0.6427016310008185, 0.7707421419991078, 0.6661040599992702, 0.7259716409998873, 1.0454087860007348, 0.6444071599999006, 0.6677819179985818, 0.7768084310000631, 0.8900255859989556, 0.6692286650004462, 0.934042401001534, 0.5477684349998526, 0.4237174549989504, 0.6225844709997546, 0.5607664739991378, 0.6525025780010765, 0.5776218400005746, 0.8482909169997583, 1.0497820769996906, 0.6531131140009165, 0.8014353640000991, 0.5880075139994005, 0.826916226999856, 0.7617004920002728, 0.5390136710011575, 0.8704443329997957, 0.5667084319993592, 0.8419703840008879, 0.4961154019993046, 0.8316964690011446, 0.5859360119993653, 0.511291639000774, 0.545556010000837, 0.8352992639993317, 0.858901065000282, 0.5901214559989967, 0.6522779369997806, 0.9126321710009506, 0.6205160730005446, 0.7135623159992974, 0.7900119869991613, 0.9207068850000724, 0.7780868680010826, 1.0155020459997104, 0.8204661769996164, 0.5297873440003968, 0.6029176290003306, 0.432240778999585, 0.5085999880002419, 0.5235930089984322, 0.7957376570002452, 0.596005114000036, 0.5923444409991134, 0.5972539149997829, 0.8811473939995267, 0.7532095470005515, 0.5386788959995101, 0.6223992889999863, 0.8519252850001067, 0.6442253800005346]
Total Epoch List: [105, 142, 63]
Total Time List: [0.15748658400025306, 0.2071282719998635, 0.12998447800055146]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f8512c80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8007;  Loss pred: 0.8007; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5116 time: 0.11s
Test loss: 0.6993 score: 0.4545 time: 0.08s
Epoch 2/1000, LR 0.000015
Train loss: 0.7419;  Loss pred: 0.7419; Loss self: 0.0000; time: 0.27s
Val loss: 0.6937 score: 0.5349 time: 0.11s
Test loss: 0.6984 score: 0.4545 time: 0.07s
Epoch 3/1000, LR 0.000045
Train loss: 0.7361;  Loss pred: 0.7361; Loss self: 0.0000; time: 0.17s
Val loss: 0.6923 score: 0.5349 time: 0.14s
Test loss: 0.6965 score: 0.4091 time: 0.07s
Epoch 4/1000, LR 0.000075
Train loss: 0.7288;  Loss pred: 0.7288; Loss self: 0.0000; time: 0.26s
Val loss: 0.6906 score: 0.5349 time: 0.12s
Test loss: 0.6940 score: 0.4545 time: 0.11s
Epoch 5/1000, LR 0.000105
Train loss: 0.7408;  Loss pred: 0.7408; Loss self: 0.0000; time: 0.50s
Val loss: 0.6891 score: 0.5349 time: 0.13s
Test loss: 0.6918 score: 0.5227 time: 0.18s
Epoch 6/1000, LR 0.000135
Train loss: 0.7113;  Loss pred: 0.7113; Loss self: 0.0000; time: 0.36s
Val loss: 0.6882 score: 0.5116 time: 0.19s
Test loss: 0.6899 score: 0.5000 time: 0.15s
Epoch 7/1000, LR 0.000165
Train loss: 0.7091;  Loss pred: 0.7091; Loss self: 0.0000; time: 0.29s
Val loss: 0.6882 score: 0.5116 time: 0.18s
Test loss: 0.6890 score: 0.5227 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 0.32s
Val loss: 0.6887 score: 0.5116 time: 0.18s
Test loss: 0.6887 score: 0.5227 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.4884 time: 0.35s
Test loss: 0.6885 score: 0.5227 time: 0.16s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6185;  Loss pred: 0.6185; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6449;  Loss pred: 0.6449; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6998 score: 0.4884 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7037 score: 0.4884 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.42s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7083 score: 0.4884 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5744;  Loss pred: 0.5744; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7130 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7039 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7158 score: 0.4884 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7058 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7191 score: 0.4884 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7081 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7210 score: 0.4884 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7090 score: 0.5000 time: 0.13s
     INFO: Early stopping counter 12 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7237 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7108 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 13 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5638;  Loss pred: 0.5638; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7262 score: 0.4884 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7121 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7259 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7107 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5487;  Loss pred: 0.5487; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7245 score: 0.4884 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7082 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 16 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5141;  Loss pred: 0.5141; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7220 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7047 score: 0.5000 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.4954;  Loss pred: 0.4954; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7183 score: 0.4884 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7001 score: 0.5000 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.5109;  Loss pred: 0.5109; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7148 score: 0.4884 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.12s
     INFO: Early stopping counter 19 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.5232;  Loss pred: 0.5232; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7101 score: 0.4884 time: 0.17s
Test loss: 0.6913 score: 0.5227 time: 0.13s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.7113,   Val_Loss: 0.6882,   Val_Precision: 0.6667,   Val_Recall: 0.0909,   Val_accuracy: 0.1600,   Val_Score: 0.5116,   Val_Loss: 0.6882,   Test_Precision: 0.5000,   Test_Recall: 0.1364,   Test_accuracy: 0.2143,   Test_Score: 0.5000,   Test_loss: 0.6899


[0.08941617700020288, 0.07162618600068527, 0.07762893400013127, 0.11839482899995346, 0.1856380259996513, 0.15117231299973355, 0.13549983600023552, 0.11591537700041954, 0.16737943999942217, 0.13914035499965394, 0.1395425829996384, 0.14215353799954755, 0.4293385399996623, 0.11192517600011342, 0.13207026600048266, 0.13043690199992852, 0.12926057200002106, 0.13268188399979408, 0.1187846550001268, 0.090692723999382, 0.06841352799983724, 0.1114647819995298, 0.0887742029999572, 0.11528633600028115, 0.1274756630000411, 0.13348960300027102]
[0.0020321858409137017, 0.0016278678636519378, 0.0017642939545484378, 0.0026907915681807604, 0.00421904604544662, 0.0034357343863575807, 0.00307954172727808, 0.0026344403863731713, 0.0038040781818050495, 0.0031622807954466803, 0.0031714223409008728, 0.0032307622272624444, 0.009757694090901416, 0.002543754000002578, 0.003001596954556424, 0.0029644750454529208, 0.0029377402727277513, 0.0030154973636316836, 0.002699651250002882, 0.002061198272713227, 0.00155485290908721, 0.002533290499989314, 0.0020175955227263, 0.00262014400000639, 0.002897174159091843, 0.003033854613642523]
[492.080979931631, 614.3004738459624, 566.7989721451746, 371.6378525283169, 237.02040442986961, 291.0585882222861, 324.72364025535444, 379.5872570025006, 262.87577494674315, 316.2274524893187, 315.3159347789486, 309.5244804961523, 102.4832292019128, 393.11977494639285, 333.15598834213904, 337.3278522056903, 340.39768909573473, 331.62025344822723, 370.4182160562156, 485.15468562064393, 643.1476534890099, 394.74351638875146, 495.639482114204, 381.65841266646464, 345.16392356387126, 329.61368534379915]
Elapsed: 0.13283086261533475~0.06530991123609152
Time per graph: 0.003018883241257608~0.001484316164456625
Speed: 375.56908359828134~114.8762326306144
Total Time: 0.1340
best val loss: 0.6881541762241098 test_score: 0.5000

Testing...
Test loss: 0.6984 score: 0.4545 time: 0.12s
test Score 0.4545
Epoch Time List: [0.5208752009993987, 0.45217325699923094, 0.3800808929991035, 0.49543678399913915, 0.8109830680014056, 0.691003849999106, 0.6013976949998323, 0.6129287080011636, 0.8046236650006904, 0.7328708429995459, 0.6376232330003404, 0.63827853300063, 1.0060939009999856, 0.6341265930004738, 0.6121218980006233, 0.8940741059996071, 0.7690539780005565, 0.5621425139997882, 0.464749056999608, 0.646321753999473, 0.29892086799918616, 0.4562400440008787, 0.5789435819997379, 0.7219928500007882, 0.624098094999681, 0.5904228160006824]
Total Epoch List: [26]
Total Time List: [0.13402178100022866]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f8513220>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7136;  Loss pred: 0.7136; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.5116 time: 0.12s
Epoch 2/1000, LR 0.000015
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5116 time: 0.20s
Epoch 3/1000, LR 0.000045
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.10s
Epoch 4/1000, LR 0.000075
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.13s
Epoch 5/1000, LR 0.000105
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.35s
Val loss: 0.6911 score: 0.4773 time: 0.21s
Test loss: 0.6916 score: 0.4884 time: 0.12s
Epoch 6/1000, LR 0.000135
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.41s
Val loss: 0.6878 score: 0.5227 time: 0.13s
Test loss: 0.6895 score: 0.6279 time: 0.11s
Epoch 7/1000, LR 0.000165
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.43s
Val loss: 0.6847 score: 0.6818 time: 0.10s
Test loss: 0.6878 score: 0.5349 time: 0.11s
Epoch 8/1000, LR 0.000195
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.44s
Val loss: 0.6822 score: 0.5227 time: 0.06s
Test loss: 0.6868 score: 0.5116 time: 0.07s
Epoch 9/1000, LR 0.000225
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6810 score: 0.5000 time: 0.18s
Test loss: 0.6875 score: 0.5116 time: 0.10s
Epoch 10/1000, LR 0.000255
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4884 time: 0.10s
Epoch 11/1000, LR 0.000285
Train loss: 0.6260;  Loss pred: 0.6260; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5985;  Loss pred: 0.5985; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6840 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4884 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5794;  Loss pred: 0.5794; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7009 score: 0.4884 time: 0.31s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7074 score: 0.4884 time: 0.56s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5663;  Loss pred: 0.5663; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7155 score: 0.4884 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7236 score: 0.4884 time: 0.13s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5763;  Loss pred: 0.5763; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7312 score: 0.4884 time: 0.20s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5297;  Loss pred: 0.5297; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7009 score: 0.5000 time: 0.14s
Test loss: 0.7400 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5289;  Loss pred: 0.5289; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7022 score: 0.5000 time: 0.13s
Test loss: 0.7462 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5160;  Loss pred: 0.5160; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5000 time: 0.13s
Test loss: 0.7504 score: 0.5116 time: 0.40s
     INFO: Early stopping counter 10 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5206;  Loss pred: 0.5206; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5000 time: 0.12s
Test loss: 0.7517 score: 0.5116 time: 0.15s
     INFO: Early stopping counter 11 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5059;  Loss pred: 0.5059; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 0.14s
Test loss: 0.7552 score: 0.5116 time: 0.34s
     INFO: Early stopping counter 12 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.14s
Test loss: 0.7561 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 13 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.4916;  Loss pred: 0.4916; Loss self: 0.0000; time: 0.57s
Val loss: 0.6886 score: 0.5227 time: 0.12s
Test loss: 0.7576 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 14 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.4781;  Loss pred: 0.4781; Loss self: 0.0000; time: 0.26s
Val loss: 0.6842 score: 0.5227 time: 0.11s
Test loss: 0.7588 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 15 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.4699;  Loss pred: 0.4699; Loss self: 0.0000; time: 0.36s
Val loss: 0.6822 score: 0.5227 time: 0.11s
Test loss: 0.7625 score: 0.5116 time: 0.16s
     INFO: Early stopping counter 16 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.4674;  Loss pred: 0.4674; Loss self: 0.0000; time: 0.31s
Val loss: 0.6790 score: 0.5227 time: 0.12s
Test loss: 0.7650 score: 0.5116 time: 0.09s
Epoch 28/1000, LR 0.000285
Train loss: 0.4393;  Loss pred: 0.4393; Loss self: 0.0000; time: 0.22s
Val loss: 0.6741 score: 0.5455 time: 0.45s
Test loss: 0.7654 score: 0.5116 time: 0.11s
Epoch 29/1000, LR 0.000285
Train loss: 0.4428;  Loss pred: 0.4428; Loss self: 0.0000; time: 0.56s
Val loss: 0.6702 score: 0.5455 time: 0.12s
Test loss: 0.7661 score: 0.5349 time: 0.11s
Epoch 30/1000, LR 0.000285
Train loss: 0.4353;  Loss pred: 0.4353; Loss self: 0.0000; time: 0.45s
Val loss: 0.6686 score: 0.5455 time: 0.12s
Test loss: 0.7693 score: 0.5349 time: 0.12s
Epoch 31/1000, LR 0.000285
Train loss: 0.4200;  Loss pred: 0.4200; Loss self: 0.0000; time: 0.31s
Val loss: 0.6648 score: 0.5455 time: 0.12s
Test loss: 0.7703 score: 0.5349 time: 0.11s
Epoch 32/1000, LR 0.000285
Train loss: 0.4172;  Loss pred: 0.4172; Loss self: 0.0000; time: 0.88s
Val loss: 0.6594 score: 0.5455 time: 0.14s
Test loss: 0.7695 score: 0.5349 time: 0.23s
Epoch 33/1000, LR 0.000285
Train loss: 0.4010;  Loss pred: 0.4010; Loss self: 0.0000; time: 0.36s
Val loss: 0.6540 score: 0.5682 time: 0.13s
Test loss: 0.7681 score: 0.5349 time: 0.12s
Epoch 34/1000, LR 0.000285
Train loss: 0.4079;  Loss pred: 0.4079; Loss self: 0.0000; time: 0.33s
Val loss: 0.6517 score: 0.5682 time: 0.10s
Test loss: 0.7705 score: 0.5349 time: 0.12s
Epoch 35/1000, LR 0.000285
Train loss: 0.3944;  Loss pred: 0.3944; Loss self: 0.0000; time: 0.49s
Val loss: 0.6463 score: 0.5682 time: 0.42s
Test loss: 0.7685 score: 0.5349 time: 0.11s
Epoch 36/1000, LR 0.000285
Train loss: 0.4013;  Loss pred: 0.4013; Loss self: 0.0000; time: 0.32s
Val loss: 0.6418 score: 0.5682 time: 0.23s
Test loss: 0.7669 score: 0.5349 time: 0.11s
Epoch 37/1000, LR 0.000285
Train loss: 0.4238;  Loss pred: 0.4238; Loss self: 0.0000; time: 0.31s
Val loss: 0.6358 score: 0.5682 time: 0.11s
Test loss: 0.7627 score: 0.5349 time: 0.10s
Epoch 38/1000, LR 0.000284
Train loss: 0.3987;  Loss pred: 0.3987; Loss self: 0.0000; time: 0.42s
Val loss: 0.6299 score: 0.5682 time: 0.26s
Test loss: 0.7591 score: 0.5349 time: 0.12s
Epoch 39/1000, LR 0.000284
Train loss: 0.3812;  Loss pred: 0.3812; Loss self: 0.0000; time: 0.38s
Val loss: 0.6253 score: 0.6136 time: 0.20s
Test loss: 0.7566 score: 0.5349 time: 0.36s
Epoch 40/1000, LR 0.000284
Train loss: 0.3512;  Loss pred: 0.3512; Loss self: 0.0000; time: 0.26s
Val loss: 0.6212 score: 0.6364 time: 0.12s
Test loss: 0.7532 score: 0.5349 time: 0.11s
Epoch 41/1000, LR 0.000284
Train loss: 0.3818;  Loss pred: 0.3818; Loss self: 0.0000; time: 0.31s
Val loss: 0.6168 score: 0.6364 time: 0.09s
Test loss: 0.7511 score: 0.5349 time: 0.14s
Epoch 42/1000, LR 0.000284
Train loss: 0.3699;  Loss pred: 0.3699; Loss self: 0.0000; time: 0.28s
Val loss: 0.6116 score: 0.6364 time: 0.09s
Test loss: 0.7476 score: 0.5349 time: 0.11s
Epoch 43/1000, LR 0.000284
Train loss: 0.3569;  Loss pred: 0.3569; Loss self: 0.0000; time: 0.25s
Val loss: 0.6033 score: 0.6591 time: 0.11s
Test loss: 0.7404 score: 0.5349 time: 0.06s
Epoch 44/1000, LR 0.000284
Train loss: 0.3648;  Loss pred: 0.3648; Loss self: 0.0000; time: 0.31s
Val loss: 0.5981 score: 0.6591 time: 0.17s
Test loss: 0.7369 score: 0.5349 time: 0.10s
Epoch 45/1000, LR 0.000284
Train loss: 0.3592;  Loss pred: 0.3592; Loss self: 0.0000; time: 0.25s
Val loss: 0.5917 score: 0.6591 time: 0.08s
Test loss: 0.7334 score: 0.5349 time: 0.10s
Epoch 46/1000, LR 0.000284
Train loss: 0.3340;  Loss pred: 0.3340; Loss self: 0.0000; time: 0.30s
Val loss: 0.5869 score: 0.6818 time: 0.07s
Test loss: 0.7321 score: 0.5349 time: 0.11s
Epoch 47/1000, LR 0.000284
Train loss: 0.3487;  Loss pred: 0.3487; Loss self: 0.0000; time: 0.24s
Val loss: 0.5826 score: 0.6818 time: 0.14s
Test loss: 0.7318 score: 0.5581 time: 0.11s
Epoch 48/1000, LR 0.000284
Train loss: 0.3260;  Loss pred: 0.3260; Loss self: 0.0000; time: 0.31s
Val loss: 0.5780 score: 0.6818 time: 0.25s
Test loss: 0.7311 score: 0.5581 time: 0.11s
Epoch 49/1000, LR 0.000284
Train loss: 0.3232;  Loss pred: 0.3232; Loss self: 0.0000; time: 0.54s
Val loss: 0.5749 score: 0.6818 time: 0.11s
Test loss: 0.7318 score: 0.5581 time: 0.12s
Epoch 50/1000, LR 0.000284
Train loss: 0.3326;  Loss pred: 0.3326; Loss self: 0.0000; time: 0.29s
Val loss: 0.5721 score: 0.6818 time: 0.13s
Test loss: 0.7335 score: 0.5581 time: 0.11s
Epoch 51/1000, LR 0.000284
Train loss: 0.3114;  Loss pred: 0.3114; Loss self: 0.0000; time: 0.33s
Val loss: 0.5690 score: 0.6818 time: 0.12s
Test loss: 0.7344 score: 0.5581 time: 0.20s
Epoch 52/1000, LR 0.000284
Train loss: 0.3207;  Loss pred: 0.3207; Loss self: 0.0000; time: 0.30s
Val loss: 0.5637 score: 0.7045 time: 0.18s
Test loss: 0.7327 score: 0.5581 time: 0.38s
Epoch 53/1000, LR 0.000284
Train loss: 0.3319;  Loss pred: 0.3319; Loss self: 0.0000; time: 0.36s
Val loss: 0.5605 score: 0.7045 time: 0.13s
Test loss: 0.7335 score: 0.5814 time: 0.25s
Epoch 54/1000, LR 0.000284
Train loss: 0.3296;  Loss pred: 0.3296; Loss self: 0.0000; time: 0.29s
Val loss: 0.5581 score: 0.7045 time: 0.21s
Test loss: 0.7339 score: 0.5814 time: 0.12s
Epoch 55/1000, LR 0.000284
Train loss: 0.3238;  Loss pred: 0.3238; Loss self: 0.0000; time: 0.30s
Val loss: 0.5569 score: 0.7045 time: 0.20s
Test loss: 0.7345 score: 0.6047 time: 0.10s
Epoch 56/1000, LR 0.000284
Train loss: 0.3221;  Loss pred: 0.3221; Loss self: 0.0000; time: 0.32s
Val loss: 0.5556 score: 0.7045 time: 0.12s
Test loss: 0.7354 score: 0.6047 time: 0.34s
Epoch 57/1000, LR 0.000283
Train loss: 0.3053;  Loss pred: 0.3053; Loss self: 0.0000; time: 0.32s
Val loss: 0.5527 score: 0.7045 time: 0.12s
Test loss: 0.7344 score: 0.6279 time: 0.12s
Epoch 58/1000, LR 0.000283
Train loss: 0.2938;  Loss pred: 0.2938; Loss self: 0.0000; time: 0.32s
Val loss: 0.5495 score: 0.7273 time: 0.21s
Test loss: 0.7333 score: 0.6279 time: 0.10s
Epoch 59/1000, LR 0.000283
Train loss: 0.3036;  Loss pred: 0.3036; Loss self: 0.0000; time: 0.29s
Val loss: 0.5468 score: 0.7500 time: 0.19s
Test loss: 0.7330 score: 0.6279 time: 0.13s
Epoch 60/1000, LR 0.000283
Train loss: 0.2938;  Loss pred: 0.2938; Loss self: 0.0000; time: 0.53s
Val loss: 0.5417 score: 0.7727 time: 0.12s
Test loss: 0.7304 score: 0.6512 time: 0.15s
Epoch 61/1000, LR 0.000283
Train loss: 0.2791;  Loss pred: 0.2791; Loss self: 0.0000; time: 0.29s
Val loss: 0.5378 score: 0.7955 time: 0.11s
Test loss: 0.7280 score: 0.6512 time: 0.10s
Epoch 62/1000, LR 0.000283
Train loss: 0.2807;  Loss pred: 0.2807; Loss self: 0.0000; time: 0.30s
Val loss: 0.5345 score: 0.7955 time: 0.11s
Test loss: 0.7245 score: 0.6512 time: 0.07s
Epoch 63/1000, LR 0.000283
Train loss: 0.2990;  Loss pred: 0.2990; Loss self: 0.0000; time: 0.30s
Val loss: 0.5301 score: 0.7955 time: 0.10s
Test loss: 0.7213 score: 0.6512 time: 0.15s
Epoch 64/1000, LR 0.000283
Train loss: 0.2771;  Loss pred: 0.2771; Loss self: 0.0000; time: 0.36s
Val loss: 0.5294 score: 0.7955 time: 0.09s
Test loss: 0.7209 score: 0.6512 time: 0.09s
Epoch 65/1000, LR 0.000283
Train loss: 0.2766;  Loss pred: 0.2766; Loss self: 0.0000; time: 0.26s
Val loss: 0.5255 score: 0.7955 time: 0.13s
Test loss: 0.7176 score: 0.6512 time: 0.12s
Epoch 66/1000, LR 0.000283
Train loss: 0.2769;  Loss pred: 0.2769; Loss self: 0.0000; time: 0.25s
Val loss: 0.5212 score: 0.7955 time: 0.09s
Test loss: 0.7125 score: 0.6744 time: 0.11s
Epoch 67/1000, LR 0.000283
Train loss: 0.2792;  Loss pred: 0.2792; Loss self: 0.0000; time: 0.34s
Val loss: 0.5192 score: 0.7955 time: 0.21s
Test loss: 0.7116 score: 0.6744 time: 0.14s
Epoch 68/1000, LR 0.000283
Train loss: 0.2732;  Loss pred: 0.2732; Loss self: 0.0000; time: 0.38s
Val loss: 0.5160 score: 0.8182 time: 0.14s
Test loss: 0.7094 score: 0.6744 time: 0.24s
Epoch 69/1000, LR 0.000283
Train loss: 0.2753;  Loss pred: 0.2753; Loss self: 0.0000; time: 0.49s
Val loss: 0.5088 score: 0.8182 time: 0.13s
Test loss: 0.7011 score: 0.6744 time: 0.14s
Epoch 70/1000, LR 0.000283
Train loss: 0.2719;  Loss pred: 0.2719; Loss self: 0.0000; time: 0.30s
Val loss: 0.5013 score: 0.8182 time: 0.38s
Test loss: 0.6943 score: 0.6744 time: 0.14s
Epoch 71/1000, LR 0.000282
Train loss: 0.2704;  Loss pred: 0.2704; Loss self: 0.0000; time: 0.32s
Val loss: 0.4987 score: 0.8182 time: 0.12s
Test loss: 0.6928 score: 0.6744 time: 0.12s
Epoch 72/1000, LR 0.000282
Train loss: 0.2508;  Loss pred: 0.2508; Loss self: 0.0000; time: 0.31s
Val loss: 0.4969 score: 0.8182 time: 0.22s
Test loss: 0.6912 score: 0.6977 time: 0.21s
Epoch 73/1000, LR 0.000282
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.57s
Val loss: 0.4975 score: 0.8182 time: 0.18s
Test loss: 0.6934 score: 0.6977 time: 0.13s
     INFO: Early stopping counter 1 of 20
Epoch 74/1000, LR 0.000282
Train loss: 0.2574;  Loss pred: 0.2574; Loss self: 0.0000; time: 0.64s
Val loss: 0.4979 score: 0.8182 time: 0.11s
Test loss: 0.6940 score: 0.6977 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 75/1000, LR 0.000282
Train loss: 0.2581;  Loss pred: 0.2581; Loss self: 0.0000; time: 0.40s
Val loss: 0.4981 score: 0.8182 time: 0.13s
Test loss: 0.6952 score: 0.6977 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 76/1000, LR 0.000282
Train loss: 0.2471;  Loss pred: 0.2471; Loss self: 0.0000; time: 0.30s
Val loss: 0.4981 score: 0.8182 time: 0.32s
Test loss: 0.6955 score: 0.6977 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 77/1000, LR 0.000282
Train loss: 0.2377;  Loss pred: 0.2377; Loss self: 0.0000; time: 0.31s
Val loss: 0.4990 score: 0.8182 time: 0.20s
Test loss: 0.6988 score: 0.6977 time: 0.35s
     INFO: Early stopping counter 5 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.2357;  Loss pred: 0.2357; Loss self: 0.0000; time: 0.64s
Val loss: 0.5014 score: 0.8182 time: 0.42s
Test loss: 0.7033 score: 0.6977 time: 0.09s
     INFO: Early stopping counter 6 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.2585;  Loss pred: 0.2585; Loss self: 0.0000; time: 0.26s
Val loss: 0.5024 score: 0.8182 time: 0.10s
Test loss: 0.7064 score: 0.6977 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.2382;  Loss pred: 0.2382; Loss self: 0.0000; time: 0.35s
Val loss: 0.5002 score: 0.8182 time: 0.09s
Test loss: 0.7050 score: 0.6977 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.2434;  Loss pred: 0.2434; Loss self: 0.0000; time: 0.28s
Val loss: 0.4982 score: 0.8182 time: 0.07s
Test loss: 0.7036 score: 0.6977 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.2328;  Loss pred: 0.2328; Loss self: 0.0000; time: 0.26s
Val loss: 0.4988 score: 0.8182 time: 0.09s
Test loss: 0.7081 score: 0.6977 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.2292;  Loss pred: 0.2292; Loss self: 0.0000; time: 0.30s
Val loss: 0.4985 score: 0.8182 time: 0.09s
Test loss: 0.7109 score: 0.6977 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2182;  Loss pred: 0.2182; Loss self: 0.0000; time: 0.29s
Val loss: 0.4978 score: 0.8182 time: 0.43s
Test loss: 0.7109 score: 0.6977 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.2107;  Loss pred: 0.2107; Loss self: 0.0000; time: 0.41s
Val loss: 0.4984 score: 0.8182 time: 0.13s
Test loss: 0.7131 score: 0.6977 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.2125;  Loss pred: 0.2125; Loss self: 0.0000; time: 0.41s
Val loss: 0.4999 score: 0.8182 time: 0.12s
Test loss: 0.7156 score: 0.6977 time: 0.23s
     INFO: Early stopping counter 14 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.2252;  Loss pred: 0.2252; Loss self: 0.0000; time: 0.28s
Val loss: 0.4977 score: 0.8182 time: 0.75s
Test loss: 0.7139 score: 0.6977 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.2097;  Loss pred: 0.2097; Loss self: 0.0000; time: 0.44s
Val loss: 0.4963 score: 0.8182 time: 0.11s
Test loss: 0.7139 score: 0.6977 time: 0.15s
Epoch 89/1000, LR 0.000281
Train loss: 0.2136;  Loss pred: 0.2136; Loss self: 0.0000; time: 0.44s
Val loss: 0.4971 score: 0.8182 time: 0.21s
Test loss: 0.7154 score: 0.6977 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.2049;  Loss pred: 0.2049; Loss self: 0.0000; time: 0.32s
Val loss: 0.4968 score: 0.8182 time: 0.31s
Test loss: 0.7144 score: 0.6977 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.2146;  Loss pred: 0.2146; Loss self: 0.0000; time: 0.32s
Val loss: 0.5005 score: 0.7955 time: 0.24s
Test loss: 0.7205 score: 0.6977 time: 0.15s
     INFO: Early stopping counter 3 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.2095;  Loss pred: 0.2095; Loss self: 0.0000; time: 0.31s
Val loss: 0.4977 score: 0.8182 time: 0.13s
Test loss: 0.7179 score: 0.6977 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1928;  Loss pred: 0.1928; Loss self: 0.0000; time: 0.28s
Val loss: 0.4973 score: 0.8182 time: 0.12s
Test loss: 0.7173 score: 0.6977 time: 0.13s
     INFO: Early stopping counter 5 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.2134;  Loss pred: 0.2134; Loss self: 0.0000; time: 0.33s
Val loss: 0.4955 score: 0.8182 time: 0.38s
Test loss: 0.7155 score: 0.6977 time: 0.33s
Epoch 95/1000, LR 0.000280
Train loss: 0.1874;  Loss pred: 0.1874; Loss self: 0.0000; time: 0.32s
Val loss: 0.4940 score: 0.8182 time: 0.17s
Test loss: 0.7135 score: 0.6977 time: 0.13s
Epoch 96/1000, LR 0.000280
Train loss: 0.1816;  Loss pred: 0.1816; Loss self: 0.0000; time: 0.44s
Val loss: 0.4917 score: 0.8182 time: 0.14s
Test loss: 0.7093 score: 0.6977 time: 0.11s
Epoch 97/1000, LR 0.000280
Train loss: 0.1836;  Loss pred: 0.1836; Loss self: 0.0000; time: 0.37s
Val loss: 0.4891 score: 0.8182 time: 0.11s
Test loss: 0.7063 score: 0.7209 time: 0.09s
Epoch 98/1000, LR 0.000280
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 0.34s
Val loss: 0.4856 score: 0.8182 time: 0.10s
Test loss: 0.7035 score: 0.7209 time: 0.08s
Epoch 99/1000, LR 0.000279
Train loss: 0.2033;  Loss pred: 0.2033; Loss self: 0.0000; time: 0.42s
Val loss: 0.4846 score: 0.8182 time: 0.13s
Test loss: 0.7028 score: 0.7442 time: 0.07s
Epoch 100/1000, LR 0.000279
Train loss: 0.1819;  Loss pred: 0.1819; Loss self: 0.0000; time: 0.26s
Val loss: 0.4826 score: 0.8182 time: 0.08s
Test loss: 0.7022 score: 0.7442 time: 0.08s
Epoch 101/1000, LR 0.000279
Train loss: 0.1815;  Loss pred: 0.1815; Loss self: 0.0000; time: 0.27s
Val loss: 0.4847 score: 0.8182 time: 0.08s
Test loss: 0.7062 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1729;  Loss pred: 0.1729; Loss self: 0.0000; time: 0.33s
Val loss: 0.4842 score: 0.8182 time: 0.07s
Test loss: 0.7075 score: 0.7442 time: 0.14s
     INFO: Early stopping counter 2 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1800;  Loss pred: 0.1800; Loss self: 0.0000; time: 0.40s
Val loss: 0.4831 score: 0.8182 time: 0.11s
Test loss: 0.7071 score: 0.7442 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1918;  Loss pred: 0.1918; Loss self: 0.0000; time: 0.44s
Val loss: 0.4801 score: 0.8182 time: 0.14s
Test loss: 0.7043 score: 0.7442 time: 0.11s
Epoch 105/1000, LR 0.000279
Train loss: 0.1867;  Loss pred: 0.1867; Loss self: 0.0000; time: 0.58s
Val loss: 0.4818 score: 0.8182 time: 0.17s
Test loss: 0.7064 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1862;  Loss pred: 0.1862; Loss self: 0.0000; time: 0.32s
Val loss: 0.4847 score: 0.8182 time: 0.13s
Test loss: 0.7104 score: 0.7442 time: 0.16s
     INFO: Early stopping counter 2 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1781;  Loss pred: 0.1781; Loss self: 0.0000; time: 0.31s
Val loss: 0.4858 score: 0.8182 time: 0.12s
Test loss: 0.7112 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 3 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1677;  Loss pred: 0.1677; Loss self: 0.0000; time: 0.33s
Val loss: 0.4846 score: 0.8182 time: 0.13s
Test loss: 0.7106 score: 0.7442 time: 0.42s
     INFO: Early stopping counter 4 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1698;  Loss pred: 0.1698; Loss self: 0.0000; time: 0.54s
Val loss: 0.4836 score: 0.8182 time: 0.13s
Test loss: 0.7076 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1751;  Loss pred: 0.1751; Loss self: 0.0000; time: 0.37s
Val loss: 0.4826 score: 0.8182 time: 0.22s
Test loss: 0.7039 score: 0.7442 time: 0.12s
     INFO: Early stopping counter 6 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.56s
Val loss: 0.4832 score: 0.8409 time: 0.13s
Test loss: 0.7024 score: 0.7442 time: 0.30s
     INFO: Early stopping counter 7 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1632;  Loss pred: 0.1632; Loss self: 0.0000; time: 0.40s
Val loss: 0.4827 score: 0.8409 time: 0.11s
Test loss: 0.7008 score: 0.7442 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 0.32s
Val loss: 0.4800 score: 0.8409 time: 0.15s
Test loss: 0.6972 score: 0.7442 time: 0.18s
Epoch 114/1000, LR 0.000277
Train loss: 0.1661;  Loss pred: 0.1661; Loss self: 0.0000; time: 0.61s
Val loss: 0.4739 score: 0.8409 time: 0.34s
Test loss: 0.6880 score: 0.7442 time: 0.15s
Epoch 115/1000, LR 0.000277
Train loss: 0.1580;  Loss pred: 0.1580; Loss self: 0.0000; time: 0.28s
Val loss: 0.4723 score: 0.8409 time: 0.10s
Test loss: 0.6861 score: 0.7674 time: 0.17s
Epoch 116/1000, LR 0.000277
Train loss: 0.1558;  Loss pred: 0.1558; Loss self: 0.0000; time: 0.31s
Val loss: 0.4727 score: 0.8409 time: 0.10s
Test loss: 0.6890 score: 0.7442 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.42s
Val loss: 0.4723 score: 0.8409 time: 0.12s
Test loss: 0.6880 score: 0.7442 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 118/1000, LR 0.000277
Train loss: 0.1511;  Loss pred: 0.1511; Loss self: 0.0000; time: 0.25s
Val loss: 0.4729 score: 0.8409 time: 0.19s
Test loss: 0.6894 score: 0.7442 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 119/1000, LR 0.000277
Train loss: 0.1472;  Loss pred: 0.1472; Loss self: 0.0000; time: 0.29s
Val loss: 0.4749 score: 0.8409 time: 0.11s
Test loss: 0.6924 score: 0.7442 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 120/1000, LR 0.000277
Train loss: 0.1468;  Loss pred: 0.1468; Loss self: 0.0000; time: 0.25s
Val loss: 0.4750 score: 0.8409 time: 0.10s
Test loss: 0.6925 score: 0.7442 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 121/1000, LR 0.000276
Train loss: 0.1386;  Loss pred: 0.1386; Loss self: 0.0000; time: 0.31s
Val loss: 0.4760 score: 0.8409 time: 0.20s
Test loss: 0.6938 score: 0.7442 time: 0.11s
     INFO: Early stopping counter 6 of 20
Epoch 122/1000, LR 0.000276
Train loss: 0.1606;  Loss pred: 0.1606; Loss self: 0.0000; time: 0.88s
Val loss: 0.4696 score: 0.8409 time: 0.13s
Test loss: 0.6861 score: 0.7442 time: 0.14s
Epoch 123/1000, LR 0.000276
Train loss: 0.1426;  Loss pred: 0.1426; Loss self: 0.0000; time: 0.34s
Val loss: 0.4637 score: 0.8409 time: 0.15s
Test loss: 0.6778 score: 0.7442 time: 0.12s
Epoch 124/1000, LR 0.000276
Train loss: 0.1378;  Loss pred: 0.1378; Loss self: 0.0000; time: 0.34s
Val loss: 0.4585 score: 0.8409 time: 0.12s
Test loss: 0.6715 score: 0.7674 time: 0.19s
Epoch 125/1000, LR 0.000276
Train loss: 0.1356;  Loss pred: 0.1356; Loss self: 0.0000; time: 0.60s
Val loss: 0.4489 score: 0.8409 time: 0.39s
Test loss: 0.6583 score: 0.7674 time: 0.29s
Epoch 126/1000, LR 0.000276
Train loss: 0.1218;  Loss pred: 0.1218; Loss self: 0.0000; time: 0.43s
Val loss: 0.4457 score: 0.8409 time: 0.24s
Test loss: 0.6538 score: 0.7674 time: 0.10s
Epoch 127/1000, LR 0.000275
Train loss: 0.1187;  Loss pred: 0.1187; Loss self: 0.0000; time: 0.34s
Val loss: 0.4470 score: 0.8409 time: 0.16s
Test loss: 0.6564 score: 0.7674 time: 0.28s
     INFO: Early stopping counter 1 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1148;  Loss pred: 0.1148; Loss self: 0.0000; time: 0.68s
Val loss: 0.4436 score: 0.8409 time: 0.11s
Test loss: 0.6519 score: 0.7674 time: 0.12s
Epoch 129/1000, LR 0.000275
Train loss: 0.1186;  Loss pred: 0.1186; Loss self: 0.0000; time: 0.38s
Val loss: 0.4390 score: 0.8409 time: 0.13s
Test loss: 0.6469 score: 0.7674 time: 0.12s
Epoch 130/1000, LR 0.000275
Train loss: 0.1083;  Loss pred: 0.1083; Loss self: 0.0000; time: 0.33s
Val loss: 0.4353 score: 0.8409 time: 0.20s
Test loss: 0.6431 score: 0.7674 time: 0.13s
Epoch 131/1000, LR 0.000275
Train loss: 0.1111;  Loss pred: 0.1111; Loss self: 0.0000; time: 0.40s
Val loss: 0.4291 score: 0.8409 time: 0.17s
Test loss: 0.6371 score: 0.7674 time: 0.12s
Epoch 132/1000, LR 0.000275
Train loss: 0.1000;  Loss pred: 0.1000; Loss self: 0.0000; time: 0.53s
Val loss: 0.4194 score: 0.8409 time: 0.11s
Test loss: 0.6265 score: 0.7907 time: 0.11s
Epoch 133/1000, LR 0.000274
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.26s
Val loss: 0.4185 score: 0.8409 time: 0.15s
Test loss: 0.6308 score: 0.7907 time: 0.12s
Epoch 134/1000, LR 0.000274
Train loss: 0.1007;  Loss pred: 0.1007; Loss self: 0.0000; time: 0.25s
Val loss: 0.4103 score: 0.8409 time: 0.16s
Test loss: 0.6185 score: 0.7907 time: 0.08s
Epoch 135/1000, LR 0.000274
Train loss: 0.0895;  Loss pred: 0.0895; Loss self: 0.0000; time: 0.31s
Val loss: 0.3988 score: 0.8636 time: 0.18s
Test loss: 0.6029 score: 0.8140 time: 0.14s
Epoch 136/1000, LR 0.000274
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.33s
Val loss: 0.3986 score: 0.8636 time: 0.31s
Test loss: 0.6032 score: 0.8140 time: 0.08s
Epoch 137/1000, LR 0.000274
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.22s
Val loss: 0.4023 score: 0.8636 time: 0.08s
Test loss: 0.6079 score: 0.8140 time: 0.19s
     INFO: Early stopping counter 1 of 20
Epoch 138/1000, LR 0.000274
Train loss: 0.0792;  Loss pred: 0.0792; Loss self: 0.0000; time: 0.30s
Val loss: 0.4074 score: 0.8409 time: 0.18s
Test loss: 0.6133 score: 0.7907 time: 0.17s
     INFO: Early stopping counter 2 of 20
Epoch 139/1000, LR 0.000273
Train loss: 0.0854;  Loss pred: 0.0854; Loss self: 0.0000; time: 0.42s
Val loss: 0.4031 score: 0.8636 time: 0.18s
Test loss: 0.6076 score: 0.8140 time: 0.13s
     INFO: Early stopping counter 3 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0811;  Loss pred: 0.0811; Loss self: 0.0000; time: 0.50s
Val loss: 0.4006 score: 0.8636 time: 0.11s
Test loss: 0.6080 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0711;  Loss pred: 0.0711; Loss self: 0.0000; time: 0.41s
Val loss: 0.4045 score: 0.8636 time: 0.09s
Test loss: 0.6139 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 5 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0652;  Loss pred: 0.0652; Loss self: 0.0000; time: 0.31s
Val loss: 0.4084 score: 0.8636 time: 0.13s
Test loss: 0.6180 score: 0.7907 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 143/1000, LR 0.000273
Train loss: 0.0819;  Loss pred: 0.0819; Loss self: 0.0000; time: 0.30s
Val loss: 0.4024 score: 0.8636 time: 0.13s
Test loss: 0.6121 score: 0.8140 time: 0.12s
     INFO: Early stopping counter 7 of 20
Epoch 144/1000, LR 0.000272
Train loss: 0.0678;  Loss pred: 0.0678; Loss self: 0.0000; time: 0.70s
Val loss: 0.4051 score: 0.8636 time: 0.13s
Test loss: 0.6154 score: 0.8140 time: 0.17s
     INFO: Early stopping counter 8 of 20
Epoch 145/1000, LR 0.000272
Train loss: 0.0602;  Loss pred: 0.0602; Loss self: 0.0000; time: 0.45s
Val loss: 0.4148 score: 0.8636 time: 0.12s
Test loss: 0.6312 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 9 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.45s
Val loss: 0.4144 score: 0.8636 time: 0.14s
Test loss: 0.6345 score: 0.7907 time: 0.43s
     INFO: Early stopping counter 10 of 20
Epoch 147/1000, LR 0.000272
Train loss: 0.0624;  Loss pred: 0.0624; Loss self: 0.0000; time: 0.36s
Val loss: 0.4117 score: 0.8636 time: 0.14s
Test loss: 0.6310 score: 0.8140 time: 0.13s
     INFO: Early stopping counter 11 of 20
Epoch 148/1000, LR 0.000272
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.34s
Val loss: 0.4102 score: 0.8636 time: 0.16s
Test loss: 0.6289 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 12 of 20
Epoch 149/1000, LR 0.000272
Train loss: 0.0789;  Loss pred: 0.0789; Loss self: 0.0000; time: 0.33s
Val loss: 0.4100 score: 0.8636 time: 0.15s
Test loss: 0.6285 score: 0.8140 time: 0.13s
     INFO: Early stopping counter 13 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.29s
Val loss: 0.4126 score: 0.8636 time: 0.11s
Test loss: 0.6341 score: 0.8140 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 151/1000, LR 0.000271
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 0.47s
Val loss: 0.4147 score: 0.8636 time: 0.11s
Test loss: 0.6377 score: 0.8140 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 152/1000, LR 0.000271
Train loss: 0.0647;  Loss pred: 0.0647; Loss self: 0.0000; time: 0.36s
Val loss: 0.4144 score: 0.8636 time: 0.13s
Test loss: 0.6313 score: 0.8140 time: 0.13s
     INFO: Early stopping counter 16 of 20
Epoch 153/1000, LR 0.000271
Train loss: 0.0542;  Loss pred: 0.0542; Loss self: 0.0000; time: 0.27s
Val loss: 0.4179 score: 0.8636 time: 0.11s
Test loss: 0.6382 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 154/1000, LR 0.000271
Train loss: 0.0543;  Loss pred: 0.0543; Loss self: 0.0000; time: 0.29s
Val loss: 0.4215 score: 0.8636 time: 0.10s
Test loss: 0.6448 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 18 of 20
Epoch 155/1000, LR 0.000270
Train loss: 0.0601;  Loss pred: 0.0601; Loss self: 0.0000; time: 0.27s
Val loss: 0.4196 score: 0.8636 time: 0.09s
Test loss: 0.6425 score: 0.8140 time: 0.15s
     INFO: Early stopping counter 19 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0499;  Loss pred: 0.0499; Loss self: 0.0000; time: 0.61s
Val loss: 0.4146 score: 0.8636 time: 0.12s
Test loss: 0.6321 score: 0.8140 time: 0.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 135,   Train_Loss: 0.0828,   Val_Loss: 0.3986,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.3986,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8140,   Test_loss: 0.6032


[0.08941617700020288, 0.07162618600068527, 0.07762893400013127, 0.11839482899995346, 0.1856380259996513, 0.15117231299973355, 0.13549983600023552, 0.11591537700041954, 0.16737943999942217, 0.13914035499965394, 0.1395425829996384, 0.14215353799954755, 0.4293385399996623, 0.11192517600011342, 0.13207026600048266, 0.13043690199992852, 0.12926057200002106, 0.13268188399979408, 0.1187846550001268, 0.090692723999382, 0.06841352799983724, 0.1114647819995298, 0.0887742029999572, 0.11528633600028115, 0.1274756630000411, 0.13348960300027102, 0.12449533899962262, 0.2014785840001423, 0.10132376100045803, 0.13127645599979587, 0.12362730499989993, 0.12029035000068689, 0.11222377800004324, 0.07963736200053972, 0.10553031400013424, 0.10719530999995186, 0.17361591999997472, 0.09600523199969757, 0.3111782129999483, 0.5662405929997476, 0.09725841099952959, 0.13232191200040688, 0.20837922400005482, 0.12818913699993573, 0.12874557599934633, 0.40127614100038045, 0.1574603980006941, 0.34848197500014066, 0.10776111300037883, 0.11690476700005092, 0.13992947599945182, 0.1600348510000913, 0.09129146400027821, 0.11345351199997822, 0.11141766699984146, 0.1265510089997406, 0.11337272800028586, 0.23523348500020802, 0.12522280099983618, 0.12819712700002128, 0.11304734299937991, 0.11432335600056831, 0.10126289900017582, 0.12853859899951203, 0.3665995629999088, 0.11512616899926797, 0.14208400000006804, 0.11313193200021487, 0.06904377299997577, 0.10406517200044618, 0.10762854400036304, 0.11635183799990045, 0.1197483870000724, 0.11968547899959958, 0.12475264899967442, 0.11446585199973924, 0.2005349959999876, 0.3837878060003277, 0.25562222999997175, 0.12076536400036275, 0.11020454699973925, 0.34190496600058395, 0.1252764800001387, 0.10238775200014061, 0.1375826080002298, 0.16363011599969468, 0.10302930800025933, 0.07741801199972542, 0.15405616800035205, 0.09063674999924842, 0.1211364640003012, 0.1134308519995102, 0.14114255499953288, 0.24749154499932047, 0.14692103000015777, 0.15033021500039467, 0.12061050100055581, 0.2202684029998636, 0.13091185099983704, 0.1331273149999106, 0.12165413099955913, 0.13110582100034662, 0.3582287789995462, 0.09737870400022075, 0.10537629800001014, 0.09457579900026758, 0.07188638200022979, 0.15391328100031387, 0.09364649200051645, 0.11227658100051485, 0.19958790599957865, 0.23611627599984786, 0.18208503400001064, 0.15975555600016378, 0.11880449799991766, 0.1329654319997644, 0.15154350499960856, 0.1273883880003268, 0.13343769399943994, 0.3334276180003144, 0.13711649699962436, 0.11569452499952604, 0.09285287700004119, 0.08303998500014131, 0.07303766200038808, 0.08427178999954776, 0.1295107089999874, 0.14186372600033792, 0.14949280400014686, 0.1173508279998714, 0.12155957499999204, 0.16131257799952436, 0.12827518999984022, 0.4300420569998096, 0.12944027699995786, 0.12935693100007484, 0.30927868200069497, 0.1845328419994985, 0.19035991900000226, 0.15488432199981617, 0.17296648400042614, 0.10705629200037947, 0.11323173300024791, 0.10905643000023701, 0.11113104699961696, 0.15514657600033388, 0.11019136799950502, 0.14561134299947298, 0.12406830100007937, 0.19192653199934284, 0.29120637500000157, 0.10861071199997241, 0.28373682600067696, 0.127877609999814, 0.13050002099953417, 0.13715226000022085, 0.12674763999984862, 0.11126135500035161, 0.12388068799918983, 0.08245852199979709, 0.14771843499966053, 0.08889140800056339, 0.19545953200031363, 0.17922576500041032, 0.13312244299959275, 0.11644692099980603, 0.12073521399997844, 0.16477813599976798, 0.12780749999910768, 0.17397124499984784, 0.12243434099946171, 0.43290787000023556, 0.13679370200043195, 0.11122717099988222, 0.13858140699994692, 0.10820057100045233, 0.07822060600028635, 0.13799829599975055, 0.11081912900044699, 0.11390645400024368, 0.15087748999940231, 0.12297654299982241]
[0.0020321858409137017, 0.0016278678636519378, 0.0017642939545484378, 0.0026907915681807604, 0.00421904604544662, 0.0034357343863575807, 0.00307954172727808, 0.0026344403863731713, 0.0038040781818050495, 0.0031622807954466803, 0.0031714223409008728, 0.0032307622272624444, 0.009757694090901416, 0.002543754000002578, 0.003001596954556424, 0.0029644750454529208, 0.0029377402727277513, 0.0030154973636316836, 0.002699651250002882, 0.002061198272713227, 0.00155485290908721, 0.002533290499989314, 0.0020175955227263, 0.00262014400000639, 0.002897174159091843, 0.003033854613642523, 0.002895240441851689, 0.004685548465119588, 0.002356366534894373, 0.0030529408372045553, 0.0028750536046488356, 0.002797450000015974, 0.002609855302326587, 0.0018520316744311564, 0.002454193348840331, 0.002492914186045392, 0.004037579534883133, 0.002232679813946455, 0.0072367026279057745, 0.01316838588371506, 0.002261823511616967, 0.003077253767451323, 0.004846028465117554, 0.0029811427209287377, 0.002994083162775496, 0.009332003279078615, 0.0036618697209463745, 0.008104231976747457, 0.002506072395357647, 0.002718715511629091, 0.003254173860452368, 0.0037217407209323558, 0.0021230573023320516, 0.0026384537674413538, 0.002591108534880034, 0.0029430467209241996, 0.0026365750697740897, 0.0054705461627955355, 0.002912158162786888, 0.002981328534884216, 0.0026290079767297653, 0.002658682697687635, 0.0023549511395389725, 0.0029892697441746986, 0.008525571232556019, 0.002677352767424837, 0.003304279069769024, 0.0026309751627956947, 0.0016056691395343202, 0.002420120279080144, 0.00250298939535728, 0.0027058566976721037, 0.002784846209304009, 0.0027833832325488274, 0.002901224395341266, 0.0026619965581334707, 0.004663604558139246, 0.008925297813961109, 0.005944703023255157, 0.0028084968372177384, 0.002562896441854401, 0.007951278279083348, 0.0029134065116311323, 0.0023811105116311767, 0.0031995955348890653, 0.0038053515348766206, 0.002396030418610682, 0.0018004188837145447, 0.0035827015814035358, 0.0021078313953313585, 0.0028171270697744466, 0.0026379267906862834, 0.003282384999989137, 0.005755617325565592, 0.0034167681395385527, 0.0034960515116370856, 0.0028048953721059493, 0.005122520999996828, 0.003044461651159001, 0.0030959840697653627, 0.0028291658371990495, 0.00304897258140341, 0.00833090183719875, 0.0022646210232609475, 0.0024506115813955846, 0.0021994371860527342, 0.0016717763255867392, 0.003579378627914276, 0.0021778253953608478, 0.002611083279081741, 0.004641579209292527, 0.005491076186042974, 0.004234535674418852, 0.0037152454883759016, 0.0027628953023236666, 0.00309221934883173, 0.0035242675581304314, 0.002962520651170391, 0.003103202186033487, 0.007754130651170102, 0.0031887557441773106, 0.002690570348826187, 0.0021593692325590974, 0.0019311624418637515, 0.0016985502790787927, 0.001959809069756925, 0.003011876953488079, 0.00329915641861251, 0.003476576837212718, 0.0027290890232528234, 0.0028269668604649313, 0.00375145530231452, 0.0029831439534846564, 0.010000978069763014, 0.00301023899999902, 0.0030083007209319727, 0.007192527488388255, 0.004291461441848802, 0.004426974860465169, 0.0036019609767399107, 0.004022476372102933, 0.0024896812093111505, 0.0026332961162848353, 0.00253619604651714, 0.002584442953479464, 0.003608059906984509, 0.002562589953476861, 0.003386310302313325, 0.0028853093255832413, 0.00446340772091495, 0.006772241279069804, 0.0025258305116272655, 0.006598530837225045, 0.0029738979069724187, 0.0030348842092914924, 0.003189587441865601, 0.0029476195348802003, 0.0025874733721012004, 0.0028809462325392983, 0.001917640046506909, 0.0034353124418525704, 0.0020672420465247298, 0.004545570511635201, 0.0041680410465211705, 0.0030958707674323895, 0.002708067930228047, 0.0028077956744181035, 0.003832049674413209, 0.0029722674418397135, 0.004045842906973206, 0.002847310255801435, 0.010067624883726409, 0.0031812488837309754, 0.002586678395346098, 0.0032228234186034168, 0.0025162923488477287, 0.0018190838604717756, 0.0032092626976686174, 0.002577189046522023, 0.0026489873023312484, 0.0035087788371954026, 0.002859919604647033]
[492.080979931631, 614.3004738459624, 566.7989721451746, 371.6378525283169, 237.02040442986961, 291.0585882222861, 324.72364025535444, 379.5872570025006, 262.87577494674315, 316.2274524893187, 315.3159347789486, 309.5244804961523, 102.4832292019128, 393.11977494639285, 333.15598834213904, 337.3278522056903, 340.39768909573473, 331.62025344822723, 370.4182160562156, 485.15468562064393, 643.1476534890099, 394.74351638875146, 495.639482114204, 381.65841266646464, 345.16392356387126, 329.61368534379915, 345.3944568971401, 213.42218684626863, 424.38219402264014, 327.5530229127062, 347.8196018269168, 357.4684087273373, 383.1630049024319, 539.9475688271615, 407.4658585773307, 401.13695272693656, 247.6731396522062, 447.89225654009624, 138.18448144378007, 75.9394514126951, 442.12114467105556, 324.9650745665448, 206.35454521122838, 335.44184013050625, 333.9920588822277, 107.15812779897928, 273.08453773761227, 123.39232179794276, 399.03077095954677, 367.82075789930167, 307.29765614335935, 268.6914739588533, 471.01884574738506, 379.00986264760365, 385.9352036159686, 339.78393645275594, 379.2799270022997, 182.7971047572669, 343.3879425844839, 335.4209334191464, 380.37161121279837, 376.12611721953164, 424.6372602854807, 334.52986367280414, 117.29419328306918, 373.50326492904856, 302.6378761857733, 380.0872065007988, 622.7933111363293, 413.20260345906735, 399.5222679947706, 369.5687213814086, 359.08625641841843, 359.2749960932509, 344.6820596179262, 375.6578861623985, 214.4264136320758, 112.0410792831789, 168.21698175469618, 356.06235575848416, 390.18353752773686, 125.76594163866739, 343.24080625471277, 419.9721076007309, 312.5395035390532, 262.7878110168927, 417.35697186175184, 555.4263005378194, 279.1189769169238, 474.42124745598846, 354.97156330973206, 379.08557717776534, 304.6565226209934, 173.74330909008654, 292.6742345867969, 286.03697533384826, 356.51953721510404, 195.21637881047616, 328.46529685118827, 322.9990779880814, 353.4610756469571, 327.97933510432244, 120.0350237635555, 441.5749874829154, 408.0614029541621, 454.6617681747351, 598.1661450128697, 279.37809993091054, 459.1736335383803, 382.9828056467343, 215.44391572548878, 182.1136633546927, 236.15340072373817, 269.1612177792171, 361.93915822976504, 323.3923235031206, 283.7469015917975, 337.5503895997937, 322.2477750565779, 128.96352215178365, 313.6019439011617, 371.66840868378307, 463.098197807925, 517.8228295673082, 588.7373558010595, 510.2537871834781, 332.01887575184367, 303.1077866931084, 287.6392632247219, 366.42263827952814, 353.7360179155185, 266.5632186482494, 335.21681004762934, 99.99022025889677, 332.1995363159954, 332.4135758908436, 139.03318431725395, 233.0208516493604, 225.88788767030044, 277.6265502201768, 248.60307618841384, 401.65784931022625, 379.7522025023308, 394.2912857124201, 386.93057575663994, 277.1572606275724, 390.2302038776137, 295.3066644001466, 346.5832904407427, 224.04406286123708, 147.6616025318806, 395.90938322926127, 151.54888636097385, 336.25902141948495, 329.5018626867002, 313.52017093944175, 339.2568098313417, 386.47740718117365, 347.10817880088956, 521.474299528505, 291.0943376843846, 483.7362909104496, 219.99438738004858, 239.92086182420007, 323.010899072304, 369.2669555433898, 356.1512716580573, 260.95695123083897, 336.44347945386784, 247.16728330614413, 351.20865313588, 99.32829356966091, 314.34195705781997, 386.59618520770914, 310.2869348123769, 397.410102390497, 549.7272675162167, 311.59805045765006, 388.0196531758209, 377.5027532672381, 284.9994389499079, 349.6601786900295]
Elapsed: 0.14943194149998237~0.07706348321362824
Time per graph: 0.003465131917802722~0.0017903703877145303
Speed: 335.63224501896616~107.22016504217771
Total Time: 0.1238
best val loss: 0.3985688781196421 test_score: 0.8140

Testing...
Test loss: 0.6029 score: 0.8140 time: 0.17s
test Score 0.8140
Epoch Time List: [0.5208752009993987, 0.45217325699923094, 0.3800808929991035, 0.49543678399913915, 0.8109830680014056, 0.691003849999106, 0.6013976949998323, 0.6129287080011636, 0.8046236650006904, 0.7328708429995459, 0.6376232330003404, 0.63827853300063, 1.0060939009999856, 0.6341265930004738, 0.6121218980006233, 0.8940741059996071, 0.7690539780005565, 0.5621425139997882, 0.464749056999608, 0.646321753999473, 0.29892086799918616, 0.4562400440008787, 0.5789435819997379, 0.7219928500007882, 0.624098094999681, 0.5904228160006824, 0.6938225380008589, 0.6424084970003605, 0.6267832750008893, 0.9287443220000569, 0.683869586999208, 0.6591599960001986, 0.6419833149993792, 0.5766205500003707, 0.4686628720000954, 0.4627579149992016, 0.7007774630001222, 0.9120874649997859, 0.9425108830000681, 1.0290073160003885, 0.5406032569999297, 0.7933808460002183, 0.7287012119995779, 0.8213244309999936, 0.6536040149994733, 1.0539748450000843, 0.6376256610001292, 0.9017298380003922, 0.7097497640006623, 0.8025911040003848, 0.4979089059988837, 0.6272299209986159, 0.5206879580000532, 0.777027671999349, 0.7878655799995613, 0.6867239090006478, 0.5456831260007675, 1.2426632610004162, 0.6043903189993216, 0.5530828889995973, 1.018620581000505, 0.6549330950001604, 0.5159046240005409, 0.8013507150008081, 0.9388399649997154, 0.4877647690009326, 0.5299199520004549, 0.4862562890002664, 0.4215563440011465, 0.5845872990003045, 0.43187476199909725, 0.4819010110004456, 0.49442493700007617, 0.6735529830002633, 0.7702514889997474, 0.5209100980000585, 0.6428102339996258, 0.8548654979986168, 0.7404759010005364, 0.6150802540014411, 0.6115368079990731, 0.7786756780005817, 0.5559272140008034, 0.6309747240002253, 0.6114125750009407, 0.805334111000775, 0.5003832630009128, 0.47594249400117405, 0.5472122549999767, 0.5363139549999687, 0.5028416440009096, 0.4520216729997628, 0.6866269960009959, 0.7614281550004307, 0.7668080290004582, 0.8272197440010132, 0.5577721559993734, 0.7439732450011434, 0.8720864240003721, 0.8721632409997255, 0.643223162999675, 0.7414817010003389, 0.8691884419995404, 1.1506823809995694, 0.4662176530000579, 0.5257098860010956, 0.4194229179993272, 0.49930511599905003, 0.47798013600004197, 0.822378277000098, 0.7386611710007855, 0.7561242339997989, 1.2008311610006785, 0.7013223379999545, 0.7583023919996776, 0.7563772009989407, 0.7086683449997508, 0.5571725999998307, 0.5252249660006783, 1.0319730680002976, 0.6263185219995648, 0.6848878860000696, 0.5619610890016702, 0.5258860029989592, 0.6233625269996992, 0.4170996050006579, 0.4743628339992938, 0.5398548290004328, 0.6575828520008145, 0.6816994269993302, 0.8644528770000761, 0.6048013310000897, 0.5516097910012832, 0.8828492090005966, 0.791296352000245, 0.7116890420002164, 0.9934500560011656, 0.683599779999895, 0.6571046650005883, 1.098812588000328, 0.5565209639999011, 0.513732732999415, 0.6510575879992757, 0.5442138149992388, 0.5065541459998713, 0.4943927630001781, 0.6216962289981893, 1.1527235720013778, 0.608646927999871, 0.645796627000891, 1.2764275939998697, 0.7815870450003786, 0.7755430700008219, 0.9135120379987711, 0.6313285670003097, 0.6555667669999821, 0.6867277330002253, 0.7446800140005507, 0.5300300540002354, 0.4910400769995249, 0.6350770899989584, 0.7282391619992268, 0.49454017500102054, 0.6501434729998437, 0.7217509749998499, 0.7223737340000298, 0.618511104999925, 0.5945885370001633, 0.5482898069985822, 1.0048929410004348, 0.6822783529996741, 1.0201162700004716, 0.6260839700007637, 0.6022172709999722, 0.610705331000645, 0.49758735099931073, 0.6469937960000607, 0.6158797930002038, 0.4843325479996565, 0.5004499379992922, 0.4983058669995444, 0.8507829120007955]
Total Epoch List: [26, 156]
Total Time List: [0.13402178100022866, 0.12384335000024294]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7c11f8511f00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7016 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7026 score: 0.4884 time: 0.34s
Epoch 2/1000, LR 0.000015
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7011 score: 0.4884 time: 0.15s
Epoch 3/1000, LR 0.000045
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.4884 time: 0.12s
Epoch 4/1000, LR 0.000075
Train loss: 0.7287;  Loss pred: 0.7287; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.14s
Epoch 5/1000, LR 0.000105
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.40s
Val loss: 0.6891 score: 0.5227 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.4884 time: 0.21s
Epoch 6/1000, LR 0.000135
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.30s
Val loss: 0.6844 score: 0.5455 time: 0.12s
Test loss: 0.6806 score: 0.5116 time: 0.34s
Epoch 7/1000, LR 0.000165
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.33s
Val loss: 0.6807 score: 0.5909 time: 0.10s
Test loss: 0.6748 score: 0.6279 time: 0.09s
Epoch 8/1000, LR 0.000195
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.42s
Val loss: 0.6788 score: 0.6818 time: 0.11s
Test loss: 0.6706 score: 0.8140 time: 0.08s
Epoch 9/1000, LR 0.000225
Train loss: 0.6156;  Loss pred: 0.6156; Loss self: 0.0000; time: 0.41s
Val loss: 0.6794 score: 0.5227 time: 0.13s
Test loss: 0.6685 score: 0.5814 time: 0.12s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6167;  Loss pred: 0.6167; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6826 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6694 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6111;  Loss pred: 0.6111; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6714 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6196;  Loss pred: 0.6196; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6731 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5981;  Loss pred: 0.5981; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6744 score: 0.5116 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5596;  Loss pred: 0.5596; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6755 score: 0.5116 time: 0.36s
     INFO: Early stopping counter 6 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6757 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 7 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5976;  Loss pred: 0.5976; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6758 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5776;  Loss pred: 0.5776; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7011 score: 0.5000 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6758 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6745 score: 0.5116 time: 0.15s
     INFO: Early stopping counter 10 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7029 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6740 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7041 score: 0.5000 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6733 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 12 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5293;  Loss pred: 0.5293; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7035 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6705 score: 0.5116 time: 0.16s
     INFO: Early stopping counter 13 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6670 score: 0.5116 time: 0.17s
     INFO: Early stopping counter 14 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5401;  Loss pred: 0.5401; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6635 score: 0.5116 time: 0.18s
     INFO: Early stopping counter 15 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.5117;  Loss pred: 0.5117; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.5000 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6612 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 16 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.4894;  Loss pred: 0.4894; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7011 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6564 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 17 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.5020;  Loss pred: 0.5020; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6515 score: 0.5116 time: 0.13s
     INFO: Early stopping counter 18 of 20
Epoch 27/1000, LR 0.000285
Train loss: 0.5003;  Loss pred: 0.5003; Loss self: 0.0000; time: 0.29s
Val loss: 0.6996 score: 0.5227 time: 0.26s
Test loss: 0.6477 score: 0.5349 time: 0.09s
     INFO: Early stopping counter 19 of 20
Epoch 28/1000, LR 0.000285
Train loss: 0.4995;  Loss pred: 0.4995; Loss self: 0.0000; time: 0.29s
Val loss: 0.6980 score: 0.5455 time: 0.28s
Test loss: 0.6426 score: 0.5581 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 007,   Train_Loss: 0.6512,   Val_Loss: 0.6788,   Val_Precision: 1.0000,   Val_Recall: 0.3636,   Val_accuracy: 0.5333,   Val_Score: 0.6818,   Val_Loss: 0.6788,   Test_Precision: 1.0000,   Test_Recall: 0.6190,   Test_accuracy: 0.7647,   Test_Score: 0.8140,   Test_loss: 0.6706


[0.08941617700020288, 0.07162618600068527, 0.07762893400013127, 0.11839482899995346, 0.1856380259996513, 0.15117231299973355, 0.13549983600023552, 0.11591537700041954, 0.16737943999942217, 0.13914035499965394, 0.1395425829996384, 0.14215353799954755, 0.4293385399996623, 0.11192517600011342, 0.13207026600048266, 0.13043690199992852, 0.12926057200002106, 0.13268188399979408, 0.1187846550001268, 0.090692723999382, 0.06841352799983724, 0.1114647819995298, 0.0887742029999572, 0.11528633600028115, 0.1274756630000411, 0.13348960300027102, 0.12449533899962262, 0.2014785840001423, 0.10132376100045803, 0.13127645599979587, 0.12362730499989993, 0.12029035000068689, 0.11222377800004324, 0.07963736200053972, 0.10553031400013424, 0.10719530999995186, 0.17361591999997472, 0.09600523199969757, 0.3111782129999483, 0.5662405929997476, 0.09725841099952959, 0.13232191200040688, 0.20837922400005482, 0.12818913699993573, 0.12874557599934633, 0.40127614100038045, 0.1574603980006941, 0.34848197500014066, 0.10776111300037883, 0.11690476700005092, 0.13992947599945182, 0.1600348510000913, 0.09129146400027821, 0.11345351199997822, 0.11141766699984146, 0.1265510089997406, 0.11337272800028586, 0.23523348500020802, 0.12522280099983618, 0.12819712700002128, 0.11304734299937991, 0.11432335600056831, 0.10126289900017582, 0.12853859899951203, 0.3665995629999088, 0.11512616899926797, 0.14208400000006804, 0.11313193200021487, 0.06904377299997577, 0.10406517200044618, 0.10762854400036304, 0.11635183799990045, 0.1197483870000724, 0.11968547899959958, 0.12475264899967442, 0.11446585199973924, 0.2005349959999876, 0.3837878060003277, 0.25562222999997175, 0.12076536400036275, 0.11020454699973925, 0.34190496600058395, 0.1252764800001387, 0.10238775200014061, 0.1375826080002298, 0.16363011599969468, 0.10302930800025933, 0.07741801199972542, 0.15405616800035205, 0.09063674999924842, 0.1211364640003012, 0.1134308519995102, 0.14114255499953288, 0.24749154499932047, 0.14692103000015777, 0.15033021500039467, 0.12061050100055581, 0.2202684029998636, 0.13091185099983704, 0.1331273149999106, 0.12165413099955913, 0.13110582100034662, 0.3582287789995462, 0.09737870400022075, 0.10537629800001014, 0.09457579900026758, 0.07188638200022979, 0.15391328100031387, 0.09364649200051645, 0.11227658100051485, 0.19958790599957865, 0.23611627599984786, 0.18208503400001064, 0.15975555600016378, 0.11880449799991766, 0.1329654319997644, 0.15154350499960856, 0.1273883880003268, 0.13343769399943994, 0.3334276180003144, 0.13711649699962436, 0.11569452499952604, 0.09285287700004119, 0.08303998500014131, 0.07303766200038808, 0.08427178999954776, 0.1295107089999874, 0.14186372600033792, 0.14949280400014686, 0.1173508279998714, 0.12155957499999204, 0.16131257799952436, 0.12827518999984022, 0.4300420569998096, 0.12944027699995786, 0.12935693100007484, 0.30927868200069497, 0.1845328419994985, 0.19035991900000226, 0.15488432199981617, 0.17296648400042614, 0.10705629200037947, 0.11323173300024791, 0.10905643000023701, 0.11113104699961696, 0.15514657600033388, 0.11019136799950502, 0.14561134299947298, 0.12406830100007937, 0.19192653199934284, 0.29120637500000157, 0.10861071199997241, 0.28373682600067696, 0.127877609999814, 0.13050002099953417, 0.13715226000022085, 0.12674763999984862, 0.11126135500035161, 0.12388068799918983, 0.08245852199979709, 0.14771843499966053, 0.08889140800056339, 0.19545953200031363, 0.17922576500041032, 0.13312244299959275, 0.11644692099980603, 0.12073521399997844, 0.16477813599976798, 0.12780749999910768, 0.17397124499984784, 0.12243434099946171, 0.43290787000023556, 0.13679370200043195, 0.11122717099988222, 0.13858140699994692, 0.10820057100045233, 0.07822060600028635, 0.13799829599975055, 0.11081912900044699, 0.11390645400024368, 0.15087748999940231, 0.12297654299982241, 0.34947249200013175, 0.1558451579994653, 0.12326381499951822, 0.14602675599962822, 0.21135212199988018, 0.34039282399953663, 0.09672236000005796, 0.08914824099974794, 0.1295362560003923, 0.10815276900029858, 0.09949718999996549, 0.14063955000074202, 0.17035300499992445, 0.3694458659992961, 0.1437169709997761, 0.1227389489995403, 0.10159128200029954, 0.1578779729998132, 0.12122182699931727, 0.12269431299955613, 0.16259514600005787, 0.17539573300018674, 0.19050837499980844, 0.08807631599938759, 0.12187742599962803, 0.13702303799982474, 0.0949864570002319, 0.10169001299982483]
[0.0020321858409137017, 0.0016278678636519378, 0.0017642939545484378, 0.0026907915681807604, 0.00421904604544662, 0.0034357343863575807, 0.00307954172727808, 0.0026344403863731713, 0.0038040781818050495, 0.0031622807954466803, 0.0031714223409008728, 0.0032307622272624444, 0.009757694090901416, 0.002543754000002578, 0.003001596954556424, 0.0029644750454529208, 0.0029377402727277513, 0.0030154973636316836, 0.002699651250002882, 0.002061198272713227, 0.00155485290908721, 0.002533290499989314, 0.0020175955227263, 0.00262014400000639, 0.002897174159091843, 0.003033854613642523, 0.002895240441851689, 0.004685548465119588, 0.002356366534894373, 0.0030529408372045553, 0.0028750536046488356, 0.002797450000015974, 0.002609855302326587, 0.0018520316744311564, 0.002454193348840331, 0.002492914186045392, 0.004037579534883133, 0.002232679813946455, 0.0072367026279057745, 0.01316838588371506, 0.002261823511616967, 0.003077253767451323, 0.004846028465117554, 0.0029811427209287377, 0.002994083162775496, 0.009332003279078615, 0.0036618697209463745, 0.008104231976747457, 0.002506072395357647, 0.002718715511629091, 0.003254173860452368, 0.0037217407209323558, 0.0021230573023320516, 0.0026384537674413538, 0.002591108534880034, 0.0029430467209241996, 0.0026365750697740897, 0.0054705461627955355, 0.002912158162786888, 0.002981328534884216, 0.0026290079767297653, 0.002658682697687635, 0.0023549511395389725, 0.0029892697441746986, 0.008525571232556019, 0.002677352767424837, 0.003304279069769024, 0.0026309751627956947, 0.0016056691395343202, 0.002420120279080144, 0.00250298939535728, 0.0027058566976721037, 0.002784846209304009, 0.0027833832325488274, 0.002901224395341266, 0.0026619965581334707, 0.004663604558139246, 0.008925297813961109, 0.005944703023255157, 0.0028084968372177384, 0.002562896441854401, 0.007951278279083348, 0.0029134065116311323, 0.0023811105116311767, 0.0031995955348890653, 0.0038053515348766206, 0.002396030418610682, 0.0018004188837145447, 0.0035827015814035358, 0.0021078313953313585, 0.0028171270697744466, 0.0026379267906862834, 0.003282384999989137, 0.005755617325565592, 0.0034167681395385527, 0.0034960515116370856, 0.0028048953721059493, 0.005122520999996828, 0.003044461651159001, 0.0030959840697653627, 0.0028291658371990495, 0.00304897258140341, 0.00833090183719875, 0.0022646210232609475, 0.0024506115813955846, 0.0021994371860527342, 0.0016717763255867392, 0.003579378627914276, 0.0021778253953608478, 0.002611083279081741, 0.004641579209292527, 0.005491076186042974, 0.004234535674418852, 0.0037152454883759016, 0.0027628953023236666, 0.00309221934883173, 0.0035242675581304314, 0.002962520651170391, 0.003103202186033487, 0.007754130651170102, 0.0031887557441773106, 0.002690570348826187, 0.0021593692325590974, 0.0019311624418637515, 0.0016985502790787927, 0.001959809069756925, 0.003011876953488079, 0.00329915641861251, 0.003476576837212718, 0.0027290890232528234, 0.0028269668604649313, 0.00375145530231452, 0.0029831439534846564, 0.010000978069763014, 0.00301023899999902, 0.0030083007209319727, 0.007192527488388255, 0.004291461441848802, 0.004426974860465169, 0.0036019609767399107, 0.004022476372102933, 0.0024896812093111505, 0.0026332961162848353, 0.00253619604651714, 0.002584442953479464, 0.003608059906984509, 0.002562589953476861, 0.003386310302313325, 0.0028853093255832413, 0.00446340772091495, 0.006772241279069804, 0.0025258305116272655, 0.006598530837225045, 0.0029738979069724187, 0.0030348842092914924, 0.003189587441865601, 0.0029476195348802003, 0.0025874733721012004, 0.0028809462325392983, 0.001917640046506909, 0.0034353124418525704, 0.0020672420465247298, 0.004545570511635201, 0.0041680410465211705, 0.0030958707674323895, 0.002708067930228047, 0.0028077956744181035, 0.003832049674413209, 0.0029722674418397135, 0.004045842906973206, 0.002847310255801435, 0.010067624883726409, 0.0031812488837309754, 0.002586678395346098, 0.0032228234186034168, 0.0025162923488477287, 0.0018190838604717756, 0.0032092626976686174, 0.002577189046522023, 0.0026489873023312484, 0.0035087788371954026, 0.002859919604647033, 0.008127267255817017, 0.003624305999987565, 0.002866600348826005, 0.0033959710697587958, 0.00491516562790419, 0.007916112186035735, 0.0022493572093036734, 0.0020732149069708824, 0.0030124710697765652, 0.0025151806744255485, 0.002313888139534081, 0.003270687209319582, 0.003961697790695918, 0.008591764325565025, 0.0033422551395296766, 0.002854394162780007, 0.002362587953495338, 0.0036715807674375166, 0.0028191122557980762, 0.0028533561162687473, 0.003781282465117625, 0.004078970534888064, 0.00443042732557694, 0.002048286418590409, 0.002834358744177396, 0.0031865822790656914, 0.0022089873720984162, 0.00236488402325174]
[492.080979931631, 614.3004738459624, 566.7989721451746, 371.6378525283169, 237.02040442986961, 291.0585882222861, 324.72364025535444, 379.5872570025006, 262.87577494674315, 316.2274524893187, 315.3159347789486, 309.5244804961523, 102.4832292019128, 393.11977494639285, 333.15598834213904, 337.3278522056903, 340.39768909573473, 331.62025344822723, 370.4182160562156, 485.15468562064393, 643.1476534890099, 394.74351638875146, 495.639482114204, 381.65841266646464, 345.16392356387126, 329.61368534379915, 345.3944568971401, 213.42218684626863, 424.38219402264014, 327.5530229127062, 347.8196018269168, 357.4684087273373, 383.1630049024319, 539.9475688271615, 407.4658585773307, 401.13695272693656, 247.6731396522062, 447.89225654009624, 138.18448144378007, 75.9394514126951, 442.12114467105556, 324.9650745665448, 206.35454521122838, 335.44184013050625, 333.9920588822277, 107.15812779897928, 273.08453773761227, 123.39232179794276, 399.03077095954677, 367.82075789930167, 307.29765614335935, 268.6914739588533, 471.01884574738506, 379.00986264760365, 385.9352036159686, 339.78393645275594, 379.2799270022997, 182.7971047572669, 343.3879425844839, 335.4209334191464, 380.37161121279837, 376.12611721953164, 424.6372602854807, 334.52986367280414, 117.29419328306918, 373.50326492904856, 302.6378761857733, 380.0872065007988, 622.7933111363293, 413.20260345906735, 399.5222679947706, 369.5687213814086, 359.08625641841843, 359.2749960932509, 344.6820596179262, 375.6578861623985, 214.4264136320758, 112.0410792831789, 168.21698175469618, 356.06235575848416, 390.18353752773686, 125.76594163866739, 343.24080625471277, 419.9721076007309, 312.5395035390532, 262.7878110168927, 417.35697186175184, 555.4263005378194, 279.1189769169238, 474.42124745598846, 354.97156330973206, 379.08557717776534, 304.6565226209934, 173.74330909008654, 292.6742345867969, 286.03697533384826, 356.51953721510404, 195.21637881047616, 328.46529685118827, 322.9990779880814, 353.4610756469571, 327.97933510432244, 120.0350237635555, 441.5749874829154, 408.0614029541621, 454.6617681747351, 598.1661450128697, 279.37809993091054, 459.1736335383803, 382.9828056467343, 215.44391572548878, 182.1136633546927, 236.15340072373817, 269.1612177792171, 361.93915822976504, 323.3923235031206, 283.7469015917975, 337.5503895997937, 322.2477750565779, 128.96352215178365, 313.6019439011617, 371.66840868378307, 463.098197807925, 517.8228295673082, 588.7373558010595, 510.2537871834781, 332.01887575184367, 303.1077866931084, 287.6392632247219, 366.42263827952814, 353.7360179155185, 266.5632186482494, 335.21681004762934, 99.99022025889677, 332.1995363159954, 332.4135758908436, 139.03318431725395, 233.0208516493604, 225.88788767030044, 277.6265502201768, 248.60307618841384, 401.65784931022625, 379.7522025023308, 394.2912857124201, 386.93057575663994, 277.1572606275724, 390.2302038776137, 295.3066644001466, 346.5832904407427, 224.04406286123708, 147.6616025318806, 395.90938322926127, 151.54888636097385, 336.25902141948495, 329.5018626867002, 313.52017093944175, 339.2568098313417, 386.47740718117365, 347.10817880088956, 521.474299528505, 291.0943376843846, 483.7362909104496, 219.99438738004858, 239.92086182420007, 323.010899072304, 369.2669555433898, 356.1512716580573, 260.95695123083897, 336.44347945386784, 247.16728330614413, 351.20865313588, 99.32829356966091, 314.34195705781997, 386.59618520770914, 310.2869348123769, 397.410102390497, 549.7272675162167, 311.59805045765006, 388.0196531758209, 377.5027532672381, 284.9994389499079, 349.6601786900295, 123.04258842777185, 275.91489239689776, 348.84527953453386, 294.4665839191105, 203.45194357700547, 126.32463720815258, 444.57145173023315, 482.3426633860513, 331.9533953480157, 397.5857520567161, 432.1730091072413, 305.74614324187706, 252.41703250270854, 116.39052959407572, 299.19918086825663, 350.3370393057631, 423.26466556326375, 272.3622503061328, 354.72159646828436, 350.46449137504493, 264.4605393077644, 245.15989793180555, 225.7118617490871, 488.2129720355128, 352.8134898429118, 313.8158416838999, 452.69611435128087, 422.85371720892664]
Elapsed: 0.15032597893329824~0.07680348671346808
Time per graph: 0.0034872607543432925~0.0017846637773890174
Speed: 333.5065150165722~106.68644791167202
Total Time: 0.1027
best val loss: 0.6788338747891512 test_score: 0.8140

Testing...
Test loss: 0.6706 score: 0.8140 time: 0.10s
test Score 0.8140
Epoch Time List: [0.5208752009993987, 0.45217325699923094, 0.3800808929991035, 0.49543678399913915, 0.8109830680014056, 0.691003849999106, 0.6013976949998323, 0.6129287080011636, 0.8046236650006904, 0.7328708429995459, 0.6376232330003404, 0.63827853300063, 1.0060939009999856, 0.6341265930004738, 0.6121218980006233, 0.8940741059996071, 0.7690539780005565, 0.5621425139997882, 0.464749056999608, 0.646321753999473, 0.29892086799918616, 0.4562400440008787, 0.5789435819997379, 0.7219928500007882, 0.624098094999681, 0.5904228160006824, 0.6938225380008589, 0.6424084970003605, 0.6267832750008893, 0.9287443220000569, 0.683869586999208, 0.6591599960001986, 0.6419833149993792, 0.5766205500003707, 0.4686628720000954, 0.4627579149992016, 0.7007774630001222, 0.9120874649997859, 0.9425108830000681, 1.0290073160003885, 0.5406032569999297, 0.7933808460002183, 0.7287012119995779, 0.8213244309999936, 0.6536040149994733, 1.0539748450000843, 0.6376256610001292, 0.9017298380003922, 0.7097497640006623, 0.8025911040003848, 0.4979089059988837, 0.6272299209986159, 0.5206879580000532, 0.777027671999349, 0.7878655799995613, 0.6867239090006478, 0.5456831260007675, 1.2426632610004162, 0.6043903189993216, 0.5530828889995973, 1.018620581000505, 0.6549330950001604, 0.5159046240005409, 0.8013507150008081, 0.9388399649997154, 0.4877647690009326, 0.5299199520004549, 0.4862562890002664, 0.4215563440011465, 0.5845872990003045, 0.43187476199909725, 0.4819010110004456, 0.49442493700007617, 0.6735529830002633, 0.7702514889997474, 0.5209100980000585, 0.6428102339996258, 0.8548654979986168, 0.7404759010005364, 0.6150802540014411, 0.6115368079990731, 0.7786756780005817, 0.5559272140008034, 0.6309747240002253, 0.6114125750009407, 0.805334111000775, 0.5003832630009128, 0.47594249400117405, 0.5472122549999767, 0.5363139549999687, 0.5028416440009096, 0.4520216729997628, 0.6866269960009959, 0.7614281550004307, 0.7668080290004582, 0.8272197440010132, 0.5577721559993734, 0.7439732450011434, 0.8720864240003721, 0.8721632409997255, 0.643223162999675, 0.7414817010003389, 0.8691884419995404, 1.1506823809995694, 0.4662176530000579, 0.5257098860010956, 0.4194229179993272, 0.49930511599905003, 0.47798013600004197, 0.822378277000098, 0.7386611710007855, 0.7561242339997989, 1.2008311610006785, 0.7013223379999545, 0.7583023919996776, 0.7563772009989407, 0.7086683449997508, 0.5571725999998307, 0.5252249660006783, 1.0319730680002976, 0.6263185219995648, 0.6848878860000696, 0.5619610890016702, 0.5258860029989592, 0.6233625269996992, 0.4170996050006579, 0.4743628339992938, 0.5398548290004328, 0.6575828520008145, 0.6816994269993302, 0.8644528770000761, 0.6048013310000897, 0.5516097910012832, 0.8828492090005966, 0.791296352000245, 0.7116890420002164, 0.9934500560011656, 0.683599779999895, 0.6571046650005883, 1.098812588000328, 0.5565209639999011, 0.513732732999415, 0.6510575879992757, 0.5442138149992388, 0.5065541459998713, 0.4943927630001781, 0.6216962289981893, 1.1527235720013778, 0.608646927999871, 0.645796627000891, 1.2764275939998697, 0.7815870450003786, 0.7755430700008219, 0.9135120379987711, 0.6313285670003097, 0.6555667669999821, 0.6867277330002253, 0.7446800140005507, 0.5300300540002354, 0.4910400769995249, 0.6350770899989584, 0.7282391619992268, 0.49454017500102054, 0.6501434729998437, 0.7217509749998499, 0.7223737340000298, 0.618511104999925, 0.5945885370001633, 0.5482898069985822, 1.0048929410004348, 0.6822783529996741, 1.0201162700004716, 0.6260839700007637, 0.6022172709999722, 0.610705331000645, 0.49758735099931073, 0.6469937960000607, 0.6158797930002038, 0.4843325479996565, 0.5004499379992922, 0.4983058669995444, 0.8507829120007955, 0.98730249899927, 0.7218812789997173, 0.8584729520007386, 0.8449261119994844, 0.720879743000296, 0.7564937049992295, 0.5268212379987744, 0.6141006180005206, 0.6636332950001815, 0.7334628690005047, 0.7496914610010208, 0.4423804919997565, 0.653028175000145, 0.9661056190007002, 0.8094308819991056, 0.6980186669998147, 1.042921803000354, 0.588700294998489, 0.5277400139993915, 1.0722634709991326, 0.7241706119984883, 0.6920218460008982, 0.7512850590010203, 0.8341013599992948, 0.459669670999574, 0.532507092000742, 0.6415964470006656, 0.6573607530008303]
Total Epoch List: [26, 156, 28]
Total Time List: [0.13402178100022866, 0.12384335000024294, 0.10271456400005263]
T-times Epoch Time: 0.5800375750360139 ~ 0.15673306937316434
T-times Total Epoch: 93.33333333333333 ~ 16.555182695279267
T-times Total Time: 0.13172444311122591 ~ 0.023793467967241577
T-times Inference Elapsed: 0.13006100408208235 ~ 0.0355001407994784
T-times Time Per Graph: 0.003004481776097271 ~ 0.0008276739223965849
T-times Speed: 420.4839693816771 ~ 138.32348738942483
T-times cross validation test micro f1 score:0.6757223382094383 ~ 0.040279566364593636
T-times cross validation test precision:0.9087682379349046 ~ 0.06809911998960794
T-times cross validation test recall:0.569023569023569 ~ 0.0751662331131443
T-times cross validation test f1_score:0.6757223382094383 ~ 0.06737762209460887
