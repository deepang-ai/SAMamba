Namespace(seed=15, model='FAGNN', dataset='mining/Times', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Times/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=10, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
Data(edge_index=[2, 72], edge_attr=[72, 2], x=[25, 14887], y=[1, 1], num_nodes=25)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcd300>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.43s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.6931,   Val_Loss: 0.6933,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6933,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6932


[0.4328834720654413, 0.22508069791365415, 0.21878322307020426]
[0.00983826072876003, 0.005115470407128503, 0.0049723459788682785]
[101.64398236333746, 195.48544325590885, 201.11231282976877]
Elapsed: 0.29224913101643324~0.09947672410859909
Time per graph: 0.006642025704918938~0.0022608346388317976
Speed: 166.08057948300504~45.62142553356607
Total Time: 0.2190
best val loss: 0.6932650804519653 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.20s
test Score 0.5000
Epoch Time List: [1.1145050170598552, 0.77490505494643, 0.8007492340402678]
Total Epoch List: [3]
Total Time List: [0.2190348730655387]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcc520>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.20s
Epoch 2/1000, LR 0.000000
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.22s
Epoch 4/1000, LR 0.000060
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.22s
Epoch 5/1000, LR 0.000090
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.19s
Epoch 6/1000, LR 0.000120
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 0.21s
Epoch 7/1000, LR 0.000150
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.20s
Epoch 8/1000, LR 0.000180
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.18s
Epoch 10/1000, LR 0.000240
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.20s
Epoch 11/1000, LR 0.000270
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.22s
Epoch 12/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.20s
Epoch 13/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.18s
Epoch 16/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.35s
Epoch 17/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.20s
Epoch 18/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.21s
Epoch 22/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.24s
Test loss: 0.6916 score: 0.5581 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.38s
Val loss: 0.6921 score: 0.5682 time: 0.21s
Test loss: 0.6915 score: 0.5814 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.32s
Val loss: 0.6919 score: 0.5909 time: 0.23s
Test loss: 0.6914 score: 0.5349 time: 0.20s
Epoch 26/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.36s
Val loss: 0.6917 score: 0.6136 time: 0.20s
Test loss: 0.6913 score: 0.6744 time: 0.21s
Epoch 27/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.35s
Val loss: 0.6915 score: 0.5682 time: 0.23s
Test loss: 0.6912 score: 0.6512 time: 0.18s
Epoch 28/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.39s
Val loss: 0.6913 score: 0.5682 time: 0.20s
Test loss: 0.6910 score: 0.5581 time: 0.22s
Epoch 29/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.40s
Val loss: 0.6910 score: 0.5227 time: 0.25s
Test loss: 0.6909 score: 0.4651 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.36s
Val loss: 0.6908 score: 0.5227 time: 0.21s
Test loss: 0.6907 score: 0.5581 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.49s
Val loss: 0.6905 score: 0.5227 time: 0.23s
Test loss: 0.6905 score: 0.5116 time: 0.20s
Epoch 32/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.22s
Test loss: 0.6903 score: 0.5116 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.22s
Test loss: 0.6900 score: 0.5116 time: 0.19s
Epoch 34/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.21s
Test loss: 0.6897 score: 0.5116 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.23s
Test loss: 0.6894 score: 0.5116 time: 0.20s
Epoch 36/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.20s
Test loss: 0.6890 score: 0.5116 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.5000 time: 0.24s
Test loss: 0.6885 score: 0.5116 time: 0.21s
Epoch 38/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.20s
Test loss: 0.6880 score: 0.5116 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5000 time: 0.23s
Test loss: 0.6875 score: 0.5116 time: 0.19s
Epoch 40/1000, LR 0.000269
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5000 time: 0.22s
Test loss: 0.6869 score: 0.5116 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5000 time: 0.26s
Test loss: 0.6862 score: 0.5116 time: 0.21s
Epoch 42/1000, LR 0.000269
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5000 time: 0.24s
Test loss: 0.6854 score: 0.5116 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6818 score: 0.5000 time: 0.24s
Test loss: 0.6846 score: 0.5116 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6804 score: 0.5000 time: 0.21s
Test loss: 0.6836 score: 0.5116 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6788 score: 0.5000 time: 0.24s
Test loss: 0.6825 score: 0.5116 time: 0.19s
Epoch 46/1000, LR 0.000269
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.5000 time: 0.22s
Test loss: 0.6813 score: 0.5116 time: 0.22s
Epoch 47/1000, LR 0.000269
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.32s
Val loss: 0.6750 score: 0.5455 time: 0.22s
Test loss: 0.6799 score: 0.5116 time: 0.19s
Epoch 48/1000, LR 0.000269
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.48s
Val loss: 0.6728 score: 0.5682 time: 0.22s
Test loss: 0.6784 score: 0.5116 time: 0.21s
Epoch 49/1000, LR 0.000269
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.32s
Val loss: 0.6704 score: 0.6136 time: 0.23s
Test loss: 0.6768 score: 0.5349 time: 0.20s
Epoch 50/1000, LR 0.000269
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.40s
Val loss: 0.6677 score: 0.6364 time: 0.20s
Test loss: 0.6750 score: 0.5349 time: 0.21s
Epoch 51/1000, LR 0.000269
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.35s
Val loss: 0.6648 score: 0.6591 time: 0.22s
Test loss: 0.6729 score: 0.5581 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.35s
Val loss: 0.6616 score: 0.7273 time: 0.23s
Test loss: 0.6708 score: 0.5581 time: 0.19s
Epoch 53/1000, LR 0.000269
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 0.35s
Val loss: 0.6582 score: 0.7273 time: 0.21s
Test loss: 0.6684 score: 0.5581 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.41s
Val loss: 0.6546 score: 0.7500 time: 0.24s
Test loss: 0.6658 score: 0.5581 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.6341;  Loss pred: 0.6341; Loss self: 0.0000; time: 0.36s
Val loss: 0.6506 score: 0.7500 time: 0.20s
Test loss: 0.6631 score: 0.5581 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.32s
Val loss: 0.6463 score: 0.7500 time: 0.23s
Test loss: 0.6601 score: 0.5581 time: 0.29s
Epoch 57/1000, LR 0.000269
Train loss: 0.6223;  Loss pred: 0.6223; Loss self: 0.0000; time: 0.36s
Val loss: 0.6418 score: 0.7500 time: 0.23s
Test loss: 0.6569 score: 0.5581 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.6169;  Loss pred: 0.6169; Loss self: 0.0000; time: 0.32s
Val loss: 0.6368 score: 0.7500 time: 0.22s
Test loss: 0.6535 score: 0.5814 time: 0.19s
Epoch 59/1000, LR 0.000268
Train loss: 0.6111;  Loss pred: 0.6111; Loss self: 0.0000; time: 0.45s
Val loss: 0.6315 score: 0.7500 time: 0.21s
Test loss: 0.6499 score: 0.6279 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.5974;  Loss pred: 0.5974; Loss self: 0.0000; time: 0.32s
Val loss: 0.6258 score: 0.7500 time: 0.24s
Test loss: 0.6461 score: 0.6512 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.5906;  Loss pred: 0.5906; Loss self: 0.0000; time: 0.33s
Val loss: 0.6197 score: 0.7500 time: 0.22s
Test loss: 0.6420 score: 0.6744 time: 0.19s
Epoch 62/1000, LR 0.000268
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.34s
Val loss: 0.6133 score: 0.7500 time: 0.21s
Test loss: 0.6377 score: 0.6744 time: 0.22s
Epoch 63/1000, LR 0.000268
Train loss: 0.5723;  Loss pred: 0.5723; Loss self: 0.0000; time: 0.32s
Val loss: 0.6066 score: 0.7727 time: 0.23s
Test loss: 0.6332 score: 0.6744 time: 0.19s
Epoch 64/1000, LR 0.000268
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.42s
Val loss: 0.5998 score: 0.7727 time: 0.20s
Test loss: 0.6285 score: 0.6512 time: 0.30s
Epoch 65/1000, LR 0.000268
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.34s
Val loss: 0.5926 score: 0.7727 time: 0.22s
Test loss: 0.6238 score: 0.6512 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.5335;  Loss pred: 0.5335; Loss self: 0.0000; time: 0.33s
Val loss: 0.5855 score: 0.7727 time: 0.22s
Test loss: 0.6193 score: 0.6512 time: 0.19s
Epoch 67/1000, LR 0.000268
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.33s
Val loss: 0.5782 score: 0.7727 time: 0.20s
Test loss: 0.6149 score: 0.6512 time: 0.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.5123;  Loss pred: 0.5123; Loss self: 0.0000; time: 0.31s
Val loss: 0.5711 score: 0.7727 time: 0.23s
Test loss: 0.6106 score: 0.6512 time: 0.19s
Epoch 69/1000, LR 0.000268
Train loss: 0.4952;  Loss pred: 0.4952; Loss self: 0.0000; time: 0.39s
Val loss: 0.5641 score: 0.7727 time: 0.20s
Test loss: 0.6065 score: 0.6512 time: 0.20s
Epoch 70/1000, LR 0.000268
Train loss: 0.4802;  Loss pred: 0.4802; Loss self: 0.0000; time: 0.36s
Val loss: 0.5573 score: 0.7727 time: 0.22s
Test loss: 0.6025 score: 0.6512 time: 0.21s
Epoch 71/1000, LR 0.000268
Train loss: 0.4713;  Loss pred: 0.4713; Loss self: 0.0000; time: 0.30s
Val loss: 0.5505 score: 0.7727 time: 0.23s
Test loss: 0.5985 score: 0.6512 time: 0.21s
Epoch 72/1000, LR 0.000267
Train loss: 0.4610;  Loss pred: 0.4610; Loss self: 0.0000; time: 0.36s
Val loss: 0.5440 score: 0.7727 time: 0.22s
Test loss: 0.5947 score: 0.6744 time: 0.21s
Epoch 73/1000, LR 0.000267
Train loss: 0.4540;  Loss pred: 0.4540; Loss self: 0.0000; time: 0.35s
Val loss: 0.5377 score: 0.7727 time: 0.30s
Test loss: 0.5909 score: 0.6977 time: 0.21s
Epoch 74/1000, LR 0.000267
Train loss: 0.4426;  Loss pred: 0.4426; Loss self: 0.0000; time: 0.36s
Val loss: 0.5318 score: 0.7727 time: 0.20s
Test loss: 0.5870 score: 0.6977 time: 0.20s
Epoch 75/1000, LR 0.000267
Train loss: 0.4226;  Loss pred: 0.4226; Loss self: 0.0000; time: 0.34s
Val loss: 0.5262 score: 0.7727 time: 0.22s
Test loss: 0.5830 score: 0.6977 time: 0.20s
Epoch 76/1000, LR 0.000267
Train loss: 0.4092;  Loss pred: 0.4092; Loss self: 0.0000; time: 0.38s
Val loss: 0.5208 score: 0.7727 time: 0.22s
Test loss: 0.5788 score: 0.7209 time: 0.19s
Epoch 77/1000, LR 0.000267
Train loss: 0.3921;  Loss pred: 0.3921; Loss self: 0.0000; time: 0.35s
Val loss: 0.5160 score: 0.7727 time: 0.22s
Test loss: 0.5754 score: 0.7209 time: 0.20s
Epoch 78/1000, LR 0.000267
Train loss: 0.3754;  Loss pred: 0.3754; Loss self: 0.0000; time: 0.31s
Val loss: 0.5118 score: 0.7955 time: 0.23s
Test loss: 0.5722 score: 0.7209 time: 0.21s
Epoch 79/1000, LR 0.000267
Train loss: 0.3601;  Loss pred: 0.3601; Loss self: 0.0000; time: 0.37s
Val loss: 0.5081 score: 0.7955 time: 0.20s
Test loss: 0.5697 score: 0.7209 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.3601;  Loss pred: 0.3601; Loss self: 0.0000; time: 0.35s
Val loss: 0.5047 score: 0.7955 time: 0.23s
Test loss: 0.5672 score: 0.7209 time: 0.18s
Epoch 81/1000, LR 0.000267
Train loss: 0.3437;  Loss pred: 0.3437; Loss self: 0.0000; time: 0.48s
Val loss: 0.5017 score: 0.7955 time: 0.20s
Test loss: 0.5649 score: 0.7442 time: 0.21s
Epoch 82/1000, LR 0.000267
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.34s
Val loss: 0.4986 score: 0.7955 time: 0.23s
Test loss: 0.5618 score: 0.7674 time: 0.20s
Epoch 83/1000, LR 0.000266
Train loss: 0.3202;  Loss pred: 0.3202; Loss self: 0.0000; time: 0.39s
Val loss: 0.4952 score: 0.7955 time: 0.20s
Test loss: 0.5576 score: 0.7674 time: 0.19s
Epoch 84/1000, LR 0.000266
Train loss: 0.2960;  Loss pred: 0.2960; Loss self: 0.0000; time: 0.35s
Val loss: 0.4917 score: 0.8182 time: 0.22s
Test loss: 0.5529 score: 0.7907 time: 0.20s
Epoch 85/1000, LR 0.000266
Train loss: 0.2960;  Loss pred: 0.2960; Loss self: 0.0000; time: 0.31s
Val loss: 0.4890 score: 0.8409 time: 0.23s
Test loss: 0.5491 score: 0.8140 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.37s
Val loss: 0.4870 score: 0.8409 time: 0.24s
Test loss: 0.5463 score: 0.8140 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.2588;  Loss pred: 0.2588; Loss self: 0.0000; time: 0.36s
Val loss: 0.4851 score: 0.8409 time: 0.25s
Test loss: 0.5434 score: 0.8372 time: 0.18s
Epoch 88/1000, LR 0.000266
Train loss: 0.2574;  Loss pred: 0.2574; Loss self: 0.0000; time: 0.48s
Val loss: 0.4821 score: 0.8409 time: 0.28s
Test loss: 0.5381 score: 0.8372 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.32s
Val loss: 0.4804 score: 0.8409 time: 0.32s
Test loss: 0.5343 score: 0.8372 time: 0.19s
Epoch 90/1000, LR 0.000266
Train loss: 0.2140;  Loss pred: 0.2140; Loss self: 0.0000; time: 0.36s
Val loss: 0.4801 score: 0.8409 time: 0.20s
Test loss: 0.5323 score: 0.8372 time: 0.22s
Epoch 91/1000, LR 0.000266
Train loss: 0.2208;  Loss pred: 0.2208; Loss self: 0.0000; time: 0.32s
Val loss: 0.4804 score: 0.8409 time: 0.23s
Test loss: 0.5310 score: 0.8372 time: 0.19s
     INFO: Early stopping counter 1 of 2
Epoch 92/1000, LR 0.000266
Train loss: 0.1955;  Loss pred: 0.1955; Loss self: 0.0000; time: 0.51s
Val loss: 0.4806 score: 0.8409 time: 0.26s
Test loss: 0.5294 score: 0.8372 time: 0.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.2140,   Val_Loss: 0.4801,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.4801,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.5323


[0.4328834720654413, 0.22508069791365415, 0.21878322307020426, 0.2060844919178635, 0.2109182069543749, 0.22917460708413273, 0.22530515200924128, 0.1928865979425609, 0.21519464498851448, 0.206509641953744, 0.25485545909032226, 0.18893125094473362, 0.20399433304555714, 0.2263384769903496, 0.20412931602913886, 0.24255537101998925, 0.24326441704761237, 0.18890444107819349, 0.3516630900558084, 0.20379606098867953, 0.23481170402374119, 0.2110923359869048, 0.22722931497264653, 0.21192577306646854, 0.22601611400023103, 0.20127717591822147, 0.23770815203897655, 0.20896173303481191, 0.2180363649968058, 0.1883169619832188, 0.21944448607973754, 0.2427116099279374, 0.22024962806608528, 0.20086082501802593, 0.2066709800856188, 0.19859043892938644, 0.22831305500585586, 0.1998794369865209, 0.2226204019971192, 0.2135907900519669, 0.21898180106654763, 0.18972882302477956, 0.23913783894386142, 0.21864233107771724, 0.2271022170316428, 0.21296891197562218, 0.23918740497902036, 0.1970461109885946, 0.22141923801973462, 0.19711707695387304, 0.21813808905426413, 0.19963740196544677, 0.21721633290871978, 0.2184286559931934, 0.19821578194387257, 0.22264727705623955, 0.21965861495118588, 0.2193506780313328, 0.2933899760246277, 0.21869181096553802, 0.19478022900875658, 0.21207246696576476, 0.20754416997078806, 0.19658456603065133, 0.2195595409721136, 0.1936187690589577, 0.3083338920259848, 0.20255063893273473, 0.19642358203418553, 0.21909096697345376, 0.1939482579473406, 0.2082974889781326, 0.21157478005625308, 0.2108664729166776, 0.2165022320114076, 0.2122620870359242, 0.20913149102125317, 0.2062095010187477, 0.19874843198340386, 0.2027198210125789, 0.2124674569349736, 0.21831288700923324, 0.18471952306572348, 0.2165106690954417, 0.20229291601572186, 0.19803632190451026, 0.20133076107595116, 0.22090682294219732, 0.22182104201056063, 0.18532476900145411, 0.2327380779897794, 0.19729728903621435, 0.2224583759671077, 0.1917282110080123, 0.22898211400024593]
[0.00983826072876003, 0.005115470407128503, 0.0049723459788682785, 0.004792662602741011, 0.0049050745803343, 0.005329642025212389, 0.005239654697889332, 0.004485734835873509, 0.005004526627639872, 0.004802549812877767, 0.0059268711416354015, 0.00439375002197055, 0.004744054256873422, 0.005263685511403479, 0.004747193396026485, 0.005640822581860216, 0.005657312024363079, 0.004393126536702174, 0.008178211396646708, 0.004739443278806501, 0.005460737302877702, 0.004909124092718716, 0.005284402673782477, 0.004928506350382989, 0.005256188697679792, 0.0046808645562377085, 0.005528096559045966, 0.004859575186856091, 0.0050706131394606, 0.004379464232167879, 0.005103360141389245, 0.005644456044835753, 0.00512208437362989, 0.0046711819771633935, 0.004806301862456251, 0.004618382300683405, 0.005309605930368741, 0.004648358999686532, 0.005177218651095795, 0.004967227675627137, 0.005092600024803434, 0.004412298209878595, 0.005561345091717708, 0.005084705373900401, 0.005281446907712623, 0.004952765394781911, 0.005562497790209776, 0.004582467697409176, 0.0051492846051101075, 0.004584118068694722, 0.005072978815215445, 0.004642730278266204, 0.0050515426257841806, 0.0050797361858882185, 0.00460966934753192, 0.005177843652470687, 0.005108339882585718, 0.005101178558868205, 0.006823022698247155, 0.005085856068966, 0.0045297727676455015, 0.004931917836413134, 0.004826608603971816, 0.004571734093736077, 0.005106035836560782, 0.004502762071138551, 0.007170555628511275, 0.004710479975179877, 0.00456799027986478, 0.005095138766824506, 0.004510424603426526, 0.004844127650654247, 0.004920343722238443, 0.004903871463178549, 0.00503493562817227, 0.0049363276054866095, 0.004863523047005888, 0.004795569791133667, 0.004622056557753578, 0.0047144144421529975, 0.004941103649650549, 0.005077043883935656, 0.004295802861993569, 0.005035131839428877, 0.004704486418970276, 0.0046054958582444246, 0.004682110722696539, 0.005137367975399937, 0.005158628883966526, 0.004309878348871026, 0.005412513441622777, 0.004588309047353822, 0.005173450603886226, 0.004458795604837495, 0.005325165441866184]
[101.64398236333746, 195.48544325590885, 201.11231282976877, 208.65228431229056, 203.87049852600734, 187.62986243154847, 190.85227131528833, 222.9289150136021, 199.81909866899812, 208.22272312898374, 168.72308779840793, 227.596015931628, 210.79016930532578, 189.98095494754696, 210.65078175180813, 177.27910876257744, 176.7623909894883, 227.6283170187669, 122.27612512071131, 210.99524589137454, 183.1254544094292, 203.70232675177522, 189.23614677611585, 202.90122988728442, 190.25192159509496, 213.63574783795067, 180.89409063661128, 205.77930406442613, 197.21480864272317, 228.33843296511864, 195.94932991105313, 177.16499022344655, 195.23301981285513, 214.07857901679452, 208.06017362566402, 216.52603333682984, 188.33789420800804, 215.12968341460638, 193.1539050969661, 201.31954186572392, 196.3633497878323, 226.6392597311584, 179.8126143060715, 196.66822882854962, 189.3420529400146, 201.9074032970693, 179.77535231745009, 218.2230330975115, 194.20173416082076, 218.14446857926137, 197.12284171199144, 215.39050086136885, 197.95933125374034, 196.8606170489826, 216.9352993909235, 193.13059009088363, 195.7583134608937, 196.0331300815844, 146.56260783903028, 196.62373186335745, 220.76162564767742, 202.7608798785821, 207.18481278492317, 218.73538125722175, 195.84664738145656, 222.0859073166937, 139.45920676270055, 212.29259125803063, 218.9146514623498, 196.26550831376863, 221.7086167985847, 206.4355178305304, 203.2378338692695, 203.92051616944883, 198.61227110921567, 202.57974752091496, 205.6122671435938, 208.52579433811164, 216.35390815858432, 212.11542011637738, 202.38393502850792, 196.96501012412233, 232.78535634103244, 198.6045314979136, 212.5630538474126, 217.13188563830158, 213.57888764836306, 194.6522041614415, 193.84995945494126, 232.02511046789763, 184.75704694050248, 217.94521460508895, 193.29458741691928, 224.2758109196723, 187.78759287703065]
Elapsed: 0.21823171821398366~0.03247774605291213
Time per graph: 0.005070278372879728~0.0007394691671906392
Speed: 200.13048374923716~20.69892552440747
Total Time: 0.2297
best val loss: 0.48009157180786133 test_score: 0.8372

Testing...
Test loss: 0.5491 score: 0.8140 time: 0.20s
test Score 0.8140
Epoch Time List: [1.1145050170598552, 0.77490505494643, 0.8007492340402678, 0.7980877921218053, 0.8299681359203532, 0.789919356815517, 0.8976561198942363, 0.8400031649507582, 0.9143048329278827, 0.7483091459143907, 0.9128950570011511, 0.7660714099183679, 0.8242860989412293, 0.7770778930280358, 0.8442487401189283, 0.8258887599222362, 0.8954029959859326, 0.8692364228190854, 1.0065824908670038, 0.7660452370764688, 0.8134961730102077, 0.7706379290902987, 0.8130793418968096, 0.7980231149122119, 0.9313583469484001, 0.7616070358781144, 0.8221791739342734, 0.7544190760236233, 0.7733732131309807, 0.7616092229727656, 0.8054097110871226, 0.8959052880527452, 0.7838183880085126, 0.919333896948956, 0.7894921938423067, 0.7589556731982157, 0.8056486920686439, 0.8617826780537143, 0.789630405139178, 0.814330643042922, 0.7694699009880424, 0.7603100358974189, 0.9698563690762967, 1.0179908119607717, 0.8198053169762716, 0.8356421128846705, 0.860624223947525, 0.8626375609310344, 0.7877580390777439, 0.7342484148684889, 0.9131057731574401, 0.745980296866037, 0.8166510320734233, 0.7858324549160898, 0.7712801268789917, 0.7768004758981988, 0.8616417180746794, 0.7785199820064008, 0.8330913360696286, 0.7969418700085953, 0.7371239999774843, 0.8671067799441516, 0.7672891510883346, 0.7512121220352128, 0.7581543270498514, 0.7318470940226689, 0.9239648021757603, 0.7579237199388444, 0.7482736989622936, 0.7511937860399485, 0.721722773858346, 0.789293205132708, 0.7828199319774285, 0.7358766219113022, 0.789500804967247, 0.8613688689656556, 0.7662655080202967, 0.7549584939843044, 0.7923688009614125, 0.7748573970748112, 0.7508983960142359, 0.7860248599899933, 0.7610132809495553, 0.8888588140252978, 0.7652429369045421, 0.7773880051681772, 0.7716645988402888, 0.7595762070268393, 0.8257070240797475, 0.7832980559905991, 0.9815679229795933, 0.8343055018922314, 0.7846864447928965, 0.7325896469410509, 0.9929293390596285]
Total Epoch List: [3, 92]
Total Time List: [0.2190348730655387, 0.22973988694138825]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bce440>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.26s
Epoch 6/1000, LR 0.000120
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.26s
Epoch 13/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.22s
Epoch 14/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.26s
Epoch 17/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.23s
Epoch 18/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.22s
Epoch 20/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.26s
Epoch 21/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.20s
Epoch 23/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.52s
Val loss: 0.6931 score: 0.5227 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.36s
Val loss: 0.6931 score: 0.5455 time: 0.23s
Test loss: 0.6925 score: 0.6744 time: 0.23s
Epoch 27/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.44s
Val loss: 0.6931 score: 0.5455 time: 0.18s
Test loss: 0.6924 score: 0.6047 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.36s
Val loss: 0.6930 score: 0.4773 time: 0.18s
Test loss: 0.6924 score: 0.5116 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.42s
Val loss: 0.6930 score: 0.4773 time: 0.20s
Test loss: 0.6923 score: 0.5116 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.39s
Val loss: 0.6929 score: 0.4773 time: 0.18s
Test loss: 0.6922 score: 0.5349 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.31s
Val loss: 0.6928 score: 0.5455 time: 0.19s
Test loss: 0.6921 score: 0.5581 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.18s
Test loss: 0.6919 score: 0.5349 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.19s
Test loss: 0.6917 score: 0.5116 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.18s
Test loss: 0.6914 score: 0.5116 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4884 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4884 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4884 time: 0.22s
Epoch 44/1000, LR 0.000269
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4884 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.4884 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.4884 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.32s
Val loss: 0.6871 score: 0.5227 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.4884 time: 0.22s
Epoch 48/1000, LR 0.000269
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.38s
Val loss: 0.6862 score: 0.5227 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.4884 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.36s
Val loss: 0.6851 score: 0.5227 time: 0.18s
Test loss: 0.6818 score: 0.5581 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.34s
Val loss: 0.6840 score: 0.6364 time: 0.19s
Test loss: 0.6803 score: 0.6047 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.39s
Val loss: 0.6828 score: 0.6364 time: 0.17s
Test loss: 0.6787 score: 0.6512 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6760;  Loss pred: 0.6760; Loss self: 0.0000; time: 0.33s
Val loss: 0.6815 score: 0.6364 time: 0.20s
Test loss: 0.6770 score: 0.6512 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.44s
Val loss: 0.6801 score: 0.6818 time: 0.17s
Test loss: 0.6750 score: 0.7209 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.32s
Val loss: 0.6785 score: 0.7045 time: 0.20s
Test loss: 0.6729 score: 0.7209 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.6698;  Loss pred: 0.6698; Loss self: 0.0000; time: 0.37s
Val loss: 0.6768 score: 0.7500 time: 0.19s
Test loss: 0.6707 score: 0.7209 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.35s
Val loss: 0.6749 score: 0.7727 time: 0.18s
Test loss: 0.6681 score: 0.7442 time: 0.23s
Epoch 57/1000, LR 0.000269
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.35s
Val loss: 0.6727 score: 0.7955 time: 0.19s
Test loss: 0.6653 score: 0.7674 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.34s
Val loss: 0.6704 score: 0.7955 time: 0.17s
Test loss: 0.6621 score: 0.7907 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 0.37s
Val loss: 0.6677 score: 0.8409 time: 0.19s
Test loss: 0.6586 score: 0.8140 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.40s
Val loss: 0.6649 score: 0.8409 time: 0.18s
Test loss: 0.6547 score: 0.8140 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 0.50s
Val loss: 0.6617 score: 0.8409 time: 0.19s
Test loss: 0.6505 score: 0.8140 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.6461;  Loss pred: 0.6461; Loss self: 0.0000; time: 0.36s
Val loss: 0.6582 score: 0.8636 time: 0.20s
Test loss: 0.6458 score: 0.8372 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.6391;  Loss pred: 0.6391; Loss self: 0.0000; time: 0.31s
Val loss: 0.6544 score: 0.8409 time: 0.19s
Test loss: 0.6408 score: 0.8140 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.38s
Val loss: 0.6501 score: 0.8409 time: 0.16s
Test loss: 0.6352 score: 0.8372 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.39s
Val loss: 0.6454 score: 0.8864 time: 0.19s
Test loss: 0.6289 score: 0.8372 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.6233;  Loss pred: 0.6233; Loss self: 0.0000; time: 0.39s
Val loss: 0.6403 score: 0.8864 time: 0.19s
Test loss: 0.6223 score: 0.8605 time: 0.23s
Epoch 67/1000, LR 0.000268
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.36s
Val loss: 0.6350 score: 0.9091 time: 0.19s
Test loss: 0.6155 score: 0.8837 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.6133;  Loss pred: 0.6133; Loss self: 0.0000; time: 0.36s
Val loss: 0.6292 score: 0.9091 time: 0.19s
Test loss: 0.6080 score: 0.9070 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.6054;  Loss pred: 0.6054; Loss self: 0.0000; time: 0.46s
Val loss: 0.6229 score: 0.9091 time: 0.19s
Test loss: 0.6002 score: 0.9070 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 0.37s
Val loss: 0.6162 score: 0.8864 time: 0.19s
Test loss: 0.5917 score: 0.9070 time: 0.21s
Epoch 71/1000, LR 0.000268
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.44s
Val loss: 0.6092 score: 0.9091 time: 0.20s
Test loss: 0.5831 score: 0.9070 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.34s
Val loss: 0.6016 score: 0.9091 time: 0.20s
Test loss: 0.5736 score: 0.9302 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.36s
Val loss: 0.5936 score: 0.9091 time: 0.17s
Test loss: 0.5638 score: 0.9302 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.49s
Val loss: 0.5853 score: 0.9091 time: 0.20s
Test loss: 0.5537 score: 0.9302 time: 0.28s
Epoch 75/1000, LR 0.000267
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.36s
Val loss: 0.5766 score: 0.9091 time: 0.17s
Test loss: 0.5431 score: 0.9302 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.42s
Val loss: 0.5674 score: 0.9091 time: 0.19s
Test loss: 0.5318 score: 0.9302 time: 0.22s
Epoch 77/1000, LR 0.000267
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.43s
Val loss: 0.5579 score: 0.9091 time: 0.23s
Test loss: 0.5199 score: 0.9302 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.34s
Val loss: 0.5479 score: 0.8864 time: 0.20s
Test loss: 0.5074 score: 0.9302 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.4975;  Loss pred: 0.4975; Loss self: 0.0000; time: 0.36s
Val loss: 0.5376 score: 0.8864 time: 0.17s
Test loss: 0.4947 score: 0.9302 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.4867;  Loss pred: 0.4867; Loss self: 0.0000; time: 0.34s
Val loss: 0.5274 score: 0.8864 time: 0.19s
Test loss: 0.4828 score: 0.9302 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.4689;  Loss pred: 0.4689; Loss self: 0.0000; time: 0.42s
Val loss: 0.5171 score: 0.8864 time: 0.16s
Test loss: 0.4710 score: 0.9302 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.4573;  Loss pred: 0.4573; Loss self: 0.0000; time: 0.35s
Val loss: 0.5069 score: 0.8864 time: 0.19s
Test loss: 0.4595 score: 0.9302 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.4326;  Loss pred: 0.4326; Loss self: 0.0000; time: 0.32s
Val loss: 0.4966 score: 0.9091 time: 0.19s
Test loss: 0.4479 score: 0.9070 time: 0.21s
Epoch 84/1000, LR 0.000266
Train loss: 0.4361;  Loss pred: 0.4361; Loss self: 0.0000; time: 0.42s
Val loss: 0.4858 score: 0.8864 time: 0.17s
Test loss: 0.4356 score: 0.9070 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.4018;  Loss pred: 0.4018; Loss self: 0.0000; time: 0.41s
Val loss: 0.4742 score: 0.8864 time: 0.20s
Test loss: 0.4217 score: 0.9070 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3986;  Loss pred: 0.3986; Loss self: 0.0000; time: 0.38s
Val loss: 0.4622 score: 0.8864 time: 0.18s
Test loss: 0.4069 score: 0.9070 time: 0.24s
Epoch 87/1000, LR 0.000266
Train loss: 0.3768;  Loss pred: 0.3768; Loss self: 0.0000; time: 0.34s
Val loss: 0.4506 score: 0.8864 time: 0.20s
Test loss: 0.3931 score: 0.9070 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.3769;  Loss pred: 0.3769; Loss self: 0.0000; time: 0.38s
Val loss: 0.4394 score: 0.8864 time: 0.18s
Test loss: 0.3801 score: 0.9070 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.3552;  Loss pred: 0.3552; Loss self: 0.0000; time: 0.35s
Val loss: 0.4287 score: 0.9091 time: 0.19s
Test loss: 0.3690 score: 0.9070 time: 0.23s
Epoch 90/1000, LR 0.000266
Train loss: 0.3374;  Loss pred: 0.3374; Loss self: 0.0000; time: 0.38s
Val loss: 0.4177 score: 0.9091 time: 0.18s
Test loss: 0.3572 score: 0.9070 time: 0.22s
Epoch 91/1000, LR 0.000266
Train loss: 0.3273;  Loss pred: 0.3273; Loss self: 0.0000; time: 0.36s
Val loss: 0.4076 score: 0.9091 time: 0.19s
Test loss: 0.3475 score: 0.9070 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 0.2944;  Loss pred: 0.2944; Loss self: 0.0000; time: 0.33s
Val loss: 0.3980 score: 0.9091 time: 0.19s
Test loss: 0.3388 score: 0.9070 time: 0.24s
Epoch 93/1000, LR 0.000265
Train loss: 0.3431;  Loss pred: 0.3431; Loss self: 0.0000; time: 0.45s
Val loss: 0.3882 score: 0.9091 time: 0.20s
Test loss: 0.3283 score: 0.9070 time: 0.25s
Epoch 94/1000, LR 0.000265
Train loss: 0.2611;  Loss pred: 0.2611; Loss self: 0.0000; time: 0.33s
Val loss: 0.3790 score: 0.8864 time: 0.18s
Test loss: 0.3195 score: 0.9070 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.2555;  Loss pred: 0.2555; Loss self: 0.0000; time: 0.37s
Val loss: 0.3708 score: 0.9091 time: 0.17s
Test loss: 0.3141 score: 0.8837 time: 0.24s
Epoch 96/1000, LR 0.000265
Train loss: 0.2427;  Loss pred: 0.2427; Loss self: 0.0000; time: 0.35s
Val loss: 0.3633 score: 0.8864 time: 0.19s
Test loss: 0.3105 score: 0.8605 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.2466;  Loss pred: 0.2466; Loss self: 0.0000; time: 0.45s
Val loss: 0.3556 score: 0.8636 time: 0.16s
Test loss: 0.3050 score: 0.8605 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.2577;  Loss pred: 0.2577; Loss self: 0.0000; time: 0.34s
Val loss: 0.3485 score: 0.8636 time: 0.27s
Test loss: 0.3002 score: 0.8605 time: 0.28s
Epoch 99/1000, LR 0.000265
Train loss: 0.2224;  Loss pred: 0.2224; Loss self: 0.0000; time: 0.37s
Val loss: 0.3414 score: 0.8636 time: 0.19s
Test loss: 0.2932 score: 0.8605 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.1985;  Loss pred: 0.1985; Loss self: 0.0000; time: 0.37s
Val loss: 0.3351 score: 0.8636 time: 0.19s
Test loss: 0.2888 score: 0.8605 time: 0.25s
Epoch 101/1000, LR 0.000265
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.40s
Val loss: 0.3297 score: 0.8636 time: 0.20s
Test loss: 0.2887 score: 0.8605 time: 0.22s
Epoch 102/1000, LR 0.000264
Train loss: 0.1834;  Loss pred: 0.1834; Loss self: 0.0000; time: 0.35s
Val loss: 0.3245 score: 0.8636 time: 0.16s
Test loss: 0.2851 score: 0.8372 time: 0.24s
Epoch 103/1000, LR 0.000264
Train loss: 0.1723;  Loss pred: 0.1723; Loss self: 0.0000; time: 0.31s
Val loss: 0.3200 score: 0.8409 time: 0.18s
Test loss: 0.2851 score: 0.8372 time: 0.23s
Epoch 104/1000, LR 0.000264
Train loss: 0.1503;  Loss pred: 0.1503; Loss self: 0.0000; time: 0.38s
Val loss: 0.3162 score: 0.8409 time: 0.18s
Test loss: 0.2896 score: 0.8372 time: 0.22s
Epoch 105/1000, LR 0.000264
Train loss: 0.1494;  Loss pred: 0.1494; Loss self: 0.0000; time: 0.41s
Val loss: 0.3130 score: 0.8636 time: 0.18s
Test loss: 0.2954 score: 0.8372 time: 0.24s
Epoch 106/1000, LR 0.000264
Train loss: 0.1271;  Loss pred: 0.1271; Loss self: 0.0000; time: 0.35s
Val loss: 0.3110 score: 0.8636 time: 0.20s
Test loss: 0.3057 score: 0.8372 time: 0.21s
Epoch 107/1000, LR 0.000264
Train loss: 0.1384;  Loss pred: 0.1384; Loss self: 0.0000; time: 0.34s
Val loss: 0.3112 score: 0.8864 time: 0.16s
Test loss: 0.3204 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 108/1000, LR 0.000264
Train loss: 0.1617;  Loss pred: 0.1617; Loss self: 0.0000; time: 0.32s
Val loss: 0.3069 score: 0.8636 time: 0.19s
Test loss: 0.3191 score: 0.8140 time: 0.23s
Epoch 109/1000, LR 0.000264
Train loss: 0.1158;  Loss pred: 0.1158; Loss self: 0.0000; time: 0.46s
Val loss: 0.3054 score: 0.8636 time: 0.18s
Test loss: 0.3269 score: 0.7907 time: 0.23s
Epoch 110/1000, LR 0.000263
Train loss: 0.1157;  Loss pred: 0.1157; Loss self: 0.0000; time: 0.34s
Val loss: 0.3051 score: 0.8636 time: 0.19s
Test loss: 0.3370 score: 0.7907 time: 0.23s
Epoch 111/1000, LR 0.000263
Train loss: 0.1370;  Loss pred: 0.1370; Loss self: 0.0000; time: 0.36s
Val loss: 0.3069 score: 0.8636 time: 0.19s
Test loss: 0.3526 score: 0.7907 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 112/1000, LR 0.000263
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.36s
Val loss: 0.3085 score: 0.8182 time: 0.17s
Test loss: 0.3655 score: 0.8140 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 109,   Train_Loss: 0.1157,   Val_Loss: 0.3051,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3051,   Test_Precision: 0.7500,   Test_Recall: 0.8571,   Test_accuracy: 0.8000,   Test_Score: 0.7907,   Test_loss: 0.3370


[0.4328834720654413, 0.22508069791365415, 0.21878322307020426, 0.2060844919178635, 0.2109182069543749, 0.22917460708413273, 0.22530515200924128, 0.1928865979425609, 0.21519464498851448, 0.206509641953744, 0.25485545909032226, 0.18893125094473362, 0.20399433304555714, 0.2263384769903496, 0.20412931602913886, 0.24255537101998925, 0.24326441704761237, 0.18890444107819349, 0.3516630900558084, 0.20379606098867953, 0.23481170402374119, 0.2110923359869048, 0.22722931497264653, 0.21192577306646854, 0.22601611400023103, 0.20127717591822147, 0.23770815203897655, 0.20896173303481191, 0.2180363649968058, 0.1883169619832188, 0.21944448607973754, 0.2427116099279374, 0.22024962806608528, 0.20086082501802593, 0.2066709800856188, 0.19859043892938644, 0.22831305500585586, 0.1998794369865209, 0.2226204019971192, 0.2135907900519669, 0.21898180106654763, 0.18972882302477956, 0.23913783894386142, 0.21864233107771724, 0.2271022170316428, 0.21296891197562218, 0.23918740497902036, 0.1970461109885946, 0.22141923801973462, 0.19711707695387304, 0.21813808905426413, 0.19963740196544677, 0.21721633290871978, 0.2184286559931934, 0.19821578194387257, 0.22264727705623955, 0.21965861495118588, 0.2193506780313328, 0.2933899760246277, 0.21869181096553802, 0.19478022900875658, 0.21207246696576476, 0.20754416997078806, 0.19658456603065133, 0.2195595409721136, 0.1936187690589577, 0.3083338920259848, 0.20255063893273473, 0.19642358203418553, 0.21909096697345376, 0.1939482579473406, 0.2082974889781326, 0.21157478005625308, 0.2108664729166776, 0.2165022320114076, 0.2122620870359242, 0.20913149102125317, 0.2062095010187477, 0.19874843198340386, 0.2027198210125789, 0.2124674569349736, 0.21831288700923324, 0.18471952306572348, 0.2165106690954417, 0.20229291601572186, 0.19803632190451026, 0.20133076107595116, 0.22090682294219732, 0.22182104201056063, 0.18532476900145411, 0.2327380779897794, 0.19729728903621435, 0.2224583759671077, 0.1917282110080123, 0.22898211400024593, 0.2514989139745012, 0.22910595301073045, 0.2525012600235641, 0.2432532450184226, 0.2691760619636625, 0.2446958109503612, 0.24450745701324195, 0.22871846798807383, 0.2273619269253686, 0.24903876997996122, 0.24584118300117552, 0.2608353530522436, 0.22897003300022334, 0.2536707679973915, 0.24169215699657798, 0.26143285899888724, 0.2331926020560786, 0.22495062893722206, 0.22060449398122728, 0.26309869706165045, 0.2067374720936641, 0.2023447669344023, 0.2023631710326299, 0.23867573798634112, 0.23387468291912228, 0.23808232008013874, 0.23108444490935653, 0.2529900249792263, 0.23382348893210292, 0.24787811597343534, 0.25137846905272454, 0.24111018399707973, 0.23717649304307997, 0.22214152908418328, 0.2505428040167317, 0.2353516750736162, 0.25542440393473953, 0.2624889339786023, 0.24662897398229688, 0.2279561529867351, 0.23990758100990206, 0.2343665260123089, 0.22432919405400753, 0.2546606359537691, 0.22528667701408267, 0.2547755859559402, 0.2195348140085116, 0.23983903496991843, 0.23949761409312487, 0.22096897498704493, 0.2534793349914253, 0.23535872797947377, 0.25202290702145547, 0.22275020100641996, 0.2341138340998441, 0.2337740579387173, 0.2210979809751734, 0.2440839339978993, 0.23175594501663, 0.2353363169822842, 0.2383532050298527, 0.24621469003614038, 0.2527757849311456, 0.24060923198703676, 0.23368600197136402, 0.23726456193253398, 0.23661476303823292, 0.22114849195349962, 0.24961895192973316, 0.21796482906211168, 0.26220520702190697, 0.22458679706323892, 0.24732188100460917, 0.28557477006688714, 0.25501065398566425, 0.22658921603579074, 0.25172908208332956, 0.2278770679840818, 0.24531433393713087, 0.2288360829697922, 0.23388021311257035, 0.25032586592715234, 0.21911492303479463, 0.25544437707867473, 0.22930073307361454, 0.24735522700939327, 0.2405382440192625, 0.243583697010763, 0.23306411399971694, 0.22781200997997075, 0.25371879304293543, 0.23972046410199255, 0.25413460296113044, 0.23744621500372887, 0.2459849399747327, 0.23674509907141328, 0.23306829202920198, 0.28543368505779654, 0.22289796499535441, 0.25247683306224644, 0.22074752394109964, 0.24404460203368217, 0.23292566102463752, 0.2246376359835267, 0.24352535791695118, 0.2172269409056753, 0.2486798430327326, 0.2366005580406636, 0.23696974595077336, 0.23409483407158405, 0.2226938169915229, 0.2511091739870608]
[0.00983826072876003, 0.005115470407128503, 0.0049723459788682785, 0.004792662602741011, 0.0049050745803343, 0.005329642025212389, 0.005239654697889332, 0.004485734835873509, 0.005004526627639872, 0.004802549812877767, 0.0059268711416354015, 0.00439375002197055, 0.004744054256873422, 0.005263685511403479, 0.004747193396026485, 0.005640822581860216, 0.005657312024363079, 0.004393126536702174, 0.008178211396646708, 0.004739443278806501, 0.005460737302877702, 0.004909124092718716, 0.005284402673782477, 0.004928506350382989, 0.005256188697679792, 0.0046808645562377085, 0.005528096559045966, 0.004859575186856091, 0.0050706131394606, 0.004379464232167879, 0.005103360141389245, 0.005644456044835753, 0.00512208437362989, 0.0046711819771633935, 0.004806301862456251, 0.004618382300683405, 0.005309605930368741, 0.004648358999686532, 0.005177218651095795, 0.004967227675627137, 0.005092600024803434, 0.004412298209878595, 0.005561345091717708, 0.005084705373900401, 0.005281446907712623, 0.004952765394781911, 0.005562497790209776, 0.004582467697409176, 0.0051492846051101075, 0.004584118068694722, 0.005072978815215445, 0.004642730278266204, 0.0050515426257841806, 0.0050797361858882185, 0.00460966934753192, 0.005177843652470687, 0.005108339882585718, 0.005101178558868205, 0.006823022698247155, 0.005085856068966, 0.0045297727676455015, 0.004931917836413134, 0.004826608603971816, 0.004571734093736077, 0.005106035836560782, 0.004502762071138551, 0.007170555628511275, 0.004710479975179877, 0.00456799027986478, 0.005095138766824506, 0.004510424603426526, 0.004844127650654247, 0.004920343722238443, 0.004903871463178549, 0.00503493562817227, 0.0049363276054866095, 0.004863523047005888, 0.004795569791133667, 0.004622056557753578, 0.0047144144421529975, 0.004941103649650549, 0.005077043883935656, 0.004295802861993569, 0.005035131839428877, 0.004704486418970276, 0.0046054958582444246, 0.004682110722696539, 0.005137367975399937, 0.005158628883966526, 0.004309878348871026, 0.005412513441622777, 0.004588309047353822, 0.005173450603886226, 0.004458795604837495, 0.005325165441866184, 0.005848811952895377, 0.005328045418854196, 0.005872122326129397, 0.005657052209730758, 0.006259908417759593, 0.005690600254659562, 0.00568621993054051, 0.005319034139257531, 0.005287486672682991, 0.005791599301859563, 0.005717236813980826, 0.006065938443075432, 0.005324884488377287, 0.0058993201859858495, 0.00562074783712972, 0.00607983393020668, 0.005423083768746014, 0.005231409975284234, 0.005130337069330867, 0.006118574350270941, 0.0048078481882247465, 0.004705692254288425, 0.0047061202565727885, 0.005550598557821886, 0.005438946114398192, 0.005536798141398575, 0.005374056858357129, 0.0058834889530052625, 0.005437755556560533, 0.005764607348219427, 0.005846010908202896, 0.005607213581327436, 0.005515732396350697, 0.005166082071725192, 0.005826576837598411, 0.005473294769153866, 0.005940102417086966, 0.006104393813455867, 0.005735557534472021, 0.005301305883412444, 0.005579246069997722, 0.005450384325867649, 0.005216958001255989, 0.005922340371017886, 0.005239225046839132, 0.00592501362688233, 0.005105460790895619, 0.005577651976044614, 0.005569711955654067, 0.005138813371791742, 0.005894868255614541, 0.00547345879022032, 0.005860997837708267, 0.005180237232707441, 0.005444507769763816, 0.005436605998574821, 0.005141813511050544, 0.005676370558090681, 0.0053896731399216274, 0.005472937604239167, 0.005543097791391923, 0.005725923024096288, 0.005878506626305712, 0.0055955635345822504, 0.005434558185380559, 0.00551778051005893, 0.005502668907865882, 0.005142988184965108, 0.005805091905342632, 0.005068949513072365, 0.006097795512137371, 0.005222948768912533, 0.0057516716512699805, 0.006641273722485747, 0.005930480325248006, 0.005269516651995133, 0.005854164699612315, 0.005299466697304228, 0.005704984510165834, 0.005321769371390516, 0.005439074723548148, 0.005821531765747729, 0.005095695884530108, 0.005940566908806389, 0.005332575187758478, 0.005752447139753332, 0.005593912651610755, 0.005664737139785186, 0.005420095674412022, 0.005297953720464436, 0.005900437047510127, 0.005574894513999827, 0.005910107045607685, 0.005522005000086718, 0.005720579999412389, 0.00550569997840496, 0.005420192837888418, 0.00663799267576271, 0.005183673604543126, 0.005871554257261546, 0.005133663347467433, 0.005675455861248423, 0.005416875837782268, 0.005224131069384342, 0.0056633804166732835, 0.0050517893233877975, 0.005783252163551921, 0.0055023385590852, 0.00551092432443659, 0.00544406590864149, 0.005178925976547045, 0.005839748232257228]
[101.64398236333746, 195.48544325590885, 201.11231282976877, 208.65228431229056, 203.87049852600734, 187.62986243154847, 190.85227131528833, 222.9289150136021, 199.81909866899812, 208.22272312898374, 168.72308779840793, 227.596015931628, 210.79016930532578, 189.98095494754696, 210.65078175180813, 177.27910876257744, 176.7623909894883, 227.6283170187669, 122.27612512071131, 210.99524589137454, 183.1254544094292, 203.70232675177522, 189.23614677611585, 202.90122988728442, 190.25192159509496, 213.63574783795067, 180.89409063661128, 205.77930406442613, 197.21480864272317, 228.33843296511864, 195.94932991105313, 177.16499022344655, 195.23301981285513, 214.07857901679452, 208.06017362566402, 216.52603333682984, 188.33789420800804, 215.12968341460638, 193.1539050969661, 201.31954186572392, 196.3633497878323, 226.6392597311584, 179.8126143060715, 196.66822882854962, 189.3420529400146, 201.9074032970693, 179.77535231745009, 218.2230330975115, 194.20173416082076, 218.14446857926137, 197.12284171199144, 215.39050086136885, 197.95933125374034, 196.8606170489826, 216.9352993909235, 193.13059009088363, 195.7583134608937, 196.0331300815844, 146.56260783903028, 196.62373186335745, 220.76162564767742, 202.7608798785821, 207.18481278492317, 218.73538125722175, 195.84664738145656, 222.0859073166937, 139.45920676270055, 212.29259125803063, 218.9146514623498, 196.26550831376863, 221.7086167985847, 206.4355178305304, 203.2378338692695, 203.92051616944883, 198.61227110921567, 202.57974752091496, 205.6122671435938, 208.52579433811164, 216.35390815858432, 212.11542011637738, 202.38393502850792, 196.96501012412233, 232.78535634103244, 198.6045314979136, 212.5630538474126, 217.13188563830158, 213.57888764836306, 194.6522041614415, 193.84995945494126, 232.02511046789763, 184.75704694050248, 217.94521460508895, 193.29458741691928, 224.2758109196723, 187.78759287703065, 170.9748933721426, 187.6860877464238, 170.29617989227904, 176.77050925566655, 159.7467460007822, 175.7283863299276, 175.86375698010397, 188.0040574696494, 189.1257722059803, 172.6638788147724, 174.90966922248495, 164.85495350542985, 187.79750099419368, 169.5110569478079, 177.91226878995838, 164.4781767856619, 184.39693035227282, 191.15305524218024, 194.91896662657814, 163.43676529087503, 207.9932562032997, 212.508584489067, 212.4892577072065, 180.1607501574408, 183.85914825535053, 180.60979910447008, 186.07916260597662, 169.96717559726258, 183.89940290595112, 173.47235285832267, 171.0568139031076, 178.34169957964448, 181.29958600994078, 193.57028907325392, 171.62735991862047, 182.70530679906963, 168.34726571775195, 163.81642969949087, 174.350966578187, 188.63276747130487, 179.23568658809992, 183.47330026874934, 191.6825858592783, 168.85216609529786, 190.86792246179772, 168.77598314085023, 195.86870626511586, 179.28691218005122, 179.54249842038146, 194.59745424678295, 169.6390753173414, 182.69983173834174, 170.61941118050544, 193.0413521770994, 183.67133307321558, 183.93828801685186, 194.4839107546096, 176.16890753805237, 185.54000846413874, 182.7172301809966, 180.4045386954811, 174.64433171590326, 170.11123123092233, 178.71300965840206, 184.00759838952285, 181.2322904430499, 181.72999625155228, 194.4394900465408, 172.26256126619884, 197.27953443234935, 163.9936921481784, 191.46272426643185, 173.8624978321212, 150.57352577025122, 168.62040596318496, 189.77072586370556, 170.8185627347013, 188.69823269361942, 175.28531378447715, 187.90742894194824, 183.85480083047213, 171.77609609273654, 196.2440504025906, 168.33410267925512, 187.52665734476878, 173.8390593960122, 178.7657516804543, 176.5306977753113, 184.49858822989893, 188.75212067959262, 169.4789711250256, 179.37559132083535, 169.2016730463769, 181.0936426142852, 174.80744961222794, 181.6299478580936, 184.49528087077744, 150.64795169950972, 192.91338079688703, 170.31265593147964, 194.7926718828038, 176.1973001724713, 184.60825574495937, 191.41939333421988, 176.5729875845791, 197.94966416560433, 172.91308968029267, 181.74090693653292, 181.45776300461813, 183.68624053810174, 193.0902284621438, 171.2402590365564]
Elapsed: 0.2295930009022589~0.026603995221725203
Time per graph: 0.005337133480468804~0.0006108338538057201
Speed: 189.4573933780821~18.91937598826481
Total Time: 0.2518
best val loss: 0.3050636351108551 test_score: 0.7907

Testing...
Test loss: 0.6155 score: 0.8837 time: 0.23s
test Score 0.8837
Epoch Time List: [1.1145050170598552, 0.77490505494643, 0.8007492340402678, 0.7980877921218053, 0.8299681359203532, 0.789919356815517, 0.8976561198942363, 0.8400031649507582, 0.9143048329278827, 0.7483091459143907, 0.9128950570011511, 0.7660714099183679, 0.8242860989412293, 0.7770778930280358, 0.8442487401189283, 0.8258887599222362, 0.8954029959859326, 0.8692364228190854, 1.0065824908670038, 0.7660452370764688, 0.8134961730102077, 0.7706379290902987, 0.8130793418968096, 0.7980231149122119, 0.9313583469484001, 0.7616070358781144, 0.8221791739342734, 0.7544190760236233, 0.7733732131309807, 0.7616092229727656, 0.8054097110871226, 0.8959052880527452, 0.7838183880085126, 0.919333896948956, 0.7894921938423067, 0.7589556731982157, 0.8056486920686439, 0.8617826780537143, 0.789630405139178, 0.814330643042922, 0.7694699009880424, 0.7603100358974189, 0.9698563690762967, 1.0179908119607717, 0.8198053169762716, 0.8356421128846705, 0.860624223947525, 0.8626375609310344, 0.7877580390777439, 0.7342484148684889, 0.9131057731574401, 0.745980296866037, 0.8166510320734233, 0.7858324549160898, 0.7712801268789917, 0.7768004758981988, 0.8616417180746794, 0.7785199820064008, 0.8330913360696286, 0.7969418700085953, 0.7371239999774843, 0.8671067799441516, 0.7672891510883346, 0.7512121220352128, 0.7581543270498514, 0.7318470940226689, 0.9239648021757603, 0.7579237199388444, 0.7482736989622936, 0.7511937860399485, 0.721722773858346, 0.789293205132708, 0.7828199319774285, 0.7358766219113022, 0.789500804967247, 0.8613688689656556, 0.7662655080202967, 0.7549584939843044, 0.7923688009614125, 0.7748573970748112, 0.7508983960142359, 0.7860248599899933, 0.7610132809495553, 0.8888588140252978, 0.7652429369045421, 0.7773880051681772, 0.7716645988402888, 0.7595762070268393, 0.8257070240797475, 0.7832980559905991, 0.9815679229795933, 0.8343055018922314, 0.7846864447928965, 0.7325896469410509, 0.9929293390596285, 0.8010895439656451, 0.805719260009937, 0.792527534882538, 0.7560265690553933, 0.9352894580224529, 0.7669294339139014, 0.797536536003463, 0.7875154948560521, 0.7858866528840736, 0.7809160158503801, 0.7483286421047524, 0.8059374508447945, 0.8984693349339068, 0.7999024299206212, 0.7404435750795528, 0.8514247910352424, 0.7977065639570355, 0.6915830770740286, 0.7152707929490134, 0.7753819439094514, 0.7552526390645653, 0.6565756150521338, 0.6388774691149592, 0.8670018709963188, 0.9242824529064819, 0.8147776211844757, 0.8457131950417534, 0.7839552650693804, 0.851548639126122, 0.8187383570475504, 0.7519059280166402, 0.7888266330119222, 0.7698279810138047, 0.7618642530869693, 0.7778255090815946, 0.7836968338815495, 0.8634892220143229, 0.7913563919719309, 0.7826632099458948, 0.8132144369883463, 0.7808148592012003, 0.7701294060098007, 0.7758454729337245, 0.8004483780823648, 0.8203341049375013, 0.7914868709631264, 0.736167160095647, 0.7916724450187758, 0.774244436994195, 0.7447269089752808, 0.8159738219110295, 0.7586265238933265, 0.8595146428560838, 0.7370194679824635, 0.7865896670846269, 0.7669897070154548, 0.7619467290351167, 0.7476412469986826, 0.7843626179965213, 0.8100312040187418, 0.922066131955944, 0.8037883798824623, 0.7517558850813657, 0.7847636649385095, 0.8107628629077226, 0.8112169350497425, 0.7835375790018588, 0.7699547859374434, 0.8961254559690133, 0.77604489994701, 0.888757246080786, 0.7625525490147993, 0.7698664460331202, 0.9692724109627306, 0.7851127769099548, 0.8329594320384786, 0.905963837984018, 0.7645311650121585, 0.7709224909776822, 0.7548059311229736, 0.802170277107507, 0.7784786290721968, 0.7276883031008765, 0.8402721738675609, 0.832248879945837, 0.8025828009704128, 0.770964797004126, 0.7900728728855029, 0.7643809608416632, 0.7849125710781664, 0.7948422379558906, 0.7577812279341742, 0.8997529109474272, 0.7475650809938088, 0.7822059439495206, 0.7674651639536023, 0.8403072919463739, 0.8915516119450331, 0.7767708490137011, 0.8121313320007175, 0.8097128209192306, 0.7570266399998218, 0.7268407479859889, 0.7803891261573881, 0.8295391558203846, 0.7602794840931892, 0.7460384430596605, 0.7461556820198894, 0.8696020679781213, 0.75640001706779, 0.7693856449332088, 0.7794365158770233]
Total Epoch List: [3, 92, 112]
Total Time List: [0.2190348730655387, 0.22973988694138825, 0.25175471790134907]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcea10>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.19s
Epoch 3/1000, LR 0.000030
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.19s
Epoch 4/1000, LR 0.000060
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.30s
Epoch 5/1000, LR 0.000090
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.20s
Epoch 6/1000, LR 0.000120
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.20s
Epoch 8/1000, LR 0.000180
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.19s
Epoch 10/1000, LR 0.000240
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.21s
Epoch 11/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.21s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.19s
Epoch 13/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.32s
Epoch 14/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.20s
Epoch 15/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.22s
Epoch 16/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.29s
Epoch 21/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.21s
Epoch 22/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.20s
Epoch 23/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.21s
Epoch 24/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.19s
Epoch 26/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.21s
Epoch 27/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.29s
Epoch 28/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4884 time: 0.25s
Test loss: 0.6914 score: 0.5227 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4884 time: 0.26s
Test loss: 0.6912 score: 0.5227 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.35s
Val loss: 0.6915 score: 0.5349 time: 0.24s
Test loss: 0.6909 score: 0.5682 time: 0.23s
Epoch 31/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.27s
Val loss: 0.6912 score: 0.5581 time: 0.26s
Test loss: 0.6906 score: 0.5682 time: 0.19s
Epoch 32/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.50s
Val loss: 0.6908 score: 0.6047 time: 0.25s
Test loss: 0.6902 score: 0.6364 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.30s
Val loss: 0.6904 score: 0.6047 time: 0.29s
Test loss: 0.6898 score: 0.6364 time: 0.28s
Epoch 34/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.33s
Val loss: 0.6899 score: 0.6279 time: 0.24s
Test loss: 0.6893 score: 0.6364 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.36s
Val loss: 0.6894 score: 0.6279 time: 0.28s
Test loss: 0.6888 score: 0.6364 time: 0.19s
Epoch 36/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.33s
Val loss: 0.6888 score: 0.6512 time: 0.24s
Test loss: 0.6882 score: 0.6591 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.29s
Val loss: 0.6881 score: 0.6977 time: 0.26s
Test loss: 0.6876 score: 0.7045 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.37s
Val loss: 0.6873 score: 0.7209 time: 0.24s
Test loss: 0.6868 score: 0.7045 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.31s
Val loss: 0.6864 score: 0.7442 time: 0.24s
Test loss: 0.6860 score: 0.7273 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.29s
Val loss: 0.6854 score: 0.7442 time: 0.26s
Test loss: 0.6850 score: 0.7500 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.37s
Val loss: 0.6843 score: 0.7442 time: 0.24s
Test loss: 0.6840 score: 0.7273 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.29s
Val loss: 0.6830 score: 0.7442 time: 0.26s
Test loss: 0.6828 score: 0.7273 time: 0.21s
Epoch 43/1000, LR 0.000269
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.35s
Val loss: 0.6815 score: 0.7442 time: 0.28s
Test loss: 0.6814 score: 0.7273 time: 0.22s
Epoch 44/1000, LR 0.000269
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.29s
Val loss: 0.6799 score: 0.7674 time: 0.26s
Test loss: 0.6800 score: 0.7273 time: 0.20s
Epoch 45/1000, LR 0.000269
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.41s
Val loss: 0.6782 score: 0.7674 time: 0.25s
Test loss: 0.6783 score: 0.7273 time: 0.21s
Epoch 46/1000, LR 0.000269
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.30s
Val loss: 0.6762 score: 0.7674 time: 0.37s
Test loss: 0.6765 score: 0.7273 time: 0.19s
Epoch 47/1000, LR 0.000269
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 0.42s
Val loss: 0.6741 score: 0.7674 time: 0.24s
Test loss: 0.6745 score: 0.7273 time: 0.21s
Epoch 48/1000, LR 0.000269
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.31s
Val loss: 0.6718 score: 0.8140 time: 0.30s
Test loss: 0.6724 score: 0.7273 time: 0.20s
Epoch 49/1000, LR 0.000269
Train loss: 0.6673;  Loss pred: 0.6673; Loss self: 0.0000; time: 0.35s
Val loss: 0.6692 score: 0.8140 time: 0.25s
Test loss: 0.6701 score: 0.7500 time: 0.20s
Epoch 50/1000, LR 0.000269
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.33s
Val loss: 0.6665 score: 0.8140 time: 0.24s
Test loss: 0.6675 score: 0.7500 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6610;  Loss pred: 0.6610; Loss self: 0.0000; time: 0.27s
Val loss: 0.6635 score: 0.8140 time: 0.26s
Test loss: 0.6648 score: 0.7500 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.38s
Val loss: 0.6603 score: 0.8140 time: 0.24s
Test loss: 0.6618 score: 0.7500 time: 0.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6538;  Loss pred: 0.6538; Loss self: 0.0000; time: 0.26s
Val loss: 0.6568 score: 0.8140 time: 0.28s
Test loss: 0.6585 score: 0.7500 time: 0.19s
Epoch 54/1000, LR 0.000269
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 0.35s
Val loss: 0.6531 score: 0.8140 time: 0.24s
Test loss: 0.6550 score: 0.7500 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.39s
Val loss: 0.6491 score: 0.8140 time: 0.35s
Test loss: 0.6513 score: 0.7727 time: 0.21s
Epoch 56/1000, LR 0.000269
Train loss: 0.6404;  Loss pred: 0.6404; Loss self: 0.0000; time: 0.39s
Val loss: 0.6448 score: 0.8140 time: 0.23s
Test loss: 0.6472 score: 0.7727 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.26s
Val loss: 0.6401 score: 0.8140 time: 0.29s
Test loss: 0.6429 score: 0.7727 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.32s
Val loss: 0.6352 score: 0.8140 time: 0.25s
Test loss: 0.6382 score: 0.7727 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.6245;  Loss pred: 0.6245; Loss self: 0.0000; time: 0.29s
Val loss: 0.6300 score: 0.8140 time: 0.25s
Test loss: 0.6332 score: 0.7727 time: 0.20s
Epoch 60/1000, LR 0.000268
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 0.33s
Val loss: 0.6245 score: 0.8140 time: 0.24s
Test loss: 0.6280 score: 0.7727 time: 0.19s
Epoch 61/1000, LR 0.000268
Train loss: 0.6112;  Loss pred: 0.6112; Loss self: 0.0000; time: 0.32s
Val loss: 0.6187 score: 0.8140 time: 0.24s
Test loss: 0.6225 score: 0.7727 time: 0.22s
Epoch 62/1000, LR 0.000268
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.26s
Val loss: 0.6125 score: 0.8140 time: 0.26s
Test loss: 0.6166 score: 0.7727 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.6035;  Loss pred: 0.6035; Loss self: 0.0000; time: 0.35s
Val loss: 0.6059 score: 0.8140 time: 0.36s
Test loss: 0.6104 score: 0.7727 time: 0.21s
Epoch 64/1000, LR 0.000268
Train loss: 0.5877;  Loss pred: 0.5877; Loss self: 0.0000; time: 0.26s
Val loss: 0.5989 score: 0.8605 time: 0.27s
Test loss: 0.6038 score: 0.7727 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.5781;  Loss pred: 0.5781; Loss self: 0.0000; time: 0.32s
Val loss: 0.5914 score: 0.8605 time: 0.25s
Test loss: 0.5968 score: 0.7727 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.5696;  Loss pred: 0.5696; Loss self: 0.0000; time: 0.30s
Val loss: 0.5836 score: 0.8605 time: 0.25s
Test loss: 0.5892 score: 0.7727 time: 0.20s
Epoch 67/1000, LR 0.000268
Train loss: 0.5669;  Loss pred: 0.5669; Loss self: 0.0000; time: 0.34s
Val loss: 0.5753 score: 0.8837 time: 0.25s
Test loss: 0.5812 score: 0.7955 time: 0.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.5636;  Loss pred: 0.5636; Loss self: 0.0000; time: 0.30s
Val loss: 0.5666 score: 0.8837 time: 0.24s
Test loss: 0.5729 score: 0.8182 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.5478;  Loss pred: 0.5478; Loss self: 0.0000; time: 0.30s
Val loss: 0.5577 score: 0.8837 time: 0.27s
Test loss: 0.5641 score: 0.8182 time: 0.21s
Epoch 70/1000, LR 0.000268
Train loss: 0.5433;  Loss pred: 0.5433; Loss self: 0.0000; time: 0.31s
Val loss: 0.5486 score: 0.8372 time: 0.24s
Test loss: 0.5550 score: 0.8182 time: 0.32s
Epoch 71/1000, LR 0.000268
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.30s
Val loss: 0.5394 score: 0.8605 time: 0.26s
Test loss: 0.5457 score: 0.8182 time: 0.20s
Epoch 72/1000, LR 0.000267
Train loss: 0.5150;  Loss pred: 0.5150; Loss self: 0.0000; time: 0.32s
Val loss: 0.5302 score: 0.8605 time: 0.24s
Test loss: 0.5364 score: 0.8409 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 0.27s
Val loss: 0.5207 score: 0.8605 time: 0.26s
Test loss: 0.5269 score: 0.8636 time: 0.19s
Epoch 74/1000, LR 0.000267
Train loss: 0.4832;  Loss pred: 0.4832; Loss self: 0.0000; time: 0.33s
Val loss: 0.5105 score: 0.8605 time: 0.24s
Test loss: 0.5168 score: 0.8636 time: 0.19s
Epoch 75/1000, LR 0.000267
Train loss: 0.4760;  Loss pred: 0.4760; Loss self: 0.0000; time: 0.34s
Val loss: 0.4999 score: 0.8837 time: 0.24s
Test loss: 0.5064 score: 0.8636 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.4614;  Loss pred: 0.4614; Loss self: 0.0000; time: 0.27s
Val loss: 0.4892 score: 0.8837 time: 0.27s
Test loss: 0.4957 score: 0.8636 time: 0.21s
Epoch 77/1000, LR 0.000267
Train loss: 0.4558;  Loss pred: 0.4558; Loss self: 0.0000; time: 0.33s
Val loss: 0.4783 score: 0.8837 time: 0.24s
Test loss: 0.4848 score: 0.8636 time: 0.21s
Epoch 78/1000, LR 0.000267
Train loss: 0.4382;  Loss pred: 0.4382; Loss self: 0.0000; time: 0.39s
Val loss: 0.4671 score: 0.8605 time: 0.27s
Test loss: 0.4735 score: 0.8636 time: 0.20s
Epoch 79/1000, LR 0.000267
Train loss: 0.4268;  Loss pred: 0.4268; Loss self: 0.0000; time: 0.34s
Val loss: 0.4563 score: 0.8605 time: 0.23s
Test loss: 0.4624 score: 0.8636 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.4235;  Loss pred: 0.4235; Loss self: 0.0000; time: 0.28s
Val loss: 0.4456 score: 0.8605 time: 0.26s
Test loss: 0.4516 score: 0.8636 time: 0.20s
Epoch 81/1000, LR 0.000267
Train loss: 0.4129;  Loss pred: 0.4129; Loss self: 0.0000; time: 0.32s
Val loss: 0.4352 score: 0.8605 time: 0.24s
Test loss: 0.4408 score: 0.8636 time: 0.21s
Epoch 82/1000, LR 0.000267
Train loss: 0.3894;  Loss pred: 0.3894; Loss self: 0.0000; time: 0.35s
Val loss: 0.4251 score: 0.8605 time: 0.26s
Test loss: 0.4303 score: 0.8409 time: 0.22s
Epoch 83/1000, LR 0.000266
Train loss: 0.3749;  Loss pred: 0.3749; Loss self: 0.0000; time: 0.28s
Val loss: 0.4158 score: 0.8605 time: 0.26s
Test loss: 0.4206 score: 0.8636 time: 0.20s
Epoch 84/1000, LR 0.000266
Train loss: 0.3702;  Loss pred: 0.3702; Loss self: 0.0000; time: 0.32s
Val loss: 0.4078 score: 0.8605 time: 0.24s
Test loss: 0.4120 score: 0.8636 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.3200;  Loss pred: 0.3200; Loss self: 0.0000; time: 0.27s
Val loss: 0.4004 score: 0.8605 time: 0.27s
Test loss: 0.4043 score: 0.8864 time: 0.28s
Epoch 86/1000, LR 0.000266
Train loss: 0.3304;  Loss pred: 0.3304; Loss self: 0.0000; time: 0.33s
Val loss: 0.3940 score: 0.8372 time: 0.24s
Test loss: 0.3977 score: 0.8864 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.3299;  Loss pred: 0.3299; Loss self: 0.0000; time: 0.28s
Val loss: 0.3859 score: 0.8372 time: 0.26s
Test loss: 0.3889 score: 0.8864 time: 0.19s
Epoch 88/1000, LR 0.000266
Train loss: 0.3031;  Loss pred: 0.3031; Loss self: 0.0000; time: 0.41s
Val loss: 0.3781 score: 0.8372 time: 0.25s
Test loss: 0.3801 score: 0.8864 time: 0.22s
Epoch 89/1000, LR 0.000266
Train loss: 0.2730;  Loss pred: 0.2730; Loss self: 0.0000; time: 0.30s
Val loss: 0.3709 score: 0.8372 time: 0.26s
Test loss: 0.3718 score: 0.8636 time: 0.21s
Epoch 90/1000, LR 0.000266
Train loss: 0.2753;  Loss pred: 0.2753; Loss self: 0.0000; time: 0.42s
Val loss: 0.3638 score: 0.8372 time: 0.23s
Test loss: 0.3633 score: 0.8636 time: 0.20s
Epoch 91/1000, LR 0.000266
Train loss: 0.3028;  Loss pred: 0.3028; Loss self: 0.0000; time: 0.30s
Val loss: 0.3566 score: 0.8372 time: 0.26s
Test loss: 0.3541 score: 0.8636 time: 0.22s
Epoch 92/1000, LR 0.000266
Train loss: 0.2788;  Loss pred: 0.2788; Loss self: 0.0000; time: 0.29s
Val loss: 0.3496 score: 0.8372 time: 0.26s
Test loss: 0.3451 score: 0.8864 time: 0.20s
Epoch 93/1000, LR 0.000265
Train loss: 0.2474;  Loss pred: 0.2474; Loss self: 0.0000; time: 0.32s
Val loss: 0.3433 score: 0.8372 time: 0.24s
Test loss: 0.3368 score: 0.8864 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.2588;  Loss pred: 0.2588; Loss self: 0.0000; time: 0.27s
Val loss: 0.3379 score: 0.8372 time: 0.26s
Test loss: 0.3294 score: 0.8864 time: 0.19s
Epoch 95/1000, LR 0.000265
Train loss: 0.2329;  Loss pred: 0.2329; Loss self: 0.0000; time: 0.47s
Val loss: 0.3341 score: 0.8372 time: 0.25s
Test loss: 0.3246 score: 0.8636 time: 0.21s
Epoch 96/1000, LR 0.000265
Train loss: 0.2048;  Loss pred: 0.2048; Loss self: 0.0000; time: 0.29s
Val loss: 0.3306 score: 0.8372 time: 0.26s
Test loss: 0.3203 score: 0.8636 time: 0.20s
Epoch 97/1000, LR 0.000265
Train loss: 0.2214;  Loss pred: 0.2214; Loss self: 0.0000; time: 0.48s
Val loss: 0.3284 score: 0.8372 time: 0.32s
Test loss: 0.3184 score: 0.8636 time: 0.20s
Epoch 98/1000, LR 0.000265
Train loss: 0.1796;  Loss pred: 0.1796; Loss self: 0.0000; time: 0.27s
Val loss: 0.3276 score: 0.8372 time: 0.26s
Test loss: 0.3187 score: 0.8636 time: 0.21s
Epoch 99/1000, LR 0.000265
Train loss: 0.2188;  Loss pred: 0.2188; Loss self: 0.0000; time: 0.34s
Val loss: 0.3299 score: 0.8605 time: 0.23s
Test loss: 0.3244 score: 0.8409 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 100/1000, LR 0.000265
Train loss: 0.2192;  Loss pred: 0.2192; Loss self: 0.0000; time: 0.27s
Val loss: 0.3284 score: 0.8605 time: 0.28s
Test loss: 0.3230 score: 0.8409 time: 0.18s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.1796,   Val_Loss: 0.3276,   Val_Precision: 0.8261,   Val_Recall: 0.8636,   Val_accuracy: 0.8444,   Val_Score: 0.8372,   Val_Loss: 0.3276,   Test_Precision: 0.8333,   Test_Recall: 0.9091,   Test_accuracy: 0.8696,   Test_Score: 0.8636,   Test_loss: 0.3187


[0.22862422303296626, 0.19526325596962124, 0.19915195391513407, 0.30758272006642073, 0.2046249359846115, 0.22750074591021985, 0.2007625149562955, 0.2203284880379215, 0.19578433607239276, 0.2101050189230591, 0.21510635898448527, 0.19556984794326127, 0.32747843698598444, 0.20391071308404207, 0.226819853996858, 0.21493619889952242, 0.22282760695088655, 0.21068638702854514, 0.21242628595791757, 0.2928501630667597, 0.2180708929663524, 0.20697432407177985, 0.214301300002262, 0.2104814969934523, 0.19704321993049234, 0.21822482894640416, 0.29869245109148324, 0.23347408999688923, 0.22160861699376255, 0.23289073805790395, 0.1964448969811201, 0.22658289596438408, 0.28171762300189584, 0.2252649200381711, 0.1964615600882098, 0.22767848195508122, 0.2055820020614192, 0.2180394169408828, 0.21118808607570827, 0.22241458005737513, 0.23819970595650375, 0.21591165999416262, 0.22540965501684695, 0.2084304599557072, 0.209498459007591, 0.19568025798071176, 0.2163479820592329, 0.20383141306228936, 0.20763899700250477, 0.22314589901361614, 0.2107539540156722, 0.22915560205001384, 0.1899319989606738, 0.219640635070391, 0.21921175997704268, 0.22582414897624403, 0.216970507055521, 0.22763559804297984, 0.20915250305552036, 0.19876326399389654, 0.22564117598813027, 0.21324046002700925, 0.2157518289750442, 0.22162577998824418, 0.22342550801113248, 0.20909748098347336, 0.2191111519932747, 0.22647733893245459, 0.21683869196567684, 0.3225954909576103, 0.2013804349116981, 0.22770464199129492, 0.19620646396651864, 0.19814019103068858, 0.22123636992182583, 0.21419390907976776, 0.21909486898221076, 0.20150529802776873, 0.22675039898604155, 0.20555364806205034, 0.21569098893087357, 0.2287145929876715, 0.19961069396231323, 0.22732917999383062, 0.28464854101184756, 0.22242675197776407, 0.19885018095374107, 0.22072648000903428, 0.21053687203675508, 0.20766397891566157, 0.22246998001355678, 0.20350249798502773, 0.22960752493236214, 0.1954265219392255, 0.21766748605296016, 0.2082507919985801, 0.20504000899381936, 0.21862693107686937, 0.22070753399748355, 0.18724638095591217]
[0.005196005068931051, 0.004437801272036846, 0.004526180770798502, 0.006990516365145926, 0.004650566726922989, 0.005170471497959542, 0.004562784430824898, 0.0050074656372254885, 0.00444964400164529, 0.0047751140664331615, 0.004888780886011029, 0.004444769271437756, 0.007442691749681465, 0.0046343343882736835, 0.005154996681746773, 0.004884913611352782, 0.005064263794338331, 0.004788326977921481, 0.004827870135407217, 0.006655685524244539, 0.004956156658326191, 0.004703961910722269, 0.0048704840909605, 0.004783670386214825, 0.004478254998420281, 0.004959655203327367, 0.00678846479753371, 0.005306229318111119, 0.005036559477130967, 0.005292971319497817, 0.004464656749570911, 0.00514961127191782, 0.006402673250043087, 0.005119657273594798, 0.004465035456550223, 0.005174510953524573, 0.004672318228668618, 0.004955441294110973, 0.004799729228993369, 0.0050548768194857985, 0.005413629680829631, 0.004907083181685514, 0.00512294670492834, 0.004737055908084254, 0.004761328613808887, 0.004447278590470722, 0.004916999592255293, 0.004632532115052031, 0.00471906811369329, 0.005071497704854913, 0.004789862591265278, 0.005208081864773042, 0.004316636340015314, 0.004991832615236159, 0.004982085454023697, 0.0051323670221873645, 0.004931147887625478, 0.005173536319158633, 0.004753465978534554, 0.004517346908952194, 0.005128208545184779, 0.004846374091522937, 0.004903450658523731, 0.005036949545187367, 0.005077852454798465, 0.004752215476897122, 0.004979798908938061, 0.005147212248464877, 0.00492815209012902, 0.007331715703582053, 0.004576828066174957, 0.005175105499802157, 0.004459237817420878, 0.004503186159788377, 0.005028099316405132, 0.00486804338817654, 0.00497942884050479, 0.004579665864267471, 0.005153418158773671, 0.004671673819592053, 0.004902067930247126, 0.005198058931537988, 0.004536606680961664, 0.005166572272587059, 0.006469285022996535, 0.005055153454040093, 0.004519322294403206, 0.005016510909296234, 0.004784928909926252, 0.004719635884446854, 0.005056135909399018, 0.0046250567723869936, 0.005218352839371867, 0.0044415118622551245, 0.0049469883193854584, 0.004732972545422275, 0.004660000204404985, 0.0049687938881106675, 0.0050160803181246265, 0.004255599567179822]
[192.45554743188984, 225.33681404373107, 220.9368230389043, 143.05094899511406, 215.02755657946278, 193.40595928139953, 219.16441926212408, 199.70181973212203, 224.73708000690445, 209.41908111254062, 204.5499733607296, 224.98355683522996, 134.35999146986015, 215.78071762156677, 193.98654581891012, 204.71191090789205, 197.46206765886978, 208.84121001153528, 207.13067500844298, 150.24748335198817, 201.76924761246815, 212.58675537329236, 205.318399839551, 209.044503334033, 223.30126362897008, 201.62691941349334, 147.30871114825638, 188.4577427867317, 198.54823606086777, 188.9297975819898, 223.9813844807012, 194.18941492793098, 156.18476235582855, 195.32557485002195, 223.9623872489067, 193.25497790643553, 214.0265176854084, 201.79837488709555, 208.3450862101499, 197.8287573982315, 184.7189517859211, 203.78704883835167, 195.2001567843732, 211.10158279816818, 210.02541120555782, 224.85661279298338, 203.37605916727918, 215.86466648569976, 211.90624417950363, 197.18041063938693, 208.77425624350585, 192.0092705846086, 231.66185919577657, 200.3272299130749, 200.71915851872168, 194.841872312906, 202.79253893590595, 193.29138490761173, 210.3728110216306, 221.36887428730853, 194.99987006943536, 206.33982872868927, 203.93801623387137, 198.5328602220099, 196.9336464384704, 210.4281686850052, 200.81132155861476, 194.2799231366928, 202.9158154438817, 136.3937228923689, 218.4919305556831, 193.23277564838625, 224.25356999200758, 222.06499232245687, 198.88230861656, 205.42134082633507, 200.82624574641474, 218.35654164257517, 194.04596506446978, 214.0560404294929, 203.99554111229693, 192.37950419006168, 220.42907184275037, 193.55192325593262, 154.5765871259767, 197.81793156067246, 221.27211445804926, 199.3417373311942, 208.98952081096073, 211.88075192313295, 197.77949365266608, 216.21356217080543, 191.63135011782185, 225.14855999782512, 202.14319004582276, 211.28371026939482, 214.59226526529423, 201.25608397498647, 199.3588492565989, 234.98451492293438]
Elapsed: 0.21996925979736262~0.025486074016458353
Time per graph: 0.004999301359030968~0.0005792289549195079
Speed: 202.19086544396453~18.944134999582076
Total Time: 0.1877
best val loss: 0.3276478946208954 test_score: 0.8636

Testing...
Test loss: 0.5812 score: 0.7955 time: 0.25s
test Score 0.7955
Epoch Time List: [0.7898838280234486, 0.7205882560228929, 0.7656336849322543, 0.8824030329706147, 0.767019784078002, 0.7941134249558672, 0.7363117591012269, 0.7735699759796262, 0.7501403699861839, 0.8489646909292787, 0.7500926699722186, 0.7205987370107323, 0.9068748289719224, 0.7825824060710147, 0.7867603519698605, 0.738455738988705, 0.7907570028910413, 0.7740578721277416, 0.879472263972275, 0.8421672009862959, 0.7816656631184742, 0.7421637490624562, 0.8206897580530494, 0.7504172939807177, 0.7058746269904077, 0.8397899919655174, 0.8443760080263019, 0.9241102138767019, 0.7774654519744217, 0.8169060529908165, 0.7163150478154421, 0.9701322691980749, 0.87105261196848, 0.7901832379866391, 0.8341476010391489, 0.787848562002182, 0.746992215863429, 0.8230913488660008, 0.7525776319671422, 0.7664412459125742, 0.8479503970593214, 0.7663035730365664, 0.8464073849609122, 0.7496550299692899, 0.8610875440062955, 0.8577079308452085, 0.8797863899962977, 0.8086332740494981, 0.8047918088268489, 0.7841789000667632, 0.7380566911306232, 0.8432616080390289, 0.730171972187236, 0.8067083200439811, 0.9519881979795173, 0.842390819103457, 0.7609388530254364, 0.7945975009351969, 0.7457539071328938, 0.7702745769638568, 0.7816245548892766, 0.7314273479860276, 0.914555991999805, 0.7473525707609951, 0.7931130080251023, 0.7577285250881687, 0.8053644499741495, 0.7665772911859676, 0.7756491530453786, 0.8698272091569379, 0.7512433810625225, 0.7833422098774463, 0.723176580038853, 0.7632837810087949, 0.7925888227764517, 0.7449938040226698, 0.79454866994638, 0.8495713201118633, 0.794006644980982, 0.7371998059097677, 0.7799072340130806, 0.8372029260499403, 0.7397278650896624, 0.7797489340882748, 0.8212045159889385, 0.7852598280878738, 0.7329141719965264, 0.8724282749462873, 0.759778494015336, 0.8552589541068301, 0.7765551779884845, 0.7444196350406855, 0.7775634209392592, 0.7136641340330243, 0.925817362847738, 0.7518663968658075, 1.0032506260322407, 0.7411291749449447, 0.7877623869571835, 0.7380407440941781]
Total Epoch List: [100]
Total Time List: [0.18766220496036112]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcf070>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.20s
Epoch 2/1000, LR 0.000000
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.21s
Epoch 5/1000, LR 0.000090
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 0.19s
Epoch 6/1000, LR 0.000120
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.21s
Epoch 8/1000, LR 0.000180
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.20s
Epoch 10/1000, LR 0.000240
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.21s
Epoch 11/1000, LR 0.000270
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.20s
Epoch 12/1000, LR 0.000270
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.22s
Epoch 13/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.20s
Epoch 14/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.22s
Epoch 15/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.19s
Epoch 18/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.20s
Epoch 21/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.35s
Val loss: 0.6923 score: 0.5455 time: 0.23s
Test loss: 0.6920 score: 0.5814 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.55s
Val loss: 0.6922 score: 0.5909 time: 0.21s
Test loss: 0.6919 score: 0.5581 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.30s
Val loss: 0.6920 score: 0.5682 time: 0.27s
Test loss: 0.6919 score: 0.6512 time: 0.21s
Epoch 24/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.38s
Val loss: 0.6919 score: 0.5000 time: 0.23s
Test loss: 0.6918 score: 0.4884 time: 0.20s
Epoch 25/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.20s
Test loss: 0.6918 score: 0.5116 time: 0.22s
Epoch 26/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.23s
Test loss: 0.6917 score: 0.5116 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.19s
Epoch 29/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.19s
Epoch 31/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.21s
Epoch 32/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.21s
Epoch 34/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4884 time: 0.20s
Epoch 35/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.20s
Epoch 36/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.21s
Epoch 37/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4884 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4884 time: 0.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4884 time: 0.19s
Epoch 40/1000, LR 0.000269
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4884 time: 0.20s
Epoch 41/1000, LR 0.000269
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6849 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.4884 time: 0.19s
Epoch 42/1000, LR 0.000269
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6839 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4884 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.4884 time: 0.19s
Epoch 44/1000, LR 0.000269
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6814 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.4884 time: 0.21s
Epoch 45/1000, LR 0.000269
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6839 score: 0.4884 time: 0.20s
Epoch 46/1000, LR 0.000269
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.4884 time: 0.19s
Epoch 47/1000, LR 0.000269
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6765 score: 0.5000 time: 0.24s
Test loss: 0.6815 score: 0.5116 time: 0.29s
Epoch 48/1000, LR 0.000269
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6745 score: 0.5000 time: 0.21s
Test loss: 0.6800 score: 0.5116 time: 0.23s
Epoch 49/1000, LR 0.000269
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.31s
Val loss: 0.6723 score: 0.5227 time: 0.24s
Test loss: 0.6785 score: 0.5116 time: 0.21s
Epoch 50/1000, LR 0.000269
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.38s
Val loss: 0.6699 score: 0.6364 time: 0.21s
Test loss: 0.6767 score: 0.5116 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6593;  Loss pred: 0.6593; Loss self: 0.0000; time: 0.37s
Val loss: 0.6672 score: 0.6364 time: 0.23s
Test loss: 0.6748 score: 0.5581 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 0.37s
Val loss: 0.6642 score: 0.7273 time: 0.21s
Test loss: 0.6727 score: 0.5581 time: 0.21s
Epoch 53/1000, LR 0.000269
Train loss: 0.6502;  Loss pred: 0.6502; Loss self: 0.0000; time: 0.37s
Val loss: 0.6610 score: 0.7273 time: 0.23s
Test loss: 0.6704 score: 0.5581 time: 0.18s
Epoch 54/1000, LR 0.000269
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 0.39s
Val loss: 0.6574 score: 0.7500 time: 0.21s
Test loss: 0.6679 score: 0.5581 time: 0.21s
Epoch 55/1000, LR 0.000269
Train loss: 0.6388;  Loss pred: 0.6388; Loss self: 0.0000; time: 0.39s
Val loss: 0.6535 score: 0.7500 time: 0.22s
Test loss: 0.6651 score: 0.6279 time: 0.20s
Epoch 56/1000, LR 0.000269
Train loss: 0.6334;  Loss pred: 0.6334; Loss self: 0.0000; time: 0.36s
Val loss: 0.6492 score: 0.7727 time: 0.22s
Test loss: 0.6620 score: 0.6744 time: 0.19s
Epoch 57/1000, LR 0.000269
Train loss: 0.6283;  Loss pred: 0.6283; Loss self: 0.0000; time: 0.50s
Val loss: 0.6447 score: 0.7727 time: 0.23s
Test loss: 0.6587 score: 0.6744 time: 0.19s
Epoch 58/1000, LR 0.000269
Train loss: 0.6222;  Loss pred: 0.6222; Loss self: 0.0000; time: 0.38s
Val loss: 0.6398 score: 0.7727 time: 0.21s
Test loss: 0.6552 score: 0.6744 time: 0.20s
Epoch 59/1000, LR 0.000268
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.44s
Val loss: 0.6346 score: 0.7727 time: 0.23s
Test loss: 0.6513 score: 0.6977 time: 0.20s
Epoch 60/1000, LR 0.000268
Train loss: 0.6037;  Loss pred: 0.6037; Loss self: 0.0000; time: 0.39s
Val loss: 0.6291 score: 0.7955 time: 0.21s
Test loss: 0.6471 score: 0.6977 time: 0.21s
Epoch 61/1000, LR 0.000268
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.37s
Val loss: 0.6233 score: 0.7955 time: 0.22s
Test loss: 0.6427 score: 0.6744 time: 0.22s
Epoch 62/1000, LR 0.000268
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 0.32s
Val loss: 0.6171 score: 0.8182 time: 0.24s
Test loss: 0.6381 score: 0.6744 time: 0.19s
Epoch 63/1000, LR 0.000268
Train loss: 0.5771;  Loss pred: 0.5771; Loss self: 0.0000; time: 0.41s
Val loss: 0.6106 score: 0.8182 time: 0.21s
Test loss: 0.6332 score: 0.7209 time: 0.21s
Epoch 64/1000, LR 0.000268
Train loss: 0.5686;  Loss pred: 0.5686; Loss self: 0.0000; time: 0.31s
Val loss: 0.6038 score: 0.8182 time: 0.23s
Test loss: 0.6282 score: 0.7209 time: 0.21s
Epoch 65/1000, LR 0.000268
Train loss: 0.5585;  Loss pred: 0.5585; Loss self: 0.0000; time: 0.55s
Val loss: 0.5967 score: 0.8182 time: 0.23s
Test loss: 0.6232 score: 0.7209 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.36s
Val loss: 0.5896 score: 0.7955 time: 0.21s
Test loss: 0.6181 score: 0.6977 time: 0.19s
Epoch 67/1000, LR 0.000268
Train loss: 0.5343;  Loss pred: 0.5343; Loss self: 0.0000; time: 0.43s
Val loss: 0.5823 score: 0.7955 time: 0.22s
Test loss: 0.6131 score: 0.6977 time: 0.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.5205;  Loss pred: 0.5205; Loss self: 0.0000; time: 0.34s
Val loss: 0.5749 score: 0.7955 time: 0.22s
Test loss: 0.6083 score: 0.6977 time: 0.19s
Epoch 69/1000, LR 0.000268
Train loss: 0.5050;  Loss pred: 0.5050; Loss self: 0.0000; time: 0.47s
Val loss: 0.5676 score: 0.7955 time: 0.22s
Test loss: 0.6037 score: 0.6977 time: 0.20s
Epoch 70/1000, LR 0.000268
Train loss: 0.4959;  Loss pred: 0.4959; Loss self: 0.0000; time: 0.37s
Val loss: 0.5604 score: 0.8182 time: 0.22s
Test loss: 0.5993 score: 0.7209 time: 0.21s
Epoch 71/1000, LR 0.000268
Train loss: 0.4761;  Loss pred: 0.4761; Loss self: 0.0000; time: 0.45s
Val loss: 0.5532 score: 0.8182 time: 0.23s
Test loss: 0.5949 score: 0.7209 time: 0.20s
Epoch 72/1000, LR 0.000267
Train loss: 0.4659;  Loss pred: 0.4659; Loss self: 0.0000; time: 0.43s
Val loss: 0.5463 score: 0.8182 time: 0.23s
Test loss: 0.5908 score: 0.7209 time: 0.31s
Epoch 73/1000, LR 0.000267
Train loss: 0.4510;  Loss pred: 0.4510; Loss self: 0.0000; time: 0.31s
Val loss: 0.5396 score: 0.7955 time: 0.32s
Test loss: 0.5868 score: 0.6977 time: 0.20s
Epoch 74/1000, LR 0.000267
Train loss: 0.4487;  Loss pred: 0.4487; Loss self: 0.0000; time: 0.35s
Val loss: 0.5331 score: 0.7955 time: 0.23s
Test loss: 0.5827 score: 0.6977 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.4234;  Loss pred: 0.4234; Loss self: 0.0000; time: 0.32s
Val loss: 0.5268 score: 0.7955 time: 0.24s
Test loss: 0.5786 score: 0.6977 time: 0.21s
Epoch 76/1000, LR 0.000267
Train loss: 0.4014;  Loss pred: 0.4014; Loss self: 0.0000; time: 0.39s
Val loss: 0.5210 score: 0.7955 time: 0.23s
Test loss: 0.5748 score: 0.7209 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.4031;  Loss pred: 0.4031; Loss self: 0.0000; time: 0.34s
Val loss: 0.5150 score: 0.7955 time: 0.24s
Test loss: 0.5705 score: 0.7209 time: 0.18s
Epoch 78/1000, LR 0.000267
Train loss: 0.3824;  Loss pred: 0.3824; Loss self: 0.0000; time: 0.40s
Val loss: 0.5094 score: 0.7955 time: 0.30s
Test loss: 0.5662 score: 0.7209 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.3646;  Loss pred: 0.3646; Loss self: 0.0000; time: 0.34s
Val loss: 0.5043 score: 0.7955 time: 0.24s
Test loss: 0.5627 score: 0.7209 time: 0.20s
Epoch 80/1000, LR 0.000267
Train loss: 0.3647;  Loss pred: 0.3647; Loss self: 0.0000; time: 0.38s
Val loss: 0.5001 score: 0.7955 time: 0.20s
Test loss: 0.5601 score: 0.7674 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.3492;  Loss pred: 0.3492; Loss self: 0.0000; time: 0.37s
Val loss: 0.4958 score: 0.7955 time: 0.24s
Test loss: 0.5570 score: 0.7674 time: 0.21s
Epoch 82/1000, LR 0.000267
Train loss: 0.3234;  Loss pred: 0.3234; Loss self: 0.0000; time: 0.45s
Val loss: 0.4921 score: 0.7955 time: 0.20s
Test loss: 0.5543 score: 0.7674 time: 0.21s
Epoch 83/1000, LR 0.000266
Train loss: 0.3090;  Loss pred: 0.3090; Loss self: 0.0000; time: 0.35s
Val loss: 0.4885 score: 0.7955 time: 0.41s
Test loss: 0.5515 score: 0.7907 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2994;  Loss pred: 0.2994; Loss self: 0.0000; time: 0.53s
Val loss: 0.4850 score: 0.7955 time: 0.22s
Test loss: 0.5483 score: 0.7907 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.2864;  Loss pred: 0.2864; Loss self: 0.0000; time: 0.35s
Val loss: 0.4824 score: 0.8409 time: 0.23s
Test loss: 0.5460 score: 0.8140 time: 0.20s
Epoch 86/1000, LR 0.000266
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 0.40s
Val loss: 0.4796 score: 0.8409 time: 0.23s
Test loss: 0.5427 score: 0.8140 time: 0.21s
Epoch 87/1000, LR 0.000266
Train loss: 0.2624;  Loss pred: 0.2624; Loss self: 0.0000; time: 0.32s
Val loss: 0.4771 score: 0.8409 time: 0.23s
Test loss: 0.5391 score: 0.8140 time: 0.19s
Epoch 88/1000, LR 0.000266
Train loss: 0.2492;  Loss pred: 0.2492; Loss self: 0.0000; time: 0.36s
Val loss: 0.4756 score: 0.8409 time: 0.20s
Test loss: 0.5363 score: 0.8372 time: 0.22s
Epoch 89/1000, LR 0.000266
Train loss: 0.2382;  Loss pred: 0.2382; Loss self: 0.0000; time: 0.36s
Val loss: 0.4742 score: 0.8409 time: 0.23s
Test loss: 0.5328 score: 0.8372 time: 0.19s
Epoch 90/1000, LR 0.000266
Train loss: 0.2234;  Loss pred: 0.2234; Loss self: 0.0000; time: 0.36s
Val loss: 0.4734 score: 0.8409 time: 0.23s
Test loss: 0.5294 score: 0.8372 time: 0.21s
Epoch 91/1000, LR 0.000266
Train loss: 0.2227;  Loss pred: 0.2227; Loss self: 0.0000; time: 0.44s
Val loss: 0.4735 score: 0.8409 time: 0.23s
Test loss: 0.5271 score: 0.8372 time: 0.19s
     INFO: Early stopping counter 1 of 2
Epoch 92/1000, LR 0.000266
Train loss: 0.2025;  Loss pred: 0.2025; Loss self: 0.0000; time: 0.35s
Val loss: 0.4741 score: 0.8409 time: 0.20s
Test loss: 0.5250 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.2234,   Val_Loss: 0.4734,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.4734,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.5294


[0.22862422303296626, 0.19526325596962124, 0.19915195391513407, 0.30758272006642073, 0.2046249359846115, 0.22750074591021985, 0.2007625149562955, 0.2203284880379215, 0.19578433607239276, 0.2101050189230591, 0.21510635898448527, 0.19556984794326127, 0.32747843698598444, 0.20391071308404207, 0.226819853996858, 0.21493619889952242, 0.22282760695088655, 0.21068638702854514, 0.21242628595791757, 0.2928501630667597, 0.2180708929663524, 0.20697432407177985, 0.214301300002262, 0.2104814969934523, 0.19704321993049234, 0.21822482894640416, 0.29869245109148324, 0.23347408999688923, 0.22160861699376255, 0.23289073805790395, 0.1964448969811201, 0.22658289596438408, 0.28171762300189584, 0.2252649200381711, 0.1964615600882098, 0.22767848195508122, 0.2055820020614192, 0.2180394169408828, 0.21118808607570827, 0.22241458005737513, 0.23819970595650375, 0.21591165999416262, 0.22540965501684695, 0.2084304599557072, 0.209498459007591, 0.19568025798071176, 0.2163479820592329, 0.20383141306228936, 0.20763899700250477, 0.22314589901361614, 0.2107539540156722, 0.22915560205001384, 0.1899319989606738, 0.219640635070391, 0.21921175997704268, 0.22582414897624403, 0.216970507055521, 0.22763559804297984, 0.20915250305552036, 0.19876326399389654, 0.22564117598813027, 0.21324046002700925, 0.2157518289750442, 0.22162577998824418, 0.22342550801113248, 0.20909748098347336, 0.2191111519932747, 0.22647733893245459, 0.21683869196567684, 0.3225954909576103, 0.2013804349116981, 0.22770464199129492, 0.19620646396651864, 0.19814019103068858, 0.22123636992182583, 0.21419390907976776, 0.21909486898221076, 0.20150529802776873, 0.22675039898604155, 0.20555364806205034, 0.21569098893087357, 0.2287145929876715, 0.19961069396231323, 0.22732917999383062, 0.28464854101184756, 0.22242675197776407, 0.19885018095374107, 0.22072648000903428, 0.21053687203675508, 0.20766397891566157, 0.22246998001355678, 0.20350249798502773, 0.22960752493236214, 0.1954265219392255, 0.21766748605296016, 0.2082507919985801, 0.20504000899381936, 0.21862693107686937, 0.22070753399748355, 0.18724638095591217, 0.20385796995833516, 0.23046769108623266, 0.212879495928064, 0.21536330599337816, 0.1974936000769958, 0.22138772602193058, 0.21107104001566768, 0.21582338004373014, 0.2044291190104559, 0.21315676590893418, 0.20306242897640914, 0.22490200505126268, 0.20240289298817515, 0.22608136804774404, 0.21122231695335358, 0.2146204230375588, 0.194832026027143, 0.21652250993065536, 0.20591698109637946, 0.20692912605591118, 0.20584141893777996, 0.22287536307703704, 0.21428271103650331, 0.2041442859917879, 0.22214753099251539, 0.19967138196807355, 0.2215029839426279, 0.19550216500647366, 0.22133302805013955, 0.1913884220412001, 0.2158114470075816, 0.2046468899352476, 0.21293877402786165, 0.20444612798746675, 0.20017879910301417, 0.21936052001547068, 0.20531710505019873, 0.22362931398674846, 0.19352318299934268, 0.2083052200032398, 0.19822376198135316, 0.22650872403755784, 0.19485514401458204, 0.21612016099970788, 0.20231651095673442, 0.19362455199006945, 0.2989163240417838, 0.2324890160234645, 0.20972965494729578, 0.22185824892949313, 0.21506992599461228, 0.21776679100003093, 0.18451940291561186, 0.21115881099831313, 0.20860134903341532, 0.19604350498411804, 0.19805897097103298, 0.20000635692849755, 0.20880646002478898, 0.21116133907344192, 0.22682516009081155, 0.19792011694516987, 0.2190224410733208, 0.21372392610646784, 0.20562209805939347, 0.19706184708047658, 0.21246954100206494, 0.19306890794541687, 0.20643059501890093, 0.21310932899359614, 0.20441412401851267, 0.31152665603440255, 0.2008538480149582, 0.2348405959783122, 0.21419736102689058, 0.24123695597518235, 0.1891133349854499, 0.22430676803924143, 0.20575573202222586, 0.26312991802114993, 0.2112658160040155, 0.21645028295461088, 0.23444714292418212, 0.23378657293505967, 0.20029954006895423, 0.21242467896081507, 0.18973220197949558, 0.22102462698239833, 0.19578493502922356, 0.21771703998092562, 0.19553878298029304, 0.21520683204289526]
[0.005196005068931051, 0.004437801272036846, 0.004526180770798502, 0.006990516365145926, 0.004650566726922989, 0.005170471497959542, 0.004562784430824898, 0.0050074656372254885, 0.00444964400164529, 0.0047751140664331615, 0.004888780886011029, 0.004444769271437756, 0.007442691749681465, 0.0046343343882736835, 0.005154996681746773, 0.004884913611352782, 0.005064263794338331, 0.004788326977921481, 0.004827870135407217, 0.006655685524244539, 0.004956156658326191, 0.004703961910722269, 0.0048704840909605, 0.004783670386214825, 0.004478254998420281, 0.004959655203327367, 0.00678846479753371, 0.005306229318111119, 0.005036559477130967, 0.005292971319497817, 0.004464656749570911, 0.00514961127191782, 0.006402673250043087, 0.005119657273594798, 0.004465035456550223, 0.005174510953524573, 0.004672318228668618, 0.004955441294110973, 0.004799729228993369, 0.0050548768194857985, 0.005413629680829631, 0.004907083181685514, 0.00512294670492834, 0.004737055908084254, 0.004761328613808887, 0.004447278590470722, 0.004916999592255293, 0.004632532115052031, 0.00471906811369329, 0.005071497704854913, 0.004789862591265278, 0.005208081864773042, 0.004316636340015314, 0.004991832615236159, 0.004982085454023697, 0.0051323670221873645, 0.004931147887625478, 0.005173536319158633, 0.004753465978534554, 0.004517346908952194, 0.005128208545184779, 0.004846374091522937, 0.004903450658523731, 0.005036949545187367, 0.005077852454798465, 0.004752215476897122, 0.004979798908938061, 0.005147212248464877, 0.00492815209012902, 0.007331715703582053, 0.004576828066174957, 0.005175105499802157, 0.004459237817420878, 0.004503186159788377, 0.005028099316405132, 0.00486804338817654, 0.00497942884050479, 0.004579665864267471, 0.005153418158773671, 0.004671673819592053, 0.004902067930247126, 0.005198058931537988, 0.004536606680961664, 0.005166572272587059, 0.006469285022996535, 0.005055153454040093, 0.004519322294403206, 0.005016510909296234, 0.004784928909926252, 0.004719635884446854, 0.005056135909399018, 0.0046250567723869936, 0.005218352839371867, 0.0044415118622551245, 0.0049469883193854584, 0.004732972545422275, 0.004660000204404985, 0.0049687938881106675, 0.0050160803181246265, 0.004255599567179822, 0.004740883022286865, 0.005359713746191457, 0.004950685951815441, 0.00500844897659019, 0.004592874420395251, 0.0051485517679518734, 0.004908628837573667, 0.005019148373110003, 0.004754165558382695, 0.004957134090905446, 0.004722382069218817, 0.005230279187238667, 0.004707044022980817, 0.005257706233668466, 0.004912146905891944, 0.004991172628780437, 0.004530977349468442, 0.005035407207689659, 0.004788767002241383, 0.004812305257114214, 0.004787009742739069, 0.005183147978535745, 0.004983318861314031, 0.004747541534692742, 0.00516622165098873, 0.004643520510885431, 0.005151232184712277, 0.004546561976894736, 0.0051472797220962685, 0.004450893535841863, 0.005018870860641432, 0.0047592299984941305, 0.004952064512275853, 0.004754561115987599, 0.004655320909372423, 0.005101407442220249, 0.00477481639651625, 0.005200681720622057, 0.004500539139519597, 0.00484430744193581, 0.004609854929798911, 0.005267644745059485, 0.0045315149770833035, 0.00502605025580716, 0.004705035138528707, 0.004502896557908592, 0.006951542419576367, 0.005406721302871268, 0.004877433835983623, 0.005159494161151003, 0.005001626185921215, 0.005064343976744905, 0.004291148905014229, 0.004910670023216585, 0.004851194163567798, 0.0045591512787004195, 0.004606022580721697, 0.004651310626244129, 0.004855964186622999, 0.00491072881566144, 0.005275003723042129, 0.004602793417329532, 0.0050935451412400185, 0.004970323862941112, 0.0047819092571951965, 0.004582833653034339, 0.004941152116327092, 0.004489974603381788, 0.004800711512067463, 0.004956030906827817, 0.004753816837639829, 0.007244805954288431, 0.004671019721278097, 0.005461409208797958, 0.004981333977369548, 0.005610161766864706, 0.004397984534545347, 0.0052164364660288705, 0.004785017023772695, 0.00611930041909651, 0.004913158511721291, 0.0050337275105723465, 0.005452259137771677, 0.005436897045001388, 0.004658128838812889, 0.004940108813042211, 0.004412376790220828, 0.005140107604241822, 0.004553138023935432, 0.005063186976300596, 0.004547413557681234, 0.005004810047509192]
[192.45554743188984, 225.33681404373107, 220.9368230389043, 143.05094899511406, 215.02755657946278, 193.40595928139953, 219.16441926212408, 199.70181973212203, 224.73708000690445, 209.41908111254062, 204.5499733607296, 224.98355683522996, 134.35999146986015, 215.78071762156677, 193.98654581891012, 204.71191090789205, 197.46206765886978, 208.84121001153528, 207.13067500844298, 150.24748335198817, 201.76924761246815, 212.58675537329236, 205.318399839551, 209.044503334033, 223.30126362897008, 201.62691941349334, 147.30871114825638, 188.4577427867317, 198.54823606086777, 188.9297975819898, 223.9813844807012, 194.18941492793098, 156.18476235582855, 195.32557485002195, 223.9623872489067, 193.25497790643553, 214.0265176854084, 201.79837488709555, 208.3450862101499, 197.8287573982315, 184.7189517859211, 203.78704883835167, 195.2001567843732, 211.10158279816818, 210.02541120555782, 224.85661279298338, 203.37605916727918, 215.86466648569976, 211.90624417950363, 197.18041063938693, 208.77425624350585, 192.0092705846086, 231.66185919577657, 200.3272299130749, 200.71915851872168, 194.841872312906, 202.79253893590595, 193.29138490761173, 210.3728110216306, 221.36887428730853, 194.99987006943536, 206.33982872868927, 203.93801623387137, 198.5328602220099, 196.9336464384704, 210.4281686850052, 200.81132155861476, 194.2799231366928, 202.9158154438817, 136.3937228923689, 218.4919305556831, 193.23277564838625, 224.25356999200758, 222.06499232245687, 198.88230861656, 205.42134082633507, 200.82624574641474, 218.35654164257517, 194.04596506446978, 214.0560404294929, 203.99554111229693, 192.37950419006168, 220.42907184275037, 193.55192325593262, 154.5765871259767, 197.81793156067246, 221.27211445804926, 199.3417373311942, 208.98952081096073, 211.88075192313295, 197.77949365266608, 216.21356217080543, 191.63135011782185, 225.14855999782512, 202.14319004582276, 211.28371026939482, 214.59226526529423, 201.25608397498647, 199.3588492565989, 234.98451492293438, 210.9311694254015, 186.57712843537342, 201.99221072249492, 199.66261105465261, 217.7285744106939, 194.22937654520393, 203.72287925813097, 199.23698716648465, 210.34185446839737, 201.72946336768246, 211.75753789981277, 191.1943825943164, 212.44755628326004, 190.19700902959516, 203.57697340047707, 200.35371933115124, 220.702935122224, 198.59367053232205, 208.8220202678372, 207.80061666322229, 208.89867657294806, 192.93294425340775, 200.66947908212202, 210.6353346658439, 193.56505925536828, 215.35384578484806, 194.1283103036551, 219.94641337386795, 194.27737639887627, 224.67398780655293, 199.24800373767656, 210.1180233601676, 201.93597993747127, 210.32435499407475, 214.80796264479406, 196.0243347206114, 209.43213664291034, 192.28248405103116, 222.19560123784268, 206.42785619741653, 216.92656606953608, 189.83816266992918, 220.6767505033487, 198.96339055595152, 212.53826391458279, 222.07927433812924, 143.8529666716672, 184.9549743703166, 205.02584630106662, 193.81744968908262, 199.93497371211814, 197.45894129465265, 233.03782323458756, 203.6381991199198, 206.13481264261594, 219.3390696798832, 217.1070554854541, 214.99316651906489, 205.93232601565654, 203.6357611136601, 189.57332591668646, 217.25937041514766, 196.3269142160878, 201.19413293287198, 209.1214923192718, 218.2056072093933, 202.3819498889118, 222.71840897425426, 208.30245631013605, 201.77436719014838, 210.3572842946299, 138.02992189294844, 214.08601540358669, 183.10292486215246, 200.74943871321426, 178.24797956919875, 227.37687960136896, 191.70175013389408, 208.98567236685835, 163.4173731492735, 203.53505746136756, 198.6599389616737, 183.41021120443244, 183.92844148472278, 214.678475972521, 202.4246910027436, 226.635223495941, 194.54845637370707, 219.62874719437258, 197.50406308926148, 219.90522465476153, 199.80778301419906]
Elapsed: 0.21656425803242504~0.022830055415241298
Time per graph: 0.0049758245404944905~0.0005171552935451349
Speed: 202.7397330133731~17.281214048928497
Total Time: 0.2159
best val loss: 0.4734000861644745 test_score: 0.8372

Testing...
Test loss: 0.5460 score: 0.8140 time: 0.20s
test Score 0.8140
Epoch Time List: [0.7898838280234486, 0.7205882560228929, 0.7656336849322543, 0.8824030329706147, 0.767019784078002, 0.7941134249558672, 0.7363117591012269, 0.7735699759796262, 0.7501403699861839, 0.8489646909292787, 0.7500926699722186, 0.7205987370107323, 0.9068748289719224, 0.7825824060710147, 0.7867603519698605, 0.738455738988705, 0.7907570028910413, 0.7740578721277416, 0.879472263972275, 0.8421672009862959, 0.7816656631184742, 0.7421637490624562, 0.8206897580530494, 0.7504172939807177, 0.7058746269904077, 0.8397899919655174, 0.8443760080263019, 0.9241102138767019, 0.7774654519744217, 0.8169060529908165, 0.7163150478154421, 0.9701322691980749, 0.87105261196848, 0.7901832379866391, 0.8341476010391489, 0.787848562002182, 0.746992215863429, 0.8230913488660008, 0.7525776319671422, 0.7664412459125742, 0.8479503970593214, 0.7663035730365664, 0.8464073849609122, 0.7496550299692899, 0.8610875440062955, 0.8577079308452085, 0.8797863899962977, 0.8086332740494981, 0.8047918088268489, 0.7841789000667632, 0.7380566911306232, 0.8432616080390289, 0.730171972187236, 0.8067083200439811, 0.9519881979795173, 0.842390819103457, 0.7609388530254364, 0.7945975009351969, 0.7457539071328938, 0.7702745769638568, 0.7816245548892766, 0.7314273479860276, 0.914555991999805, 0.7473525707609951, 0.7931130080251023, 0.7577285250881687, 0.8053644499741495, 0.7665772911859676, 0.7756491530453786, 0.8698272091569379, 0.7512433810625225, 0.7833422098774463, 0.723176580038853, 0.7632837810087949, 0.7925888227764517, 0.7449938040226698, 0.79454866994638, 0.8495713201118633, 0.794006644980982, 0.7371998059097677, 0.7799072340130806, 0.8372029260499403, 0.7397278650896624, 0.7797489340882748, 0.8212045159889385, 0.7852598280878738, 0.7329141719965264, 0.8724282749462873, 0.759778494015336, 0.8552589541068301, 0.7765551779884845, 0.7444196350406855, 0.7775634209392592, 0.7136641340330243, 0.925817362847738, 0.7518663968658075, 1.0032506260322407, 0.7411291749449447, 0.7877623869571835, 0.7380407440941781, 0.790858761058189, 0.8244695380562916, 0.7412062980001792, 0.8128272511530668, 0.7360187250887975, 0.9722515800967813, 0.7717658679466695, 0.8219221140025184, 0.7595205809921026, 0.8887388819130138, 0.763341317884624, 0.9089841729728505, 0.778868104913272, 0.9768269408959895, 0.7633105319691822, 0.7637291537830606, 0.7654665720183402, 0.871524426038377, 0.7644346699817106, 0.8751771348761395, 0.7794440829893574, 0.9824308040551841, 0.7749240680132061, 0.8076414410024881, 0.7765992438653484, 0.7265207460150123, 0.9152284690644592, 0.7368711670860648, 0.9165198688860983, 0.8184085190296173, 0.7709732229122892, 0.7526593011571094, 0.8827223090920597, 0.7660090330755338, 0.7736875719856471, 0.7704690741375089, 0.7273238018387929, 0.8626846310216933, 0.8130612840177491, 0.9257160859415308, 0.7740252289222553, 0.8424675349378958, 0.722651582909748, 0.7703924410743639, 0.7507457099854946, 0.7554256760049611, 1.0232365739066154, 0.7947751950705424, 0.7595848710043356, 0.806355073931627, 0.8189795840298757, 0.7929326019948348, 0.7835985409328714, 0.8034741898300126, 0.8169705619802698, 0.7718268701573834, 0.9302155408076942, 0.7902594100451097, 0.8745374729624018, 0.8081299408804625, 0.8097862239228562, 0.7457365141017362, 0.8319266339531168, 0.7489694872638211, 0.9828370918985456, 0.7719638941343874, 0.8576675929361954, 0.7540824931347743, 0.8958443870069459, 0.7958448349963874, 0.8850701830815524, 0.9671427299035713, 0.8235101898899302, 0.8163069180445746, 0.7709170370362699, 0.8542781281284988, 0.7602752608945593, 0.9177412149729207, 0.7784936310490593, 0.8387259918963537, 0.8129754749825224, 0.8527422229526564, 0.9936605229740962, 0.9759925769176334, 0.7786119010997936, 0.8385634600417688, 0.7384265671717003, 0.7784638578305021, 0.7799574240343645, 0.8097827930469066, 0.867069347994402, 0.7653172969585285]
Total Epoch List: [100, 92]
Total Time List: [0.18766220496036112, 0.21585489204153419]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcccd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7014;  Loss pred: 0.7014; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5116 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.7014;  Loss pred: 0.7014; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5116 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.7014;  Loss pred: 0.7014; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5116 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.7013;  Loss pred: 0.7013; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6987 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5116 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.7012;  Loss pred: 0.7012; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5116 time: 0.31s
Epoch 6/1000, LR 0.000120
Train loss: 0.7010;  Loss pred: 0.7010; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5116 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5116 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5116 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.7001;  Loss pred: 0.7001; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5116 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5116 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5116 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5116 time: 0.35s
Epoch 14/1000, LR 0.000270
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5116 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.22s
Epoch 17/1000, LR 0.000270
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6955;  Loss pred: 0.6955; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.23s
Epoch 23/1000, LR 0.000270
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5116 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.32s
Epoch 32/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5116 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5116 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.35s
Val loss: 0.6924 score: 0.5227 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5116 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.39s
Val loss: 0.6922 score: 0.5682 time: 0.16s
Test loss: 0.6898 score: 0.5581 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.35s
Val loss: 0.6919 score: 0.5909 time: 0.19s
Test loss: 0.6894 score: 0.7209 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.41s
Val loss: 0.6916 score: 0.6136 time: 0.17s
Test loss: 0.6890 score: 0.7442 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.35s
Val loss: 0.6912 score: 0.6364 time: 0.19s
Test loss: 0.6885 score: 0.7442 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.33s
Val loss: 0.6908 score: 0.5909 time: 0.20s
Test loss: 0.6879 score: 0.6977 time: 0.30s
Epoch 40/1000, LR 0.000269
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.34s
Val loss: 0.6904 score: 0.5909 time: 0.21s
Test loss: 0.6873 score: 0.6744 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.31s
Val loss: 0.6898 score: 0.5682 time: 0.19s
Test loss: 0.6865 score: 0.6047 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.36s
Val loss: 0.6892 score: 0.5682 time: 0.17s
Test loss: 0.6857 score: 0.6279 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.37s
Val loss: 0.6886 score: 0.5682 time: 0.19s
Test loss: 0.6848 score: 0.5581 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.41s
Val loss: 0.6878 score: 0.5909 time: 0.17s
Test loss: 0.6837 score: 0.5581 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.34s
Val loss: 0.6870 score: 0.5909 time: 0.18s
Test loss: 0.6825 score: 0.5581 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.41s
Val loss: 0.6860 score: 0.6136 time: 0.19s
Test loss: 0.6812 score: 0.5581 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.38s
Val loss: 0.6850 score: 0.6591 time: 0.18s
Test loss: 0.6797 score: 0.5581 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.45s
Val loss: 0.6838 score: 0.6818 time: 0.18s
Test loss: 0.6781 score: 0.5814 time: 0.23s
Epoch 49/1000, LR 0.000269
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.35s
Val loss: 0.6826 score: 0.6136 time: 0.17s
Test loss: 0.6763 score: 0.6047 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 0.35s
Val loss: 0.6812 score: 0.6364 time: 0.19s
Test loss: 0.6744 score: 0.6047 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.33s
Val loss: 0.6796 score: 0.6136 time: 0.17s
Test loss: 0.6722 score: 0.6279 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.39s
Val loss: 0.6779 score: 0.6136 time: 0.20s
Test loss: 0.6697 score: 0.6512 time: 0.28s
Epoch 53/1000, LR 0.000269
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.36s
Val loss: 0.6759 score: 0.6364 time: 0.18s
Test loss: 0.6670 score: 0.6512 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.33s
Val loss: 0.6738 score: 0.6591 time: 0.20s
Test loss: 0.6640 score: 0.6279 time: 0.28s
Epoch 55/1000, LR 0.000269
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.40s
Val loss: 0.6714 score: 0.6591 time: 0.19s
Test loss: 0.6607 score: 0.6744 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6550;  Loss pred: 0.6550; Loss self: 0.0000; time: 0.35s
Val loss: 0.6688 score: 0.6591 time: 0.26s
Test loss: 0.6570 score: 0.6977 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.33s
Val loss: 0.6660 score: 0.6591 time: 0.19s
Test loss: 0.6531 score: 0.6977 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.36s
Val loss: 0.6630 score: 0.6591 time: 0.18s
Test loss: 0.6490 score: 0.6977 time: 0.31s
Epoch 59/1000, LR 0.000268
Train loss: 0.6430;  Loss pred: 0.6430; Loss self: 0.0000; time: 0.31s
Val loss: 0.6597 score: 0.6591 time: 0.19s
Test loss: 0.6445 score: 0.6977 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.44s
Val loss: 0.6562 score: 0.6591 time: 0.18s
Test loss: 0.6396 score: 0.6977 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.6363;  Loss pred: 0.6363; Loss self: 0.0000; time: 0.34s
Val loss: 0.6523 score: 0.6591 time: 0.19s
Test loss: 0.6342 score: 0.6977 time: 0.29s
Epoch 62/1000, LR 0.000268
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.37s
Val loss: 0.6481 score: 0.6818 time: 0.18s
Test loss: 0.6283 score: 0.6977 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.6178;  Loss pred: 0.6178; Loss self: 0.0000; time: 0.46s
Val loss: 0.6436 score: 0.6818 time: 0.19s
Test loss: 0.6222 score: 0.6977 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.6162;  Loss pred: 0.6162; Loss self: 0.0000; time: 0.39s
Val loss: 0.6389 score: 0.6818 time: 0.17s
Test loss: 0.6156 score: 0.6977 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.32s
Val loss: 0.6339 score: 0.6818 time: 0.19s
Test loss: 0.6088 score: 0.6977 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.6003;  Loss pred: 0.6003; Loss self: 0.0000; time: 0.37s
Val loss: 0.6284 score: 0.7045 time: 0.18s
Test loss: 0.6014 score: 0.6977 time: 0.22s
Epoch 67/1000, LR 0.000268
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 0.35s
Val loss: 0.6225 score: 0.7273 time: 0.18s
Test loss: 0.5936 score: 0.6977 time: 0.24s
Epoch 68/1000, LR 0.000268
Train loss: 0.5833;  Loss pred: 0.5833; Loss self: 0.0000; time: 0.31s
Val loss: 0.6165 score: 0.7273 time: 0.20s
Test loss: 0.5857 score: 0.6977 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.5759;  Loss pred: 0.5759; Loss self: 0.0000; time: 0.39s
Val loss: 0.6100 score: 0.7727 time: 0.17s
Test loss: 0.5773 score: 0.7209 time: 0.33s
Epoch 70/1000, LR 0.000268
Train loss: 0.5688;  Loss pred: 0.5688; Loss self: 0.0000; time: 0.32s
Val loss: 0.6030 score: 0.7500 time: 0.20s
Test loss: 0.5685 score: 0.7209 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.5599;  Loss pred: 0.5599; Loss self: 0.0000; time: 0.36s
Val loss: 0.5952 score: 0.7500 time: 0.17s
Test loss: 0.5589 score: 0.7674 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.34s
Val loss: 0.5872 score: 0.7727 time: 0.19s
Test loss: 0.5490 score: 0.7674 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.45s
Val loss: 0.5787 score: 0.7727 time: 0.16s
Test loss: 0.5388 score: 0.7674 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.5231;  Loss pred: 0.5231; Loss self: 0.0000; time: 0.36s
Val loss: 0.5701 score: 0.7727 time: 0.19s
Test loss: 0.5284 score: 0.7674 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.5256;  Loss pred: 0.5256; Loss self: 0.0000; time: 0.36s
Val loss: 0.5613 score: 0.7727 time: 0.19s
Test loss: 0.5177 score: 0.7907 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.5042;  Loss pred: 0.5042; Loss self: 0.0000; time: 0.36s
Val loss: 0.5517 score: 0.7955 time: 0.17s
Test loss: 0.5058 score: 0.7907 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.4875;  Loss pred: 0.4875; Loss self: 0.0000; time: 0.32s
Val loss: 0.5419 score: 0.8182 time: 0.19s
Test loss: 0.4939 score: 0.7907 time: 0.33s
Epoch 78/1000, LR 0.000267
Train loss: 0.4665;  Loss pred: 0.4665; Loss self: 0.0000; time: 0.34s
Val loss: 0.5323 score: 0.8182 time: 0.17s
Test loss: 0.4825 score: 0.8140 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.4650;  Loss pred: 0.4650; Loss self: 0.0000; time: 0.37s
Val loss: 0.5230 score: 0.8182 time: 0.19s
Test loss: 0.4718 score: 0.7907 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.4700;  Loss pred: 0.4700; Loss self: 0.0000; time: 0.39s
Val loss: 0.5137 score: 0.8182 time: 0.26s
Test loss: 0.4614 score: 0.7907 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.4312;  Loss pred: 0.4312; Loss self: 0.0000; time: 0.33s
Val loss: 0.5046 score: 0.8182 time: 0.19s
Test loss: 0.4518 score: 0.7907 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.4219;  Loss pred: 0.4219; Loss self: 0.0000; time: 0.38s
Val loss: 0.4952 score: 0.8182 time: 0.18s
Test loss: 0.4419 score: 0.7907 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.4124;  Loss pred: 0.4124; Loss self: 0.0000; time: 0.35s
Val loss: 0.4855 score: 0.7955 time: 0.19s
Test loss: 0.4317 score: 0.8140 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.4060;  Loss pred: 0.4060; Loss self: 0.0000; time: 0.35s
Val loss: 0.4751 score: 0.7955 time: 0.19s
Test loss: 0.4208 score: 0.8140 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.3801;  Loss pred: 0.3801; Loss self: 0.0000; time: 0.45s
Val loss: 0.4651 score: 0.8182 time: 0.23s
Test loss: 0.4106 score: 0.8140 time: 0.26s
Epoch 86/1000, LR 0.000266
Train loss: 0.3677;  Loss pred: 0.3677; Loss self: 0.0000; time: 0.37s
Val loss: 0.4549 score: 0.8409 time: 0.19s
Test loss: 0.4005 score: 0.7907 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.3434;  Loss pred: 0.3434; Loss self: 0.0000; time: 0.37s
Val loss: 0.4439 score: 0.8636 time: 0.19s
Test loss: 0.3889 score: 0.7907 time: 0.25s
Epoch 88/1000, LR 0.000266
Train loss: 0.3228;  Loss pred: 0.3228; Loss self: 0.0000; time: 0.33s
Val loss: 0.4329 score: 0.8636 time: 0.19s
Test loss: 0.3774 score: 0.8140 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.3169;  Loss pred: 0.3169; Loss self: 0.0000; time: 0.38s
Val loss: 0.4227 score: 0.8636 time: 0.17s
Test loss: 0.3672 score: 0.7907 time: 0.24s
Epoch 90/1000, LR 0.000266
Train loss: 0.3098;  Loss pred: 0.3098; Loss self: 0.0000; time: 0.35s
Val loss: 0.4136 score: 0.8864 time: 0.20s
Test loss: 0.3599 score: 0.7907 time: 0.22s
Epoch 91/1000, LR 0.000266
Train loss: 0.2948;  Loss pred: 0.2948; Loss self: 0.0000; time: 0.37s
Val loss: 0.4048 score: 0.8864 time: 0.18s
Test loss: 0.3536 score: 0.8140 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.2969;  Loss pred: 0.2969; Loss self: 0.0000; time: 0.38s
Val loss: 0.3946 score: 0.8864 time: 0.28s
Test loss: 0.3439 score: 0.8140 time: 0.26s
Epoch 93/1000, LR 0.000265
Train loss: 0.3065;  Loss pred: 0.3065; Loss self: 0.0000; time: 0.35s
Val loss: 0.3834 score: 0.9091 time: 0.17s
Test loss: 0.3308 score: 0.8140 time: 0.24s
Epoch 94/1000, LR 0.000265
Train loss: 0.2752;  Loss pred: 0.2752; Loss self: 0.0000; time: 0.33s
Val loss: 0.3740 score: 0.9318 time: 0.19s
Test loss: 0.3217 score: 0.8140 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.2543;  Loss pred: 0.2543; Loss self: 0.0000; time: 0.39s
Val loss: 0.3649 score: 0.9091 time: 0.18s
Test loss: 0.3131 score: 0.8372 time: 0.22s
Epoch 96/1000, LR 0.000265
Train loss: 0.2311;  Loss pred: 0.2311; Loss self: 0.0000; time: 0.35s
Val loss: 0.3564 score: 0.9091 time: 0.18s
Test loss: 0.3060 score: 0.8605 time: 0.24s
Epoch 97/1000, LR 0.000265
Train loss: 0.2373;  Loss pred: 0.2373; Loss self: 0.0000; time: 0.30s
Val loss: 0.3480 score: 0.9318 time: 0.19s
Test loss: 0.2980 score: 0.8837 time: 0.30s
Epoch 98/1000, LR 0.000265
Train loss: 0.2088;  Loss pred: 0.2088; Loss self: 0.0000; time: 0.35s
Val loss: 0.3401 score: 0.9318 time: 0.17s
Test loss: 0.2918 score: 0.8837 time: 0.24s
Epoch 99/1000, LR 0.000265
Train loss: 0.2018;  Loss pred: 0.2018; Loss self: 0.0000; time: 0.38s
Val loss: 0.3328 score: 0.9091 time: 0.19s
Test loss: 0.2885 score: 0.8837 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.2170;  Loss pred: 0.2170; Loss self: 0.0000; time: 0.43s
Val loss: 0.3261 score: 0.9091 time: 0.17s
Test loss: 0.2879 score: 0.8605 time: 0.24s
Epoch 101/1000, LR 0.000265
Train loss: 0.1905;  Loss pred: 0.1905; Loss self: 0.0000; time: 0.34s
Val loss: 0.3199 score: 0.9545 time: 0.18s
Test loss: 0.2886 score: 0.8605 time: 0.23s
Epoch 102/1000, LR 0.000264
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 0.38s
Val loss: 0.3149 score: 0.9545 time: 0.18s
Test loss: 0.2925 score: 0.8605 time: 0.30s
Epoch 103/1000, LR 0.000264
Train loss: 0.2010;  Loss pred: 0.2010; Loss self: 0.0000; time: 0.36s
Val loss: 0.3105 score: 0.9318 time: 0.20s
Test loss: 0.2964 score: 0.8605 time: 0.23s
Epoch 104/1000, LR 0.000264
Train loss: 0.2135;  Loss pred: 0.2135; Loss self: 0.0000; time: 0.44s
Val loss: 0.3082 score: 0.9091 time: 0.17s
Test loss: 0.3062 score: 0.8372 time: 0.22s
Epoch 105/1000, LR 0.000264
Train loss: 0.1733;  Loss pred: 0.1733; Loss self: 0.0000; time: 0.34s
Val loss: 0.3039 score: 0.9091 time: 0.18s
Test loss: 0.3092 score: 0.8140 time: 0.24s
Epoch 106/1000, LR 0.000264
Train loss: 0.1483;  Loss pred: 0.1483; Loss self: 0.0000; time: 0.32s
Val loss: 0.3019 score: 0.8864 time: 0.18s
Test loss: 0.3188 score: 0.8140 time: 0.23s
Epoch 107/1000, LR 0.000264
Train loss: 0.1537;  Loss pred: 0.1537; Loss self: 0.0000; time: 0.32s
Val loss: 0.2983 score: 0.8864 time: 0.18s
Test loss: 0.3232 score: 0.8140 time: 0.24s
Epoch 108/1000, LR 0.000264
Train loss: 0.1279;  Loss pred: 0.1279; Loss self: 0.0000; time: 0.36s
Val loss: 0.2926 score: 0.8864 time: 0.17s
Test loss: 0.3204 score: 0.8140 time: 0.23s
Epoch 109/1000, LR 0.000264
Train loss: 0.1545;  Loss pred: 0.1545; Loss self: 0.0000; time: 0.35s
Val loss: 0.2850 score: 0.9091 time: 0.19s
Test loss: 0.3084 score: 0.8140 time: 0.23s
Epoch 110/1000, LR 0.000263
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 0.50s
Val loss: 0.2802 score: 0.8864 time: 0.17s
Test loss: 0.3027 score: 0.8372 time: 0.32s
Epoch 111/1000, LR 0.000263
Train loss: 0.1265;  Loss pred: 0.1265; Loss self: 0.0000; time: 0.31s
Val loss: 0.2764 score: 0.8864 time: 0.18s
Test loss: 0.2987 score: 0.8372 time: 0.23s
Epoch 112/1000, LR 0.000263
Train loss: 0.1500;  Loss pred: 0.1500; Loss self: 0.0000; time: 0.36s
Val loss: 0.2734 score: 0.9091 time: 0.18s
Test loss: 0.2907 score: 0.8372 time: 0.22s
Epoch 113/1000, LR 0.000263
Train loss: 0.1010;  Loss pred: 0.1010; Loss self: 0.0000; time: 0.36s
Val loss: 0.2720 score: 0.8636 time: 0.17s
Test loss: 0.2846 score: 0.8605 time: 0.25s
Epoch 114/1000, LR 0.000263
Train loss: 0.1174;  Loss pred: 0.1174; Loss self: 0.0000; time: 0.35s
Val loss: 0.2703 score: 0.8636 time: 0.20s
Test loss: 0.2881 score: 0.8372 time: 0.23s
Epoch 115/1000, LR 0.000263
Train loss: 0.1300;  Loss pred: 0.1300; Loss self: 0.0000; time: 0.37s
Val loss: 0.2674 score: 0.8864 time: 0.16s
Test loss: 0.3021 score: 0.8372 time: 0.23s
Epoch 116/1000, LR 0.000263
Train loss: 0.1214;  Loss pred: 0.1214; Loss self: 0.0000; time: 0.34s
Val loss: 0.2654 score: 0.8864 time: 0.18s
Test loss: 0.3131 score: 0.8372 time: 0.23s
Epoch 117/1000, LR 0.000262
Train loss: 0.1366;  Loss pred: 0.1366; Loss self: 0.0000; time: 0.36s
Val loss: 0.2642 score: 0.8636 time: 0.19s
Test loss: 0.3304 score: 0.8140 time: 0.21s
Epoch 118/1000, LR 0.000262
Train loss: 0.0853;  Loss pred: 0.0853; Loss self: 0.0000; time: 0.37s
Val loss: 0.2654 score: 0.8636 time: 0.16s
Test loss: 0.3578 score: 0.8140 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 119/1000, LR 0.000262
Train loss: 0.1021;  Loss pred: 0.1021; Loss self: 0.0000; time: 0.44s
Val loss: 0.2711 score: 0.8864 time: 0.19s
Test loss: 0.3925 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 116,   Train_Loss: 0.1366,   Val_Loss: 0.2642,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.2642,   Test_Precision: 0.7826,   Test_Recall: 0.8571,   Test_accuracy: 0.8182,   Test_Score: 0.8140,   Test_loss: 0.3304


[0.22862422303296626, 0.19526325596962124, 0.19915195391513407, 0.30758272006642073, 0.2046249359846115, 0.22750074591021985, 0.2007625149562955, 0.2203284880379215, 0.19578433607239276, 0.2101050189230591, 0.21510635898448527, 0.19556984794326127, 0.32747843698598444, 0.20391071308404207, 0.226819853996858, 0.21493619889952242, 0.22282760695088655, 0.21068638702854514, 0.21242628595791757, 0.2928501630667597, 0.2180708929663524, 0.20697432407177985, 0.214301300002262, 0.2104814969934523, 0.19704321993049234, 0.21822482894640416, 0.29869245109148324, 0.23347408999688923, 0.22160861699376255, 0.23289073805790395, 0.1964448969811201, 0.22658289596438408, 0.28171762300189584, 0.2252649200381711, 0.1964615600882098, 0.22767848195508122, 0.2055820020614192, 0.2180394169408828, 0.21118808607570827, 0.22241458005737513, 0.23819970595650375, 0.21591165999416262, 0.22540965501684695, 0.2084304599557072, 0.209498459007591, 0.19568025798071176, 0.2163479820592329, 0.20383141306228936, 0.20763899700250477, 0.22314589901361614, 0.2107539540156722, 0.22915560205001384, 0.1899319989606738, 0.219640635070391, 0.21921175997704268, 0.22582414897624403, 0.216970507055521, 0.22763559804297984, 0.20915250305552036, 0.19876326399389654, 0.22564117598813027, 0.21324046002700925, 0.2157518289750442, 0.22162577998824418, 0.22342550801113248, 0.20909748098347336, 0.2191111519932747, 0.22647733893245459, 0.21683869196567684, 0.3225954909576103, 0.2013804349116981, 0.22770464199129492, 0.19620646396651864, 0.19814019103068858, 0.22123636992182583, 0.21419390907976776, 0.21909486898221076, 0.20150529802776873, 0.22675039898604155, 0.20555364806205034, 0.21569098893087357, 0.2287145929876715, 0.19961069396231323, 0.22732917999383062, 0.28464854101184756, 0.22242675197776407, 0.19885018095374107, 0.22072648000903428, 0.21053687203675508, 0.20766397891566157, 0.22246998001355678, 0.20350249798502773, 0.22960752493236214, 0.1954265219392255, 0.21766748605296016, 0.2082507919985801, 0.20504000899381936, 0.21862693107686937, 0.22070753399748355, 0.18724638095591217, 0.20385796995833516, 0.23046769108623266, 0.212879495928064, 0.21536330599337816, 0.1974936000769958, 0.22138772602193058, 0.21107104001566768, 0.21582338004373014, 0.2044291190104559, 0.21315676590893418, 0.20306242897640914, 0.22490200505126268, 0.20240289298817515, 0.22608136804774404, 0.21122231695335358, 0.2146204230375588, 0.194832026027143, 0.21652250993065536, 0.20591698109637946, 0.20692912605591118, 0.20584141893777996, 0.22287536307703704, 0.21428271103650331, 0.2041442859917879, 0.22214753099251539, 0.19967138196807355, 0.2215029839426279, 0.19550216500647366, 0.22133302805013955, 0.1913884220412001, 0.2158114470075816, 0.2046468899352476, 0.21293877402786165, 0.20444612798746675, 0.20017879910301417, 0.21936052001547068, 0.20531710505019873, 0.22362931398674846, 0.19352318299934268, 0.2083052200032398, 0.19822376198135316, 0.22650872403755784, 0.19485514401458204, 0.21612016099970788, 0.20231651095673442, 0.19362455199006945, 0.2989163240417838, 0.2324890160234645, 0.20972965494729578, 0.22185824892949313, 0.21506992599461228, 0.21776679100003093, 0.18451940291561186, 0.21115881099831313, 0.20860134903341532, 0.19604350498411804, 0.19805897097103298, 0.20000635692849755, 0.20880646002478898, 0.21116133907344192, 0.22682516009081155, 0.19792011694516987, 0.2190224410733208, 0.21372392610646784, 0.20562209805939347, 0.19706184708047658, 0.21246954100206494, 0.19306890794541687, 0.20643059501890093, 0.21310932899359614, 0.20441412401851267, 0.31152665603440255, 0.2008538480149582, 0.2348405959783122, 0.21419736102689058, 0.24123695597518235, 0.1891133349854499, 0.22430676803924143, 0.20575573202222586, 0.26312991802114993, 0.2112658160040155, 0.21645028295461088, 0.23444714292418212, 0.23378657293505967, 0.20029954006895423, 0.21242467896081507, 0.18973220197949558, 0.22102462698239833, 0.19578493502922356, 0.21771703998092562, 0.19553878298029304, 0.21520683204289526, 0.2476281519047916, 0.22519341809675097, 0.24120509799104184, 0.23273700498975813, 0.3166472839657217, 0.23571759497281164, 0.22409558901563287, 0.2503088789526373, 0.22214518603868783, 0.246047563967295, 0.24451458198018372, 0.23870449198875576, 0.35444741090759635, 0.23827127402182668, 0.23428868898190558, 0.22477528802119195, 0.24562652106396854, 0.21506668801885098, 0.2514822989469394, 0.22924177290406078, 0.24938309798017144, 0.23165524599608034, 0.2383407719898969, 0.24008128501009196, 0.2519876549486071, 0.23997778596822172, 0.23215664306189865, 0.23799827706534415, 0.23460731795057654, 0.22327323199715465, 0.3274626809870824, 0.23666632501408458, 0.24467903398908675, 0.2385591350030154, 0.25284737593028694, 0.2321593320230022, 0.23228604905307293, 0.26379750901833177, 0.3027445339830592, 0.2465939320391044, 0.2349716239841655, 0.2365966138895601, 0.23417990701273084, 0.23665983695536852, 0.24764542595949024, 0.23793481290340424, 0.2365907320054248, 0.2330608230549842, 0.238493240904063, 0.22019808203913271, 0.24314770998898894, 0.2849992469418794, 0.2501188430469483, 0.2817605669843033, 0.22249252896290272, 0.24337138200644404, 0.22098724101670086, 0.3159685790305957, 0.22957240603864193, 0.2397145900176838, 0.29008101497311145, 0.23976051504723728, 0.23398244101554155, 0.2588240639306605, 0.22813103802036494, 0.22607109299860895, 0.24256354698445648, 0.2538806829834357, 0.3336913560051471, 0.251913822023198, 0.24662517406977713, 0.2356907119974494, 0.23734758596401662, 0.24668793799355626, 0.22414913098327816, 0.24974141200073063, 0.33311175694689155, 0.24320202798116952, 0.22450160898733884, 0.24405622901394963, 0.23276557202916592, 0.23732711700722575, 0.24397244199644774, 0.2215908960206434, 0.26328912610188127, 0.22354692302178591, 0.2575955209322274, 0.235720414086245, 0.2494150639977306, 0.2284212619997561, 0.23944847797974944, 0.2597909360192716, 0.24204697902314365, 0.2315649010706693, 0.22403526597190648, 0.24190229002851993, 0.30514516599942, 0.24373123398981988, 0.221553361043334, 0.23978593293577433, 0.23786980309523642, 0.3050624430179596, 0.23103806900326163, 0.2209472559625283, 0.24285798298660666, 0.23390296497382224, 0.2487350390292704, 0.2379297378938645, 0.239309499040246, 0.3229912269162014, 0.235452747088857, 0.2223172039957717, 0.2561833920190111, 0.23595439305063337, 0.23888890002854168, 0.2360651909839362, 0.21732917905319482, 0.2407731640851125, 0.23884328198619187]
[0.005196005068931051, 0.004437801272036846, 0.004526180770798502, 0.006990516365145926, 0.004650566726922989, 0.005170471497959542, 0.004562784430824898, 0.0050074656372254885, 0.00444964400164529, 0.0047751140664331615, 0.004888780886011029, 0.004444769271437756, 0.007442691749681465, 0.0046343343882736835, 0.005154996681746773, 0.004884913611352782, 0.005064263794338331, 0.004788326977921481, 0.004827870135407217, 0.006655685524244539, 0.004956156658326191, 0.004703961910722269, 0.0048704840909605, 0.004783670386214825, 0.004478254998420281, 0.004959655203327367, 0.00678846479753371, 0.005306229318111119, 0.005036559477130967, 0.005292971319497817, 0.004464656749570911, 0.00514961127191782, 0.006402673250043087, 0.005119657273594798, 0.004465035456550223, 0.005174510953524573, 0.004672318228668618, 0.004955441294110973, 0.004799729228993369, 0.0050548768194857985, 0.005413629680829631, 0.004907083181685514, 0.00512294670492834, 0.004737055908084254, 0.004761328613808887, 0.004447278590470722, 0.004916999592255293, 0.004632532115052031, 0.00471906811369329, 0.005071497704854913, 0.004789862591265278, 0.005208081864773042, 0.004316636340015314, 0.004991832615236159, 0.004982085454023697, 0.0051323670221873645, 0.004931147887625478, 0.005173536319158633, 0.004753465978534554, 0.004517346908952194, 0.005128208545184779, 0.004846374091522937, 0.004903450658523731, 0.005036949545187367, 0.005077852454798465, 0.004752215476897122, 0.004979798908938061, 0.005147212248464877, 0.00492815209012902, 0.007331715703582053, 0.004576828066174957, 0.005175105499802157, 0.004459237817420878, 0.004503186159788377, 0.005028099316405132, 0.00486804338817654, 0.00497942884050479, 0.004579665864267471, 0.005153418158773671, 0.004671673819592053, 0.004902067930247126, 0.005198058931537988, 0.004536606680961664, 0.005166572272587059, 0.006469285022996535, 0.005055153454040093, 0.004519322294403206, 0.005016510909296234, 0.004784928909926252, 0.004719635884446854, 0.005056135909399018, 0.0046250567723869936, 0.005218352839371867, 0.0044415118622551245, 0.0049469883193854584, 0.004732972545422275, 0.004660000204404985, 0.0049687938881106675, 0.0050160803181246265, 0.004255599567179822, 0.004740883022286865, 0.005359713746191457, 0.004950685951815441, 0.00500844897659019, 0.004592874420395251, 0.0051485517679518734, 0.004908628837573667, 0.005019148373110003, 0.004754165558382695, 0.004957134090905446, 0.004722382069218817, 0.005230279187238667, 0.004707044022980817, 0.005257706233668466, 0.004912146905891944, 0.004991172628780437, 0.004530977349468442, 0.005035407207689659, 0.004788767002241383, 0.004812305257114214, 0.004787009742739069, 0.005183147978535745, 0.004983318861314031, 0.004747541534692742, 0.00516622165098873, 0.004643520510885431, 0.005151232184712277, 0.004546561976894736, 0.0051472797220962685, 0.004450893535841863, 0.005018870860641432, 0.0047592299984941305, 0.004952064512275853, 0.004754561115987599, 0.004655320909372423, 0.005101407442220249, 0.00477481639651625, 0.005200681720622057, 0.004500539139519597, 0.00484430744193581, 0.004609854929798911, 0.005267644745059485, 0.0045315149770833035, 0.00502605025580716, 0.004705035138528707, 0.004502896557908592, 0.006951542419576367, 0.005406721302871268, 0.004877433835983623, 0.005159494161151003, 0.005001626185921215, 0.005064343976744905, 0.004291148905014229, 0.004910670023216585, 0.004851194163567798, 0.0045591512787004195, 0.004606022580721697, 0.004651310626244129, 0.004855964186622999, 0.00491072881566144, 0.005275003723042129, 0.004602793417329532, 0.0050935451412400185, 0.004970323862941112, 0.0047819092571951965, 0.004582833653034339, 0.004941152116327092, 0.004489974603381788, 0.004800711512067463, 0.004956030906827817, 0.004753816837639829, 0.007244805954288431, 0.004671019721278097, 0.005461409208797958, 0.004981333977369548, 0.005610161766864706, 0.004397984534545347, 0.0052164364660288705, 0.004785017023772695, 0.00611930041909651, 0.004913158511721291, 0.0050337275105723465, 0.005452259137771677, 0.005436897045001388, 0.004658128838812889, 0.004940108813042211, 0.004412376790220828, 0.005140107604241822, 0.004553138023935432, 0.005063186976300596, 0.004547413557681234, 0.005004810047509192, 0.005758794230343991, 0.005237056234808162, 0.005609420883512601, 0.00541248848813391, 0.007363890324784226, 0.005481804534251434, 0.0052115253259449506, 0.005821136719828775, 0.005166167117178787, 0.005722036371332442, 0.005686385627446133, 0.00555126725555246, 0.008242963044362705, 0.005541192419112249, 0.005448574162369897, 0.0052273322795626034, 0.005712244675906245, 0.005001550884159325, 0.005848425556905568, 0.005331204021024669, 0.005799606929771429, 0.005387331302234426, 0.005542808650927835, 0.005583285697909115, 0.00586017802206063, 0.005580878743447017, 0.005398991699113922, 0.005534843652682422, 0.005455984138385501, 0.005192400744119876, 0.007615411185746102, 0.005503868023583362, 0.005690210092769459, 0.005547886860535242, 0.005880171533262487, 0.005399054233093075, 0.005402001140769138, 0.0061348257911239944, 0.007040570557745564, 0.005734742605560567, 0.005464456371724779, 0.0055022468346409325, 0.005446044349133275, 0.005503717138496942, 0.005759195952546285, 0.005533367741939633, 0.005502110046637786, 0.005420019140813586, 0.005546354439629372, 0.00512088562881704, 0.005654597906720673, 0.006627889463764637, 0.005816717280161588, 0.006552571325216356, 0.005174244859602389, 0.0056597995815452105, 0.00513923816317909, 0.007348106489083622, 0.005338893163689348, 0.005574757907387995, 0.006746070115653755, 0.0055758259313310994, 0.005441452116640501, 0.006019164277457221, 0.005305372977217789, 0.005257467279037417, 0.005641012720568756, 0.005904201929847342, 0.007760264093142955, 0.005858460977283674, 0.005735469164413421, 0.005481179348777893, 0.0055197113014887585, 0.00573692879054782, 0.005212770487983213, 0.00580793981397048, 0.007746785045276547, 0.005655861115841152, 0.005220967650868345, 0.005675726256138364, 0.00541315283788758, 0.005519235279237808, 0.005673777720847622, 0.00515327665164287, 0.00612300293260189, 0.00519876565166944, 0.005990593510051799, 0.005481870095028953, 0.005800350325528619, 0.005312122372087351, 0.00556856925534301, 0.006041649674866782, 0.0056289995121661315, 0.005385230257457426, 0.005210122464462941, 0.0056256346518260445, 0.007096399209288837, 0.0056681682323213925, 0.0051524037451938145, 0.005576417045018008, 0.005531855885935731, 0.007094475419022316, 0.005372978348913061, 0.005138308278198333, 0.005647860069455969, 0.0054396038366005175, 0.005784535791378382, 0.005533249718461966, 0.005565337186982465, 0.0075114238817721255, 0.005475645281136209, 0.0051701675347853885, 0.0059577533027676995, 0.0054873114662937995, 0.005555555814617249, 0.005489888162417121, 0.005054166954725461, 0.005599375908956105, 0.005554494929911439]
[192.45554743188984, 225.33681404373107, 220.9368230389043, 143.05094899511406, 215.02755657946278, 193.40595928139953, 219.16441926212408, 199.70181973212203, 224.73708000690445, 209.41908111254062, 204.5499733607296, 224.98355683522996, 134.35999146986015, 215.78071762156677, 193.98654581891012, 204.71191090789205, 197.46206765886978, 208.84121001153528, 207.13067500844298, 150.24748335198817, 201.76924761246815, 212.58675537329236, 205.318399839551, 209.044503334033, 223.30126362897008, 201.62691941349334, 147.30871114825638, 188.4577427867317, 198.54823606086777, 188.9297975819898, 223.9813844807012, 194.18941492793098, 156.18476235582855, 195.32557485002195, 223.9623872489067, 193.25497790643553, 214.0265176854084, 201.79837488709555, 208.3450862101499, 197.8287573982315, 184.7189517859211, 203.78704883835167, 195.2001567843732, 211.10158279816818, 210.02541120555782, 224.85661279298338, 203.37605916727918, 215.86466648569976, 211.90624417950363, 197.18041063938693, 208.77425624350585, 192.0092705846086, 231.66185919577657, 200.3272299130749, 200.71915851872168, 194.841872312906, 202.79253893590595, 193.29138490761173, 210.3728110216306, 221.36887428730853, 194.99987006943536, 206.33982872868927, 203.93801623387137, 198.5328602220099, 196.9336464384704, 210.4281686850052, 200.81132155861476, 194.2799231366928, 202.9158154438817, 136.3937228923689, 218.4919305556831, 193.23277564838625, 224.25356999200758, 222.06499232245687, 198.88230861656, 205.42134082633507, 200.82624574641474, 218.35654164257517, 194.04596506446978, 214.0560404294929, 203.99554111229693, 192.37950419006168, 220.42907184275037, 193.55192325593262, 154.5765871259767, 197.81793156067246, 221.27211445804926, 199.3417373311942, 208.98952081096073, 211.88075192313295, 197.77949365266608, 216.21356217080543, 191.63135011782185, 225.14855999782512, 202.14319004582276, 211.28371026939482, 214.59226526529423, 201.25608397498647, 199.3588492565989, 234.98451492293438, 210.9311694254015, 186.57712843537342, 201.99221072249492, 199.66261105465261, 217.7285744106939, 194.22937654520393, 203.72287925813097, 199.23698716648465, 210.34185446839737, 201.72946336768246, 211.75753789981277, 191.1943825943164, 212.44755628326004, 190.19700902959516, 203.57697340047707, 200.35371933115124, 220.702935122224, 198.59367053232205, 208.8220202678372, 207.80061666322229, 208.89867657294806, 192.93294425340775, 200.66947908212202, 210.6353346658439, 193.56505925536828, 215.35384578484806, 194.1283103036551, 219.94641337386795, 194.27737639887627, 224.67398780655293, 199.24800373767656, 210.1180233601676, 201.93597993747127, 210.32435499407475, 214.80796264479406, 196.0243347206114, 209.43213664291034, 192.28248405103116, 222.19560123784268, 206.42785619741653, 216.92656606953608, 189.83816266992918, 220.6767505033487, 198.96339055595152, 212.53826391458279, 222.07927433812924, 143.8529666716672, 184.9549743703166, 205.02584630106662, 193.81744968908262, 199.93497371211814, 197.45894129465265, 233.03782323458756, 203.6381991199198, 206.13481264261594, 219.3390696798832, 217.1070554854541, 214.99316651906489, 205.93232601565654, 203.6357611136601, 189.57332591668646, 217.25937041514766, 196.3269142160878, 201.19413293287198, 209.1214923192718, 218.2056072093933, 202.3819498889118, 222.71840897425426, 208.30245631013605, 201.77436719014838, 210.3572842946299, 138.02992189294844, 214.08601540358669, 183.10292486215246, 200.74943871321426, 178.24797956919875, 227.37687960136896, 191.70175013389408, 208.98567236685835, 163.4173731492735, 203.53505746136756, 198.6599389616737, 183.41021120443244, 183.92844148472278, 214.678475972521, 202.4246910027436, 226.635223495941, 194.54845637370707, 219.62874719437258, 197.50406308926148, 219.90522465476153, 199.80778301419906, 173.64746160417454, 190.94696622760836, 178.27152227768, 184.75789873592413, 135.79778566695336, 182.42168135543614, 191.88240245550003, 171.78775351447413, 193.56710251876135, 174.76295764389533, 175.85863244542554, 180.13904825061076, 121.31559908956424, 180.46657187916415, 183.5342550545449, 191.3021683947122, 175.06252913463487, 199.93798386959384, 170.98618940600917, 187.57488853480376, 172.42547850383568, 185.62066149249895, 180.41394949338647, 179.10600569383186, 170.6432801589818, 179.18325159352096, 185.21976986260586, 180.67357684355858, 183.28499032182182, 192.5891411853078, 131.3126731583079, 181.69040313378326, 175.74043553694062, 180.2488091661486, 170.0630660760965, 185.21762457405583, 185.1165843807319, 163.0038136448508, 142.03394338543586, 174.37574251900546, 183.0008205709883, 181.74393662316646, 183.61951095002513, 181.6953841986688, 173.6353491424224, 180.72176776189937, 181.74845496067044, 184.50119344964017, 180.2986107153339, 195.27872178449863, 176.84723414399946, 150.8775916477039, 171.91827483357073, 152.6118450861703, 193.2649163566728, 176.68470156799881, 194.58136950427073, 136.08948121337167, 187.3047407655874, 179.3799868286193, 148.23445099978642, 179.34562741295497, 183.7744739022697, 166.13602053447315, 188.48816177376727, 190.20565358289565, 177.27313330702344, 169.3709009078312, 128.86159388359084, 170.69329366151356, 174.35365291555397, 182.44248844420028, 181.16889550551008, 174.3092927434628, 191.83656796424458, 172.17809275409317, 129.08580710003443, 176.80773617286354, 191.53537559913462, 176.1889060309222, 184.73522362066512, 181.18452093567885, 176.24941427042847, 194.05129349715756, 163.31855643503067, 192.35335212289817, 166.9283683364711, 182.41949967162043, 172.40337977497322, 188.24867537964096, 179.57934150509232, 165.51770688724184, 177.6514632553563, 185.69308129679465, 191.9340681185082, 177.757721908835, 140.91653675444996, 176.42383906280972, 194.08416914780884, 179.32661634290855, 180.7711590141772, 140.95474872161995, 186.1165139819142, 194.61658309661297, 177.0582110219181, 183.8369171797905, 172.87471908989826, 180.72562253307487, 179.6836321686021, 133.13055097671787, 182.6268775015495, 193.41732995534537, 167.84850751296563, 182.23860740229, 179.99999160640155, 182.15307314379135, 197.856542721652, 178.59133165189309, 180.03437083269498]
Elapsed: 0.2278884798909726~0.02818516894600807
Time per graph: 0.005262348546338847~0.0006642907077990341
Speed: 192.641220170709~21.047000732370293
Total Time: 0.2397
best val loss: 0.2641826868057251 test_score: 0.8140

Testing...
Test loss: 0.2886 score: 0.8605 time: 0.24s
test Score 0.8605
Epoch Time List: [0.7898838280234486, 0.7205882560228929, 0.7656336849322543, 0.8824030329706147, 0.767019784078002, 0.7941134249558672, 0.7363117591012269, 0.7735699759796262, 0.7501403699861839, 0.8489646909292787, 0.7500926699722186, 0.7205987370107323, 0.9068748289719224, 0.7825824060710147, 0.7867603519698605, 0.738455738988705, 0.7907570028910413, 0.7740578721277416, 0.879472263972275, 0.8421672009862959, 0.7816656631184742, 0.7421637490624562, 0.8206897580530494, 0.7504172939807177, 0.7058746269904077, 0.8397899919655174, 0.8443760080263019, 0.9241102138767019, 0.7774654519744217, 0.8169060529908165, 0.7163150478154421, 0.9701322691980749, 0.87105261196848, 0.7901832379866391, 0.8341476010391489, 0.787848562002182, 0.746992215863429, 0.8230913488660008, 0.7525776319671422, 0.7664412459125742, 0.8479503970593214, 0.7663035730365664, 0.8464073849609122, 0.7496550299692899, 0.8610875440062955, 0.8577079308452085, 0.8797863899962977, 0.8086332740494981, 0.8047918088268489, 0.7841789000667632, 0.7380566911306232, 0.8432616080390289, 0.730171972187236, 0.8067083200439811, 0.9519881979795173, 0.842390819103457, 0.7609388530254364, 0.7945975009351969, 0.7457539071328938, 0.7702745769638568, 0.7816245548892766, 0.7314273479860276, 0.914555991999805, 0.7473525707609951, 0.7931130080251023, 0.7577285250881687, 0.8053644499741495, 0.7665772911859676, 0.7756491530453786, 0.8698272091569379, 0.7512433810625225, 0.7833422098774463, 0.723176580038853, 0.7632837810087949, 0.7925888227764517, 0.7449938040226698, 0.79454866994638, 0.8495713201118633, 0.794006644980982, 0.7371998059097677, 0.7799072340130806, 0.8372029260499403, 0.7397278650896624, 0.7797489340882748, 0.8212045159889385, 0.7852598280878738, 0.7329141719965264, 0.8724282749462873, 0.759778494015336, 0.8552589541068301, 0.7765551779884845, 0.7444196350406855, 0.7775634209392592, 0.7136641340330243, 0.925817362847738, 0.7518663968658075, 1.0032506260322407, 0.7411291749449447, 0.7877623869571835, 0.7380407440941781, 0.790858761058189, 0.8244695380562916, 0.7412062980001792, 0.8128272511530668, 0.7360187250887975, 0.9722515800967813, 0.7717658679466695, 0.8219221140025184, 0.7595205809921026, 0.8887388819130138, 0.763341317884624, 0.9089841729728505, 0.778868104913272, 0.9768269408959895, 0.7633105319691822, 0.7637291537830606, 0.7654665720183402, 0.871524426038377, 0.7644346699817106, 0.8751771348761395, 0.7794440829893574, 0.9824308040551841, 0.7749240680132061, 0.8076414410024881, 0.7765992438653484, 0.7265207460150123, 0.9152284690644592, 0.7368711670860648, 0.9165198688860983, 0.8184085190296173, 0.7709732229122892, 0.7526593011571094, 0.8827223090920597, 0.7660090330755338, 0.7736875719856471, 0.7704690741375089, 0.7273238018387929, 0.8626846310216933, 0.8130612840177491, 0.9257160859415308, 0.7740252289222553, 0.8424675349378958, 0.722651582909748, 0.7703924410743639, 0.7507457099854946, 0.7554256760049611, 1.0232365739066154, 0.7947751950705424, 0.7595848710043356, 0.806355073931627, 0.8189795840298757, 0.7929326019948348, 0.7835985409328714, 0.8034741898300126, 0.8169705619802698, 0.7718268701573834, 0.9302155408076942, 0.7902594100451097, 0.8745374729624018, 0.8081299408804625, 0.8097862239228562, 0.7457365141017362, 0.8319266339531168, 0.7489694872638211, 0.9828370918985456, 0.7719638941343874, 0.8576675929361954, 0.7540824931347743, 0.8958443870069459, 0.7958448349963874, 0.8850701830815524, 0.9671427299035713, 0.8235101898899302, 0.8163069180445746, 0.7709170370362699, 0.8542781281284988, 0.7602752608945593, 0.9177412149729207, 0.7784936310490593, 0.8387259918963537, 0.8129754749825224, 0.8527422229526564, 0.9936605229740962, 0.9759925769176334, 0.7786119010997936, 0.8385634600417688, 0.7384265671717003, 0.7784638578305021, 0.7799574240343645, 0.8097827930469066, 0.867069347994402, 0.7653172969585285, 0.7819904159987345, 0.7561710079899058, 0.8050940430257469, 0.7681676448555663, 0.8595840360503644, 0.7764663719572127, 0.7612541910493746, 0.7788387220352888, 0.7269482370465994, 0.813629993936047, 0.7893989781150594, 0.8021493190899491, 0.9126524118473753, 0.7632070520194247, 0.75859700597357, 0.7566882361425087, 0.7927467710105702, 0.7191069559194148, 0.8613737800624222, 0.8049011649563909, 0.8616115320473909, 0.7311951369047165, 0.7901518038706854, 0.7673191730864346, 0.7438547479687259, 0.7840614030137658, 0.7516514069866389, 0.8744084298377857, 0.7627310450188816, 0.8192414080258459, 0.8339160761097446, 0.734910445054993, 0.7789515919284895, 0.7733682469697669, 0.7964985590660945, 0.7723061291035265, 0.8114640680141747, 0.7934945840388536, 0.8252280900487676, 0.7995898461667821, 0.7318917199736461, 0.7610745509155095, 0.7898703558603302, 0.8155851599294692, 0.7676690090447664, 0.8267868958646432, 0.7940000829985365, 0.8613350698724389, 0.7583226959686726, 0.7571991289732978, 0.7394726159982383, 0.8715168639319018, 0.7856335662072524, 0.8104774000821635, 0.8108478049980476, 0.8488809340633452, 0.741843250929378, 0.8490885909413919, 0.7232107080053538, 0.8618376309750602, 0.818633403046988, 0.7862288639880717, 0.873056480078958, 0.8181065479293466, 0.7312989911297336, 0.7714323430554941, 0.7721444639610127, 0.757677017012611, 0.8841893870849162, 0.7633622419089079, 0.7684581089997664, 0.7657654010690749, 0.8445140470284969, 0.790163240977563, 0.7686309989076108, 0.7798789190128446, 0.8461831649765372, 0.7559790511149913, 0.7875154591165483, 0.8900234759785235, 0.7522411219542846, 0.7878382280468941, 0.7837319930549711, 0.7637188049266115, 0.9471220339182764, 0.7800696189515293, 0.813174112001434, 0.7500905180349946, 0.7887890200363472, 0.7712651379406452, 0.7922241510823369, 0.9156878360081464, 0.7525267759338021, 0.7417147239902988, 0.7888987720943987, 0.7659960930468515, 0.794366188114509, 0.7599545960547403, 0.7937492029741406, 0.8339703378733248, 0.7572242051828653, 0.8555664729792625, 0.7852003839798272, 0.8180908398935571, 0.7606226798379794, 0.7376594400266185, 0.7507863360224292, 0.764239066047594, 0.7681019330630079, 0.9859829619526863, 0.7268656610976905, 0.7649068450555205, 0.7865602379897609, 0.7779724200954661, 0.7647240930236876, 0.7555260490626097, 0.7667818710906431, 0.7683687600074336, 0.8666534462245181]
Total Epoch List: [100, 92, 119]
Total Time List: [0.18766220496036112, 0.21585489204153419, 0.2396685240091756]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcd600>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.19s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.20s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.31s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.20s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.21s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.21s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.20s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.6926,   Val_Loss: 0.6936,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6936,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6931


[0.19736693298909813, 0.2251862499397248, 0.21400988893583417, 0.22936506301630288, 0.19939212501049042, 0.31217566691339016, 0.20219013502355665, 0.22117027698550373, 0.21759947901591659, 0.21866998297628015, 0.20702882600016892, 0.21976079302839935, 0.21193898003548384]
[0.004485612113388594, 0.005117869316811927, 0.0048638611121780495, 0.005212842341279611, 0.0045316392047838735, 0.007094901520758867, 0.0045952303414444695, 0.005026597204215994, 0.0049454427049071955, 0.004969772340370004, 0.00470520059091293, 0.004994563477918167, 0.004816795000806451]
[222.93501415675547, 195.39381295162295, 205.5979759570473, 191.83392370821772, 220.6707010002781, 140.94628333798784, 217.6169475077193, 198.94174117656829, 202.2063664811512, 201.21646053620822, 212.53079027731192, 200.2176975868209, 207.60692531705735]
Elapsed: 0.22121956922078076~0.027906192025580426
Time per graph: 0.005027717482290472~0.0006342316369450096
Speed: 201.3626646149805~19.74548084454775
Total Time: 0.2122
best val loss: 0.693645715713501 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.20s
test Score 0.5000
Epoch Time List: [0.7373733698623255, 0.7496859501115978, 0.8021889929659665, 0.7947309329174459, 0.7662587490631267, 0.8990391058614478, 0.9350886319298297, 0.885808655875735, 0.747311522020027, 0.8021732069319114, 0.8196605310076848, 0.7946640090085566, 0.7722186479950324]
Total Epoch List: [13]
Total Time List: [0.21221521904226393]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcda20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.20s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.20s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.20s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.22s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.19s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.20s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.20s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.20s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.21s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.19s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.20s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.19s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.19s
Epoch 20/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.21s
Epoch 21/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.19s
Epoch 23/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.19s
Epoch 25/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.20s
Epoch 26/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.30s
Val loss: 0.6908 score: 0.5227 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.38s
Val loss: 0.6905 score: 0.5455 time: 0.20s
Test loss: 0.6924 score: 0.5116 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.42s
Val loss: 0.6900 score: 0.5909 time: 0.24s
Test loss: 0.6922 score: 0.5116 time: 0.19s
Epoch 29/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.37s
Val loss: 0.6895 score: 0.6364 time: 0.20s
Test loss: 0.6919 score: 0.5349 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.34s
Val loss: 0.6890 score: 0.6591 time: 0.23s
Test loss: 0.6915 score: 0.5349 time: 0.19s
Epoch 31/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.38s
Val loss: 0.6883 score: 0.6591 time: 0.20s
Test loss: 0.6911 score: 0.5349 time: 0.21s
Epoch 32/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.37s
Val loss: 0.6876 score: 0.6591 time: 0.22s
Test loss: 0.6906 score: 0.5581 time: 0.29s
Epoch 33/1000, LR 0.000270
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.39s
Val loss: 0.6867 score: 0.6591 time: 0.21s
Test loss: 0.6901 score: 0.5581 time: 0.19s
Epoch 34/1000, LR 0.000270
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.37s
Val loss: 0.6858 score: 0.6591 time: 0.22s
Test loss: 0.6894 score: 0.5581 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.38s
Val loss: 0.6847 score: 0.6591 time: 0.23s
Test loss: 0.6887 score: 0.5581 time: 0.19s
Epoch 36/1000, LR 0.000270
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.35s
Val loss: 0.6835 score: 0.7045 time: 0.20s
Test loss: 0.6879 score: 0.6047 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.40s
Val loss: 0.6821 score: 0.7273 time: 0.23s
Test loss: 0.6870 score: 0.6047 time: 0.19s
Epoch 38/1000, LR 0.000270
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.35s
Val loss: 0.6806 score: 0.7500 time: 0.20s
Test loss: 0.6859 score: 0.6047 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.33s
Val loss: 0.6789 score: 0.7955 time: 0.22s
Test loss: 0.6847 score: 0.6047 time: 0.20s
Epoch 40/1000, LR 0.000269
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.40s
Val loss: 0.6770 score: 0.8182 time: 0.21s
Test loss: 0.6834 score: 0.6047 time: 0.20s
Epoch 41/1000, LR 0.000269
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 0.35s
Val loss: 0.6748 score: 0.8182 time: 0.22s
Test loss: 0.6819 score: 0.6047 time: 0.20s
Epoch 42/1000, LR 0.000269
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 0.31s
Val loss: 0.6725 score: 0.8182 time: 0.23s
Test loss: 0.6803 score: 0.6512 time: 0.20s
Epoch 43/1000, LR 0.000269
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.36s
Val loss: 0.6699 score: 0.8182 time: 0.22s
Test loss: 0.6786 score: 0.6512 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 0.33s
Val loss: 0.6671 score: 0.8182 time: 0.22s
Test loss: 0.6767 score: 0.6744 time: 0.20s
Epoch 45/1000, LR 0.000269
Train loss: 0.6573;  Loss pred: 0.6573; Loss self: 0.0000; time: 0.45s
Val loss: 0.6640 score: 0.8182 time: 0.21s
Test loss: 0.6747 score: 0.6977 time: 0.20s
Epoch 46/1000, LR 0.000269
Train loss: 0.6510;  Loss pred: 0.6510; Loss self: 0.0000; time: 0.36s
Val loss: 0.6606 score: 0.8182 time: 0.22s
Test loss: 0.6725 score: 0.7209 time: 0.20s
Epoch 47/1000, LR 0.000269
Train loss: 0.6467;  Loss pred: 0.6467; Loss self: 0.0000; time: 0.33s
Val loss: 0.6569 score: 0.8182 time: 0.22s
Test loss: 0.6702 score: 0.7209 time: 0.19s
Epoch 48/1000, LR 0.000269
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.34s
Val loss: 0.6528 score: 0.8182 time: 0.20s
Test loss: 0.6676 score: 0.7209 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6359;  Loss pred: 0.6359; Loss self: 0.0000; time: 0.32s
Val loss: 0.6484 score: 0.8182 time: 0.22s
Test loss: 0.6649 score: 0.7209 time: 0.20s
Epoch 50/1000, LR 0.000269
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.42s
Val loss: 0.6436 score: 0.8182 time: 0.20s
Test loss: 0.6619 score: 0.7209 time: 0.20s
Epoch 51/1000, LR 0.000269
Train loss: 0.6237;  Loss pred: 0.6237; Loss self: 0.0000; time: 0.34s
Val loss: 0.6385 score: 0.8182 time: 0.22s
Test loss: 0.6585 score: 0.6977 time: 0.20s
Epoch 52/1000, LR 0.000269
Train loss: 0.6175;  Loss pred: 0.6175; Loss self: 0.0000; time: 0.30s
Val loss: 0.6331 score: 0.8409 time: 0.23s
Test loss: 0.6547 score: 0.7907 time: 0.18s
Epoch 53/1000, LR 0.000269
Train loss: 0.6090;  Loss pred: 0.6090; Loss self: 0.0000; time: 0.46s
Val loss: 0.6274 score: 0.8409 time: 0.22s
Test loss: 0.6506 score: 0.7907 time: 0.36s
Epoch 54/1000, LR 0.000269
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.32s
Val loss: 0.6212 score: 0.8636 time: 0.22s
Test loss: 0.6463 score: 0.7907 time: 0.19s
Epoch 55/1000, LR 0.000269
Train loss: 0.5934;  Loss pred: 0.5934; Loss self: 0.0000; time: 0.38s
Val loss: 0.6147 score: 0.8636 time: 0.20s
Test loss: 0.6417 score: 0.7907 time: 0.21s
Epoch 56/1000, LR 0.000269
Train loss: 0.5857;  Loss pred: 0.5857; Loss self: 0.0000; time: 0.32s
Val loss: 0.6079 score: 0.8636 time: 0.22s
Test loss: 0.6369 score: 0.7907 time: 0.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.37s
Val loss: 0.6007 score: 0.8636 time: 0.21s
Test loss: 0.6318 score: 0.7907 time: 0.19s
Epoch 58/1000, LR 0.000269
Train loss: 0.5691;  Loss pred: 0.5691; Loss self: 0.0000; time: 0.35s
Val loss: 0.5931 score: 0.8636 time: 0.21s
Test loss: 0.6264 score: 0.7907 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.5580;  Loss pred: 0.5580; Loss self: 0.0000; time: 0.32s
Val loss: 0.5852 score: 0.8636 time: 0.23s
Test loss: 0.6207 score: 0.7674 time: 0.20s
Epoch 60/1000, LR 0.000268
Train loss: 0.5475;  Loss pred: 0.5475; Loss self: 0.0000; time: 0.37s
Val loss: 0.5770 score: 0.8636 time: 0.21s
Test loss: 0.6150 score: 0.7907 time: 0.21s
Epoch 61/1000, LR 0.000268
Train loss: 0.5345;  Loss pred: 0.5345; Loss self: 0.0000; time: 0.33s
Val loss: 0.5683 score: 0.8636 time: 0.22s
Test loss: 0.6089 score: 0.7674 time: 0.29s
Epoch 62/1000, LR 0.000268
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.37s
Val loss: 0.5594 score: 0.8636 time: 0.20s
Test loss: 0.6025 score: 0.7907 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.5090;  Loss pred: 0.5090; Loss self: 0.0000; time: 0.33s
Val loss: 0.5501 score: 0.8636 time: 0.22s
Test loss: 0.5960 score: 0.7907 time: 0.20s
Epoch 64/1000, LR 0.000268
Train loss: 0.4972;  Loss pred: 0.4972; Loss self: 0.0000; time: 0.42s
Val loss: 0.5404 score: 0.8636 time: 0.21s
Test loss: 0.5892 score: 0.7907 time: 0.21s
Epoch 65/1000, LR 0.000268
Train loss: 0.4873;  Loss pred: 0.4873; Loss self: 0.0000; time: 0.35s
Val loss: 0.5305 score: 0.8409 time: 0.22s
Test loss: 0.5821 score: 0.8140 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.4687;  Loss pred: 0.4687; Loss self: 0.0000; time: 0.32s
Val loss: 0.5203 score: 0.8409 time: 0.25s
Test loss: 0.5750 score: 0.8372 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.4536;  Loss pred: 0.4536; Loss self: 0.0000; time: 0.40s
Val loss: 0.5098 score: 0.8409 time: 0.22s
Test loss: 0.5675 score: 0.8372 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.4360;  Loss pred: 0.4360; Loss self: 0.0000; time: 0.30s
Val loss: 0.4991 score: 0.8409 time: 0.25s
Test loss: 0.5596 score: 0.8372 time: 0.21s
Epoch 69/1000, LR 0.000268
Train loss: 0.4191;  Loss pred: 0.4191; Loss self: 0.0000; time: 0.36s
Val loss: 0.4883 score: 0.8636 time: 0.22s
Test loss: 0.5515 score: 0.8372 time: 0.21s
Epoch 70/1000, LR 0.000268
Train loss: 0.4016;  Loss pred: 0.4016; Loss self: 0.0000; time: 0.36s
Val loss: 0.4774 score: 0.8636 time: 0.41s
Test loss: 0.5435 score: 0.8372 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.4023;  Loss pred: 0.4023; Loss self: 0.0000; time: 0.35s
Val loss: 0.4665 score: 0.8636 time: 0.20s
Test loss: 0.5356 score: 0.8372 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.3765;  Loss pred: 0.3765; Loss self: 0.0000; time: 0.34s
Val loss: 0.4557 score: 0.8636 time: 0.23s
Test loss: 0.5278 score: 0.8372 time: 0.19s
Epoch 73/1000, LR 0.000267
Train loss: 0.3549;  Loss pred: 0.3549; Loss self: 0.0000; time: 0.38s
Val loss: 0.4451 score: 0.8636 time: 0.20s
Test loss: 0.5200 score: 0.8372 time: 0.21s
Epoch 74/1000, LR 0.000267
Train loss: 0.3377;  Loss pred: 0.3377; Loss self: 0.0000; time: 0.34s
Val loss: 0.4347 score: 0.8636 time: 0.22s
Test loss: 0.5126 score: 0.8372 time: 0.20s
Epoch 75/1000, LR 0.000267
Train loss: 0.3345;  Loss pred: 0.3345; Loss self: 0.0000; time: 0.37s
Val loss: 0.4247 score: 0.8636 time: 0.22s
Test loss: 0.5056 score: 0.8372 time: 0.20s
Epoch 76/1000, LR 0.000267
Train loss: 0.3317;  Loss pred: 0.3317; Loss self: 0.0000; time: 0.37s
Val loss: 0.4151 score: 0.8636 time: 0.22s
Test loss: 0.5000 score: 0.8372 time: 0.22s
Epoch 77/1000, LR 0.000267
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 0.30s
Val loss: 0.4061 score: 0.8636 time: 0.24s
Test loss: 0.4947 score: 0.8372 time: 0.19s
Epoch 78/1000, LR 0.000267
Train loss: 0.2966;  Loss pred: 0.2966; Loss self: 0.0000; time: 0.42s
Val loss: 0.3978 score: 0.8409 time: 0.20s
Test loss: 0.4902 score: 0.8372 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.2610;  Loss pred: 0.2610; Loss self: 0.0000; time: 0.37s
Val loss: 0.3900 score: 0.8409 time: 0.22s
Test loss: 0.4858 score: 0.8372 time: 0.19s
Epoch 80/1000, LR 0.000267
Train loss: 0.2397;  Loss pred: 0.2397; Loss self: 0.0000; time: 0.35s
Val loss: 0.3832 score: 0.8409 time: 0.20s
Test loss: 0.4832 score: 0.8372 time: 0.21s
Epoch 81/1000, LR 0.000267
Train loss: 0.2488;  Loss pred: 0.2488; Loss self: 0.0000; time: 0.33s
Val loss: 0.3771 score: 0.8409 time: 0.22s
Test loss: 0.4815 score: 0.8372 time: 0.20s
Epoch 82/1000, LR 0.000267
Train loss: 0.2360;  Loss pred: 0.2360; Loss self: 0.0000; time: 0.36s
Val loss: 0.3716 score: 0.8409 time: 0.21s
Test loss: 0.4795 score: 0.8372 time: 0.19s
Epoch 83/1000, LR 0.000266
Train loss: 0.2147;  Loss pred: 0.2147; Loss self: 0.0000; time: 0.38s
Val loss: 0.3669 score: 0.8409 time: 0.20s
Test loss: 0.4781 score: 0.8372 time: 0.22s
Epoch 84/1000, LR 0.000266
Train loss: 0.2024;  Loss pred: 0.2024; Loss self: 0.0000; time: 0.29s
Val loss: 0.3628 score: 0.8409 time: 0.23s
Test loss: 0.4764 score: 0.8372 time: 0.20s
Epoch 85/1000, LR 0.000266
Train loss: 0.1793;  Loss pred: 0.1793; Loss self: 0.0000; time: 0.38s
Val loss: 0.3592 score: 0.8409 time: 0.21s
Test loss: 0.4746 score: 0.8372 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.1673;  Loss pred: 0.1673; Loss self: 0.0000; time: 0.33s
Val loss: 0.3562 score: 0.8409 time: 0.31s
Test loss: 0.4719 score: 0.8372 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.1715;  Loss pred: 0.1715; Loss self: 0.0000; time: 0.43s
Val loss: 0.3540 score: 0.8409 time: 0.20s
Test loss: 0.4692 score: 0.8372 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.1463;  Loss pred: 0.1463; Loss self: 0.0000; time: 0.32s
Val loss: 0.3531 score: 0.8636 time: 0.23s
Test loss: 0.4672 score: 0.8372 time: 0.21s
Epoch 89/1000, LR 0.000266
Train loss: 0.1634;  Loss pred: 0.1634; Loss self: 0.0000; time: 0.37s
Val loss: 0.3534 score: 0.8636 time: 0.21s
Test loss: 0.4660 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 90/1000, LR 0.000266
Train loss: 0.1407;  Loss pred: 0.1407; Loss self: 0.0000; time: 0.34s
Val loss: 0.3549 score: 0.8636 time: 0.23s
Test loss: 0.4667 score: 0.8372 time: 0.20s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.1463,   Val_Loss: 0.3531,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3531,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.4672


[0.19736693298909813, 0.2251862499397248, 0.21400988893583417, 0.22936506301630288, 0.19939212501049042, 0.31217566691339016, 0.20219013502355665, 0.22117027698550373, 0.21759947901591659, 0.21866998297628015, 0.20702882600016892, 0.21976079302839935, 0.21193898003548384, 0.20146099396515638, 0.2070404200349003, 0.21253455604892224, 0.2076476289657876, 0.2194756920216605, 0.1936508739599958, 0.21984776097815484, 0.2079086620360613, 0.20818237692583352, 0.20506687101442367, 0.21471938700415194, 0.19074751902371645, 0.2089205130469054, 0.1993107380112633, 0.20937752805184573, 0.24356618395540863, 0.2111559510231018, 0.2265710480278358, 0.1924955699359998, 0.21675924002192914, 0.20335456100292504, 0.19592491106595844, 0.20859804295469075, 0.19798275001812726, 0.20892840204760432, 0.20862208399921656, 0.21986195095814764, 0.19407352001871914, 0.22212462092284113, 0.19198067393153906, 0.21113846392836422, 0.2900692679686472, 0.19258417200762779, 0.2295914499554783, 0.1967274349881336, 0.22165314690209925, 0.1971892589936033, 0.21390863903798163, 0.20602870895527303, 0.20813538203947246, 0.20041954808402807, 0.2083726259879768, 0.21312769793439656, 0.20754923799540848, 0.20430616103112698, 0.20675029000267386, 0.19655122002586722, 0.2210573919583112, 0.1996976420050487, 0.2071262950776145, 0.2023576069623232, 0.18113727099262178, 0.36511120095383376, 0.19481733196880668, 0.21312082698568702, 0.20210614893585443, 0.1945370880421251, 0.22475601395126432, 0.20644460595212877, 0.2148760830750689, 0.2951431059045717, 0.2122901719994843, 0.20648369402624667, 0.2163188069825992, 0.20171368995215744, 0.2134234820259735, 0.22216041409410536, 0.20972951594740152, 0.21423231100197881, 0.20918782998342067, 0.22473739203996956, 0.1924046630738303, 0.21388185489922762, 0.20234181196428835, 0.20837488397955894, 0.22002701507881284, 0.19127645099069923, 0.22064701607450843, 0.19020528194960207, 0.21228564204648137, 0.2038806910859421, 0.19418790098279715, 0.220287712989375, 0.20264303707517684, 0.21278266794979572, 0.21936072304379195, 0.22734374995343387, 0.20943892805371433, 0.21559872198849916, 0.20355911296792328]
[0.004485612113388594, 0.005117869316811927, 0.0048638611121780495, 0.005212842341279611, 0.0045316392047838735, 0.007094901520758867, 0.0045952303414444695, 0.005026597204215994, 0.0049454427049071955, 0.004969772340370004, 0.00470520059091293, 0.004994563477918167, 0.004816795000806451, 0.00468513939453852, 0.004814893489183728, 0.004942664094160982, 0.004829014627111339, 0.005104085860968849, 0.0045035086967440885, 0.00511273862739895, 0.0048350851636293326, 0.004841450626182175, 0.0047689970003354345, 0.0049934741163756265, 0.004435988814505034, 0.004858616582486172, 0.0046351334421224025, 0.004869244838415017, 0.005664329859428108, 0.004910603512165158, 0.005269094140182228, 0.004476641161302321, 0.005040912558649515, 0.004729175837277327, 0.004556393280603685, 0.0048511172780160644, 0.0046042500004215645, 0.004858800047618705, 0.004851676372074803, 0.0051130686269336666, 0.004513337674853933, 0.005165688858670724, 0.004464666835617187, 0.004910196835543354, 0.006745796929503423, 0.004478701674595995, 0.00533933604547624, 0.004575056627631014, 0.005154724346560447, 0.0045857967207814715, 0.004974619512511201, 0.004791365324541233, 0.004840357721848197, 0.004660919722884373, 0.004845875022976205, 0.004956458091497595, 0.0048267264650095, 0.004751306070491325, 0.00480814627913195, 0.004570958605252726, 0.005140869580425842, 0.004644131209419737, 0.004816890583200337, 0.0047059908595889115, 0.004212494674247018, 0.008490958161717065, 0.0045306356271815505, 0.004956298301992721, 0.004700142998508242, 0.004524118326561049, 0.0052268840453782404, 0.0048010373477239245, 0.004997118211048114, 0.0068637931605714355, 0.004936980744174054, 0.004801946372703411, 0.005030669929827888, 0.00469101604539901, 0.0049633367913017095, 0.0051665212580024504, 0.0048774306034279425, 0.0049821467674878795, 0.0048648332554283875, 0.005226450977673711, 0.004474527048228612, 0.004973996625563433, 0.004705623534053217, 0.004845927534408347, 0.005116907327414252, 0.004448289557923238, 0.005131325955221126, 0.004423378649990746, 0.004936875396429799, 0.004741411420603304, 0.004515997697274352, 0.005122970069520349, 0.004712628769190159, 0.004948434138367343, 0.005101412163809115, 0.005287063952405439, 0.004870672745435217, 0.0050139237671743995, 0.004733932859719146]
[222.93501415675547, 195.39381295162295, 205.5979759570473, 191.83392370821772, 220.6707010002781, 140.94628333798784, 217.6169475077193, 198.94174117656829, 202.2063664811512, 201.21646053620822, 212.53079027731192, 200.2176975868209, 207.60692531705735, 213.44082124124262, 207.68891404273424, 202.32004055896704, 207.08158438488485, 195.92146904248622, 222.04908824156865, 195.58989279073302, 206.82158972550044, 206.54966397717257, 209.68769741932394, 200.26137648748283, 225.42888222128659, 205.8199042922412, 215.74351903493536, 205.37065462608962, 176.54339080121366, 203.64095727188635, 189.7859429714831, 223.3817641325277, 198.37677967338217, 211.4533344515518, 219.47183625630933, 206.1380796815048, 217.19063906356956, 205.81213266639762, 206.114324886916, 195.5772693392745, 221.56551803590972, 193.585023674331, 223.98087848849798, 203.65782340156264, 148.2404540857729, 223.27899303322226, 187.28920440346727, 218.57652951452204, 193.99679454581545, 218.0646157882002, 201.02039914509913, 208.70877761669084, 206.59630082426415, 214.54992993982694, 206.3610793218161, 201.7569767643995, 207.17975365899088, 210.46844492099655, 207.98036123404657, 218.77249092801847, 194.519620534152, 215.32552697298692, 207.60280573689118, 212.49510035966247, 237.3890241603094, 117.77233864002191, 220.71958159700583, 201.7634813461372, 212.7594841938608, 221.0375431891361, 191.31857361255766, 208.2883192887637, 200.1153380340499, 145.69203596408295, 202.55294719957388, 208.248889592871, 198.7806820858574, 213.17343413924343, 201.4773613091315, 193.5538344008737, 205.02598218356664, 200.71668834120368, 205.55689116048478, 191.334426415131, 223.48730697602616, 201.04557266094332, 212.51168793323419, 206.35884315222074, 195.43054740163413, 224.80550939378764, 194.8814027264239, 226.0715347084501, 202.55726946707426, 210.90766256954737, 221.4350110505047, 195.19926652501945, 212.19579325613736, 202.08412844106968, 196.02415329117838, 189.14089350953154, 205.31044729646385, 199.44459597628682, 211.24084976129717]
Elapsed: 0.21301885362764258~0.02390146967774625
Time per graph: 0.004939169473103952~0.0005499610421118963
Speed: 204.3102808267694~16.99859544571024
Total Time: 0.2040
best val loss: 0.3530784845352173 test_score: 0.8372

Testing...
Test loss: 0.6463 score: 0.7907 time: 0.21s
test Score 0.7907
Epoch Time List: [0.7373733698623255, 0.7496859501115978, 0.8021889929659665, 0.7947309329174459, 0.7662587490631267, 0.8990391058614478, 0.9350886319298297, 0.885808655875735, 0.747311522020027, 0.8021732069319114, 0.8196605310076848, 0.7946640090085566, 0.7722186479950324, 0.7507141740061343, 0.8223324898863211, 0.8772286550374702, 0.8606212110025808, 0.8010004090610892, 0.7813395208213478, 0.7806279240176082, 0.739067738992162, 0.7847590749152005, 0.7485481610056013, 0.8993486379040405, 0.7736597078619525, 0.7794995759613812, 0.7525986420223489, 0.7435179100139067, 0.8877573719946668, 0.7443032389273867, 0.8204220311017707, 0.8220545490039513, 0.7618272709660232, 0.7657455409644172, 0.7563634988619015, 0.8488737948937342, 0.7432741899974644, 0.8525574719533324, 0.7451480719028041, 0.7945579119259492, 0.8552161789266393, 0.7867332178866491, 0.7552934939740226, 0.786515349172987, 0.8745677710976452, 0.7875743659678847, 0.8105193220544606, 0.8011824829736724, 0.7729539660504088, 0.8262063530273736, 0.7652366779511794, 0.7518508348148316, 0.8203099759994075, 0.7659593689022586, 0.7442165069514886, 0.7887912379810587, 0.7562784000765532, 0.8674126340774819, 0.7836968520423397, 0.7405316970543936, 0.7635135991731659, 0.7350189781282097, 0.8282860859762877, 0.750303591019474, 0.7045526228612289, 1.0414181341184303, 0.7239058439154178, 0.7835177950328216, 0.7388466539559886, 0.7698117310646921, 0.7790681850165129, 0.7542527100304142, 0.7846765831345692, 0.8454048389103264, 0.7785018398426473, 0.7552219739882275, 0.8403273120056838, 0.7599261200521141, 0.7723754791077226, 0.8341902589891106, 0.7604750419268385, 0.7934102891013026, 0.9717739679617807, 0.7696560120675713, 0.7590714390389621, 0.7907043099403381, 0.7664356310851872, 0.7904766538413242, 0.8105359949404374, 0.718918225960806, 0.8437572880648077, 0.7743302950402722, 0.7604135669535026, 0.7463005707832053, 0.7627484290860593, 0.7967421419452876, 0.7214811320882291, 0.7982497799675912, 0.8553869220195338, 0.8569112418917939, 0.751556109986268, 0.7856901650084183, 0.7644658170174807]
Total Epoch List: [13, 90]
Total Time List: [0.21221521904226393, 0.20403788797557354]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcdd50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.24s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.23s
Epoch 14/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.33s
Epoch 15/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.22s
Epoch 20/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.31s
Epoch 23/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.22s
Epoch 25/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4884 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.4884 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4884 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4884 time: 0.21s
Epoch 34/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.4884 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4884 time: 0.23s
Epoch 36/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6856 score: 0.4884 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.4884 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.36s
Val loss: 0.6856 score: 0.5227 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6832 score: 0.4884 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.38s
Val loss: 0.6845 score: 0.5227 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6817 score: 0.4884 time: 0.23s
Epoch 40/1000, LR 0.000269
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.32s
Val loss: 0.6834 score: 0.5227 time: 0.18s
Test loss: 0.6801 score: 0.5116 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.36s
Val loss: 0.6821 score: 0.5227 time: 0.17s
Test loss: 0.6784 score: 0.5349 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.33s
Val loss: 0.6807 score: 0.5227 time: 0.19s
Test loss: 0.6764 score: 0.5581 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.45s
Val loss: 0.6792 score: 0.5455 time: 0.16s
Test loss: 0.6743 score: 0.5814 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.36s
Val loss: 0.6776 score: 0.5682 time: 0.21s
Test loss: 0.6721 score: 0.5814 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.39s
Val loss: 0.6759 score: 0.5682 time: 0.19s
Test loss: 0.6697 score: 0.5814 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.44s
Val loss: 0.6740 score: 0.5682 time: 0.19s
Test loss: 0.6671 score: 0.6279 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.38s
Val loss: 0.6720 score: 0.6136 time: 0.19s
Test loss: 0.6643 score: 0.6512 time: 0.22s
Epoch 48/1000, LR 0.000269
Train loss: 0.6603;  Loss pred: 0.6603; Loss self: 0.0000; time: 0.36s
Val loss: 0.6697 score: 0.6364 time: 0.18s
Test loss: 0.6614 score: 0.6512 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.34s
Val loss: 0.6673 score: 0.6364 time: 0.21s
Test loss: 0.6581 score: 0.6512 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6544;  Loss pred: 0.6544; Loss self: 0.0000; time: 0.37s
Val loss: 0.6646 score: 0.6364 time: 0.18s
Test loss: 0.6545 score: 0.6512 time: 0.24s
Epoch 51/1000, LR 0.000269
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.34s
Val loss: 0.6615 score: 0.6364 time: 0.25s
Test loss: 0.6506 score: 0.6512 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6434;  Loss pred: 0.6434; Loss self: 0.0000; time: 0.37s
Val loss: 0.6581 score: 0.6818 time: 0.17s
Test loss: 0.6462 score: 0.6977 time: 0.33s
Epoch 53/1000, LR 0.000269
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.37s
Val loss: 0.6543 score: 0.7045 time: 0.19s
Test loss: 0.6414 score: 0.6977 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6357;  Loss pred: 0.6357; Loss self: 0.0000; time: 0.39s
Val loss: 0.6504 score: 0.7045 time: 0.16s
Test loss: 0.6364 score: 0.7209 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.36s
Val loss: 0.6460 score: 0.7273 time: 0.18s
Test loss: 0.6308 score: 0.7209 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.31s
Val loss: 0.6414 score: 0.7500 time: 0.20s
Test loss: 0.6250 score: 0.7209 time: 0.23s
Epoch 57/1000, LR 0.000269
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 0.41s
Val loss: 0.6366 score: 0.7500 time: 0.17s
Test loss: 0.6189 score: 0.7442 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.6140;  Loss pred: 0.6140; Loss self: 0.0000; time: 0.32s
Val loss: 0.6315 score: 0.7500 time: 0.19s
Test loss: 0.6124 score: 0.7674 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.42s
Val loss: 0.6264 score: 0.7727 time: 0.15s
Test loss: 0.6058 score: 0.7674 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5962;  Loss pred: 0.5962; Loss self: 0.0000; time: 0.36s
Val loss: 0.6208 score: 0.7955 time: 0.19s
Test loss: 0.5987 score: 0.7674 time: 0.31s
Epoch 61/1000, LR 0.000268
Train loss: 0.5911;  Loss pred: 0.5911; Loss self: 0.0000; time: 0.40s
Val loss: 0.6150 score: 0.7955 time: 0.17s
Test loss: 0.5913 score: 0.7674 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.5797;  Loss pred: 0.5797; Loss self: 0.0000; time: 0.36s
Val loss: 0.6087 score: 0.7955 time: 0.19s
Test loss: 0.5835 score: 0.7907 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.5697;  Loss pred: 0.5697; Loss self: 0.0000; time: 0.32s
Val loss: 0.6021 score: 0.7955 time: 0.20s
Test loss: 0.5753 score: 0.7907 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5608;  Loss pred: 0.5608; Loss self: 0.0000; time: 0.44s
Val loss: 0.5953 score: 0.7955 time: 0.18s
Test loss: 0.5668 score: 0.7907 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.32s
Val loss: 0.5881 score: 0.7955 time: 0.19s
Test loss: 0.5580 score: 0.7907 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5463;  Loss pred: 0.5463; Loss self: 0.0000; time: 0.41s
Val loss: 0.5802 score: 0.7955 time: 0.17s
Test loss: 0.5483 score: 0.8140 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.5361;  Loss pred: 0.5361; Loss self: 0.0000; time: 0.32s
Val loss: 0.5717 score: 0.8182 time: 0.20s
Test loss: 0.5376 score: 0.8372 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.5226;  Loss pred: 0.5226; Loss self: 0.0000; time: 0.48s
Val loss: 0.5626 score: 0.8636 time: 0.17s
Test loss: 0.5260 score: 0.8140 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.30s
Val loss: 0.5532 score: 0.8864 time: 0.19s
Test loss: 0.5141 score: 0.8140 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.4951;  Loss pred: 0.4951; Loss self: 0.0000; time: 0.38s
Val loss: 0.5440 score: 0.8864 time: 0.16s
Test loss: 0.5026 score: 0.8140 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.4845;  Loss pred: 0.4845; Loss self: 0.0000; time: 0.38s
Val loss: 0.5348 score: 0.8864 time: 0.19s
Test loss: 0.4918 score: 0.8372 time: 0.25s
Epoch 72/1000, LR 0.000267
Train loss: 0.4636;  Loss pred: 0.4636; Loss self: 0.0000; time: 0.36s
Val loss: 0.5259 score: 0.8864 time: 0.19s
Test loss: 0.4818 score: 0.8140 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.4677;  Loss pred: 0.4677; Loss self: 0.0000; time: 0.34s
Val loss: 0.5166 score: 0.8864 time: 0.16s
Test loss: 0.4712 score: 0.8140 time: 0.26s
Epoch 74/1000, LR 0.000267
Train loss: 0.4395;  Loss pred: 0.4395; Loss self: 0.0000; time: 0.32s
Val loss: 0.5071 score: 0.8636 time: 0.19s
Test loss: 0.4604 score: 0.8372 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.4235;  Loss pred: 0.4235; Loss self: 0.0000; time: 0.46s
Val loss: 0.4971 score: 0.8636 time: 0.25s
Test loss: 0.4490 score: 0.7907 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.4186;  Loss pred: 0.4186; Loss self: 0.0000; time: 0.31s
Val loss: 0.4869 score: 0.8636 time: 0.19s
Test loss: 0.4376 score: 0.7907 time: 0.22s
Epoch 77/1000, LR 0.000267
Train loss: 0.4007;  Loss pred: 0.4007; Loss self: 0.0000; time: 0.42s
Val loss: 0.4768 score: 0.9091 time: 0.17s
Test loss: 0.4266 score: 0.7907 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.3936;  Loss pred: 0.3936; Loss self: 0.0000; time: 0.33s
Val loss: 0.4667 score: 0.9091 time: 0.19s
Test loss: 0.4159 score: 0.7907 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.3792;  Loss pred: 0.3792; Loss self: 0.0000; time: 0.44s
Val loss: 0.4565 score: 0.9091 time: 0.18s
Test loss: 0.4051 score: 0.7907 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.3443;  Loss pred: 0.3443; Loss self: 0.0000; time: 0.33s
Val loss: 0.4461 score: 0.9091 time: 0.18s
Test loss: 0.3944 score: 0.7907 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.3367;  Loss pred: 0.3367; Loss self: 0.0000; time: 0.37s
Val loss: 0.4360 score: 0.9091 time: 0.19s
Test loss: 0.3845 score: 0.8140 time: 0.22s
Epoch 82/1000, LR 0.000267
Train loss: 0.3157;  Loss pred: 0.3157; Loss self: 0.0000; time: 0.40s
Val loss: 0.4257 score: 0.9091 time: 0.19s
Test loss: 0.3743 score: 0.8140 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2966;  Loss pred: 0.2966; Loss self: 0.0000; time: 0.31s
Val loss: 0.4158 score: 0.9091 time: 0.20s
Test loss: 0.3650 score: 0.8140 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.3094;  Loss pred: 0.3094; Loss self: 0.0000; time: 0.39s
Val loss: 0.4051 score: 0.9091 time: 0.17s
Test loss: 0.3544 score: 0.8140 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.2905;  Loss pred: 0.2905; Loss self: 0.0000; time: 0.34s
Val loss: 0.3945 score: 0.9091 time: 0.19s
Test loss: 0.3434 score: 0.8140 time: 0.32s
Epoch 86/1000, LR 0.000266
Train loss: 0.2740;  Loss pred: 0.2740; Loss self: 0.0000; time: 0.37s
Val loss: 0.3847 score: 0.9091 time: 0.17s
Test loss: 0.3339 score: 0.8372 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.2682;  Loss pred: 0.2682; Loss self: 0.0000; time: 0.35s
Val loss: 0.3758 score: 0.9091 time: 0.19s
Test loss: 0.3268 score: 0.8372 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.2458;  Loss pred: 0.2458; Loss self: 0.0000; time: 0.45s
Val loss: 0.3682 score: 0.9091 time: 0.17s
Test loss: 0.3225 score: 0.8372 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.2338;  Loss pred: 0.2338; Loss self: 0.0000; time: 0.34s
Val loss: 0.3602 score: 0.8864 time: 0.18s
Test loss: 0.3171 score: 0.8372 time: 0.23s
Epoch 90/1000, LR 0.000266
Train loss: 0.2340;  Loss pred: 0.2340; Loss self: 0.0000; time: 0.41s
Val loss: 0.3531 score: 0.9091 time: 0.17s
Test loss: 0.3130 score: 0.8372 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.2004;  Loss pred: 0.2004; Loss self: 0.0000; time: 0.37s
Val loss: 0.3463 score: 0.9091 time: 0.20s
Test loss: 0.3095 score: 0.8372 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.35s
Val loss: 0.3390 score: 0.9091 time: 0.20s
Test loss: 0.3047 score: 0.8372 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.1931;  Loss pred: 0.1931; Loss self: 0.0000; time: 0.40s
Val loss: 0.3307 score: 0.8409 time: 0.18s
Test loss: 0.2972 score: 0.8605 time: 0.35s
Epoch 94/1000, LR 0.000265
Train loss: 0.1683;  Loss pred: 0.1683; Loss self: 0.0000; time: 0.37s
Val loss: 0.3230 score: 0.8409 time: 0.18s
Test loss: 0.2899 score: 0.8605 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.1665;  Loss pred: 0.1665; Loss self: 0.0000; time: 0.36s
Val loss: 0.3167 score: 0.8409 time: 0.18s
Test loss: 0.2864 score: 0.8605 time: 0.25s
Epoch 96/1000, LR 0.000265
Train loss: 0.1625;  Loss pred: 0.1625; Loss self: 0.0000; time: 0.33s
Val loss: 0.3114 score: 0.8409 time: 0.20s
Test loss: 0.2864 score: 0.8605 time: 0.24s
Epoch 97/1000, LR 0.000265
Train loss: 0.1549;  Loss pred: 0.1549; Loss self: 0.0000; time: 0.42s
Val loss: 0.3059 score: 0.8409 time: 0.18s
Test loss: 0.2843 score: 0.8605 time: 0.26s
Epoch 98/1000, LR 0.000265
Train loss: 0.1366;  Loss pred: 0.1366; Loss self: 0.0000; time: 0.33s
Val loss: 0.3014 score: 0.8409 time: 0.20s
Test loss: 0.2855 score: 0.8605 time: 0.22s
Epoch 99/1000, LR 0.000265
Train loss: 0.1475;  Loss pred: 0.1475; Loss self: 0.0000; time: 0.42s
Val loss: 0.2976 score: 0.8409 time: 0.20s
Test loss: 0.2897 score: 0.8605 time: 0.29s
Epoch 100/1000, LR 0.000265
Train loss: 0.1182;  Loss pred: 0.1182; Loss self: 0.0000; time: 0.34s
Val loss: 0.2943 score: 0.8636 time: 0.29s
Test loss: 0.2955 score: 0.8605 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.1266;  Loss pred: 0.1266; Loss self: 0.0000; time: 0.36s
Val loss: 0.2923 score: 0.8864 time: 0.18s
Test loss: 0.3048 score: 0.8372 time: 0.26s
Epoch 102/1000, LR 0.000264
Train loss: 0.1163;  Loss pred: 0.1163; Loss self: 0.0000; time: 0.46s
Val loss: 0.2900 score: 0.8864 time: 0.19s
Test loss: 0.3124 score: 0.8372 time: 0.23s
Epoch 103/1000, LR 0.000264
Train loss: 0.1196;  Loss pred: 0.1196; Loss self: 0.0000; time: 0.43s
Val loss: 0.2859 score: 0.8864 time: 0.28s
Test loss: 0.3127 score: 0.8372 time: 0.24s
Epoch 104/1000, LR 0.000264
Train loss: 0.0940;  Loss pred: 0.0940; Loss self: 0.0000; time: 0.39s
Val loss: 0.2816 score: 0.8864 time: 0.19s
Test loss: 0.3112 score: 0.8372 time: 0.25s
Epoch 105/1000, LR 0.000264
Train loss: 0.1059;  Loss pred: 0.1059; Loss self: 0.0000; time: 0.34s
Val loss: 0.2811 score: 0.8864 time: 0.19s
Test loss: 0.3233 score: 0.8140 time: 0.22s
Epoch 106/1000, LR 0.000264
Train loss: 0.0880;  Loss pred: 0.0880; Loss self: 0.0000; time: 0.40s
Val loss: 0.2826 score: 0.8864 time: 0.16s
Test loss: 0.3418 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 107/1000, LR 0.000264
Train loss: 0.0874;  Loss pred: 0.0874; Loss self: 0.0000; time: 0.36s
Val loss: 0.2868 score: 0.8864 time: 0.19s
Test loss: 0.3665 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 104,   Train_Loss: 0.1059,   Val_Loss: 0.2811,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.2811,   Test_Precision: 0.7826,   Test_Recall: 0.8571,   Test_accuracy: 0.8182,   Test_Score: 0.8140,   Test_loss: 0.3233


[0.19736693298909813, 0.2251862499397248, 0.21400988893583417, 0.22936506301630288, 0.19939212501049042, 0.31217566691339016, 0.20219013502355665, 0.22117027698550373, 0.21759947901591659, 0.21866998297628015, 0.20702882600016892, 0.21976079302839935, 0.21193898003548384, 0.20146099396515638, 0.2070404200349003, 0.21253455604892224, 0.2076476289657876, 0.2194756920216605, 0.1936508739599958, 0.21984776097815484, 0.2079086620360613, 0.20818237692583352, 0.20506687101442367, 0.21471938700415194, 0.19074751902371645, 0.2089205130469054, 0.1993107380112633, 0.20937752805184573, 0.24356618395540863, 0.2111559510231018, 0.2265710480278358, 0.1924955699359998, 0.21675924002192914, 0.20335456100292504, 0.19592491106595844, 0.20859804295469075, 0.19798275001812726, 0.20892840204760432, 0.20862208399921656, 0.21986195095814764, 0.19407352001871914, 0.22212462092284113, 0.19198067393153906, 0.21113846392836422, 0.2900692679686472, 0.19258417200762779, 0.2295914499554783, 0.1967274349881336, 0.22165314690209925, 0.1971892589936033, 0.21390863903798163, 0.20602870895527303, 0.20813538203947246, 0.20041954808402807, 0.2083726259879768, 0.21312769793439656, 0.20754923799540848, 0.20430616103112698, 0.20675029000267386, 0.19655122002586722, 0.2210573919583112, 0.1996976420050487, 0.2071262950776145, 0.2023576069623232, 0.18113727099262178, 0.36511120095383376, 0.19481733196880668, 0.21312082698568702, 0.20210614893585443, 0.1945370880421251, 0.22475601395126432, 0.20644460595212877, 0.2148760830750689, 0.2951431059045717, 0.2122901719994843, 0.20648369402624667, 0.2163188069825992, 0.20171368995215744, 0.2134234820259735, 0.22216041409410536, 0.20972951594740152, 0.21423231100197881, 0.20918782998342067, 0.22473739203996956, 0.1924046630738303, 0.21388185489922762, 0.20234181196428835, 0.20837488397955894, 0.22002701507881284, 0.19127645099069923, 0.22064701607450843, 0.19020528194960207, 0.21228564204648137, 0.2038806910859421, 0.19418790098279715, 0.220287712989375, 0.20264303707517684, 0.21278266794979572, 0.21936072304379195, 0.22734374995343387, 0.20943892805371433, 0.21559872198849916, 0.20355911296792328, 0.24279166606720537, 0.24270368891302496, 0.2351788190426305, 0.24334442801773548, 0.2332302600843832, 0.22580579097848386, 0.24161921406630427, 0.2454490199452266, 0.25039879488758743, 0.22391732491087168, 0.2593246119795367, 0.23115814302582294, 0.23908067296724766, 0.33184909797273576, 0.24581952497828752, 0.23486834194045514, 0.2494420459261164, 0.23583015601616353, 0.22916938597336411, 0.23952524294145405, 0.2350570900598541, 0.3115193750709295, 0.238347198930569, 0.22252016910351813, 0.2502776669571176, 0.21953223401214927, 0.23704771499615163, 0.22981183207593858, 0.23581925395410508, 0.24359979399014264, 0.22799133392982185, 0.24695005896501243, 0.2167185670696199, 0.24919736303854734, 0.23101568792480975, 0.24031380692031235, 0.24648876208811998, 0.21799733000807464, 0.23862977500539273, 0.23715450300369412, 0.242293702904135, 0.2316161369672045, 0.23729215597268194, 0.23208269802853465, 0.22335475694853812, 0.2377328110160306, 0.22429590590763837, 0.2504009259864688, 0.23474238894414157, 0.2455094780307263, 0.23119781399145722, 0.33314175403211266, 0.22599269694183022, 0.2323454211000353, 0.2503722009714693, 0.2334387550363317, 0.24489761900622398, 0.22594901907723397, 0.2358901359839365, 0.313133234041743, 0.22959798900410533, 0.24060293892398477, 0.23754260898567736, 0.24910706898663193, 0.23905419907532632, 0.24413424194790423, 0.22880502708721906, 0.24561100301798433, 0.22516776900738478, 0.23617782397195697, 0.25211128301452845, 0.22086052992381155, 0.2640271279960871, 0.23133296601008624, 0.2380306590348482, 0.22908447089139372, 0.2481164881028235, 0.2272829080466181, 0.23977526603266597, 0.23682862508576363, 0.2201158490497619, 0.2478647860698402, 0.23591608204878867, 0.2392564759356901, 0.3253803770057857, 0.23829784092959017, 0.22831916401628405, 0.23789943906012923, 0.2352957339026034, 0.23134370299521834, 0.25679681496694684, 0.23041963391005993, 0.3566059379372746, 0.22647811495698988, 0.24961097701452672, 0.24073987395968288, 0.2617378890281543, 0.22723801503889263, 0.29129941400606185, 0.2302324309712276, 0.26243010198231786, 0.23413978202734143, 0.2435569859808311, 0.2507069050334394, 0.22763203491922468, 0.2413407820276916, 0.2468525079311803]
[0.004485612113388594, 0.005117869316811927, 0.0048638611121780495, 0.005212842341279611, 0.0045316392047838735, 0.007094901520758867, 0.0045952303414444695, 0.005026597204215994, 0.0049454427049071955, 0.004969772340370004, 0.00470520059091293, 0.004994563477918167, 0.004816795000806451, 0.00468513939453852, 0.004814893489183728, 0.004942664094160982, 0.004829014627111339, 0.005104085860968849, 0.0045035086967440885, 0.00511273862739895, 0.0048350851636293326, 0.004841450626182175, 0.0047689970003354345, 0.0049934741163756265, 0.004435988814505034, 0.004858616582486172, 0.0046351334421224025, 0.004869244838415017, 0.005664329859428108, 0.004910603512165158, 0.005269094140182228, 0.004476641161302321, 0.005040912558649515, 0.004729175837277327, 0.004556393280603685, 0.0048511172780160644, 0.0046042500004215645, 0.004858800047618705, 0.004851676372074803, 0.0051130686269336666, 0.004513337674853933, 0.005165688858670724, 0.004464666835617187, 0.004910196835543354, 0.006745796929503423, 0.004478701674595995, 0.00533933604547624, 0.004575056627631014, 0.005154724346560447, 0.0045857967207814715, 0.004974619512511201, 0.004791365324541233, 0.004840357721848197, 0.004660919722884373, 0.004845875022976205, 0.004956458091497595, 0.0048267264650095, 0.004751306070491325, 0.00480814627913195, 0.004570958605252726, 0.005140869580425842, 0.004644131209419737, 0.004816890583200337, 0.0047059908595889115, 0.004212494674247018, 0.008490958161717065, 0.0045306356271815505, 0.004956298301992721, 0.004700142998508242, 0.004524118326561049, 0.0052268840453782404, 0.0048010373477239245, 0.004997118211048114, 0.0068637931605714355, 0.004936980744174054, 0.004801946372703411, 0.005030669929827888, 0.00469101604539901, 0.0049633367913017095, 0.0051665212580024504, 0.0048774306034279425, 0.0049821467674878795, 0.0048648332554283875, 0.005226450977673711, 0.004474527048228612, 0.004973996625563433, 0.004705623534053217, 0.004845927534408347, 0.005116907327414252, 0.004448289557923238, 0.005131325955221126, 0.004423378649990746, 0.004936875396429799, 0.004741411420603304, 0.004515997697274352, 0.005122970069520349, 0.004712628769190159, 0.004948434138367343, 0.005101412163809115, 0.005287063952405439, 0.004870672745435217, 0.0050139237671743995, 0.004733932859719146, 0.005646317815516404, 0.005644271835186627, 0.005469274861456523, 0.0056591727445985, 0.00542395953684612, 0.005251297464615903, 0.005619051489914053, 0.0057081167429122465, 0.005823227788083429, 0.005207379649090039, 0.006030804929756667, 0.005375770768042394, 0.005560015650401109, 0.007717420883086878, 0.005716733139029942, 0.005462054463731515, 0.005800977812235265, 0.005484422232934036, 0.005329520604031724, 0.005570354487010559, 0.005466443954880328, 0.0072446366295565, 0.005542958114664395, 0.005174887653570189, 0.005820410859467852, 0.005105400790980216, 0.005512737558050038, 0.005344461211068339, 0.005484168696607095, 0.005665111488142852, 0.0053021240448795776, 0.005743024627093312, 0.005039966676037672, 0.005795287512524357, 0.005372457858716506, 0.00558869318419331, 0.005732296792746976, 0.005069705349024992, 0.005549529651288203, 0.005515221000085909, 0.005634737276840348, 0.005386421789934989, 0.005518422231922836, 0.0053972720471752245, 0.005194296673221817, 0.0055286700236286184, 0.005216183858317172, 0.00582327734852253, 0.005459125324282362, 0.0057095227449006115, 0.00537669334863854, 0.007747482651909597, 0.005255644114926284, 0.005403381886047332, 0.005822609324917891, 0.005428808256658877, 0.005695293465261023, 0.005254628350633348, 0.0054858171159055, 0.007282168233528907, 0.005339488116374543, 0.005595417184278716, 0.005524246720597148, 0.005793187650851905, 0.005559399978495961, 0.005677540510416378, 0.005321047141563234, 0.005711883791115915, 0.005236459744357785, 0.005492507534231557, 0.005863053093361127, 0.005136291393577013, 0.006140165767350862, 0.005379836418839215, 0.005535596721740656, 0.005327545834683575, 0.005770150886112174, 0.005285649024339956, 0.005576168977503859, 0.0055076424438549685, 0.005118973233715393, 0.0057642973504614, 0.005486420512762527, 0.005564104091527677, 0.007566985511762458, 0.005541810254176515, 0.005309748000378699, 0.00553254509442161, 0.0054719938116884505, 0.005380086116167868, 0.005972018952719694, 0.005358596137443254, 0.008293161347378479, 0.005266932905976509, 0.005804906442198296, 0.005598601719992625, 0.0060869276518175424, 0.00528460500090448, 0.006774404976885159, 0.005354242580726223, 0.006103025627495764, 0.005445111209938173, 0.005664115953042583, 0.005830393140312544, 0.005293768253935458, 0.005612576326225386, 0.005740755998399542]
[222.93501415675547, 195.39381295162295, 205.5979759570473, 191.83392370821772, 220.6707010002781, 140.94628333798784, 217.6169475077193, 198.94174117656829, 202.2063664811512, 201.21646053620822, 212.53079027731192, 200.2176975868209, 207.60692531705735, 213.44082124124262, 207.68891404273424, 202.32004055896704, 207.08158438488485, 195.92146904248622, 222.04908824156865, 195.58989279073302, 206.82158972550044, 206.54966397717257, 209.68769741932394, 200.26137648748283, 225.42888222128659, 205.8199042922412, 215.74351903493536, 205.37065462608962, 176.54339080121366, 203.64095727188635, 189.7859429714831, 223.3817641325277, 198.37677967338217, 211.4533344515518, 219.47183625630933, 206.1380796815048, 217.19063906356956, 205.81213266639762, 206.114324886916, 195.5772693392745, 221.56551803590972, 193.585023674331, 223.98087848849798, 203.65782340156264, 148.2404540857729, 223.27899303322226, 187.28920440346727, 218.57652951452204, 193.99679454581545, 218.0646157882002, 201.02039914509913, 208.70877761669084, 206.59630082426415, 214.54992993982694, 206.3610793218161, 201.7569767643995, 207.17975365899088, 210.46844492099655, 207.98036123404657, 218.77249092801847, 194.519620534152, 215.32552697298692, 207.60280573689118, 212.49510035966247, 237.3890241603094, 117.77233864002191, 220.71958159700583, 201.7634813461372, 212.7594841938608, 221.0375431891361, 191.31857361255766, 208.2883192887637, 200.1153380340499, 145.69203596408295, 202.55294719957388, 208.248889592871, 198.7806820858574, 213.17343413924343, 201.4773613091315, 193.5538344008737, 205.02598218356664, 200.71668834120368, 205.55689116048478, 191.334426415131, 223.48730697602616, 201.04557266094332, 212.51168793323419, 206.35884315222074, 195.43054740163413, 224.80550939378764, 194.8814027264239, 226.0715347084501, 202.55726946707426, 210.90766256954737, 221.4350110505047, 195.19926652501945, 212.19579325613736, 202.08412844106968, 196.02415329117838, 189.14089350953154, 205.31044729646385, 199.44459597628682, 211.24084976129717, 177.10657328780587, 177.17077228030692, 182.83959488803052, 176.70427200768313, 184.36715709377725, 190.4291285607343, 177.96597909717602, 175.1891289262255, 171.72606609110946, 192.0351630545594, 165.81534499082997, 186.0198366241263, 179.85560884668703, 129.57696815416028, 174.92507970551998, 183.0812941613968, 172.38473105186287, 182.3346120207495, 187.63413715738542, 179.52178848435724, 182.93428200379165, 138.03314798705338, 180.4090847005338, 193.240910130695, 171.8091770743875, 195.8710081619281, 181.39807844466287, 187.10960010880189, 182.34304145652425, 176.51903269565167, 188.60365987961566, 174.12427508710246, 198.4140103057549, 172.55399285003068, 186.13454517424591, 178.9327069928143, 174.45014383506643, 197.25012227630162, 180.19545129700705, 181.31639692850445, 177.47056355407312, 185.6520040574226, 181.21121544763722, 185.27878366319723, 192.51884574774192, 180.8753272895951, 191.71103380597032, 171.72460457404077, 183.17952796430015, 175.14598762096838, 185.98791769540196, 129.07418382582895, 190.2716352425674, 185.0692808854044, 171.74430640924064, 184.2024902562028, 175.5835772290917, 190.30841636582508, 182.2882496575787, 137.321738242156, 187.28387032706604, 178.7176839663844, 181.02015543069447, 172.6165386430987, 179.87552683168153, 176.13260498367848, 187.93293376202206, 175.07358982957055, 190.96871719055733, 182.0662045099784, 170.55960164036782, 194.69300383745957, 162.86205257149663, 185.87925768489555, 180.64899779866047, 187.70368778242414, 173.30569334102498, 189.19152508898844, 179.33459406168217, 181.5658896150254, 195.35167588172595, 173.48168201627482, 182.26820158494908, 179.72345296750922, 132.15302162869952, 180.4464523566758, 188.33285495444954, 180.74863971886762, 182.74874468314462, 185.8706307683195, 167.44755968073306, 186.61604165547914, 120.58127873227818, 189.86381976980894, 172.26806494771066, 178.6160277179562, 164.2864934827018, 189.22890165468306, 147.6144404434167, 186.7677799283358, 163.85315432639382, 183.6509781792601, 176.55005799498716, 171.5150206742995, 188.90135571321747, 178.1712963665882, 174.19308541920066]
Elapsed: 0.22839940918631674~0.02821543154697672
Time per graph: 0.00530437603564305~0.000657725003723665
Speed: 190.98851124148032~20.32200348358957
Total Time: 0.2475
best val loss: 0.2810980975627899 test_score: 0.8140

Testing...
Test loss: 0.4266 score: 0.7907 time: 0.22s
test Score 0.7907
Epoch Time List: [0.7373733698623255, 0.7496859501115978, 0.8021889929659665, 0.7947309329174459, 0.7662587490631267, 0.8990391058614478, 0.9350886319298297, 0.885808655875735, 0.747311522020027, 0.8021732069319114, 0.8196605310076848, 0.7946640090085566, 0.7722186479950324, 0.7507141740061343, 0.8223324898863211, 0.8772286550374702, 0.8606212110025808, 0.8010004090610892, 0.7813395208213478, 0.7806279240176082, 0.739067738992162, 0.7847590749152005, 0.7485481610056013, 0.8993486379040405, 0.7736597078619525, 0.7794995759613812, 0.7525986420223489, 0.7435179100139067, 0.8877573719946668, 0.7443032389273867, 0.8204220311017707, 0.8220545490039513, 0.7618272709660232, 0.7657455409644172, 0.7563634988619015, 0.8488737948937342, 0.7432741899974644, 0.8525574719533324, 0.7451480719028041, 0.7945579119259492, 0.8552161789266393, 0.7867332178866491, 0.7552934939740226, 0.786515349172987, 0.8745677710976452, 0.7875743659678847, 0.8105193220544606, 0.8011824829736724, 0.7729539660504088, 0.8262063530273736, 0.7652366779511794, 0.7518508348148316, 0.8203099759994075, 0.7659593689022586, 0.7442165069514886, 0.7887912379810587, 0.7562784000765532, 0.8674126340774819, 0.7836968520423397, 0.7405316970543936, 0.7635135991731659, 0.7350189781282097, 0.8282860859762877, 0.750303591019474, 0.7045526228612289, 1.0414181341184303, 0.7239058439154178, 0.7835177950328216, 0.7388466539559886, 0.7698117310646921, 0.7790681850165129, 0.7542527100304142, 0.7846765831345692, 0.8454048389103264, 0.7785018398426473, 0.7552219739882275, 0.8403273120056838, 0.7599261200521141, 0.7723754791077226, 0.8341902589891106, 0.7604750419268385, 0.7934102891013026, 0.9717739679617807, 0.7696560120675713, 0.7590714390389621, 0.7907043099403381, 0.7664356310851872, 0.7904766538413242, 0.8105359949404374, 0.718918225960806, 0.8437572880648077, 0.7743302950402722, 0.7604135669535026, 0.7463005707832053, 0.7627484290860593, 0.7967421419452876, 0.7214811320882291, 0.7982497799675912, 0.8553869220195338, 0.8569112418917939, 0.751556109986268, 0.7856901650084183, 0.7644658170174807, 0.75472454901319, 0.7816581799415872, 0.7667287498479709, 0.9093559710308909, 0.7412435199366882, 0.7680202380288392, 0.7767064109211788, 0.7629687139997259, 0.8154639799613506, 0.7510731419315562, 0.8117291951784864, 0.7589125429512933, 0.9017141718650237, 0.8550147779751569, 0.7716371971182525, 0.7544315299019217, 0.8497080890228972, 0.8072675630683079, 0.7738747010007501, 0.7698886269936338, 0.7922399799572304, 0.8638479269575328, 0.7671111330855638, 0.772525253938511, 0.7820772819686681, 0.7380391949554905, 0.7873794811312109, 0.798713929951191, 0.8282867609523237, 0.7948912859428674, 0.8342036590911448, 0.798199972952716, 0.746753805084154, 0.7889136259909719, 0.7296803429489955, 0.7922499440610409, 0.7970877529587597, 0.7689044130966067, 0.8721140160923824, 0.7369130728766322, 0.766048583900556, 0.7504056477919221, 0.8475633520865813, 0.7976259719580412, 0.80310993490275, 0.8645826649153605, 0.7866462459787726, 0.7832950637675822, 0.7845607770141214, 0.7888107699109241, 0.8182535731466487, 0.8697650580434129, 0.778176112100482, 0.7779821959557012, 0.7957862340845168, 0.7320487321121618, 0.8207633271813393, 0.7342750580282882, 0.8018706131260842, 0.8577367929974571, 0.7894628420472145, 0.7840498520527035, 0.752304092864506, 0.8653769809752703, 0.7520573410438374, 0.8210856200894341, 0.7368798650568351, 0.8959380771266297, 0.711751893046312, 0.7753287479281425, 0.811287194956094, 0.7632843420142308, 0.764961002045311, 0.7390546180540696, 0.9448125710478052, 0.7200652719475329, 0.8353969838935882, 0.7402434259420261, 0.8598575420910493, 0.7437413230072707, 0.7715293540386483, 0.8276168920565397, 0.7396383150480688, 0.7938067610375583, 0.8475974419852719, 0.7785783269209787, 0.7605906439712271, 0.8552055740728974, 0.7539821850368753, 0.8121584019390866, 0.8266219949582592, 0.775427483022213, 0.9393915861146525, 0.7722576929954812, 0.7787589051295072, 0.7580338860861957, 0.8482358299661428, 0.7535455421311781, 0.9012827879050747, 0.8543184250593185, 0.8029393960023299, 0.8883883710950613, 0.9494325539562851, 0.8312436379492283, 0.7580392919480801, 0.8050245940685272, 0.7885538437403738]
Total Epoch List: [13, 90, 107]
Total Time List: [0.21221521904226393, 0.20403788797557354, 0.2475319819059223]
========================training times:3========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcc100>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5116 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5000 time: 0.20s
Epoch 3/1000, LR 0.000030
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.20s
Epoch 5/1000, LR 0.000090
Train loss: 0.6981;  Loss pred: 0.6981; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.22s
Epoch 6/1000, LR 0.000120
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5116 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.5000 time: 0.19s
Epoch 7/1000, LR 0.000150
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.21s
Epoch 8/1000, LR 0.000180
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.20s
Epoch 9/1000, LR 0.000210
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.21s
Epoch 10/1000, LR 0.000240
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.20s
Epoch 12/1000, LR 0.000270
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.35s
Epoch 14/1000, LR 0.000270
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.19s
Epoch 17/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.21s
Epoch 22/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5116 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.23s
Epoch 23/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.19s
Epoch 24/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.32s
Val loss: 0.6915 score: 0.5349 time: 0.25s
Test loss: 0.6924 score: 0.5000 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.34s
Val loss: 0.6915 score: 0.5581 time: 0.26s
Test loss: 0.6923 score: 0.5909 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.34s
Val loss: 0.6914 score: 0.6279 time: 0.25s
Test loss: 0.6921 score: 0.5682 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.33s
Val loss: 0.6913 score: 0.6279 time: 0.24s
Test loss: 0.6920 score: 0.6136 time: 0.21s
Epoch 28/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.26s
Val loss: 0.6912 score: 0.5349 time: 0.29s
Test loss: 0.6919 score: 0.5455 time: 0.20s
Epoch 29/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.32s
Val loss: 0.6911 score: 0.5581 time: 0.25s
Test loss: 0.6917 score: 0.5909 time: 0.21s
Epoch 30/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.34s
Val loss: 0.6909 score: 0.5581 time: 0.26s
Test loss: 0.6916 score: 0.5682 time: 0.19s
Epoch 31/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.34s
Val loss: 0.6907 score: 0.5116 time: 0.24s
Test loss: 0.6913 score: 0.5455 time: 0.31s
Epoch 32/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.29s
Val loss: 0.6904 score: 0.5116 time: 0.25s
Test loss: 0.6911 score: 0.5227 time: 0.21s
Epoch 33/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.32s
Val loss: 0.6900 score: 0.5116 time: 0.25s
Test loss: 0.6908 score: 0.5000 time: 0.21s
Epoch 34/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.31s
Val loss: 0.6897 score: 0.5116 time: 0.24s
Test loss: 0.6905 score: 0.4773 time: 0.21s
Epoch 35/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.26s
Val loss: 0.6893 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5000 time: 0.21s
Epoch 36/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.40s
Val loss: 0.6889 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5000 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.26s
Val loss: 0.6884 score: 0.5116 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.21s
Epoch 38/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.37s
Val loss: 0.6879 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.32s
Epoch 39/1000, LR 0.000269
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.31s
Val loss: 0.6873 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5000 time: 0.20s
Epoch 40/1000, LR 0.000269
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.5000 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.5000 time: 0.18s
Epoch 42/1000, LR 0.000269
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6853 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.5000 time: 0.20s
Epoch 43/1000, LR 0.000269
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.5000 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.5000 time: 0.21s
Epoch 45/1000, LR 0.000269
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6823 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.5000 time: 0.31s
Epoch 46/1000, LR 0.000269
Train loss: 0.6797;  Loss pred: 0.6797; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6811 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6822 score: 0.5000 time: 0.20s
Epoch 47/1000, LR 0.000269
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6798 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6809 score: 0.5000 time: 0.22s
Epoch 48/1000, LR 0.000269
Train loss: 0.6755;  Loss pred: 0.6755; Loss self: 0.0000; time: 0.26s
Val loss: 0.6783 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5000 time: 0.21s
Epoch 49/1000, LR 0.000269
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.35s
Val loss: 0.6766 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6779 score: 0.5000 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 0.28s
Val loss: 0.6748 score: 0.5116 time: 0.26s
Test loss: 0.6761 score: 0.5227 time: 0.20s
Epoch 51/1000, LR 0.000269
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.41s
Val loss: 0.6728 score: 0.5581 time: 0.24s
Test loss: 0.6743 score: 0.5455 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 0.32s
Val loss: 0.6707 score: 0.5814 time: 0.27s
Test loss: 0.6722 score: 0.5455 time: 0.20s
Epoch 53/1000, LR 0.000269
Train loss: 0.6659;  Loss pred: 0.6659; Loss self: 0.0000; time: 0.41s
Val loss: 0.6683 score: 0.6279 time: 0.25s
Test loss: 0.6700 score: 0.6364 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6624;  Loss pred: 0.6624; Loss self: 0.0000; time: 0.31s
Val loss: 0.6657 score: 0.6279 time: 0.25s
Test loss: 0.6675 score: 0.6818 time: 0.21s
Epoch 55/1000, LR 0.000269
Train loss: 0.6595;  Loss pred: 0.6595; Loss self: 0.0000; time: 0.43s
Val loss: 0.6629 score: 0.6744 time: 0.25s
Test loss: 0.6648 score: 0.7045 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6563;  Loss pred: 0.6563; Loss self: 0.0000; time: 0.32s
Val loss: 0.6597 score: 0.6977 time: 0.26s
Test loss: 0.6618 score: 0.7045 time: 0.21s
Epoch 57/1000, LR 0.000269
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.43s
Val loss: 0.6563 score: 0.6977 time: 0.25s
Test loss: 0.6586 score: 0.7045 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.6461;  Loss pred: 0.6461; Loss self: 0.0000; time: 0.30s
Val loss: 0.6525 score: 0.6977 time: 0.27s
Test loss: 0.6550 score: 0.7045 time: 0.21s
Epoch 59/1000, LR 0.000268
Train loss: 0.6427;  Loss pred: 0.6427; Loss self: 0.0000; time: 0.35s
Val loss: 0.6485 score: 0.6977 time: 0.25s
Test loss: 0.6512 score: 0.7045 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.32s
Val loss: 0.6441 score: 0.6977 time: 0.24s
Test loss: 0.6470 score: 0.7045 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.27s
Val loss: 0.6393 score: 0.6977 time: 0.27s
Test loss: 0.6426 score: 0.7045 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.6291;  Loss pred: 0.6291; Loss self: 0.0000; time: 0.38s
Val loss: 0.6342 score: 0.6977 time: 0.24s
Test loss: 0.6378 score: 0.7045 time: 0.22s
Epoch 63/1000, LR 0.000268
Train loss: 0.6232;  Loss pred: 0.6232; Loss self: 0.0000; time: 0.29s
Val loss: 0.6288 score: 0.6977 time: 0.25s
Test loss: 0.6327 score: 0.7273 time: 0.20s
Epoch 64/1000, LR 0.000268
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.45s
Val loss: 0.6229 score: 0.6977 time: 0.35s
Test loss: 0.6272 score: 0.7273 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.6104;  Loss pred: 0.6104; Loss self: 0.0000; time: 0.26s
Val loss: 0.6166 score: 0.7209 time: 0.32s
Test loss: 0.6214 score: 0.7273 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.36s
Val loss: 0.6099 score: 0.7209 time: 0.24s
Test loss: 0.6152 score: 0.7500 time: 0.31s
Epoch 67/1000, LR 0.000268
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.31s
Val loss: 0.6027 score: 0.7442 time: 0.27s
Test loss: 0.6087 score: 0.7727 time: 0.19s
Epoch 68/1000, LR 0.000268
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 0.48s
Val loss: 0.5952 score: 0.7674 time: 0.25s
Test loss: 0.6018 score: 0.7955 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.5742;  Loss pred: 0.5742; Loss self: 0.0000; time: 0.33s
Val loss: 0.5873 score: 0.7907 time: 0.24s
Test loss: 0.5946 score: 0.7955 time: 0.26s
Epoch 70/1000, LR 0.000268
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.47s
Val loss: 0.5791 score: 0.8140 time: 0.23s
Test loss: 0.5870 score: 0.7955 time: 0.21s
Epoch 71/1000, LR 0.000268
Train loss: 0.5533;  Loss pred: 0.5533; Loss self: 0.0000; time: 0.29s
Val loss: 0.5705 score: 0.8140 time: 0.25s
Test loss: 0.5792 score: 0.8182 time: 0.18s
Epoch 72/1000, LR 0.000267
Train loss: 0.5516;  Loss pred: 0.5516; Loss self: 0.0000; time: 0.27s
Val loss: 0.5617 score: 0.8605 time: 0.22s
Test loss: 0.5711 score: 0.8182 time: 0.21s
Epoch 73/1000, LR 0.000267
Train loss: 0.5352;  Loss pred: 0.5352; Loss self: 0.0000; time: 0.43s
Val loss: 0.5528 score: 0.8605 time: 0.25s
Test loss: 0.5627 score: 0.8182 time: 0.18s
Epoch 74/1000, LR 0.000267
Train loss: 0.5265;  Loss pred: 0.5265; Loss self: 0.0000; time: 0.25s
Val loss: 0.5436 score: 0.8605 time: 0.22s
Test loss: 0.5542 score: 0.7955 time: 0.17s
Epoch 75/1000, LR 0.000267
Train loss: 0.5046;  Loss pred: 0.5046; Loss self: 0.0000; time: 0.25s
Val loss: 0.5343 score: 0.8605 time: 0.21s
Test loss: 0.5454 score: 0.7727 time: 0.17s
Epoch 76/1000, LR 0.000267
Train loss: 0.4870;  Loss pred: 0.4870; Loss self: 0.0000; time: 0.26s
Val loss: 0.5248 score: 0.8605 time: 0.21s
Test loss: 0.5364 score: 0.7727 time: 0.18s
Epoch 77/1000, LR 0.000267
Train loss: 0.4940;  Loss pred: 0.4940; Loss self: 0.0000; time: 0.41s
Val loss: 0.5154 score: 0.8605 time: 0.25s
Test loss: 0.5273 score: 0.8182 time: 0.20s
Epoch 78/1000, LR 0.000267
Train loss: 0.4740;  Loss pred: 0.4740; Loss self: 0.0000; time: 0.32s
Val loss: 0.5060 score: 0.8605 time: 0.23s
Test loss: 0.5181 score: 0.8182 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.4663;  Loss pred: 0.4663; Loss self: 0.0000; time: 0.43s
Val loss: 0.4965 score: 0.8605 time: 0.26s
Test loss: 0.5087 score: 0.8182 time: 0.20s
Epoch 80/1000, LR 0.000267
Train loss: 0.4623;  Loss pred: 0.4623; Loss self: 0.0000; time: 0.32s
Val loss: 0.4871 score: 0.8837 time: 0.23s
Test loss: 0.4991 score: 0.8182 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.4404;  Loss pred: 0.4404; Loss self: 0.0000; time: 0.44s
Val loss: 0.4777 score: 0.8837 time: 0.36s
Test loss: 0.4894 score: 0.8182 time: 0.22s
Epoch 82/1000, LR 0.000267
Train loss: 0.4319;  Loss pred: 0.4319; Loss self: 0.0000; time: 0.32s
Val loss: 0.4683 score: 0.8605 time: 0.25s
Test loss: 0.4796 score: 0.8182 time: 0.21s
Epoch 83/1000, LR 0.000266
Train loss: 0.4354;  Loss pred: 0.4354; Loss self: 0.0000; time: 0.31s
Val loss: 0.4590 score: 0.8605 time: 0.25s
Test loss: 0.4698 score: 0.8182 time: 0.20s
Epoch 84/1000, LR 0.000266
Train loss: 0.4162;  Loss pred: 0.4162; Loss self: 0.0000; time: 0.33s
Val loss: 0.4500 score: 0.8605 time: 0.24s
Test loss: 0.4601 score: 0.8182 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.4084;  Loss pred: 0.4084; Loss self: 0.0000; time: 0.27s
Val loss: 0.4411 score: 0.8605 time: 0.27s
Test loss: 0.4503 score: 0.8182 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.3960;  Loss pred: 0.3960; Loss self: 0.0000; time: 0.34s
Val loss: 0.4326 score: 0.8605 time: 0.24s
Test loss: 0.4409 score: 0.8409 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.3724;  Loss pred: 0.3724; Loss self: 0.0000; time: 0.27s
Val loss: 0.4245 score: 0.8372 time: 0.26s
Test loss: 0.4322 score: 0.8409 time: 0.19s
Epoch 88/1000, LR 0.000266
Train loss: 0.3502;  Loss pred: 0.3502; Loss self: 0.0000; time: 0.36s
Val loss: 0.4171 score: 0.8372 time: 0.27s
Test loss: 0.4248 score: 0.8409 time: 0.33s
Epoch 89/1000, LR 0.000266
Train loss: 0.3526;  Loss pred: 0.3526; Loss self: 0.0000; time: 0.29s
Val loss: 0.4103 score: 0.8140 time: 0.28s
Test loss: 0.4182 score: 0.8182 time: 0.21s
Epoch 90/1000, LR 0.000266
Train loss: 0.3370;  Loss pred: 0.3370; Loss self: 0.0000; time: 0.34s
Val loss: 0.4038 score: 0.8372 time: 0.25s
Test loss: 0.4119 score: 0.8182 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.3263;  Loss pred: 0.3263; Loss self: 0.0000; time: 0.27s
Val loss: 0.3978 score: 0.8372 time: 0.26s
Test loss: 0.4062 score: 0.8409 time: 0.19s
Epoch 92/1000, LR 0.000266
Train loss: 0.3143;  Loss pred: 0.3143; Loss self: 0.0000; time: 0.35s
Val loss: 0.3920 score: 0.8372 time: 0.25s
Test loss: 0.4007 score: 0.8636 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 0.2919;  Loss pred: 0.2919; Loss self: 0.0000; time: 0.39s
Val loss: 0.3860 score: 0.8372 time: 0.27s
Test loss: 0.3944 score: 0.8636 time: 0.21s
Epoch 94/1000, LR 0.000265
Train loss: 0.2934;  Loss pred: 0.2934; Loss self: 0.0000; time: 0.34s
Val loss: 0.3807 score: 0.8372 time: 0.25s
Test loss: 0.3889 score: 0.8636 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.2647;  Loss pred: 0.2647; Loss self: 0.0000; time: 0.33s
Val loss: 0.3753 score: 0.8372 time: 0.26s
Test loss: 0.3827 score: 0.8636 time: 0.22s
Epoch 96/1000, LR 0.000265
Train loss: 0.2748;  Loss pred: 0.2748; Loss self: 0.0000; time: 0.40s
Val loss: 0.3694 score: 0.8372 time: 0.27s
Test loss: 0.3748 score: 0.8636 time: 0.21s
Epoch 97/1000, LR 0.000265
Train loss: 0.2610;  Loss pred: 0.2610; Loss self: 0.0000; time: 0.30s
Val loss: 0.3646 score: 0.8372 time: 0.25s
Test loss: 0.3684 score: 0.8636 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.2269;  Loss pred: 0.2269; Loss self: 0.0000; time: 0.34s
Val loss: 0.3610 score: 0.8372 time: 0.25s
Test loss: 0.3642 score: 0.8409 time: 0.20s
Epoch 99/1000, LR 0.000265
Train loss: 0.2189;  Loss pred: 0.2189; Loss self: 0.0000; time: 0.32s
Val loss: 0.3591 score: 0.8372 time: 0.24s
Test loss: 0.3630 score: 0.8409 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.2188;  Loss pred: 0.2188; Loss self: 0.0000; time: 0.34s
Val loss: 0.3592 score: 0.8372 time: 0.27s
Test loss: 0.3645 score: 0.8409 time: 0.20s
     INFO: Early stopping counter 1 of 2
Epoch 101/1000, LR 0.000265
Train loss: 0.1957;  Loss pred: 0.1957; Loss self: 0.0000; time: 0.35s
Val loss: 0.3597 score: 0.8372 time: 0.24s
Test loss: 0.3660 score: 0.8182 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 098,   Train_Loss: 0.2189,   Val_Loss: 0.3591,   Val_Precision: 0.8261,   Val_Recall: 0.8636,   Val_accuracy: 0.8444,   Val_Score: 0.8372,   Val_Loss: 0.3591,   Test_Precision: 0.8000,   Test_Recall: 0.9091,   Test_accuracy: 0.8511,   Test_Score: 0.8409,   Test_loss: 0.3630


[0.23379505902994424, 0.20276715804357082, 0.23171440698206425, 0.20047628902830184, 0.22950230410788208, 0.19515630009118468, 0.21669817401561886, 0.20439003000501543, 0.21178645093459636, 0.2205719139892608, 0.20068409002851695, 0.2316785310395062, 0.35345249506644905, 0.2368146589724347, 0.23003412503749132, 0.19495943107176572, 0.21418365801218897, 0.233204395044595, 0.2047619940713048, 0.23235565901268274, 0.212342860060744, 0.22987492301035672, 0.19457893399521708, 0.21409163100179285, 0.23201430204790086, 0.20824944600462914, 0.2192484389524907, 0.20037360000424087, 0.21707503299694508, 0.19751151592936367, 0.3121983199380338, 0.2097845160169527, 0.2159446309087798, 0.21335967094637454, 0.21493532799649984, 0.22876782796811312, 0.20959361002314836, 0.3205054629361257, 0.20158674498088658, 0.23186935298144817, 0.18928307003807276, 0.20823949505575, 0.21650661097373813, 0.214922665967606, 0.31421202002093196, 0.20417309598997235, 0.22936302598100156, 0.21424906409811229, 0.2331376860383898, 0.20926417806185782, 0.2262612299527973, 0.2085081769619137, 0.226179537945427, 0.21066956198774278, 0.22496848506852984, 0.21203006501309574, 0.21371461101807654, 0.21836581104435027, 0.21187443402595818, 0.231906607048586, 0.21119934495072812, 0.22833663097117096, 0.20600728096906096, 0.23060898506082594, 0.20644967397674918, 0.3174274790799245, 0.19691102905198932, 0.25507753901183605, 0.26817020296584815, 0.21830244699958712, 0.18812865810468793, 0.21765924303326756, 0.18598705297335982, 0.17773374600801617, 0.17706275393720716, 0.1844470640644431, 0.1996261270251125, 0.2293589919572696, 0.20138115296140313, 0.22975606506224722, 0.2224964089691639, 0.21335216297302395, 0.20322150900028646, 0.2391045429976657, 0.22095006797462702, 0.23085882898885757, 0.19841732899658382, 0.32987157197203487, 0.21501992805860937, 0.2310473220422864, 0.19747954094782472, 0.2202677900204435, 0.2170792620163411, 0.22200949606485665, 0.22917207493446767, 0.21601268404629081, 0.23602291499264538, 0.2003719520289451, 0.22576245397794992, 0.2062133230501786, 0.2382234790129587]
[0.005313524068862369, 0.004608344500990246, 0.005266236522319642, 0.004556279296097769, 0.00521596145699732, 0.004435370456617834, 0.004924958500354974, 0.004645227954659442, 0.004813328430331735, 0.005012998045210473, 0.004561002046102658, 0.005265421159988778, 0.008033011251510206, 0.005382151340282607, 0.005228048296306621, 0.004430896160721948, 0.004867810409367931, 0.0053000998873771596, 0.0046536816834387455, 0.005280810432106426, 0.004825974092289636, 0.0052244300684171985, 0.004422248499891298, 0.004865718886404383, 0.005273052319270474, 0.004732941954650662, 0.0049829190671020615, 0.004553945454641838, 0.004933523477203297, 0.00448889808930372, 0.007095416362228041, 0.004767829909476198, 0.004907832520654087, 0.004849083430599421, 0.004884893818102269, 0.005199268817457117, 0.0047634911368897356, 0.007284215066730129, 0.004581516931383786, 0.00526975802230564, 0.004301887955410744, 0.004732715796721591, 0.004920604794857685, 0.004884606044718318, 0.007141182273202999, 0.004640297636135735, 0.005212796045022763, 0.004869296911320734, 0.005298583773599768, 0.004756004046860405, 0.005142300680745393, 0.004738822203679857, 0.00514044404421425, 0.004787944590630518, 0.0051129201151938605, 0.004818865113933994, 0.00485715025041083, 0.004962859341917051, 0.004815328046044504, 0.005270604705649682, 0.004799985112516548, 0.005189468885708431, 0.004681983658387749, 0.005241113296836953, 0.0046920380449261175, 0.0072142608881801025, 0.004475250660272484, 0.005797216795723547, 0.006094777340132912, 0.004961419249990617, 0.004275651320561089, 0.0049468009780288084, 0.004226978476667268, 0.004039403318364004, 0.00402415349857289, 0.004191978728737344, 0.004536957432388921, 0.0052127043626652185, 0.0045768443854864345, 0.005221728751414709, 0.005056736567480998, 0.004848912794841453, 0.00461867065909742, 0.0054341941590378565, 0.005021592453968796, 0.005246791567928581, 0.00450948474992236, 0.007497081181182611, 0.0048868165467865765, 0.005251075500961055, 0.004488171385177834, 0.005006086136828261, 0.00493361959128048, 0.005045670365110378, 0.005208456248510629, 0.004909379182870246, 0.005364157158923758, 0.004553908000657843, 0.005130964863135226, 0.004686666432958605, 0.005414169977567243]
[188.19901576433455, 216.99766581797846, 189.8889265155006, 219.47732678645298, 191.7192080969999, 225.46031042524106, 203.04739622230795, 215.27468829531608, 207.75644431374897, 199.48142628051127, 219.25006608021422, 189.91833124363626, 124.48631885234819, 185.79930900781616, 191.2759682626602, 225.6879790739815, 205.43117251968872, 188.67568937363296, 214.88362720611994, 189.3648736035232, 207.21205312678333, 191.40843822280536, 226.12930956380694, 205.51947684321922, 189.64348150794564, 211.28507587492902, 200.68557938300503, 219.58980623729246, 202.6948902991493, 222.77182063518657, 140.93605631424688, 209.73902571743835, 203.75593417086003, 206.22454002124368, 204.71274038633857, 192.33473688500007, 209.93006416150027, 137.28315142250435, 218.26831920011335, 189.7620338101363, 232.4560775094664, 211.29517236017256, 203.22705067577417, 204.72480090411617, 140.0328351444634, 215.50341775765105, 191.83562743737335, 205.3684583651242, 188.72967621697467, 210.2605443870749, 194.46548579789518, 211.02289915487142, 194.5357232563469, 208.85788903173406, 195.58295014786952, 207.5177404547907, 205.8820395591875, 201.49674433725144, 207.6701712610086, 189.73154995442496, 208.33397949347338, 192.6979469428858, 213.58468396370955, 190.79915723316012, 213.12700161955877, 138.61433839167188, 223.45117087566956, 172.4965677905428, 164.0749028541529, 201.55523039136258, 233.8824953267637, 202.1508454537579, 236.57560726177223, 247.56131566604975, 248.49946711889496, 238.55082878753245, 220.41203050773458, 191.83900149071704, 218.49115149535902, 191.5074580863804, 197.75600066470298, 206.23179716984316, 216.51251492251234, 184.01992470895695, 199.1400156756357, 190.5926673574337, 221.7548246542395, 133.38524364788287, 204.63219570981641, 190.4371780251454, 222.80789082665058, 199.75685049510085, 202.69094150821192, 198.18972061963547, 191.99546896183537, 203.69174242828697, 186.4225768136584, 219.59161227138168, 194.8951175216117, 213.3712766429419, 184.70051811142645]
Elapsed: 0.22201295857234757~0.03015923319331991
Time per graph: 0.0050457490584624445~0.0006854371180299979
Speed: 201.1182216903678~22.074429855318233
Total Time: 0.2387
best val loss: 0.3591155707836151 test_score: 0.8409

Testing...
Test loss: 0.4991 score: 0.8182 time: 0.23s
test Score 0.8182
Epoch Time List: [0.7912269990192726, 0.8504630500683561, 0.7699238790664822, 0.7307065869681537, 0.8664836249081418, 0.8023839630186558, 0.801322272978723, 0.744461297057569, 0.7259293330134824, 0.9023134288145229, 0.8198414910584688, 0.7898966569919139, 0.8626002121018246, 1.0587109461193904, 0.7989897489314899, 0.7340424091089517, 0.8224723039893433, 0.8046348779462278, 0.763363633188419, 0.8006022449117154, 0.7502125070896, 0.8942237239098176, 0.7260744191007689, 0.7798488080734387, 0.8244283848907799, 0.7975586081156507, 0.784965481958352, 0.7498563909903169, 0.7774221090367064, 0.7921960230451077, 0.8905185529729351, 0.7530310319270939, 0.7803475098917261, 0.760377602186054, 0.7350020391168073, 0.8618523558834568, 0.7626998840132728, 0.9197040060535073, 0.7543218028731644, 0.8094374119536951, 0.6993014530744404, 0.8254979610210285, 0.7905985580291599, 0.7838428380200639, 0.8767616130644456, 0.7868294421350583, 0.7836813719477504, 0.7329800759907812, 0.8301194069208577, 0.7482515439623967, 0.8728915560059249, 0.7889121830230579, 0.8811418521218002, 0.7717918639536947, 0.8961595289874822, 0.7845567950280383, 0.8857993041165173, 0.7807101059006527, 0.811533579020761, 0.7851270161336288, 0.7439471769612283, 0.8424407279817387, 0.7427519590128213, 1.0251477651763707, 0.7764840049203485, 0.9141849658917636, 0.77310079196468, 0.9748072050279006, 0.8307198388502002, 0.915519735077396, 0.7198613660875708, 0.703710075118579, 0.8595428459811956, 0.6404848380479962, 0.6305229231948033, 0.6482257081661373, 0.8547220329055563, 0.7787925900192931, 0.8856178311398253, 0.7795212911441922, 1.0130722511094064, 0.7744764208327979, 0.7608968269778416, 0.7986939699621871, 0.7560391909210011, 0.812858929974027, 0.7296778169693425, 0.9571004960453138, 0.7712677870877087, 0.816897195065394, 0.7297419920796528, 0.8179621760500595, 0.8682248760014772, 0.813436417025514, 0.8098351111402735, 0.8808826141757891, 0.7820466079283506, 0.7873500789282843, 0.7816660599783063, 0.810152544057928, 0.8230005459627137]
Total Epoch List: [101]
Total Time List: [0.23873791506048292]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcf460>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.19s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.26s
     INFO: Early stopping counter 1 of 2
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 004,   Train_Loss: 0.6929,   Val_Loss: 0.6932,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.5000,   Val_Loss: 0.6932,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.4884,   Test_loss: 0.6935


[0.23379505902994424, 0.20276715804357082, 0.23171440698206425, 0.20047628902830184, 0.22950230410788208, 0.19515630009118468, 0.21669817401561886, 0.20439003000501543, 0.21178645093459636, 0.2205719139892608, 0.20068409002851695, 0.2316785310395062, 0.35345249506644905, 0.2368146589724347, 0.23003412503749132, 0.19495943107176572, 0.21418365801218897, 0.233204395044595, 0.2047619940713048, 0.23235565901268274, 0.212342860060744, 0.22987492301035672, 0.19457893399521708, 0.21409163100179285, 0.23201430204790086, 0.20824944600462914, 0.2192484389524907, 0.20037360000424087, 0.21707503299694508, 0.19751151592936367, 0.3121983199380338, 0.2097845160169527, 0.2159446309087798, 0.21335967094637454, 0.21493532799649984, 0.22876782796811312, 0.20959361002314836, 0.3205054629361257, 0.20158674498088658, 0.23186935298144817, 0.18928307003807276, 0.20823949505575, 0.21650661097373813, 0.214922665967606, 0.31421202002093196, 0.20417309598997235, 0.22936302598100156, 0.21424906409811229, 0.2331376860383898, 0.20926417806185782, 0.2262612299527973, 0.2085081769619137, 0.226179537945427, 0.21066956198774278, 0.22496848506852984, 0.21203006501309574, 0.21371461101807654, 0.21836581104435027, 0.21187443402595818, 0.231906607048586, 0.21119934495072812, 0.22833663097117096, 0.20600728096906096, 0.23060898506082594, 0.20644967397674918, 0.3174274790799245, 0.19691102905198932, 0.25507753901183605, 0.26817020296584815, 0.21830244699958712, 0.18812865810468793, 0.21765924303326756, 0.18598705297335982, 0.17773374600801617, 0.17706275393720716, 0.1844470640644431, 0.1996261270251125, 0.2293589919572696, 0.20138115296140313, 0.22975606506224722, 0.2224964089691639, 0.21335216297302395, 0.20322150900028646, 0.2391045429976657, 0.22095006797462702, 0.23085882898885757, 0.19841732899658382, 0.32987157197203487, 0.21501992805860937, 0.2310473220422864, 0.19747954094782472, 0.2202677900204435, 0.2170792620163411, 0.22200949606485665, 0.22917207493446767, 0.21601268404629081, 0.23602291499264538, 0.2003719520289451, 0.22576245397794992, 0.2062133230501786, 0.2382234790129587, 0.20990975003223866, 0.2190065709874034, 0.19521291507408023, 0.21169267897494137, 0.2176629489986226, 0.2641569320112467, 0.21649700799025595]
[0.005313524068862369, 0.004608344500990246, 0.005266236522319642, 0.004556279296097769, 0.00521596145699732, 0.004435370456617834, 0.004924958500354974, 0.004645227954659442, 0.004813328430331735, 0.005012998045210473, 0.004561002046102658, 0.005265421159988778, 0.008033011251510206, 0.005382151340282607, 0.005228048296306621, 0.004430896160721948, 0.004867810409367931, 0.0053000998873771596, 0.0046536816834387455, 0.005280810432106426, 0.004825974092289636, 0.0052244300684171985, 0.004422248499891298, 0.004865718886404383, 0.005273052319270474, 0.004732941954650662, 0.0049829190671020615, 0.004553945454641838, 0.004933523477203297, 0.00448889808930372, 0.007095416362228041, 0.004767829909476198, 0.004907832520654087, 0.004849083430599421, 0.004884893818102269, 0.005199268817457117, 0.0047634911368897356, 0.007284215066730129, 0.004581516931383786, 0.00526975802230564, 0.004301887955410744, 0.004732715796721591, 0.004920604794857685, 0.004884606044718318, 0.007141182273202999, 0.004640297636135735, 0.005212796045022763, 0.004869296911320734, 0.005298583773599768, 0.004756004046860405, 0.005142300680745393, 0.004738822203679857, 0.00514044404421425, 0.004787944590630518, 0.0051129201151938605, 0.004818865113933994, 0.00485715025041083, 0.004962859341917051, 0.004815328046044504, 0.005270604705649682, 0.004799985112516548, 0.005189468885708431, 0.004681983658387749, 0.005241113296836953, 0.0046920380449261175, 0.0072142608881801025, 0.004475250660272484, 0.005797216795723547, 0.006094777340132912, 0.004961419249990617, 0.004275651320561089, 0.0049468009780288084, 0.004226978476667268, 0.004039403318364004, 0.00402415349857289, 0.004191978728737344, 0.004536957432388921, 0.0052127043626652185, 0.0045768443854864345, 0.005221728751414709, 0.005056736567480998, 0.004848912794841453, 0.00461867065909742, 0.0054341941590378565, 0.005021592453968796, 0.005246791567928581, 0.00450948474992236, 0.007497081181182611, 0.0048868165467865765, 0.005251075500961055, 0.004488171385177834, 0.005006086136828261, 0.00493361959128048, 0.005045670365110378, 0.005208456248510629, 0.004909379182870246, 0.005364157158923758, 0.004553908000657843, 0.005130964863135226, 0.004686666432958605, 0.005414169977567243, 0.004881622093772992, 0.005093176069474498, 0.004539835234280936, 0.004923085557556776, 0.005061929046479595, 0.0061431844653778295, 0.005034814139308278]
[188.19901576433455, 216.99766581797846, 189.8889265155006, 219.47732678645298, 191.7192080969999, 225.46031042524106, 203.04739622230795, 215.27468829531608, 207.75644431374897, 199.48142628051127, 219.25006608021422, 189.91833124363626, 124.48631885234819, 185.79930900781616, 191.2759682626602, 225.6879790739815, 205.43117251968872, 188.67568937363296, 214.88362720611994, 189.3648736035232, 207.21205312678333, 191.40843822280536, 226.12930956380694, 205.51947684321922, 189.64348150794564, 211.28507587492902, 200.68557938300503, 219.58980623729246, 202.6948902991493, 222.77182063518657, 140.93605631424688, 209.73902571743835, 203.75593417086003, 206.22454002124368, 204.71274038633857, 192.33473688500007, 209.93006416150027, 137.28315142250435, 218.26831920011335, 189.7620338101363, 232.4560775094664, 211.29517236017256, 203.22705067577417, 204.72480090411617, 140.0328351444634, 215.50341775765105, 191.83562743737335, 205.3684583651242, 188.72967621697467, 210.2605443870749, 194.46548579789518, 211.02289915487142, 194.5357232563469, 208.85788903173406, 195.58295014786952, 207.5177404547907, 205.8820395591875, 201.49674433725144, 207.6701712610086, 189.73154995442496, 208.33397949347338, 192.6979469428858, 213.58468396370955, 190.79915723316012, 213.12700161955877, 138.61433839167188, 223.45117087566956, 172.4965677905428, 164.0749028541529, 201.55523039136258, 233.8824953267637, 202.1508454537579, 236.57560726177223, 247.56131566604975, 248.49946711889496, 238.55082878753245, 220.41203050773458, 191.83900149071704, 218.49115149535902, 191.5074580863804, 197.75600066470298, 206.23179716984316, 216.51251492251234, 184.01992470895695, 199.1400156756357, 190.5926673574337, 221.7548246542395, 133.38524364788287, 204.63219570981641, 190.4371780251454, 222.80789082665058, 199.75685049510085, 202.69094150821192, 198.18972061963547, 191.99546896183537, 203.69174242828697, 186.4225768136584, 219.59161227138168, 194.8951175216117, 213.3712766429419, 184.70051811142645, 204.84994143147668, 196.3411408440034, 220.27231130523396, 203.1246437440098, 197.55314442731412, 162.78202382426687, 198.61706357593326]
Elapsed: 0.22182821870255456~0.02960692602783284
Time per graph: 0.005049058347323685~0.0006732726424818562
Speed: 200.89333944332768~21.753276629555984
Total Time: 0.2170
best val loss: 0.6932044625282288 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.20s
test Score 0.4884
Epoch Time List: [0.7912269990192726, 0.8504630500683561, 0.7699238790664822, 0.7307065869681537, 0.8664836249081418, 0.8023839630186558, 0.801322272978723, 0.744461297057569, 0.7259293330134824, 0.9023134288145229, 0.8198414910584688, 0.7898966569919139, 0.8626002121018246, 1.0587109461193904, 0.7989897489314899, 0.7340424091089517, 0.8224723039893433, 0.8046348779462278, 0.763363633188419, 0.8006022449117154, 0.7502125070896, 0.8942237239098176, 0.7260744191007689, 0.7798488080734387, 0.8244283848907799, 0.7975586081156507, 0.784965481958352, 0.7498563909903169, 0.7774221090367064, 0.7921960230451077, 0.8905185529729351, 0.7530310319270939, 0.7803475098917261, 0.760377602186054, 0.7350020391168073, 0.8618523558834568, 0.7626998840132728, 0.9197040060535073, 0.7543218028731644, 0.8094374119536951, 0.6993014530744404, 0.8254979610210285, 0.7905985580291599, 0.7838428380200639, 0.8767616130644456, 0.7868294421350583, 0.7836813719477504, 0.7329800759907812, 0.8301194069208577, 0.7482515439623967, 0.8728915560059249, 0.7889121830230579, 0.8811418521218002, 0.7717918639536947, 0.8961595289874822, 0.7845567950280383, 0.8857993041165173, 0.7807101059006527, 0.811533579020761, 0.7851270161336288, 0.7439471769612283, 0.8424407279817387, 0.7427519590128213, 1.0251477651763707, 0.7764840049203485, 0.9141849658917636, 0.77310079196468, 0.9748072050279006, 0.8307198388502002, 0.915519735077396, 0.7198613660875708, 0.703710075118579, 0.8595428459811956, 0.6404848380479962, 0.6305229231948033, 0.6482257081661373, 0.8547220329055563, 0.7787925900192931, 0.8856178311398253, 0.7795212911441922, 1.0130722511094064, 0.7744764208327979, 0.7608968269778416, 0.7986939699621871, 0.7560391909210011, 0.812858929974027, 0.7296778169693425, 0.9571004960453138, 0.7712677870877087, 0.816897195065394, 0.7297419920796528, 0.8179621760500595, 0.8682248760014772, 0.813436417025514, 0.8098351111402735, 0.8808826141757891, 0.7820466079283506, 0.7873500789282843, 0.7816660599783063, 0.810152544057928, 0.8230005459627137, 0.8073698360240087, 0.9788435650989413, 0.7446791509864852, 0.8860555901192129, 0.7872609379701316, 0.9190492201596498, 0.7851988420588896]
Total Epoch List: [101, 7]
Total Time List: [0.23873791506048292, 0.21703661198262125]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcfd00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5116 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.7023;  Loss pred: 0.7023; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5116 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.7022;  Loss pred: 0.7022; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5116 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.7021;  Loss pred: 0.7021; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.7019;  Loss pred: 0.7019; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5116 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.7016;  Loss pred: 0.7016; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5116 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.7012;  Loss pred: 0.7012; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5116 time: 0.26s
Epoch 8/1000, LR 0.000180
Train loss: 0.7008;  Loss pred: 0.7008; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5116 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5116 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5116 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6989;  Loss pred: 0.6989; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5116 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.34s
Epoch 17/1000, LR 0.000270
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.32s
Epoch 25/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.36s
Val loss: 0.6932 score: 0.5227 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.32s
Val loss: 0.6931 score: 0.5227 time: 0.20s
Test loss: 0.6908 score: 0.5349 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.39s
Val loss: 0.6930 score: 0.5682 time: 0.17s
Test loss: 0.6906 score: 0.6977 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.33s
Val loss: 0.6929 score: 0.5455 time: 0.19s
Test loss: 0.6904 score: 0.6512 time: 0.23s
Epoch 31/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.38s
Val loss: 0.6927 score: 0.5455 time: 0.18s
Test loss: 0.6902 score: 0.5581 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.48s
Val loss: 0.6926 score: 0.4091 time: 0.20s
Test loss: 0.6900 score: 0.5349 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.39s
Val loss: 0.6925 score: 0.4318 time: 0.18s
Test loss: 0.6898 score: 0.5116 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.36s
Val loss: 0.6923 score: 0.4545 time: 0.16s
Test loss: 0.6895 score: 0.5116 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.32s
Val loss: 0.6921 score: 0.4773 time: 0.18s
Test loss: 0.6891 score: 0.5349 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.41s
Val loss: 0.6918 score: 0.4773 time: 0.19s
Test loss: 0.6888 score: 0.5349 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.37s
Val loss: 0.6916 score: 0.5455 time: 0.19s
Test loss: 0.6883 score: 0.5581 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.20s
Test loss: 0.6878 score: 0.5349 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.27s
Test loss: 0.6871 score: 0.5116 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.4884 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.4884 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.4884 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6833 score: 0.4884 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4884 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6807 score: 0.4884 time: 0.21s
Epoch 46/1000, LR 0.000269
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.4884 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6774 score: 0.4884 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6755 score: 0.4884 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6819 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6733 score: 0.4884 time: 0.35s
Epoch 50/1000, LR 0.000269
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6803 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6708 score: 0.4884 time: 0.24s
Epoch 51/1000, LR 0.000269
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6681 score: 0.4884 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6631;  Loss pred: 0.6631; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6764 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6651 score: 0.4884 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.6597;  Loss pred: 0.6597; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6741 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6618 score: 0.4884 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6574;  Loss pred: 0.6574; Loss self: 0.0000; time: 0.39s
Val loss: 0.6716 score: 0.5227 time: 0.16s
Test loss: 0.6582 score: 0.5116 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6529;  Loss pred: 0.6529; Loss self: 0.0000; time: 0.45s
Val loss: 0.6689 score: 0.5227 time: 0.22s
Test loss: 0.6542 score: 0.5116 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.38s
Val loss: 0.6661 score: 0.5227 time: 0.18s
Test loss: 0.6499 score: 0.5581 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.6441;  Loss pred: 0.6441; Loss self: 0.0000; time: 0.40s
Val loss: 0.6629 score: 0.5227 time: 0.19s
Test loss: 0.6453 score: 0.5814 time: 0.35s
Epoch 58/1000, LR 0.000269
Train loss: 0.6368;  Loss pred: 0.6368; Loss self: 0.0000; time: 0.40s
Val loss: 0.6595 score: 0.5682 time: 0.18s
Test loss: 0.6403 score: 0.6047 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.6327;  Loss pred: 0.6327; Loss self: 0.0000; time: 0.32s
Val loss: 0.6558 score: 0.5909 time: 0.20s
Test loss: 0.6351 score: 0.6047 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.6307;  Loss pred: 0.6307; Loss self: 0.0000; time: 0.39s
Val loss: 0.6517 score: 0.5909 time: 0.17s
Test loss: 0.6291 score: 0.6279 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.34s
Val loss: 0.6471 score: 0.6364 time: 0.20s
Test loss: 0.6227 score: 0.6744 time: 0.22s
Epoch 62/1000, LR 0.000268
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 0.39s
Val loss: 0.6422 score: 0.6591 time: 0.16s
Test loss: 0.6158 score: 0.6977 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.36s
Val loss: 0.6367 score: 0.6591 time: 0.19s
Test loss: 0.6082 score: 0.6977 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.6000;  Loss pred: 0.6000; Loss self: 0.0000; time: 0.37s
Val loss: 0.6310 score: 0.7045 time: 0.19s
Test loss: 0.6003 score: 0.6977 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.5880;  Loss pred: 0.5880; Loss self: 0.0000; time: 0.38s
Val loss: 0.6248 score: 0.7273 time: 0.20s
Test loss: 0.5919 score: 0.6977 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.5850;  Loss pred: 0.5850; Loss self: 0.0000; time: 0.44s
Val loss: 0.6180 score: 0.7500 time: 0.18s
Test loss: 0.5828 score: 0.6977 time: 0.22s
Epoch 67/1000, LR 0.000268
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.38s
Val loss: 0.6107 score: 0.7500 time: 0.19s
Test loss: 0.5730 score: 0.7442 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.31s
Val loss: 0.6029 score: 0.7273 time: 0.20s
Test loss: 0.5626 score: 0.7442 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.5479;  Loss pred: 0.5479; Loss self: 0.0000; time: 0.37s
Val loss: 0.5949 score: 0.7500 time: 0.17s
Test loss: 0.5520 score: 0.7442 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.34s
Val loss: 0.5864 score: 0.7727 time: 0.19s
Test loss: 0.5408 score: 0.7442 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.5299;  Loss pred: 0.5299; Loss self: 0.0000; time: 0.46s
Val loss: 0.5777 score: 0.7955 time: 0.18s
Test loss: 0.5295 score: 0.7674 time: 0.25s
Epoch 72/1000, LR 0.000267
Train loss: 0.5099;  Loss pred: 0.5099; Loss self: 0.0000; time: 0.36s
Val loss: 0.5688 score: 0.7955 time: 0.17s
Test loss: 0.5185 score: 0.7674 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.5058;  Loss pred: 0.5058; Loss self: 0.0000; time: 0.42s
Val loss: 0.5596 score: 0.7955 time: 0.19s
Test loss: 0.5072 score: 0.7674 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.4870;  Loss pred: 0.4870; Loss self: 0.0000; time: 0.47s
Val loss: 0.5501 score: 0.7727 time: 0.18s
Test loss: 0.4957 score: 0.7674 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.4763;  Loss pred: 0.4763; Loss self: 0.0000; time: 0.59s
Val loss: 0.5399 score: 0.8182 time: 0.20s
Test loss: 0.4837 score: 0.7674 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.4817;  Loss pred: 0.4817; Loss self: 0.0000; time: 0.33s
Val loss: 0.5289 score: 0.8409 time: 0.18s
Test loss: 0.4707 score: 0.7674 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.4493;  Loss pred: 0.4493; Loss self: 0.0000; time: 0.37s
Val loss: 0.5184 score: 0.8409 time: 0.18s
Test loss: 0.4587 score: 0.7674 time: 0.21s
Epoch 78/1000, LR 0.000267
Train loss: 0.4413;  Loss pred: 0.4413; Loss self: 0.0000; time: 0.36s
Val loss: 0.5076 score: 0.8636 time: 0.17s
Test loss: 0.4464 score: 0.7907 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.4126;  Loss pred: 0.4126; Loss self: 0.0000; time: 0.41s
Val loss: 0.4971 score: 0.8636 time: 0.19s
Test loss: 0.4348 score: 0.8140 time: 0.28s
Epoch 80/1000, LR 0.000267
Train loss: 0.4121;  Loss pred: 0.4121; Loss self: 0.0000; time: 0.38s
Val loss: 0.4863 score: 0.8636 time: 0.17s
Test loss: 0.4228 score: 0.8140 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.3896;  Loss pred: 0.3896; Loss self: 0.0000; time: 0.39s
Val loss: 0.4763 score: 0.8636 time: 0.20s
Test loss: 0.4123 score: 0.8372 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.3781;  Loss pred: 0.3781; Loss self: 0.0000; time: 0.47s
Val loss: 0.4670 score: 0.8636 time: 0.21s
Test loss: 0.4036 score: 0.8605 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.3536;  Loss pred: 0.3536; Loss self: 0.0000; time: 0.33s
Val loss: 0.4589 score: 0.8636 time: 0.19s
Test loss: 0.3974 score: 0.8372 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.3823;  Loss pred: 0.3823; Loss self: 0.0000; time: 0.38s
Val loss: 0.4505 score: 0.8636 time: 0.17s
Test loss: 0.3904 score: 0.8605 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 0.32s
Val loss: 0.4410 score: 0.8864 time: 0.19s
Test loss: 0.3818 score: 0.8605 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3149;  Loss pred: 0.3149; Loss self: 0.0000; time: 0.40s
Val loss: 0.4299 score: 0.8636 time: 0.18s
Test loss: 0.3702 score: 0.8605 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.3145;  Loss pred: 0.3145; Loss self: 0.0000; time: 0.36s
Val loss: 0.4186 score: 0.8636 time: 0.18s
Test loss: 0.3583 score: 0.8605 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.3071;  Loss pred: 0.3071; Loss self: 0.0000; time: 0.33s
Val loss: 0.4079 score: 0.8636 time: 0.19s
Test loss: 0.3469 score: 0.8372 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.2658;  Loss pred: 0.2658; Loss self: 0.0000; time: 0.38s
Val loss: 0.3976 score: 0.8636 time: 0.17s
Test loss: 0.3362 score: 0.8372 time: 0.24s
Epoch 90/1000, LR 0.000266
Train loss: 0.2601;  Loss pred: 0.2601; Loss self: 0.0000; time: 0.45s
Val loss: 0.3886 score: 0.8636 time: 0.19s
Test loss: 0.3285 score: 0.8372 time: 0.21s
Epoch 91/1000, LR 0.000266
Train loss: 0.2751;  Loss pred: 0.2751; Loss self: 0.0000; time: 0.37s
Val loss: 0.3804 score: 0.8636 time: 0.16s
Test loss: 0.3228 score: 0.8372 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.2476;  Loss pred: 0.2476; Loss self: 0.0000; time: 0.34s
Val loss: 0.3721 score: 0.8636 time: 0.19s
Test loss: 0.3165 score: 0.8372 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 0.2189;  Loss pred: 0.2189; Loss self: 0.0000; time: 0.38s
Val loss: 0.3640 score: 0.8636 time: 0.17s
Test loss: 0.3105 score: 0.8605 time: 0.22s
Epoch 94/1000, LR 0.000265
Train loss: 0.2355;  Loss pred: 0.2355; Loss self: 0.0000; time: 0.43s
Val loss: 0.3549 score: 0.8409 time: 0.18s
Test loss: 0.3010 score: 0.8605 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.2156;  Loss pred: 0.2156; Loss self: 0.0000; time: 0.35s
Val loss: 0.3456 score: 0.8864 time: 0.20s
Test loss: 0.2878 score: 0.8605 time: 0.22s
Epoch 96/1000, LR 0.000265
Train loss: 0.2044;  Loss pred: 0.2044; Loss self: 0.0000; time: 0.40s
Val loss: 0.3383 score: 0.8636 time: 0.17s
Test loss: 0.2780 score: 0.8837 time: 0.27s
Epoch 97/1000, LR 0.000265
Train loss: 0.1952;  Loss pred: 0.1952; Loss self: 0.0000; time: 0.31s
Val loss: 0.3320 score: 0.8636 time: 0.20s
Test loss: 0.2709 score: 0.8837 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.1753;  Loss pred: 0.1753; Loss self: 0.0000; time: 0.47s
Val loss: 0.3260 score: 0.8636 time: 0.18s
Test loss: 0.2678 score: 0.8837 time: 0.25s
Epoch 99/1000, LR 0.000265
Train loss: 0.1827;  Loss pred: 0.1827; Loss self: 0.0000; time: 0.31s
Val loss: 0.3200 score: 0.8636 time: 0.20s
Test loss: 0.2700 score: 0.8837 time: 0.23s
Epoch 100/1000, LR 0.000265
Train loss: 0.1606;  Loss pred: 0.1606; Loss self: 0.0000; time: 0.37s
Val loss: 0.3151 score: 0.8864 time: 0.17s
Test loss: 0.2799 score: 0.8605 time: 0.24s
Epoch 101/1000, LR 0.000265
Train loss: 0.1784;  Loss pred: 0.1784; Loss self: 0.0000; time: 0.43s
Val loss: 0.3138 score: 0.8864 time: 0.19s
Test loss: 0.2990 score: 0.8372 time: 0.23s
Epoch 102/1000, LR 0.000264
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.37s
Val loss: 0.3137 score: 0.8636 time: 0.17s
Test loss: 0.3160 score: 0.8372 time: 0.24s
Epoch 103/1000, LR 0.000264
Train loss: 0.1529;  Loss pred: 0.1529; Loss self: 0.0000; time: 0.35s
Val loss: 0.3137 score: 0.8409 time: 0.19s
Test loss: 0.3312 score: 0.8140 time: 0.23s
Epoch 104/1000, LR 0.000264
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 0.37s
Val loss: 0.3170 score: 0.8409 time: 0.18s
Test loss: 0.3525 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 105/1000, LR 0.000264
Train loss: 0.1707;  Loss pred: 0.1707; Loss self: 0.0000; time: 0.39s
Val loss: 0.3241 score: 0.8636 time: 0.18s
Test loss: 0.3777 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 102,   Train_Loss: 0.1529,   Val_Loss: 0.3137,   Val_Precision: 0.8571,   Val_Recall: 0.8182,   Val_accuracy: 0.8372,   Val_Score: 0.8409,   Val_Loss: 0.3137,   Test_Precision: 0.7600,   Test_Recall: 0.9048,   Test_accuracy: 0.8261,   Test_Score: 0.8140,   Test_loss: 0.3312


[0.23379505902994424, 0.20276715804357082, 0.23171440698206425, 0.20047628902830184, 0.22950230410788208, 0.19515630009118468, 0.21669817401561886, 0.20439003000501543, 0.21178645093459636, 0.2205719139892608, 0.20068409002851695, 0.2316785310395062, 0.35345249506644905, 0.2368146589724347, 0.23003412503749132, 0.19495943107176572, 0.21418365801218897, 0.233204395044595, 0.2047619940713048, 0.23235565901268274, 0.212342860060744, 0.22987492301035672, 0.19457893399521708, 0.21409163100179285, 0.23201430204790086, 0.20824944600462914, 0.2192484389524907, 0.20037360000424087, 0.21707503299694508, 0.19751151592936367, 0.3121983199380338, 0.2097845160169527, 0.2159446309087798, 0.21335967094637454, 0.21493532799649984, 0.22876782796811312, 0.20959361002314836, 0.3205054629361257, 0.20158674498088658, 0.23186935298144817, 0.18928307003807276, 0.20823949505575, 0.21650661097373813, 0.214922665967606, 0.31421202002093196, 0.20417309598997235, 0.22936302598100156, 0.21424906409811229, 0.2331376860383898, 0.20926417806185782, 0.2262612299527973, 0.2085081769619137, 0.226179537945427, 0.21066956198774278, 0.22496848506852984, 0.21203006501309574, 0.21371461101807654, 0.21836581104435027, 0.21187443402595818, 0.231906607048586, 0.21119934495072812, 0.22833663097117096, 0.20600728096906096, 0.23060898506082594, 0.20644967397674918, 0.3174274790799245, 0.19691102905198932, 0.25507753901183605, 0.26817020296584815, 0.21830244699958712, 0.18812865810468793, 0.21765924303326756, 0.18598705297335982, 0.17773374600801617, 0.17706275393720716, 0.1844470640644431, 0.1996261270251125, 0.2293589919572696, 0.20138115296140313, 0.22975606506224722, 0.2224964089691639, 0.21335216297302395, 0.20322150900028646, 0.2391045429976657, 0.22095006797462702, 0.23085882898885757, 0.19841732899658382, 0.32987157197203487, 0.21501992805860937, 0.2310473220422864, 0.19747954094782472, 0.2202677900204435, 0.2170792620163411, 0.22200949606485665, 0.22917207493446767, 0.21601268404629081, 0.23602291499264538, 0.2003719520289451, 0.22576245397794992, 0.2062133230501786, 0.2382234790129587, 0.20990975003223866, 0.2190065709874034, 0.19521291507408023, 0.21169267897494137, 0.2176629489986226, 0.2641569320112467, 0.21649700799025595, 0.2448156150057912, 0.22415870788972825, 0.23718334001023322, 0.24124007404316217, 0.24648034293204546, 0.22488651296589524, 0.2607875280082226, 0.23833243106491864, 0.2402795139933005, 0.22646537399850786, 0.24607926607131958, 0.23260148998815566, 0.23973794793710113, 0.24501548404805362, 0.2322603880893439, 0.3449837670195848, 0.2259252689545974, 0.2522873239358887, 0.21931755798868835, 0.23910896095912904, 0.2351557279471308, 0.2196075120009482, 0.24207305198069662, 0.33000508206896484, 0.24682445998769253, 0.22731006296817213, 0.25143039296381176, 0.2321461079409346, 0.24396445602178574, 0.23103090899530798, 0.22189500194508582, 0.23073788394685835, 0.2253378169843927, 0.2455709729110822, 0.24048732896335423, 0.2360691709909588, 0.25700147298630327, 0.2372679190011695, 0.25112219294533134, 0.2252619529608637, 0.23823956097476184, 0.22624481900129467, 0.23523260094225407, 0.2492547109723091, 0.2180062310071662, 0.24971676501445472, 0.23065933503676206, 0.2265060431091115, 0.3520829579792917, 0.24847349908668548, 0.23250211996491998, 0.24278690805658698, 0.2357626329176128, 0.2388866349356249, 0.22020558500662446, 0.24234396603424102, 0.3513341649668291, 0.24937323108315468, 0.24692392896395177, 0.24841619096696377, 0.22783360606990755, 0.24161606596317142, 0.24996045301668346, 0.22939521400257945, 0.24930359807331115, 0.22643056395463645, 0.25857066502794623, 0.24277915200218558, 0.2466200889321044, 0.23145521595142782, 0.2502511920174584, 0.2418012369889766, 0.22915219189599156, 0.23520191991701722, 0.23494692600797862, 0.2342834259616211, 0.2181488439673558, 0.24400683108251542, 0.280605512089096, 0.26801045308820903, 0.2395383670227602, 0.2395462580025196, 0.23457036097534, 0.24511356896255165, 0.2305701810400933, 0.23325427598319948, 0.23516429995652288, 0.2337700540665537, 0.24540623906068504, 0.21816876309458166, 0.2487021730048582, 0.22676032199524343, 0.22291479795239866, 0.23919254692737013, 0.22094762104097754, 0.27200146799441427, 0.23171347100287676, 0.250291622010991, 0.23668641597032547, 0.24583923199679703, 0.23778033896815032, 0.2423310950398445, 0.23597995890304446, 0.23385872703511268, 0.23558898793999106]
[0.005313524068862369, 0.004608344500990246, 0.005266236522319642, 0.004556279296097769, 0.00521596145699732, 0.004435370456617834, 0.004924958500354974, 0.004645227954659442, 0.004813328430331735, 0.005012998045210473, 0.004561002046102658, 0.005265421159988778, 0.008033011251510206, 0.005382151340282607, 0.005228048296306621, 0.004430896160721948, 0.004867810409367931, 0.0053000998873771596, 0.0046536816834387455, 0.005280810432106426, 0.004825974092289636, 0.0052244300684171985, 0.004422248499891298, 0.004865718886404383, 0.005273052319270474, 0.004732941954650662, 0.0049829190671020615, 0.004553945454641838, 0.004933523477203297, 0.00448889808930372, 0.007095416362228041, 0.004767829909476198, 0.004907832520654087, 0.004849083430599421, 0.004884893818102269, 0.005199268817457117, 0.0047634911368897356, 0.007284215066730129, 0.004581516931383786, 0.00526975802230564, 0.004301887955410744, 0.004732715796721591, 0.004920604794857685, 0.004884606044718318, 0.007141182273202999, 0.004640297636135735, 0.005212796045022763, 0.004869296911320734, 0.005298583773599768, 0.004756004046860405, 0.005142300680745393, 0.004738822203679857, 0.00514044404421425, 0.004787944590630518, 0.0051129201151938605, 0.004818865113933994, 0.00485715025041083, 0.004962859341917051, 0.004815328046044504, 0.005270604705649682, 0.004799985112516548, 0.005189468885708431, 0.004681983658387749, 0.005241113296836953, 0.0046920380449261175, 0.0072142608881801025, 0.004475250660272484, 0.005797216795723547, 0.006094777340132912, 0.004961419249990617, 0.004275651320561089, 0.0049468009780288084, 0.004226978476667268, 0.004039403318364004, 0.00402415349857289, 0.004191978728737344, 0.004536957432388921, 0.0052127043626652185, 0.0045768443854864345, 0.005221728751414709, 0.005056736567480998, 0.004848912794841453, 0.00461867065909742, 0.0054341941590378565, 0.005021592453968796, 0.005246791567928581, 0.00450948474992236, 0.007497081181182611, 0.0048868165467865765, 0.005251075500961055, 0.004488171385177834, 0.005006086136828261, 0.00493361959128048, 0.005045670365110378, 0.005208456248510629, 0.004909379182870246, 0.005364157158923758, 0.004553908000657843, 0.005130964863135226, 0.004686666432958605, 0.005414169977567243, 0.004881622093772992, 0.005093176069474498, 0.004539835234280936, 0.004923085557556776, 0.005061929046479595, 0.0061431844653778295, 0.005034814139308278, 0.005693386395483516, 0.005212993206737866, 0.005515891628144959, 0.0056102342800735385, 0.005732100998419662, 0.00522991890618361, 0.006064826232749362, 0.005542614675928341, 0.0055878956742628026, 0.005266636604616462, 0.005722773629565572, 0.005409336976468736, 0.005575301114816305, 0.005698034512745433, 0.005401404374170788, 0.00802287830278104, 0.00525407602219994, 0.005867147068276481, 0.005100408325318334, 0.005560673510677419, 0.0054687378592356, 0.005107151441882516, 0.005629605860016201, 0.007674536792301508, 0.005740103720644012, 0.005286280534143538, 0.005847218441018878, 0.005398746696300805, 0.005673592000506645, 0.005372811837100186, 0.005160348882443856, 0.005365997301089729, 0.005240414348474249, 0.00571095285839726, 0.005592728580543121, 0.005489980720719973, 0.005976778441541936, 0.005517858581422547, 0.005840050998728636, 0.005238650068857296, 0.005540454906389811, 0.00526150741863476, 0.0054705256033082345, 0.005796621185402537, 0.005069912349003865, 0.005807366628243133, 0.005364170582250281, 0.005267582397886314, 0.008187975766960271, 0.00577845346713222, 0.005407026045695814, 0.005646207164106674, 0.005482851928316577, 0.0055555031380377885, 0.005121060116433127, 0.005635906186842814, 0.00817056197597277, 0.005799377467050109, 0.0057424169526500415, 0.0057771207201619485, 0.005298455955114129, 0.0056189782782132885, 0.005813033791085662, 0.005334772418664638, 0.005797758094728166, 0.005265827068712476, 0.006013271279719679, 0.005646026790748502, 0.005735350905397777, 0.005382679440730879, 0.005819795163196706, 0.00562328458113899, 0.005329120741767245, 0.005469812091093424, 0.005463882000185549, 0.005448451766549328, 0.0050732289294733906, 0.005674577467035242, 0.006525709583467349, 0.006232801234609512, 0.005570659698203726, 0.005570843209360921, 0.005455124673845116, 0.005700315557268643, 0.005362097233490542, 0.005424518046120918, 0.00546893720829123, 0.005436512885268691, 0.005707121838620583, 0.005073692164990271, 0.0057837714652292605, 0.005273495860354498, 0.005184065068660434, 0.005562617370403956, 0.0051383167683948265, 0.00632561553475382, 0.005388685372159924, 0.005820735395604441, 0.005504335255123848, 0.0057171914417859775, 0.005529775324840705, 0.005635606861391733, 0.005487906021001034, 0.0054385750473282014, 0.005478813673023048]
[188.19901576433455, 216.99766581797846, 189.8889265155006, 219.47732678645298, 191.7192080969999, 225.46031042524106, 203.04739622230795, 215.27468829531608, 207.75644431374897, 199.48142628051127, 219.25006608021422, 189.91833124363626, 124.48631885234819, 185.79930900781616, 191.2759682626602, 225.6879790739815, 205.43117251968872, 188.67568937363296, 214.88362720611994, 189.3648736035232, 207.21205312678333, 191.40843822280536, 226.12930956380694, 205.51947684321922, 189.64348150794564, 211.28507587492902, 200.68557938300503, 219.58980623729246, 202.6948902991493, 222.77182063518657, 140.93605631424688, 209.73902571743835, 203.75593417086003, 206.22454002124368, 204.71274038633857, 192.33473688500007, 209.93006416150027, 137.28315142250435, 218.26831920011335, 189.7620338101363, 232.4560775094664, 211.29517236017256, 203.22705067577417, 204.72480090411617, 140.0328351444634, 215.50341775765105, 191.83562743737335, 205.3684583651242, 188.72967621697467, 210.2605443870749, 194.46548579789518, 211.02289915487142, 194.5357232563469, 208.85788903173406, 195.58295014786952, 207.5177404547907, 205.8820395591875, 201.49674433725144, 207.6701712610086, 189.73154995442496, 208.33397949347338, 192.6979469428858, 213.58468396370955, 190.79915723316012, 213.12700161955877, 138.61433839167188, 223.45117087566956, 172.4965677905428, 164.0749028541529, 201.55523039136258, 233.8824953267637, 202.1508454537579, 236.57560726177223, 247.56131566604975, 248.49946711889496, 238.55082878753245, 220.41203050773458, 191.83900149071704, 218.49115149535902, 191.5074580863804, 197.75600066470298, 206.23179716984316, 216.51251492251234, 184.01992470895695, 199.1400156756357, 190.5926673574337, 221.7548246542395, 133.38524364788287, 204.63219570981641, 190.4371780251454, 222.80789082665058, 199.75685049510085, 202.69094150821192, 198.18972061963547, 191.99546896183537, 203.69174242828697, 186.4225768136584, 219.59161227138168, 194.8951175216117, 213.3712766429419, 184.70051811142645, 204.84994143147668, 196.3411408440034, 220.27231130523396, 203.1246437440098, 197.55314442731412, 162.78202382426687, 198.61706357593326, 175.64239110721275, 191.82837198166422, 181.29435228521857, 178.24567568449066, 174.45610261851624, 191.20755368073625, 164.8851857618138, 180.42026344407725, 178.95824444358968, 189.87450152217673, 174.74044313647127, 184.86553977874178, 179.36250964858388, 175.49911250330052, 185.13703672732663, 124.64354590214352, 190.32842230959744, 170.4405886477561, 196.06273384740948, 179.8343308737392, 182.8575488055623, 195.80386667198496, 177.6323289526209, 130.30102364003537, 174.21287988291, 189.16892388534882, 171.02148826609425, 185.22817539952285, 176.2551836492122, 186.122282022763, 193.78534722761157, 186.35864758950206, 190.82460536563312, 175.10212827787956, 178.8035992804943, 182.15000213495412, 167.31421614183378, 181.22972621422136, 171.23138140706263, 190.88887153291566, 180.4905945262183, 190.05960087755173, 182.79779175062487, 172.51429203589686, 197.2420687305333, 172.19508669156022, 186.42211030889663, 189.84040959686993, 122.13030771722019, 173.05668474929996, 184.9445502109308, 177.110044129636, 182.38683317990575, 180.0016983435998, 195.27206813899124, 177.43375543307104, 122.39060213247352, 172.4323008256706, 174.14270127816383, 173.0966078846224, 188.7342290794715, 177.96829788030038, 172.02721262923134, 187.44942080402987, 172.48046290673776, 189.90369166158462, 166.2988336103501, 177.11570225606891, 174.35724796870903, 185.78108003849778, 171.82735336181804, 177.83201002383754, 187.64821599226508, 182.82163689467777, 183.0200578940103, 183.53837802868736, 197.11312339768622, 176.22457457126993, 153.24004036794162, 160.44150332393048, 179.51195265480902, 179.50603928677407, 183.31386719620778, 175.42888458602437, 186.4941936811978, 184.34817462080372, 182.8508834374513, 183.94143840065166, 175.21966908659886, 197.09512668116662, 172.89756450644293, 189.6275310497309, 192.89881333576716, 179.77148766703328, 194.61626152573558, 158.0873820904637, 185.57401869598746, 171.79959782318144, 181.67498047454959, 174.91105732286147, 180.8391736112365, 177.4431795182119, 182.2188638386327, 183.87169273158565, 182.5212645802991]
Elapsed: 0.23196577548952294~0.028604425597436502
Time per graph: 0.005338911401284384~0.0006791355152410294
Speed: 189.9471646561895~21.313678432984826
Total Time: 0.2362
best val loss: 0.31370091438293457 test_score: 0.8140

Testing...
Test loss: 0.3818 score: 0.8605 time: 0.31s
test Score 0.8605
Epoch Time List: [0.7912269990192726, 0.8504630500683561, 0.7699238790664822, 0.7307065869681537, 0.8664836249081418, 0.8023839630186558, 0.801322272978723, 0.744461297057569, 0.7259293330134824, 0.9023134288145229, 0.8198414910584688, 0.7898966569919139, 0.8626002121018246, 1.0587109461193904, 0.7989897489314899, 0.7340424091089517, 0.8224723039893433, 0.8046348779462278, 0.763363633188419, 0.8006022449117154, 0.7502125070896, 0.8942237239098176, 0.7260744191007689, 0.7798488080734387, 0.8244283848907799, 0.7975586081156507, 0.784965481958352, 0.7498563909903169, 0.7774221090367064, 0.7921960230451077, 0.8905185529729351, 0.7530310319270939, 0.7803475098917261, 0.760377602186054, 0.7350020391168073, 0.8618523558834568, 0.7626998840132728, 0.9197040060535073, 0.7543218028731644, 0.8094374119536951, 0.6993014530744404, 0.8254979610210285, 0.7905985580291599, 0.7838428380200639, 0.8767616130644456, 0.7868294421350583, 0.7836813719477504, 0.7329800759907812, 0.8301194069208577, 0.7482515439623967, 0.8728915560059249, 0.7889121830230579, 0.8811418521218002, 0.7717918639536947, 0.8961595289874822, 0.7845567950280383, 0.8857993041165173, 0.7807101059006527, 0.811533579020761, 0.7851270161336288, 0.7439471769612283, 0.8424407279817387, 0.7427519590128213, 1.0251477651763707, 0.7764840049203485, 0.9141849658917636, 0.77310079196468, 0.9748072050279006, 0.8307198388502002, 0.915519735077396, 0.7198613660875708, 0.703710075118579, 0.8595428459811956, 0.6404848380479962, 0.6305229231948033, 0.6482257081661373, 0.8547220329055563, 0.7787925900192931, 0.8856178311398253, 0.7795212911441922, 1.0130722511094064, 0.7744764208327979, 0.7608968269778416, 0.7986939699621871, 0.7560391909210011, 0.812858929974027, 0.7296778169693425, 0.9571004960453138, 0.7712677870877087, 0.816897195065394, 0.7297419920796528, 0.8179621760500595, 0.8682248760014772, 0.813436417025514, 0.8098351111402735, 0.8808826141757891, 0.7820466079283506, 0.7873500789282843, 0.7816660599783063, 0.810152544057928, 0.8230005459627137, 0.8073698360240087, 0.9788435650989413, 0.7446791509864852, 0.8860555901192129, 0.7872609379701316, 0.9190492201596498, 0.7851988420588896, 0.8031987639842555, 0.8082510529784486, 0.9313615139108151, 0.8188523129792884, 0.7918363100616261, 0.7816525299567729, 0.8474199570482597, 0.7656277129426599, 0.7612299119355157, 0.901330744032748, 0.7649531940696761, 0.7749050528509542, 0.7859817568678409, 0.8070684259291738, 0.7945512770675123, 0.9154270611470565, 0.7631097220582888, 0.7801493679871783, 0.7300400628009811, 0.7898523668991402, 0.747795861097984, 0.765513145015575, 0.8496452019317076, 0.8673559970920905, 0.7963387980125844, 0.7882102840812877, 0.8021081610349938, 0.7455299249850214, 0.796696166973561, 0.7461366411298513, 0.7763381140539423, 0.9019023790024221, 0.7997817859286442, 0.7615412940504029, 0.7332763999002054, 0.8303414289839566, 0.8217894990229979, 0.743122453102842, 0.8855546390404925, 0.7264394090743735, 0.7754669148707762, 0.8093931991606951, 0.7883701699320227, 0.7875114530324936, 0.7583435149863362, 0.77008561801631, 0.7526235970435664, 0.7741706019733101, 0.9392109450418502, 0.803922122111544, 0.7771568530006334, 0.8247502361191437, 0.8083011258859187, 0.7878534039482474, 0.8908612409140915, 0.8022197420941666, 0.9423763249069452, 0.8208993548760191, 0.7559762839227915, 0.8048039010027424, 0.7626214430201799, 0.7869891539448872, 0.7891272120177746, 0.7838288189377636, 0.8184000110486522, 0.8382779869716614, 0.8317447189474478, 0.7490355451591313, 0.7825636438792571, 0.7570084900362417, 0.8837299980223179, 0.7685185769805685, 0.8367776040686294, 0.873816394014284, 1.0144843149464577, 0.7458299018908292, 0.7645977649372071, 0.7702823319705203, 0.8764502760022879, 0.8090736599406227, 0.825168019044213, 0.9192102488595992, 0.7551803699461743, 0.7924704608740285, 0.7399968261597678, 0.8063476630486548, 0.7709073340520263, 0.7424321620492265, 0.783374169957824, 0.8570633969502524, 0.7733530139084905, 0.7510003539500758, 0.7772763679968193, 0.8472727090120316, 0.7708922849269584, 0.8330812719650567, 0.7436722380807623, 0.8973635270958766, 0.7511279131285846, 0.7812359520466998, 0.84650552587118, 0.782227055169642, 0.7725287650246173, 0.7748480950249359, 0.8053051539463922]
Total Epoch List: [101, 7, 105]
Total Time List: [0.23873791506048292, 0.21703661198262125, 0.23624782694969326]
========================training times:4========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcce20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.19s
Epoch 2/1000, LR 0.000000
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.5000 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.5000 time: 0.20s
Epoch 5/1000, LR 0.000090
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6961 score: 0.5000 time: 0.22s
Epoch 6/1000, LR 0.000120
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.21s
Epoch 7/1000, LR 0.000150
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5000 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5000 time: 0.21s
Epoch 10/1000, LR 0.000240
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.4884 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.21s
Epoch 12/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.20s
Epoch 13/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.21s
Epoch 14/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.22s
Epoch 15/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.22s
Epoch 16/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.20s
Epoch 19/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.22s
Epoch 20/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.20s
Epoch 21/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.21s
Epoch 23/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.19s
Epoch 25/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.21s
Epoch 26/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.20s
Epoch 28/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.22s
Epoch 29/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.20s
Epoch 30/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4884 time: 0.23s
Test loss: 0.6908 score: 0.5227 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4884 time: 0.26s
Test loss: 0.6905 score: 0.5227 time: 0.20s
Epoch 32/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4884 time: 0.28s
Test loss: 0.6901 score: 0.5227 time: 0.26s
Epoch 33/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.30s
Val loss: 0.6906 score: 0.5116 time: 0.25s
Test loss: 0.6897 score: 0.5455 time: 0.21s
Epoch 34/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.32s
Val loss: 0.6900 score: 0.5349 time: 0.25s
Test loss: 0.6892 score: 0.5682 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.31s
Val loss: 0.6894 score: 0.5814 time: 0.24s
Test loss: 0.6887 score: 0.6136 time: 0.30s
Epoch 36/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.31s
Val loss: 0.6888 score: 0.6047 time: 0.25s
Test loss: 0.6881 score: 0.6136 time: 0.20s
Epoch 37/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.32s
Val loss: 0.6881 score: 0.6279 time: 0.25s
Test loss: 0.6874 score: 0.6364 time: 0.22s
Epoch 38/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.29s
Val loss: 0.6872 score: 0.6279 time: 0.25s
Test loss: 0.6867 score: 0.6364 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.33s
Val loss: 0.6864 score: 0.6512 time: 0.25s
Test loss: 0.6859 score: 0.6591 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.33s
Val loss: 0.6854 score: 0.6977 time: 0.24s
Test loss: 0.6850 score: 0.7045 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.27s
Val loss: 0.6843 score: 0.7209 time: 0.26s
Test loss: 0.6840 score: 0.7273 time: 0.21s
Epoch 42/1000, LR 0.000269
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.32s
Val loss: 0.6830 score: 0.7442 time: 0.25s
Test loss: 0.6829 score: 0.7273 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.39s
Val loss: 0.6817 score: 0.7442 time: 0.26s
Test loss: 0.6816 score: 0.7273 time: 0.20s
Epoch 44/1000, LR 0.000269
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.38s
Val loss: 0.6802 score: 0.7442 time: 0.24s
Test loss: 0.6802 score: 0.7273 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.28s
Val loss: 0.6786 score: 0.7442 time: 0.25s
Test loss: 0.6788 score: 0.7273 time: 0.20s
Epoch 46/1000, LR 0.000269
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 0.32s
Val loss: 0.6769 score: 0.7442 time: 0.25s
Test loss: 0.6771 score: 0.7273 time: 0.21s
Epoch 47/1000, LR 0.000269
Train loss: 0.6727;  Loss pred: 0.6727; Loss self: 0.0000; time: 0.33s
Val loss: 0.6750 score: 0.7674 time: 0.24s
Test loss: 0.6753 score: 0.7273 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6712;  Loss pred: 0.6712; Loss self: 0.0000; time: 0.27s
Val loss: 0.6730 score: 0.8140 time: 0.26s
Test loss: 0.6734 score: 0.7727 time: 0.19s
Epoch 49/1000, LR 0.000269
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.32s
Val loss: 0.6707 score: 0.8140 time: 0.24s
Test loss: 0.6713 score: 0.8182 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.32s
Val loss: 0.6683 score: 0.8140 time: 0.25s
Test loss: 0.6690 score: 0.8182 time: 0.30s
Epoch 51/1000, LR 0.000269
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 0.32s
Val loss: 0.6657 score: 0.8140 time: 0.23s
Test loss: 0.6664 score: 0.8182 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6600;  Loss pred: 0.6600; Loss self: 0.0000; time: 0.31s
Val loss: 0.6629 score: 0.8140 time: 0.25s
Test loss: 0.6637 score: 0.8182 time: 0.20s
Epoch 53/1000, LR 0.000269
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.34s
Val loss: 0.6598 score: 0.8372 time: 0.24s
Test loss: 0.6607 score: 0.8182 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.6509;  Loss pred: 0.6509; Loss self: 0.0000; time: 0.31s
Val loss: 0.6565 score: 0.8605 time: 0.23s
Test loss: 0.6574 score: 0.8182 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.25s
Val loss: 0.6529 score: 0.8605 time: 0.25s
Test loss: 0.6539 score: 0.8182 time: 0.18s
Epoch 56/1000, LR 0.000269
Train loss: 0.6451;  Loss pred: 0.6451; Loss self: 0.0000; time: 0.34s
Val loss: 0.6490 score: 0.8605 time: 0.24s
Test loss: 0.6501 score: 0.8182 time: 0.21s
Epoch 57/1000, LR 0.000269
Train loss: 0.6388;  Loss pred: 0.6388; Loss self: 0.0000; time: 0.35s
Val loss: 0.6448 score: 0.8605 time: 0.24s
Test loss: 0.6460 score: 0.8182 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6361;  Loss pred: 0.6361; Loss self: 0.0000; time: 0.27s
Val loss: 0.6404 score: 0.8605 time: 0.26s
Test loss: 0.6416 score: 0.8182 time: 0.20s
Epoch 59/1000, LR 0.000268
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.32s
Val loss: 0.6356 score: 0.8605 time: 0.23s
Test loss: 0.6369 score: 0.7955 time: 0.22s
Epoch 60/1000, LR 0.000268
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.28s
Val loss: 0.6305 score: 0.8605 time: 0.26s
Test loss: 0.6318 score: 0.8182 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.33s
Val loss: 0.6250 score: 0.8605 time: 0.24s
Test loss: 0.6264 score: 0.8182 time: 0.19s
Epoch 62/1000, LR 0.000268
Train loss: 0.6112;  Loss pred: 0.6112; Loss self: 0.0000; time: 0.44s
Val loss: 0.6191 score: 0.8837 time: 0.27s
Test loss: 0.6207 score: 0.8409 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.6025;  Loss pred: 0.6025; Loss self: 0.0000; time: 0.31s
Val loss: 0.6129 score: 0.8605 time: 0.26s
Test loss: 0.6146 score: 0.8182 time: 0.20s
Epoch 64/1000, LR 0.000268
Train loss: 0.5973;  Loss pred: 0.5973; Loss self: 0.0000; time: 0.34s
Val loss: 0.6064 score: 0.8605 time: 0.23s
Test loss: 0.6081 score: 0.8409 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.25s
Val loss: 0.5997 score: 0.8605 time: 0.26s
Test loss: 0.6014 score: 0.8409 time: 0.21s
Epoch 66/1000, LR 0.000268
Train loss: 0.5862;  Loss pred: 0.5862; Loss self: 0.0000; time: 0.38s
Val loss: 0.5925 score: 0.8605 time: 0.23s
Test loss: 0.5943 score: 0.8409 time: 0.22s
Epoch 67/1000, LR 0.000268
Train loss: 0.5764;  Loss pred: 0.5764; Loss self: 0.0000; time: 0.27s
Val loss: 0.5853 score: 0.8605 time: 0.26s
Test loss: 0.5871 score: 0.8409 time: 0.19s
Epoch 68/1000, LR 0.000268
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.38s
Val loss: 0.5777 score: 0.8605 time: 0.23s
Test loss: 0.5797 score: 0.8409 time: 0.20s
Epoch 69/1000, LR 0.000268
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.30s
Val loss: 0.5698 score: 0.8605 time: 0.25s
Test loss: 0.5721 score: 0.8409 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.31s
Val loss: 0.5616 score: 0.8372 time: 0.35s
Test loss: 0.5641 score: 0.8636 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.5392;  Loss pred: 0.5392; Loss self: 0.0000; time: 0.30s
Val loss: 0.5532 score: 0.8372 time: 0.26s
Test loss: 0.5559 score: 0.8864 time: 0.21s
Epoch 72/1000, LR 0.000267
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.33s
Val loss: 0.5444 score: 0.8372 time: 0.25s
Test loss: 0.5473 score: 0.8864 time: 0.20s
Epoch 73/1000, LR 0.000267
Train loss: 0.5205;  Loss pred: 0.5205; Loss self: 0.0000; time: 0.39s
Val loss: 0.5354 score: 0.8372 time: 0.25s
Test loss: 0.5388 score: 0.8864 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.4978;  Loss pred: 0.4978; Loss self: 0.0000; time: 0.30s
Val loss: 0.5263 score: 0.8372 time: 0.28s
Test loss: 0.5300 score: 0.8864 time: 0.20s
Epoch 75/1000, LR 0.000267
Train loss: 0.4915;  Loss pred: 0.4915; Loss self: 0.0000; time: 0.31s
Val loss: 0.5170 score: 0.8372 time: 0.24s
Test loss: 0.5210 score: 0.8864 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.4757;  Loss pred: 0.4757; Loss self: 0.0000; time: 0.28s
Val loss: 0.5077 score: 0.8372 time: 0.26s
Test loss: 0.5120 score: 0.8864 time: 0.20s
Epoch 77/1000, LR 0.000267
Train loss: 0.4677;  Loss pred: 0.4677; Loss self: 0.0000; time: 0.41s
Val loss: 0.4980 score: 0.8372 time: 0.24s
Test loss: 0.5025 score: 0.8864 time: 0.21s
Epoch 78/1000, LR 0.000267
Train loss: 0.4653;  Loss pred: 0.4653; Loss self: 0.0000; time: 0.30s
Val loss: 0.4879 score: 0.8372 time: 0.33s
Test loss: 0.4926 score: 0.8864 time: 0.21s
Epoch 79/1000, LR 0.000267
Train loss: 0.4459;  Loss pred: 0.4459; Loss self: 0.0000; time: 0.43s
Val loss: 0.4776 score: 0.8372 time: 0.24s
Test loss: 0.4823 score: 0.8864 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.4408;  Loss pred: 0.4408; Loss self: 0.0000; time: 0.29s
Val loss: 0.4671 score: 0.8372 time: 0.25s
Test loss: 0.4716 score: 0.8864 time: 0.21s
Epoch 81/1000, LR 0.000267
Train loss: 0.4202;  Loss pred: 0.4202; Loss self: 0.0000; time: 0.47s
Val loss: 0.4570 score: 0.8372 time: 0.24s
Test loss: 0.4612 score: 0.8864 time: 0.21s
Epoch 82/1000, LR 0.000267
Train loss: 0.4129;  Loss pred: 0.4129; Loss self: 0.0000; time: 0.28s
Val loss: 0.4470 score: 0.8372 time: 0.25s
Test loss: 0.4509 score: 0.8864 time: 0.20s
Epoch 83/1000, LR 0.000266
Train loss: 0.4118;  Loss pred: 0.4118; Loss self: 0.0000; time: 0.30s
Val loss: 0.4373 score: 0.8372 time: 0.25s
Test loss: 0.4411 score: 0.8864 time: 0.19s
Epoch 84/1000, LR 0.000266
Train loss: 0.3799;  Loss pred: 0.3799; Loss self: 0.0000; time: 0.32s
Val loss: 0.4276 score: 0.8372 time: 0.24s
Test loss: 0.4312 score: 0.8864 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.3626;  Loss pred: 0.3626; Loss self: 0.0000; time: 0.28s
Val loss: 0.4186 score: 0.8372 time: 0.25s
Test loss: 0.4223 score: 0.8864 time: 0.29s
Epoch 86/1000, LR 0.000266
Train loss: 0.3824;  Loss pred: 0.3824; Loss self: 0.0000; time: 0.31s
Val loss: 0.4097 score: 0.8372 time: 0.24s
Test loss: 0.4132 score: 0.8864 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.3445;  Loss pred: 0.3445; Loss self: 0.0000; time: 0.28s
Val loss: 0.4016 score: 0.8605 time: 0.25s
Test loss: 0.4052 score: 0.8864 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.3691;  Loss pred: 0.3691; Loss self: 0.0000; time: 0.39s
Val loss: 0.3936 score: 0.8605 time: 0.25s
Test loss: 0.3971 score: 0.8864 time: 0.20s
Epoch 89/1000, LR 0.000266
Train loss: 0.3499;  Loss pred: 0.3499; Loss self: 0.0000; time: 0.30s
Val loss: 0.3860 score: 0.8605 time: 0.26s
Test loss: 0.3893 score: 0.8864 time: 0.21s
Epoch 90/1000, LR 0.000266
Train loss: 0.3047;  Loss pred: 0.3047; Loss self: 0.0000; time: 0.29s
Val loss: 0.3785 score: 0.8605 time: 0.25s
Test loss: 0.3814 score: 0.8864 time: 0.20s
Epoch 91/1000, LR 0.000266
Train loss: 0.2826;  Loss pred: 0.2826; Loss self: 0.0000; time: 0.34s
Val loss: 0.3721 score: 0.8605 time: 0.23s
Test loss: 0.3750 score: 0.8636 time: 0.22s
Epoch 92/1000, LR 0.000266
Train loss: 0.3085;  Loss pred: 0.3085; Loss self: 0.0000; time: 0.29s
Val loss: 0.3660 score: 0.8605 time: 0.28s
Test loss: 0.3686 score: 0.8636 time: 0.20s
Epoch 93/1000, LR 0.000265
Train loss: 0.2828;  Loss pred: 0.2828; Loss self: 0.0000; time: 0.43s
Val loss: 0.3595 score: 0.8605 time: 0.29s
Test loss: 0.3616 score: 0.8636 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.2648;  Loss pred: 0.2648; Loss self: 0.0000; time: 0.30s
Val loss: 0.3534 score: 0.8605 time: 0.26s
Test loss: 0.3550 score: 0.8636 time: 0.21s
Epoch 95/1000, LR 0.000265
Train loss: 0.2566;  Loss pred: 0.2566; Loss self: 0.0000; time: 0.31s
Val loss: 0.3475 score: 0.8605 time: 0.25s
Test loss: 0.3485 score: 0.8636 time: 0.20s
Epoch 96/1000, LR 0.000265
Train loss: 0.2375;  Loss pred: 0.2375; Loss self: 0.0000; time: 0.34s
Val loss: 0.3420 score: 0.8605 time: 0.24s
Test loss: 0.3424 score: 0.8636 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.2452;  Loss pred: 0.2452; Loss self: 0.0000; time: 0.27s
Val loss: 0.3400 score: 0.8605 time: 0.26s
Test loss: 0.3416 score: 0.8636 time: 0.21s
Epoch 98/1000, LR 0.000265
Train loss: 0.2285;  Loss pred: 0.2285; Loss self: 0.0000; time: 0.43s
Val loss: 0.3363 score: 0.8605 time: 0.24s
Test loss: 0.3380 score: 0.8636 time: 0.22s
Epoch 99/1000, LR 0.000265
Train loss: 0.2063;  Loss pred: 0.2063; Loss self: 0.0000; time: 0.26s
Val loss: 0.3324 score: 0.8605 time: 0.32s
Test loss: 0.3341 score: 0.8636 time: 0.19s
Epoch 100/1000, LR 0.000265
Train loss: 0.2311;  Loss pred: 0.2311; Loss self: 0.0000; time: 0.32s
Val loss: 0.3274 score: 0.8605 time: 0.32s
Test loss: 0.3280 score: 0.8636 time: 0.32s
Epoch 101/1000, LR 0.000265
Train loss: 0.2044;  Loss pred: 0.2044; Loss self: 0.0000; time: 0.36s
Val loss: 0.3231 score: 0.8605 time: 0.24s
Test loss: 0.3233 score: 0.8409 time: 0.21s
Epoch 102/1000, LR 0.000264
Train loss: 0.1813;  Loss pred: 0.1813; Loss self: 0.0000; time: 0.40s
Val loss: 0.3185 score: 0.8605 time: 0.26s
Test loss: 0.3176 score: 0.8636 time: 0.19s
Epoch 103/1000, LR 0.000264
Train loss: 0.2075;  Loss pred: 0.2075; Loss self: 0.0000; time: 0.32s
Val loss: 0.3146 score: 0.8605 time: 0.24s
Test loss: 0.3131 score: 0.8636 time: 0.20s
Epoch 104/1000, LR 0.000264
Train loss: 0.1804;  Loss pred: 0.1804; Loss self: 0.0000; time: 0.34s
Val loss: 0.3118 score: 0.8605 time: 0.24s
Test loss: 0.3101 score: 0.8409 time: 0.22s
Epoch 105/1000, LR 0.000264
Train loss: 0.1593;  Loss pred: 0.1593; Loss self: 0.0000; time: 0.25s
Val loss: 0.3104 score: 0.8605 time: 0.26s
Test loss: 0.3098 score: 0.8409 time: 0.21s
Epoch 106/1000, LR 0.000264
Train loss: 0.1756;  Loss pred: 0.1756; Loss self: 0.0000; time: 0.33s
Val loss: 0.3121 score: 0.8605 time: 0.25s
Test loss: 0.3163 score: 0.8636 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 107/1000, LR 0.000264
Train loss: 0.1922;  Loss pred: 0.1922; Loss self: 0.0000; time: 0.31s
Val loss: 0.3098 score: 0.8605 time: 0.25s
Test loss: 0.3148 score: 0.8636 time: 0.20s
Epoch 108/1000, LR 0.000264
Train loss: 0.1535;  Loss pred: 0.1535; Loss self: 0.0000; time: 0.32s
Val loss: 0.3101 score: 0.8605 time: 0.25s
Test loss: 0.3189 score: 0.8636 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 109/1000, LR 0.000264
Train loss: 0.1760;  Loss pred: 0.1760; Loss self: 0.0000; time: 0.33s
Val loss: 0.3150 score: 0.8605 time: 0.24s
Test loss: 0.3327 score: 0.8636 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 106,   Train_Loss: 0.1922,   Val_Loss: 0.3098,   Val_Precision: 0.8333,   Val_Recall: 0.9091,   Val_accuracy: 0.8696,   Val_Score: 0.8605,   Val_Loss: 0.3098,   Test_Precision: 0.8077,   Test_Recall: 0.9545,   Test_accuracy: 0.8750,   Test_Score: 0.8636,   Test_loss: 0.3148


[0.19713162095285952, 0.22920872899703681, 0.23005992302205414, 0.19949545699637383, 0.22712681198026985, 0.21923925599548966, 0.2267439819406718, 0.21074741799384356, 0.21498056198470294, 0.22731553192716092, 0.21652775001712143, 0.2065255899215117, 0.21105593908578157, 0.2231210300233215, 0.22004244290292263, 0.23138600902166218, 0.2287252020323649, 0.19947855698410422, 0.2283304890152067, 0.20496008405461907, 0.23771347105503082, 0.21878368104808033, 0.23411090997979045, 0.19325228896923363, 0.21596957801375538, 0.22561158693861216, 0.20186153298709542, 0.22584467998240143, 0.20066162396688014, 0.22674467402976006, 0.2027860271045938, 0.2635700849350542, 0.21057493903208524, 0.21969607996288687, 0.30838725494686514, 0.20095182699151337, 0.22547778696753085, 0.21086700889281929, 0.21553722908720374, 0.2269744420191273, 0.21611578692682087, 0.22772368392907083, 0.20032804599031806, 0.22596722492016852, 0.2058582230238244, 0.2161397390300408, 0.2316172739956528, 0.19722359103616327, 0.22235691198147833, 0.30869364098180085, 0.21904785395599902, 0.20167745696380734, 0.19973964802920818, 0.22927696502301842, 0.1871840839739889, 0.21212612104136497, 0.22744919103570282, 0.20044733001850545, 0.22512534609995782, 0.20698259805794805, 0.19819483102764934, 0.21332740609068424, 0.20269839407410473, 0.22388405702076852, 0.21028061409015208, 0.22684923594351858, 0.19051526603288949, 0.20857967995107174, 0.22238733409903944, 0.20415830006822944, 0.21750200097449124, 0.20339257502928376, 0.23837803408969194, 0.2001676249783486, 0.2277069160481915, 0.20566058799158782, 0.20977262302767485, 0.21361254598014057, 0.30348861205857247, 0.21050990105140954, 0.21286279906053096, 0.2044873449485749, 0.1977680429117754, 0.22848730999976397, 0.2904353520134464, 0.21970687992870808, 0.21079320507124066, 0.2061683420324698, 0.21616915299091488, 0.20154842105694115, 0.22730971104465425, 0.2086457380792126, 0.23051037697587162, 0.21168164699338377, 0.20329637499526143, 0.23302869696635753, 0.21409610600676388, 0.22680465795565397, 0.19716970098670572, 0.3243252179818228, 0.21282060106750578, 0.19118949200492352, 0.20284985401667655, 0.22098138800356537, 0.21177647006697953, 0.2187227699905634, 0.20716368197463453, 0.21346592903137207, 0.2191230229800567]
[0.004480264112564989, 0.005209289295387201, 0.005228634614137594, 0.004533987659008496, 0.005161972999551587, 0.004982710363533856, 0.00515327231683345, 0.004789714045314627, 0.004885921863288703, 0.005166262089253657, 0.004921085227661851, 0.004693763407307084, 0.004796725888313217, 0.005070932500530034, 0.0050009646114300595, 0.005258772932310504, 0.005198300046190111, 0.004533603567820551, 0.005189329295800152, 0.00465818372851407, 0.005402578887614337, 0.004972356387456371, 0.005320702499540692, 0.004392097476573492, 0.004908399500312622, 0.00512753606678664, 0.0045877621133430775, 0.005132833635963669, 0.004560491453792731, 0.0051532880461309105, 0.004608773343286223, 0.005990229203069413, 0.004785794068911028, 0.004993092726429247, 0.00700880124879239, 0.004567086977079849, 0.005124495158352974, 0.004792432020291348, 0.00489857338834554, 0.005158510045889257, 0.0049117224301550196, 0.005175538271115246, 0.004552910136143592, 0.005135618748185648, 0.0046785959778141905, 0.004912266796137291, 0.005264028954446654, 0.004482354341730984, 0.005053566181397235, 0.007015764567768201, 0.004978360317181796, 0.004583578567359258, 0.004539537455209277, 0.0052108401141595095, 0.0042541837266815655, 0.0048210482054855675, 0.005169299796265973, 0.004555621136784215, 0.005116485138635405, 0.0047041499558624555, 0.004504427977901121, 0.004848350138424642, 0.0046067816835023805, 0.005088274023199285, 0.0047791048656852745, 0.005155664453261786, 0.004329892409838398, 0.004740447271615267, 0.005054257593159987, 0.004639961365187032, 0.0049432272948748005, 0.004622558523392813, 0.005417682592947544, 0.004549264204053377, 0.005175157182913443, 0.004674104272536087, 0.004767559614265338, 0.00485483059045774, 0.006897468455876647, 0.00478431593298658, 0.00483779088773934, 0.004647439657922157, 0.004494728247994895, 0.005192893409085545, 0.006600803454851054, 0.004993338180197911, 0.004790754660710015, 0.004685644137101586, 0.004912935295248066, 0.004580645933112299, 0.005166129796469415, 0.004741948592709377, 0.005238872203997083, 0.004810946522576904, 0.004620372158983214, 0.005296106749235399, 0.004865820591062816, 0.0051546513171739534, 0.004481129567879675, 0.007371027681405063, 0.004836831842443313, 0.004345215727384625, 0.004610223954924467, 0.005022304272808304, 0.004813101592431353, 0.004970972045240077, 0.004708265499423512, 0.004851498387076638, 0.0049800687040921976]
[223.2011271825427, 191.96476588188239, 191.25451935312543, 220.55640094500887, 193.7243763357283, 200.69398520904122, 194.05145672846444, 208.78073107061908, 204.66966684704659, 193.56354414927964, 203.20721014521604, 213.04865908734033, 208.47553587258514, 197.20238829751253, 199.9614229851635, 190.15842913769586, 192.37058098116336, 220.57508669218123, 192.70313040440965, 214.67594630901203, 185.0967881825005, 201.11189184320597, 187.94510688886, 227.6816499027597, 203.73239788984347, 195.02544438008934, 217.97119713151503, 194.82415969873023, 219.27461330255326, 194.05086442835272, 216.97747437650375, 166.93852039711547, 208.95174042194895, 200.27667315426336, 142.67775108793379, 218.95794956797388, 195.14117373493679, 208.66232338110595, 204.14106735221196, 193.8544252321241, 203.594566716678, 193.21661779239744, 219.63974031936866, 194.71850404652562, 213.7393364894041, 203.5720048402785, 189.9685599478467, 223.09704315206432, 197.88006411811057, 142.5361399089981, 200.86934980352945, 218.17014485608155, 220.286760461127, 191.9076344873223, 235.0627204293408, 207.42377121684095, 193.4497977312801, 219.5090350963412, 195.44667342993702, 212.5782573648124, 222.00377160119652, 206.2557305988892, 217.0712807123375, 196.53029601798914, 209.24420536995484, 193.96142031069135, 230.95262083829064, 210.95055860820884, 197.85299454331673, 215.51903589173332, 202.29698946613527, 216.3304141936599, 184.5807654552793, 219.8157669341349, 193.230845876846, 213.94473501067566, 209.75091680192784, 205.98041092628827, 144.98072827690854, 209.0162970018905, 206.70591664768955, 215.17223968586052, 222.48286099300915, 192.57086969094894, 151.49670897488718, 200.2668283045001, 208.73538112924254, 213.41782916928324, 203.5443049631102, 218.309822370522, 193.56850086953102, 210.88377076408503, 190.8807775912216, 207.85930488048047, 216.43278194717234, 188.817946342258, 205.5151811056756, 193.9995430279175, 223.15801961360546, 135.66629284580034, 206.7469022232645, 230.13817097681763, 216.90920219436148, 199.11179125768768, 207.76623572054024, 201.1678985315445, 212.39244051178542, 206.12188652144826, 200.80044260800756]
Elapsed: 0.219019400316863~0.023058303460058837
Time per graph: 0.004977713643565069~0.0005240523513649736
Speed: 202.71292233097657~17.53232897752212
Total Time: 0.2196
best val loss: 0.309805691242218 test_score: 0.8636

Testing...
Test loss: 0.6207 score: 0.8409 time: 0.19s
test Score 0.8409
Epoch Time List: [0.7187908830819651, 0.8099405858665705, 0.7892764230491593, 0.7691002930514514, 0.7870204178616405, 0.7609148488845676, 0.8133880511159077, 0.7536462909774855, 0.7905409910017624, 0.8014952337834984, 0.840861066011712, 0.7561806011945009, 0.7267973079578951, 0.8152129440568388, 0.7511090820189565, 0.8390064099803567, 0.8127623070031404, 0.7620169429574162, 0.8118296769680455, 0.8837945461273193, 0.8103100060252473, 0.7455872889840975, 0.818573541007936, 0.7231642759870738, 0.8186835971428081, 0.7974246608791873, 0.7460343650309369, 0.8876215890049934, 0.7500274189515039, 0.7819136319449171, 0.7113079830305651, 0.8698543170467019, 0.7570639118785039, 0.7863324610516429, 0.8584822549019009, 0.7539411700563505, 0.7935636161128059, 0.750584244960919, 0.7910177819430828, 0.7955891389865428, 0.7366951819276437, 0.7937384290853515, 0.8448574650101364, 0.8371233119396493, 0.7299459909554571, 0.7767199870431796, 0.8001176918623969, 0.7278467180440202, 0.7787667859811336, 0.8717806262429804, 0.7671118580037728, 0.7626969860866666, 0.7739716989453882, 0.7645181040279567, 0.6884431561920792, 0.7921270270599052, 0.8207065940368921, 0.7319079850567505, 0.7772948219208047, 0.7388118138769642, 0.7649029970634729, 0.9124931680271402, 0.7629036779981107, 0.7939712370280176, 0.7126472550444305, 0.8338400539942086, 0.7222169450251386, 0.8198915290413424, 0.7693301460240036, 0.8595348689705133, 0.7738273240393028, 0.774035581969656, 0.8756562878843397, 0.7757323129335418, 0.7740406870143488, 0.7422311330446973, 0.8513688981765881, 0.8406878729583696, 0.9673156979260966, 0.7454716619104147, 0.9233602149179205, 0.7260452040936798, 0.7486323179909959, 0.7878230300266296, 0.8248524130322039, 0.7654973169555888, 0.7363554680487141, 0.8360748760169372, 0.7683531630318612, 0.7354616539087147, 0.793669311911799, 0.7673577399691567, 0.9497937289997935, 0.7690339110558853, 0.7659654000308365, 0.8026534270029515, 0.741617848048918, 0.8874269680818543, 0.7797088660299778, 0.9564900972181931, 0.8167023309506476, 0.8429044469958171, 0.7581101660616696, 0.8016599391121417, 0.7111334489891306, 0.7868915201397613, 0.7633705689804628, 0.773661358980462, 0.7803304221015424]
Total Epoch List: [109]
Total Time List: [0.21962832601275295]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bce230>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5116 time: 0.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5116 time: 0.18s
Epoch 3/1000, LR 0.000030
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5116 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.26s
Epoch 5/1000, LR 0.000090
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5116 time: 0.19s
Epoch 6/1000, LR 0.000120
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5116 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5116 time: 0.18s
Epoch 8/1000, LR 0.000180
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5116 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6986;  Loss pred: 0.6986; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5116 time: 0.18s
Epoch 10/1000, LR 0.000240
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.20s
Epoch 11/1000, LR 0.000270
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.21s
Epoch 12/1000, LR 0.000270
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 0.19s
Epoch 13/1000, LR 0.000270
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.22s
Epoch 14/1000, LR 0.000270
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.19s
Epoch 15/1000, LR 0.000270
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.22s
Epoch 16/1000, LR 0.000270
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.19s
Epoch 17/1000, LR 0.000270
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.20s
Epoch 21/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.22s
Epoch 25/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.19s
Epoch 26/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5116 time: 0.29s
Epoch 27/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.20s
Epoch 28/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.19s
Epoch 29/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.21s
Test loss: 0.6909 score: 0.5581 time: 0.21s
Epoch 30/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.29s
Val loss: 0.6915 score: 0.5227 time: 0.35s
Test loss: 0.6907 score: 0.5581 time: 0.20s
Epoch 31/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.51s
Val loss: 0.6912 score: 0.5682 time: 0.23s
Test loss: 0.6905 score: 0.6047 time: 0.21s
Epoch 32/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.34s
Val loss: 0.6910 score: 0.6136 time: 0.24s
Test loss: 0.6903 score: 0.5814 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.51s
Val loss: 0.6906 score: 0.6364 time: 0.20s
Test loss: 0.6901 score: 0.5581 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.33s
Val loss: 0.6903 score: 0.6818 time: 0.22s
Test loss: 0.6898 score: 0.5814 time: 0.19s
Epoch 35/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.49s
Val loss: 0.6899 score: 0.6364 time: 0.31s
Test loss: 0.6895 score: 0.6279 time: 0.22s
Epoch 36/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.30s
Val loss: 0.6895 score: 0.6136 time: 0.23s
Test loss: 0.6892 score: 0.6744 time: 0.19s
Epoch 37/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.47s
Val loss: 0.6891 score: 0.6364 time: 0.21s
Test loss: 0.6889 score: 0.6977 time: 0.21s
Epoch 38/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.35s
Val loss: 0.6886 score: 0.6364 time: 0.23s
Test loss: 0.6885 score: 0.7209 time: 0.20s
Epoch 39/1000, LR 0.000269
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.42s
Val loss: 0.6881 score: 0.6364 time: 0.21s
Test loss: 0.6882 score: 0.6744 time: 0.20s
Epoch 40/1000, LR 0.000269
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.35s
Val loss: 0.6875 score: 0.6136 time: 0.22s
Test loss: 0.6878 score: 0.5814 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.36s
Val loss: 0.6868 score: 0.5682 time: 0.23s
Test loss: 0.6873 score: 0.5581 time: 0.20s
Epoch 42/1000, LR 0.000269
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.35s
Val loss: 0.6861 score: 0.5909 time: 0.21s
Test loss: 0.6868 score: 0.5349 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.31s
Val loss: 0.6853 score: 0.5682 time: 0.23s
Test loss: 0.6863 score: 0.5349 time: 0.30s
Epoch 44/1000, LR 0.000269
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.34s
Val loss: 0.6845 score: 0.5909 time: 0.22s
Test loss: 0.6857 score: 0.5581 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.31s
Val loss: 0.6835 score: 0.5909 time: 0.24s
Test loss: 0.6851 score: 0.5581 time: 0.21s
Epoch 46/1000, LR 0.000269
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.38s
Val loss: 0.6825 score: 0.6136 time: 0.21s
Test loss: 0.6843 score: 0.5814 time: 0.21s
Epoch 47/1000, LR 0.000269
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.35s
Val loss: 0.6813 score: 0.6364 time: 0.23s
Test loss: 0.6835 score: 0.5814 time: 0.19s
Epoch 48/1000, LR 0.000269
Train loss: 0.6763;  Loss pred: 0.6763; Loss self: 0.0000; time: 0.37s
Val loss: 0.6801 score: 0.6364 time: 0.21s
Test loss: 0.6826 score: 0.5814 time: 0.19s
Epoch 49/1000, LR 0.000269
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.44s
Val loss: 0.6786 score: 0.6364 time: 0.22s
Test loss: 0.6816 score: 0.5814 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 0.40s
Val loss: 0.6771 score: 0.6818 time: 0.23s
Test loss: 0.6805 score: 0.6279 time: 0.20s
Epoch 51/1000, LR 0.000269
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.36s
Val loss: 0.6755 score: 0.7045 time: 0.22s
Test loss: 0.6793 score: 0.6279 time: 0.20s
Epoch 52/1000, LR 0.000269
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.36s
Val loss: 0.6736 score: 0.7045 time: 0.22s
Test loss: 0.6780 score: 0.6279 time: 0.19s
Epoch 53/1000, LR 0.000269
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.44s
Val loss: 0.6716 score: 0.7500 time: 0.22s
Test loss: 0.6765 score: 0.6279 time: 0.21s
Epoch 54/1000, LR 0.000269
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.35s
Val loss: 0.6694 score: 0.7500 time: 0.22s
Test loss: 0.6749 score: 0.6279 time: 0.19s
Epoch 55/1000, LR 0.000269
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.38s
Val loss: 0.6670 score: 0.7727 time: 0.21s
Test loss: 0.6731 score: 0.6279 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.35s
Val loss: 0.6644 score: 0.7727 time: 0.23s
Test loss: 0.6711 score: 0.6512 time: 0.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 0.38s
Val loss: 0.6615 score: 0.7955 time: 0.21s
Test loss: 0.6690 score: 0.6512 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.33s
Val loss: 0.6583 score: 0.7955 time: 0.22s
Test loss: 0.6666 score: 0.6512 time: 0.20s
Epoch 59/1000, LR 0.000268
Train loss: 0.6425;  Loss pred: 0.6425; Loss self: 0.0000; time: 0.43s
Val loss: 0.6549 score: 0.7955 time: 0.20s
Test loss: 0.6641 score: 0.6512 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.34s
Val loss: 0.6513 score: 0.7955 time: 0.20s
Test loss: 0.6613 score: 0.6744 time: 0.22s
Epoch 61/1000, LR 0.000268
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 0.42s
Val loss: 0.6474 score: 0.7955 time: 0.22s
Test loss: 0.6584 score: 0.6977 time: 0.19s
Epoch 62/1000, LR 0.000268
Train loss: 0.6220;  Loss pred: 0.6220; Loss self: 0.0000; time: 0.39s
Val loss: 0.6432 score: 0.7955 time: 0.23s
Test loss: 0.6552 score: 0.6977 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 0.30s
Val loss: 0.6387 score: 0.7955 time: 0.24s
Test loss: 0.6517 score: 0.6977 time: 0.20s
Epoch 64/1000, LR 0.000268
Train loss: 0.6098;  Loss pred: 0.6098; Loss self: 0.0000; time: 0.35s
Val loss: 0.6337 score: 0.7955 time: 0.22s
Test loss: 0.6480 score: 0.7209 time: 0.21s
Epoch 65/1000, LR 0.000268
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.33s
Val loss: 0.6284 score: 0.7955 time: 0.23s
Test loss: 0.6440 score: 0.6977 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.5965;  Loss pred: 0.5965; Loss self: 0.0000; time: 0.41s
Val loss: 0.6226 score: 0.8182 time: 0.20s
Test loss: 0.6399 score: 0.6977 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.5841;  Loss pred: 0.5841; Loss self: 0.0000; time: 0.33s
Val loss: 0.6165 score: 0.8182 time: 0.20s
Test loss: 0.6355 score: 0.6977 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.5732;  Loss pred: 0.5732; Loss self: 0.0000; time: 0.30s
Val loss: 0.6101 score: 0.8182 time: 0.23s
Test loss: 0.6309 score: 0.6977 time: 0.20s
Epoch 69/1000, LR 0.000268
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 0.43s
Val loss: 0.6035 score: 0.8182 time: 0.29s
Test loss: 0.6262 score: 0.6977 time: 0.21s
Epoch 70/1000, LR 0.000268
Train loss: 0.5543;  Loss pred: 0.5543; Loss self: 0.0000; time: 0.36s
Val loss: 0.5966 score: 0.7955 time: 0.22s
Test loss: 0.6214 score: 0.6744 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.5393;  Loss pred: 0.5393; Loss self: 0.0000; time: 0.36s
Val loss: 0.5895 score: 0.7955 time: 0.21s
Test loss: 0.6166 score: 0.6744 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.5279;  Loss pred: 0.5279; Loss self: 0.0000; time: 0.31s
Val loss: 0.5822 score: 0.7955 time: 0.23s
Test loss: 0.6117 score: 0.6977 time: 0.21s
Epoch 73/1000, LR 0.000267
Train loss: 0.5175;  Loss pred: 0.5175; Loss self: 0.0000; time: 0.42s
Val loss: 0.5748 score: 0.7955 time: 0.20s
Test loss: 0.6069 score: 0.6977 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.5004;  Loss pred: 0.5004; Loss self: 0.0000; time: 0.31s
Val loss: 0.5674 score: 0.7955 time: 0.25s
Test loss: 0.6023 score: 0.6977 time: 0.18s
Epoch 75/1000, LR 0.000267
Train loss: 0.4916;  Loss pred: 0.4916; Loss self: 0.0000; time: 0.38s
Val loss: 0.5599 score: 0.7955 time: 0.21s
Test loss: 0.5977 score: 0.6977 time: 0.21s
Epoch 76/1000, LR 0.000267
Train loss: 0.4734;  Loss pred: 0.4734; Loss self: 0.0000; time: 0.36s
Val loss: 0.5525 score: 0.8182 time: 0.22s
Test loss: 0.5930 score: 0.7209 time: 0.19s
Epoch 77/1000, LR 0.000267
Train loss: 0.4629;  Loss pred: 0.4629; Loss self: 0.0000; time: 0.30s
Val loss: 0.5450 score: 0.8182 time: 0.23s
Test loss: 0.5882 score: 0.7442 time: 0.20s
Epoch 78/1000, LR 0.000267
Train loss: 0.4454;  Loss pred: 0.4454; Loss self: 0.0000; time: 0.52s
Val loss: 0.5376 score: 0.8182 time: 0.22s
Test loss: 0.5829 score: 0.7442 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.4346;  Loss pred: 0.4346; Loss self: 0.0000; time: 0.32s
Val loss: 0.5304 score: 0.7955 time: 0.23s
Test loss: 0.5777 score: 0.7209 time: 0.19s
Epoch 80/1000, LR 0.000267
Train loss: 0.4150;  Loss pred: 0.4150; Loss self: 0.0000; time: 0.40s
Val loss: 0.5236 score: 0.7955 time: 0.21s
Test loss: 0.5727 score: 0.7209 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.4010;  Loss pred: 0.4010; Loss self: 0.0000; time: 0.31s
Val loss: 0.5173 score: 0.7955 time: 0.23s
Test loss: 0.5680 score: 0.7209 time: 0.20s
Epoch 82/1000, LR 0.000267
Train loss: 0.3737;  Loss pred: 0.3737; Loss self: 0.0000; time: 0.39s
Val loss: 0.5114 score: 0.7955 time: 0.21s
Test loss: 0.5636 score: 0.7442 time: 0.22s
Epoch 83/1000, LR 0.000266
Train loss: 0.3670;  Loss pred: 0.3670; Loss self: 0.0000; time: 0.30s
Val loss: 0.5054 score: 0.7955 time: 0.23s
Test loss: 0.5580 score: 0.7674 time: 0.18s
Epoch 84/1000, LR 0.000266
Train loss: 0.3490;  Loss pred: 0.3490; Loss self: 0.0000; time: 0.37s
Val loss: 0.4999 score: 0.7955 time: 0.21s
Test loss: 0.5526 score: 0.7674 time: 0.21s
Epoch 85/1000, LR 0.000266
Train loss: 0.3301;  Loss pred: 0.3301; Loss self: 0.0000; time: 0.37s
Val loss: 0.4947 score: 0.7955 time: 0.22s
Test loss: 0.5473 score: 0.7674 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3241;  Loss pred: 0.3241; Loss self: 0.0000; time: 0.31s
Val loss: 0.4902 score: 0.8182 time: 0.23s
Test loss: 0.5423 score: 0.7674 time: 0.20s
Epoch 87/1000, LR 0.000266
Train loss: 0.3111;  Loss pred: 0.3111; Loss self: 0.0000; time: 0.45s
Val loss: 0.4862 score: 0.8182 time: 0.22s
Test loss: 0.5372 score: 0.8140 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.2905;  Loss pred: 0.2905; Loss self: 0.0000; time: 0.34s
Val loss: 0.4826 score: 0.8182 time: 0.22s
Test loss: 0.5324 score: 0.8140 time: 0.20s
Epoch 89/1000, LR 0.000266
Train loss: 0.2806;  Loss pred: 0.2806; Loss self: 0.0000; time: 0.43s
Val loss: 0.4793 score: 0.8182 time: 0.21s
Test loss: 0.5270 score: 0.8140 time: 0.34s
Epoch 90/1000, LR 0.000266
Train loss: 0.2607;  Loss pred: 0.2607; Loss self: 0.0000; time: 0.34s
Val loss: 0.4772 score: 0.8182 time: 0.22s
Test loss: 0.5230 score: 0.8140 time: 0.20s
Epoch 91/1000, LR 0.000266
Train loss: 0.2476;  Loss pred: 0.2476; Loss self: 0.0000; time: 0.35s
Val loss: 0.4759 score: 0.8182 time: 0.21s
Test loss: 0.5197 score: 0.8140 time: 0.19s
Epoch 92/1000, LR 0.000266
Train loss: 0.2327;  Loss pred: 0.2327; Loss self: 0.0000; time: 0.34s
Val loss: 0.4758 score: 0.8182 time: 0.20s
Test loss: 0.5172 score: 0.8140 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 0.2293;  Loss pred: 0.2293; Loss self: 0.0000; time: 0.30s
Val loss: 0.4768 score: 0.8182 time: 0.23s
Test loss: 0.5154 score: 0.8140 time: 0.18s
     INFO: Early stopping counter 1 of 2
Epoch 94/1000, LR 0.000265
Train loss: 0.1993;  Loss pred: 0.1993; Loss self: 0.0000; time: 0.42s
Val loss: 0.4801 score: 0.8182 time: 0.19s
Test loss: 0.5172 score: 0.8140 time: 0.20s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 091,   Train_Loss: 0.2327,   Val_Loss: 0.4758,   Val_Precision: 0.8889,   Val_Recall: 0.7273,   Val_accuracy: 0.8000,   Val_Score: 0.8182,   Val_Loss: 0.4758,   Test_Precision: 0.8889,   Test_Recall: 0.7273,   Test_accuracy: 0.8000,   Test_Score: 0.8140,   Test_loss: 0.5172


[0.19713162095285952, 0.22920872899703681, 0.23005992302205414, 0.19949545699637383, 0.22712681198026985, 0.21923925599548966, 0.2267439819406718, 0.21074741799384356, 0.21498056198470294, 0.22731553192716092, 0.21652775001712143, 0.2065255899215117, 0.21105593908578157, 0.2231210300233215, 0.22004244290292263, 0.23138600902166218, 0.2287252020323649, 0.19947855698410422, 0.2283304890152067, 0.20496008405461907, 0.23771347105503082, 0.21878368104808033, 0.23411090997979045, 0.19325228896923363, 0.21596957801375538, 0.22561158693861216, 0.20186153298709542, 0.22584467998240143, 0.20066162396688014, 0.22674467402976006, 0.2027860271045938, 0.2635700849350542, 0.21057493903208524, 0.21969607996288687, 0.30838725494686514, 0.20095182699151337, 0.22547778696753085, 0.21086700889281929, 0.21553722908720374, 0.2269744420191273, 0.21611578692682087, 0.22772368392907083, 0.20032804599031806, 0.22596722492016852, 0.2058582230238244, 0.2161397390300408, 0.2316172739956528, 0.19722359103616327, 0.22235691198147833, 0.30869364098180085, 0.21904785395599902, 0.20167745696380734, 0.19973964802920818, 0.22927696502301842, 0.1871840839739889, 0.21212612104136497, 0.22744919103570282, 0.20044733001850545, 0.22512534609995782, 0.20698259805794805, 0.19819483102764934, 0.21332740609068424, 0.20269839407410473, 0.22388405702076852, 0.21028061409015208, 0.22684923594351858, 0.19051526603288949, 0.20857967995107174, 0.22238733409903944, 0.20415830006822944, 0.21750200097449124, 0.20339257502928376, 0.23837803408969194, 0.2001676249783486, 0.2277069160481915, 0.20566058799158782, 0.20977262302767485, 0.21361254598014057, 0.30348861205857247, 0.21050990105140954, 0.21286279906053096, 0.2044873449485749, 0.1977680429117754, 0.22848730999976397, 0.2904353520134464, 0.21970687992870808, 0.21079320507124066, 0.2061683420324698, 0.21616915299091488, 0.20154842105694115, 0.22730971104465425, 0.2086457380792126, 0.23051037697587162, 0.21168164699338377, 0.20329637499526143, 0.23302869696635753, 0.21409610600676388, 0.22680465795565397, 0.19716970098670572, 0.3243252179818228, 0.21282060106750578, 0.19118949200492352, 0.20284985401667655, 0.22098138800356537, 0.21177647006697953, 0.2187227699905634, 0.20716368197463453, 0.21346592903137207, 0.2191230229800567, 0.22181331098545343, 0.18593815306667238, 0.21036354708485305, 0.2663464960642159, 0.19603362097404897, 0.226608230965212, 0.18347690789960325, 0.21138907491695136, 0.18923414300661534, 0.20310173300094903, 0.2172867429908365, 0.19607416808139533, 0.22538848698604852, 0.1928321779705584, 0.22407184494659305, 0.1964221679372713, 0.2150750079890713, 0.2110391448950395, 0.21800657105632126, 0.20765163109172136, 0.20998398202937096, 0.2201275940751657, 0.20434890501201153, 0.2223624570760876, 0.19564835098572075, 0.29646124597638845, 0.20074065902736038, 0.19710596499498934, 0.21633405203465372, 0.2023549519944936, 0.2161006280221045, 0.20379619498271495, 0.22067502001300454, 0.19312881596852094, 0.22071106499060988, 0.19832397694699466, 0.21721240296028554, 0.20632417604792863, 0.20865359297022223, 0.22738396294880658, 0.19992872804868966, 0.22500517405569553, 0.30045974196400493, 0.22586504300124943, 0.21074507490266114, 0.21948222909122705, 0.1912255259230733, 0.19260805600788444, 0.23249941691756248, 0.2046618580352515, 0.20873149496037513, 0.19922972295898944, 0.21015394804999232, 0.19717568799387664, 0.22449891199357808, 0.20456720399670303, 0.21346299501601607, 0.2044579719658941, 0.2192272610263899, 0.22106104192789644, 0.19672563998028636, 0.20932155195623636, 0.2061786890262738, 0.21405666205100715, 0.20299156592227519, 0.2178157949820161, 0.22107289102859795, 0.2091806730022654, 0.21305737493094057, 0.2015484080184251, 0.22256114205811173, 0.2106360869947821, 0.22321214503608644, 0.18216727999970317, 0.21021121798548847, 0.19583742402028292, 0.20557018800172955, 0.22512495494447649, 0.19625318504404277, 0.22689411998726428, 0.20923132100142539, 0.22849767899606377, 0.18898725498002023, 0.21132857806514949, 0.23155314300674945, 0.20266284700483084, 0.21442827698774636, 0.2063201810233295, 0.34723219100851566, 0.2029517350019887, 0.1983542850939557, 0.22167207102756947, 0.18316024506930262, 0.20866995095275342]
[0.004480264112564989, 0.005209289295387201, 0.005228634614137594, 0.004533987659008496, 0.005161972999551587, 0.004982710363533856, 0.00515327231683345, 0.004789714045314627, 0.004885921863288703, 0.005166262089253657, 0.004921085227661851, 0.004693763407307084, 0.004796725888313217, 0.005070932500530034, 0.0050009646114300595, 0.005258772932310504, 0.005198300046190111, 0.004533603567820551, 0.005189329295800152, 0.00465818372851407, 0.005402578887614337, 0.004972356387456371, 0.005320702499540692, 0.004392097476573492, 0.004908399500312622, 0.00512753606678664, 0.0045877621133430775, 0.005132833635963669, 0.004560491453792731, 0.0051532880461309105, 0.004608773343286223, 0.005990229203069413, 0.004785794068911028, 0.004993092726429247, 0.00700880124879239, 0.004567086977079849, 0.005124495158352974, 0.004792432020291348, 0.00489857338834554, 0.005158510045889257, 0.0049117224301550196, 0.005175538271115246, 0.004552910136143592, 0.005135618748185648, 0.0046785959778141905, 0.004912266796137291, 0.005264028954446654, 0.004482354341730984, 0.005053566181397235, 0.007015764567768201, 0.004978360317181796, 0.004583578567359258, 0.004539537455209277, 0.0052108401141595095, 0.0042541837266815655, 0.0048210482054855675, 0.005169299796265973, 0.004555621136784215, 0.005116485138635405, 0.0047041499558624555, 0.004504427977901121, 0.004848350138424642, 0.0046067816835023805, 0.005088274023199285, 0.0047791048656852745, 0.005155664453261786, 0.004329892409838398, 0.004740447271615267, 0.005054257593159987, 0.004639961365187032, 0.0049432272948748005, 0.004622558523392813, 0.005417682592947544, 0.004549264204053377, 0.005175157182913443, 0.004674104272536087, 0.004767559614265338, 0.00485483059045774, 0.006897468455876647, 0.00478431593298658, 0.00483779088773934, 0.004647439657922157, 0.004494728247994895, 0.005192893409085545, 0.006600803454851054, 0.004993338180197911, 0.004790754660710015, 0.004685644137101586, 0.004912935295248066, 0.004580645933112299, 0.005166129796469415, 0.004741948592709377, 0.005238872203997083, 0.004810946522576904, 0.004620372158983214, 0.005296106749235399, 0.004865820591062816, 0.0051546513171739534, 0.004481129567879675, 0.007371027681405063, 0.004836831842443313, 0.004345215727384625, 0.004610223954924467, 0.005022304272808304, 0.004813101592431353, 0.004970972045240077, 0.004708265499423512, 0.004851498387076638, 0.0049800687040921976, 0.005158449092684964, 0.004324143094573777, 0.004892175513601234, 0.006194104559632928, 0.0045589214180011385, 0.005269958859656093, 0.004266904834874494, 0.004916024998068636, 0.004400794023409659, 0.00472329611630114, 0.005053180069554337, 0.004559864373985938, 0.00524159272060578, 0.004484469255129265, 0.005210973138292861, 0.00456795739389003, 0.005001744371838868, 0.004907887090582314, 0.005069920257123751, 0.004829107699807473, 0.004883348419287697, 0.005119246373841062, 0.004752300116558408, 0.005171219932002037, 0.004549961650830715, 0.006894447580846243, 0.004668387419240939, 0.004583859651046264, 0.00503102446592218, 0.004705929116151014, 0.005025596000514058, 0.0047394463949468595, 0.0051319772096047565, 0.004491367813221417, 0.005132815464897905, 0.004612185510395225, 0.005051451231634548, 0.00479823665227741, 0.004852409138842378, 0.005287999138344339, 0.004649505303457899, 0.005232678466411524, 0.006987435859628022, 0.005252675418633708, 0.004901048253550259, 0.00510423788584249, 0.004447105254024961, 0.004479257116462429, 0.00540696318412936, 0.004759578093843058, 0.00485422081303198, 0.004633249371139289, 0.0048873011174416815, 0.004585481116136666, 0.005220904930083211, 0.004757376837132629, 0.004964255698046885, 0.004754836557346375, 0.0050983083959625555, 0.005140954463439452, 0.004575014883262473, 0.004867943068749683, 0.0047948532331691576, 0.004978061908162957, 0.004720734091215702, 0.005065483604232932, 0.005141230023920883, 0.004864666814006172, 0.0049548226728125715, 0.004687172279498259, 0.005175840512979342, 0.004898513651041444, 0.0051909801171182894, 0.00423644837208612, 0.0048886329764067085, 0.004554358698146114, 0.00478070204655185, 0.005235464068476197, 0.004564027559163786, 0.005276607441564285, 0.004865844674451753, 0.005313899511536367, 0.004395052441395819, 0.00491461809453836, 0.005384956814110452, 0.004713089465228624, 0.004986704115994101, 0.0047981437447285925, 0.008075167232756178, 0.004719807790743924, 0.004612890351022226, 0.0051551644425016155, 0.004259540583007038, 0.004852789557040777]
[223.2011271825427, 191.96476588188239, 191.25451935312543, 220.55640094500887, 193.7243763357283, 200.69398520904122, 194.05145672846444, 208.78073107061908, 204.66966684704659, 193.56354414927964, 203.20721014521604, 213.04865908734033, 208.47553587258514, 197.20238829751253, 199.9614229851635, 190.15842913769586, 192.37058098116336, 220.57508669218123, 192.70313040440965, 214.67594630901203, 185.0967881825005, 201.11189184320597, 187.94510688886, 227.6816499027597, 203.73239788984347, 195.02544438008934, 217.97119713151503, 194.82415969873023, 219.27461330255326, 194.05086442835272, 216.97747437650375, 166.93852039711547, 208.95174042194895, 200.27667315426336, 142.67775108793379, 218.95794956797388, 195.14117373493679, 208.66232338110595, 204.14106735221196, 193.8544252321241, 203.594566716678, 193.21661779239744, 219.63974031936866, 194.71850404652562, 213.7393364894041, 203.5720048402785, 189.9685599478467, 223.09704315206432, 197.88006411811057, 142.5361399089981, 200.86934980352945, 218.17014485608155, 220.286760461127, 191.9076344873223, 235.0627204293408, 207.42377121684095, 193.4497977312801, 219.5090350963412, 195.44667342993702, 212.5782573648124, 222.00377160119652, 206.2557305988892, 217.0712807123375, 196.53029601798914, 209.24420536995484, 193.96142031069135, 230.95262083829064, 210.95055860820884, 197.85299454331673, 215.51903589173332, 202.29698946613527, 216.3304141936599, 184.5807654552793, 219.8157669341349, 193.230845876846, 213.94473501067566, 209.75091680192784, 205.98041092628827, 144.98072827690854, 209.0162970018905, 206.70591664768955, 215.17223968586052, 222.48286099300915, 192.57086969094894, 151.49670897488718, 200.2668283045001, 208.73538112924254, 213.41782916928324, 203.5443049631102, 218.309822370522, 193.56850086953102, 210.88377076408503, 190.8807775912216, 207.85930488048047, 216.43278194717234, 188.817946342258, 205.5151811056756, 193.9995430279175, 223.15801961360546, 135.66629284580034, 206.7469022232645, 230.13817097681763, 216.90920219436148, 199.11179125768768, 207.76623572054024, 201.1678985315445, 212.39244051178542, 206.12188652144826, 200.80044260800756, 193.85671585245825, 231.25969195026565, 204.40803835017743, 161.44383588824363, 219.35012875006967, 189.75480200717507, 234.36191775985878, 203.41637814959668, 227.23172106683086, 211.71655881340547, 197.89518406934477, 219.30476829640108, 190.78170573398316, 222.9918287111046, 191.9027355277453, 218.9162274888053, 199.93024946062062, 203.75366864467767, 197.24176107008762, 207.0775932456151, 204.7775243827193, 195.341252788676, 210.42442090635367, 193.37796751043427, 219.78207218898729, 145.04425311436734, 214.20672926125656, 218.15676659553714, 198.76667401908597, 212.49788836978942, 198.98137452706345, 210.99510716403248, 194.85667203050883, 222.6493223414614, 194.82484941037924, 216.81695104113638, 197.96291286304674, 208.4098956489287, 206.08319937311933, 189.1074438247919, 215.07664466073126, 191.10671645869002, 143.1140149389844, 190.37917257413815, 204.0379829510171, 195.91563370776225, 224.8653771113076, 223.2513057410215, 184.94670038353922, 210.10265621938, 206.00628576996957, 215.83124928026606, 204.6119066474591, 218.0796245089576, 191.53767658896288, 210.19987153313696, 201.44006691545636, 210.3121711838797, 196.14348963117226, 194.516408793664, 218.57852389911645, 205.42557418545306, 208.55695708939356, 200.88139088029698, 211.83146109855895, 197.41451717746313, 194.5059830716084, 205.56392415629296, 201.823569890213, 213.34824930033204, 193.20533495812356, 204.1435568496162, 192.6418474812302, 236.0467807394943, 204.5561621881113, 219.56988157455788, 209.17429914321147, 191.00503545066903, 219.10472428944286, 189.51570892367602, 205.51416391290223, 188.1857189487721, 227.52857066761445, 203.47460998267698, 185.702510627319, 212.1750514980924, 200.53325337524055, 208.4139311371475, 123.83644464273029, 211.87303473694692, 216.78382183491487, 193.98023305629724, 234.76710234652737, 206.06704417032216]
Elapsed: 0.2162539883622248~0.023244421947753138
Time per graph: 0.004967005252650137~0.0005297284298434567
Speed: 203.16537562147016~17.562901031094878
Total Time: 0.2095
best val loss: 0.47583943605422974 test_score: 0.8140

Testing...
Test loss: 0.6399 score: 0.6977 time: 0.24s
test Score 0.6977
Epoch Time List: [0.7187908830819651, 0.8099405858665705, 0.7892764230491593, 0.7691002930514514, 0.7870204178616405, 0.7609148488845676, 0.8133880511159077, 0.7536462909774855, 0.7905409910017624, 0.8014952337834984, 0.840861066011712, 0.7561806011945009, 0.7267973079578951, 0.8152129440568388, 0.7511090820189565, 0.8390064099803567, 0.8127623070031404, 0.7620169429574162, 0.8118296769680455, 0.8837945461273193, 0.8103100060252473, 0.7455872889840975, 0.818573541007936, 0.7231642759870738, 0.8186835971428081, 0.7974246608791873, 0.7460343650309369, 0.8876215890049934, 0.7500274189515039, 0.7819136319449171, 0.7113079830305651, 0.8698543170467019, 0.7570639118785039, 0.7863324610516429, 0.8584822549019009, 0.7539411700563505, 0.7935636161128059, 0.750584244960919, 0.7910177819430828, 0.7955891389865428, 0.7366951819276437, 0.7937384290853515, 0.8448574650101364, 0.8371233119396493, 0.7299459909554571, 0.7767199870431796, 0.8001176918623969, 0.7278467180440202, 0.7787667859811336, 0.8717806262429804, 0.7671118580037728, 0.7626969860866666, 0.7739716989453882, 0.7645181040279567, 0.6884431561920792, 0.7921270270599052, 0.8207065940368921, 0.7319079850567505, 0.7772948219208047, 0.7388118138769642, 0.7649029970634729, 0.9124931680271402, 0.7629036779981107, 0.7939712370280176, 0.7126472550444305, 0.8338400539942086, 0.7222169450251386, 0.8198915290413424, 0.7693301460240036, 0.8595348689705133, 0.7738273240393028, 0.774035581969656, 0.8756562878843397, 0.7757323129335418, 0.7740406870143488, 0.7422311330446973, 0.8513688981765881, 0.8406878729583696, 0.9673156979260966, 0.7454716619104147, 0.9233602149179205, 0.7260452040936798, 0.7486323179909959, 0.7878230300266296, 0.8248524130322039, 0.7654973169555888, 0.7363554680487141, 0.8360748760169372, 0.7683531630318612, 0.7354616539087147, 0.793669311911799, 0.7673577399691567, 0.9497937289997935, 0.7690339110558853, 0.7659654000308365, 0.8026534270029515, 0.741617848048918, 0.8874269680818543, 0.7797088660299778, 0.9564900972181931, 0.8167023309506476, 0.8429044469958171, 0.7581101660616696, 0.8016599391121417, 0.7111334489891306, 0.7868915201397613, 0.7633705689804628, 0.773661358980462, 0.7803304221015424, 0.8575778751401231, 0.7217520530102775, 0.7934511329513043, 0.8480188187677413, 0.7579315741313621, 0.77970657707192, 0.7172671019798145, 0.7927281630691141, 0.8768239230848849, 0.7666936069726944, 0.761224546120502, 0.804302244912833, 0.8137429291382432, 0.8491208950290456, 0.7878851308487356, 0.754049779032357, 0.8542500251205638, 0.8759521130705252, 0.7796594799729064, 0.7536487150937319, 0.7886510819662362, 0.7733865760965273, 0.7404871839098632, 0.8191987049067393, 0.7456987110199407, 0.8655263379914686, 0.7579060848802328, 0.7725591490743682, 0.7628930039936677, 0.8335207428317517, 0.9523761300370097, 0.7770670909667388, 0.9298087240895256, 0.7472598800668493, 1.0114153480390087, 0.7179372970713302, 0.8885081569897011, 0.7739636600017548, 0.8337156160268933, 0.8022719720611349, 0.7873690919950604, 0.7816270319744945, 0.8354695021407679, 0.7824025499867275, 0.7518440108979121, 0.7959081790177152, 0.7677194911520928, 0.7732016408117488, 0.8935485240072012, 0.8275367009919137, 0.792010054923594, 0.7700597631046548, 0.870291948900558, 0.7665549629600719, 0.8025294460821897, 0.7856303708394989, 0.7965236810268834, 0.7524020980345085, 0.843331390991807, 0.7647356061497703, 0.8306587870465592, 0.8246309339301661, 0.7429627709789202, 0.7853012350387871, 0.7501676640240476, 0.8237290381221101, 0.7556211609626189, 0.7316847359761596, 0.9324636560631916, 0.7765777981840074, 0.7918778759194538, 0.7510397470323369, 0.8397229489637539, 0.7431346159428358, 0.7882386611308903, 0.7684613659512252, 0.7271234629442915, 0.9561859540408477, 0.748380116885528, 0.8288967048283666, 0.747235918068327, 0.8237749300897121, 0.7189778200117871, 0.7890034238807857, 0.8163037869380787, 0.7372872969135642, 0.8822675428818911, 0.7654476369498298, 0.9774260788690299, 0.761963743949309, 0.7544270298676565, 0.766663089976646, 0.7086967069189996, 0.8145428320858628]
Total Epoch List: [109, 94]
Total Time List: [0.21962832601275295, 0.20951268705539405]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcece0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.31s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 008,   Train_Loss: 0.6927,   Val_Loss: 0.6934,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6934,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6938


[0.19713162095285952, 0.22920872899703681, 0.23005992302205414, 0.19949545699637383, 0.22712681198026985, 0.21923925599548966, 0.2267439819406718, 0.21074741799384356, 0.21498056198470294, 0.22731553192716092, 0.21652775001712143, 0.2065255899215117, 0.21105593908578157, 0.2231210300233215, 0.22004244290292263, 0.23138600902166218, 0.2287252020323649, 0.19947855698410422, 0.2283304890152067, 0.20496008405461907, 0.23771347105503082, 0.21878368104808033, 0.23411090997979045, 0.19325228896923363, 0.21596957801375538, 0.22561158693861216, 0.20186153298709542, 0.22584467998240143, 0.20066162396688014, 0.22674467402976006, 0.2027860271045938, 0.2635700849350542, 0.21057493903208524, 0.21969607996288687, 0.30838725494686514, 0.20095182699151337, 0.22547778696753085, 0.21086700889281929, 0.21553722908720374, 0.2269744420191273, 0.21611578692682087, 0.22772368392907083, 0.20032804599031806, 0.22596722492016852, 0.2058582230238244, 0.2161397390300408, 0.2316172739956528, 0.19722359103616327, 0.22235691198147833, 0.30869364098180085, 0.21904785395599902, 0.20167745696380734, 0.19973964802920818, 0.22927696502301842, 0.1871840839739889, 0.21212612104136497, 0.22744919103570282, 0.20044733001850545, 0.22512534609995782, 0.20698259805794805, 0.19819483102764934, 0.21332740609068424, 0.20269839407410473, 0.22388405702076852, 0.21028061409015208, 0.22684923594351858, 0.19051526603288949, 0.20857967995107174, 0.22238733409903944, 0.20415830006822944, 0.21750200097449124, 0.20339257502928376, 0.23837803408969194, 0.2001676249783486, 0.2277069160481915, 0.20566058799158782, 0.20977262302767485, 0.21361254598014057, 0.30348861205857247, 0.21050990105140954, 0.21286279906053096, 0.2044873449485749, 0.1977680429117754, 0.22848730999976397, 0.2904353520134464, 0.21970687992870808, 0.21079320507124066, 0.2061683420324698, 0.21616915299091488, 0.20154842105694115, 0.22730971104465425, 0.2086457380792126, 0.23051037697587162, 0.21168164699338377, 0.20329637499526143, 0.23302869696635753, 0.21409610600676388, 0.22680465795565397, 0.19716970098670572, 0.3243252179818228, 0.21282060106750578, 0.19118949200492352, 0.20284985401667655, 0.22098138800356537, 0.21177647006697953, 0.2187227699905634, 0.20716368197463453, 0.21346592903137207, 0.2191230229800567, 0.22181331098545343, 0.18593815306667238, 0.21036354708485305, 0.2663464960642159, 0.19603362097404897, 0.226608230965212, 0.18347690789960325, 0.21138907491695136, 0.18923414300661534, 0.20310173300094903, 0.2172867429908365, 0.19607416808139533, 0.22538848698604852, 0.1928321779705584, 0.22407184494659305, 0.1964221679372713, 0.2150750079890713, 0.2110391448950395, 0.21800657105632126, 0.20765163109172136, 0.20998398202937096, 0.2201275940751657, 0.20434890501201153, 0.2223624570760876, 0.19564835098572075, 0.29646124597638845, 0.20074065902736038, 0.19710596499498934, 0.21633405203465372, 0.2023549519944936, 0.2161006280221045, 0.20379619498271495, 0.22067502001300454, 0.19312881596852094, 0.22071106499060988, 0.19832397694699466, 0.21721240296028554, 0.20632417604792863, 0.20865359297022223, 0.22738396294880658, 0.19992872804868966, 0.22500517405569553, 0.30045974196400493, 0.22586504300124943, 0.21074507490266114, 0.21948222909122705, 0.1912255259230733, 0.19260805600788444, 0.23249941691756248, 0.2046618580352515, 0.20873149496037513, 0.19922972295898944, 0.21015394804999232, 0.19717568799387664, 0.22449891199357808, 0.20456720399670303, 0.21346299501601607, 0.2044579719658941, 0.2192272610263899, 0.22106104192789644, 0.19672563998028636, 0.20932155195623636, 0.2061786890262738, 0.21405666205100715, 0.20299156592227519, 0.2178157949820161, 0.22107289102859795, 0.2091806730022654, 0.21305737493094057, 0.2015484080184251, 0.22256114205811173, 0.2106360869947821, 0.22321214503608644, 0.18216727999970317, 0.21021121798548847, 0.19583742402028292, 0.20557018800172955, 0.22512495494447649, 0.19625318504404277, 0.22689411998726428, 0.20923132100142539, 0.22849767899606377, 0.18898725498002023, 0.21132857806514949, 0.23155314300674945, 0.20266284700483084, 0.21442827698774636, 0.2063201810233295, 0.34723219100851566, 0.2029517350019887, 0.1983542850939557, 0.22167207102756947, 0.18316024506930262, 0.20866995095275342, 0.2495575100183487, 0.23503990296740085, 0.24579419195652008, 0.23268168605864048, 0.23888568195980042, 0.250631368951872, 0.22443617007229477, 0.3153400350129232, 0.22494649607688189, 0.24736447900068015, 0.21704552706796676]
[0.004480264112564989, 0.005209289295387201, 0.005228634614137594, 0.004533987659008496, 0.005161972999551587, 0.004982710363533856, 0.00515327231683345, 0.004789714045314627, 0.004885921863288703, 0.005166262089253657, 0.004921085227661851, 0.004693763407307084, 0.004796725888313217, 0.005070932500530034, 0.0050009646114300595, 0.005258772932310504, 0.005198300046190111, 0.004533603567820551, 0.005189329295800152, 0.00465818372851407, 0.005402578887614337, 0.004972356387456371, 0.005320702499540692, 0.004392097476573492, 0.004908399500312622, 0.00512753606678664, 0.0045877621133430775, 0.005132833635963669, 0.004560491453792731, 0.0051532880461309105, 0.004608773343286223, 0.005990229203069413, 0.004785794068911028, 0.004993092726429247, 0.00700880124879239, 0.004567086977079849, 0.005124495158352974, 0.004792432020291348, 0.00489857338834554, 0.005158510045889257, 0.0049117224301550196, 0.005175538271115246, 0.004552910136143592, 0.005135618748185648, 0.0046785959778141905, 0.004912266796137291, 0.005264028954446654, 0.004482354341730984, 0.005053566181397235, 0.007015764567768201, 0.004978360317181796, 0.004583578567359258, 0.004539537455209277, 0.0052108401141595095, 0.0042541837266815655, 0.0048210482054855675, 0.005169299796265973, 0.004555621136784215, 0.005116485138635405, 0.0047041499558624555, 0.004504427977901121, 0.004848350138424642, 0.0046067816835023805, 0.005088274023199285, 0.0047791048656852745, 0.005155664453261786, 0.004329892409838398, 0.004740447271615267, 0.005054257593159987, 0.004639961365187032, 0.0049432272948748005, 0.004622558523392813, 0.005417682592947544, 0.004549264204053377, 0.005175157182913443, 0.004674104272536087, 0.004767559614265338, 0.00485483059045774, 0.006897468455876647, 0.00478431593298658, 0.00483779088773934, 0.004647439657922157, 0.004494728247994895, 0.005192893409085545, 0.006600803454851054, 0.004993338180197911, 0.004790754660710015, 0.004685644137101586, 0.004912935295248066, 0.004580645933112299, 0.005166129796469415, 0.004741948592709377, 0.005238872203997083, 0.004810946522576904, 0.004620372158983214, 0.005296106749235399, 0.004865820591062816, 0.0051546513171739534, 0.004481129567879675, 0.007371027681405063, 0.004836831842443313, 0.004345215727384625, 0.004610223954924467, 0.005022304272808304, 0.004813101592431353, 0.004970972045240077, 0.004708265499423512, 0.004851498387076638, 0.0049800687040921976, 0.005158449092684964, 0.004324143094573777, 0.004892175513601234, 0.006194104559632928, 0.0045589214180011385, 0.005269958859656093, 0.004266904834874494, 0.004916024998068636, 0.004400794023409659, 0.00472329611630114, 0.005053180069554337, 0.004559864373985938, 0.00524159272060578, 0.004484469255129265, 0.005210973138292861, 0.00456795739389003, 0.005001744371838868, 0.004907887090582314, 0.005069920257123751, 0.004829107699807473, 0.004883348419287697, 0.005119246373841062, 0.004752300116558408, 0.005171219932002037, 0.004549961650830715, 0.006894447580846243, 0.004668387419240939, 0.004583859651046264, 0.00503102446592218, 0.004705929116151014, 0.005025596000514058, 0.0047394463949468595, 0.0051319772096047565, 0.004491367813221417, 0.005132815464897905, 0.004612185510395225, 0.005051451231634548, 0.00479823665227741, 0.004852409138842378, 0.005287999138344339, 0.004649505303457899, 0.005232678466411524, 0.006987435859628022, 0.005252675418633708, 0.004901048253550259, 0.00510423788584249, 0.004447105254024961, 0.004479257116462429, 0.00540696318412936, 0.004759578093843058, 0.00485422081303198, 0.004633249371139289, 0.0048873011174416815, 0.004585481116136666, 0.005220904930083211, 0.004757376837132629, 0.004964255698046885, 0.004754836557346375, 0.0050983083959625555, 0.005140954463439452, 0.004575014883262473, 0.004867943068749683, 0.0047948532331691576, 0.004978061908162957, 0.004720734091215702, 0.005065483604232932, 0.005141230023920883, 0.004864666814006172, 0.0049548226728125715, 0.004687172279498259, 0.005175840512979342, 0.004898513651041444, 0.0051909801171182894, 0.00423644837208612, 0.0048886329764067085, 0.004554358698146114, 0.00478070204655185, 0.005235464068476197, 0.004564027559163786, 0.005276607441564285, 0.004865844674451753, 0.005313899511536367, 0.004395052441395819, 0.00491461809453836, 0.005384956814110452, 0.004713089465228624, 0.004986704115994101, 0.0047981437447285925, 0.008075167232756178, 0.004719807790743924, 0.004612890351022226, 0.0051551644425016155, 0.004259540583007038, 0.004852789557040777, 0.005803663023682528, 0.005466044255055834, 0.0057161439989888395, 0.005411202001363732, 0.0055554809758093125, 0.005828636487252837, 0.0052194458156347625, 0.007333489186347051, 0.005231313862253067, 0.005752662302341399, 0.005047570396929459]
[223.2011271825427, 191.96476588188239, 191.25451935312543, 220.55640094500887, 193.7243763357283, 200.69398520904122, 194.05145672846444, 208.78073107061908, 204.66966684704659, 193.56354414927964, 203.20721014521604, 213.04865908734033, 208.47553587258514, 197.20238829751253, 199.9614229851635, 190.15842913769586, 192.37058098116336, 220.57508669218123, 192.70313040440965, 214.67594630901203, 185.0967881825005, 201.11189184320597, 187.94510688886, 227.6816499027597, 203.73239788984347, 195.02544438008934, 217.97119713151503, 194.82415969873023, 219.27461330255326, 194.05086442835272, 216.97747437650375, 166.93852039711547, 208.95174042194895, 200.27667315426336, 142.67775108793379, 218.95794956797388, 195.14117373493679, 208.66232338110595, 204.14106735221196, 193.8544252321241, 203.594566716678, 193.21661779239744, 219.63974031936866, 194.71850404652562, 213.7393364894041, 203.5720048402785, 189.9685599478467, 223.09704315206432, 197.88006411811057, 142.5361399089981, 200.86934980352945, 218.17014485608155, 220.286760461127, 191.9076344873223, 235.0627204293408, 207.42377121684095, 193.4497977312801, 219.5090350963412, 195.44667342993702, 212.5782573648124, 222.00377160119652, 206.2557305988892, 217.0712807123375, 196.53029601798914, 209.24420536995484, 193.96142031069135, 230.95262083829064, 210.95055860820884, 197.85299454331673, 215.51903589173332, 202.29698946613527, 216.3304141936599, 184.5807654552793, 219.8157669341349, 193.230845876846, 213.94473501067566, 209.75091680192784, 205.98041092628827, 144.98072827690854, 209.0162970018905, 206.70591664768955, 215.17223968586052, 222.48286099300915, 192.57086969094894, 151.49670897488718, 200.2668283045001, 208.73538112924254, 213.41782916928324, 203.5443049631102, 218.309822370522, 193.56850086953102, 210.88377076408503, 190.8807775912216, 207.85930488048047, 216.43278194717234, 188.817946342258, 205.5151811056756, 193.9995430279175, 223.15801961360546, 135.66629284580034, 206.7469022232645, 230.13817097681763, 216.90920219436148, 199.11179125768768, 207.76623572054024, 201.1678985315445, 212.39244051178542, 206.12188652144826, 200.80044260800756, 193.85671585245825, 231.25969195026565, 204.40803835017743, 161.44383588824363, 219.35012875006967, 189.75480200717507, 234.36191775985878, 203.41637814959668, 227.23172106683086, 211.71655881340547, 197.89518406934477, 219.30476829640108, 190.78170573398316, 222.9918287111046, 191.9027355277453, 218.9162274888053, 199.93024946062062, 203.75366864467767, 197.24176107008762, 207.0775932456151, 204.7775243827193, 195.341252788676, 210.42442090635367, 193.37796751043427, 219.78207218898729, 145.04425311436734, 214.20672926125656, 218.15676659553714, 198.76667401908597, 212.49788836978942, 198.98137452706345, 210.99510716403248, 194.85667203050883, 222.6493223414614, 194.82484941037924, 216.81695104113638, 197.96291286304674, 208.4098956489287, 206.08319937311933, 189.1074438247919, 215.07664466073126, 191.10671645869002, 143.1140149389844, 190.37917257413815, 204.0379829510171, 195.91563370776225, 224.8653771113076, 223.2513057410215, 184.94670038353922, 210.10265621938, 206.00628576996957, 215.83124928026606, 204.6119066474591, 218.0796245089576, 191.53767658896288, 210.19987153313696, 201.44006691545636, 210.3121711838797, 196.14348963117226, 194.516408793664, 218.57852389911645, 205.42557418545306, 208.55695708939356, 200.88139088029698, 211.83146109855895, 197.41451717746313, 194.5059830716084, 205.56392415629296, 201.823569890213, 213.34824930033204, 193.20533495812356, 204.1435568496162, 192.6418474812302, 236.0467807394943, 204.5561621881113, 219.56988157455788, 209.17429914321147, 191.00503545066903, 219.10472428944286, 189.51570892367602, 205.51416391290223, 188.1857189487721, 227.52857066761445, 203.47460998267698, 185.702510627319, 212.1750514980924, 200.53325337524055, 208.4139311371475, 123.83644464273029, 211.87303473694692, 216.78382183491487, 193.98023305629724, 234.76710234652737, 206.06704417032216, 172.3049728971828, 182.94765891714965, 174.9431085320621, 184.80182402135048, 180.0024164162171, 171.56671241841704, 191.59122162060135, 136.36073833199703, 191.1565672278955, 173.83255742180253, 198.11511704885194]
Elapsed: 0.21766954526483628~0.02411811799743477
Time per graph: 0.005003120180344097~0.0005546700961391154
Speed: 201.87006610285974~18.331602430973742
Total Time: 0.2177
best val loss: 0.6933566927909851 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.23s
test Score 0.4884
Epoch Time List: [0.7187908830819651, 0.8099405858665705, 0.7892764230491593, 0.7691002930514514, 0.7870204178616405, 0.7609148488845676, 0.8133880511159077, 0.7536462909774855, 0.7905409910017624, 0.8014952337834984, 0.840861066011712, 0.7561806011945009, 0.7267973079578951, 0.8152129440568388, 0.7511090820189565, 0.8390064099803567, 0.8127623070031404, 0.7620169429574162, 0.8118296769680455, 0.8837945461273193, 0.8103100060252473, 0.7455872889840975, 0.818573541007936, 0.7231642759870738, 0.8186835971428081, 0.7974246608791873, 0.7460343650309369, 0.8876215890049934, 0.7500274189515039, 0.7819136319449171, 0.7113079830305651, 0.8698543170467019, 0.7570639118785039, 0.7863324610516429, 0.8584822549019009, 0.7539411700563505, 0.7935636161128059, 0.750584244960919, 0.7910177819430828, 0.7955891389865428, 0.7366951819276437, 0.7937384290853515, 0.8448574650101364, 0.8371233119396493, 0.7299459909554571, 0.7767199870431796, 0.8001176918623969, 0.7278467180440202, 0.7787667859811336, 0.8717806262429804, 0.7671118580037728, 0.7626969860866666, 0.7739716989453882, 0.7645181040279567, 0.6884431561920792, 0.7921270270599052, 0.8207065940368921, 0.7319079850567505, 0.7772948219208047, 0.7388118138769642, 0.7649029970634729, 0.9124931680271402, 0.7629036779981107, 0.7939712370280176, 0.7126472550444305, 0.8338400539942086, 0.7222169450251386, 0.8198915290413424, 0.7693301460240036, 0.8595348689705133, 0.7738273240393028, 0.774035581969656, 0.8756562878843397, 0.7757323129335418, 0.7740406870143488, 0.7422311330446973, 0.8513688981765881, 0.8406878729583696, 0.9673156979260966, 0.7454716619104147, 0.9233602149179205, 0.7260452040936798, 0.7486323179909959, 0.7878230300266296, 0.8248524130322039, 0.7654973169555888, 0.7363554680487141, 0.8360748760169372, 0.7683531630318612, 0.7354616539087147, 0.793669311911799, 0.7673577399691567, 0.9497937289997935, 0.7690339110558853, 0.7659654000308365, 0.8026534270029515, 0.741617848048918, 0.8874269680818543, 0.7797088660299778, 0.9564900972181931, 0.8167023309506476, 0.8429044469958171, 0.7581101660616696, 0.8016599391121417, 0.7111334489891306, 0.7868915201397613, 0.7633705689804628, 0.773661358980462, 0.7803304221015424, 0.8575778751401231, 0.7217520530102775, 0.7934511329513043, 0.8480188187677413, 0.7579315741313621, 0.77970657707192, 0.7172671019798145, 0.7927281630691141, 0.8768239230848849, 0.7666936069726944, 0.761224546120502, 0.804302244912833, 0.8137429291382432, 0.8491208950290456, 0.7878851308487356, 0.754049779032357, 0.8542500251205638, 0.8759521130705252, 0.7796594799729064, 0.7536487150937319, 0.7886510819662362, 0.7733865760965273, 0.7404871839098632, 0.8191987049067393, 0.7456987110199407, 0.8655263379914686, 0.7579060848802328, 0.7725591490743682, 0.7628930039936677, 0.8335207428317517, 0.9523761300370097, 0.7770670909667388, 0.9298087240895256, 0.7472598800668493, 1.0114153480390087, 0.7179372970713302, 0.8885081569897011, 0.7739636600017548, 0.8337156160268933, 0.8022719720611349, 0.7873690919950604, 0.7816270319744945, 0.8354695021407679, 0.7824025499867275, 0.7518440108979121, 0.7959081790177152, 0.7677194911520928, 0.7732016408117488, 0.8935485240072012, 0.8275367009919137, 0.792010054923594, 0.7700597631046548, 0.870291948900558, 0.7665549629600719, 0.8025294460821897, 0.7856303708394989, 0.7965236810268834, 0.7524020980345085, 0.843331390991807, 0.7647356061497703, 0.8306587870465592, 0.8246309339301661, 0.7429627709789202, 0.7853012350387871, 0.7501676640240476, 0.8237290381221101, 0.7556211609626189, 0.7316847359761596, 0.9324636560631916, 0.7765777981840074, 0.7918778759194538, 0.7510397470323369, 0.8397229489637539, 0.7431346159428358, 0.7882386611308903, 0.7684613659512252, 0.7271234629442915, 0.9561859540408477, 0.748380116885528, 0.8288967048283666, 0.747235918068327, 0.8237749300897121, 0.7189778200117871, 0.7890034238807857, 0.8163037869380787, 0.7372872969135642, 0.8822675428818911, 0.7654476369498298, 0.9774260788690299, 0.761963743949309, 0.7544270298676565, 0.766663089976646, 0.7086967069189996, 0.8145428320858628, 0.7882190890377387, 0.742099617025815, 0.7908278140239418, 0.7541929469443858, 0.8072325360262766, 0.7995875719934702, 0.793405048083514, 0.8655677218921483, 0.784288304974325, 0.7562978320056573, 0.7128014949848875]
Total Epoch List: [109, 94, 11]
Total Time List: [0.21962832601275295, 0.20951268705539405, 0.21771081408951432]
========================training times:5========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcef20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.20s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.30s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.21s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.21s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.21s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.20s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.20s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.19s
Epoch 13/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.23s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.21s
Epoch 15/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.22s
Epoch 17/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.19s
Epoch 18/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.19s
Epoch 20/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.21s
Epoch 21/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.19s
Epoch 22/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.21s
Epoch 23/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4884 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.19s
Epoch 24/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.20s
Epoch 26/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4884 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.21s
Epoch 27/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5000 time: 0.21s
Epoch 28/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.5000 time: 0.21s
Epoch 29/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.4884 time: 0.25s
Test loss: 0.6898 score: 0.5227 time: 0.20s
Epoch 30/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4884 time: 0.25s
Test loss: 0.6892 score: 0.5227 time: 0.21s
Epoch 31/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.34s
Val loss: 0.6896 score: 0.5116 time: 0.25s
Test loss: 0.6886 score: 0.5455 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.30s
Val loss: 0.6889 score: 0.5349 time: 0.26s
Test loss: 0.6878 score: 0.5682 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.32s
Val loss: 0.6881 score: 0.6047 time: 0.24s
Test loss: 0.6870 score: 0.6136 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.28s
Val loss: 0.6872 score: 0.6047 time: 0.26s
Test loss: 0.6861 score: 0.6136 time: 0.20s
Epoch 35/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.36s
Val loss: 0.6861 score: 0.6279 time: 0.25s
Test loss: 0.6850 score: 0.6136 time: 0.21s
Epoch 36/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.30s
Val loss: 0.6849 score: 0.6279 time: 0.25s
Test loss: 0.6839 score: 0.6136 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.30s
Val loss: 0.6836 score: 0.6512 time: 0.25s
Test loss: 0.6826 score: 0.6591 time: 0.19s
Epoch 38/1000, LR 0.000270
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.42s
Val loss: 0.6821 score: 0.6744 time: 0.34s
Test loss: 0.6811 score: 0.6818 time: 0.20s
Epoch 39/1000, LR 0.000269
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.31s
Val loss: 0.6804 score: 0.6744 time: 0.26s
Test loss: 0.6794 score: 0.7273 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.33s
Val loss: 0.6785 score: 0.6977 time: 0.25s
Test loss: 0.6776 score: 0.7500 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 0.31s
Val loss: 0.6765 score: 0.6977 time: 0.25s
Test loss: 0.6756 score: 0.7500 time: 0.19s
Epoch 42/1000, LR 0.000269
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.32s
Val loss: 0.6742 score: 0.6977 time: 0.23s
Test loss: 0.6734 score: 0.7500 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.26s
Val loss: 0.6717 score: 0.7209 time: 0.26s
Test loss: 0.6710 score: 0.7500 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.32s
Val loss: 0.6690 score: 0.6977 time: 0.22s
Test loss: 0.6683 score: 0.7727 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.36s
Val loss: 0.6661 score: 0.6977 time: 0.26s
Test loss: 0.6655 score: 0.7727 time: 0.20s
Epoch 46/1000, LR 0.000269
Train loss: 0.6579;  Loss pred: 0.6579; Loss self: 0.0000; time: 0.32s
Val loss: 0.6630 score: 0.7209 time: 0.24s
Test loss: 0.6623 score: 0.7727 time: 0.21s
Epoch 47/1000, LR 0.000269
Train loss: 0.6514;  Loss pred: 0.6514; Loss self: 0.0000; time: 0.41s
Val loss: 0.6595 score: 0.7442 time: 0.26s
Test loss: 0.6589 score: 0.7727 time: 0.30s
Epoch 48/1000, LR 0.000269
Train loss: 0.6495;  Loss pred: 0.6495; Loss self: 0.0000; time: 0.32s
Val loss: 0.6558 score: 0.7674 time: 0.25s
Test loss: 0.6552 score: 0.7727 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6452;  Loss pred: 0.6452; Loss self: 0.0000; time: 0.35s
Val loss: 0.6518 score: 0.7674 time: 0.26s
Test loss: 0.6512 score: 0.8182 time: 0.21s
Epoch 50/1000, LR 0.000269
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 0.33s
Val loss: 0.6475 score: 0.7674 time: 0.23s
Test loss: 0.6469 score: 0.8182 time: 0.21s
Epoch 51/1000, LR 0.000269
Train loss: 0.6333;  Loss pred: 0.6333; Loss self: 0.0000; time: 0.29s
Val loss: 0.6429 score: 0.8140 time: 0.24s
Test loss: 0.6422 score: 0.7955 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6261;  Loss pred: 0.6261; Loss self: 0.0000; time: 0.34s
Val loss: 0.6379 score: 0.8372 time: 0.26s
Test loss: 0.6373 score: 0.7955 time: 0.19s
Epoch 53/1000, LR 0.000269
Train loss: 0.6269;  Loss pred: 0.6269; Loss self: 0.0000; time: 0.29s
Val loss: 0.6327 score: 0.8372 time: 0.23s
Test loss: 0.6320 score: 0.7955 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6187;  Loss pred: 0.6187; Loss self: 0.0000; time: 0.27s
Val loss: 0.6272 score: 0.8372 time: 0.25s
Test loss: 0.6264 score: 0.7955 time: 0.20s
Epoch 55/1000, LR 0.000269
Train loss: 0.6086;  Loss pred: 0.6086; Loss self: 0.0000; time: 0.39s
Val loss: 0.6213 score: 0.8372 time: 0.34s
Test loss: 0.6205 score: 0.7955 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6047;  Loss pred: 0.6047; Loss self: 0.0000; time: 0.26s
Val loss: 0.6151 score: 0.8140 time: 0.25s
Test loss: 0.6143 score: 0.7955 time: 0.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.6015;  Loss pred: 0.6015; Loss self: 0.0000; time: 0.41s
Val loss: 0.6085 score: 0.8140 time: 0.24s
Test loss: 0.6078 score: 0.7955 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.30s
Val loss: 0.6016 score: 0.8605 time: 0.23s
Test loss: 0.6010 score: 0.7955 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.25s
Val loss: 0.5942 score: 0.8605 time: 0.26s
Test loss: 0.5938 score: 0.7955 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.38s
Val loss: 0.5863 score: 0.8372 time: 0.24s
Test loss: 0.5862 score: 0.7955 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.26s
Val loss: 0.5783 score: 0.8372 time: 0.26s
Test loss: 0.5785 score: 0.8182 time: 0.20s
Epoch 62/1000, LR 0.000268
Train loss: 0.5483;  Loss pred: 0.5483; Loss self: 0.0000; time: 0.46s
Val loss: 0.5699 score: 0.8372 time: 0.25s
Test loss: 0.5705 score: 0.7955 time: 0.31s
Epoch 63/1000, LR 0.000268
Train loss: 0.5346;  Loss pred: 0.5346; Loss self: 0.0000; time: 0.27s
Val loss: 0.5613 score: 0.8372 time: 0.25s
Test loss: 0.5623 score: 0.8182 time: 0.21s
Epoch 64/1000, LR 0.000268
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 0.39s
Val loss: 0.5526 score: 0.8372 time: 0.23s
Test loss: 0.5540 score: 0.8182 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.5144;  Loss pred: 0.5144; Loss self: 0.0000; time: 0.25s
Val loss: 0.5435 score: 0.8605 time: 0.26s
Test loss: 0.5453 score: 0.8409 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.5047;  Loss pred: 0.5047; Loss self: 0.0000; time: 0.33s
Val loss: 0.5341 score: 0.8605 time: 0.24s
Test loss: 0.5362 score: 0.8409 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.5073;  Loss pred: 0.5073; Loss self: 0.0000; time: 0.31s
Val loss: 0.5243 score: 0.8372 time: 0.24s
Test loss: 0.5267 score: 0.8409 time: 0.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.4962;  Loss pred: 0.4962; Loss self: 0.0000; time: 0.26s
Val loss: 0.5141 score: 0.8372 time: 0.26s
Test loss: 0.5168 score: 0.8409 time: 0.21s
Epoch 69/1000, LR 0.000268
Train loss: 0.4666;  Loss pred: 0.4666; Loss self: 0.0000; time: 0.38s
Val loss: 0.5037 score: 0.8372 time: 0.23s
Test loss: 0.5067 score: 0.8409 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.4526;  Loss pred: 0.4526; Loss self: 0.0000; time: 0.38s
Val loss: 0.4928 score: 0.8372 time: 0.26s
Test loss: 0.4961 score: 0.8409 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.4445;  Loss pred: 0.4445; Loss self: 0.0000; time: 0.33s
Val loss: 0.4819 score: 0.8372 time: 0.24s
Test loss: 0.4854 score: 0.8409 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.4430;  Loss pred: 0.4430; Loss self: 0.0000; time: 0.29s
Val loss: 0.4707 score: 0.8605 time: 0.26s
Test loss: 0.4743 score: 0.8409 time: 0.20s
Epoch 73/1000, LR 0.000267
Train loss: 0.4134;  Loss pred: 0.4134; Loss self: 0.0000; time: 0.43s
Val loss: 0.4599 score: 0.8605 time: 0.25s
Test loss: 0.4635 score: 0.8409 time: 0.21s
Epoch 74/1000, LR 0.000267
Train loss: 0.4044;  Loss pred: 0.4044; Loss self: 0.0000; time: 0.29s
Val loss: 0.4493 score: 0.8605 time: 0.26s
Test loss: 0.4530 score: 0.8409 time: 0.20s
Epoch 75/1000, LR 0.000267
Train loss: 0.3793;  Loss pred: 0.3793; Loss self: 0.0000; time: 0.46s
Val loss: 0.4392 score: 0.8605 time: 0.24s
Test loss: 0.4430 score: 0.8409 time: 0.21s
Epoch 76/1000, LR 0.000267
Train loss: 0.3967;  Loss pred: 0.3967; Loss self: 0.0000; time: 0.30s
Val loss: 0.4294 score: 0.8372 time: 0.26s
Test loss: 0.4333 score: 0.8409 time: 0.20s
Epoch 77/1000, LR 0.000267
Train loss: 0.3711;  Loss pred: 0.3711; Loss self: 0.0000; time: 0.42s
Val loss: 0.4206 score: 0.8372 time: 0.24s
Test loss: 0.4244 score: 0.8636 time: 0.30s
Epoch 78/1000, LR 0.000267
Train loss: 0.3562;  Loss pred: 0.3562; Loss self: 0.0000; time: 0.30s
Val loss: 0.4122 score: 0.8372 time: 0.26s
Test loss: 0.4160 score: 0.8636 time: 0.20s
Epoch 79/1000, LR 0.000267
Train loss: 0.3278;  Loss pred: 0.3278; Loss self: 0.0000; time: 0.43s
Val loss: 0.4037 score: 0.8372 time: 0.24s
Test loss: 0.4073 score: 0.8636 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.3281;  Loss pred: 0.3281; Loss self: 0.0000; time: 0.27s
Val loss: 0.3962 score: 0.8372 time: 0.29s
Test loss: 0.3994 score: 0.8636 time: 0.21s
Epoch 81/1000, LR 0.000267
Train loss: 0.3194;  Loss pred: 0.3194; Loss self: 0.0000; time: 0.31s
Val loss: 0.3894 score: 0.8372 time: 0.25s
Test loss: 0.3925 score: 0.8636 time: 0.20s
Epoch 82/1000, LR 0.000267
Train loss: 0.3000;  Loss pred: 0.3000; Loss self: 0.0000; time: 0.30s
Val loss: 0.3816 score: 0.8372 time: 0.24s
Test loss: 0.3838 score: 0.8636 time: 0.22s
Epoch 83/1000, LR 0.000266
Train loss: 0.2995;  Loss pred: 0.2995; Loss self: 0.0000; time: 0.31s
Val loss: 0.3727 score: 0.8372 time: 0.27s
Test loss: 0.3735 score: 0.8636 time: 0.21s
Epoch 84/1000, LR 0.000266
Train loss: 0.2863;  Loss pred: 0.2863; Loss self: 0.0000; time: 0.34s
Val loss: 0.3652 score: 0.8372 time: 0.25s
Test loss: 0.3648 score: 0.8636 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.2790;  Loss pred: 0.2790; Loss self: 0.0000; time: 0.30s
Val loss: 0.3586 score: 0.8372 time: 0.26s
Test loss: 0.3572 score: 0.8636 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.2422;  Loss pred: 0.2422; Loss self: 0.0000; time: 0.35s
Val loss: 0.3528 score: 0.8372 time: 0.25s
Test loss: 0.3503 score: 0.8636 time: 0.21s
Epoch 87/1000, LR 0.000266
Train loss: 0.2631;  Loss pred: 0.2631; Loss self: 0.0000; time: 0.32s
Val loss: 0.3486 score: 0.8372 time: 0.24s
Test loss: 0.3457 score: 0.8636 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.2515;  Loss pred: 0.2515; Loss self: 0.0000; time: 0.26s
Val loss: 0.3446 score: 0.8372 time: 0.26s
Test loss: 0.3414 score: 0.8636 time: 0.21s
Epoch 89/1000, LR 0.000266
Train loss: 0.2243;  Loss pred: 0.2243; Loss self: 0.0000; time: 0.35s
Val loss: 0.3432 score: 0.8372 time: 0.33s
Test loss: 0.3413 score: 0.8636 time: 0.23s
Epoch 90/1000, LR 0.000266
Train loss: 0.2045;  Loss pred: 0.2045; Loss self: 0.0000; time: 0.27s
Val loss: 0.3419 score: 0.8372 time: 0.26s
Test loss: 0.3413 score: 0.8636 time: 0.19s
Epoch 91/1000, LR 0.000266
Train loss: 0.2029;  Loss pred: 0.2029; Loss self: 0.0000; time: 0.32s
Val loss: 0.3402 score: 0.8605 time: 0.25s
Test loss: 0.3406 score: 0.8409 time: 0.22s
Epoch 92/1000, LR 0.000266
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 0.29s
Val loss: 0.3386 score: 0.8605 time: 0.26s
Test loss: 0.3396 score: 0.8409 time: 0.19s
Epoch 93/1000, LR 0.000265
Train loss: 0.1817;  Loss pred: 0.1817; Loss self: 0.0000; time: 0.34s
Val loss: 0.3378 score: 0.8605 time: 0.25s
Test loss: 0.3401 score: 0.8409 time: 0.21s
Epoch 94/1000, LR 0.000265
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.35s
Val loss: 0.3348 score: 0.8605 time: 0.25s
Test loss: 0.3366 score: 0.8636 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.1567;  Loss pred: 0.1567; Loss self: 0.0000; time: 0.28s
Val loss: 0.3331 score: 0.8605 time: 0.28s
Test loss: 0.3352 score: 0.8636 time: 0.19s
Epoch 96/1000, LR 0.000265
Train loss: 0.1628;  Loss pred: 0.1628; Loss self: 0.0000; time: 0.34s
Val loss: 0.3296 score: 0.8605 time: 0.24s
Test loss: 0.3305 score: 0.8636 time: 0.22s
Epoch 97/1000, LR 0.000265
Train loss: 0.1686;  Loss pred: 0.1686; Loss self: 0.0000; time: 0.26s
Val loss: 0.3257 score: 0.8605 time: 0.28s
Test loss: 0.3249 score: 0.8636 time: 0.21s
Epoch 98/1000, LR 0.000265
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.42s
Val loss: 0.3251 score: 0.8605 time: 0.24s
Test loss: 0.3251 score: 0.8636 time: 0.23s
Epoch 99/1000, LR 0.000265
Train loss: 0.1394;  Loss pred: 0.1394; Loss self: 0.0000; time: 0.26s
Val loss: 0.3251 score: 0.8605 time: 0.29s
Test loss: 0.3265 score: 0.8636 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 100/1000, LR 0.000265
Train loss: 0.1398;  Loss pred: 0.1398; Loss self: 0.0000; time: 0.33s
Val loss: 0.3288 score: 0.8605 time: 0.23s
Test loss: 0.3345 score: 0.8636 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.1611,   Val_Loss: 0.3251,   Val_Precision: 0.8333,   Val_Recall: 0.9091,   Val_accuracy: 0.8696,   Val_Score: 0.8605,   Val_Loss: 0.3251,   Test_Precision: 0.8077,   Test_Recall: 0.9545,   Test_accuracy: 0.8750,   Test_Score: 0.8636,   Test_loss: 0.3251


[0.2011076268972829, 0.23689542105421424, 0.2113321019569412, 0.3064693030901253, 0.21006385399959981, 0.21783592400606722, 0.21178562194108963, 0.2135606340598315, 0.22292484703939408, 0.20670737605541945, 0.20663768204394728, 0.19641048600897193, 0.23756363801658154, 0.2112041680375114, 0.2184325800044462, 0.2288929510395974, 0.1958319810219109, 0.2291503589367494, 0.19624159496743232, 0.21610614599194378, 0.1956761369947344, 0.2194525330560282, 0.19704827398527414, 0.21141689096111804, 0.20458041795063764, 0.21314306289423257, 0.21051961707416922, 0.21683184592984617, 0.20912071107886732, 0.21579197607934475, 0.22812976001296192, 0.20156010997015983, 0.2286220050882548, 0.20719384599942714, 0.21873789699748158, 0.22426796704530716, 0.19821445597335696, 0.20860247197560966, 0.21541961596813053, 0.23225740191992372, 0.1987929840106517, 0.23266370093915612, 0.21251748397480696, 0.22750897402875125, 0.20267615199554712, 0.21402470802422613, 0.3010698070283979, 0.2249099399195984, 0.21460638591088355, 0.21197054395452142, 0.2254697900498286, 0.1987697359872982, 0.22067236399743706, 0.20327850303146988, 0.2199050539638847, 0.20640477701090276, 0.23270529101137072, 0.22100573894567788, 0.20932476304005831, 0.22953313600737602, 0.20228959200903773, 0.31331852497532964, 0.2185007199877873, 0.22878602799028158, 0.20476642006542534, 0.21789476298727095, 0.2130083420779556, 0.21052176202647388, 0.22046117391437292, 0.2019530120305717, 0.22213087195996195, 0.20366823894437402, 0.21801498509012163, 0.2079094290966168, 0.21445234306156635, 0.20883367699570954, 0.3030028200009838, 0.20468521607108414, 0.21406780905090272, 0.2099852479295805, 0.20240799302700907, 0.22770666307769716, 0.2135012219659984, 0.22357449203263968, 0.20985144400037825, 0.2173866170924157, 0.22928149497602135, 0.21581187203992158, 0.23267984203994274, 0.19860892998985946, 0.22451756207738072, 0.19715194997843355, 0.21731188602279872, 0.22705122106708586, 0.1974602109985426, 0.22775146295316517, 0.21344923705328256, 0.23436597699765116, 0.21188844798598439, 0.2185067250393331]
[0.004570627884029157, 0.005383986842141233, 0.004803002317203209, 0.006965211433866484, 0.004774178499990905, 0.004950816454683346, 0.004813309589570219, 0.004853650774087079, 0.0050664737963498655, 0.004697894910350442, 0.004696310955544256, 0.0044638746820220895, 0.005399173591285944, 0.004800094728125259, 0.004964376818282868, 0.005202112523627214, 0.004450726841407066, 0.005207962703107941, 0.004460036249259825, 0.004911503317998722, 0.004447184931698509, 0.004987557569455186, 0.004478369863301685, 0.00480492934002541, 0.0046495549534235825, 0.004844160520323468, 0.004784536751685664, 0.004927996498405595, 0.004752743433610621, 0.00490436309271238, 0.005184767273021862, 0.004580911590230905, 0.0051959546610967, 0.004708951045441526, 0.004971315840851854, 0.005096999251029708, 0.004504873999394476, 0.004740965272172947, 0.004895900362912057, 0.005278577316361902, 0.004518022363878448, 0.0052878113849808206, 0.004829942817609249, 0.005170658500653438, 0.00460627618171698, 0.0048641979096415025, 0.006842495614281771, 0.005111589543627237, 0.004877417861610989, 0.00481751236260276, 0.005124313410223377, 0.0045174939997113224, 0.005015280999941751, 0.004619965977987952, 0.0049978421355428345, 0.0046910176593386995, 0.005288756613894789, 0.0050228577033108604, 0.004757380978183143, 0.005216662181985818, 0.00459749072747813, 0.0071208755676211285, 0.004965925454267894, 0.005199682454324581, 0.0046537822742142125, 0.004952153704256158, 0.0048410986835899, 0.004784585500601679, 0.005010481225326657, 0.004589841182512993, 0.005048428908180954, 0.004628823612372137, 0.004954886024775492, 0.004725214297650382, 0.004873916887762872, 0.004746219931720671, 0.006886427727295086, 0.004651936728888276, 0.004865177478429608, 0.004772391998399557, 0.004600181659704752, 0.0051751514335840266, 0.004852300499227236, 0.005081238455287265, 0.004769351000008596, 0.004940604933918538, 0.005210943067636849, 0.004904815273634581, 0.005288178228180517, 0.004513839317951351, 0.0051026718653950165, 0.0044807261358734895, 0.004938906500518153, 0.005160255024251951, 0.004487732068148695, 0.0051761696125719354, 0.00485111902393824, 0.005326499477219345, 0.004815646545136009, 0.004966061932712116]
[218.78832085504794, 185.735966546734, 208.20310588196855, 143.57065962675108, 209.46011968381683, 201.98688623449684, 207.7572575358258, 206.03048026010677, 197.37593446559396, 212.86129619391687, 212.93308928350334, 224.02062585390732, 185.21353001391935, 208.32922195070165, 201.43515220625227, 192.22959816000716, 224.68240259019288, 192.01366388496464, 224.21342431150802, 203.6036494845467, 224.86134832672022, 200.49893882412562, 223.2955362161062, 208.11960577025087, 215.07434797897704, 206.43411707860278, 209.00665036122567, 202.92222210862778, 210.40479335117573, 203.90007450426052, 192.87268788386046, 218.2971621047142, 192.45741451272247, 212.36151965691903, 201.1539865929432, 196.19386834282497, 221.9817913074628, 210.92751003039214, 204.25252269741964, 189.44498490157937, 221.33577912207605, 189.11415842863448, 207.04178864274533, 193.398964536843, 217.09510254056283, 205.58374033627697, 146.1455083599591, 195.63386133902875, 205.0265177955666, 207.57601117181764, 195.14809496330327, 221.36166646018836, 199.39062238219836, 216.45181041690518, 200.08635184540222, 213.17336079714775, 189.0803591476999, 199.08985264321566, 210.1996885651783, 191.69345553047324, 217.50995472883352, 140.4321688398875, 201.37233416191626, 192.31943657795853, 214.87898253015055, 201.93234291991868, 206.56467991238168, 209.00452084600565, 199.58162799718008, 217.87246229998922, 198.0814265561916, 216.0376120894201, 201.82098942332595, 211.63061334535686, 205.17378999849956, 210.69398687503823, 145.21316996276516, 214.96423065904014, 205.5423475163299, 209.53852917684767, 217.38271963463762, 193.231060546465, 206.08781343184683, 196.80241515913377, 209.6721335876092, 202.40436411637364, 191.9038429359578, 203.8812767068752, 189.10103949807043, 221.54089447159572, 195.9757606170481, 223.17811213540196, 202.47396865988202, 193.78887192594973, 222.82970213338194, 193.19305101038216, 206.13800549222128, 187.74056099636414, 207.65643629099796, 201.36680000160004]
Elapsed: 0.21818067352636716~0.02080977689848161
Time per graph: 0.004958651671053799~0.0004729494749654911
Speed: 203.1504619836873~15.764253040968686
Total Time: 0.2190
best val loss: 0.3250862956047058 test_score: 0.8636

Testing...
Test loss: 0.6010 score: 0.7955 time: 0.22s
test Score 0.7955
Epoch Time List: [0.8288375979755074, 0.7767714678775519, 0.7101254131412134, 0.8929597741225734, 0.7675763189326972, 0.7989621850429103, 0.7746967491693795, 0.7292964061489329, 0.7980269439285621, 0.7453091139905155, 0.798474422073923, 0.9106688020983711, 0.8663298418978229, 0.7576943909516558, 0.8337169830920175, 0.7662799059180543, 0.7646357209887356, 0.749424074892886, 0.8417263978626579, 0.7697103059617803, 0.7880576000316069, 0.8812994807958603, 0.8778750221244991, 0.794453342910856, 0.7609028399456292, 0.8092601780081168, 1.0134981150040403, 0.7863568800967187, 0.7520369460107759, 0.8050765248481184, 0.8113003739854321, 0.7587537730578333, 0.7771480099763721, 0.7500251319725066, 0.827197881997563, 0.7624455509940162, 0.7374750660965219, 0.9567980048013851, 0.7806110649835318, 0.8087350600399077, 0.7568070989800617, 0.780428632046096, 0.7360683330334723, 0.7719830758869648, 0.8163467440754175, 0.779165661893785, 0.9609336709836498, 0.7872963808476925, 0.8194198539713398, 0.7635402719024569, 0.752347041037865, 0.7958442750386894, 0.7431185789173469, 0.7181737071368843, 0.9444231300149113, 0.7085592611692846, 0.8800387618830428, 0.7525984739186242, 0.7150911740027368, 0.8424844379769638, 0.7127162358956411, 1.0232520611025393, 0.7379057370126247, 0.8506699157878757, 0.7055792768951505, 0.7834799099946395, 0.7541348120430484, 0.7201697251293808, 0.8251723828725517, 0.8349872718099505, 0.792536714929156, 0.7410563880112022, 0.8901696819812059, 0.7531716158846393, 0.9083219139138237, 0.7669723719591275, 0.9550577659392729, 0.7561483710305765, 0.8870183930266649, 0.7660783018218353, 0.7622902882285416, 0.7638048628577963, 0.795261521008797, 0.8078329870477319, 0.7628426080336794, 0.8117461740039289, 0.780416791094467, 0.731162806157954, 0.9096659950446337, 0.7282650149427354, 0.7894020690582693, 0.7454393949592486, 0.80224965501111, 0.823893466964364, 0.7506523538613692, 0.7952002000529319, 0.7482619270449504, 0.891939349938184, 0.7513602840481326, 0.7727092419518158]
Total Epoch List: [100]
Total Time List: [0.2189725360367447]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcfd30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.20s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.19s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.21s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.21s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.20s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.20s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.20s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.20s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.19s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.22s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.31s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.20s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.20s
Epoch 18/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.20s
Epoch 21/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.18s
Epoch 24/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.20s
Epoch 25/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.22s
Epoch 26/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.21s
Epoch 27/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.20s
Epoch 28/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.21s
Epoch 29/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.36s
Val loss: 0.6890 score: 0.5455 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.21s
Epoch 30/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.35s
Val loss: 0.6883 score: 0.5455 time: 0.31s
Test loss: 0.6907 score: 0.5116 time: 0.21s
Epoch 31/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.37s
Val loss: 0.6875 score: 0.5909 time: 0.20s
Test loss: 0.6902 score: 0.5116 time: 0.21s
Epoch 32/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.34s
Val loss: 0.6865 score: 0.6136 time: 0.22s
Test loss: 0.6895 score: 0.5349 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.39s
Val loss: 0.6855 score: 0.6364 time: 0.22s
Test loss: 0.6888 score: 0.5581 time: 0.20s
Epoch 34/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.34s
Val loss: 0.6843 score: 0.6364 time: 0.22s
Test loss: 0.6880 score: 0.5581 time: 0.20s
Epoch 35/1000, LR 0.000270
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.29s
Val loss: 0.6830 score: 0.7045 time: 0.24s
Test loss: 0.6871 score: 0.5581 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.39s
Val loss: 0.6815 score: 0.7273 time: 0.21s
Test loss: 0.6861 score: 0.5814 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6748;  Loss pred: 0.6748; Loss self: 0.0000; time: 0.39s
Val loss: 0.6799 score: 0.7727 time: 0.23s
Test loss: 0.6849 score: 0.5814 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.37s
Val loss: 0.6780 score: 0.7955 time: 0.20s
Test loss: 0.6836 score: 0.6279 time: 0.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6698;  Loss pred: 0.6698; Loss self: 0.0000; time: 0.35s
Val loss: 0.6759 score: 0.7955 time: 0.31s
Test loss: 0.6821 score: 0.6512 time: 0.19s
Epoch 40/1000, LR 0.000269
Train loss: 0.6668;  Loss pred: 0.6668; Loss self: 0.0000; time: 0.35s
Val loss: 0.6735 score: 0.7955 time: 0.20s
Test loss: 0.6804 score: 0.6744 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6635;  Loss pred: 0.6635; Loss self: 0.0000; time: 0.31s
Val loss: 0.6708 score: 0.8182 time: 0.23s
Test loss: 0.6787 score: 0.6744 time: 0.19s
Epoch 42/1000, LR 0.000269
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.35s
Val loss: 0.6677 score: 0.8182 time: 0.21s
Test loss: 0.6767 score: 0.6977 time: 0.19s
Epoch 43/1000, LR 0.000269
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.36s
Val loss: 0.6644 score: 0.8182 time: 0.20s
Test loss: 0.6745 score: 0.6977 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6499;  Loss pred: 0.6499; Loss self: 0.0000; time: 0.31s
Val loss: 0.6608 score: 0.8182 time: 0.23s
Test loss: 0.6722 score: 0.7209 time: 0.19s
Epoch 45/1000, LR 0.000269
Train loss: 0.6460;  Loss pred: 0.6460; Loss self: 0.0000; time: 0.43s
Val loss: 0.6569 score: 0.8182 time: 0.19s
Test loss: 0.6697 score: 0.7209 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6414;  Loss pred: 0.6414; Loss self: 0.0000; time: 0.33s
Val loss: 0.6528 score: 0.8182 time: 0.31s
Test loss: 0.6670 score: 0.7209 time: 0.20s
Epoch 47/1000, LR 0.000269
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 0.34s
Val loss: 0.6483 score: 0.8182 time: 0.22s
Test loss: 0.6641 score: 0.7209 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.35s
Val loss: 0.6436 score: 0.8182 time: 0.23s
Test loss: 0.6611 score: 0.7442 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6268;  Loss pred: 0.6268; Loss self: 0.0000; time: 0.34s
Val loss: 0.6385 score: 0.8182 time: 0.23s
Test loss: 0.6578 score: 0.7674 time: 0.20s
Epoch 50/1000, LR 0.000269
Train loss: 0.6170;  Loss pred: 0.6170; Loss self: 0.0000; time: 0.35s
Val loss: 0.6332 score: 0.8409 time: 0.21s
Test loss: 0.6543 score: 0.7674 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6086;  Loss pred: 0.6086; Loss self: 0.0000; time: 0.31s
Val loss: 0.6276 score: 0.8409 time: 0.23s
Test loss: 0.6505 score: 0.7674 time: 0.20s
Epoch 52/1000, LR 0.000269
Train loss: 0.6035;  Loss pred: 0.6035; Loss self: 0.0000; time: 0.40s
Val loss: 0.6217 score: 0.8409 time: 0.20s
Test loss: 0.6464 score: 0.7674 time: 0.21s
Epoch 53/1000, LR 0.000269
Train loss: 0.5939;  Loss pred: 0.5939; Loss self: 0.0000; time: 0.34s
Val loss: 0.6154 score: 0.8409 time: 0.22s
Test loss: 0.6421 score: 0.7907 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.5901;  Loss pred: 0.5901; Loss self: 0.0000; time: 0.46s
Val loss: 0.6089 score: 0.8636 time: 0.22s
Test loss: 0.6374 score: 0.7907 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.35s
Val loss: 0.6020 score: 0.8636 time: 0.23s
Test loss: 0.6325 score: 0.7907 time: 0.20s
Epoch 56/1000, LR 0.000269
Train loss: 0.5658;  Loss pred: 0.5658; Loss self: 0.0000; time: 0.47s
Val loss: 0.5947 score: 0.8864 time: 0.21s
Test loss: 0.6272 score: 0.7907 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.5556;  Loss pred: 0.5556; Loss self: 0.0000; time: 0.44s
Val loss: 0.5871 score: 0.8864 time: 0.23s
Test loss: 0.6215 score: 0.7907 time: 0.19s
Epoch 58/1000, LR 0.000269
Train loss: 0.5446;  Loss pred: 0.5446; Loss self: 0.0000; time: 0.39s
Val loss: 0.5791 score: 0.8864 time: 0.20s
Test loss: 0.6156 score: 0.8140 time: 0.21s
Epoch 59/1000, LR 0.000268
Train loss: 0.5373;  Loss pred: 0.5373; Loss self: 0.0000; time: 0.34s
Val loss: 0.5707 score: 0.8864 time: 0.23s
Test loss: 0.6094 score: 0.8140 time: 0.20s
Epoch 60/1000, LR 0.000268
Train loss: 0.5214;  Loss pred: 0.5214; Loss self: 0.0000; time: 0.37s
Val loss: 0.5619 score: 0.8636 time: 0.21s
Test loss: 0.6030 score: 0.8140 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.5115;  Loss pred: 0.5115; Loss self: 0.0000; time: 0.35s
Val loss: 0.5528 score: 0.8636 time: 0.22s
Test loss: 0.5963 score: 0.8140 time: 0.20s
Epoch 62/1000, LR 0.000268
Train loss: 0.5034;  Loss pred: 0.5034; Loss self: 0.0000; time: 0.29s
Val loss: 0.5433 score: 0.8636 time: 0.23s
Test loss: 0.5896 score: 0.8140 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.4839;  Loss pred: 0.4839; Loss self: 0.0000; time: 0.41s
Val loss: 0.5337 score: 0.8636 time: 0.20s
Test loss: 0.5827 score: 0.8372 time: 0.21s
Epoch 64/1000, LR 0.000268
Train loss: 0.4727;  Loss pred: 0.4727; Loss self: 0.0000; time: 0.33s
Val loss: 0.5237 score: 0.8636 time: 0.23s
Test loss: 0.5756 score: 0.8372 time: 0.19s
Epoch 65/1000, LR 0.000268
Train loss: 0.4599;  Loss pred: 0.4599; Loss self: 0.0000; time: 0.51s
Val loss: 0.5137 score: 0.8864 time: 0.20s
Test loss: 0.5686 score: 0.8372 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.4481;  Loss pred: 0.4481; Loss self: 0.0000; time: 0.31s
Val loss: 0.5035 score: 0.8864 time: 0.23s
Test loss: 0.5618 score: 0.8372 time: 0.20s
Epoch 67/1000, LR 0.000268
Train loss: 0.4256;  Loss pred: 0.4256; Loss self: 0.0000; time: 0.44s
Val loss: 0.4930 score: 0.8864 time: 0.24s
Test loss: 0.5543 score: 0.8372 time: 0.27s
Epoch 68/1000, LR 0.000268
Train loss: 0.4207;  Loss pred: 0.4207; Loss self: 0.0000; time: 0.35s
Val loss: 0.4822 score: 0.8636 time: 0.23s
Test loss: 0.5468 score: 0.8372 time: 0.20s
Epoch 69/1000, LR 0.000268
Train loss: 0.3985;  Loss pred: 0.3985; Loss self: 0.0000; time: 0.38s
Val loss: 0.4714 score: 0.8636 time: 0.23s
Test loss: 0.5389 score: 0.8372 time: 0.20s
Epoch 70/1000, LR 0.000268
Train loss: 0.3908;  Loss pred: 0.3908; Loss self: 0.0000; time: 0.34s
Val loss: 0.4605 score: 0.8636 time: 0.21s
Test loss: 0.5310 score: 0.8372 time: 0.21s
Epoch 71/1000, LR 0.000268
Train loss: 0.3635;  Loss pred: 0.3635; Loss self: 0.0000; time: 0.31s
Val loss: 0.4498 score: 0.8636 time: 0.25s
Test loss: 0.5233 score: 0.8372 time: 0.21s
Epoch 72/1000, LR 0.000267
Train loss: 0.3453;  Loss pred: 0.3453; Loss self: 0.0000; time: 0.43s
Val loss: 0.4392 score: 0.8636 time: 0.21s
Test loss: 0.5156 score: 0.8372 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.3344;  Loss pred: 0.3344; Loss self: 0.0000; time: 0.31s
Val loss: 0.4291 score: 0.8636 time: 0.33s
Test loss: 0.5091 score: 0.8372 time: 0.19s
Epoch 74/1000, LR 0.000267
Train loss: 0.3066;  Loss pred: 0.3066; Loss self: 0.0000; time: 0.44s
Val loss: 0.4193 score: 0.8636 time: 0.26s
Test loss: 0.5030 score: 0.8372 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.3083;  Loss pred: 0.3083; Loss self: 0.0000; time: 0.44s
Val loss: 0.4101 score: 0.8636 time: 0.22s
Test loss: 0.4987 score: 0.8372 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.2893;  Loss pred: 0.2893; Loss self: 0.0000; time: 0.36s
Val loss: 0.4015 score: 0.8864 time: 0.23s
Test loss: 0.4950 score: 0.8372 time: 0.22s
Epoch 77/1000, LR 0.000267
Train loss: 0.2752;  Loss pred: 0.2752; Loss self: 0.0000; time: 0.35s
Val loss: 0.3934 score: 0.8864 time: 0.22s
Test loss: 0.4909 score: 0.8372 time: 0.19s
Epoch 78/1000, LR 0.000267
Train loss: 0.2639;  Loss pred: 0.2639; Loss self: 0.0000; time: 0.38s
Val loss: 0.3858 score: 0.8864 time: 0.20s
Test loss: 0.4871 score: 0.8372 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.2446;  Loss pred: 0.2446; Loss self: 0.0000; time: 0.31s
Val loss: 0.3788 score: 0.8864 time: 0.23s
Test loss: 0.4834 score: 0.8372 time: 0.18s
Epoch 80/1000, LR 0.000267
Train loss: 0.2232;  Loss pred: 0.2232; Loss self: 0.0000; time: 0.37s
Val loss: 0.3723 score: 0.8864 time: 0.21s
Test loss: 0.4797 score: 0.8372 time: 0.21s
Epoch 81/1000, LR 0.000267
Train loss: 0.2152;  Loss pred: 0.2152; Loss self: 0.0000; time: 0.39s
Val loss: 0.3662 score: 0.8636 time: 0.22s
Test loss: 0.4750 score: 0.8372 time: 0.20s
Epoch 82/1000, LR 0.000267
Train loss: 0.2047;  Loss pred: 0.2047; Loss self: 0.0000; time: 0.44s
Val loss: 0.3610 score: 0.8636 time: 0.20s
Test loss: 0.4710 score: 0.8372 time: 0.21s
Epoch 83/1000, LR 0.000266
Train loss: 0.2015;  Loss pred: 0.2015; Loss self: 0.0000; time: 0.36s
Val loss: 0.3565 score: 0.8636 time: 0.22s
Test loss: 0.4669 score: 0.8372 time: 0.21s
Epoch 84/1000, LR 0.000266
Train loss: 0.1714;  Loss pred: 0.1714; Loss self: 0.0000; time: 0.33s
Val loss: 0.3531 score: 0.8636 time: 0.23s
Test loss: 0.4630 score: 0.8372 time: 0.19s
Epoch 85/1000, LR 0.000266
Train loss: 0.1747;  Loss pred: 0.1747; Loss self: 0.0000; time: 0.35s
Val loss: 0.3507 score: 0.8636 time: 0.21s
Test loss: 0.4610 score: 0.8372 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.31s
Val loss: 0.3493 score: 0.8636 time: 0.24s
Test loss: 0.4599 score: 0.8372 time: 0.18s
Epoch 87/1000, LR 0.000266
Train loss: 0.1471;  Loss pred: 0.1471; Loss self: 0.0000; time: 0.38s
Val loss: 0.3485 score: 0.8636 time: 0.21s
Test loss: 0.4612 score: 0.8372 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.1504;  Loss pred: 0.1504; Loss self: 0.0000; time: 0.37s
Val loss: 0.3483 score: 0.8636 time: 0.22s
Test loss: 0.4645 score: 0.8372 time: 0.22s
Epoch 89/1000, LR 0.000266
Train loss: 0.1386;  Loss pred: 0.1386; Loss self: 0.0000; time: 0.34s
Val loss: 0.3488 score: 0.8636 time: 0.23s
Test loss: 0.4701 score: 0.8372 time: 0.19s
     INFO: Early stopping counter 1 of 2
Epoch 90/1000, LR 0.000266
Train loss: 0.1254;  Loss pred: 0.1254; Loss self: 0.0000; time: 0.38s
Val loss: 0.3502 score: 0.8636 time: 0.21s
Test loss: 0.4767 score: 0.8372 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.1504,   Val_Loss: 0.3483,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3483,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.4645


[0.2011076268972829, 0.23689542105421424, 0.2113321019569412, 0.3064693030901253, 0.21006385399959981, 0.21783592400606722, 0.21178562194108963, 0.2135606340598315, 0.22292484703939408, 0.20670737605541945, 0.20663768204394728, 0.19641048600897193, 0.23756363801658154, 0.2112041680375114, 0.2184325800044462, 0.2288929510395974, 0.1958319810219109, 0.2291503589367494, 0.19624159496743232, 0.21610614599194378, 0.1956761369947344, 0.2194525330560282, 0.19704827398527414, 0.21141689096111804, 0.20458041795063764, 0.21314306289423257, 0.21051961707416922, 0.21683184592984617, 0.20912071107886732, 0.21579197607934475, 0.22812976001296192, 0.20156010997015983, 0.2286220050882548, 0.20719384599942714, 0.21873789699748158, 0.22426796704530716, 0.19821445597335696, 0.20860247197560966, 0.21541961596813053, 0.23225740191992372, 0.1987929840106517, 0.23266370093915612, 0.21251748397480696, 0.22750897402875125, 0.20267615199554712, 0.21402470802422613, 0.3010698070283979, 0.2249099399195984, 0.21460638591088355, 0.21197054395452142, 0.2254697900498286, 0.1987697359872982, 0.22067236399743706, 0.20327850303146988, 0.2199050539638847, 0.20640477701090276, 0.23270529101137072, 0.22100573894567788, 0.20932476304005831, 0.22953313600737602, 0.20228959200903773, 0.31331852497532964, 0.2185007199877873, 0.22878602799028158, 0.20476642006542534, 0.21789476298727095, 0.2130083420779556, 0.21052176202647388, 0.22046117391437292, 0.2019530120305717, 0.22213087195996195, 0.20366823894437402, 0.21801498509012163, 0.2079094290966168, 0.21445234306156635, 0.20883367699570954, 0.3030028200009838, 0.20468521607108414, 0.21406780905090272, 0.2099852479295805, 0.20240799302700907, 0.22770666307769716, 0.2135012219659984, 0.22357449203263968, 0.20985144400037825, 0.2173866170924157, 0.22928149497602135, 0.21581187203992158, 0.23267984203994274, 0.19860892998985946, 0.22451756207738072, 0.19715194997843355, 0.21731188602279872, 0.22705122106708586, 0.1974602109985426, 0.22775146295316517, 0.21344923705328256, 0.23436597699765116, 0.21188844798598439, 0.2185067250393331, 0.20457382802851498, 0.1936922479653731, 0.21827859501354396, 0.21131505793891847, 0.21668751107063144, 0.20211333606857806, 0.2217849149601534, 0.20632644393481314, 0.20274599792901427, 0.20826697500888258, 0.19511750992387533, 0.22593748196959496, 0.30966832500416785, 0.20409110793843865, 0.2159384050173685, 0.21394770697224885, 0.20833040704019368, 0.2111195350298658, 0.2129300240194425, 0.20040991599671543, 0.20552046492230147, 0.22759693406987935, 0.18811557500157505, 0.20882234594319016, 0.2242502620210871, 0.21383145404979587, 0.20192049897741526, 0.21073935995809734, 0.21589517209213227, 0.21317000698763877, 0.21677349205128849, 0.1994120340095833, 0.20892774499952793, 0.2024290809640661, 0.2398945220047608, 0.2208559449063614, 0.24074343696702272, 0.22234934696462005, 0.194203200051561, 0.21955978800542653, 0.19838539802003652, 0.1957719309721142, 0.2193022919818759, 0.19521951803471893, 0.2422885779524222, 0.20697926299180835, 0.2783319059526548, 0.22324805706739426, 0.20096694491803646, 0.22216196299996227, 0.20127579802647233, 0.2139561049407348, 0.19986402499489486, 0.2261106560472399, 0.20355061802547425, 0.24304522504098713, 0.18929353496059775, 0.21424249198753387, 0.2056368920020759, 0.20776153996121138, 0.20439515600446612, 0.21105722291395068, 0.2171179309953004, 0.1934209930477664, 0.22018075792584568, 0.19944629992824048, 0.27482831198722124, 0.2021051050396636, 0.20019592391327024, 0.21862931898795068, 0.21074238198343664, 0.2410800060024485, 0.19903108105063438, 0.22449426003731787, 0.21980950108263642, 0.22803961299359798, 0.19803824892733246, 0.22685989504680037, 0.1838919169967994, 0.21852901997044683, 0.20624888001475483, 0.2127931520808488, 0.21487268607597798, 0.19490090396720916, 0.22812403505668044, 0.18768422410357744, 0.21238001598976552, 0.22749424097128212, 0.1937046549282968, 0.24132389889564365]
[0.004570627884029157, 0.005383986842141233, 0.004803002317203209, 0.006965211433866484, 0.004774178499990905, 0.004950816454683346, 0.004813309589570219, 0.004853650774087079, 0.0050664737963498655, 0.004697894910350442, 0.004696310955544256, 0.0044638746820220895, 0.005399173591285944, 0.004800094728125259, 0.004964376818282868, 0.005202112523627214, 0.004450726841407066, 0.005207962703107941, 0.004460036249259825, 0.004911503317998722, 0.004447184931698509, 0.004987557569455186, 0.004478369863301685, 0.00480492934002541, 0.0046495549534235825, 0.004844160520323468, 0.004784536751685664, 0.004927996498405595, 0.004752743433610621, 0.00490436309271238, 0.005184767273021862, 0.004580911590230905, 0.0051959546610967, 0.004708951045441526, 0.004971315840851854, 0.005096999251029708, 0.004504873999394476, 0.004740965272172947, 0.004895900362912057, 0.005278577316361902, 0.004518022363878448, 0.0052878113849808206, 0.004829942817609249, 0.005170658500653438, 0.00460627618171698, 0.0048641979096415025, 0.006842495614281771, 0.005111589543627237, 0.004877417861610989, 0.00481751236260276, 0.005124313410223377, 0.0045174939997113224, 0.005015280999941751, 0.004619965977987952, 0.0049978421355428345, 0.0046910176593386995, 0.005288756613894789, 0.0050228577033108604, 0.004757380978183143, 0.005216662181985818, 0.00459749072747813, 0.0071208755676211285, 0.004965925454267894, 0.005199682454324581, 0.0046537822742142125, 0.004952153704256158, 0.0048410986835899, 0.004784585500601679, 0.005010481225326657, 0.004589841182512993, 0.005048428908180954, 0.004628823612372137, 0.004954886024775492, 0.004725214297650382, 0.004873916887762872, 0.004746219931720671, 0.006886427727295086, 0.004651936728888276, 0.004865177478429608, 0.004772391998399557, 0.004600181659704752, 0.0051751514335840266, 0.004852300499227236, 0.005081238455287265, 0.004769351000008596, 0.004940604933918538, 0.005210943067636849, 0.004904815273634581, 0.005288178228180517, 0.004513839317951351, 0.0051026718653950165, 0.0044807261358734895, 0.004938906500518153, 0.005160255024251951, 0.004487732068148695, 0.0051761696125719354, 0.00485111902393824, 0.005326499477219345, 0.004815646545136009, 0.004966061932712116, 0.00475753088438407, 0.004504470882915654, 0.005076246395663813, 0.004914303672998104, 0.005039244443503057, 0.004700310141129723, 0.005157788720003567, 0.004798289393832864, 0.004715023207651495, 0.004843418023462386, 0.004537616509857566, 0.005254360045804534, 0.007201588953585299, 0.004746304835777643, 0.005021823372496942, 0.0049755280691220665, 0.0048448931869812485, 0.0049097566286015306, 0.004951861023707965, 0.004660695720853847, 0.004779545695867476, 0.005292951955113473, 0.0043747808139901175, 0.004856333626585818, 0.005215122372583422, 0.004972824512785951, 0.004695825557614309, 0.004900915347862729, 0.005020817955630983, 0.004957442022968344, 0.005041244001192755, 0.004637489163013565, 0.004858784767430882, 0.004707653045675955, 0.00557894237220374, 0.0051361847652642185, 0.0055986845806284356, 0.005170915045688839, 0.004516353489571186, 0.0051060415815215475, 0.004613613907442709, 0.004552835604002656, 0.0051000533019040905, 0.0045399887915050915, 0.0056346180919167954, 0.004813471232367636, 0.006472835022154762, 0.005191815280637076, 0.004673649881814801, 0.00516655727906889, 0.004680832512243543, 0.004975723370714763, 0.004648000581276624, 0.005258387349935812, 0.004733735302918006, 0.0056522145358369105, 0.0044021752316418085, 0.004982383534593811, 0.004782253302373859, 0.0048316637200281716, 0.004753375721034096, 0.00490830750962676, 0.005049254209193033, 0.004498162629017823, 0.005120482742461527, 0.0046382860448428015, 0.006391356092726075, 0.004700118721852642, 0.004655719160773727, 0.005084402767161644, 0.004900985627521782, 0.005606511767498802, 0.004628629791875219, 0.005220796745053904, 0.005111848862386893, 0.005303246813804605, 0.004605540672728662, 0.005275811512716288, 0.004276556209227894, 0.005082070231870856, 0.004796485581738484, 0.004948677955368577, 0.004997039211069255, 0.00453257916202812, 0.00530521011759722, 0.004364749397757614, 0.004939070139296873, 0.005290563743518189, 0.004504759416937135, 0.005612183695247527]
[218.78832085504794, 185.735966546734, 208.20310588196855, 143.57065962675108, 209.46011968381683, 201.98688623449684, 207.7572575358258, 206.03048026010677, 197.37593446559396, 212.86129619391687, 212.93308928350334, 224.02062585390732, 185.21353001391935, 208.32922195070165, 201.43515220625227, 192.22959816000716, 224.68240259019288, 192.01366388496464, 224.21342431150802, 203.6036494845467, 224.86134832672022, 200.49893882412562, 223.2955362161062, 208.11960577025087, 215.07434797897704, 206.43411707860278, 209.00665036122567, 202.92222210862778, 210.40479335117573, 203.90007450426052, 192.87268788386046, 218.2971621047142, 192.45741451272247, 212.36151965691903, 201.1539865929432, 196.19386834282497, 221.9817913074628, 210.92751003039214, 204.25252269741964, 189.44498490157937, 221.33577912207605, 189.11415842863448, 207.04178864274533, 193.398964536843, 217.09510254056283, 205.58374033627697, 146.1455083599591, 195.63386133902875, 205.0265177955666, 207.57601117181764, 195.14809496330327, 221.36166646018836, 199.39062238219836, 216.45181041690518, 200.08635184540222, 213.17336079714775, 189.0803591476999, 199.08985264321566, 210.1996885651783, 191.69345553047324, 217.50995472883352, 140.4321688398875, 201.37233416191626, 192.31943657795853, 214.87898253015055, 201.93234291991868, 206.56467991238168, 209.00452084600565, 199.58162799718008, 217.87246229998922, 198.0814265561916, 216.0376120894201, 201.82098942332595, 211.63061334535686, 205.17378999849956, 210.69398687503823, 145.21316996276516, 214.96423065904014, 205.5423475163299, 209.53852917684767, 217.38271963463762, 193.231060546465, 206.08781343184683, 196.80241515913377, 209.6721335876092, 202.40436411637364, 191.9038429359578, 203.8812767068752, 189.10103949807043, 221.54089447159572, 195.9757606170481, 223.17811213540196, 202.47396865988202, 193.78887192594973, 222.82970213338194, 193.19305101038216, 206.13800549222128, 187.74056099636414, 207.65643629099796, 201.36680000160004, 210.19306533192676, 222.00165701875292, 196.99595371379357, 203.4876284700418, 198.44244731752778, 212.751918485032, 193.88153611676213, 208.40760486128204, 212.0880334962529, 206.4657634661763, 220.38001621062278, 190.31813413671057, 138.85824454090118, 210.6902178852906, 199.13085861934286, 200.9836918026774, 206.40289917786177, 203.67608328578902, 201.94427816376754, 214.560241623497, 209.22490622165762, 188.9304887859239, 228.58288049588649, 205.91665995217818, 191.75005465972006, 201.09295983174863, 212.95509974353587, 204.04351616399504, 199.17073449724919, 201.71693291961785, 198.3637371576144, 215.63392707750785, 205.81277991631583, 212.42007223079318, 179.24544354183564, 194.697045706563, 178.6133841974275, 193.38936941803613, 221.41756669603532, 195.8464270285105, 216.74982347066234, 219.6433359291171, 196.0763821089189, 220.26486097743896, 177.47431745810087, 207.75028076944025, 154.49181024655667, 192.61085881262176, 213.96553556375838, 193.55248494994305, 213.63721034331473, 200.97580301300994, 215.14627257756044, 190.17237290672526, 211.24966564640238, 176.92180536666984, 227.16042578501495, 200.70715011334931, 209.1064476872463, 206.96804619386248, 210.37680559836963, 203.73621620867894, 198.0490501308745, 222.31299365411132, 195.29408657264182, 215.59688003974568, 156.46131830114862, 212.7605831211495, 214.78958791702797, 196.6799338672864, 204.04059019974252, 178.3640240973085, 216.04665850687215, 191.54164562092623, 195.62393703734554, 188.5637299393557, 217.129772823725, 189.54429997919755, 233.83300746573002, 196.77020473443383, 208.48598061198598, 202.07417193417268, 200.11850172895126, 220.62493874956309, 188.49394799331898, 229.10822795777207, 202.46726039455683, 189.01577383415227, 221.98743760658311, 178.18376131323242]
Elapsed: 0.21618508272743048~0.020158314334870482
Time per graph: 0.004966866652522204~0.0004603412850944291
Speed: 202.7801614536556~15.827436954136713
Total Time: 0.2419
best val loss: 0.3482835590839386 test_score: 0.8372

Testing...
Test loss: 0.6272 score: 0.7907 time: 0.29s
test Score 0.7907
Epoch Time List: [0.8288375979755074, 0.7767714678775519, 0.7101254131412134, 0.8929597741225734, 0.7675763189326972, 0.7989621850429103, 0.7746967491693795, 0.7292964061489329, 0.7980269439285621, 0.7453091139905155, 0.798474422073923, 0.9106688020983711, 0.8663298418978229, 0.7576943909516558, 0.8337169830920175, 0.7662799059180543, 0.7646357209887356, 0.749424074892886, 0.8417263978626579, 0.7697103059617803, 0.7880576000316069, 0.8812994807958603, 0.8778750221244991, 0.794453342910856, 0.7609028399456292, 0.8092601780081168, 1.0134981150040403, 0.7863568800967187, 0.7520369460107759, 0.8050765248481184, 0.8113003739854321, 0.7587537730578333, 0.7771480099763721, 0.7500251319725066, 0.827197881997563, 0.7624455509940162, 0.7374750660965219, 0.9567980048013851, 0.7806110649835318, 0.8087350600399077, 0.7568070989800617, 0.780428632046096, 0.7360683330334723, 0.7719830758869648, 0.8163467440754175, 0.779165661893785, 0.9609336709836498, 0.7872963808476925, 0.8194198539713398, 0.7635402719024569, 0.752347041037865, 0.7958442750386894, 0.7431185789173469, 0.7181737071368843, 0.9444231300149113, 0.7085592611692846, 0.8800387618830428, 0.7525984739186242, 0.7150911740027368, 0.8424844379769638, 0.7127162358956411, 1.0232520611025393, 0.7379057370126247, 0.8506699157878757, 0.7055792768951505, 0.7834799099946395, 0.7541348120430484, 0.7201697251293808, 0.8251723828725517, 0.8349872718099505, 0.792536714929156, 0.7410563880112022, 0.8901696819812059, 0.7531716158846393, 0.9083219139138237, 0.7669723719591275, 0.9550577659392729, 0.7561483710305765, 0.8870183930266649, 0.7660783018218353, 0.7622902882285416, 0.7638048628577963, 0.795261521008797, 0.8078329870477319, 0.7628426080336794, 0.8117461740039289, 0.780416791094467, 0.731162806157954, 0.9096659950446337, 0.7282650149427354, 0.7894020690582693, 0.7454393949592486, 0.80224965501111, 0.823893466964364, 0.7506523538613692, 0.7952002000529319, 0.7482619270449504, 0.891939349938184, 0.7513602840481326, 0.7727092419518158, 0.7653914800612256, 0.7714706299593672, 0.7703751740045846, 0.7437120141694322, 0.8629870729055256, 0.7430311820935458, 0.7910965699702501, 0.7589450930245221, 0.7812768889125437, 0.8699143650010228, 0.7559314710088074, 0.8921703151427209, 0.9701829169644043, 0.7462968030013144, 0.7881655149394646, 0.8665676399832591, 0.7741124138701707, 0.8412639390444383, 0.8420979449292645, 0.7832069450523704, 0.7290196429239586, 0.8582535969326273, 0.6934342029271647, 0.8066415290813893, 0.8664934658445418, 0.8078432769980282, 0.7610952230170369, 0.7412386479554698, 0.775164423044771, 0.8675993940560147, 0.7828786630416289, 0.7617737740511075, 0.8103859649272636, 0.7527382109547034, 0.7674838550155982, 0.8127952690701932, 0.8490572432056069, 0.7952313290443271, 0.852021805010736, 0.7704581540310755, 0.7323036520974711, 0.7554902451811358, 0.77079235506244, 0.7282493830425665, 0.8568743820069358, 0.8486624448560178, 0.8260700929677114, 0.7907107640057802, 0.7613534970441833, 0.7763606109656394, 0.7423578680027276, 0.8097205499652773, 0.7599891290301457, 0.902088759932667, 0.77794535306748, 0.921578396926634, 0.8587428039172664, 0.7993054079124704, 0.7654559100046754, 0.7892235850449651, 0.7659157909220085, 0.7299341548932716, 0.8201473809313029, 0.7424499579938129, 0.9287238249089569, 0.734098753076978, 0.9437972389860079, 0.7714588498929515, 0.8071830389089882, 0.7663509520934895, 0.7678071379195899, 0.8689086239319295, 0.8347984061110765, 0.9202995840460062, 0.8770026960410178, 0.8132442900678143, 0.7686552581144497, 0.8087690530810505, 0.7263327500550076, 0.7967777439625934, 0.8102915990166366, 0.8483389400644228, 0.7934215150307864, 0.7457740940153599, 0.7828265810385346, 0.7280885090585798, 0.7953047739574686, 0.8073068100493401, 0.7621018211357296, 0.8212220979621634]
Total Epoch List: [100, 90]
Total Time List: [0.2189725360367447, 0.2419223259203136]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcc4c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.6930,   Val_Loss: 0.6932,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6932,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6934


[0.2011076268972829, 0.23689542105421424, 0.2113321019569412, 0.3064693030901253, 0.21006385399959981, 0.21783592400606722, 0.21178562194108963, 0.2135606340598315, 0.22292484703939408, 0.20670737605541945, 0.20663768204394728, 0.19641048600897193, 0.23756363801658154, 0.2112041680375114, 0.2184325800044462, 0.2288929510395974, 0.1958319810219109, 0.2291503589367494, 0.19624159496743232, 0.21610614599194378, 0.1956761369947344, 0.2194525330560282, 0.19704827398527414, 0.21141689096111804, 0.20458041795063764, 0.21314306289423257, 0.21051961707416922, 0.21683184592984617, 0.20912071107886732, 0.21579197607934475, 0.22812976001296192, 0.20156010997015983, 0.2286220050882548, 0.20719384599942714, 0.21873789699748158, 0.22426796704530716, 0.19821445597335696, 0.20860247197560966, 0.21541961596813053, 0.23225740191992372, 0.1987929840106517, 0.23266370093915612, 0.21251748397480696, 0.22750897402875125, 0.20267615199554712, 0.21402470802422613, 0.3010698070283979, 0.2249099399195984, 0.21460638591088355, 0.21197054395452142, 0.2254697900498286, 0.1987697359872982, 0.22067236399743706, 0.20327850303146988, 0.2199050539638847, 0.20640477701090276, 0.23270529101137072, 0.22100573894567788, 0.20932476304005831, 0.22953313600737602, 0.20228959200903773, 0.31331852497532964, 0.2185007199877873, 0.22878602799028158, 0.20476642006542534, 0.21789476298727095, 0.2130083420779556, 0.21052176202647388, 0.22046117391437292, 0.2019530120305717, 0.22213087195996195, 0.20366823894437402, 0.21801498509012163, 0.2079094290966168, 0.21445234306156635, 0.20883367699570954, 0.3030028200009838, 0.20468521607108414, 0.21406780905090272, 0.2099852479295805, 0.20240799302700907, 0.22770666307769716, 0.2135012219659984, 0.22357449203263968, 0.20985144400037825, 0.2173866170924157, 0.22928149497602135, 0.21581187203992158, 0.23267984203994274, 0.19860892998985946, 0.22451756207738072, 0.19715194997843355, 0.21731188602279872, 0.22705122106708586, 0.1974602109985426, 0.22775146295316517, 0.21344923705328256, 0.23436597699765116, 0.21188844798598439, 0.2185067250393331, 0.20457382802851498, 0.1936922479653731, 0.21827859501354396, 0.21131505793891847, 0.21668751107063144, 0.20211333606857806, 0.2217849149601534, 0.20632644393481314, 0.20274599792901427, 0.20826697500888258, 0.19511750992387533, 0.22593748196959496, 0.30966832500416785, 0.20409110793843865, 0.2159384050173685, 0.21394770697224885, 0.20833040704019368, 0.2111195350298658, 0.2129300240194425, 0.20040991599671543, 0.20552046492230147, 0.22759693406987935, 0.18811557500157505, 0.20882234594319016, 0.2242502620210871, 0.21383145404979587, 0.20192049897741526, 0.21073935995809734, 0.21589517209213227, 0.21317000698763877, 0.21677349205128849, 0.1994120340095833, 0.20892774499952793, 0.2024290809640661, 0.2398945220047608, 0.2208559449063614, 0.24074343696702272, 0.22234934696462005, 0.194203200051561, 0.21955978800542653, 0.19838539802003652, 0.1957719309721142, 0.2193022919818759, 0.19521951803471893, 0.2422885779524222, 0.20697926299180835, 0.2783319059526548, 0.22324805706739426, 0.20096694491803646, 0.22216196299996227, 0.20127579802647233, 0.2139561049407348, 0.19986402499489486, 0.2261106560472399, 0.20355061802547425, 0.24304522504098713, 0.18929353496059775, 0.21424249198753387, 0.2056368920020759, 0.20776153996121138, 0.20439515600446612, 0.21105722291395068, 0.2171179309953004, 0.1934209930477664, 0.22018075792584568, 0.19944629992824048, 0.27482831198722124, 0.2021051050396636, 0.20019592391327024, 0.21862931898795068, 0.21074238198343664, 0.2410800060024485, 0.19903108105063438, 0.22449426003731787, 0.21980950108263642, 0.22803961299359798, 0.19803824892733246, 0.22685989504680037, 0.1838919169967994, 0.21852901997044683, 0.20624888001475483, 0.2127931520808488, 0.21487268607597798, 0.19490090396720916, 0.22812403505668044, 0.18768422410357744, 0.21238001598976552, 0.22749424097128212, 0.1937046549282968, 0.24132389889564365, 0.22837575990706682, 0.22745811194181442, 0.24883491802029312]
[0.004570627884029157, 0.005383986842141233, 0.004803002317203209, 0.006965211433866484, 0.004774178499990905, 0.004950816454683346, 0.004813309589570219, 0.004853650774087079, 0.0050664737963498655, 0.004697894910350442, 0.004696310955544256, 0.0044638746820220895, 0.005399173591285944, 0.004800094728125259, 0.004964376818282868, 0.005202112523627214, 0.004450726841407066, 0.005207962703107941, 0.004460036249259825, 0.004911503317998722, 0.004447184931698509, 0.004987557569455186, 0.004478369863301685, 0.00480492934002541, 0.0046495549534235825, 0.004844160520323468, 0.004784536751685664, 0.004927996498405595, 0.004752743433610621, 0.00490436309271238, 0.005184767273021862, 0.004580911590230905, 0.0051959546610967, 0.004708951045441526, 0.004971315840851854, 0.005096999251029708, 0.004504873999394476, 0.004740965272172947, 0.004895900362912057, 0.005278577316361902, 0.004518022363878448, 0.0052878113849808206, 0.004829942817609249, 0.005170658500653438, 0.00460627618171698, 0.0048641979096415025, 0.006842495614281771, 0.005111589543627237, 0.004877417861610989, 0.00481751236260276, 0.005124313410223377, 0.0045174939997113224, 0.005015280999941751, 0.004619965977987952, 0.0049978421355428345, 0.0046910176593386995, 0.005288756613894789, 0.0050228577033108604, 0.004757380978183143, 0.005216662181985818, 0.00459749072747813, 0.0071208755676211285, 0.004965925454267894, 0.005199682454324581, 0.0046537822742142125, 0.004952153704256158, 0.0048410986835899, 0.004784585500601679, 0.005010481225326657, 0.004589841182512993, 0.005048428908180954, 0.004628823612372137, 0.004954886024775492, 0.004725214297650382, 0.004873916887762872, 0.004746219931720671, 0.006886427727295086, 0.004651936728888276, 0.004865177478429608, 0.004772391998399557, 0.004600181659704752, 0.0051751514335840266, 0.004852300499227236, 0.005081238455287265, 0.004769351000008596, 0.004940604933918538, 0.005210943067636849, 0.004904815273634581, 0.005288178228180517, 0.004513839317951351, 0.0051026718653950165, 0.0044807261358734895, 0.004938906500518153, 0.005160255024251951, 0.004487732068148695, 0.0051761696125719354, 0.00485111902393824, 0.005326499477219345, 0.004815646545136009, 0.004966061932712116, 0.00475753088438407, 0.004504470882915654, 0.005076246395663813, 0.004914303672998104, 0.005039244443503057, 0.004700310141129723, 0.005157788720003567, 0.004798289393832864, 0.004715023207651495, 0.004843418023462386, 0.004537616509857566, 0.005254360045804534, 0.007201588953585299, 0.004746304835777643, 0.005021823372496942, 0.0049755280691220665, 0.0048448931869812485, 0.0049097566286015306, 0.004951861023707965, 0.004660695720853847, 0.004779545695867476, 0.005292951955113473, 0.0043747808139901175, 0.004856333626585818, 0.005215122372583422, 0.004972824512785951, 0.004695825557614309, 0.004900915347862729, 0.005020817955630983, 0.004957442022968344, 0.005041244001192755, 0.004637489163013565, 0.004858784767430882, 0.004707653045675955, 0.00557894237220374, 0.0051361847652642185, 0.0055986845806284356, 0.005170915045688839, 0.004516353489571186, 0.0051060415815215475, 0.004613613907442709, 0.004552835604002656, 0.0051000533019040905, 0.0045399887915050915, 0.0056346180919167954, 0.004813471232367636, 0.006472835022154762, 0.005191815280637076, 0.004673649881814801, 0.00516655727906889, 0.004680832512243543, 0.004975723370714763, 0.004648000581276624, 0.005258387349935812, 0.004733735302918006, 0.0056522145358369105, 0.0044021752316418085, 0.004982383534593811, 0.004782253302373859, 0.0048316637200281716, 0.004753375721034096, 0.00490830750962676, 0.005049254209193033, 0.004498162629017823, 0.005120482742461527, 0.0046382860448428015, 0.006391356092726075, 0.004700118721852642, 0.004655719160773727, 0.005084402767161644, 0.004900985627521782, 0.005606511767498802, 0.004628629791875219, 0.005220796745053904, 0.005111848862386893, 0.005303246813804605, 0.004605540672728662, 0.005275811512716288, 0.004276556209227894, 0.005082070231870856, 0.004796485581738484, 0.004948677955368577, 0.004997039211069255, 0.00453257916202812, 0.00530521011759722, 0.004364749397757614, 0.004939070139296873, 0.005290563743518189, 0.004504759416937135, 0.005612183695247527, 0.005311064183885275, 0.005289723533530568, 0.005786858558611468]
[218.78832085504794, 185.735966546734, 208.20310588196855, 143.57065962675108, 209.46011968381683, 201.98688623449684, 207.7572575358258, 206.03048026010677, 197.37593446559396, 212.86129619391687, 212.93308928350334, 224.02062585390732, 185.21353001391935, 208.32922195070165, 201.43515220625227, 192.22959816000716, 224.68240259019288, 192.01366388496464, 224.21342431150802, 203.6036494845467, 224.86134832672022, 200.49893882412562, 223.2955362161062, 208.11960577025087, 215.07434797897704, 206.43411707860278, 209.00665036122567, 202.92222210862778, 210.40479335117573, 203.90007450426052, 192.87268788386046, 218.2971621047142, 192.45741451272247, 212.36151965691903, 201.1539865929432, 196.19386834282497, 221.9817913074628, 210.92751003039214, 204.25252269741964, 189.44498490157937, 221.33577912207605, 189.11415842863448, 207.04178864274533, 193.398964536843, 217.09510254056283, 205.58374033627697, 146.1455083599591, 195.63386133902875, 205.0265177955666, 207.57601117181764, 195.14809496330327, 221.36166646018836, 199.39062238219836, 216.45181041690518, 200.08635184540222, 213.17336079714775, 189.0803591476999, 199.08985264321566, 210.1996885651783, 191.69345553047324, 217.50995472883352, 140.4321688398875, 201.37233416191626, 192.31943657795853, 214.87898253015055, 201.93234291991868, 206.56467991238168, 209.00452084600565, 199.58162799718008, 217.87246229998922, 198.0814265561916, 216.0376120894201, 201.82098942332595, 211.63061334535686, 205.17378999849956, 210.69398687503823, 145.21316996276516, 214.96423065904014, 205.5423475163299, 209.53852917684767, 217.38271963463762, 193.231060546465, 206.08781343184683, 196.80241515913377, 209.6721335876092, 202.40436411637364, 191.9038429359578, 203.8812767068752, 189.10103949807043, 221.54089447159572, 195.9757606170481, 223.17811213540196, 202.47396865988202, 193.78887192594973, 222.82970213338194, 193.19305101038216, 206.13800549222128, 187.74056099636414, 207.65643629099796, 201.36680000160004, 210.19306533192676, 222.00165701875292, 196.99595371379357, 203.4876284700418, 198.44244731752778, 212.751918485032, 193.88153611676213, 208.40760486128204, 212.0880334962529, 206.4657634661763, 220.38001621062278, 190.31813413671057, 138.85824454090118, 210.6902178852906, 199.13085861934286, 200.9836918026774, 206.40289917786177, 203.67608328578902, 201.94427816376754, 214.560241623497, 209.22490622165762, 188.9304887859239, 228.58288049588649, 205.91665995217818, 191.75005465972006, 201.09295983174863, 212.95509974353587, 204.04351616399504, 199.17073449724919, 201.71693291961785, 198.3637371576144, 215.63392707750785, 205.81277991631583, 212.42007223079318, 179.24544354183564, 194.697045706563, 178.6133841974275, 193.38936941803613, 221.41756669603532, 195.8464270285105, 216.74982347066234, 219.6433359291171, 196.0763821089189, 220.26486097743896, 177.47431745810087, 207.75028076944025, 154.49181024655667, 192.61085881262176, 213.96553556375838, 193.55248494994305, 213.63721034331473, 200.97580301300994, 215.14627257756044, 190.17237290672526, 211.24966564640238, 176.92180536666984, 227.16042578501495, 200.70715011334931, 209.1064476872463, 206.96804619386248, 210.37680559836963, 203.73621620867894, 198.0490501308745, 222.31299365411132, 195.29408657264182, 215.59688003974568, 156.46131830114862, 212.7605831211495, 214.78958791702797, 196.6799338672864, 204.04059019974252, 178.3640240973085, 216.04665850687215, 191.54164562092623, 195.62393703734554, 188.5637299393557, 217.129772823725, 189.54429997919755, 233.83300746573002, 196.77020473443383, 208.48598061198598, 202.07417193417268, 200.11850172895126, 220.62493874956309, 188.49394799331898, 229.10822795777207, 202.46726039455683, 189.01577383415227, 221.98743760658311, 178.18376131323242, 188.2861824630514, 189.04579675311706, 172.8053295015985]
Elapsed: 0.21647582646674077~0.020171973308022322
Time per graph: 0.004974571555726664~0.00046173426429574067
Speed: 202.47859059540068~15.913646646644546
Total Time: 0.2494
best val loss: 0.6931712627410889 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
test Score 0.4884
Epoch Time List: [0.8288375979755074, 0.7767714678775519, 0.7101254131412134, 0.8929597741225734, 0.7675763189326972, 0.7989621850429103, 0.7746967491693795, 0.7292964061489329, 0.7980269439285621, 0.7453091139905155, 0.798474422073923, 0.9106688020983711, 0.8663298418978229, 0.7576943909516558, 0.8337169830920175, 0.7662799059180543, 0.7646357209887356, 0.749424074892886, 0.8417263978626579, 0.7697103059617803, 0.7880576000316069, 0.8812994807958603, 0.8778750221244991, 0.794453342910856, 0.7609028399456292, 0.8092601780081168, 1.0134981150040403, 0.7863568800967187, 0.7520369460107759, 0.8050765248481184, 0.8113003739854321, 0.7587537730578333, 0.7771480099763721, 0.7500251319725066, 0.827197881997563, 0.7624455509940162, 0.7374750660965219, 0.9567980048013851, 0.7806110649835318, 0.8087350600399077, 0.7568070989800617, 0.780428632046096, 0.7360683330334723, 0.7719830758869648, 0.8163467440754175, 0.779165661893785, 0.9609336709836498, 0.7872963808476925, 0.8194198539713398, 0.7635402719024569, 0.752347041037865, 0.7958442750386894, 0.7431185789173469, 0.7181737071368843, 0.9444231300149113, 0.7085592611692846, 0.8800387618830428, 0.7525984739186242, 0.7150911740027368, 0.8424844379769638, 0.7127162358956411, 1.0232520611025393, 0.7379057370126247, 0.8506699157878757, 0.7055792768951505, 0.7834799099946395, 0.7541348120430484, 0.7201697251293808, 0.8251723828725517, 0.8349872718099505, 0.792536714929156, 0.7410563880112022, 0.8901696819812059, 0.7531716158846393, 0.9083219139138237, 0.7669723719591275, 0.9550577659392729, 0.7561483710305765, 0.8870183930266649, 0.7660783018218353, 0.7622902882285416, 0.7638048628577963, 0.795261521008797, 0.8078329870477319, 0.7628426080336794, 0.8117461740039289, 0.780416791094467, 0.731162806157954, 0.9096659950446337, 0.7282650149427354, 0.7894020690582693, 0.7454393949592486, 0.80224965501111, 0.823893466964364, 0.7506523538613692, 0.7952002000529319, 0.7482619270449504, 0.891939349938184, 0.7513602840481326, 0.7727092419518158, 0.7653914800612256, 0.7714706299593672, 0.7703751740045846, 0.7437120141694322, 0.8629870729055256, 0.7430311820935458, 0.7910965699702501, 0.7589450930245221, 0.7812768889125437, 0.8699143650010228, 0.7559314710088074, 0.8921703151427209, 0.9701829169644043, 0.7462968030013144, 0.7881655149394646, 0.8665676399832591, 0.7741124138701707, 0.8412639390444383, 0.8420979449292645, 0.7832069450523704, 0.7290196429239586, 0.8582535969326273, 0.6934342029271647, 0.8066415290813893, 0.8664934658445418, 0.8078432769980282, 0.7610952230170369, 0.7412386479554698, 0.775164423044771, 0.8675993940560147, 0.7828786630416289, 0.7617737740511075, 0.8103859649272636, 0.7527382109547034, 0.7674838550155982, 0.8127952690701932, 0.8490572432056069, 0.7952313290443271, 0.852021805010736, 0.7704581540310755, 0.7323036520974711, 0.7554902451811358, 0.77079235506244, 0.7282493830425665, 0.8568743820069358, 0.8486624448560178, 0.8260700929677114, 0.7907107640057802, 0.7613534970441833, 0.7763606109656394, 0.7423578680027276, 0.8097205499652773, 0.7599891290301457, 0.902088759932667, 0.77794535306748, 0.921578396926634, 0.8587428039172664, 0.7993054079124704, 0.7654559100046754, 0.7892235850449651, 0.7659157909220085, 0.7299341548932716, 0.8201473809313029, 0.7424499579938129, 0.9287238249089569, 0.734098753076978, 0.9437972389860079, 0.7714588498929515, 0.8071830389089882, 0.7663509520934895, 0.7678071379195899, 0.8689086239319295, 0.8347984061110765, 0.9202995840460062, 0.8770026960410178, 0.8132442900678143, 0.7686552581144497, 0.8087690530810505, 0.7263327500550076, 0.7967777439625934, 0.8102915990166366, 0.8483389400644228, 0.7934215150307864, 0.7457740940153599, 0.7828265810385346, 0.7280885090585798, 0.7953047739574686, 0.8073068100493401, 0.7621018211357296, 0.8212220979621634, 0.7423004419542849, 0.7713446411071345, 0.7927002020878717]
Total Epoch List: [100, 90, 3]
Total Time List: [0.2189725360367447, 0.2419223259203136, 0.24941265094093978]
========================training times:6========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bceaa0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7012;  Loss pred: 0.7012; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6987 score: 0.5000 time: 0.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.7011;  Loss pred: 0.7011; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6986 score: 0.5000 time: 0.19s
Epoch 3/1000, LR 0.000030
Train loss: 0.7011;  Loss pred: 0.7011; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5000 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.7009;  Loss pred: 0.7009; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5000 time: 0.28s
Epoch 5/1000, LR 0.000090
Train loss: 0.7007;  Loss pred: 0.7007; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5000 time: 0.22s
Epoch 6/1000, LR 0.000120
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5000 time: 0.20s
Epoch 7/1000, LR 0.000150
Train loss: 0.7001;  Loss pred: 0.7001; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.5000 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.5000 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6992;  Loss pred: 0.6992; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5000 time: 0.20s
Epoch 10/1000, LR 0.000240
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.21s
Epoch 11/1000, LR 0.000270
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5000 time: 0.19s
Epoch 12/1000, LR 0.000270
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5116 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5000 time: 0.22s
Epoch 13/1000, LR 0.000270
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.20s
Epoch 14/1000, LR 0.000270
Train loss: 0.6966;  Loss pred: 0.6966; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.22s
Epoch 15/1000, LR 0.000270
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.24s
Epoch 19/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.26s
Epoch 23/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.30s
Epoch 24/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.31s
Val loss: 0.6913 score: 0.5349 time: 0.25s
Test loss: 0.6924 score: 0.4773 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.29s
Val loss: 0.6911 score: 0.6279 time: 0.25s
Test loss: 0.6922 score: 0.5000 time: 0.20s
Epoch 26/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.48s
Val loss: 0.6910 score: 0.6047 time: 0.25s
Test loss: 0.6920 score: 0.5682 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.30s
Val loss: 0.6908 score: 0.6047 time: 0.26s
Test loss: 0.6918 score: 0.5909 time: 0.20s
Epoch 28/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.41s
Val loss: 0.6907 score: 0.5349 time: 0.22s
Test loss: 0.6916 score: 0.5455 time: 0.20s
Epoch 29/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.30s
Val loss: 0.6905 score: 0.5349 time: 0.33s
Test loss: 0.6914 score: 0.5455 time: 0.21s
Epoch 30/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.36s
Val loss: 0.6903 score: 0.5116 time: 0.25s
Test loss: 0.6912 score: 0.5455 time: 0.21s
Epoch 31/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.32s
Val loss: 0.6901 score: 0.5116 time: 0.24s
Test loss: 0.6910 score: 0.4773 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4884 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5000 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.19s
Epoch 35/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.5000 time: 0.21s
Epoch 36/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5000 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5000 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6876 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.5000 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.19s
Epoch 40/1000, LR 0.000269
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5000 time: 0.20s
Epoch 41/1000, LR 0.000269
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.5000 time: 0.21s
Epoch 42/1000, LR 0.000269
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.5000 time: 0.20s
Epoch 43/1000, LR 0.000269
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6836 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6851 score: 0.5000 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.5000 time: 0.21s
Epoch 45/1000, LR 0.000269
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6812 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6828 score: 0.5000 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6798 score: 0.4884 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6815 score: 0.5000 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6801 score: 0.5000 time: 0.34s
Epoch 48/1000, LR 0.000269
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6764 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6784 score: 0.5000 time: 0.28s
Epoch 49/1000, LR 0.000269
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6744 score: 0.4884 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6766 score: 0.5000 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6722 score: 0.4884 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6746 score: 0.5000 time: 0.21s
Epoch 51/1000, LR 0.000269
Train loss: 0.6665;  Loss pred: 0.6665; Loss self: 0.0000; time: 0.32s
Val loss: 0.6698 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6724 score: 0.5000 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6642;  Loss pred: 0.6642; Loss self: 0.0000; time: 0.32s
Val loss: 0.6671 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6699 score: 0.5000 time: 0.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.26s
Val loss: 0.6642 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6673 score: 0.5000 time: 0.21s
Epoch 54/1000, LR 0.000269
Train loss: 0.6568;  Loss pred: 0.6568; Loss self: 0.0000; time: 0.32s
Val loss: 0.6610 score: 0.5349 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6643 score: 0.5000 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 0.38s
Val loss: 0.6575 score: 0.5581 time: 0.27s
Test loss: 0.6611 score: 0.5682 time: 0.19s
Epoch 56/1000, LR 0.000269
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.31s
Val loss: 0.6537 score: 0.5581 time: 0.24s
Test loss: 0.6576 score: 0.5909 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.6470;  Loss pred: 0.6470; Loss self: 0.0000; time: 0.31s
Val loss: 0.6496 score: 0.5581 time: 0.26s
Test loss: 0.6538 score: 0.5909 time: 0.19s
Epoch 58/1000, LR 0.000269
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 0.33s
Val loss: 0.6452 score: 0.6279 time: 0.24s
Test loss: 0.6496 score: 0.6364 time: 0.20s
Epoch 59/1000, LR 0.000268
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.43s
Val loss: 0.6404 score: 0.6279 time: 0.25s
Test loss: 0.6452 score: 0.6364 time: 0.20s
Epoch 60/1000, LR 0.000268
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.30s
Val loss: 0.6352 score: 0.6744 time: 0.26s
Test loss: 0.6404 score: 0.7045 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.6274;  Loss pred: 0.6274; Loss self: 0.0000; time: 0.45s
Val loss: 0.6298 score: 0.6744 time: 0.25s
Test loss: 0.6353 score: 0.7045 time: 0.20s
Epoch 62/1000, LR 0.000268
Train loss: 0.6153;  Loss pred: 0.6153; Loss self: 0.0000; time: 0.33s
Val loss: 0.6240 score: 0.6744 time: 0.32s
Test loss: 0.6299 score: 0.7045 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.6107;  Loss pred: 0.6107; Loss self: 0.0000; time: 0.29s
Val loss: 0.6178 score: 0.6977 time: 0.26s
Test loss: 0.6241 score: 0.7045 time: 0.21s
Epoch 64/1000, LR 0.000268
Train loss: 0.6048;  Loss pred: 0.6048; Loss self: 0.0000; time: 0.29s
Val loss: 0.6112 score: 0.6977 time: 0.26s
Test loss: 0.6180 score: 0.7500 time: 0.20s
Epoch 65/1000, LR 0.000268
Train loss: 0.5962;  Loss pred: 0.5962; Loss self: 0.0000; time: 0.32s
Val loss: 0.6042 score: 0.7209 time: 0.23s
Test loss: 0.6115 score: 0.7500 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5879;  Loss pred: 0.5879; Loss self: 0.0000; time: 0.29s
Val loss: 0.5968 score: 0.7442 time: 0.26s
Test loss: 0.6046 score: 0.7500 time: 0.19s
Epoch 67/1000, LR 0.000268
Train loss: 0.5840;  Loss pred: 0.5840; Loss self: 0.0000; time: 0.35s
Val loss: 0.5891 score: 0.7674 time: 0.24s
Test loss: 0.5975 score: 0.7500 time: 0.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.5769;  Loss pred: 0.5769; Loss self: 0.0000; time: 0.31s
Val loss: 0.5810 score: 0.7674 time: 0.24s
Test loss: 0.5900 score: 0.7955 time: 0.23s
Epoch 69/1000, LR 0.000268
Train loss: 0.5618;  Loss pred: 0.5618; Loss self: 0.0000; time: 0.27s
Val loss: 0.5726 score: 0.7674 time: 0.27s
Test loss: 0.5824 score: 0.7955 time: 0.21s
Epoch 70/1000, LR 0.000268
Train loss: 0.5461;  Loss pred: 0.5461; Loss self: 0.0000; time: 0.32s
Val loss: 0.5639 score: 0.8605 time: 0.25s
Test loss: 0.5745 score: 0.7955 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.30s
Val loss: 0.5551 score: 0.8605 time: 0.26s
Test loss: 0.5664 score: 0.7955 time: 0.20s
Epoch 72/1000, LR 0.000267
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.34s
Val loss: 0.5460 score: 0.8605 time: 0.25s
Test loss: 0.5581 score: 0.7955 time: 0.21s
Epoch 73/1000, LR 0.000267
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.30s
Val loss: 0.5368 score: 0.8605 time: 0.34s
Test loss: 0.5495 score: 0.7955 time: 0.21s
Epoch 74/1000, LR 0.000267
Train loss: 0.5016;  Loss pred: 0.5016; Loss self: 0.0000; time: 0.32s
Val loss: 0.5277 score: 0.8605 time: 0.25s
Test loss: 0.5409 score: 0.7955 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.4923;  Loss pred: 0.4923; Loss self: 0.0000; time: 0.28s
Val loss: 0.5186 score: 0.8605 time: 0.26s
Test loss: 0.5321 score: 0.7727 time: 0.19s
Epoch 76/1000, LR 0.000267
Train loss: 0.4872;  Loss pred: 0.4872; Loss self: 0.0000; time: 0.32s
Val loss: 0.5096 score: 0.8605 time: 0.25s
Test loss: 0.5234 score: 0.7727 time: 0.21s
Epoch 77/1000, LR 0.000267
Train loss: 0.4645;  Loss pred: 0.4645; Loss self: 0.0000; time: 0.32s
Val loss: 0.5007 score: 0.8605 time: 0.24s
Test loss: 0.5146 score: 0.7727 time: 0.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.4655;  Loss pred: 0.4655; Loss self: 0.0000; time: 0.28s
Val loss: 0.4919 score: 0.8605 time: 0.26s
Test loss: 0.5057 score: 0.7500 time: 0.19s
Epoch 79/1000, LR 0.000267
Train loss: 0.4575;  Loss pred: 0.4575; Loss self: 0.0000; time: 0.42s
Val loss: 0.4831 score: 0.8605 time: 0.24s
Test loss: 0.4970 score: 0.7727 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.4477;  Loss pred: 0.4477; Loss self: 0.0000; time: 0.27s
Val loss: 0.4747 score: 0.8605 time: 0.32s
Test loss: 0.4882 score: 0.7955 time: 0.20s
Epoch 81/1000, LR 0.000267
Train loss: 0.4411;  Loss pred: 0.4411; Loss self: 0.0000; time: 0.37s
Val loss: 0.4665 score: 0.8372 time: 0.23s
Test loss: 0.4794 score: 0.7955 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.4110;  Loss pred: 0.4110; Loss self: 0.0000; time: 0.35s
Val loss: 0.4585 score: 0.8372 time: 0.28s
Test loss: 0.4706 score: 0.8182 time: 0.20s
Epoch 83/1000, LR 0.000266
Train loss: 0.4112;  Loss pred: 0.4112; Loss self: 0.0000; time: 0.32s
Val loss: 0.4508 score: 0.8372 time: 0.24s
Test loss: 0.4618 score: 0.8409 time: 0.22s
Epoch 84/1000, LR 0.000266
Train loss: 0.4070;  Loss pred: 0.4070; Loss self: 0.0000; time: 0.28s
Val loss: 0.4433 score: 0.8372 time: 0.26s
Test loss: 0.4530 score: 0.8409 time: 0.19s
Epoch 85/1000, LR 0.000266
Train loss: 0.3971;  Loss pred: 0.3971; Loss self: 0.0000; time: 0.39s
Val loss: 0.4360 score: 0.8372 time: 0.24s
Test loss: 0.4442 score: 0.8409 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.3830;  Loss pred: 0.3830; Loss self: 0.0000; time: 0.30s
Val loss: 0.4289 score: 0.8372 time: 0.26s
Test loss: 0.4355 score: 0.8409 time: 0.21s
Epoch 87/1000, LR 0.000266
Train loss: 0.3675;  Loss pred: 0.3675; Loss self: 0.0000; time: 0.34s
Val loss: 0.4221 score: 0.8372 time: 0.25s
Test loss: 0.4269 score: 0.8636 time: 0.20s
Epoch 88/1000, LR 0.000266
Train loss: 0.3425;  Loss pred: 0.3425; Loss self: 0.0000; time: 0.32s
Val loss: 0.4156 score: 0.8372 time: 0.24s
Test loss: 0.4184 score: 0.8864 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.3441;  Loss pred: 0.3441; Loss self: 0.0000; time: 0.26s
Val loss: 0.4094 score: 0.8372 time: 0.26s
Test loss: 0.4104 score: 0.8864 time: 0.21s
Epoch 90/1000, LR 0.000266
Train loss: 0.3352;  Loss pred: 0.3352; Loss self: 0.0000; time: 0.41s
Val loss: 0.4041 score: 0.8605 time: 0.33s
Test loss: 0.4037 score: 0.8864 time: 0.21s
Epoch 91/1000, LR 0.000266
Train loss: 0.3398;  Loss pred: 0.3398; Loss self: 0.0000; time: 0.31s
Val loss: 0.3992 score: 0.8605 time: 0.26s
Test loss: 0.3979 score: 0.8636 time: 0.20s
Epoch 92/1000, LR 0.000266
Train loss: 0.3075;  Loss pred: 0.3075; Loss self: 0.0000; time: 0.33s
Val loss: 0.3948 score: 0.8372 time: 0.24s
Test loss: 0.3933 score: 0.8636 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 0.2964;  Loss pred: 0.2964; Loss self: 0.0000; time: 0.28s
Val loss: 0.3914 score: 0.8372 time: 0.26s
Test loss: 0.3911 score: 0.8636 time: 0.19s
Epoch 94/1000, LR 0.000265
Train loss: 0.2738;  Loss pred: 0.2738; Loss self: 0.0000; time: 0.49s
Val loss: 0.3881 score: 0.8372 time: 0.25s
Test loss: 0.3896 score: 0.8409 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.2722;  Loss pred: 0.2722; Loss self: 0.0000; time: 0.27s
Val loss: 0.3847 score: 0.8372 time: 0.26s
Test loss: 0.3872 score: 0.8409 time: 0.20s
Epoch 96/1000, LR 0.000265
Train loss: 0.2833;  Loss pred: 0.2833; Loss self: 0.0000; time: 0.44s
Val loss: 0.3791 score: 0.8372 time: 0.24s
Test loss: 0.3796 score: 0.8409 time: 0.21s
Epoch 97/1000, LR 0.000265
Train loss: 0.2419;  Loss pred: 0.2419; Loss self: 0.0000; time: 0.29s
Val loss: 0.3730 score: 0.8372 time: 0.26s
Test loss: 0.3698 score: 0.8409 time: 0.31s
Epoch 98/1000, LR 0.000265
Train loss: 0.2452;  Loss pred: 0.2452; Loss self: 0.0000; time: 0.40s
Val loss: 0.3668 score: 0.8372 time: 0.23s
Test loss: 0.3586 score: 0.8409 time: 0.22s
Epoch 99/1000, LR 0.000265
Train loss: 0.2424;  Loss pred: 0.2424; Loss self: 0.0000; time: 0.25s
Val loss: 0.3609 score: 0.8372 time: 0.26s
Test loss: 0.3472 score: 0.8636 time: 0.21s
Epoch 100/1000, LR 0.000265
Train loss: 0.2276;  Loss pred: 0.2276; Loss self: 0.0000; time: 0.34s
Val loss: 0.3565 score: 0.8372 time: 0.23s
Test loss: 0.3404 score: 0.8864 time: 0.24s
Epoch 101/1000, LR 0.000265
Train loss: 0.2034;  Loss pred: 0.2034; Loss self: 0.0000; time: 0.30s
Val loss: 0.3527 score: 0.8372 time: 0.25s
Test loss: 0.3357 score: 0.8864 time: 0.22s
Epoch 102/1000, LR 0.000264
Train loss: 0.2120;  Loss pred: 0.2120; Loss self: 0.0000; time: 0.28s
Val loss: 0.3500 score: 0.8372 time: 0.26s
Test loss: 0.3339 score: 0.8636 time: 0.19s
Epoch 103/1000, LR 0.000264
Train loss: 0.1930;  Loss pred: 0.1930; Loss self: 0.0000; time: 0.34s
Val loss: 0.3480 score: 0.8372 time: 0.23s
Test loss: 0.3334 score: 0.8409 time: 0.22s
Epoch 104/1000, LR 0.000264
Train loss: 0.1999;  Loss pred: 0.1999; Loss self: 0.0000; time: 0.29s
Val loss: 0.3469 score: 0.8372 time: 0.25s
Test loss: 0.3356 score: 0.8409 time: 0.20s
Epoch 105/1000, LR 0.000264
Train loss: 0.1815;  Loss pred: 0.1815; Loss self: 0.0000; time: 0.57s
Val loss: 0.3465 score: 0.8372 time: 0.24s
Test loss: 0.3388 score: 0.8409 time: 0.22s
Epoch 106/1000, LR 0.000264
Train loss: 0.1532;  Loss pred: 0.1532; Loss self: 0.0000; time: 0.27s
Val loss: 0.3473 score: 0.8372 time: 0.26s
Test loss: 0.3446 score: 0.8409 time: 0.20s
     INFO: Early stopping counter 1 of 2
Epoch 107/1000, LR 0.000264
Train loss: 0.1613;  Loss pred: 0.1613; Loss self: 0.0000; time: 0.33s
Val loss: 0.3490 score: 0.8372 time: 0.25s
Test loss: 0.3524 score: 0.8409 time: 0.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 104,   Train_Loss: 0.1815,   Val_Loss: 0.3465,   Val_Precision: 0.8261,   Val_Recall: 0.8636,   Val_accuracy: 0.8444,   Val_Score: 0.8372,   Val_Loss: 0.3465,   Test_Precision: 0.8000,   Test_Recall: 0.9091,   Test_accuracy: 0.8511,   Test_Score: 0.8409,   Test_loss: 0.3388


[0.22677201498299837, 0.1989461249904707, 0.23860981198959053, 0.28399317304138094, 0.22186995297670364, 0.20030690694693476, 0.21999915700871497, 0.21828265697695315, 0.2034589999821037, 0.21911106701008976, 0.19824813294690102, 0.2288160390453413, 0.2016532599227503, 0.22572159895207733, 0.20288608502596617, 0.21513689903076738, 0.22228247195016593, 0.2418078240007162, 0.20784154592547566, 0.22202336299233139, 0.20647422992624342, 0.2651357500581071, 0.3044410989386961, 0.21815371501725167, 0.2055462800199166, 0.21992008504457772, 0.20182721002493054, 0.20419409696478397, 0.21482222294434905, 0.2145374430110678, 0.2246936900774017, 0.20211914496030658, 0.22789123200345784, 0.1950372870778665, 0.21621569094713777, 0.23038189799990505, 0.20142882992513478, 0.23080903210211545, 0.19446461903862655, 0.20194428297691047, 0.20989986998029053, 0.20448758592829108, 0.23101040802430362, 0.21073597006034106, 0.25399410794489086, 0.2397034679306671, 0.3416898250579834, 0.2800042279995978, 0.24449155398178846, 0.21642331592738628, 0.2108991569839418, 0.223706308985129, 0.21732211904600263, 0.2288183969212696, 0.19553256302606314, 0.22530835401266813, 0.19371072005014867, 0.2035262209828943, 0.20830345596186817, 0.19932770694140345, 0.20892478502355516, 0.21187711309175938, 0.21340012201108038, 0.20425730606075376, 0.2338520979974419, 0.1921234360197559, 0.21336142206564546, 0.23004389402922243, 0.21335666300728917, 0.2322171040577814, 0.2069724720204249, 0.21828713896684349, 0.2125179689610377, 0.2270964259514585, 0.19611645303666592, 0.21919468499254435, 0.22874542290810496, 0.19868647400289774, 0.22871298401150852, 0.20534175797365606, 0.2355126809561625, 0.2017333140829578, 0.2285941750742495, 0.19236925209406763, 0.21251236903481185, 0.21775790606625378, 0.2043022169964388, 0.23021220904774964, 0.2161337659927085, 0.21720758103765547, 0.2018729120027274, 0.22575920599047095, 0.19708438904490322, 0.22433787304908037, 0.20482022897340357, 0.21374656807165593, 0.31811921391636133, 0.22409073892049491, 0.21010333497542888, 0.24563380295876414, 0.22458819998428226, 0.19732112903147936, 0.22264620498754084, 0.2093913060380146, 0.22452981502283365, 0.20636654901318252, 0.21924365404993296]
[0.005153909431431781, 0.004521502840692516, 0.005422950272490693, 0.006454390296395021, 0.005042498931288719, 0.004552429703339426, 0.004999980841107158, 0.0049609694767489354, 0.004624068181411448, 0.00497979697750204, 0.004505639385156842, 0.0052003645237577575, 0.004583028634607961, 0.00513003633981994, 0.004611047386953776, 0.004889474977971986, 0.005051874362503771, 0.005495632363652641, 0.004723671498306265, 0.005045985522552986, 0.004692596134687351, 0.006025812501320615, 0.006919115884970365, 0.004958038977664811, 0.004671506364089014, 0.00499818375101313, 0.0045869820460211486, 0.004640774931017818, 0.004882323248735206, 0.004875850977524268, 0.005106674774486402, 0.004593616930916059, 0.005179346181896769, 0.0044326656154060565, 0.004913992976071313, 0.005235952227270569, 0.004577927952843972, 0.005245659820502624, 0.004419650432696058, 0.004589642794929783, 0.004770451590461148, 0.004647445134733888, 0.0052502365460069, 0.004789453865007751, 0.005772593362383883, 0.005447806089333343, 0.007765677842226895, 0.0063637324545363135, 0.005556626226858829, 0.0049187117256224155, 0.00479316265872595, 0.005084234295116568, 0.004939139069227333, 0.005200418111847036, 0.004443921886955981, 0.005120644409378821, 0.0044025163647761064, 0.004625595931429416, 0.004734169453678822, 0.0045301751577591694, 0.004748290568717162, 0.004815388933903622, 0.0048500027729791, 0.004642211501380767, 0.00531482040903277, 0.004366441727721725, 0.00484912322876467, 0.0052282703188459645, 0.004849015068347481, 0.005277661455858668, 0.004703919818646021, 0.004961071340155534, 0.004829953840023584, 0.005161282407987694, 0.00445719211446968, 0.00498169738619419, 0.00519875961154784, 0.00451560168188404, 0.005198022363897921, 0.00466685813576491, 0.0053525609308218754, 0.00458484804733995, 0.005195322160778398, 0.004372028456683355, 0.004829826568972997, 0.004949043319687586, 0.004643232204464518, 0.005232095660176128, 0.004912131045288829, 0.004936535932673988, 0.004588020727334713, 0.005130891045237976, 0.004479190660111437, 0.005098588023842735, 0.00465500520394099, 0.0048578765470830895, 0.007229982134462757, 0.005092971339102157, 0.004775075794896111, 0.005582586430881003, 0.005104277272370051, 0.004484571114351804, 0.00506014102244411, 0.0047588933190457865, 0.005102950341428037, 0.004690148841208694, 0.004982810319316658]
[194.0274685273612, 221.1654034583863, 184.40146963411345, 154.93330184239574, 198.31437024111148, 219.66291961992337, 200.00076635865017, 201.57350386588723, 216.25978700313206, 200.81139944416347, 221.9440826299484, 192.2942123444464, 218.1963238127447, 194.9303930340383, 216.87046696361017, 204.5209361956427, 197.9463320430613, 181.96268124008128, 211.69973406460701, 198.17734227149666, 213.10165445691527, 165.95272418131833, 144.52713563768893, 201.69264592409286, 214.06371351374776, 200.07267635914755, 218.00826555827103, 215.48125364068872, 204.8205227417205, 205.09240430226475, 195.8221434026164, 217.69338084544634, 193.07456286572875, 225.59788776406373, 203.50049437789178, 190.9872276510984, 218.4394359851741, 190.63378759169765, 226.2622384345415, 217.8818798501505, 209.62376014873936, 215.17198611474512, 190.4676086948037, 208.792072788529, 173.23236493953115, 183.56013110634993, 128.77175957034535, 157.1404843217696, 179.9653169339234, 203.30526686303406, 208.63051625829985, 196.68645108674573, 202.4644347899355, 192.29223083465288, 225.02645758361538, 195.2879208266111, 227.1428240450972, 216.1883603376002, 211.23029282843294, 220.74201662759668, 210.60210733273811, 207.66754538959833, 206.18544912413586, 215.41457120223035, 188.1531120600907, 229.01943100515606, 206.22284747644028, 191.2678455808555, 206.22744741042737, 189.4778602916851, 212.58865766292774, 201.56936504941473, 207.04131615367925, 193.75029710685507, 224.35649492280876, 200.73479428343168, 192.35357560652193, 221.45443075988294, 192.3808575633204, 214.2769226980364, 186.8264580122121, 218.10973660952249, 192.48084508587496, 228.7267820664199, 207.04677191186136, 202.05925375959856, 215.36721748235834, 191.1280039490595, 203.57763072283848, 202.57119843516, 217.95891070024066, 194.89792146884673, 223.25461805082466, 196.13273230228822, 214.8225310582653, 205.85125832406132, 138.3129282205768, 196.34903348509528, 209.4207595759758, 179.12844026351908, 195.91412194887945, 222.98676384007777, 197.62295073685274, 210.132888669274, 195.9650659112929, 213.21284971039233, 200.68995926321753]
Elapsed: 0.2199792204028709~0.02396428592619125
Time per graph: 0.004999527736428885~0.0005446428619588921
Speed: 201.9571378189182~18.100054135902436
Total Time: 0.2197
best val loss: 0.3465059995651245 test_score: 0.8409

Testing...
Test loss: 0.5745 score: 0.7955 time: 0.20s
test Score 0.7955
Epoch Time List: [0.7662305890116841, 0.7416868330910802, 0.8892229539342225, 0.8428584159119055, 0.7423475019168109, 0.7168208649381995, 0.790173293906264, 0.809435788076371, 0.7465004530968145, 0.7989866989664733, 0.7266524700680748, 0.8578677281038836, 0.7818674109876156, 0.7739287997828797, 0.7365823710570112, 0.7959130691597238, 0.7598615800961852, 0.7398264069342986, 0.7322435430251062, 0.7527792049804702, 0.7535213879309595, 0.9295428120531142, 0.8680908239912242, 0.7752511529251933, 0.7394449850544333, 0.9477971009910107, 0.7490128639619797, 0.8285058999899775, 0.8354538918938488, 0.8171827809419483, 0.7776478679152206, 0.800379907945171, 0.7872135770739987, 0.7213576788781211, 0.7927466600667685, 0.8251050019171089, 0.7539033390348777, 0.7821969259530306, 0.712690947111696, 0.7764849580125883, 0.8586055560735986, 0.7685182560235262, 0.7864923839224502, 0.7283992749871686, 0.8665032258722931, 0.813510378007777, 0.923719099140726, 0.8473154449602589, 0.9753620130941272, 1.0115618840791285, 0.7938205379759893, 0.7792012739228085, 0.7396686758147553, 0.7944604089716449, 0.8386519131017849, 0.7738621619064361, 0.7575373998843133, 0.7744305619271472, 0.8885641600936651, 0.7501551801105961, 0.8994488879106939, 0.8610730009386316, 0.7571170690935105, 0.7531451468821615, 0.7851668470539153, 0.7441266508540139, 0.7983309159753844, 0.7744016881333664, 0.7485995491733775, 0.800937442923896, 0.7639800329925492, 0.7975999539485201, 0.8513203868642449, 0.793590247980319, 0.7328228511614725, 0.7868016180582345, 0.7887286490295082, 0.7330441049998626, 0.8827339279232547, 0.7869718340225518, 0.8326107069151476, 0.8358561939094216, 0.7803067991044372, 0.7238890191074461, 0.8355779969133437, 0.7691804190399125, 0.7872981819091365, 0.7845210748491809, 0.7380894989473745, 0.9509063859004527, 0.7665044490713626, 0.7945272430079058, 0.7287040499504656, 0.9534159868489951, 0.7256300998851657, 0.8964422550052404, 0.8656480059726164, 0.8565027189906687, 0.7115742448950186, 0.8133973251096904, 0.7655778599437326, 0.7322928839130327, 0.7938767969608307, 0.7447285470552742, 1.0313521588686854, 0.733785588061437, 0.7947981479810551]
Total Epoch List: [107]
Total Time List: [0.21971471200231463]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bce680>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4884 time: 0.20s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.4884 time: 0.22s
Epoch 5/1000, LR 0.000090
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4884 time: 0.19s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.31s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.19s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.18s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.22s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.21s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.23s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.31s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.22s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.23s
Epoch 18/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.19s
Epoch 19/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.21s
Epoch 21/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.18s
Epoch 23/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.21s
Epoch 24/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.19s
Epoch 25/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.54s
Val loss: 0.6914 score: 0.5455 time: 0.23s
Test loss: 0.6928 score: 0.5116 time: 0.20s
Epoch 26/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.31s
Val loss: 0.6911 score: 0.5909 time: 0.22s
Test loss: 0.6926 score: 0.5116 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.34s
Val loss: 0.6908 score: 0.6591 time: 0.21s
Test loss: 0.6924 score: 0.5349 time: 0.19s
Epoch 28/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.30s
Val loss: 0.6905 score: 0.6591 time: 0.20s
Test loss: 0.6922 score: 0.5349 time: 0.20s
Epoch 29/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.32s
Val loss: 0.6900 score: 0.6591 time: 0.20s
Test loss: 0.6919 score: 0.5349 time: 0.17s
Epoch 30/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.29s
Val loss: 0.6896 score: 0.6591 time: 0.22s
Test loss: 0.6916 score: 0.5581 time: 0.17s
Epoch 31/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.36s
Val loss: 0.6890 score: 0.6591 time: 0.18s
Test loss: 0.6913 score: 0.5581 time: 0.18s
Epoch 32/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.40s
Val loss: 0.6884 score: 0.6591 time: 0.22s
Test loss: 0.6909 score: 0.5581 time: 0.19s
Epoch 33/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.35s
Val loss: 0.6877 score: 0.6591 time: 0.21s
Test loss: 0.6904 score: 0.5581 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.31s
Val loss: 0.6869 score: 0.6818 time: 0.24s
Test loss: 0.6899 score: 0.5581 time: 0.21s
Epoch 35/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.38s
Val loss: 0.6860 score: 0.7273 time: 0.20s
Test loss: 0.6893 score: 0.6047 time: 0.22s
Epoch 36/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.31s
Val loss: 0.6850 score: 0.7273 time: 0.23s
Test loss: 0.6887 score: 0.6047 time: 0.19s
Epoch 37/1000, LR 0.000270
Train loss: 0.6803;  Loss pred: 0.6803; Loss self: 0.0000; time: 0.36s
Val loss: 0.6839 score: 0.8182 time: 0.21s
Test loss: 0.6879 score: 0.6047 time: 0.21s
Epoch 38/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.36s
Val loss: 0.6827 score: 0.8182 time: 0.22s
Test loss: 0.6871 score: 0.6047 time: 0.20s
Epoch 39/1000, LR 0.000269
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.30s
Val loss: 0.6814 score: 0.8182 time: 0.30s
Test loss: 0.6862 score: 0.6279 time: 0.19s
Epoch 40/1000, LR 0.000269
Train loss: 0.6754;  Loss pred: 0.6754; Loss self: 0.0000; time: 0.35s
Val loss: 0.6799 score: 0.8182 time: 0.20s
Test loss: 0.6852 score: 0.6279 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.32s
Val loss: 0.6782 score: 0.8182 time: 0.23s
Test loss: 0.6841 score: 0.6512 time: 0.20s
Epoch 42/1000, LR 0.000269
Train loss: 0.6717;  Loss pred: 0.6717; Loss self: 0.0000; time: 0.42s
Val loss: 0.6763 score: 0.8182 time: 0.20s
Test loss: 0.6829 score: 0.6512 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6698;  Loss pred: 0.6698; Loss self: 0.0000; time: 0.35s
Val loss: 0.6743 score: 0.8182 time: 0.22s
Test loss: 0.6815 score: 0.6744 time: 0.20s
Epoch 44/1000, LR 0.000269
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.30s
Val loss: 0.6721 score: 0.8182 time: 0.23s
Test loss: 0.6801 score: 0.6744 time: 0.20s
Epoch 45/1000, LR 0.000269
Train loss: 0.6625;  Loss pred: 0.6625; Loss self: 0.0000; time: 0.38s
Val loss: 0.6696 score: 0.8182 time: 0.21s
Test loss: 0.6785 score: 0.6744 time: 0.21s
Epoch 46/1000, LR 0.000269
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 0.34s
Val loss: 0.6669 score: 0.8182 time: 0.22s
Test loss: 0.6767 score: 0.6744 time: 0.20s
Epoch 47/1000, LR 0.000269
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 0.36s
Val loss: 0.6638 score: 0.8182 time: 0.21s
Test loss: 0.6748 score: 0.6744 time: 0.19s
Epoch 48/1000, LR 0.000269
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 0.35s
Val loss: 0.6605 score: 0.8182 time: 0.28s
Test loss: 0.6726 score: 0.6744 time: 0.21s
Epoch 49/1000, LR 0.000269
Train loss: 0.6469;  Loss pred: 0.6469; Loss self: 0.0000; time: 0.31s
Val loss: 0.6569 score: 0.8182 time: 0.23s
Test loss: 0.6703 score: 0.7209 time: 0.19s
Epoch 50/1000, LR 0.000269
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.36s
Val loss: 0.6529 score: 0.8182 time: 0.21s
Test loss: 0.6678 score: 0.7209 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6371;  Loss pred: 0.6371; Loss self: 0.0000; time: 0.33s
Val loss: 0.6486 score: 0.8182 time: 0.22s
Test loss: 0.6651 score: 0.6977 time: 0.20s
Epoch 52/1000, LR 0.000269
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.46s
Val loss: 0.6439 score: 0.8182 time: 0.22s
Test loss: 0.6620 score: 0.6977 time: 0.20s
Epoch 53/1000, LR 0.000269
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.36s
Val loss: 0.6390 score: 0.8182 time: 0.22s
Test loss: 0.6588 score: 0.6977 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.6206;  Loss pred: 0.6206; Loss self: 0.0000; time: 0.48s
Val loss: 0.6337 score: 0.8182 time: 0.22s
Test loss: 0.6553 score: 0.6977 time: 0.20s
Epoch 55/1000, LR 0.000269
Train loss: 0.6086;  Loss pred: 0.6086; Loss self: 0.0000; time: 0.36s
Val loss: 0.6280 score: 0.8182 time: 0.22s
Test loss: 0.6514 score: 0.7674 time: 0.21s
Epoch 56/1000, LR 0.000269
Train loss: 0.6022;  Loss pred: 0.6022; Loss self: 0.0000; time: 0.37s
Val loss: 0.6220 score: 0.8409 time: 0.22s
Test loss: 0.6472 score: 0.7907 time: 0.28s
Epoch 57/1000, LR 0.000269
Train loss: 0.5929;  Loss pred: 0.5929; Loss self: 0.0000; time: 0.36s
Val loss: 0.6158 score: 0.8636 time: 0.22s
Test loss: 0.6427 score: 0.7907 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.5905;  Loss pred: 0.5905; Loss self: 0.0000; time: 0.39s
Val loss: 0.6092 score: 0.8636 time: 0.22s
Test loss: 0.6379 score: 0.7907 time: 0.21s
Epoch 59/1000, LR 0.000268
Train loss: 0.5816;  Loss pred: 0.5816; Loss self: 0.0000; time: 0.34s
Val loss: 0.6024 score: 0.8636 time: 0.21s
Test loss: 0.6330 score: 0.7907 time: 0.22s
Epoch 60/1000, LR 0.000268
Train loss: 0.5703;  Loss pred: 0.5703; Loss self: 0.0000; time: 0.30s
Val loss: 0.5951 score: 0.8636 time: 0.23s
Test loss: 0.6279 score: 0.7907 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.5581;  Loss pred: 0.5581; Loss self: 0.0000; time: 0.38s
Val loss: 0.5875 score: 0.8636 time: 0.20s
Test loss: 0.6225 score: 0.7907 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 0.36s
Val loss: 0.5795 score: 0.8636 time: 0.23s
Test loss: 0.6169 score: 0.7907 time: 0.18s
Epoch 63/1000, LR 0.000268
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 0.36s
Val loss: 0.5712 score: 0.8636 time: 0.24s
Test loss: 0.6111 score: 0.7907 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5264;  Loss pred: 0.5264; Loss self: 0.0000; time: 0.36s
Val loss: 0.5625 score: 0.8636 time: 0.22s
Test loss: 0.6050 score: 0.7907 time: 0.20s
Epoch 65/1000, LR 0.000268
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.31s
Val loss: 0.5535 score: 0.8636 time: 0.23s
Test loss: 0.5986 score: 0.7907 time: 0.20s
Epoch 66/1000, LR 0.000268
Train loss: 0.5008;  Loss pred: 0.5008; Loss self: 0.0000; time: 0.49s
Val loss: 0.5442 score: 0.8636 time: 0.22s
Test loss: 0.5918 score: 0.8140 time: 0.20s
Epoch 67/1000, LR 0.000268
Train loss: 0.4817;  Loss pred: 0.4817; Loss self: 0.0000; time: 0.35s
Val loss: 0.5346 score: 0.8636 time: 0.21s
Test loss: 0.5847 score: 0.8140 time: 0.19s
Epoch 68/1000, LR 0.000268
Train loss: 0.4717;  Loss pred: 0.4717; Loss self: 0.0000; time: 0.34s
Val loss: 0.5246 score: 0.8409 time: 0.20s
Test loss: 0.5773 score: 0.8140 time: 0.21s
Epoch 69/1000, LR 0.000268
Train loss: 0.4517;  Loss pred: 0.4517; Loss self: 0.0000; time: 0.31s
Val loss: 0.5144 score: 0.8409 time: 0.25s
Test loss: 0.5697 score: 0.8140 time: 0.20s
Epoch 70/1000, LR 0.000268
Train loss: 0.4443;  Loss pred: 0.4443; Loss self: 0.0000; time: 0.37s
Val loss: 0.5038 score: 0.8409 time: 0.22s
Test loss: 0.5621 score: 0.8140 time: 0.21s
Epoch 71/1000, LR 0.000268
Train loss: 0.4249;  Loss pred: 0.4249; Loss self: 0.0000; time: 0.34s
Val loss: 0.4931 score: 0.8409 time: 0.23s
Test loss: 0.5545 score: 0.8140 time: 0.20s
Epoch 72/1000, LR 0.000267
Train loss: 0.4108;  Loss pred: 0.4108; Loss self: 0.0000; time: 0.36s
Val loss: 0.4822 score: 0.8409 time: 0.21s
Test loss: 0.5468 score: 0.8140 time: 0.20s
Epoch 73/1000, LR 0.000267
Train loss: 0.3983;  Loss pred: 0.3983; Loss self: 0.0000; time: 0.35s
Val loss: 0.4713 score: 0.8409 time: 0.21s
Test loss: 0.5390 score: 0.8140 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.3852;  Loss pred: 0.3852; Loss self: 0.0000; time: 0.40s
Val loss: 0.4605 score: 0.8409 time: 0.22s
Test loss: 0.5315 score: 0.8140 time: 0.19s
Epoch 75/1000, LR 0.000267
Train loss: 0.3725;  Loss pred: 0.3725; Loss self: 0.0000; time: 0.35s
Val loss: 0.4498 score: 0.8409 time: 0.21s
Test loss: 0.5247 score: 0.8140 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.3452;  Loss pred: 0.3452; Loss self: 0.0000; time: 0.31s
Val loss: 0.4395 score: 0.8409 time: 0.23s
Test loss: 0.5184 score: 0.8140 time: 0.19s
Epoch 77/1000, LR 0.000267
Train loss: 0.3251;  Loss pred: 0.3251; Loss self: 0.0000; time: 0.49s
Val loss: 0.4294 score: 0.8409 time: 0.22s
Test loss: 0.5121 score: 0.8140 time: 0.21s
Epoch 78/1000, LR 0.000267
Train loss: 0.3117;  Loss pred: 0.3117; Loss self: 0.0000; time: 0.34s
Val loss: 0.4198 score: 0.8409 time: 0.22s
Test loss: 0.5057 score: 0.8140 time: 0.20s
Epoch 79/1000, LR 0.000267
Train loss: 0.2969;  Loss pred: 0.2969; Loss self: 0.0000; time: 0.53s
Val loss: 0.4106 score: 0.8409 time: 0.20s
Test loss: 0.4995 score: 0.8372 time: 0.30s
Epoch 80/1000, LR 0.000267
Train loss: 0.2903;  Loss pred: 0.2903; Loss self: 0.0000; time: 0.29s
Val loss: 0.4021 score: 0.8409 time: 0.26s
Test loss: 0.4939 score: 0.8372 time: 0.21s
Epoch 81/1000, LR 0.000267
Train loss: 0.2602;  Loss pred: 0.2602; Loss self: 0.0000; time: 0.36s
Val loss: 0.3944 score: 0.8409 time: 0.20s
Test loss: 0.4902 score: 0.8372 time: 0.21s
Epoch 82/1000, LR 0.000267
Train loss: 0.2436;  Loss pred: 0.2436; Loss self: 0.0000; time: 0.32s
Val loss: 0.3875 score: 0.8409 time: 0.31s
Test loss: 0.4875 score: 0.8372 time: 0.20s
Epoch 83/1000, LR 0.000266
Train loss: 0.2358;  Loss pred: 0.2358; Loss self: 0.0000; time: 0.36s
Val loss: 0.3816 score: 0.8409 time: 0.22s
Test loss: 0.4855 score: 0.8372 time: 0.21s
Epoch 84/1000, LR 0.000266
Train loss: 0.2197;  Loss pred: 0.2197; Loss self: 0.0000; time: 0.32s
Val loss: 0.3764 score: 0.8409 time: 0.22s
Test loss: 0.4838 score: 0.8372 time: 0.20s
Epoch 85/1000, LR 0.000266
Train loss: 0.2210;  Loss pred: 0.2210; Loss self: 0.0000; time: 0.38s
Val loss: 0.3722 score: 0.8409 time: 0.28s
Test loss: 0.4829 score: 0.8372 time: 0.20s
Epoch 86/1000, LR 0.000266
Train loss: 0.2043;  Loss pred: 0.2043; Loss self: 0.0000; time: 0.33s
Val loss: 0.3690 score: 0.8409 time: 0.21s
Test loss: 0.4831 score: 0.8372 time: 0.20s
Epoch 87/1000, LR 0.000266
Train loss: 0.1878;  Loss pred: 0.1878; Loss self: 0.0000; time: 0.31s
Val loss: 0.3662 score: 0.8409 time: 0.33s
Test loss: 0.4823 score: 0.8372 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.1758;  Loss pred: 0.1758; Loss self: 0.0000; time: 0.35s
Val loss: 0.3638 score: 0.8409 time: 0.23s
Test loss: 0.4799 score: 0.8372 time: 0.20s
Epoch 89/1000, LR 0.000266
Train loss: 0.1689;  Loss pred: 0.1689; Loss self: 0.0000; time: 0.31s
Val loss: 0.3625 score: 0.8409 time: 0.26s
Test loss: 0.4772 score: 0.8372 time: 0.19s
Epoch 90/1000, LR 0.000266
Train loss: 0.1680;  Loss pred: 0.1680; Loss self: 0.0000; time: 0.37s
Val loss: 0.3622 score: 0.8409 time: 0.20s
Test loss: 0.4759 score: 0.8372 time: 0.22s
Epoch 91/1000, LR 0.000266
Train loss: 0.1646;  Loss pred: 0.1646; Loss self: 0.0000; time: 0.43s
Val loss: 0.3631 score: 0.8409 time: 0.25s
Test loss: 0.4743 score: 0.8372 time: 0.19s
     INFO: Early stopping counter 1 of 2
Epoch 92/1000, LR 0.000266
Train loss: 0.1267;  Loss pred: 0.1267; Loss self: 0.0000; time: 0.36s
Val loss: 0.3650 score: 0.8409 time: 0.20s
Test loss: 0.4740 score: 0.8605 time: 0.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.1680,   Val_Loss: 0.3622,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3622,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.4759


[0.22677201498299837, 0.1989461249904707, 0.23860981198959053, 0.28399317304138094, 0.22186995297670364, 0.20030690694693476, 0.21999915700871497, 0.21828265697695315, 0.2034589999821037, 0.21911106701008976, 0.19824813294690102, 0.2288160390453413, 0.2016532599227503, 0.22572159895207733, 0.20288608502596617, 0.21513689903076738, 0.22228247195016593, 0.2418078240007162, 0.20784154592547566, 0.22202336299233139, 0.20647422992624342, 0.2651357500581071, 0.3044410989386961, 0.21815371501725167, 0.2055462800199166, 0.21992008504457772, 0.20182721002493054, 0.20419409696478397, 0.21482222294434905, 0.2145374430110678, 0.2246936900774017, 0.20211914496030658, 0.22789123200345784, 0.1950372870778665, 0.21621569094713777, 0.23038189799990505, 0.20142882992513478, 0.23080903210211545, 0.19446461903862655, 0.20194428297691047, 0.20989986998029053, 0.20448758592829108, 0.23101040802430362, 0.21073597006034106, 0.25399410794489086, 0.2397034679306671, 0.3416898250579834, 0.2800042279995978, 0.24449155398178846, 0.21642331592738628, 0.2108991569839418, 0.223706308985129, 0.21732211904600263, 0.2288183969212696, 0.19553256302606314, 0.22530835401266813, 0.19371072005014867, 0.2035262209828943, 0.20830345596186817, 0.19932770694140345, 0.20892478502355516, 0.21187711309175938, 0.21340012201108038, 0.20425730606075376, 0.2338520979974419, 0.1921234360197559, 0.21336142206564546, 0.23004389402922243, 0.21335666300728917, 0.2322171040577814, 0.2069724720204249, 0.21828713896684349, 0.2125179689610377, 0.2270964259514585, 0.19611645303666592, 0.21919468499254435, 0.22874542290810496, 0.19868647400289774, 0.22871298401150852, 0.20534175797365606, 0.2355126809561625, 0.2017333140829578, 0.2285941750742495, 0.19236925209406763, 0.21251236903481185, 0.21775790606625378, 0.2043022169964388, 0.23021220904774964, 0.2161337659927085, 0.21720758103765547, 0.2018729120027274, 0.22575920599047095, 0.19708438904490322, 0.22433787304908037, 0.20482022897340357, 0.21374656807165593, 0.31811921391636133, 0.22409073892049491, 0.21010333497542888, 0.24563380295876414, 0.22458819998428226, 0.19732112903147936, 0.22264620498754084, 0.2093913060380146, 0.22452981502283365, 0.20636654901318252, 0.21924365404993296, 0.21400514198467135, 0.20224183204118162, 0.21011048299260437, 0.2259024519007653, 0.19234359893016517, 0.31072792201302946, 0.1901002119993791, 0.22197060601320118, 0.1806232100352645, 0.2372235539369285, 0.2255183740053326, 0.2173781800083816, 0.23205564206000417, 0.31233209394849837, 0.2199651210103184, 0.2417225189274177, 0.23116695310454816, 0.19546064198948443, 0.2084502459038049, 0.21246002393309027, 0.2414515420095995, 0.18269978591706604, 0.21769223199225962, 0.19359448994509876, 0.20958174404222518, 0.2057983319973573, 0.19720434409100562, 0.20177889405749738, 0.17543639603536576, 0.1759925439255312, 0.1798248690320179, 0.1959011439466849, 0.22824258694890887, 0.21260057005565614, 0.22170809598173946, 0.19431814702693373, 0.20961108792107552, 0.20057520491536707, 0.1943874639691785, 0.22184563998598605, 0.20375577406957746, 0.22024937602691352, 0.20486085093580186, 0.20768430503085256, 0.21276650903746486, 0.20918764802627265, 0.19644554797559977, 0.21509003406390548, 0.19412492599803954, 0.22223343793302774, 0.20510532497428358, 0.2079412640305236, 0.20702934509608895, 0.20419515203684568, 0.2110964609310031, 0.28211264696437865, 0.2109537289943546, 0.20929544896353036, 0.22630280593875796, 0.20728508802130818, 0.21857145393732935, 0.18456666497513652, 0.24336819699965417, 0.20674892200622708, 0.20011014805641025, 0.2071909880032763, 0.19578082498628646, 0.21881145611405373, 0.19966025301255286, 0.2184429100016132, 0.2018949929624796, 0.2016810909844935, 0.22502167196944356, 0.19762713403906673, 0.22314600797835737, 0.1928361461032182, 0.21288587502203882, 0.20241811801679432, 0.3010791289852932, 0.21380801300983876, 0.21342629205901176, 0.20292402908671647, 0.21651010809000582, 0.20409339002799243, 0.20444750902242959, 0.20608894003089517, 0.21293049398809671, 0.20255993504542857, 0.19477490906137973, 0.22209509299136698, 0.19280308892484754, 0.22298636694904417]
[0.005153909431431781, 0.004521502840692516, 0.005422950272490693, 0.006454390296395021, 0.005042498931288719, 0.004552429703339426, 0.004999980841107158, 0.0049609694767489354, 0.004624068181411448, 0.00497979697750204, 0.004505639385156842, 0.0052003645237577575, 0.004583028634607961, 0.00513003633981994, 0.004611047386953776, 0.004889474977971986, 0.005051874362503771, 0.005495632363652641, 0.004723671498306265, 0.005045985522552986, 0.004692596134687351, 0.006025812501320615, 0.006919115884970365, 0.004958038977664811, 0.004671506364089014, 0.00499818375101313, 0.0045869820460211486, 0.004640774931017818, 0.004882323248735206, 0.004875850977524268, 0.005106674774486402, 0.004593616930916059, 0.005179346181896769, 0.0044326656154060565, 0.004913992976071313, 0.005235952227270569, 0.004577927952843972, 0.005245659820502624, 0.004419650432696058, 0.004589642794929783, 0.004770451590461148, 0.004647445134733888, 0.0052502365460069, 0.004789453865007751, 0.005772593362383883, 0.005447806089333343, 0.007765677842226895, 0.0063637324545363135, 0.005556626226858829, 0.0049187117256224155, 0.00479316265872595, 0.005084234295116568, 0.004939139069227333, 0.005200418111847036, 0.004443921886955981, 0.005120644409378821, 0.0044025163647761064, 0.004625595931429416, 0.004734169453678822, 0.0045301751577591694, 0.004748290568717162, 0.004815388933903622, 0.0048500027729791, 0.004642211501380767, 0.00531482040903277, 0.004366441727721725, 0.00484912322876467, 0.0052282703188459645, 0.004849015068347481, 0.005277661455858668, 0.004703919818646021, 0.004961071340155534, 0.004829953840023584, 0.005161282407987694, 0.00445719211446968, 0.00498169738619419, 0.00519875961154784, 0.00451560168188404, 0.005198022363897921, 0.00466685813576491, 0.0053525609308218754, 0.00458484804733995, 0.005195322160778398, 0.004372028456683355, 0.004829826568972997, 0.004949043319687586, 0.004643232204464518, 0.005232095660176128, 0.004912131045288829, 0.004936535932673988, 0.004588020727334713, 0.005130891045237976, 0.004479190660111437, 0.005098588023842735, 0.00465500520394099, 0.0048578765470830895, 0.007229982134462757, 0.005092971339102157, 0.004775075794896111, 0.005582586430881003, 0.005104277272370051, 0.004484571114351804, 0.00506014102244411, 0.0047588933190457865, 0.005102950341428037, 0.004690148841208694, 0.004982810319316658, 0.00497686376708538, 0.004703298419562363, 0.00488629030215359, 0.005253545393041054, 0.004473106951864306, 0.007226230744489057, 0.004420935162776258, 0.005162107116586074, 0.004200539768261965, 0.005516826835742524, 0.005244613348961223, 0.005055306511822828, 0.005396642838604748, 0.007263537068569729, 0.0051154679304725205, 0.005621453928544597, 0.005375975653594143, 0.004545596325336848, 0.004847680137297789, 0.0049409307891416345, 0.005615152139758128, 0.004248832230629443, 0.0050626100463316195, 0.004502197440583692, 0.004873994047493609, 0.0047860077208687745, 0.004586147537000131, 0.004692532419941799, 0.004079916186868971, 0.004092849858733284, 0.004181973698419021, 0.004555840556899649, 0.005307967138346718, 0.00494419930361991, 0.0051560022321334754, 0.00451902667504497, 0.004874676463280826, 0.004664539649194583, 0.00452063869695764, 0.005159200929906653, 0.004738506373711104, 0.005122078512253803, 0.004764205835716322, 0.004829867558857036, 0.004948058349708485, 0.004864829023866806, 0.0045685011157116226, 0.005002093815439662, 0.004514533162745106, 0.005168219486814599, 0.004769891278471711, 0.004835843349547061, 0.0048146359324671845, 0.004748724465973155, 0.004909220021651234, 0.006560759231729736, 0.0049059006742873165, 0.004867336022407683, 0.005262855952064139, 0.004820583442356004, 0.005083057068309985, 0.0042922480226775936, 0.0056597255116198645, 0.004808114465261095, 0.004653724373404889, 0.004818395069843635, 0.004553042441541546, 0.005088638514280319, 0.004643261697966345, 0.005080067674456121, 0.00469523239447627, 0.004690257929871941, 0.005233062138824269, 0.004595979861373645, 0.005189442046008311, 0.004484561537284144, 0.004950834302838112, 0.004707398093413822, 0.007001840208960307, 0.004972279372321832, 0.00496340214090725, 0.004719163467132941, 0.005035118792790833, 0.0047463579076277304, 0.004754593233079757, 0.00479276604723012, 0.004951871953211551, 0.004710696163847176, 0.0045296490479390635, 0.0051650021625899295, 0.004483792765694129, 0.00518572946393126]
[194.0274685273612, 221.1654034583863, 184.40146963411345, 154.93330184239574, 198.31437024111148, 219.66291961992337, 200.00076635865017, 201.57350386588723, 216.25978700313206, 200.81139944416347, 221.9440826299484, 192.2942123444464, 218.1963238127447, 194.9303930340383, 216.87046696361017, 204.5209361956427, 197.9463320430613, 181.96268124008128, 211.69973406460701, 198.17734227149666, 213.10165445691527, 165.95272418131833, 144.52713563768893, 201.69264592409286, 214.06371351374776, 200.07267635914755, 218.00826555827103, 215.48125364068872, 204.8205227417205, 205.09240430226475, 195.8221434026164, 217.69338084544634, 193.07456286572875, 225.59788776406373, 203.50049437789178, 190.9872276510984, 218.4394359851741, 190.63378759169765, 226.2622384345415, 217.8818798501505, 209.62376014873936, 215.17198611474512, 190.4676086948037, 208.792072788529, 173.23236493953115, 183.56013110634993, 128.77175957034535, 157.1404843217696, 179.9653169339234, 203.30526686303406, 208.63051625829985, 196.68645108674573, 202.4644347899355, 192.29223083465288, 225.02645758361538, 195.2879208266111, 227.1428240450972, 216.1883603376002, 211.23029282843294, 220.74201662759668, 210.60210733273811, 207.66754538959833, 206.18544912413586, 215.41457120223035, 188.1531120600907, 229.01943100515606, 206.22284747644028, 191.2678455808555, 206.22744741042737, 189.4778602916851, 212.58865766292774, 201.56936504941473, 207.04131615367925, 193.75029710685507, 224.35649492280876, 200.73479428343168, 192.35357560652193, 221.45443075988294, 192.3808575633204, 214.2769226980364, 186.8264580122121, 218.10973660952249, 192.48084508587496, 228.7267820664199, 207.04677191186136, 202.05925375959856, 215.36721748235834, 191.1280039490595, 203.57763072283848, 202.57119843516, 217.95891070024066, 194.89792146884673, 223.25461805082466, 196.13273230228822, 214.8225310582653, 205.85125832406132, 138.3129282205768, 196.34903348509528, 209.4207595759758, 179.12844026351908, 195.91412194887945, 222.98676384007777, 197.62295073685274, 210.132888669274, 195.9650659112929, 213.21284971039233, 200.68995926321753, 200.92975150606418, 212.61674484457842, 204.65423422739715, 190.34764624373838, 223.55825844566917, 138.38473131551055, 226.19648630449947, 193.71934317034155, 238.06464291939432, 181.26361942723685, 190.6718252543947, 197.81194229495352, 185.30038579661513, 137.67397213777983, 195.48553790026966, 177.88992184427659, 186.01274716180004, 219.99313806773105, 206.28423734191003, 202.39101551425, 178.08956464767755, 235.35878700766096, 197.52657045442453, 222.113759602323, 205.17054191197414, 208.94241261660065, 218.0479349894869, 213.10454793030556, 245.10307422943038, 244.32853256666857, 239.12154215079025, 219.498463019198, 188.39604201307694, 202.2572187306137, 193.9487135532552, 221.28658932734638, 205.14181967411338, 214.38342799222815, 221.20768038219765, 193.82846560660963, 211.03696421047968, 195.2332432249233, 209.89857165767165, 207.0450147574326, 202.09947606194154, 205.557069959501, 218.89017309438324, 199.91628244023735, 221.50684554766684, 193.4902344126921, 209.64838433810243, 206.7891632787615, 207.70002426488054, 210.58286433872303, 203.69834629323586, 152.42138366604124, 203.8361692158129, 205.45119453358362, 190.01090075585122, 207.4437694021663, 196.73200331242396, 232.97814914623206, 176.6870138749522, 207.9817373785624, 214.88165601615802, 207.53798422603228, 219.63335787869906, 196.51621886555424, 215.3658494928209, 196.84777134530228, 212.98200301575173, 213.20789068572674, 191.09270508770868, 217.58145817921846, 192.69894357317165, 222.98724004255746, 201.98615805557068, 212.43157688301577, 142.81959744243986, 201.11500684504878, 201.47470859921336, 211.9019624907241, 198.60504610770593, 210.6878620326819, 210.32293426124622, 208.6477808734122, 201.9438324432939, 212.28284848312325, 220.76765537828766, 193.61076114217187, 223.02547246408957, 192.83690114484068]
Elapsed: 0.21657681573432724~0.024104122591843448
Time per graph: 0.004974154126835717~0.000547896876689928
Speed: 203.0550139646757~18.646091400367666
Total Time: 0.2237
best val loss: 0.36219000816345215 test_score: 0.8372

Testing...
Test loss: 0.6427 score: 0.7907 time: 0.19s
test Score 0.7907
Epoch Time List: [0.7662305890116841, 0.7416868330910802, 0.8892229539342225, 0.8428584159119055, 0.7423475019168109, 0.7168208649381995, 0.790173293906264, 0.809435788076371, 0.7465004530968145, 0.7989866989664733, 0.7266524700680748, 0.8578677281038836, 0.7818674109876156, 0.7739287997828797, 0.7365823710570112, 0.7959130691597238, 0.7598615800961852, 0.7398264069342986, 0.7322435430251062, 0.7527792049804702, 0.7535213879309595, 0.9295428120531142, 0.8680908239912242, 0.7752511529251933, 0.7394449850544333, 0.9477971009910107, 0.7490128639619797, 0.8285058999899775, 0.8354538918938488, 0.8171827809419483, 0.7776478679152206, 0.800379907945171, 0.7872135770739987, 0.7213576788781211, 0.7927466600667685, 0.8251050019171089, 0.7539033390348777, 0.7821969259530306, 0.712690947111696, 0.7764849580125883, 0.8586055560735986, 0.7685182560235262, 0.7864923839224502, 0.7283992749871686, 0.8665032258722931, 0.813510378007777, 0.923719099140726, 0.8473154449602589, 0.9753620130941272, 1.0115618840791285, 0.7938205379759893, 0.7792012739228085, 0.7396686758147553, 0.7944604089716449, 0.8386519131017849, 0.7738621619064361, 0.7575373998843133, 0.7744305619271472, 0.8885641600936651, 0.7501551801105961, 0.8994488879106939, 0.8610730009386316, 0.7571170690935105, 0.7531451468821615, 0.7851668470539153, 0.7441266508540139, 0.7983309159753844, 0.7744016881333664, 0.7485995491733775, 0.800937442923896, 0.7639800329925492, 0.7975999539485201, 0.8513203868642449, 0.793590247980319, 0.7328228511614725, 0.7868016180582345, 0.7887286490295082, 0.7330441049998626, 0.8827339279232547, 0.7869718340225518, 0.8326107069151476, 0.8358561939094216, 0.7803067991044372, 0.7238890191074461, 0.8355779969133437, 0.7691804190399125, 0.7872981819091365, 0.7845210748491809, 0.7380894989473745, 0.9509063859004527, 0.7665044490713626, 0.7945272430079058, 0.7287040499504656, 0.9534159868489951, 0.7256300998851657, 0.8964422550052404, 0.8656480059726164, 0.8565027189906687, 0.7115742448950186, 0.8133973251096904, 0.7655778599437326, 0.7322928839130327, 0.7938767969608307, 0.7447285470552742, 1.0313521588686854, 0.733785588061437, 0.7947981479810551, 0.7600699820322916, 0.757488199044019, 0.7820037410128862, 0.7981138149043545, 0.7291743600508198, 0.9290025229565799, 0.7370927109150216, 0.7626874910201877, 0.7183315750444308, 0.8114962809486315, 0.7976999708916992, 0.7489654120290652, 0.819769193069078, 0.8471711518941447, 0.7850059641059488, 0.8015643660910428, 0.8018897930160165, 0.7560511608608067, 0.7814448288409039, 0.8352677889633924, 0.8190807800274342, 0.8874184727901593, 0.9042377610458061, 0.9293581871315837, 0.9749710810137913, 0.7345480069052428, 0.7402483390178531, 0.692471212008968, 0.690630929893814, 0.6829842829611152, 0.7227846399182454, 0.8102811919525266, 0.7846355850342661, 0.7564458650304005, 0.8024427659111097, 0.7344777849502861, 0.7829335979185998, 0.7752134880283847, 0.793220107909292, 0.7732411550823599, 0.7483594759833068, 0.8327421918511391, 0.7733943819766864, 0.7400565251009539, 0.7985686860047281, 0.7665600419277325, 0.7708323939004913, 0.8433104940922931, 0.7369939930504188, 0.7857849749270827, 0.7529200969729573, 0.8822400331264362, 0.7794280049856752, 0.9001813340000808, 0.7941420809365809, 0.8719170400872827, 0.7918082160176709, 0.8082803478464484, 0.7705496619455516, 0.7341957009630278, 0.795298240147531, 0.7726786219282076, 0.8365629491163418, 0.7879472099011764, 0.7440905149560422, 0.9137802999466658, 0.7535070240264758, 0.759850473026745, 0.7504352269461378, 0.8067089169053361, 0.7683841049438342, 0.7736502269981429, 0.7807497880421579, 0.8146676360629499, 0.7709678841056302, 0.7269415609771386, 0.9242710028775036, 0.7599873999133706, 1.0319352890364826, 0.7652621589368209, 0.768875419977121, 0.8310028188861907, 0.788874470978044, 0.7465601230505854, 0.8618138760793954, 0.7501291779335588, 0.8479886919958517, 0.7779031890677288, 0.7603579909773543, 0.7950495058903471, 0.8628720390843228, 0.7793837669305503]
Total Epoch List: [107, 92]
Total Time List: [0.21971471200231463, 0.22365625097882003]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bce290>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 3/1000, LR 0.000030
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.22s
Epoch 5/1000, LR 0.000090
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.33s
Epoch 7/1000, LR 0.000150
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
Val loss: 0.6931 score: 0.4773 time: 0.17s
Test loss: 0.6930 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 14/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 011,   Train_Loss: 0.6929,   Val_Loss: 0.6931,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6931,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6930


[0.22677201498299837, 0.1989461249904707, 0.23860981198959053, 0.28399317304138094, 0.22186995297670364, 0.20030690694693476, 0.21999915700871497, 0.21828265697695315, 0.2034589999821037, 0.21911106701008976, 0.19824813294690102, 0.2288160390453413, 0.2016532599227503, 0.22572159895207733, 0.20288608502596617, 0.21513689903076738, 0.22228247195016593, 0.2418078240007162, 0.20784154592547566, 0.22202336299233139, 0.20647422992624342, 0.2651357500581071, 0.3044410989386961, 0.21815371501725167, 0.2055462800199166, 0.21992008504457772, 0.20182721002493054, 0.20419409696478397, 0.21482222294434905, 0.2145374430110678, 0.2246936900774017, 0.20211914496030658, 0.22789123200345784, 0.1950372870778665, 0.21621569094713777, 0.23038189799990505, 0.20142882992513478, 0.23080903210211545, 0.19446461903862655, 0.20194428297691047, 0.20989986998029053, 0.20448758592829108, 0.23101040802430362, 0.21073597006034106, 0.25399410794489086, 0.2397034679306671, 0.3416898250579834, 0.2800042279995978, 0.24449155398178846, 0.21642331592738628, 0.2108991569839418, 0.223706308985129, 0.21732211904600263, 0.2288183969212696, 0.19553256302606314, 0.22530835401266813, 0.19371072005014867, 0.2035262209828943, 0.20830345596186817, 0.19932770694140345, 0.20892478502355516, 0.21187711309175938, 0.21340012201108038, 0.20425730606075376, 0.2338520979974419, 0.1921234360197559, 0.21336142206564546, 0.23004389402922243, 0.21335666300728917, 0.2322171040577814, 0.2069724720204249, 0.21828713896684349, 0.2125179689610377, 0.2270964259514585, 0.19611645303666592, 0.21919468499254435, 0.22874542290810496, 0.19868647400289774, 0.22871298401150852, 0.20534175797365606, 0.2355126809561625, 0.2017333140829578, 0.2285941750742495, 0.19236925209406763, 0.21251236903481185, 0.21775790606625378, 0.2043022169964388, 0.23021220904774964, 0.2161337659927085, 0.21720758103765547, 0.2018729120027274, 0.22575920599047095, 0.19708438904490322, 0.22433787304908037, 0.20482022897340357, 0.21374656807165593, 0.31811921391636133, 0.22409073892049491, 0.21010333497542888, 0.24563380295876414, 0.22458819998428226, 0.19732112903147936, 0.22264620498754084, 0.2093913060380146, 0.22452981502283365, 0.20636654901318252, 0.21924365404993296, 0.21400514198467135, 0.20224183204118162, 0.21011048299260437, 0.2259024519007653, 0.19234359893016517, 0.31072792201302946, 0.1901002119993791, 0.22197060601320118, 0.1806232100352645, 0.2372235539369285, 0.2255183740053326, 0.2173781800083816, 0.23205564206000417, 0.31233209394849837, 0.2199651210103184, 0.2417225189274177, 0.23116695310454816, 0.19546064198948443, 0.2084502459038049, 0.21246002393309027, 0.2414515420095995, 0.18269978591706604, 0.21769223199225962, 0.19359448994509876, 0.20958174404222518, 0.2057983319973573, 0.19720434409100562, 0.20177889405749738, 0.17543639603536576, 0.1759925439255312, 0.1798248690320179, 0.1959011439466849, 0.22824258694890887, 0.21260057005565614, 0.22170809598173946, 0.19431814702693373, 0.20961108792107552, 0.20057520491536707, 0.1943874639691785, 0.22184563998598605, 0.20375577406957746, 0.22024937602691352, 0.20486085093580186, 0.20768430503085256, 0.21276650903746486, 0.20918764802627265, 0.19644554797559977, 0.21509003406390548, 0.19412492599803954, 0.22223343793302774, 0.20510532497428358, 0.2079412640305236, 0.20702934509608895, 0.20419515203684568, 0.2110964609310031, 0.28211264696437865, 0.2109537289943546, 0.20929544896353036, 0.22630280593875796, 0.20728508802130818, 0.21857145393732935, 0.18456666497513652, 0.24336819699965417, 0.20674892200622708, 0.20011014805641025, 0.2071909880032763, 0.19578082498628646, 0.21881145611405373, 0.19966025301255286, 0.2184429100016132, 0.2018949929624796, 0.2016810909844935, 0.22502167196944356, 0.19762713403906673, 0.22314600797835737, 0.1928361461032182, 0.21288587502203882, 0.20241811801679432, 0.3010791289852932, 0.21380801300983876, 0.21342629205901176, 0.20292402908671647, 0.21651010809000582, 0.20409339002799243, 0.20444750902242959, 0.20608894003089517, 0.21293049398809671, 0.20255993504542857, 0.19477490906137973, 0.22209509299136698, 0.19280308892484754, 0.22298636694904417, 0.23639164690393955, 0.24517187906894833, 0.2349472490604967, 0.22570738603826612, 0.25626355200074613, 0.3334721029968932, 0.2492213590303436, 0.22720072604715824, 0.2481896170647815, 0.22095964301843196, 0.24343681999016553, 0.23138992092572153, 0.22579497506376356, 0.25616031396202743]
[0.005153909431431781, 0.004521502840692516, 0.005422950272490693, 0.006454390296395021, 0.005042498931288719, 0.004552429703339426, 0.004999980841107158, 0.0049609694767489354, 0.004624068181411448, 0.00497979697750204, 0.004505639385156842, 0.0052003645237577575, 0.004583028634607961, 0.00513003633981994, 0.004611047386953776, 0.004889474977971986, 0.005051874362503771, 0.005495632363652641, 0.004723671498306265, 0.005045985522552986, 0.004692596134687351, 0.006025812501320615, 0.006919115884970365, 0.004958038977664811, 0.004671506364089014, 0.00499818375101313, 0.0045869820460211486, 0.004640774931017818, 0.004882323248735206, 0.004875850977524268, 0.005106674774486402, 0.004593616930916059, 0.005179346181896769, 0.0044326656154060565, 0.004913992976071313, 0.005235952227270569, 0.004577927952843972, 0.005245659820502624, 0.004419650432696058, 0.004589642794929783, 0.004770451590461148, 0.004647445134733888, 0.0052502365460069, 0.004789453865007751, 0.005772593362383883, 0.005447806089333343, 0.007765677842226895, 0.0063637324545363135, 0.005556626226858829, 0.0049187117256224155, 0.00479316265872595, 0.005084234295116568, 0.004939139069227333, 0.005200418111847036, 0.004443921886955981, 0.005120644409378821, 0.0044025163647761064, 0.004625595931429416, 0.004734169453678822, 0.0045301751577591694, 0.004748290568717162, 0.004815388933903622, 0.0048500027729791, 0.004642211501380767, 0.00531482040903277, 0.004366441727721725, 0.00484912322876467, 0.0052282703188459645, 0.004849015068347481, 0.005277661455858668, 0.004703919818646021, 0.004961071340155534, 0.004829953840023584, 0.005161282407987694, 0.00445719211446968, 0.00498169738619419, 0.00519875961154784, 0.00451560168188404, 0.005198022363897921, 0.00466685813576491, 0.0053525609308218754, 0.00458484804733995, 0.005195322160778398, 0.004372028456683355, 0.004829826568972997, 0.004949043319687586, 0.004643232204464518, 0.005232095660176128, 0.004912131045288829, 0.004936535932673988, 0.004588020727334713, 0.005130891045237976, 0.004479190660111437, 0.005098588023842735, 0.00465500520394099, 0.0048578765470830895, 0.007229982134462757, 0.005092971339102157, 0.004775075794896111, 0.005582586430881003, 0.005104277272370051, 0.004484571114351804, 0.00506014102244411, 0.0047588933190457865, 0.005102950341428037, 0.004690148841208694, 0.004982810319316658, 0.00497686376708538, 0.004703298419562363, 0.00488629030215359, 0.005253545393041054, 0.004473106951864306, 0.007226230744489057, 0.004420935162776258, 0.005162107116586074, 0.004200539768261965, 0.005516826835742524, 0.005244613348961223, 0.005055306511822828, 0.005396642838604748, 0.007263537068569729, 0.0051154679304725205, 0.005621453928544597, 0.005375975653594143, 0.004545596325336848, 0.004847680137297789, 0.0049409307891416345, 0.005615152139758128, 0.004248832230629443, 0.0050626100463316195, 0.004502197440583692, 0.004873994047493609, 0.0047860077208687745, 0.004586147537000131, 0.004692532419941799, 0.004079916186868971, 0.004092849858733284, 0.004181973698419021, 0.004555840556899649, 0.005307967138346718, 0.00494419930361991, 0.0051560022321334754, 0.00451902667504497, 0.004874676463280826, 0.004664539649194583, 0.00452063869695764, 0.005159200929906653, 0.004738506373711104, 0.005122078512253803, 0.004764205835716322, 0.004829867558857036, 0.004948058349708485, 0.004864829023866806, 0.0045685011157116226, 0.005002093815439662, 0.004514533162745106, 0.005168219486814599, 0.004769891278471711, 0.004835843349547061, 0.0048146359324671845, 0.004748724465973155, 0.004909220021651234, 0.006560759231729736, 0.0049059006742873165, 0.004867336022407683, 0.005262855952064139, 0.004820583442356004, 0.005083057068309985, 0.0042922480226775936, 0.0056597255116198645, 0.004808114465261095, 0.004653724373404889, 0.004818395069843635, 0.004553042441541546, 0.005088638514280319, 0.004643261697966345, 0.005080067674456121, 0.00469523239447627, 0.004690257929871941, 0.005233062138824269, 0.004595979861373645, 0.005189442046008311, 0.004484561537284144, 0.004950834302838112, 0.004707398093413822, 0.007001840208960307, 0.004972279372321832, 0.00496340214090725, 0.004719163467132941, 0.005035118792790833, 0.0047463579076277304, 0.004754593233079757, 0.00479276604723012, 0.004951871953211551, 0.004710696163847176, 0.0045296490479390635, 0.0051650021625899295, 0.004483792765694129, 0.00518572946393126, 0.005497480160556734, 0.005701671606254613, 0.005463889513034807, 0.0052490089776340955, 0.005959617488389445, 0.007755165185974261, 0.0057958455588452, 0.0052837378150501916, 0.005771851559646081, 0.00513859634926586, 0.005661321395120129, 0.0053811609517609655, 0.0052510459317154316, 0.005957216603768079]
[194.0274685273612, 221.1654034583863, 184.40146963411345, 154.93330184239574, 198.31437024111148, 219.66291961992337, 200.00076635865017, 201.57350386588723, 216.25978700313206, 200.81139944416347, 221.9440826299484, 192.2942123444464, 218.1963238127447, 194.9303930340383, 216.87046696361017, 204.5209361956427, 197.9463320430613, 181.96268124008128, 211.69973406460701, 198.17734227149666, 213.10165445691527, 165.95272418131833, 144.52713563768893, 201.69264592409286, 214.06371351374776, 200.07267635914755, 218.00826555827103, 215.48125364068872, 204.8205227417205, 205.09240430226475, 195.8221434026164, 217.69338084544634, 193.07456286572875, 225.59788776406373, 203.50049437789178, 190.9872276510984, 218.4394359851741, 190.63378759169765, 226.2622384345415, 217.8818798501505, 209.62376014873936, 215.17198611474512, 190.4676086948037, 208.792072788529, 173.23236493953115, 183.56013110634993, 128.77175957034535, 157.1404843217696, 179.9653169339234, 203.30526686303406, 208.63051625829985, 196.68645108674573, 202.4644347899355, 192.29223083465288, 225.02645758361538, 195.2879208266111, 227.1428240450972, 216.1883603376002, 211.23029282843294, 220.74201662759668, 210.60210733273811, 207.66754538959833, 206.18544912413586, 215.41457120223035, 188.1531120600907, 229.01943100515606, 206.22284747644028, 191.2678455808555, 206.22744741042737, 189.4778602916851, 212.58865766292774, 201.56936504941473, 207.04131615367925, 193.75029710685507, 224.35649492280876, 200.73479428343168, 192.35357560652193, 221.45443075988294, 192.3808575633204, 214.2769226980364, 186.8264580122121, 218.10973660952249, 192.48084508587496, 228.7267820664199, 207.04677191186136, 202.05925375959856, 215.36721748235834, 191.1280039490595, 203.57763072283848, 202.57119843516, 217.95891070024066, 194.89792146884673, 223.25461805082466, 196.13273230228822, 214.8225310582653, 205.85125832406132, 138.3129282205768, 196.34903348509528, 209.4207595759758, 179.12844026351908, 195.91412194887945, 222.98676384007777, 197.62295073685274, 210.132888669274, 195.9650659112929, 213.21284971039233, 200.68995926321753, 200.92975150606418, 212.61674484457842, 204.65423422739715, 190.34764624373838, 223.55825844566917, 138.38473131551055, 226.19648630449947, 193.71934317034155, 238.06464291939432, 181.26361942723685, 190.6718252543947, 197.81194229495352, 185.30038579661513, 137.67397213777983, 195.48553790026966, 177.88992184427659, 186.01274716180004, 219.99313806773105, 206.28423734191003, 202.39101551425, 178.08956464767755, 235.35878700766096, 197.52657045442453, 222.113759602323, 205.17054191197414, 208.94241261660065, 218.0479349894869, 213.10454793030556, 245.10307422943038, 244.32853256666857, 239.12154215079025, 219.498463019198, 188.39604201307694, 202.2572187306137, 193.9487135532552, 221.28658932734638, 205.14181967411338, 214.38342799222815, 221.20768038219765, 193.82846560660963, 211.03696421047968, 195.2332432249233, 209.89857165767165, 207.0450147574326, 202.09947606194154, 205.557069959501, 218.89017309438324, 199.91628244023735, 221.50684554766684, 193.4902344126921, 209.64838433810243, 206.7891632787615, 207.70002426488054, 210.58286433872303, 203.69834629323586, 152.42138366604124, 203.8361692158129, 205.45119453358362, 190.01090075585122, 207.4437694021663, 196.73200331242396, 232.97814914623206, 176.6870138749522, 207.9817373785624, 214.88165601615802, 207.53798422603228, 219.63335787869906, 196.51621886555424, 215.3658494928209, 196.84777134530228, 212.98200301575173, 213.20789068572674, 191.09270508770868, 217.58145817921846, 192.69894357317165, 222.98724004255746, 201.98615805557068, 212.43157688301577, 142.81959744243986, 201.11500684504878, 201.47470859921336, 211.9019624907241, 198.60504610770593, 210.6878620326819, 210.32293426124622, 208.6477808734122, 201.9438324432939, 212.28284848312325, 220.76765537828766, 193.61076114217187, 223.02547246408957, 192.83690114484068, 181.90152047747077, 175.3871617058796, 183.01980624139128, 190.51215272463364, 167.79600401337916, 128.9463184882983, 172.53737868737244, 189.25995857546175, 173.25462889439208, 194.605672839601, 176.63720714778123, 185.83350488201873, 190.43825039887173, 167.86362936131556]
Elapsed: 0.21846522780423852~0.02531692779563357
Time per graph: 0.005022179719893537~0.0005821513644584221
Speed: 201.34244588454615~19.566085634892573
Total Time: 0.2568
best val loss: 0.6931387782096863 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.22s
test Score 0.5116
Epoch Time List: [0.7662305890116841, 0.7416868330910802, 0.8892229539342225, 0.8428584159119055, 0.7423475019168109, 0.7168208649381995, 0.790173293906264, 0.809435788076371, 0.7465004530968145, 0.7989866989664733, 0.7266524700680748, 0.8578677281038836, 0.7818674109876156, 0.7739287997828797, 0.7365823710570112, 0.7959130691597238, 0.7598615800961852, 0.7398264069342986, 0.7322435430251062, 0.7527792049804702, 0.7535213879309595, 0.9295428120531142, 0.8680908239912242, 0.7752511529251933, 0.7394449850544333, 0.9477971009910107, 0.7490128639619797, 0.8285058999899775, 0.8354538918938488, 0.8171827809419483, 0.7776478679152206, 0.800379907945171, 0.7872135770739987, 0.7213576788781211, 0.7927466600667685, 0.8251050019171089, 0.7539033390348777, 0.7821969259530306, 0.712690947111696, 0.7764849580125883, 0.8586055560735986, 0.7685182560235262, 0.7864923839224502, 0.7283992749871686, 0.8665032258722931, 0.813510378007777, 0.923719099140726, 0.8473154449602589, 0.9753620130941272, 1.0115618840791285, 0.7938205379759893, 0.7792012739228085, 0.7396686758147553, 0.7944604089716449, 0.8386519131017849, 0.7738621619064361, 0.7575373998843133, 0.7744305619271472, 0.8885641600936651, 0.7501551801105961, 0.8994488879106939, 0.8610730009386316, 0.7571170690935105, 0.7531451468821615, 0.7851668470539153, 0.7441266508540139, 0.7983309159753844, 0.7744016881333664, 0.7485995491733775, 0.800937442923896, 0.7639800329925492, 0.7975999539485201, 0.8513203868642449, 0.793590247980319, 0.7328228511614725, 0.7868016180582345, 0.7887286490295082, 0.7330441049998626, 0.8827339279232547, 0.7869718340225518, 0.8326107069151476, 0.8358561939094216, 0.7803067991044372, 0.7238890191074461, 0.8355779969133437, 0.7691804190399125, 0.7872981819091365, 0.7845210748491809, 0.7380894989473745, 0.9509063859004527, 0.7665044490713626, 0.7945272430079058, 0.7287040499504656, 0.9534159868489951, 0.7256300998851657, 0.8964422550052404, 0.8656480059726164, 0.8565027189906687, 0.7115742448950186, 0.8133973251096904, 0.7655778599437326, 0.7322928839130327, 0.7938767969608307, 0.7447285470552742, 1.0313521588686854, 0.733785588061437, 0.7947981479810551, 0.7600699820322916, 0.757488199044019, 0.7820037410128862, 0.7981138149043545, 0.7291743600508198, 0.9290025229565799, 0.7370927109150216, 0.7626874910201877, 0.7183315750444308, 0.8114962809486315, 0.7976999708916992, 0.7489654120290652, 0.819769193069078, 0.8471711518941447, 0.7850059641059488, 0.8015643660910428, 0.8018897930160165, 0.7560511608608067, 0.7814448288409039, 0.8352677889633924, 0.8190807800274342, 0.8874184727901593, 0.9042377610458061, 0.9293581871315837, 0.9749710810137913, 0.7345480069052428, 0.7402483390178531, 0.692471212008968, 0.690630929893814, 0.6829842829611152, 0.7227846399182454, 0.8102811919525266, 0.7846355850342661, 0.7564458650304005, 0.8024427659111097, 0.7344777849502861, 0.7829335979185998, 0.7752134880283847, 0.793220107909292, 0.7732411550823599, 0.7483594759833068, 0.8327421918511391, 0.7733943819766864, 0.7400565251009539, 0.7985686860047281, 0.7665600419277325, 0.7708323939004913, 0.8433104940922931, 0.7369939930504188, 0.7857849749270827, 0.7529200969729573, 0.8822400331264362, 0.7794280049856752, 0.9001813340000808, 0.7941420809365809, 0.8719170400872827, 0.7918082160176709, 0.8082803478464484, 0.7705496619455516, 0.7341957009630278, 0.795298240147531, 0.7726786219282076, 0.8365629491163418, 0.7879472099011764, 0.7440905149560422, 0.9137802999466658, 0.7535070240264758, 0.759850473026745, 0.7504352269461378, 0.8067089169053361, 0.7683841049438342, 0.7736502269981429, 0.7807497880421579, 0.8146676360629499, 0.7709678841056302, 0.7269415609771386, 0.9242710028775036, 0.7599873999133706, 1.0319352890364826, 0.7652621589368209, 0.768875419977121, 0.8310028188861907, 0.788874470978044, 0.7465601230505854, 0.8618138760793954, 0.7501291779335588, 0.8479886919958517, 0.7779031890677288, 0.7603579909773543, 0.7950495058903471, 0.8628720390843228, 0.7793837669305503, 0.8175439420156181, 0.8003628349397331, 0.7294140269514173, 0.7771044569090009, 0.8030214320169762, 0.8767458180664107, 0.7839402670506388, 0.780527308001183, 0.785887171048671, 0.7314948580460623, 0.7990166039671749, 0.7680724819656461, 0.7738629709929228, 0.8104559619678184]
Total Epoch List: [107, 92, 14]
Total Time List: [0.21971471200231463, 0.22365625097882003, 0.25683896092232317]
========================training times:7========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bce200>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.31s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.20s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.6929,   Val_Loss: 0.6936,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6936,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6933


[0.31343801703769714, 0.21900986309628934, 0.20916565402876586]
[0.007123591296311299, 0.00497749688855203, 0.004753764864290133]
[140.37863184512213, 200.90419389310821, 210.35958415022012]
Elapsed: 0.24720451138758412~0.04700627664520944
Time per graph: 0.005618284349717821~0.0010683244692093056
Speed: 183.88080329615016~31.00193856244638
Total Time: 0.2094
best val loss: 0.6935957074165344 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.23s
test Score 0.5000
Epoch Time List: [0.8741116289747879, 0.8869374430505559, 0.7683747530682012]
Total Epoch List: [3]
Total Time List: [0.20943512301892042]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcebc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.20s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.19s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.21s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.19s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.21s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.20s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.20s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.20s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.21s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.21s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.20s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.21s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.19s
Epoch 22/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.21s
Epoch 23/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.29s
Epoch 24/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.19s
Epoch 25/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.21s
Epoch 26/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.20s
Epoch 29/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.19s
Epoch 31/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.45s
Val loss: 0.6870 score: 0.5455 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4884 time: 0.20s
Epoch 32/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.34s
Val loss: 0.6860 score: 0.5682 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.21s
Epoch 33/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.57s
Val loss: 0.6848 score: 0.6136 time: 0.21s
Test loss: 0.6880 score: 0.5116 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6801;  Loss pred: 0.6801; Loss self: 0.0000; time: 0.34s
Val loss: 0.6835 score: 0.6136 time: 0.23s
Test loss: 0.6871 score: 0.5116 time: 0.19s
Epoch 35/1000, LR 0.000270
Train loss: 0.6787;  Loss pred: 0.6787; Loss self: 0.0000; time: 0.47s
Val loss: 0.6821 score: 0.6136 time: 0.22s
Test loss: 0.6860 score: 0.5349 time: 0.22s
Epoch 36/1000, LR 0.000270
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.34s
Val loss: 0.6804 score: 0.6591 time: 0.22s
Test loss: 0.6849 score: 0.5581 time: 0.20s
Epoch 37/1000, LR 0.000270
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.43s
Val loss: 0.6786 score: 0.6818 time: 0.25s
Test loss: 0.6836 score: 0.5581 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6722;  Loss pred: 0.6722; Loss self: 0.0000; time: 0.33s
Val loss: 0.6766 score: 0.7273 time: 0.22s
Test loss: 0.6822 score: 0.5814 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.42s
Val loss: 0.6743 score: 0.7500 time: 0.22s
Test loss: 0.6806 score: 0.6279 time: 0.19s
Epoch 40/1000, LR 0.000269
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.35s
Val loss: 0.6717 score: 0.7727 time: 0.21s
Test loss: 0.6789 score: 0.6512 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6614;  Loss pred: 0.6614; Loss self: 0.0000; time: 0.40s
Val loss: 0.6689 score: 0.7727 time: 0.22s
Test loss: 0.6770 score: 0.6512 time: 0.19s
Epoch 42/1000, LR 0.000269
Train loss: 0.6568;  Loss pred: 0.6568; Loss self: 0.0000; time: 0.35s
Val loss: 0.6657 score: 0.7727 time: 0.20s
Test loss: 0.6749 score: 0.6512 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6530;  Loss pred: 0.6530; Loss self: 0.0000; time: 0.35s
Val loss: 0.6623 score: 0.7727 time: 0.23s
Test loss: 0.6725 score: 0.6744 time: 0.20s
Epoch 44/1000, LR 0.000269
Train loss: 0.6496;  Loss pred: 0.6496; Loss self: 0.0000; time: 0.39s
Val loss: 0.6585 score: 0.7727 time: 0.21s
Test loss: 0.6700 score: 0.6977 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.34s
Val loss: 0.6545 score: 0.7727 time: 0.23s
Test loss: 0.6672 score: 0.7209 time: 0.19s
Epoch 46/1000, LR 0.000269
Train loss: 0.6389;  Loss pred: 0.6389; Loss self: 0.0000; time: 0.39s
Val loss: 0.6501 score: 0.7955 time: 0.20s
Test loss: 0.6642 score: 0.7209 time: 0.21s
Epoch 47/1000, LR 0.000269
Train loss: 0.6301;  Loss pred: 0.6301; Loss self: 0.0000; time: 0.37s
Val loss: 0.6454 score: 0.7955 time: 0.24s
Test loss: 0.6610 score: 0.7442 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6250;  Loss pred: 0.6250; Loss self: 0.0000; time: 0.37s
Val loss: 0.6403 score: 0.7955 time: 0.22s
Test loss: 0.6575 score: 0.7442 time: 0.20s
Epoch 49/1000, LR 0.000269
Train loss: 0.6180;  Loss pred: 0.6180; Loss self: 0.0000; time: 0.39s
Val loss: 0.6350 score: 0.8182 time: 0.30s
Test loss: 0.6538 score: 0.7674 time: 0.21s
Epoch 50/1000, LR 0.000269
Train loss: 0.6081;  Loss pred: 0.6081; Loss self: 0.0000; time: 0.51s
Val loss: 0.6293 score: 0.8182 time: 0.20s
Test loss: 0.6499 score: 0.8140 time: 0.21s
Epoch 51/1000, LR 0.000269
Train loss: 0.6070;  Loss pred: 0.6070; Loss self: 0.0000; time: 0.39s
Val loss: 0.6234 score: 0.7955 time: 0.23s
Test loss: 0.6458 score: 0.7907 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.34s
Val loss: 0.6172 score: 0.7955 time: 0.23s
Test loss: 0.6414 score: 0.7674 time: 0.20s
Epoch 53/1000, LR 0.000269
Train loss: 0.5839;  Loss pred: 0.5839; Loss self: 0.0000; time: 0.40s
Val loss: 0.6106 score: 0.7955 time: 0.22s
Test loss: 0.6368 score: 0.7674 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.5746;  Loss pred: 0.5746; Loss self: 0.0000; time: 0.30s
Val loss: 0.6038 score: 0.7955 time: 0.23s
Test loss: 0.6319 score: 0.7674 time: 0.21s
Epoch 55/1000, LR 0.000269
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.38s
Val loss: 0.5968 score: 0.7955 time: 0.20s
Test loss: 0.6266 score: 0.7442 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.5593;  Loss pred: 0.5593; Loss self: 0.0000; time: 0.31s
Val loss: 0.5894 score: 0.7955 time: 0.22s
Test loss: 0.6211 score: 0.7674 time: 0.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.5481;  Loss pred: 0.5481; Loss self: 0.0000; time: 0.43s
Val loss: 0.5817 score: 0.7727 time: 0.20s
Test loss: 0.6154 score: 0.7674 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.5305;  Loss pred: 0.5305; Loss self: 0.0000; time: 0.44s
Val loss: 0.5737 score: 0.7727 time: 0.23s
Test loss: 0.6094 score: 0.7674 time: 0.20s
Epoch 59/1000, LR 0.000268
Train loss: 0.5238;  Loss pred: 0.5238; Loss self: 0.0000; time: 0.49s
Val loss: 0.5654 score: 0.7727 time: 0.21s
Test loss: 0.6031 score: 0.7674 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.33s
Val loss: 0.5568 score: 0.7727 time: 0.23s
Test loss: 0.5967 score: 0.7674 time: 0.18s
Epoch 61/1000, LR 0.000268
Train loss: 0.4945;  Loss pred: 0.4945; Loss self: 0.0000; time: 0.41s
Val loss: 0.5481 score: 0.7727 time: 0.22s
Test loss: 0.5900 score: 0.7674 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.4770;  Loss pred: 0.4770; Loss self: 0.0000; time: 0.33s
Val loss: 0.5391 score: 0.7727 time: 0.22s
Test loss: 0.5830 score: 0.7674 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.4639;  Loss pred: 0.4639; Loss self: 0.0000; time: 0.36s
Val loss: 0.5297 score: 0.7955 time: 0.22s
Test loss: 0.5758 score: 0.7674 time: 0.19s
Epoch 64/1000, LR 0.000268
Train loss: 0.4577;  Loss pred: 0.4577; Loss self: 0.0000; time: 0.36s
Val loss: 0.5203 score: 0.7955 time: 0.21s
Test loss: 0.5688 score: 0.7674 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.4374;  Loss pred: 0.4374; Loss self: 0.0000; time: 0.31s
Val loss: 0.5107 score: 0.7955 time: 0.23s
Test loss: 0.5616 score: 0.7674 time: 0.18s
Epoch 66/1000, LR 0.000268
Train loss: 0.4226;  Loss pred: 0.4226; Loss self: 0.0000; time: 0.38s
Val loss: 0.5009 score: 0.7955 time: 0.19s
Test loss: 0.5541 score: 0.7674 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.4066;  Loss pred: 0.4066; Loss self: 0.0000; time: 0.47s
Val loss: 0.4911 score: 0.7955 time: 0.23s
Test loss: 0.5467 score: 0.7907 time: 0.20s
Epoch 68/1000, LR 0.000268
Train loss: 0.3938;  Loss pred: 0.3938; Loss self: 0.0000; time: 0.36s
Val loss: 0.4814 score: 0.7955 time: 0.22s
Test loss: 0.5395 score: 0.7907 time: 0.19s
Epoch 69/1000, LR 0.000268
Train loss: 0.3771;  Loss pred: 0.3771; Loss self: 0.0000; time: 0.35s
Val loss: 0.4718 score: 0.7955 time: 0.21s
Test loss: 0.5327 score: 0.7907 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.3550;  Loss pred: 0.3550; Loss self: 0.0000; time: 0.30s
Val loss: 0.4625 score: 0.7955 time: 0.24s
Test loss: 0.5264 score: 0.7907 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.3369;  Loss pred: 0.3369; Loss self: 0.0000; time: 0.37s
Val loss: 0.4534 score: 0.7955 time: 0.19s
Test loss: 0.5207 score: 0.8140 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.3351;  Loss pred: 0.3351; Loss self: 0.0000; time: 0.34s
Val loss: 0.4450 score: 0.8182 time: 0.22s
Test loss: 0.5161 score: 0.8140 time: 0.19s
Epoch 73/1000, LR 0.000267
Train loss: 0.3024;  Loss pred: 0.3024; Loss self: 0.0000; time: 0.29s
Val loss: 0.4373 score: 0.8636 time: 0.23s
Test loss: 0.5124 score: 0.8140 time: 0.20s
Epoch 74/1000, LR 0.000267
Train loss: 0.2817;  Loss pred: 0.2817; Loss self: 0.0000; time: 0.42s
Val loss: 0.4302 score: 0.8636 time: 0.20s
Test loss: 0.5088 score: 0.8140 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.2747;  Loss pred: 0.2747; Loss self: 0.0000; time: 0.38s
Val loss: 0.4233 score: 0.8636 time: 0.22s
Test loss: 0.5044 score: 0.8140 time: 0.19s
Epoch 76/1000, LR 0.000267
Train loss: 0.2671;  Loss pred: 0.2671; Loss self: 0.0000; time: 0.37s
Val loss: 0.4165 score: 0.8636 time: 0.20s
Test loss: 0.4994 score: 0.8140 time: 0.21s
Epoch 77/1000, LR 0.000267
Train loss: 0.2525;  Loss pred: 0.2525; Loss self: 0.0000; time: 0.32s
Val loss: 0.4100 score: 0.8636 time: 0.23s
Test loss: 0.4937 score: 0.8372 time: 0.19s
Epoch 78/1000, LR 0.000267
Train loss: 0.2442;  Loss pred: 0.2442; Loss self: 0.0000; time: 0.36s
Val loss: 0.4045 score: 0.8409 time: 0.21s
Test loss: 0.4888 score: 0.8372 time: 0.20s
Epoch 79/1000, LR 0.000267
Train loss: 0.2082;  Loss pred: 0.2082; Loss self: 0.0000; time: 0.35s
Val loss: 0.4003 score: 0.8409 time: 0.22s
Test loss: 0.4855 score: 0.8372 time: 0.20s
Epoch 80/1000, LR 0.000267
Train loss: 0.2064;  Loss pred: 0.2064; Loss self: 0.0000; time: 0.30s
Val loss: 0.3971 score: 0.8409 time: 0.24s
Test loss: 0.4833 score: 0.8372 time: 0.20s
Epoch 81/1000, LR 0.000267
Train loss: 0.2002;  Loss pred: 0.2002; Loss self: 0.0000; time: 0.47s
Val loss: 0.3949 score: 0.8409 time: 0.24s
Test loss: 0.4820 score: 0.8372 time: 0.22s
Epoch 82/1000, LR 0.000267
Train loss: 0.1916;  Loss pred: 0.1916; Loss self: 0.0000; time: 0.33s
Val loss: 0.3930 score: 0.8409 time: 0.24s
Test loss: 0.4799 score: 0.8372 time: 0.19s
Epoch 83/1000, LR 0.000266
Train loss: 0.1887;  Loss pred: 0.1887; Loss self: 0.0000; time: 0.40s
Val loss: 0.3924 score: 0.8409 time: 0.21s
Test loss: 0.4792 score: 0.8372 time: 0.30s
Epoch 84/1000, LR 0.000266
Train loss: 0.1627;  Loss pred: 0.1627; Loss self: 0.0000; time: 0.36s
Val loss: 0.3930 score: 0.8409 time: 0.23s
Test loss: 0.4804 score: 0.8372 time: 0.19s
     INFO: Early stopping counter 1 of 2
Epoch 85/1000, LR 0.000266
Train loss: 0.1496;  Loss pred: 0.1496; Loss self: 0.0000; time: 0.41s
Val loss: 0.3945 score: 0.8409 time: 0.22s
Test loss: 0.4825 score: 0.8372 time: 0.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 082,   Train_Loss: 0.1887,   Val_Loss: 0.3924,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3924,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.4792


[0.31343801703769714, 0.21900986309628934, 0.20916565402876586, 0.20637578901369125, 0.22996490402147174, 0.21842012798879296, 0.19758289901074022, 0.2160945520736277, 0.19608385395258665, 0.2173821770120412, 0.2021875570062548, 0.20835206203628331, 0.20242177695035934, 0.21425539697520435, 0.21707938704639673, 0.2003914089873433, 0.21131563803646713, 0.2020946650300175, 0.23530041193589568, 0.21682829200290143, 0.22318276099395007, 0.20957842201460153, 0.22282688599079847, 0.19609311397653073, 0.21323801507242024, 0.2928057840326801, 0.19661808898672462, 0.21184711495880038, 0.20756907807663083, 0.22500853997189552, 0.20460736798122525, 0.24537184997461736, 0.1981513339560479, 0.20557183702476323, 0.21008739795070142, 0.22203900292515755, 0.19002743298187852, 0.2196032910142094, 0.20734036806970835, 0.20367468509357423, 0.21611798903904855, 0.19426919205579907, 0.22089988796506077, 0.19225731608457863, 0.22178562299814075, 0.20668049494270235, 0.22493666596710682, 0.1897855979623273, 0.21379003999754786, 0.2318186720367521, 0.19949150492902845, 0.21535559801850468, 0.21455700800288469, 0.21176709199789912, 0.20254071697127074, 0.20182467496488243, 0.21359056502114981, 0.22381970600690693, 0.20479991589672863, 0.20953143306542188, 0.20560403598938137, 0.23095413495320827, 0.18833136290777475, 0.21181881497614086, 0.21757157798856497, 0.19741593103390187, 0.22606484300922602, 0.18527392693795264, 0.21084476099349558, 0.20673649397213012, 0.19786283699795604, 0.22599645901937038, 0.20892532798461616, 0.22038996394257993, 0.19624560698866844, 0.20413724891841412, 0.2274248320609331, 0.19209695304743946, 0.21805279003456235, 0.19818204105831683, 0.20617981406394392, 0.20082866598386317, 0.2086984529159963, 0.22470212902408093, 0.19497841002885252, 0.3047871949383989, 0.19904929003678262, 0.22006059193518013]
[0.007123591296311299, 0.00497749688855203, 0.004753764864290133, 0.004799436953806773, 0.005348021023755156, 0.005079537860204487, 0.004594951139784656, 0.005025454699386691, 0.004560089626804341, 0.005055399465396307, 0.004702036209447786, 0.004845396791541472, 0.0047074831848920775, 0.004982683650586147, 0.005048357838288296, 0.004660265325287054, 0.004914317163638771, 0.004699875930930639, 0.005472102603160365, 0.005042518418672127, 0.005190296767301164, 0.004873916791037245, 0.005182020604437174, 0.004560304976198389, 0.004959023606335354, 0.0068094368379693045, 0.004572513697365689, 0.004926677092065125, 0.004827187862247229, 0.005232756743532454, 0.0047583108832843085, 0.005706322092432962, 0.004608170557117393, 0.0047807403959247265, 0.004885753440713987, 0.005163697742445524, 0.004419242627485547, 0.0051070532794002185, 0.0048218690248769385, 0.004736620583571494, 0.005025999745094153, 0.004517888187344165, 0.005137206696861878, 0.004471100374059968, 0.005157805186003273, 0.00480652313820238, 0.005231085255048996, 0.004413618557263426, 0.004971861395291811, 0.005391131907831444, 0.004639337323930895, 0.005008269721360574, 0.004989697860532202, 0.004924816092974398, 0.004710249231890017, 0.004693597092206569, 0.004967222442352321, 0.005205109442021091, 0.004762788741784387, 0.004872824024777253, 0.0047814892090553805, 0.005371026394260657, 0.004379799137390111, 0.004926018952933508, 0.0050598041392689525, 0.004591068163579113, 0.005257321930447117, 0.004308695975301224, 0.0049033665347324555, 0.004807825441212329, 0.004601461325533861, 0.005255731605101637, 0.004858728557781771, 0.00512534799866465, 0.004563851325317871, 0.004747377881823584, 0.0052889495828123976, 0.004467371001103243, 0.005070995117082845, 0.00460888467577481, 0.0047948793968359054, 0.004670434092647981, 0.004853452393395263, 0.0052256309075367656, 0.004534381628577965, 0.007088074300892998, 0.004629053256669363, 0.005117688184539073]
[140.37863184512213, 200.90419389310821, 210.35958415022012, 208.35777396905468, 186.98505401496007, 196.8683032829571, 217.6301705020666, 198.98696930290518, 219.29393539152622, 197.80830512898098, 212.6738194807397, 206.38144676730772, 212.42773701440757, 200.69506116093945, 198.0842151116335, 214.58005718556464, 203.48706986171752, 212.77157412152928, 182.74511143531168, 198.313603832772, 192.6672105340861, 205.17379407028912, 192.9749177654247, 219.28357976479697, 201.6525992581401, 146.85502249231786, 218.69808735097257, 202.97656641848798, 207.15995079057566, 191.10385768189454, 210.1586097522435, 175.24422628124685, 217.00585679396872, 209.17262122252774, 204.67672225675466, 193.65966984860748, 226.2831177859493, 195.80763021086827, 207.3884617854218, 211.12098432971442, 198.96539011489085, 221.3423525622596, 194.65831511332053, 223.65858878984486, 193.8809171609851, 208.05059525293285, 191.16492109066834, 226.57145990885758, 201.13191428605936, 185.4898038290154, 215.54802554273064, 199.66975734851886, 200.4129364043978, 203.05326759847367, 212.30299093934434, 213.0562083525318, 201.31975396826223, 192.11891913875112, 209.96102372269996, 205.21980578720203, 209.13986339363873, 186.1841530081801, 228.3209728644982, 203.00368503545587, 197.6361085282011, 217.81423502551925, 190.21091217728673, 232.08878178741523, 203.9415150624801, 207.99424027088693, 217.32226552702363, 190.26846786265102, 205.8151609227878, 195.10870291354627, 219.11318505327304, 210.64259574295264, 189.07346049387942, 223.84529956277288, 197.19995324611213, 216.97223305590472, 208.55581908064056, 214.11285978195514, 206.03890157876745, 191.3644529615995, 220.53723791078687, 141.0820425336137, 216.02689460512, 195.4007285987209]
Elapsed: 0.21338432943130928~0.020646317707454724
Time per graph: 0.00495797202349526~0.0004714507420209991
Speed: 203.19531567436968~16.024115360126437
Total Time: 0.2205
best val loss: 0.3924185037612915 test_score: 0.8372

Testing...
Test loss: 0.5124 score: 0.8140 time: 0.19s
test Score 0.8140
Epoch Time List: [0.8741116289747879, 0.8869374430505559, 0.7683747530682012, 0.7585825490532443, 0.8588974250014871, 0.8703764589736238, 0.8092612040927634, 0.8113696557702497, 0.7981063859770074, 0.7644046199275181, 0.7396953308489174, 0.8262916340027004, 0.7550656460225582, 0.7517965000588447, 0.7762144809821621, 0.7663062089122832, 0.8204934949753806, 0.8868632070953026, 1.0104619240155444, 0.7734723821049556, 0.8159620328806341, 0.7556495568715036, 0.8554468700895086, 0.72365092986729, 0.7988321629818529, 0.8385764700360596, 0.7474032749887556, 0.8008176198927686, 0.7423410979099572, 0.8189776679500937, 0.7354069059947506, 0.9353537309216335, 0.751455779070966, 0.8692385391332209, 0.7695001279935241, 0.9973576498450711, 0.753335818182677, 0.9096006110776216, 0.7616706960834563, 0.8880848749540746, 0.7663890639087185, 0.8280335620511323, 0.7706028299871832, 0.809731048066169, 0.7689229829702526, 0.7911868758965284, 0.8205758959520608, 0.7529718840960413, 0.7969924901844934, 0.8324863029411063, 0.784584827022627, 0.9010841400595382, 0.9222825950710103, 0.8289881729288027, 0.7720697259064764, 0.8173036470543593, 0.7438863950083032, 0.8025109550217167, 0.7329735619714484, 0.8411505869589746, 0.879109955043532, 0.9245308580575511, 0.7399806160246953, 0.8312220741063356, 0.7657927260734141, 0.7748841689899564, 0.7866890330333263, 0.721943341079168, 0.7775950520299375, 0.8941296000266448, 0.7677873080829158, 0.7814401889918372, 0.7432090119691566, 0.7695906319422647, 0.7475992600666359, 0.7259408219251782, 0.8405201440909877, 0.7913840579567477, 0.7830475369701162, 0.7497388040646911, 0.7731010989518836, 0.7646437609801069, 0.7431044819531962, 0.9315205580787733, 0.7514330609701574, 0.9169313819147646, 0.785949517856352, 0.8413740170653909]
Total Epoch List: [3, 85]
Total Time List: [0.20943512301892042, 0.22050783096347004]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcf040>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.33s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.27s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.23s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.28s
Epoch 17/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.23s
Epoch 18/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.21s
Epoch 23/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.4884 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.4884 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4884 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4884 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.4884 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.35s
Val loss: 0.6876 score: 0.5227 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.4884 time: 0.31s
Epoch 34/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.47s
Val loss: 0.6868 score: 0.5227 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.4884 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.33s
Val loss: 0.6859 score: 0.5227 time: 0.19s
Test loss: 0.6838 score: 0.5116 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.39s
Val loss: 0.6848 score: 0.5455 time: 0.18s
Test loss: 0.6824 score: 0.5814 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.44s
Val loss: 0.6837 score: 0.5682 time: 0.39s
Test loss: 0.6808 score: 0.5814 time: 0.22s
Epoch 38/1000, LR 0.000270
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.36s
Val loss: 0.6824 score: 0.6136 time: 0.19s
Test loss: 0.6791 score: 0.6279 time: 0.25s
Epoch 39/1000, LR 0.000269
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.32s
Val loss: 0.6810 score: 0.6364 time: 0.20s
Test loss: 0.6772 score: 0.6744 time: 0.22s
Epoch 40/1000, LR 0.000269
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.42s
Val loss: 0.6794 score: 0.6364 time: 0.17s
Test loss: 0.6751 score: 0.6744 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6717;  Loss pred: 0.6717; Loss self: 0.0000; time: 0.34s
Val loss: 0.6778 score: 0.6818 time: 0.20s
Test loss: 0.6728 score: 0.6744 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.44s
Val loss: 0.6760 score: 0.6818 time: 0.18s
Test loss: 0.6703 score: 0.6744 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6676;  Loss pred: 0.6676; Loss self: 0.0000; time: 0.33s
Val loss: 0.6740 score: 0.6818 time: 0.19s
Test loss: 0.6677 score: 0.6744 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.39s
Val loss: 0.6719 score: 0.7273 time: 0.18s
Test loss: 0.6647 score: 0.6744 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.52s
Val loss: 0.6695 score: 0.7273 time: 0.20s
Test loss: 0.6615 score: 0.6977 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6567;  Loss pred: 0.6567; Loss self: 0.0000; time: 0.40s
Val loss: 0.6669 score: 0.7273 time: 0.16s
Test loss: 0.6580 score: 0.6977 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6538;  Loss pred: 0.6538; Loss self: 0.0000; time: 0.35s
Val loss: 0.6641 score: 0.7500 time: 0.19s
Test loss: 0.6543 score: 0.7209 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6521;  Loss pred: 0.6521; Loss self: 0.0000; time: 0.39s
Val loss: 0.6612 score: 0.7273 time: 0.16s
Test loss: 0.6504 score: 0.7209 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6451;  Loss pred: 0.6451; Loss self: 0.0000; time: 0.34s
Val loss: 0.6580 score: 0.7273 time: 0.19s
Test loss: 0.6462 score: 0.7209 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.36s
Val loss: 0.6544 score: 0.7273 time: 0.18s
Test loss: 0.6417 score: 0.7209 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6364;  Loss pred: 0.6364; Loss self: 0.0000; time: 0.43s
Val loss: 0.6506 score: 0.7955 time: 0.18s
Test loss: 0.6369 score: 0.7674 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6331;  Loss pred: 0.6331; Loss self: 0.0000; time: 0.35s
Val loss: 0.6464 score: 0.7955 time: 0.19s
Test loss: 0.6316 score: 0.7907 time: 0.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6252;  Loss pred: 0.6252; Loss self: 0.0000; time: 0.44s
Val loss: 0.6419 score: 0.8182 time: 0.20s
Test loss: 0.6259 score: 0.7907 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.44s
Val loss: 0.6372 score: 0.8182 time: 0.18s
Test loss: 0.6201 score: 0.7907 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6101;  Loss pred: 0.6101; Loss self: 0.0000; time: 0.36s
Val loss: 0.6322 score: 0.8182 time: 0.23s
Test loss: 0.6138 score: 0.7907 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.36s
Val loss: 0.6268 score: 0.8409 time: 0.19s
Test loss: 0.6071 score: 0.8140 time: 0.21s
Epoch 57/1000, LR 0.000269
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.41s
Val loss: 0.6213 score: 0.8636 time: 0.20s
Test loss: 0.6001 score: 0.8140 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5937;  Loss pred: 0.5937; Loss self: 0.0000; time: 0.33s
Val loss: 0.6154 score: 0.8636 time: 0.19s
Test loss: 0.5928 score: 0.7907 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.5793;  Loss pred: 0.5793; Loss self: 0.0000; time: 0.37s
Val loss: 0.6091 score: 0.8636 time: 0.17s
Test loss: 0.5850 score: 0.7907 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5710;  Loss pred: 0.5710; Loss self: 0.0000; time: 0.33s
Val loss: 0.6025 score: 0.8409 time: 0.19s
Test loss: 0.5769 score: 0.7907 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5653;  Loss pred: 0.5653; Loss self: 0.0000; time: 0.54s
Val loss: 0.5952 score: 0.8182 time: 0.17s
Test loss: 0.5680 score: 0.7907 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.5546;  Loss pred: 0.5546; Loss self: 0.0000; time: 0.31s
Val loss: 0.5874 score: 0.8182 time: 0.19s
Test loss: 0.5581 score: 0.8605 time: 0.23s
Epoch 63/1000, LR 0.000268
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.46s
Val loss: 0.5794 score: 0.8636 time: 0.19s
Test loss: 0.5480 score: 0.8605 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.5388;  Loss pred: 0.5388; Loss self: 0.0000; time: 0.34s
Val loss: 0.5708 score: 0.8636 time: 0.19s
Test loss: 0.5369 score: 0.9070 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.44s
Val loss: 0.5619 score: 0.8636 time: 0.17s
Test loss: 0.5253 score: 0.9070 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5050;  Loss pred: 0.5050; Loss self: 0.0000; time: 0.36s
Val loss: 0.5529 score: 0.8636 time: 0.27s
Test loss: 0.5138 score: 0.9070 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.5069;  Loss pred: 0.5069; Loss self: 0.0000; time: 0.39s
Val loss: 0.5435 score: 0.8636 time: 0.18s
Test loss: 0.5023 score: 0.9070 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.4889;  Loss pred: 0.4889; Loss self: 0.0000; time: 0.35s
Val loss: 0.5338 score: 0.8864 time: 0.17s
Test loss: 0.4906 score: 0.8837 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.4766;  Loss pred: 0.4766; Loss self: 0.0000; time: 0.40s
Val loss: 0.5240 score: 0.8864 time: 0.20s
Test loss: 0.4794 score: 0.8837 time: 0.23s
Epoch 70/1000, LR 0.000268
Train loss: 0.4555;  Loss pred: 0.4555; Loss self: 0.0000; time: 0.37s
Val loss: 0.5139 score: 0.8864 time: 0.17s
Test loss: 0.4684 score: 0.8837 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.4477;  Loss pred: 0.4477; Loss self: 0.0000; time: 0.34s
Val loss: 0.5037 score: 0.8636 time: 0.19s
Test loss: 0.4576 score: 0.8837 time: 0.23s
Epoch 72/1000, LR 0.000267
Train loss: 0.4279;  Loss pred: 0.4279; Loss self: 0.0000; time: 0.45s
Val loss: 0.4934 score: 0.8636 time: 0.18s
Test loss: 0.4471 score: 0.8837 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.4205;  Loss pred: 0.4205; Loss self: 0.0000; time: 0.35s
Val loss: 0.4827 score: 0.8636 time: 0.19s
Test loss: 0.4355 score: 0.8605 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.4220;  Loss pred: 0.4220; Loss self: 0.0000; time: 0.40s
Val loss: 0.4718 score: 0.8636 time: 0.18s
Test loss: 0.4234 score: 0.8605 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.4020;  Loss pred: 0.4020; Loss self: 0.0000; time: 0.37s
Val loss: 0.4610 score: 0.8636 time: 0.20s
Test loss: 0.4108 score: 0.8837 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.3961;  Loss pred: 0.3961; Loss self: 0.0000; time: 0.37s
Val loss: 0.4503 score: 0.8864 time: 0.18s
Test loss: 0.3986 score: 0.8837 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.3701;  Loss pred: 0.3701; Loss self: 0.0000; time: 0.46s
Val loss: 0.4397 score: 0.8864 time: 0.23s
Test loss: 0.3871 score: 0.8837 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.3888;  Loss pred: 0.3888; Loss self: 0.0000; time: 0.39s
Val loss: 0.4292 score: 0.9091 time: 0.18s
Test loss: 0.3756 score: 0.8837 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.3393;  Loss pred: 0.3393; Loss self: 0.0000; time: 0.36s
Val loss: 0.4189 score: 0.9091 time: 0.22s
Test loss: 0.3652 score: 0.8837 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.3309;  Loss pred: 0.3309; Loss self: 0.0000; time: 0.37s
Val loss: 0.4091 score: 0.9091 time: 0.19s
Test loss: 0.3570 score: 0.8837 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.3109;  Loss pred: 0.3109; Loss self: 0.0000; time: 0.34s
Val loss: 0.3996 score: 0.9091 time: 0.18s
Test loss: 0.3495 score: 0.8605 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.3119;  Loss pred: 0.3119; Loss self: 0.0000; time: 0.40s
Val loss: 0.3907 score: 0.9545 time: 0.28s
Test loss: 0.3429 score: 0.8372 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.2837;  Loss pred: 0.2837; Loss self: 0.0000; time: 0.36s
Val loss: 0.3818 score: 0.9545 time: 0.19s
Test loss: 0.3354 score: 0.8372 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.2744;  Loss pred: 0.2744; Loss self: 0.0000; time: 0.43s
Val loss: 0.3734 score: 0.9318 time: 0.21s
Test loss: 0.3292 score: 0.8140 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2472;  Loss pred: 0.2472; Loss self: 0.0000; time: 0.45s
Val loss: 0.3649 score: 0.9545 time: 0.24s
Test loss: 0.3226 score: 0.8140 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.2477;  Loss pred: 0.2477; Loss self: 0.0000; time: 0.37s
Val loss: 0.3565 score: 0.9545 time: 0.17s
Test loss: 0.3154 score: 0.8140 time: 0.24s
Epoch 87/1000, LR 0.000266
Train loss: 0.2422;  Loss pred: 0.2422; Loss self: 0.0000; time: 0.34s
Val loss: 0.3479 score: 0.9318 time: 0.20s
Test loss: 0.3064 score: 0.8372 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.2493;  Loss pred: 0.2493; Loss self: 0.0000; time: 0.38s
Val loss: 0.3398 score: 0.9091 time: 0.18s
Test loss: 0.2963 score: 0.8605 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.2195;  Loss pred: 0.2195; Loss self: 0.0000; time: 0.36s
Val loss: 0.3330 score: 0.8864 time: 0.20s
Test loss: 0.2910 score: 0.8605 time: 0.24s
Epoch 90/1000, LR 0.000266
Train loss: 0.1995;  Loss pred: 0.1995; Loss self: 0.0000; time: 0.41s
Val loss: 0.3269 score: 0.8864 time: 0.18s
Test loss: 0.2884 score: 0.8605 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.1741;  Loss pred: 0.1741; Loss self: 0.0000; time: 0.38s
Val loss: 0.3211 score: 0.8636 time: 0.19s
Test loss: 0.2879 score: 0.8605 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.1704;  Loss pred: 0.1704; Loss self: 0.0000; time: 0.36s
Val loss: 0.3161 score: 0.8409 time: 0.19s
Test loss: 0.2900 score: 0.8372 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 0.1616;  Loss pred: 0.1616; Loss self: 0.0000; time: 0.49s
Val loss: 0.3114 score: 0.8636 time: 0.19s
Test loss: 0.2925 score: 0.8372 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.1516;  Loss pred: 0.1516; Loss self: 0.0000; time: 0.36s
Val loss: 0.3068 score: 0.8636 time: 0.18s
Test loss: 0.2936 score: 0.8372 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.1631;  Loss pred: 0.1631; Loss self: 0.0000; time: 0.45s
Val loss: 0.3032 score: 0.8636 time: 0.18s
Test loss: 0.2970 score: 0.8372 time: 0.23s
Epoch 96/1000, LR 0.000265
Train loss: 0.1393;  Loss pred: 0.1393; Loss self: 0.0000; time: 0.36s
Val loss: 0.3002 score: 0.8636 time: 0.19s
Test loss: 0.3008 score: 0.8140 time: 0.22s
Epoch 97/1000, LR 0.000265
Train loss: 0.1180;  Loss pred: 0.1180; Loss self: 0.0000; time: 0.43s
Val loss: 0.2988 score: 0.8636 time: 0.19s
Test loss: 0.3097 score: 0.8140 time: 0.27s
Epoch 98/1000, LR 0.000265
Train loss: 0.1126;  Loss pred: 0.1126; Loss self: 0.0000; time: 0.37s
Val loss: 0.2973 score: 0.8636 time: 0.20s
Test loss: 0.3172 score: 0.7907 time: 0.22s
Epoch 99/1000, LR 0.000265
Train loss: 0.1259;  Loss pred: 0.1259; Loss self: 0.0000; time: 0.38s
Val loss: 0.2960 score: 0.8636 time: 0.19s
Test loss: 0.3239 score: 0.7907 time: 0.25s
Epoch 100/1000, LR 0.000265
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 0.33s
Val loss: 0.2946 score: 0.8636 time: 0.19s
Test loss: 0.3297 score: 0.7907 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.1040;  Loss pred: 0.1040; Loss self: 0.0000; time: 0.44s
Val loss: 0.2942 score: 0.8636 time: 0.18s
Test loss: 0.3382 score: 0.7907 time: 0.26s
Epoch 102/1000, LR 0.000264
Train loss: 0.0981;  Loss pred: 0.0981; Loss self: 0.0000; time: 0.31s
Val loss: 0.2975 score: 0.8636 time: 0.21s
Test loss: 0.3547 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 103/1000, LR 0.000264
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.37s
Val loss: 0.3027 score: 0.8636 time: 0.17s
Test loss: 0.3731 score: 0.8140 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 100,   Train_Loss: 0.1040,   Val_Loss: 0.2942,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.2942,   Test_Precision: 0.7500,   Test_Recall: 0.8571,   Test_accuracy: 0.8000,   Test_Score: 0.7907,   Test_loss: 0.3382


[0.31343801703769714, 0.21900986309628934, 0.20916565402876586, 0.20637578901369125, 0.22996490402147174, 0.21842012798879296, 0.19758289901074022, 0.2160945520736277, 0.19608385395258665, 0.2173821770120412, 0.2021875570062548, 0.20835206203628331, 0.20242177695035934, 0.21425539697520435, 0.21707938704639673, 0.2003914089873433, 0.21131563803646713, 0.2020946650300175, 0.23530041193589568, 0.21682829200290143, 0.22318276099395007, 0.20957842201460153, 0.22282688599079847, 0.19609311397653073, 0.21323801507242024, 0.2928057840326801, 0.19661808898672462, 0.21184711495880038, 0.20756907807663083, 0.22500853997189552, 0.20460736798122525, 0.24537184997461736, 0.1981513339560479, 0.20557183702476323, 0.21008739795070142, 0.22203900292515755, 0.19002743298187852, 0.2196032910142094, 0.20734036806970835, 0.20367468509357423, 0.21611798903904855, 0.19426919205579907, 0.22089988796506077, 0.19225731608457863, 0.22178562299814075, 0.20668049494270235, 0.22493666596710682, 0.1897855979623273, 0.21379003999754786, 0.2318186720367521, 0.19949150492902845, 0.21535559801850468, 0.21455700800288469, 0.21176709199789912, 0.20254071697127074, 0.20182467496488243, 0.21359056502114981, 0.22381970600690693, 0.20479991589672863, 0.20953143306542188, 0.20560403598938137, 0.23095413495320827, 0.18833136290777475, 0.21181881497614086, 0.21757157798856497, 0.19741593103390187, 0.22606484300922602, 0.18527392693795264, 0.21084476099349558, 0.20673649397213012, 0.19786283699795604, 0.22599645901937038, 0.20892532798461616, 0.22038996394257993, 0.19624560698866844, 0.20413724891841412, 0.2274248320609331, 0.19209695304743946, 0.21805279003456235, 0.19818204105831683, 0.20617981406394392, 0.20082866598386317, 0.2086984529159963, 0.22470212902408093, 0.19497841002885252, 0.3047871949383989, 0.19904929003678262, 0.22006059193518013, 0.24496546003501862, 0.22662632900755852, 0.2494894420960918, 0.2584986430592835, 0.33120067697018385, 0.23426949104759842, 0.2457081099273637, 0.22747645003255457, 0.2252422400051728, 0.24736044101882726, 0.23112890892662108, 0.27549094904679805, 0.23568698193412274, 0.2463709149742499, 0.23163624096196145, 0.2820925679989159, 0.23497412307187915, 0.22640122706070542, 0.25723941600881517, 0.23917729803361, 0.2494237630162388, 0.21857981197535992, 0.2416214500553906, 0.23393413401208818, 0.23543106892611831, 0.24616046505980194, 0.22418079792987555, 0.25142330199014395, 0.22162691806443036, 0.2512598449829966, 0.2507964549586177, 0.24191671900916845, 0.3182337189791724, 0.23685948201455176, 0.2463257770286873, 0.22966416203416884, 0.22492896090261638, 0.2526531049516052, 0.22237598593346775, 0.25604714290238917, 0.23355506907682866, 0.2526994820218533, 0.2356072779512033, 0.2218501130118966, 0.2192696649581194, 0.243965056957677, 0.26841302204411477, 0.24158577295020223, 0.22982752206735313, 0.22267603292129934, 0.24018891900777817, 0.2248735709581524, 0.24697826791089028, 0.2307522330665961, 0.23751620098482817, 0.21850501198787242, 0.2541113090701401, 0.2386977969435975, 0.2504229680635035, 0.23372043401468545, 0.24925469397567213, 0.23177187191322446, 0.2582559729926288, 0.23556277400348336, 0.2354893219890073, 0.24507697892840952, 0.21988903696183115, 0.2450402940157801, 0.23668301501311362, 0.24728563393000513, 0.23170904396101832, 0.2371025918982923, 0.2383027890464291, 0.23407028103247285, 0.24563781707547605, 0.2342852730071172, 0.237575936014764, 0.2461021140916273, 0.22796865494456142, 0.24025765201076865, 0.23794146196451038, 0.23602237296290696, 0.24255600303877145, 0.24602833203971386, 0.22595662903040648, 0.24923987896181643, 0.22474597208201885, 0.24173715896904469, 0.24127273599151522, 0.237385347019881, 0.24674448801670223, 0.22220624797046185, 0.2313208649866283, 0.22351593896746635, 0.23688090103678405, 0.22368715098127723, 0.2756225740304217, 0.22480508405715227, 0.2568397510331124, 0.23493229399900883, 0.26291286502964795, 0.23316847102250904, 0.2481873449869454]
[0.007123591296311299, 0.00497749688855203, 0.004753764864290133, 0.004799436953806773, 0.005348021023755156, 0.005079537860204487, 0.004594951139784656, 0.005025454699386691, 0.004560089626804341, 0.005055399465396307, 0.004702036209447786, 0.004845396791541472, 0.0047074831848920775, 0.004982683650586147, 0.005048357838288296, 0.004660265325287054, 0.004914317163638771, 0.004699875930930639, 0.005472102603160365, 0.005042518418672127, 0.005190296767301164, 0.004873916791037245, 0.005182020604437174, 0.004560304976198389, 0.004959023606335354, 0.0068094368379693045, 0.004572513697365689, 0.004926677092065125, 0.004827187862247229, 0.005232756743532454, 0.0047583108832843085, 0.005706322092432962, 0.004608170557117393, 0.0047807403959247265, 0.004885753440713987, 0.005163697742445524, 0.004419242627485547, 0.0051070532794002185, 0.0048218690248769385, 0.004736620583571494, 0.005025999745094153, 0.004517888187344165, 0.005137206696861878, 0.004471100374059968, 0.005157805186003273, 0.00480652313820238, 0.005231085255048996, 0.004413618557263426, 0.004971861395291811, 0.005391131907831444, 0.004639337323930895, 0.005008269721360574, 0.004989697860532202, 0.004924816092974398, 0.004710249231890017, 0.004693597092206569, 0.004967222442352321, 0.005205109442021091, 0.004762788741784387, 0.004872824024777253, 0.0047814892090553805, 0.005371026394260657, 0.004379799137390111, 0.004926018952933508, 0.0050598041392689525, 0.004591068163579113, 0.005257321930447117, 0.004308695975301224, 0.0049033665347324555, 0.004807825441212329, 0.004601461325533861, 0.005255731605101637, 0.004858728557781771, 0.00512534799866465, 0.004563851325317871, 0.004747377881823584, 0.0052889495828123976, 0.004467371001103243, 0.005070995117082845, 0.00460888467577481, 0.0047948793968359054, 0.004670434092647981, 0.004853452393395263, 0.0052256309075367656, 0.004534381628577965, 0.007088074300892998, 0.004629053256669363, 0.005117688184539073, 0.005696871163605084, 0.005270379744361826, 0.0058020800487463215, 0.0060115963502158955, 0.007702341324887996, 0.005448127698781359, 0.0057141420913340395, 0.005290150000757083, 0.005238191628027274, 0.005752568395786681, 0.005375090905270258, 0.00640676625690228, 0.0054810926031191335, 0.005729556162191858, 0.005386889324696778, 0.006560292279044556, 0.005464514490043701, 0.005265144815365243, 0.005982312000205004, 0.0055622627449676745, 0.005800552628284624, 0.00508325144128744, 0.005619103489660247, 0.005440328697955539, 0.005475141137816705, 0.005724661978134929, 0.005213506928601757, 0.00584705353465451, 0.005154114373591404, 0.005843252208906897, 0.005832475696712039, 0.005625970209515546, 0.007400784162306335, 0.005508360046850041, 0.005728506442527611, 0.005341027024050438, 0.005230906067502706, 0.005875653603525702, 0.005171534556592273, 0.0059545847186602135, 0.005431513234344852, 0.0058767321400431, 0.005479239022121007, 0.005159304953765037, 0.005099294533909753, 0.00567360597575993, 0.0062421633033515065, 0.0056182737895395865, 0.005344826094589607, 0.005178512393518589, 0.005585788814134376, 0.0052296179292593585, 0.005743680649090472, 0.005366331001548747, 0.005523632581042516, 0.0050815119066947075, 0.005909565327212561, 0.005551111556827849, 0.005823789954965197, 0.00543535893057408, 0.00579662079013191, 0.005390043532865685, 0.006005952860293693, 0.005478204046592636, 0.005476495860209472, 0.005699464626242082, 0.005113698533996073, 0.005698611488739072, 0.005504256163095665, 0.005750828696046631, 0.0053885824176981, 0.005514013765076565, 0.005541925326661142, 0.005443494907731927, 0.00571250737384828, 0.005448494721095749, 0.005525021767785209, 0.005723304978875053, 0.005301596626617708, 0.005587387256064388, 0.005533522371267683, 0.005488892394486208, 0.005640837279971429, 0.005721589117202648, 0.005254805326288523, 0.0057962762549259635, 0.005226650513535322, 0.005621794394628946, 0.005610993860267796, 0.005520589465578628, 0.005738243907365168, 0.005167587162103764, 0.00537955499968903, 0.00519804509226666, 0.005508858163646141, 0.005202026767006447, 0.006409827303033062, 0.005228025210631448, 0.005973017465886335, 0.005463541720907183, 0.0061142526751080915, 0.005422522581918815, 0.005771798720626637]
[140.37863184512213, 200.90419389310821, 210.35958415022012, 208.35777396905468, 186.98505401496007, 196.8683032829571, 217.6301705020666, 198.98696930290518, 219.29393539152622, 197.80830512898098, 212.6738194807397, 206.38144676730772, 212.42773701440757, 200.69506116093945, 198.0842151116335, 214.58005718556464, 203.48706986171752, 212.77157412152928, 182.74511143531168, 198.313603832772, 192.6672105340861, 205.17379407028912, 192.9749177654247, 219.28357976479697, 201.6525992581401, 146.85502249231786, 218.69808735097257, 202.97656641848798, 207.15995079057566, 191.10385768189454, 210.1586097522435, 175.24422628124685, 217.00585679396872, 209.17262122252774, 204.67672225675466, 193.65966984860748, 226.2831177859493, 195.80763021086827, 207.3884617854218, 211.12098432971442, 198.96539011489085, 221.3423525622596, 194.65831511332053, 223.65858878984486, 193.8809171609851, 208.05059525293285, 191.16492109066834, 226.57145990885758, 201.13191428605936, 185.4898038290154, 215.54802554273064, 199.66975734851886, 200.4129364043978, 203.05326759847367, 212.30299093934434, 213.0562083525318, 201.31975396826223, 192.11891913875112, 209.96102372269996, 205.21980578720203, 209.13986339363873, 186.1841530081801, 228.3209728644982, 203.00368503545587, 197.6361085282011, 217.81423502551925, 190.21091217728673, 232.08878178741523, 203.9415150624801, 207.99424027088693, 217.32226552702363, 190.26846786265102, 205.8151609227878, 195.10870291354627, 219.11318505327304, 210.64259574295264, 189.07346049387942, 223.84529956277288, 197.19995324611213, 216.97223305590472, 208.55581908064056, 214.11285978195514, 206.03890157876745, 191.3644529615995, 220.53723791078687, 141.0820425336137, 216.02689460512, 195.4007285987209, 175.53495090227418, 189.73964847026159, 172.35198266802163, 166.34516719740952, 129.83065250156795, 183.5492953338228, 175.0043985634486, 189.0305567624525, 190.90557791918818, 173.835395113672, 186.04336514932305, 156.08498264200878, 182.4453758418401, 174.53358893639802, 185.63589109124473, 152.4322328129015, 182.99887424985175, 189.92829923342387, 167.15945272759623, 179.78294910730824, 172.39736695497, 196.72448068921986, 177.96433218219016, 183.81242302065266, 182.64369352837633, 174.68280290075674, 191.80946984340082, 171.02631164109698, 194.01975344664243, 171.13757275027342, 171.45377914968995, 177.74711965389352, 135.12081666875233, 181.54223607293983, 174.565571328705, 187.22990831108675, 191.17146954952133, 170.19383161048623, 193.36620282760697, 167.93782392015422, 184.11075456407679, 170.1625965196137, 182.50709559534806, 193.82455756375543, 196.1055580041727, 176.25474949660364, 160.20087130099364, 177.99061374720745, 187.09682640792883, 193.10564965560323, 179.02574430841045, 191.21855812927893, 174.10438725529644, 186.3470590448848, 181.04028197531972, 196.7918246304877, 169.21718343566945, 180.14410082787893, 171.70948947899956, 183.98049011537503, 172.51430379961835, 185.52725852815837, 166.5014733317603, 182.54157594257293, 182.59851290415304, 175.455076147976, 195.55317806709954, 175.48134347745636, 181.677591007608, 173.8879825593559, 185.5775642802882, 181.35609423639414, 180.4427055682601, 183.70550849227442, 175.05447863016784, 183.53693105880254, 180.99476201717582, 174.72422030470852, 188.62242272060146, 178.97452855350758, 180.71671765391426, 182.1861184607183, 177.2786468332703, 174.77661878819285, 190.30200700247474, 172.52455818511933, 191.32712191303509, 177.87914850735177, 178.22154593344592, 181.14007684054212, 174.26934374756655, 193.5139105796703, 185.88898153431015, 192.3800163811084, 181.52582083146814, 192.23276710962773, 156.0104434524797, 191.27681289035303, 167.41956736461847, 183.03145671485, 163.5522856409138, 184.41601392946893, 173.25621498655298]
Elapsed: 0.22831700160655716~0.023406943474796382
Time per graph: 0.005307645495410113~0.0005421547926931499
Speed: 190.2392643014914~18.185824228861634
Total Time: 0.2488
best val loss: 0.29420414566993713 test_score: 0.7907

Testing...
Test loss: 0.3429 score: 0.8372 time: 0.28s
test Score 0.8372
Epoch Time List: [0.8741116289747879, 0.8869374430505559, 0.7683747530682012, 0.7585825490532443, 0.8588974250014871, 0.8703764589736238, 0.8092612040927634, 0.8113696557702497, 0.7981063859770074, 0.7644046199275181, 0.7396953308489174, 0.8262916340027004, 0.7550656460225582, 0.7517965000588447, 0.7762144809821621, 0.7663062089122832, 0.8204934949753806, 0.8868632070953026, 1.0104619240155444, 0.7734723821049556, 0.8159620328806341, 0.7556495568715036, 0.8554468700895086, 0.72365092986729, 0.7988321629818529, 0.8385764700360596, 0.7474032749887556, 0.8008176198927686, 0.7423410979099572, 0.8189776679500937, 0.7354069059947506, 0.9353537309216335, 0.751455779070966, 0.8692385391332209, 0.7695001279935241, 0.9973576498450711, 0.753335818182677, 0.9096006110776216, 0.7616706960834563, 0.8880848749540746, 0.7663890639087185, 0.8280335620511323, 0.7706028299871832, 0.809731048066169, 0.7689229829702526, 0.7911868758965284, 0.8205758959520608, 0.7529718840960413, 0.7969924901844934, 0.8324863029411063, 0.784584827022627, 0.9010841400595382, 0.9222825950710103, 0.8289881729288027, 0.7720697259064764, 0.8173036470543593, 0.7438863950083032, 0.8025109550217167, 0.7329735619714484, 0.8411505869589746, 0.879109955043532, 0.9245308580575511, 0.7399806160246953, 0.8312220741063356, 0.7657927260734141, 0.7748841689899564, 0.7866890330333263, 0.721943341079168, 0.7775950520299375, 0.8941296000266448, 0.7677873080829158, 0.7814401889918372, 0.7432090119691566, 0.7695906319422647, 0.7475992600666359, 0.7259408219251782, 0.8405201440909877, 0.7913840579567477, 0.7830475369701162, 0.7497388040646911, 0.7731010989518836, 0.7646437609801069, 0.7431044819531962, 0.9315205580787733, 0.7514330609701574, 0.9169313819147646, 0.785949517856352, 0.8413740170653909, 0.7748013698728755, 0.7535879709757864, 0.8127943730214611, 0.7644169391132891, 0.8864942018408328, 0.7403667129110545, 0.7842476810328662, 0.7283752619987354, 0.7782644771505147, 0.7666558771161363, 0.7488104229560122, 0.9507364950841293, 0.841260752058588, 0.8048390338663012, 0.745381894055754, 0.9208186749601737, 0.7655688669765368, 0.7787522121798247, 0.7793544731102884, 0.756172999041155, 0.8655199269996956, 0.7425464029656723, 0.8287175570148975, 0.7637456480879337, 0.8582769880304113, 0.7897559559205547, 0.8061263029230759, 0.7860590380150825, 0.8077548871515319, 0.7898391319904476, 0.7498438369948417, 0.7787170910742134, 0.8518550979206339, 0.8711169869638979, 0.7630414790473878, 0.7942620059475303, 1.0539469950599596, 0.799012009985745, 0.7342785050859675, 0.8429250499466434, 0.7629315980011597, 0.8745406270027161, 0.7577272129710764, 0.7899755670223385, 0.9310915949754417, 0.7998201170703396, 0.8008993931580335, 0.784343380946666, 0.7590980450622737, 0.7609981109853834, 0.8460253340890631, 0.7648562631802633, 0.8840757280122489, 0.8514877618290484, 0.8187424149364233, 0.7635900889290497, 0.85523693903815, 0.7566017999779433, 0.7902481880737469, 0.7522294010268524, 0.9587917820317671, 0.731344903120771, 0.90333610214293, 0.7563494719797745, 0.84768323705066, 0.8650115380296484, 0.7820804051589221, 0.7623311599018052, 0.8297312690410763, 0.784915731055662, 0.7573041160358116, 0.8699102288810536, 0.7728676521219313, 0.814387014019303, 0.8095833699917421, 0.7779287089360878, 0.924737689900212, 0.8118612259859219, 0.8118582678725943, 0.7920795649988577, 0.7573019019328058, 0.9156780469929799, 0.7907839129911736, 0.8768874640809372, 0.9086813669418916, 0.792714579962194, 0.7634231620468199, 0.8026762430090457, 0.7913337129866704, 0.8221207919996232, 0.8076416541589424, 0.7714822459965944, 0.9090612321160734, 0.7601304551353678, 0.8658053450053558, 0.7743786729406565, 0.8893522641155869, 0.7833589499350637, 0.826299453037791, 0.7507362610194832, 0.8816901880782098, 0.7447705400409177, 0.7847919061314315]
Total Epoch List: [3, 85, 103]
Total Time List: [0.20943512301892042, 0.22050783096347004, 0.24881413497496396]
========================training times:8========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bced70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.29s
Epoch 4/1000, LR 0.000060
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.22s
Epoch 5/1000, LR 0.000090
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4884 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.18s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.21s
Epoch 11/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.20s
Epoch 13/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.21s
Epoch 14/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.21s
Epoch 15/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.20s
Epoch 18/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.22s
Epoch 20/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.20s
Epoch 21/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.30s
Epoch 22/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.19s
Epoch 23/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.22s
Epoch 24/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.19s
Epoch 25/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.21s
Epoch 28/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.21s
Epoch 29/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.19s
Epoch 30/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.4884 time: 0.25s
Test loss: 0.6903 score: 0.5227 time: 0.20s
Epoch 32/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4884 time: 0.25s
Test loss: 0.6899 score: 0.5455 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.4884 time: 0.23s
Test loss: 0.6894 score: 0.5455 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.26s
Val loss: 0.6892 score: 0.5349 time: 0.26s
Test loss: 0.6889 score: 0.5682 time: 0.21s
Epoch 35/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.34s
Val loss: 0.6885 score: 0.5581 time: 0.24s
Test loss: 0.6882 score: 0.5909 time: 0.22s
Epoch 36/1000, LR 0.000270
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.29s
Val loss: 0.6877 score: 0.5814 time: 0.35s
Test loss: 0.6875 score: 0.5909 time: 0.20s
Epoch 37/1000, LR 0.000270
Train loss: 0.6856;  Loss pred: 0.6856; Loss self: 0.0000; time: 0.33s
Val loss: 0.6869 score: 0.6047 time: 0.24s
Test loss: 0.6867 score: 0.6136 time: 0.22s
Epoch 38/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.28s
Val loss: 0.6858 score: 0.6047 time: 0.26s
Test loss: 0.6858 score: 0.6136 time: 0.20s
Epoch 39/1000, LR 0.000269
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.40s
Val loss: 0.6846 score: 0.6047 time: 0.24s
Test loss: 0.6847 score: 0.6364 time: 0.22s
Epoch 40/1000, LR 0.000269
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.29s
Val loss: 0.6832 score: 0.6279 time: 0.24s
Test loss: 0.6834 score: 0.6591 time: 0.20s
Epoch 41/1000, LR 0.000269
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.26s
Val loss: 0.6817 score: 0.6279 time: 0.28s
Test loss: 0.6820 score: 0.6818 time: 0.22s
Epoch 42/1000, LR 0.000269
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.35s
Val loss: 0.6800 score: 0.6512 time: 0.25s
Test loss: 0.6805 score: 0.6818 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.27s
Val loss: 0.6782 score: 0.6512 time: 0.25s
Test loss: 0.6788 score: 0.6818 time: 0.22s
Epoch 44/1000, LR 0.000269
Train loss: 0.6756;  Loss pred: 0.6756; Loss self: 0.0000; time: 0.26s
Val loss: 0.6762 score: 0.6512 time: 0.25s
Test loss: 0.6770 score: 0.7045 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.32s
Val loss: 0.6740 score: 0.6977 time: 0.25s
Test loss: 0.6750 score: 0.7045 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6706;  Loss pred: 0.6706; Loss self: 0.0000; time: 0.28s
Val loss: 0.6716 score: 0.6977 time: 0.26s
Test loss: 0.6727 score: 0.7045 time: 0.20s
Epoch 47/1000, LR 0.000269
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.40s
Val loss: 0.6689 score: 0.6977 time: 0.33s
Test loss: 0.6703 score: 0.7045 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.26s
Val loss: 0.6659 score: 0.7209 time: 0.26s
Test loss: 0.6675 score: 0.7045 time: 0.20s
Epoch 49/1000, LR 0.000269
Train loss: 0.6620;  Loss pred: 0.6620; Loss self: 0.0000; time: 0.39s
Val loss: 0.6627 score: 0.7442 time: 0.24s
Test loss: 0.6645 score: 0.7045 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.29s
Val loss: 0.6591 score: 0.7674 time: 0.25s
Test loss: 0.6612 score: 0.7500 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6542;  Loss pred: 0.6542; Loss self: 0.0000; time: 0.30s
Val loss: 0.6553 score: 0.8140 time: 0.26s
Test loss: 0.6577 score: 0.7500 time: 0.20s
Epoch 52/1000, LR 0.000269
Train loss: 0.6508;  Loss pred: 0.6508; Loss self: 0.0000; time: 0.32s
Val loss: 0.6512 score: 0.8140 time: 0.24s
Test loss: 0.6538 score: 0.7500 time: 0.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6464;  Loss pred: 0.6464; Loss self: 0.0000; time: 0.27s
Val loss: 0.6467 score: 0.8605 time: 0.26s
Test loss: 0.6496 score: 0.7500 time: 0.19s
Epoch 54/1000, LR 0.000269
Train loss: 0.6410;  Loss pred: 0.6410; Loss self: 0.0000; time: 0.36s
Val loss: 0.6419 score: 0.8605 time: 0.24s
Test loss: 0.6451 score: 0.7727 time: 0.20s
Epoch 55/1000, LR 0.000269
Train loss: 0.6336;  Loss pred: 0.6336; Loss self: 0.0000; time: 0.29s
Val loss: 0.6367 score: 0.8605 time: 0.24s
Test loss: 0.6403 score: 0.7727 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6298;  Loss pred: 0.6298; Loss self: 0.0000; time: 0.38s
Val loss: 0.6313 score: 0.8605 time: 0.24s
Test loss: 0.6352 score: 0.7955 time: 0.19s
Epoch 57/1000, LR 0.000269
Train loss: 0.6226;  Loss pred: 0.6226; Loss self: 0.0000; time: 0.33s
Val loss: 0.6255 score: 0.8837 time: 0.24s
Test loss: 0.6298 score: 0.7955 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.26s
Val loss: 0.6194 score: 0.8837 time: 0.32s
Test loss: 0.6241 score: 0.7955 time: 0.19s
Epoch 59/1000, LR 0.000268
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.32s
Val loss: 0.6129 score: 0.8837 time: 0.23s
Test loss: 0.6180 score: 0.7955 time: 0.22s
Epoch 60/1000, LR 0.000268
Train loss: 0.6041;  Loss pred: 0.6041; Loss self: 0.0000; time: 0.29s
Val loss: 0.6061 score: 0.8837 time: 0.25s
Test loss: 0.6116 score: 0.7727 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.5954;  Loss pred: 0.5954; Loss self: 0.0000; time: 0.32s
Val loss: 0.5989 score: 0.9070 time: 0.24s
Test loss: 0.6048 score: 0.7727 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.5900;  Loss pred: 0.5900; Loss self: 0.0000; time: 0.35s
Val loss: 0.5915 score: 0.8837 time: 0.24s
Test loss: 0.5977 score: 0.8409 time: 0.22s
Epoch 63/1000, LR 0.000268
Train loss: 0.5789;  Loss pred: 0.5789; Loss self: 0.0000; time: 0.29s
Val loss: 0.5837 score: 0.8837 time: 0.25s
Test loss: 0.5903 score: 0.8409 time: 0.19s
Epoch 64/1000, LR 0.000268
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 0.34s
Val loss: 0.5757 score: 0.8837 time: 0.31s
Test loss: 0.5825 score: 0.8636 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.5586;  Loss pred: 0.5586; Loss self: 0.0000; time: 0.27s
Val loss: 0.5672 score: 0.8837 time: 0.27s
Test loss: 0.5744 score: 0.8864 time: 0.19s
Epoch 66/1000, LR 0.000268
Train loss: 0.5526;  Loss pred: 0.5526; Loss self: 0.0000; time: 0.31s
Val loss: 0.5582 score: 0.8837 time: 0.23s
Test loss: 0.5659 score: 0.8864 time: 0.22s
Epoch 67/1000, LR 0.000268
Train loss: 0.5332;  Loss pred: 0.5332; Loss self: 0.0000; time: 0.28s
Val loss: 0.5488 score: 0.8837 time: 0.26s
Test loss: 0.5572 score: 0.9091 time: 0.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.5339;  Loss pred: 0.5339; Loss self: 0.0000; time: 0.36s
Val loss: 0.5392 score: 0.8837 time: 0.24s
Test loss: 0.5481 score: 0.9091 time: 0.21s
Epoch 69/1000, LR 0.000268
Train loss: 0.5167;  Loss pred: 0.5167; Loss self: 0.0000; time: 0.34s
Val loss: 0.5291 score: 0.8837 time: 0.25s
Test loss: 0.5385 score: 0.9091 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.5103;  Loss pred: 0.5103; Loss self: 0.0000; time: 0.31s
Val loss: 0.5189 score: 0.8837 time: 0.25s
Test loss: 0.5286 score: 0.9091 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.4943;  Loss pred: 0.4943; Loss self: 0.0000; time: 0.32s
Val loss: 0.5084 score: 0.8605 time: 0.23s
Test loss: 0.5184 score: 0.8636 time: 0.29s
Epoch 72/1000, LR 0.000267
Train loss: 0.4840;  Loss pred: 0.4840; Loss self: 0.0000; time: 0.31s
Val loss: 0.4977 score: 0.8605 time: 0.26s
Test loss: 0.5079 score: 0.8636 time: 0.20s
Epoch 73/1000, LR 0.000267
Train loss: 0.4805;  Loss pred: 0.4805; Loss self: 0.0000; time: 0.32s
Val loss: 0.4869 score: 0.8605 time: 0.23s
Test loss: 0.4973 score: 0.8636 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.4714;  Loss pred: 0.4714; Loss self: 0.0000; time: 0.27s
Val loss: 0.4763 score: 0.8605 time: 0.26s
Test loss: 0.4865 score: 0.8636 time: 0.20s
Epoch 75/1000, LR 0.000267
Train loss: 0.4385;  Loss pred: 0.4385; Loss self: 0.0000; time: 0.34s
Val loss: 0.4659 score: 0.8605 time: 0.25s
Test loss: 0.4758 score: 0.8636 time: 0.21s
Epoch 76/1000, LR 0.000267
Train loss: 0.4392;  Loss pred: 0.4392; Loss self: 0.0000; time: 0.48s
Val loss: 0.4551 score: 0.8372 time: 0.25s
Test loss: 0.4650 score: 0.8636 time: 0.22s
Epoch 77/1000, LR 0.000267
Train loss: 0.4037;  Loss pred: 0.4037; Loss self: 0.0000; time: 0.36s
Val loss: 0.4443 score: 0.8372 time: 0.26s
Test loss: 0.4541 score: 0.8636 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.4281;  Loss pred: 0.4281; Loss self: 0.0000; time: 0.26s
Val loss: 0.4336 score: 0.8372 time: 0.26s
Test loss: 0.4432 score: 0.8636 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.3835;  Loss pred: 0.3835; Loss self: 0.0000; time: 0.51s
Val loss: 0.4231 score: 0.8605 time: 0.25s
Test loss: 0.4324 score: 0.8636 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.3626;  Loss pred: 0.3626; Loss self: 0.0000; time: 0.34s
Val loss: 0.4126 score: 0.8837 time: 0.25s
Test loss: 0.4216 score: 0.8864 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.3533;  Loss pred: 0.3533; Loss self: 0.0000; time: 0.32s
Val loss: 0.4025 score: 0.8837 time: 0.24s
Test loss: 0.4110 score: 0.8864 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.3496;  Loss pred: 0.3496; Loss self: 0.0000; time: 0.27s
Val loss: 0.3920 score: 0.8837 time: 0.26s
Test loss: 0.3999 score: 0.8864 time: 0.21s
Epoch 83/1000, LR 0.000266
Train loss: 0.3428;  Loss pred: 0.3428; Loss self: 0.0000; time: 0.33s
Val loss: 0.3815 score: 0.8605 time: 0.25s
Test loss: 0.3885 score: 0.8864 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.3443;  Loss pred: 0.3443; Loss self: 0.0000; time: 0.34s
Val loss: 0.3723 score: 0.8605 time: 0.26s
Test loss: 0.3783 score: 0.8864 time: 0.20s
Epoch 85/1000, LR 0.000266
Train loss: 0.2981;  Loss pred: 0.2981; Loss self: 0.0000; time: 0.34s
Val loss: 0.3636 score: 0.8605 time: 0.24s
Test loss: 0.3685 score: 0.8864 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.2855;  Loss pred: 0.2855; Loss self: 0.0000; time: 0.31s
Val loss: 0.3564 score: 0.8837 time: 0.23s
Test loss: 0.3605 score: 0.8864 time: 0.28s
Epoch 87/1000, LR 0.000266
Train loss: 0.2874;  Loss pred: 0.2874; Loss self: 0.0000; time: 0.32s
Val loss: 0.3507 score: 0.8837 time: 0.25s
Test loss: 0.3542 score: 0.8864 time: 0.20s
Epoch 88/1000, LR 0.000266
Train loss: 0.2709;  Loss pred: 0.2709; Loss self: 0.0000; time: 0.33s
Val loss: 0.3462 score: 0.8605 time: 0.24s
Test loss: 0.3497 score: 0.9091 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.2469;  Loss pred: 0.2469; Loss self: 0.0000; time: 0.27s
Val loss: 0.3425 score: 0.8605 time: 0.28s
Test loss: 0.3461 score: 0.9091 time: 0.21s
Epoch 90/1000, LR 0.000266
Train loss: 0.2563;  Loss pred: 0.2563; Loss self: 0.0000; time: 0.34s
Val loss: 0.3397 score: 0.8605 time: 0.24s
Test loss: 0.3439 score: 0.8864 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.2633;  Loss pred: 0.2633; Loss self: 0.0000; time: 0.28s
Val loss: 0.3375 score: 0.8605 time: 0.26s
Test loss: 0.3426 score: 0.8636 time: 0.19s
Epoch 92/1000, LR 0.000266
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.32s
Val loss: 0.3303 score: 0.8605 time: 0.25s
Test loss: 0.3335 score: 0.8636 time: 0.21s
Epoch 93/1000, LR 0.000265
Train loss: 0.2410;  Loss pred: 0.2410; Loss self: 0.0000; time: 0.32s
Val loss: 0.3218 score: 0.8605 time: 0.25s
Test loss: 0.3221 score: 0.8636 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.1994;  Loss pred: 0.1994; Loss self: 0.0000; time: 0.26s
Val loss: 0.3128 score: 0.8605 time: 0.26s
Test loss: 0.3090 score: 0.8864 time: 0.20s
Epoch 95/1000, LR 0.000265
Train loss: 0.2020;  Loss pred: 0.2020; Loss self: 0.0000; time: 0.35s
Val loss: 0.3057 score: 0.8605 time: 0.24s
Test loss: 0.2975 score: 0.8864 time: 0.22s
Epoch 96/1000, LR 0.000265
Train loss: 0.2093;  Loss pred: 0.2093; Loss self: 0.0000; time: 0.27s
Val loss: 0.3013 score: 0.8837 time: 0.26s
Test loss: 0.2889 score: 0.9091 time: 0.19s
Epoch 97/1000, LR 0.000265
Train loss: 0.1871;  Loss pred: 0.1871; Loss self: 0.0000; time: 0.41s
Val loss: 0.2986 score: 0.9070 time: 0.23s
Test loss: 0.2836 score: 0.9091 time: 0.22s
Epoch 98/1000, LR 0.000265
Train loss: 0.2091;  Loss pred: 0.2091; Loss self: 0.0000; time: 0.31s
Val loss: 0.2967 score: 0.8837 time: 0.35s
Test loss: 0.2826 score: 0.8864 time: 0.21s
Epoch 99/1000, LR 0.000265
Train loss: 0.1826;  Loss pred: 0.1826; Loss self: 0.0000; time: 0.41s
Val loss: 0.2968 score: 0.8605 time: 0.24s
Test loss: 0.2868 score: 0.8636 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 100/1000, LR 0.000265
Train loss: 0.1732;  Loss pred: 0.1732; Loss self: 0.0000; time: 0.27s
Val loss: 0.2995 score: 0.8605 time: 0.30s
Test loss: 0.2956 score: 0.8409 time: 0.20s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.2091,   Val_Loss: 0.2967,   Val_Precision: 0.9048,   Val_Recall: 0.8636,   Val_accuracy: 0.8837,   Val_Score: 0.8837,   Val_Loss: 0.2967,   Test_Precision: 0.8696,   Test_Recall: 0.9091,   Test_accuracy: 0.8889,   Test_Score: 0.8864,   Test_loss: 0.2826


[0.24327209196053445, 0.23320096998941153, 0.296690039918758, 0.228139515966177, 0.2530617479933426, 0.22838954802136868, 0.1859746469417587, 0.2172384220175445, 0.22729405900463462, 0.21335777605418116, 0.2331797289662063, 0.20641815091948956, 0.2132164330687374, 0.21037336601875722, 0.20585888298228383, 0.21806486300192773, 0.2061968829948455, 0.21492108900565654, 0.22736013098619878, 0.2011941479286179, 0.30193882109597325, 0.19186742207966745, 0.21986332908272743, 0.19071811495814472, 0.23551007197238505, 0.2233777689980343, 0.2165787270059809, 0.21633886196650565, 0.19416841794736683, 0.22084778500720859, 0.20858443202450871, 0.19973850203678012, 0.2323217139346525, 0.2192384839290753, 0.22282575606368482, 0.20707419305108488, 0.22470831300597638, 0.20543932099826634, 0.2205141690792516, 0.2038702229037881, 0.21940565400291234, 0.22040204191580415, 0.2248202640330419, 0.2222965060500428, 0.22123710508458316, 0.20216087403241545, 0.23016495897900313, 0.19938346510753036, 0.25397641700692475, 0.22699258499778807, 0.2029996080091223, 0.2289539979537949, 0.1908437650417909, 0.20629018999170512, 0.22786112502217293, 0.19909618108067662, 0.22378380096051842, 0.19663566001690924, 0.22071232309099287, 0.2013079810421914, 0.21199327905196697, 0.22471527103334665, 0.1968772099353373, 0.2250322480686009, 0.19754181511234492, 0.22718935506418347, 0.21372671297285706, 0.21066060801967978, 0.22084671899210662, 0.20651560998521745, 0.29472299199551344, 0.20129545696545392, 0.22893680399283767, 0.2081552279414609, 0.21664001001045108, 0.22233344905544072, 0.23121237405575812, 0.22064029495231807, 0.2161357889417559, 0.22867278300691396, 0.23556484503205866, 0.21244486793875694, 0.2292961140628904, 0.2011207629693672, 0.21425197599455714, 0.28819568001199514, 0.2012828839942813, 0.2334110450465232, 0.21518257993739098, 0.23107246996369213, 0.192682427004911, 0.21822408097796142, 0.23016305698547512, 0.2080653989687562, 0.2275662620086223, 0.19410930201411247, 0.2256793100386858, 0.2096537830075249, 0.21540133201051503, 0.19936614308971912]
[0.005528911180921237, 0.005300022045213898, 0.006742955452699045, 0.0051849889992312955, 0.0057514033634850584, 0.0051906715459401976, 0.004226696521403606, 0.004937236864035102, 0.005165774068287151, 0.004849040364867754, 0.005299539294686507, 0.004691321611806581, 0.004845828024289486, 0.004781212864062664, 0.004678610976870087, 0.004956019613680176, 0.004686292795337398, 0.0048845702046740125, 0.005167275704231791, 0.004572594271104952, 0.006862245933999392, 0.004360623229083351, 0.00499689384278926, 0.004334502612685107, 0.005352501635736024, 0.0050767674772280525, 0.004922243795590475, 0.0049167923174205826, 0.004412918589712883, 0.005019267841072922, 0.004740555273284289, 0.004539511409926821, 0.005280038953060284, 0.004982692816569893, 0.005064221728720109, 0.004706231660251929, 0.005107007113772191, 0.004669075477233326, 0.005011685660892082, 0.004633414156904275, 0.004986492136429826, 0.005009137316268276, 0.005109551455296407, 0.005052193319319154, 0.005028116024649617, 0.004594565318918533, 0.005231021794977344, 0.004531442388807508, 0.005772191295611926, 0.005158922386313366, 0.00461362745475278, 0.0052034999534953386, 0.004337358296404339, 0.004688413408902389, 0.005178661932322112, 0.004524913206379014, 0.0050859954763754186, 0.004468992273111574, 0.005016189161158929, 0.004575181387322532, 0.004818029069362886, 0.005107165250757878, 0.004474482043984939, 0.005114369274286384, 0.004489586707098748, 0.005163394433276897, 0.004857425294837661, 0.004787741091356359, 0.005019243613456969, 0.004693536590573124, 0.006698249818079851, 0.004574896749214862, 0.005203109181655402, 0.0047308006350332025, 0.0049236365911466155, 0.005053032933078198, 0.005254826683085412, 0.005014552158007229, 0.004912177021403543, 0.00519710870470259, 0.005353746478001333, 0.004828292453153567, 0.005211275319611145, 0.004570926431121982, 0.0048693630907853894, 0.006549901818454435, 0.0045746109998700294, 0.005304796478330073, 0.00489051318039525, 0.005251647044629367, 0.004379146068293432, 0.004959638204044578, 0.005230978567851707, 0.004728759067471732, 0.005171960500195961, 0.004411575045775284, 0.00512907522815195, 0.004764858704716476, 0.004895484818420796, 0.0045310487065845255]
[180.8674379597066, 188.67846047980768, 148.30292251148177, 192.86444004958463, 173.87060805870007, 192.65329951037538, 236.59138879171735, 202.54243973677222, 193.58183048287603, 206.2263715611022, 188.6956477523684, 213.15954921600652, 206.3630807753694, 209.15195127921703, 213.73865126717232, 201.77482696793308, 213.3882887119099, 204.72630305182363, 193.5255746429478, 218.69423366931554, 145.724885062111, 229.32501788516376, 200.12432352211053, 230.70697825246597, 186.82852767825258, 196.9757339656624, 203.15938046299866, 203.38463279340093, 226.607398181135, 199.2322449535268, 210.94575262850867, 220.28802434844425, 189.3925421554712, 200.6946919694729, 197.46370786429446, 212.48422776248736, 195.8094002460415, 214.17516270106495, 199.5336634544633, 215.82357331685853, 200.54177819599832, 199.6351740552769, 195.71189540785036, 197.93383522678866, 198.88164773796856, 217.64844562823183, 191.1672402053777, 220.68028548039413, 173.2444315835841, 193.83893090793586, 216.74918701332047, 192.17834321844697, 230.55508253237875, 213.2917711781119, 193.1000735457547, 220.99871409472476, 196.61834239629707, 223.76409241444975, 199.35452349826502, 218.57056919555615, 207.55374980172869, 195.80333725281454, 223.4895548065286, 195.5275316210974, 222.73765164593914, 193.67104584442137, 205.87038179728114, 208.86676637660491, 199.23320663673806, 213.0589547354292, 149.29272976663393, 218.58416808458438, 192.19277648943046, 211.38071061263008, 203.1019108514506, 197.90094646994942, 190.3012335723397, 199.41960288581336, 203.57572531339125, 192.41467839515317, 186.78508668817676, 207.11255784575698, 191.89160784439574, 218.7740308378885, 205.36566720447786, 152.67404424024903, 218.59782176635593, 188.50864572938258, 204.47751863929756, 190.4164524961092, 228.3550227384179, 201.62761049475372, 191.1688199500092, 211.47197091913966, 193.3502778998623, 226.67641140042343, 194.96692006217827, 209.86981188133748, 204.26986030825518, 220.69945938713897]
Elapsed: 0.21924829754512756~0.020550934652162487
Time per graph: 0.004982915853298354~0.0004670666966400565
Speed: 202.23577800490398~16.763441202612675
Total Time: 0.1998
best val loss: 0.2966827154159546 test_score: 0.8864

Testing...
Test loss: 0.6048 score: 0.7727 time: 0.21s
test Score 0.7727
Epoch Time List: [0.7732664779759943, 0.7813887620577589, 0.8288674730574712, 0.7950316090136766, 0.7965285470709205, 0.8897204200038686, 0.7090583479730412, 0.7811067568836734, 0.7758594719925895, 0.7341011859243736, 0.8235915959812701, 0.737911434029229, 0.8137491289526224, 0.7548164819600061, 0.8023644528584555, 0.7700347149511799, 0.7656643980881199, 0.8406777619384229, 0.7675775809912011, 0.8128744401037693, 0.8440270129358396, 0.7188111690338701, 0.7738049409817904, 0.7840968408854678, 0.785351003985852, 0.7677554361289367, 0.736225773114711, 0.7842672599945217, 0.8653467500116676, 0.7812117339344695, 0.7350670748855919, 0.7577877099392936, 0.7985238520195708, 0.7334894760278985, 0.803243687027134, 0.8349421621533111, 0.7916383527917787, 0.737575912149623, 0.851187521009706, 0.7242460489505902, 0.7560415827902034, 0.8183124138740823, 0.7380108040524647, 0.7303960169665515, 0.7931049490580335, 0.7387767949840054, 0.9567796410992742, 0.7186160059645772, 0.8875752170570195, 0.7623694220092148, 0.7596321690361947, 0.7851218339055777, 0.7118690209463239, 0.7969890109961852, 0.7611447629751638, 0.816726791090332, 0.7826309739612043, 0.7794539500027895, 0.7711630179546773, 0.7414236011682078, 0.7703469479456544, 0.8183632269501686, 0.7269962569698691, 0.8690970919560641, 0.7338661990361288, 0.7606935399817303, 0.7453950489871204, 0.7991215400397778, 0.805327538982965, 0.762273985077627, 0.8432232819031924, 0.7705078560393304, 0.7762300530448556, 0.7308651158818975, 0.8054208350367844, 0.9440757369156927, 0.8434858899563551, 0.738293560105376, 0.9748748468700796, 0.8227267819456756, 0.792995274066925, 0.7344594849273562, 0.8107915088767186, 0.8026043430436403, 0.7909117179224268, 0.8250153720146045, 0.7692621680907905, 0.7962496160762385, 0.759355018963106, 0.8079231270821765, 0.7304338549729437, 0.7853603980038315, 0.7962508279597387, 0.7190112030366436, 0.8057455769740045, 0.72474109090399, 0.8615275900810957, 0.8660373939201236, 0.8607163380365819, 0.7651457510655746]
Total Epoch List: [100]
Total Time List: [0.19983986602164805]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcc0a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.22s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.4884 time: 0.27s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.20s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.18s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.19s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.22s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.18s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.21s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.20s
Epoch 15/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.20s
Epoch 17/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.20s
Epoch 21/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.30s
Epoch 23/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.21s
Epoch 26/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.19s
Epoch 28/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.19s
Epoch 29/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.20s
Epoch 30/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.21s
Epoch 31/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.19s
Epoch 33/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.30s
Val loss: 0.6875 score: 0.5227 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.21s
Epoch 35/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.37s
Val loss: 0.6867 score: 0.5682 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.21s
Epoch 36/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.32s
Val loss: 0.6857 score: 0.5909 time: 0.23s
Test loss: 0.6885 score: 0.5116 time: 0.20s
Epoch 37/1000, LR 0.000270
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.39s
Val loss: 0.6846 score: 0.6136 time: 0.20s
Test loss: 0.6877 score: 0.5116 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.34s
Val loss: 0.6834 score: 0.6136 time: 0.22s
Test loss: 0.6869 score: 0.5581 time: 0.20s
Epoch 39/1000, LR 0.000269
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.29s
Val loss: 0.6821 score: 0.6364 time: 0.23s
Test loss: 0.6859 score: 0.5581 time: 0.28s
Epoch 40/1000, LR 0.000269
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.35s
Val loss: 0.6806 score: 0.6591 time: 0.20s
Test loss: 0.6848 score: 0.5581 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 0.30s
Val loss: 0.6789 score: 0.7045 time: 0.22s
Test loss: 0.6837 score: 0.5581 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6717;  Loss pred: 0.6717; Loss self: 0.0000; time: 0.42s
Val loss: 0.6771 score: 0.7273 time: 0.20s
Test loss: 0.6824 score: 0.5581 time: 0.21s
Epoch 43/1000, LR 0.000269
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.33s
Val loss: 0.6751 score: 0.7500 time: 0.22s
Test loss: 0.6811 score: 0.5814 time: 0.20s
Epoch 44/1000, LR 0.000269
Train loss: 0.6666;  Loss pred: 0.6666; Loss self: 0.0000; time: 0.34s
Val loss: 0.6728 score: 0.7500 time: 0.21s
Test loss: 0.6797 score: 0.6279 time: 0.19s
Epoch 45/1000, LR 0.000269
Train loss: 0.6638;  Loss pred: 0.6638; Loss self: 0.0000; time: 0.37s
Val loss: 0.6704 score: 0.7727 time: 0.21s
Test loss: 0.6780 score: 0.6512 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6613;  Loss pred: 0.6613; Loss self: 0.0000; time: 0.30s
Val loss: 0.6678 score: 0.7727 time: 0.22s
Test loss: 0.6763 score: 0.6512 time: 0.18s
Epoch 47/1000, LR 0.000269
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.44s
Val loss: 0.6649 score: 0.7727 time: 0.25s
Test loss: 0.6743 score: 0.6512 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6528;  Loss pred: 0.6528; Loss self: 0.0000; time: 0.31s
Val loss: 0.6619 score: 0.7727 time: 0.34s
Test loss: 0.6722 score: 0.6512 time: 0.20s
Epoch 49/1000, LR 0.000269
Train loss: 0.6478;  Loss pred: 0.6478; Loss self: 0.0000; time: 0.39s
Val loss: 0.6586 score: 0.7727 time: 0.21s
Test loss: 0.6699 score: 0.6512 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6429;  Loss pred: 0.6429; Loss self: 0.0000; time: 0.34s
Val loss: 0.6551 score: 0.7727 time: 0.24s
Test loss: 0.6674 score: 0.6977 time: 0.19s
Epoch 51/1000, LR 0.000269
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.49s
Val loss: 0.6512 score: 0.7727 time: 0.25s
Test loss: 0.6647 score: 0.7209 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6351;  Loss pred: 0.6351; Loss self: 0.0000; time: 0.34s
Val loss: 0.6472 score: 0.7727 time: 0.23s
Test loss: 0.6619 score: 0.7209 time: 0.20s
Epoch 53/1000, LR 0.000269
Train loss: 0.6267;  Loss pred: 0.6267; Loss self: 0.0000; time: 0.41s
Val loss: 0.6428 score: 0.7727 time: 0.23s
Test loss: 0.6588 score: 0.7209 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6214;  Loss pred: 0.6214; Loss self: 0.0000; time: 0.35s
Val loss: 0.6383 score: 0.7727 time: 0.22s
Test loss: 0.6555 score: 0.7209 time: 0.20s
Epoch 55/1000, LR 0.000269
Train loss: 0.6142;  Loss pred: 0.6142; Loss self: 0.0000; time: 0.31s
Val loss: 0.6334 score: 0.7955 time: 0.23s
Test loss: 0.6519 score: 0.7209 time: 0.21s
Epoch 56/1000, LR 0.000269
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.40s
Val loss: 0.6281 score: 0.7955 time: 0.21s
Test loss: 0.6481 score: 0.7209 time: 0.29s
Epoch 57/1000, LR 0.000269
Train loss: 0.6012;  Loss pred: 0.6012; Loss self: 0.0000; time: 0.34s
Val loss: 0.6225 score: 0.7955 time: 0.23s
Test loss: 0.6441 score: 0.7442 time: 0.19s
Epoch 58/1000, LR 0.000269
Train loss: 0.5923;  Loss pred: 0.5923; Loss self: 0.0000; time: 0.35s
Val loss: 0.6166 score: 0.7955 time: 0.20s
Test loss: 0.6399 score: 0.7442 time: 0.21s
Epoch 59/1000, LR 0.000268
Train loss: 0.5855;  Loss pred: 0.5855; Loss self: 0.0000; time: 0.30s
Val loss: 0.6102 score: 0.7955 time: 0.23s
Test loss: 0.6354 score: 0.7442 time: 0.19s
Epoch 60/1000, LR 0.000268
Train loss: 0.5771;  Loss pred: 0.5771; Loss self: 0.0000; time: 0.37s
Val loss: 0.6036 score: 0.7955 time: 0.21s
Test loss: 0.6305 score: 0.7674 time: 0.19s
Epoch 61/1000, LR 0.000268
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.44s
Val loss: 0.5966 score: 0.7955 time: 0.22s
Test loss: 0.6255 score: 0.7907 time: 0.20s
Epoch 62/1000, LR 0.000268
Train loss: 0.5537;  Loss pred: 0.5537; Loss self: 0.0000; time: 0.36s
Val loss: 0.5892 score: 0.8182 time: 0.23s
Test loss: 0.6201 score: 0.7907 time: 0.19s
Epoch 63/1000, LR 0.000268
Train loss: 0.5470;  Loss pred: 0.5470; Loss self: 0.0000; time: 0.47s
Val loss: 0.5814 score: 0.8409 time: 0.22s
Test loss: 0.6143 score: 0.7907 time: 0.20s
Epoch 64/1000, LR 0.000268
Train loss: 0.5345;  Loss pred: 0.5345; Loss self: 0.0000; time: 0.36s
Val loss: 0.5734 score: 0.8636 time: 0.31s
Test loss: 0.6084 score: 0.7907 time: 0.21s
Epoch 65/1000, LR 0.000268
Train loss: 0.5164;  Loss pred: 0.5164; Loss self: 0.0000; time: 0.33s
Val loss: 0.5648 score: 0.8636 time: 0.31s
Test loss: 0.6021 score: 0.7907 time: 0.18s
Epoch 66/1000, LR 0.000268
Train loss: 0.5008;  Loss pred: 0.5008; Loss self: 0.0000; time: 0.36s
Val loss: 0.5560 score: 0.8636 time: 0.22s
Test loss: 0.5958 score: 0.7907 time: 0.19s
Epoch 67/1000, LR 0.000268
Train loss: 0.4859;  Loss pred: 0.4859; Loss self: 0.0000; time: 0.35s
Val loss: 0.5469 score: 0.8636 time: 0.21s
Test loss: 0.5893 score: 0.8140 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.4842;  Loss pred: 0.4842; Loss self: 0.0000; time: 0.31s
Val loss: 0.5375 score: 0.8636 time: 0.23s
Test loss: 0.5828 score: 0.8140 time: 0.21s
Epoch 69/1000, LR 0.000268
Train loss: 0.4609;  Loss pred: 0.4609; Loss self: 0.0000; time: 0.38s
Val loss: 0.5277 score: 0.8636 time: 0.20s
Test loss: 0.5759 score: 0.8140 time: 0.21s
Epoch 70/1000, LR 0.000268
Train loss: 0.4525;  Loss pred: 0.4525; Loss self: 0.0000; time: 0.32s
Val loss: 0.5178 score: 0.8636 time: 0.22s
Test loss: 0.5691 score: 0.8140 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.4308;  Loss pred: 0.4308; Loss self: 0.0000; time: 0.43s
Val loss: 0.5077 score: 0.8636 time: 0.19s
Test loss: 0.5624 score: 0.8140 time: 0.21s
Epoch 72/1000, LR 0.000267
Train loss: 0.4153;  Loss pred: 0.4153; Loss self: 0.0000; time: 0.33s
Val loss: 0.4974 score: 0.8636 time: 0.21s
Test loss: 0.5555 score: 0.8140 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.3971;  Loss pred: 0.3971; Loss self: 0.0000; time: 0.29s
Val loss: 0.4871 score: 0.8636 time: 0.23s
Test loss: 0.5490 score: 0.8140 time: 0.28s
Epoch 74/1000, LR 0.000267
Train loss: 0.3893;  Loss pred: 0.3893; Loss self: 0.0000; time: 0.34s
Val loss: 0.4769 score: 0.8636 time: 0.20s
Test loss: 0.5426 score: 0.8140 time: 0.21s
Epoch 75/1000, LR 0.000267
Train loss: 0.3682;  Loss pred: 0.3682; Loss self: 0.0000; time: 0.33s
Val loss: 0.4669 score: 0.8636 time: 0.23s
Test loss: 0.5364 score: 0.8140 time: 0.20s
Epoch 76/1000, LR 0.000267
Train loss: 0.3502;  Loss pred: 0.3502; Loss self: 0.0000; time: 0.47s
Val loss: 0.4572 score: 0.8636 time: 0.22s
Test loss: 0.5306 score: 0.8140 time: 0.20s
Epoch 77/1000, LR 0.000267
Train loss: 0.3345;  Loss pred: 0.3345; Loss self: 0.0000; time: 0.36s
Val loss: 0.4478 score: 0.8409 time: 0.22s
Test loss: 0.5247 score: 0.8140 time: 0.20s
Epoch 78/1000, LR 0.000267
Train loss: 0.3280;  Loss pred: 0.3280; Loss self: 0.0000; time: 0.51s
Val loss: 0.4386 score: 0.8636 time: 0.19s
Test loss: 0.5188 score: 0.8140 time: 0.20s
Epoch 79/1000, LR 0.000267
Train loss: 0.3044;  Loss pred: 0.3044; Loss self: 0.0000; time: 0.35s
Val loss: 0.4296 score: 0.8636 time: 0.20s
Test loss: 0.5127 score: 0.8140 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.2922;  Loss pred: 0.2922; Loss self: 0.0000; time: 0.33s
Val loss: 0.4213 score: 0.8636 time: 0.22s
Test loss: 0.5067 score: 0.8140 time: 0.20s
Epoch 81/1000, LR 0.000267
Train loss: 0.2727;  Loss pred: 0.2727; Loss self: 0.0000; time: 0.39s
Val loss: 0.4136 score: 0.8864 time: 0.21s
Test loss: 0.5011 score: 0.8140 time: 0.20s
Epoch 82/1000, LR 0.000267
Train loss: 0.2497;  Loss pred: 0.2497; Loss self: 0.0000; time: 0.34s
Val loss: 0.4067 score: 0.8864 time: 0.30s
Test loss: 0.4960 score: 0.8372 time: 0.22s
Epoch 83/1000, LR 0.000266
Train loss: 0.2510;  Loss pred: 0.2510; Loss self: 0.0000; time: 0.36s
Val loss: 0.4007 score: 0.8864 time: 0.21s
Test loss: 0.4917 score: 0.8372 time: 0.20s
Epoch 84/1000, LR 0.000266
Train loss: 0.2092;  Loss pred: 0.2092; Loss self: 0.0000; time: 0.35s
Val loss: 0.3961 score: 0.8864 time: 0.21s
Test loss: 0.4890 score: 0.8372 time: 0.21s
Epoch 85/1000, LR 0.000266
Train loss: 0.2096;  Loss pred: 0.2096; Loss self: 0.0000; time: 0.30s
Val loss: 0.3930 score: 0.8864 time: 0.23s
Test loss: 0.4884 score: 0.8372 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.1939;  Loss pred: 0.1939; Loss self: 0.0000; time: 0.37s
Val loss: 0.3907 score: 0.8864 time: 0.20s
Test loss: 0.4884 score: 0.8372 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.1839;  Loss pred: 0.1839; Loss self: 0.0000; time: 0.32s
Val loss: 0.3905 score: 0.8636 time: 0.23s
Test loss: 0.4912 score: 0.8372 time: 0.19s
Epoch 88/1000, LR 0.000266
Train loss: 0.1958;  Loss pred: 0.1958; Loss self: 0.0000; time: 0.36s
Val loss: 0.3914 score: 0.8636 time: 0.21s
Test loss: 0.4951 score: 0.8372 time: 0.20s
     INFO: Early stopping counter 1 of 2
Epoch 89/1000, LR 0.000266
Train loss: 0.1650;  Loss pred: 0.1650; Loss self: 0.0000; time: 0.43s
Val loss: 0.3924 score: 0.8636 time: 0.22s
Test loss: 0.4982 score: 0.8372 time: 0.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.1839,   Val_Loss: 0.3905,   Val_Precision: 1.0000,   Val_Recall: 0.7273,   Val_accuracy: 0.8421,   Val_Score: 0.8636,   Val_Loss: 0.3905,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.4912


[0.24327209196053445, 0.23320096998941153, 0.296690039918758, 0.228139515966177, 0.2530617479933426, 0.22838954802136868, 0.1859746469417587, 0.2172384220175445, 0.22729405900463462, 0.21335777605418116, 0.2331797289662063, 0.20641815091948956, 0.2132164330687374, 0.21037336601875722, 0.20585888298228383, 0.21806486300192773, 0.2061968829948455, 0.21492108900565654, 0.22736013098619878, 0.2011941479286179, 0.30193882109597325, 0.19186742207966745, 0.21986332908272743, 0.19071811495814472, 0.23551007197238505, 0.2233777689980343, 0.2165787270059809, 0.21633886196650565, 0.19416841794736683, 0.22084778500720859, 0.20858443202450871, 0.19973850203678012, 0.2323217139346525, 0.2192384839290753, 0.22282575606368482, 0.20707419305108488, 0.22470831300597638, 0.20543932099826634, 0.2205141690792516, 0.2038702229037881, 0.21940565400291234, 0.22040204191580415, 0.2248202640330419, 0.2222965060500428, 0.22123710508458316, 0.20216087403241545, 0.23016495897900313, 0.19938346510753036, 0.25397641700692475, 0.22699258499778807, 0.2029996080091223, 0.2289539979537949, 0.1908437650417909, 0.20629018999170512, 0.22786112502217293, 0.19909618108067662, 0.22378380096051842, 0.19663566001690924, 0.22071232309099287, 0.2013079810421914, 0.21199327905196697, 0.22471527103334665, 0.1968772099353373, 0.2250322480686009, 0.19754181511234492, 0.22718935506418347, 0.21372671297285706, 0.21066060801967978, 0.22084671899210662, 0.20651560998521745, 0.29472299199551344, 0.20129545696545392, 0.22893680399283767, 0.2081552279414609, 0.21664001001045108, 0.22233344905544072, 0.23121237405575812, 0.22064029495231807, 0.2161357889417559, 0.22867278300691396, 0.23556484503205866, 0.21244486793875694, 0.2292961140628904, 0.2011207629693672, 0.21425197599455714, 0.28819568001199514, 0.2012828839942813, 0.2334110450465232, 0.21518257993739098, 0.23107246996369213, 0.192682427004911, 0.21822408097796142, 0.23016305698547512, 0.2080653989687562, 0.2275662620086223, 0.19410930201411247, 0.2256793100386858, 0.2096537830075249, 0.21540133201051503, 0.19936614308971912, 0.21145546692423522, 0.21021693001966923, 0.21267077210359275, 0.22387891809921712, 0.27166264096740633, 0.22093291801866144, 0.20920203905552626, 0.22227816295344383, 0.1844957519788295, 0.19783104094676673, 0.22086604998912662, 0.18913091893773526, 0.21522422996349633, 0.20598821609746665, 0.21580448502209038, 0.20070895995013416, 0.22018672106787562, 0.2097185970051214, 0.21255374606698751, 0.20365623792167753, 0.2014662290457636, 0.30838475504424423, 0.19944811391178519, 0.2188764939783141, 0.2143371100537479, 0.22216839203611016, 0.19268478197045624, 0.1983075230382383, 0.20024113997351378, 0.21244384499732405, 0.22287285700440407, 0.19451795693021268, 0.22537851601373404, 0.21145007794257253, 0.2197191179729998, 0.20255885599181056, 0.20465885708108544, 0.19988138100598007, 0.28385291004087776, 0.22633098997175694, 0.2317934730090201, 0.21582359995227307, 0.2033575070090592, 0.19847139692865312, 0.22059393906965852, 0.18575209996197373, 0.23481787601485848, 0.20759335602633655, 0.219740001950413, 0.19179440999869257, 0.21449954307172447, 0.20437734108418226, 0.22414758906234056, 0.20700023497920483, 0.2116039659595117, 0.2960486609954387, 0.195024715969339, 0.2189893729519099, 0.19675885001197457, 0.19884199695661664, 0.20887371303979307, 0.19661881402134895, 0.20903418096713722, 0.20966345397755504, 0.18410070496611297, 0.19545647804625332, 0.22125260496977717, 0.2123793630162254, 0.21696390607394278, 0.20434288191609085, 0.21210991905536503, 0.22194114292506129, 0.2886947470251471, 0.21615418698638678, 0.19936340709682554, 0.2031672679586336, 0.20633007702417672, 0.20271803496871144, 0.21452681592199951, 0.20023596996907145, 0.20577455998864025, 0.2291867920430377, 0.20047088793944567, 0.21316260599996895, 0.2097926129354164, 0.219783989014104, 0.19861484901048243, 0.2043123310431838, 0.22271435509901494]
[0.005528911180921237, 0.005300022045213898, 0.006742955452699045, 0.0051849889992312955, 0.0057514033634850584, 0.0051906715459401976, 0.004226696521403606, 0.004937236864035102, 0.005165774068287151, 0.004849040364867754, 0.005299539294686507, 0.004691321611806581, 0.004845828024289486, 0.004781212864062664, 0.004678610976870087, 0.004956019613680176, 0.004686292795337398, 0.0048845702046740125, 0.005167275704231791, 0.004572594271104952, 0.006862245933999392, 0.004360623229083351, 0.00499689384278926, 0.004334502612685107, 0.005352501635736024, 0.0050767674772280525, 0.004922243795590475, 0.0049167923174205826, 0.004412918589712883, 0.005019267841072922, 0.004740555273284289, 0.004539511409926821, 0.005280038953060284, 0.004982692816569893, 0.005064221728720109, 0.004706231660251929, 0.005107007113772191, 0.004669075477233326, 0.005011685660892082, 0.004633414156904275, 0.004986492136429826, 0.005009137316268276, 0.005109551455296407, 0.005052193319319154, 0.005028116024649617, 0.004594565318918533, 0.005231021794977344, 0.004531442388807508, 0.005772191295611926, 0.005158922386313366, 0.00461362745475278, 0.0052034999534953386, 0.004337358296404339, 0.004688413408902389, 0.005178661932322112, 0.004524913206379014, 0.0050859954763754186, 0.004468992273111574, 0.005016189161158929, 0.004575181387322532, 0.004818029069362886, 0.005107165250757878, 0.004474482043984939, 0.005114369274286384, 0.004489586707098748, 0.005163394433276897, 0.004857425294837661, 0.004787741091356359, 0.005019243613456969, 0.004693536590573124, 0.006698249818079851, 0.004574896749214862, 0.005203109181655402, 0.0047308006350332025, 0.0049236365911466155, 0.005053032933078198, 0.005254826683085412, 0.005014552158007229, 0.004912177021403543, 0.00519710870470259, 0.005353746478001333, 0.004828292453153567, 0.005211275319611145, 0.004570926431121982, 0.0048693630907853894, 0.006549901818454435, 0.0045746109998700294, 0.005304796478330073, 0.00489051318039525, 0.005251647044629367, 0.004379146068293432, 0.004959638204044578, 0.005230978567851707, 0.004728759067471732, 0.005171960500195961, 0.004411575045775284, 0.00512907522815195, 0.004764858704716476, 0.004895484818420796, 0.0045310487065845255, 0.0049175689982380285, 0.004888765814410913, 0.004945831909385878, 0.005206486467423654, 0.00631773583645131, 0.005137974837643289, 0.004865163698965727, 0.005169259603568461, 0.004290598883228593, 0.004600721882482947, 0.005136419767188991, 0.004398393463668262, 0.005005214650313868, 0.004790423630173643, 0.0050187089540021015, 0.004667650231398469, 0.0051206214201831544, 0.004877176674537707, 0.004943110373650872, 0.004736191579573896, 0.0046852611405991534, 0.007171738489401029, 0.004638328230506633, 0.005090151022751491, 0.004984583954738323, 0.005166706791537446, 0.004481041441173401, 0.004611802861354378, 0.00465677069705846, 0.00494055453482149, 0.005183089697776839, 0.00452367341698169, 0.005241360837528699, 0.004917443673083082, 0.005109746929604647, 0.00471067106957699, 0.00475950830421129, 0.0046484042094413975, 0.006601230466066924, 0.0052635113946920215, 0.0053905458839306995, 0.005019153487262165, 0.004729244349047888, 0.004615613882061701, 0.005130091606271129, 0.004319816278185435, 0.005460880837554849, 0.004827752465728757, 0.0051102326034979766, 0.0044603351162486645, 0.00498836146678429, 0.004752961420562378, 0.005212734629356757, 0.004813958953004764, 0.004921022464174691, 0.006884852581289272, 0.004535458510914861, 0.0050927761151606955, 0.004575787209580804, 0.004624232487363177, 0.004857528210227746, 0.004572530558636022, 0.004861260022491563, 0.004875894278547791, 0.004281411743397976, 0.004545499489447752, 0.005145409417901795, 0.004939054953865707, 0.005045672234277739, 0.004752160044560252, 0.004932788815241047, 0.0051614219284897974, 0.006713831326166211, 0.005026841557822948, 0.004636358304577338, 0.004724820185084503, 0.0047983738842831795, 0.004714372906249103, 0.004988995719116268, 0.0046566504643970105, 0.00478545488345675, 0.0053299253963497144, 0.004662113673010364, 0.004957269906976022, 0.004878897975242242, 0.005111255558467535, 0.004618949976987963, 0.004751449559143809, 0.005179403606953836]
[180.8674379597066, 188.67846047980768, 148.30292251148177, 192.86444004958463, 173.87060805870007, 192.65329951037538, 236.59138879171735, 202.54243973677222, 193.58183048287603, 206.2263715611022, 188.6956477523684, 213.15954921600652, 206.3630807753694, 209.15195127921703, 213.73865126717232, 201.77482696793308, 213.3882887119099, 204.72630305182363, 193.5255746429478, 218.69423366931554, 145.724885062111, 229.32501788516376, 200.12432352211053, 230.70697825246597, 186.82852767825258, 196.9757339656624, 203.15938046299866, 203.38463279340093, 226.607398181135, 199.2322449535268, 210.94575262850867, 220.28802434844425, 189.3925421554712, 200.6946919694729, 197.46370786429446, 212.48422776248736, 195.8094002460415, 214.17516270106495, 199.5336634544633, 215.82357331685853, 200.54177819599832, 199.6351740552769, 195.71189540785036, 197.93383522678866, 198.88164773796856, 217.64844562823183, 191.1672402053777, 220.68028548039413, 173.2444315835841, 193.83893090793586, 216.74918701332047, 192.17834321844697, 230.55508253237875, 213.2917711781119, 193.1000735457547, 220.99871409472476, 196.61834239629707, 223.76409241444975, 199.35452349826502, 218.57056919555615, 207.55374980172869, 195.80333725281454, 223.4895548065286, 195.5275316210974, 222.73765164593914, 193.67104584442137, 205.87038179728114, 208.86676637660491, 199.23320663673806, 213.0589547354292, 149.29272976663393, 218.58416808458438, 192.19277648943046, 211.38071061263008, 203.1019108514506, 197.90094646994942, 190.3012335723397, 199.41960288581336, 203.57572531339125, 192.41467839515317, 186.78508668817676, 207.11255784575698, 191.89160784439574, 218.7740308378885, 205.36566720447786, 152.67404424024903, 218.59782176635593, 188.50864572938258, 204.47751863929756, 190.4164524961092, 228.3550227384179, 201.62761049475372, 191.1688199500092, 211.47197091913966, 193.3502778998623, 226.67641140042343, 194.96692006217827, 209.86981188133748, 204.26986030825518, 220.69945938713897, 203.3525102257439, 204.5506039688461, 202.19045416854243, 192.06810701552325, 158.28455413256134, 194.6292131821114, 205.54292966803717, 193.45130186723, 233.0676968916561, 217.35719427150278, 194.68813791036203, 227.35573983096992, 199.79163130142516, 208.749805278443, 199.2544316008928, 214.24056011591753, 195.2887975780549, 205.03665680611968, 202.3017744718943, 211.14010765796928, 213.43527500200756, 139.43620524896164, 215.5949191829343, 196.45782522567436, 200.61854892611538, 193.54688399154003, 223.16240836597598, 216.83494070826856, 214.74108670020397, 202.40642886378578, 192.93511366954075, 221.05928253928317, 190.79014610860105, 203.35769283413703, 195.7044084133091, 212.28397933753382, 210.10573699707257, 215.12759109220644, 151.4869091664678, 189.9872395086763, 185.50996903319484, 199.23678415849315, 211.45027116252183, 216.6559044044907, 194.9282930498901, 231.49132639040286, 183.12064111030077, 207.13572352741753, 195.6858087664142, 224.19840077869384, 200.46662750056132, 210.39514347282002, 191.83788761627375, 207.72923279202928, 203.20980188163213, 145.24639245256546, 220.48487437233484, 196.3565602310885, 218.54163102388054, 216.25210296686842, 205.86602006643105, 218.6972808987193, 205.7079842208205, 205.09058295206398, 233.56781826508987, 219.99782473223718, 194.3479942569433, 202.46788289272192, 198.18964720032884, 210.43062325829905, 202.72507854182962, 193.74505976352032, 148.9462501243725, 198.9320706644841, 215.6865225478216, 211.64826614076006, 208.4039351905126, 212.11728895575, 200.4411421257215, 214.74663122036367, 208.96655058999426, 187.6198868908871, 214.49498449365177, 201.7239365144854, 204.96431880200345, 195.64664465727117, 216.49942194266936, 210.46208900093967, 193.0724222104271]
Elapsed: 0.21665419600830074~0.0212396560658083
Time per graph: 0.004977156571950166~0.0004842511254741935
Speed: 202.54038183137294~16.911754274407038
Total Time: 0.2233
best val loss: 0.39054691791534424 test_score: 0.8372

Testing...
Test loss: 0.5011 score: 0.8140 time: 0.19s
test Score 0.8140
Epoch Time List: [0.7732664779759943, 0.7813887620577589, 0.8288674730574712, 0.7950316090136766, 0.7965285470709205, 0.8897204200038686, 0.7090583479730412, 0.7811067568836734, 0.7758594719925895, 0.7341011859243736, 0.8235915959812701, 0.737911434029229, 0.8137491289526224, 0.7548164819600061, 0.8023644528584555, 0.7700347149511799, 0.7656643980881199, 0.8406777619384229, 0.7675775809912011, 0.8128744401037693, 0.8440270129358396, 0.7188111690338701, 0.7738049409817904, 0.7840968408854678, 0.785351003985852, 0.7677554361289367, 0.736225773114711, 0.7842672599945217, 0.8653467500116676, 0.7812117339344695, 0.7350670748855919, 0.7577877099392936, 0.7985238520195708, 0.7334894760278985, 0.803243687027134, 0.8349421621533111, 0.7916383527917787, 0.737575912149623, 0.851187521009706, 0.7242460489505902, 0.7560415827902034, 0.8183124138740823, 0.7380108040524647, 0.7303960169665515, 0.7931049490580335, 0.7387767949840054, 0.9567796410992742, 0.7186160059645772, 0.8875752170570195, 0.7623694220092148, 0.7596321690361947, 0.7851218339055777, 0.7118690209463239, 0.7969890109961852, 0.7611447629751638, 0.816726791090332, 0.7826309739612043, 0.7794539500027895, 0.7711630179546773, 0.7414236011682078, 0.7703469479456544, 0.8183632269501686, 0.7269962569698691, 0.8690970919560641, 0.7338661990361288, 0.7606935399817303, 0.7453950489871204, 0.7991215400397778, 0.805327538982965, 0.762273985077627, 0.8432232819031924, 0.7705078560393304, 0.7762300530448556, 0.7308651158818975, 0.8054208350367844, 0.9440757369156927, 0.8434858899563551, 0.738293560105376, 0.9748748468700796, 0.8227267819456756, 0.792995274066925, 0.7344594849273562, 0.8107915088767186, 0.8026043430436403, 0.7909117179224268, 0.8250153720146045, 0.7692621680907905, 0.7962496160762385, 0.759355018963106, 0.8079231270821765, 0.7304338549729437, 0.7853603980038315, 0.7962508279597387, 0.7190112030366436, 0.8057455769740045, 0.72474109090399, 0.8615275900810957, 0.8660373939201236, 0.8607163380365819, 0.7651457510655746, 0.7937388708814979, 0.769650814938359, 0.7518147090449929, 0.8489000670379028, 0.8504795398330316, 0.7862286239396781, 0.7412116770865396, 0.8212429590057582, 0.7617201040266082, 0.7539216190343723, 0.7694299820577726, 0.723049662890844, 0.8997155219549313, 0.8584593860432506, 0.7913590649841353, 0.7286333000520244, 0.9512326279655099, 0.7441427300218493, 0.955275894026272, 0.7366696598473936, 0.820235259947367, 0.8832085480680689, 0.7989495259243995, 0.7682132769841701, 0.7629421029705554, 0.8255186141468585, 0.7300417899386957, 0.7782565668458119, 0.7699648708803579, 0.7377288189018145, 0.906755656003952, 0.7447551351506263, 0.8493402630556375, 0.7440091291209683, 0.7975087690865621, 0.7413135250099003, 0.7895126298535615, 0.75007045106031, 0.7997397780418396, 0.7698245899518952, 0.7455802869517356, 0.8314993060193956, 0.7452866779640317, 0.7528035989962518, 0.7973252580268309, 0.7093929649563506, 0.9289047050988302, 0.8442672158125788, 0.8132702020229772, 0.7741190451197326, 0.9496016580378637, 0.7644116439623758, 0.8528837249614298, 0.7725550329778343, 0.7454468799987808, 0.9002405379433185, 0.7594567449996248, 0.7621744051575661, 0.722983960993588, 0.7725384400691837, 0.8628053279826418, 0.7807934479787946, 0.9005882239434868, 0.8738047359511256, 0.8165225811535493, 0.7627214901149273, 0.7819654460763559, 0.7476258500246331, 0.7980920620029792, 0.7383388559101149, 0.8277270100079477, 0.7609620769508183, 0.8045150911202654, 0.7495339361485094, 0.7512870909413323, 0.8885110749397427, 0.7867105369223282, 0.8957934479694813, 0.7562049580737948, 0.7500790630001575, 0.7971631409600377, 0.8612333229975775, 0.7645560872042552, 0.7760440589627251, 0.7391999771352857, 0.7926183650270104, 0.7382275080308318, 0.7695347709814087, 0.8680282189743593]
Total Epoch List: [100, 89]
Total Time List: [0.19983986602164805, 0.22330781805794686]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcfd00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7057;  Loss pred: 0.7057; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7025 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5116 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7025 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6993 score: 0.5116 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.7057;  Loss pred: 0.7057; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7023 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6992 score: 0.5116 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.7055;  Loss pred: 0.7055; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.7052;  Loss pred: 0.7052; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7019 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6987 score: 0.5116 time: 0.22s
Epoch 6/1000, LR 0.000120
Train loss: 0.7049;  Loss pred: 0.7049; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.5116 time: 0.26s
Epoch 7/1000, LR 0.000150
Train loss: 0.7045;  Loss pred: 0.7045; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7012 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5116 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.7040;  Loss pred: 0.7040; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7008 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5116 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.7034;  Loss pred: 0.7034; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5116 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.7028;  Loss pred: 0.7028; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6998 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.5116 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.7021;  Loss pred: 0.7021; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5116 time: 0.22s
Epoch 12/1000, LR 0.000270
Train loss: 0.7015;  Loss pred: 0.7015; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6989 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5116 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.7009;  Loss pred: 0.7009; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6984 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5116 time: 0.23s
Epoch 14/1000, LR 0.000270
Train loss: 0.7002;  Loss pred: 0.7002; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5116 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5116 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5116 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 0.6973;  Loss pred: 0.6973; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.23s
Epoch 20/1000, LR 0.000270
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.22s
Epoch 25/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5116 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5116 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.23s
Epoch 31/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.36s
Val loss: 0.6936 score: 0.5227 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5116 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.36s
Val loss: 0.6933 score: 0.5227 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5116 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.42s
Val loss: 0.6931 score: 0.5227 time: 0.19s
Test loss: 0.6904 score: 0.5349 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.37s
Val loss: 0.6929 score: 0.5682 time: 0.35s
Test loss: 0.6901 score: 0.6512 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.37s
Val loss: 0.6927 score: 0.5682 time: 0.18s
Test loss: 0.6899 score: 0.6279 time: 0.22s
Epoch 36/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.45s
Val loss: 0.6926 score: 0.5455 time: 0.19s
Test loss: 0.6896 score: 0.6512 time: 0.28s
Epoch 37/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.38s
Val loss: 0.6923 score: 0.5682 time: 0.20s
Test loss: 0.6893 score: 0.6279 time: 0.22s
Epoch 38/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.39s
Val loss: 0.6921 score: 0.4091 time: 0.17s
Test loss: 0.6889 score: 0.5814 time: 0.25s
Epoch 39/1000, LR 0.000269
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.31s
Val loss: 0.6919 score: 0.4545 time: 0.19s
Test loss: 0.6886 score: 0.5349 time: 0.23s
Epoch 40/1000, LR 0.000269
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.43s
Val loss: 0.6916 score: 0.5000 time: 0.21s
Test loss: 0.6882 score: 0.5116 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.41s
Val loss: 0.6913 score: 0.4545 time: 0.20s
Test loss: 0.6877 score: 0.5349 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.36s
Val loss: 0.6910 score: 0.4773 time: 0.17s
Test loss: 0.6871 score: 0.5116 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.32s
Val loss: 0.6906 score: 0.4773 time: 0.18s
Test loss: 0.6865 score: 0.5349 time: 0.27s
Epoch 44/1000, LR 0.000269
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.43s
Val loss: 0.6901 score: 0.4773 time: 0.18s
Test loss: 0.6858 score: 0.5349 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.35s
Val loss: 0.6895 score: 0.4773 time: 0.17s
Test loss: 0.6850 score: 0.5349 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.31s
Val loss: 0.6887 score: 0.4773 time: 0.19s
Test loss: 0.6840 score: 0.5349 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6804;  Loss pred: 0.6804; Loss self: 0.0000; time: 0.41s
Val loss: 0.6879 score: 0.4773 time: 0.17s
Test loss: 0.6828 score: 0.5349 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.34s
Val loss: 0.6870 score: 0.4773 time: 0.18s
Test loss: 0.6815 score: 0.5349 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.49s
Val loss: 0.6858 score: 0.5227 time: 0.17s
Test loss: 0.6800 score: 0.5581 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 0.34s
Val loss: 0.6845 score: 0.5682 time: 0.19s
Test loss: 0.6782 score: 0.5581 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.6735;  Loss pred: 0.6735; Loss self: 0.0000; time: 0.39s
Val loss: 0.6831 score: 0.5909 time: 0.18s
Test loss: 0.6762 score: 0.5581 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.36s
Val loss: 0.6815 score: 0.6136 time: 0.19s
Test loss: 0.6740 score: 0.6279 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.33s
Val loss: 0.6799 score: 0.6136 time: 0.19s
Test loss: 0.6716 score: 0.6512 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.37s
Val loss: 0.6781 score: 0.6364 time: 0.17s
Test loss: 0.6691 score: 0.6977 time: 0.24s
Epoch 55/1000, LR 0.000269
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.34s
Val loss: 0.6763 score: 0.6591 time: 0.19s
Test loss: 0.6665 score: 0.7209 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.6608;  Loss pred: 0.6608; Loss self: 0.0000; time: 0.37s
Val loss: 0.6742 score: 0.6818 time: 0.18s
Test loss: 0.6636 score: 0.7442 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.45s
Val loss: 0.6720 score: 0.7045 time: 0.19s
Test loss: 0.6606 score: 0.7442 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.6545;  Loss pred: 0.6545; Loss self: 0.0000; time: 0.37s
Val loss: 0.6696 score: 0.7045 time: 0.18s
Test loss: 0.6573 score: 0.7442 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.40s
Val loss: 0.6671 score: 0.7045 time: 0.19s
Test loss: 0.6538 score: 0.7442 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.6475;  Loss pred: 0.6475; Loss self: 0.0000; time: 0.35s
Val loss: 0.6643 score: 0.7500 time: 0.19s
Test loss: 0.6501 score: 0.7442 time: 0.22s
Epoch 61/1000, LR 0.000268
Train loss: 0.6416;  Loss pred: 0.6416; Loss self: 0.0000; time: 0.46s
Val loss: 0.6613 score: 0.7955 time: 0.19s
Test loss: 0.6460 score: 0.7442 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.37s
Val loss: 0.6579 score: 0.7955 time: 0.19s
Test loss: 0.6414 score: 0.7442 time: 0.22s
Epoch 63/1000, LR 0.000268
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.47s
Val loss: 0.6542 score: 0.8182 time: 0.19s
Test loss: 0.6363 score: 0.7674 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 0.39s
Val loss: 0.6501 score: 0.8182 time: 0.18s
Test loss: 0.6307 score: 0.7674 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.6266;  Loss pred: 0.6266; Loss self: 0.0000; time: 0.47s
Val loss: 0.6456 score: 0.8182 time: 0.25s
Test loss: 0.6245 score: 0.7674 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.6173;  Loss pred: 0.6173; Loss self: 0.0000; time: 0.37s
Val loss: 0.6407 score: 0.7955 time: 0.17s
Test loss: 0.6178 score: 0.7907 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.6051;  Loss pred: 0.6051; Loss self: 0.0000; time: 0.35s
Val loss: 0.6355 score: 0.7955 time: 0.19s
Test loss: 0.6107 score: 0.7907 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.6013;  Loss pred: 0.6013; Loss self: 0.0000; time: 0.41s
Val loss: 0.6296 score: 0.8182 time: 0.17s
Test loss: 0.6027 score: 0.7907 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.36s
Val loss: 0.6231 score: 0.7955 time: 0.19s
Test loss: 0.5939 score: 0.7907 time: 0.23s
Epoch 70/1000, LR 0.000268
Train loss: 0.5872;  Loss pred: 0.5872; Loss self: 0.0000; time: 0.39s
Val loss: 0.6160 score: 0.8409 time: 0.17s
Test loss: 0.5844 score: 0.7907 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.37s
Val loss: 0.6086 score: 0.8409 time: 0.19s
Test loss: 0.5744 score: 0.8140 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.5747;  Loss pred: 0.5747; Loss self: 0.0000; time: 0.37s
Val loss: 0.6006 score: 0.8636 time: 0.19s
Test loss: 0.5635 score: 0.7907 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.5539;  Loss pred: 0.5539; Loss self: 0.0000; time: 0.45s
Val loss: 0.5923 score: 0.8864 time: 0.19s
Test loss: 0.5523 score: 0.8372 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.5484;  Loss pred: 0.5484; Loss self: 0.0000; time: 0.36s
Val loss: 0.5837 score: 0.8636 time: 0.19s
Test loss: 0.5409 score: 0.8605 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.36s
Val loss: 0.5750 score: 0.8864 time: 0.18s
Test loss: 0.5297 score: 0.8605 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 0.32s
Val loss: 0.5661 score: 0.8864 time: 0.20s
Test loss: 0.5185 score: 0.8605 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.5121;  Loss pred: 0.5121; Loss self: 0.0000; time: 0.39s
Val loss: 0.5569 score: 0.8864 time: 0.17s
Test loss: 0.5069 score: 0.8372 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.4791;  Loss pred: 0.4791; Loss self: 0.0000; time: 0.33s
Val loss: 0.5477 score: 0.8636 time: 0.21s
Test loss: 0.4958 score: 0.8372 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.4899;  Loss pred: 0.4899; Loss self: 0.0000; time: 0.38s
Val loss: 0.5382 score: 0.8636 time: 0.18s
Test loss: 0.4844 score: 0.8372 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.4742;  Loss pred: 0.4742; Loss self: 0.0000; time: 0.35s
Val loss: 0.5280 score: 0.8636 time: 0.19s
Test loss: 0.4720 score: 0.8372 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.4456;  Loss pred: 0.4456; Loss self: 0.0000; time: 0.45s
Val loss: 0.5172 score: 0.8636 time: 0.17s
Test loss: 0.4590 score: 0.8372 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.4413;  Loss pred: 0.4413; Loss self: 0.0000; time: 0.33s
Val loss: 0.5060 score: 0.8636 time: 0.19s
Test loss: 0.4454 score: 0.8372 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.4186;  Loss pred: 0.4186; Loss self: 0.0000; time: 0.36s
Val loss: 0.4952 score: 0.8636 time: 0.19s
Test loss: 0.4328 score: 0.8372 time: 0.22s
Epoch 84/1000, LR 0.000266
Train loss: 0.4120;  Loss pred: 0.4120; Loss self: 0.0000; time: 0.36s
Val loss: 0.4846 score: 0.8636 time: 0.17s
Test loss: 0.4206 score: 0.8372 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.3911;  Loss pred: 0.3911; Loss self: 0.0000; time: 0.32s
Val loss: 0.4741 score: 0.8636 time: 0.19s
Test loss: 0.4090 score: 0.8372 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3841;  Loss pred: 0.3841; Loss self: 0.0000; time: 0.38s
Val loss: 0.4643 score: 0.8636 time: 0.17s
Test loss: 0.3985 score: 0.8372 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.3641;  Loss pred: 0.3641; Loss self: 0.0000; time: 0.34s
Val loss: 0.4546 score: 0.8636 time: 0.19s
Test loss: 0.3887 score: 0.8372 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.3579;  Loss pred: 0.3579; Loss self: 0.0000; time: 0.34s
Val loss: 0.4444 score: 0.8636 time: 0.19s
Test loss: 0.3780 score: 0.8372 time: 0.22s
Epoch 89/1000, LR 0.000266
Train loss: 0.3543;  Loss pred: 0.3543; Loss self: 0.0000; time: 0.41s
Val loss: 0.4337 score: 0.8636 time: 0.19s
Test loss: 0.3663 score: 0.8372 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.3230;  Loss pred: 0.3230; Loss self: 0.0000; time: 0.31s
Val loss: 0.4232 score: 0.8636 time: 0.20s
Test loss: 0.3548 score: 0.8372 time: 0.21s
Epoch 91/1000, LR 0.000266
Train loss: 0.3193;  Loss pred: 0.3193; Loss self: 0.0000; time: 0.37s
Val loss: 0.4142 score: 0.8636 time: 0.17s
Test loss: 0.3462 score: 0.8372 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.3171;  Loss pred: 0.3171; Loss self: 0.0000; time: 0.40s
Val loss: 0.4057 score: 0.8636 time: 0.19s
Test loss: 0.3389 score: 0.8372 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.2701;  Loss pred: 0.2701; Loss self: 0.0000; time: 0.45s
Val loss: 0.3968 score: 0.8636 time: 0.16s
Test loss: 0.3306 score: 0.8372 time: 0.24s
Epoch 94/1000, LR 0.000265
Train loss: 0.2518;  Loss pred: 0.2518; Loss self: 0.0000; time: 0.35s
Val loss: 0.3878 score: 0.8409 time: 0.19s
Test loss: 0.3222 score: 0.8605 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.2356;  Loss pred: 0.2356; Loss self: 0.0000; time: 0.43s
Val loss: 0.3786 score: 0.8409 time: 0.17s
Test loss: 0.3130 score: 0.8605 time: 0.23s
Epoch 96/1000, LR 0.000265
Train loss: 0.2745;  Loss pred: 0.2745; Loss self: 0.0000; time: 0.36s
Val loss: 0.3704 score: 0.8182 time: 0.20s
Test loss: 0.3057 score: 0.8605 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.2231;  Loss pred: 0.2231; Loss self: 0.0000; time: 0.45s
Val loss: 0.3622 score: 0.8182 time: 0.16s
Test loss: 0.2981 score: 0.8605 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.2240;  Loss pred: 0.2240; Loss self: 0.0000; time: 0.35s
Val loss: 0.3537 score: 0.8409 time: 0.19s
Test loss: 0.2882 score: 0.8605 time: 0.25s
Epoch 99/1000, LR 0.000265
Train loss: 0.2222;  Loss pred: 0.2222; Loss self: 0.0000; time: 0.32s
Val loss: 0.3467 score: 0.8182 time: 0.20s
Test loss: 0.2826 score: 0.8605 time: 0.23s
Epoch 100/1000, LR 0.000265
Train loss: 0.2313;  Loss pred: 0.2313; Loss self: 0.0000; time: 0.37s
Val loss: 0.3407 score: 0.8409 time: 0.16s
Test loss: 0.2825 score: 0.8605 time: 0.24s
Epoch 101/1000, LR 0.000265
Train loss: 0.1915;  Loss pred: 0.1915; Loss self: 0.0000; time: 0.36s
Val loss: 0.3363 score: 0.8409 time: 0.19s
Test loss: 0.2874 score: 0.8605 time: 0.23s
Epoch 102/1000, LR 0.000264
Train loss: 0.1613;  Loss pred: 0.1613; Loss self: 0.0000; time: 0.50s
Val loss: 0.3330 score: 0.8182 time: 0.18s
Test loss: 0.2934 score: 0.8605 time: 0.24s
Epoch 103/1000, LR 0.000264
Train loss: 0.1854;  Loss pred: 0.1854; Loss self: 0.0000; time: 0.35s
Val loss: 0.3277 score: 0.8182 time: 0.20s
Test loss: 0.2933 score: 0.8605 time: 0.23s
Epoch 104/1000, LR 0.000264
Train loss: 0.1771;  Loss pred: 0.1771; Loss self: 0.0000; time: 0.42s
Val loss: 0.3219 score: 0.8409 time: 0.20s
Test loss: 0.2919 score: 0.8372 time: 0.23s
Epoch 105/1000, LR 0.000264
Train loss: 0.1694;  Loss pred: 0.1694; Loss self: 0.0000; time: 0.45s
Val loss: 0.3147 score: 0.8409 time: 0.19s
Test loss: 0.2853 score: 0.8605 time: 0.23s
Epoch 106/1000, LR 0.000264
Train loss: 0.1797;  Loss pred: 0.1797; Loss self: 0.0000; time: 0.55s
Val loss: 0.3081 score: 0.8409 time: 0.16s
Test loss: 0.2767 score: 0.8605 time: 0.31s
Epoch 107/1000, LR 0.000264
Train loss: 0.1667;  Loss pred: 0.1667; Loss self: 0.0000; time: 0.34s
Val loss: 0.3030 score: 0.8636 time: 0.19s
Test loss: 0.2686 score: 0.8605 time: 0.23s
Epoch 108/1000, LR 0.000264
Train loss: 0.1487;  Loss pred: 0.1487; Loss self: 0.0000; time: 0.53s
Val loss: 0.2989 score: 0.8409 time: 0.21s
Test loss: 0.2650 score: 0.8605 time: 0.25s
Epoch 109/1000, LR 0.000264
Train loss: 0.1616;  Loss pred: 0.1616; Loss self: 0.0000; time: 0.39s
Val loss: 0.2941 score: 0.8636 time: 0.19s
Test loss: 0.2717 score: 0.8605 time: 0.24s
Epoch 110/1000, LR 0.000263
Train loss: 0.1326;  Loss pred: 0.1326; Loss self: 0.0000; time: 0.37s
Val loss: 0.2897 score: 0.8409 time: 0.17s
Test loss: 0.2834 score: 0.8372 time: 0.25s
Epoch 111/1000, LR 0.000263
Train loss: 0.1151;  Loss pred: 0.1151; Loss self: 0.0000; time: 0.35s
Val loss: 0.2880 score: 0.8636 time: 0.20s
Test loss: 0.3024 score: 0.8372 time: 0.23s
Epoch 112/1000, LR 0.000263
Train loss: 0.1291;  Loss pred: 0.1291; Loss self: 0.0000; time: 0.37s
Val loss: 0.2879 score: 0.8636 time: 0.18s
Test loss: 0.3199 score: 0.8372 time: 0.24s
Epoch 113/1000, LR 0.000263
Train loss: 0.1124;  Loss pred: 0.1124; Loss self: 0.0000; time: 0.42s
Val loss: 0.2876 score: 0.8636 time: 0.18s
Test loss: 0.3323 score: 0.8372 time: 0.24s
Epoch 114/1000, LR 0.000263
Train loss: 0.1340;  Loss pred: 0.1340; Loss self: 0.0000; time: 0.36s
Val loss: 0.2894 score: 0.8636 time: 0.18s
Test loss: 0.3477 score: 0.8372 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 115/1000, LR 0.000263
Train loss: 0.1017;  Loss pred: 0.1017; Loss self: 0.0000; time: 0.34s
Val loss: 0.2874 score: 0.8636 time: 0.19s
Test loss: 0.3535 score: 0.8372 time: 0.24s
Epoch 116/1000, LR 0.000263
Train loss: 0.1178;  Loss pred: 0.1178; Loss self: 0.0000; time: 0.42s
Val loss: 0.2827 score: 0.8409 time: 0.17s
Test loss: 0.3513 score: 0.8372 time: 0.24s
Epoch 117/1000, LR 0.000262
Train loss: 0.1072;  Loss pred: 0.1072; Loss self: 0.0000; time: 0.35s
Val loss: 0.2780 score: 0.8409 time: 0.19s
Test loss: 0.3450 score: 0.8140 time: 0.23s
Epoch 118/1000, LR 0.000262
Train loss: 0.0989;  Loss pred: 0.0989; Loss self: 0.0000; time: 0.43s
Val loss: 0.2779 score: 0.8409 time: 0.17s
Test loss: 0.3519 score: 0.8140 time: 0.24s
Epoch 119/1000, LR 0.000262
Train loss: 0.1091;  Loss pred: 0.1091; Loss self: 0.0000; time: 0.33s
Val loss: 0.2805 score: 0.8182 time: 0.20s
Test loss: 0.3661 score: 0.8372 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 120/1000, LR 0.000262
Train loss: 0.0607;  Loss pred: 0.0607; Loss self: 0.0000; time: 0.38s
Val loss: 0.2823 score: 0.8182 time: 0.18s
Test loss: 0.3766 score: 0.8372 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 117,   Train_Loss: 0.0989,   Val_Loss: 0.2779,   Val_Precision: 0.8947,   Val_Recall: 0.7727,   Val_accuracy: 0.8293,   Val_Score: 0.8409,   Val_Loss: 0.2779,   Test_Precision: 0.7826,   Test_Recall: 0.8571,   Test_accuracy: 0.8182,   Test_Score: 0.8140,   Test_loss: 0.3519


[0.24327209196053445, 0.23320096998941153, 0.296690039918758, 0.228139515966177, 0.2530617479933426, 0.22838954802136868, 0.1859746469417587, 0.2172384220175445, 0.22729405900463462, 0.21335777605418116, 0.2331797289662063, 0.20641815091948956, 0.2132164330687374, 0.21037336601875722, 0.20585888298228383, 0.21806486300192773, 0.2061968829948455, 0.21492108900565654, 0.22736013098619878, 0.2011941479286179, 0.30193882109597325, 0.19186742207966745, 0.21986332908272743, 0.19071811495814472, 0.23551007197238505, 0.2233777689980343, 0.2165787270059809, 0.21633886196650565, 0.19416841794736683, 0.22084778500720859, 0.20858443202450871, 0.19973850203678012, 0.2323217139346525, 0.2192384839290753, 0.22282575606368482, 0.20707419305108488, 0.22470831300597638, 0.20543932099826634, 0.2205141690792516, 0.2038702229037881, 0.21940565400291234, 0.22040204191580415, 0.2248202640330419, 0.2222965060500428, 0.22123710508458316, 0.20216087403241545, 0.23016495897900313, 0.19938346510753036, 0.25397641700692475, 0.22699258499778807, 0.2029996080091223, 0.2289539979537949, 0.1908437650417909, 0.20629018999170512, 0.22786112502217293, 0.19909618108067662, 0.22378380096051842, 0.19663566001690924, 0.22071232309099287, 0.2013079810421914, 0.21199327905196697, 0.22471527103334665, 0.1968772099353373, 0.2250322480686009, 0.19754181511234492, 0.22718935506418347, 0.21372671297285706, 0.21066060801967978, 0.22084671899210662, 0.20651560998521745, 0.29472299199551344, 0.20129545696545392, 0.22893680399283767, 0.2081552279414609, 0.21664001001045108, 0.22233344905544072, 0.23121237405575812, 0.22064029495231807, 0.2161357889417559, 0.22867278300691396, 0.23556484503205866, 0.21244486793875694, 0.2292961140628904, 0.2011207629693672, 0.21425197599455714, 0.28819568001199514, 0.2012828839942813, 0.2334110450465232, 0.21518257993739098, 0.23107246996369213, 0.192682427004911, 0.21822408097796142, 0.23016305698547512, 0.2080653989687562, 0.2275662620086223, 0.19410930201411247, 0.2256793100386858, 0.2096537830075249, 0.21540133201051503, 0.19936614308971912, 0.21145546692423522, 0.21021693001966923, 0.21267077210359275, 0.22387891809921712, 0.27166264096740633, 0.22093291801866144, 0.20920203905552626, 0.22227816295344383, 0.1844957519788295, 0.19783104094676673, 0.22086604998912662, 0.18913091893773526, 0.21522422996349633, 0.20598821609746665, 0.21580448502209038, 0.20070895995013416, 0.22018672106787562, 0.2097185970051214, 0.21255374606698751, 0.20365623792167753, 0.2014662290457636, 0.30838475504424423, 0.19944811391178519, 0.2188764939783141, 0.2143371100537479, 0.22216839203611016, 0.19268478197045624, 0.1983075230382383, 0.20024113997351378, 0.21244384499732405, 0.22287285700440407, 0.19451795693021268, 0.22537851601373404, 0.21145007794257253, 0.2197191179729998, 0.20255885599181056, 0.20465885708108544, 0.19988138100598007, 0.28385291004087776, 0.22633098997175694, 0.2317934730090201, 0.21582359995227307, 0.2033575070090592, 0.19847139692865312, 0.22059393906965852, 0.18575209996197373, 0.23481787601485848, 0.20759335602633655, 0.219740001950413, 0.19179440999869257, 0.21449954307172447, 0.20437734108418226, 0.22414758906234056, 0.20700023497920483, 0.2116039659595117, 0.2960486609954387, 0.195024715969339, 0.2189893729519099, 0.19675885001197457, 0.19884199695661664, 0.20887371303979307, 0.19661881402134895, 0.20903418096713722, 0.20966345397755504, 0.18410070496611297, 0.19545647804625332, 0.22125260496977717, 0.2123793630162254, 0.21696390607394278, 0.20434288191609085, 0.21210991905536503, 0.22194114292506129, 0.2886947470251471, 0.21615418698638678, 0.19936340709682554, 0.2031672679586336, 0.20633007702417672, 0.20271803496871144, 0.21452681592199951, 0.20023596996907145, 0.20577455998864025, 0.2291867920430377, 0.20047088793944567, 0.21316260599996895, 0.2097926129354164, 0.219783989014104, 0.19861484901048243, 0.2043123310431838, 0.22271435509901494, 0.23495933995582163, 0.2335742610739544, 0.23369529098272324, 0.24560998298693448, 0.22108179796487093, 0.26553250406868756, 0.23404733696952462, 0.23908537707757205, 0.22327711794059724, 0.2474698539590463, 0.22767123894300312, 0.24683470197487622, 0.23050741699989885, 0.24116121395491064, 0.23401425196789205, 0.24491081899031997, 0.24022346106357872, 0.25190298596862704, 0.23213516594842076, 0.23881031398195773, 0.24513675807975233, 0.22554334008600563, 0.2563943018903956, 0.224077302031219, 0.2534773970255628, 0.24002932605799288, 0.25148141803219914, 0.23295595007948577, 0.22862003894988447, 0.2369625970022753, 0.22706612606998533, 0.24219334893859923, 0.22714079392608255, 0.2400125820422545, 0.22258985799271613, 0.2807762409793213, 0.22187094297260046, 0.25118832709267735, 0.23350535100325942, 0.2454579690238461, 0.24031445989385247, 0.23446274898014963, 0.26973368192557245, 0.22495882492512465, 0.24988273507915437, 0.23008493205998093, 0.2374217240139842, 0.2401599360164255, 0.24260477093048394, 0.23568027303554118, 0.22584236203692853, 0.25386512896511704, 0.23628102405928075, 0.2441351240267977, 0.23313174303621054, 0.2263604860054329, 0.2307886310154572, 0.2244189550401643, 0.24792122398503125, 0.22573240206111223, 0.2448614399181679, 0.227451577084139, 0.24615381797775626, 0.23580327397212386, 0.23849635396618396, 0.24991231609601527, 0.2318575259996578, 0.24474891205318272, 0.23117472999729216, 0.237788678961806, 0.24633927294053137, 0.23034017498139292, 0.2394231460057199, 0.22643698391038924, 0.24798683798871934, 0.2362927800277248, 0.24898629903327674, 0.23702008405234665, 0.24314290296752006, 0.23187642509583384, 0.2444909509504214, 0.23897948407102376, 0.2227373169735074, 0.2524915389949456, 0.2371801439439878, 0.23464417608920485, 0.2341131060384214, 0.22195720404852182, 0.2519095700699836, 0.2182202780386433, 0.24099047703202814, 0.2309884310234338, 0.2407762580551207, 0.23472059692721814, 0.2387665610294789, 0.23877065407577902, 0.23193839902523905, 0.25372428400442004, 0.23468229197897017, 0.24137716402765363, 0.23249671491794288, 0.24382975697517395, 0.23622034396976233, 0.23024766496382654, 0.23490917903836817, 0.31669861800037324, 0.2300916468957439, 0.250467372010462, 0.24023429700173438, 0.25056451896671206, 0.23695898603182286, 0.2439523380016908, 0.23950265592429787, 0.24383795901667327, 0.2439587169792503, 0.24297618493437767, 0.23575275705661625, 0.23950542497914284, 0.23329099104739726, 0.23610336903948337]
[0.005528911180921237, 0.005300022045213898, 0.006742955452699045, 0.0051849889992312955, 0.0057514033634850584, 0.0051906715459401976, 0.004226696521403606, 0.004937236864035102, 0.005165774068287151, 0.004849040364867754, 0.005299539294686507, 0.004691321611806581, 0.004845828024289486, 0.004781212864062664, 0.004678610976870087, 0.004956019613680176, 0.004686292795337398, 0.0048845702046740125, 0.005167275704231791, 0.004572594271104952, 0.006862245933999392, 0.004360623229083351, 0.00499689384278926, 0.004334502612685107, 0.005352501635736024, 0.0050767674772280525, 0.004922243795590475, 0.0049167923174205826, 0.004412918589712883, 0.005019267841072922, 0.004740555273284289, 0.004539511409926821, 0.005280038953060284, 0.004982692816569893, 0.005064221728720109, 0.004706231660251929, 0.005107007113772191, 0.004669075477233326, 0.005011685660892082, 0.004633414156904275, 0.004986492136429826, 0.005009137316268276, 0.005109551455296407, 0.005052193319319154, 0.005028116024649617, 0.004594565318918533, 0.005231021794977344, 0.004531442388807508, 0.005772191295611926, 0.005158922386313366, 0.00461362745475278, 0.0052034999534953386, 0.004337358296404339, 0.004688413408902389, 0.005178661932322112, 0.004524913206379014, 0.0050859954763754186, 0.004468992273111574, 0.005016189161158929, 0.004575181387322532, 0.004818029069362886, 0.005107165250757878, 0.004474482043984939, 0.005114369274286384, 0.004489586707098748, 0.005163394433276897, 0.004857425294837661, 0.004787741091356359, 0.005019243613456969, 0.004693536590573124, 0.006698249818079851, 0.004574896749214862, 0.005203109181655402, 0.0047308006350332025, 0.0049236365911466155, 0.005053032933078198, 0.005254826683085412, 0.005014552158007229, 0.004912177021403543, 0.00519710870470259, 0.005353746478001333, 0.004828292453153567, 0.005211275319611145, 0.004570926431121982, 0.0048693630907853894, 0.006549901818454435, 0.0045746109998700294, 0.005304796478330073, 0.00489051318039525, 0.005251647044629367, 0.004379146068293432, 0.004959638204044578, 0.005230978567851707, 0.004728759067471732, 0.005171960500195961, 0.004411575045775284, 0.00512907522815195, 0.004764858704716476, 0.004895484818420796, 0.0045310487065845255, 0.0049175689982380285, 0.004888765814410913, 0.004945831909385878, 0.005206486467423654, 0.00631773583645131, 0.005137974837643289, 0.004865163698965727, 0.005169259603568461, 0.004290598883228593, 0.004600721882482947, 0.005136419767188991, 0.004398393463668262, 0.005005214650313868, 0.004790423630173643, 0.0050187089540021015, 0.004667650231398469, 0.0051206214201831544, 0.004877176674537707, 0.004943110373650872, 0.004736191579573896, 0.0046852611405991534, 0.007171738489401029, 0.004638328230506633, 0.005090151022751491, 0.004984583954738323, 0.005166706791537446, 0.004481041441173401, 0.004611802861354378, 0.00465677069705846, 0.00494055453482149, 0.005183089697776839, 0.00452367341698169, 0.005241360837528699, 0.004917443673083082, 0.005109746929604647, 0.00471067106957699, 0.00475950830421129, 0.0046484042094413975, 0.006601230466066924, 0.0052635113946920215, 0.0053905458839306995, 0.005019153487262165, 0.004729244349047888, 0.004615613882061701, 0.005130091606271129, 0.004319816278185435, 0.005460880837554849, 0.004827752465728757, 0.0051102326034979766, 0.0044603351162486645, 0.00498836146678429, 0.004752961420562378, 0.005212734629356757, 0.004813958953004764, 0.004921022464174691, 0.006884852581289272, 0.004535458510914861, 0.0050927761151606955, 0.004575787209580804, 0.004624232487363177, 0.004857528210227746, 0.004572530558636022, 0.004861260022491563, 0.004875894278547791, 0.004281411743397976, 0.004545499489447752, 0.005145409417901795, 0.004939054953865707, 0.005045672234277739, 0.004752160044560252, 0.004932788815241047, 0.0051614219284897974, 0.006713831326166211, 0.005026841557822948, 0.004636358304577338, 0.004724820185084503, 0.0047983738842831795, 0.004714372906249103, 0.004988995719116268, 0.0046566504643970105, 0.00478545488345675, 0.0053299253963497144, 0.004662113673010364, 0.004957269906976022, 0.004878897975242242, 0.005111255558467535, 0.004618949976987963, 0.004751449559143809, 0.005179403606953836, 0.005464170696647014, 0.005431959559859405, 0.00543477420890054, 0.005711860069463593, 0.005141437161973742, 0.006175174513225292, 0.005442961324872666, 0.005560125048315629, 0.00519249111489761, 0.005755112882768519, 0.005294679975418677, 0.00574034190639247, 0.005360637604648811, 0.0056084003245328055, 0.0054421919062300475, 0.005695600441635348, 0.005586592117757644, 0.005858208976014582, 0.005398492231358623, 0.005553728232138552, 0.0057008548390640075, 0.005245193955488503, 0.005962658183497571, 0.005211100047237651, 0.005894823186640996, 0.005582077350185881, 0.00584840507051626, 0.005417580234406646, 0.005316745091857779, 0.005510758069820355, 0.005280607583022914, 0.0056324034636883544, 0.005282344044792617, 0.005581687954471035, 0.005176508325412003, 0.006529680022774914, 0.005159789371455825, 0.0058415890021552876, 0.005430357000075801, 0.005708324861019676, 0.005588708369624476, 0.005452622069305805, 0.006272876323850522, 0.005231600579654061, 0.005811226397189636, 0.005350812373487929, 0.0055214354421856794, 0.005585114791079662, 0.005641971416987999, 0.0054809365822218875, 0.0052521479543471755, 0.005903840208491094, 0.005494907536262343, 0.005677561023879016, 0.005421668442702571, 0.005264197348963556, 0.005367177465475749, 0.005219045466050332, 0.005765609860117006, 0.005249590745607261, 0.0056944520911201835, 0.005289571560096256, 0.005724507394831541, 0.0054837970691191595, 0.0055464268364228825, 0.005811914327814308, 0.0053920354883641355, 0.005691835164027505, 0.005376156511564934, 0.005529969278181535, 0.00572882030094259, 0.005356748255381231, 0.005567980139667905, 0.005265976370009052, 0.00576713576717952, 0.005495180930877321, 0.005790379047285505, 0.00551209497796155, 0.005654486115523722, 0.005392475002228694, 0.005685836068614451, 0.005557662420256367, 0.005179937604035056, 0.005871896255696409, 0.005515817301022971, 0.005456841304400112, 0.005444490838102823, 0.005161795442988879, 0.005858362094650782, 0.005074890186945193, 0.005604429698419259, 0.005371823977289158, 0.005599447861746993, 0.00545861853319112, 0.005552710721615789, 0.005552805908739047, 0.005393916256400908, 0.0059005647442888385, 0.005457727720441167, 0.005613422419247759, 0.005406900346928904, 0.005670459464538929, 0.005493496371389821, 0.005354596859623873, 0.005463004163682981, 0.007365084139543564, 0.00535096853245916, 0.005824822604894465, 0.005586844116319404, 0.005827081836435164, 0.005510674093763323, 0.0056733101860858325, 0.005569829207541811, 0.005670650209690076, 0.00567345853440117, 0.005650608951962272, 0.005482622257130611, 0.005569893604166112, 0.005425371884823192, 0.005490776024174032]
[180.8674379597066, 188.67846047980768, 148.30292251148177, 192.86444004958463, 173.87060805870007, 192.65329951037538, 236.59138879171735, 202.54243973677222, 193.58183048287603, 206.2263715611022, 188.6956477523684, 213.15954921600652, 206.3630807753694, 209.15195127921703, 213.73865126717232, 201.77482696793308, 213.3882887119099, 204.72630305182363, 193.5255746429478, 218.69423366931554, 145.724885062111, 229.32501788516376, 200.12432352211053, 230.70697825246597, 186.82852767825258, 196.9757339656624, 203.15938046299866, 203.38463279340093, 226.607398181135, 199.2322449535268, 210.94575262850867, 220.28802434844425, 189.3925421554712, 200.6946919694729, 197.46370786429446, 212.48422776248736, 195.8094002460415, 214.17516270106495, 199.5336634544633, 215.82357331685853, 200.54177819599832, 199.6351740552769, 195.71189540785036, 197.93383522678866, 198.88164773796856, 217.64844562823183, 191.1672402053777, 220.68028548039413, 173.2444315835841, 193.83893090793586, 216.74918701332047, 192.17834321844697, 230.55508253237875, 213.2917711781119, 193.1000735457547, 220.99871409472476, 196.61834239629707, 223.76409241444975, 199.35452349826502, 218.57056919555615, 207.55374980172869, 195.80333725281454, 223.4895548065286, 195.5275316210974, 222.73765164593914, 193.67104584442137, 205.87038179728114, 208.86676637660491, 199.23320663673806, 213.0589547354292, 149.29272976663393, 218.58416808458438, 192.19277648943046, 211.38071061263008, 203.1019108514506, 197.90094646994942, 190.3012335723397, 199.41960288581336, 203.57572531339125, 192.41467839515317, 186.78508668817676, 207.11255784575698, 191.89160784439574, 218.7740308378885, 205.36566720447786, 152.67404424024903, 218.59782176635593, 188.50864572938258, 204.47751863929756, 190.4164524961092, 228.3550227384179, 201.62761049475372, 191.1688199500092, 211.47197091913966, 193.3502778998623, 226.67641140042343, 194.96692006217827, 209.86981188133748, 204.26986030825518, 220.69945938713897, 203.3525102257439, 204.5506039688461, 202.19045416854243, 192.06810701552325, 158.28455413256134, 194.6292131821114, 205.54292966803717, 193.45130186723, 233.0676968916561, 217.35719427150278, 194.68813791036203, 227.35573983096992, 199.79163130142516, 208.749805278443, 199.2544316008928, 214.24056011591753, 195.2887975780549, 205.03665680611968, 202.3017744718943, 211.14010765796928, 213.43527500200756, 139.43620524896164, 215.5949191829343, 196.45782522567436, 200.61854892611538, 193.54688399154003, 223.16240836597598, 216.83494070826856, 214.74108670020397, 202.40642886378578, 192.93511366954075, 221.05928253928317, 190.79014610860105, 203.35769283413703, 195.7044084133091, 212.28397933753382, 210.10573699707257, 215.12759109220644, 151.4869091664678, 189.9872395086763, 185.50996903319484, 199.23678415849315, 211.45027116252183, 216.6559044044907, 194.9282930498901, 231.49132639040286, 183.12064111030077, 207.13572352741753, 195.6858087664142, 224.19840077869384, 200.46662750056132, 210.39514347282002, 191.83788761627375, 207.72923279202928, 203.20980188163213, 145.24639245256546, 220.48487437233484, 196.3565602310885, 218.54163102388054, 216.25210296686842, 205.86602006643105, 218.6972808987193, 205.7079842208205, 205.09058295206398, 233.56781826508987, 219.99782473223718, 194.3479942569433, 202.46788289272192, 198.18964720032884, 210.43062325829905, 202.72507854182962, 193.74505976352032, 148.9462501243725, 198.9320706644841, 215.6865225478216, 211.64826614076006, 208.4039351905126, 212.11728895575, 200.4411421257215, 214.74663122036367, 208.96655058999426, 187.6198868908871, 214.49498449365177, 201.7239365144854, 204.96431880200345, 195.64664465727117, 216.49942194266936, 210.46208900093967, 193.0724222104271, 183.01038812964447, 184.09562681388647, 184.00028438390285, 175.0743169193063, 194.49814682089993, 161.93874324657753, 183.7235174592012, 179.85207010819616, 192.58578933932733, 173.7585379070699, 188.86882769924634, 174.20565121502528, 186.5449735182225, 178.30396229486394, 183.74949234245707, 175.57411378261554, 179.0000019549273, 170.70063633686098, 185.2369063701205, 180.05922475881286, 175.41228960044606, 190.65071920812633, 167.7104353839416, 191.89806200901657, 169.64037229585213, 179.14477662454829, 170.9867883538591, 184.58425288269382, 188.08499988675962, 181.463237422179, 189.3721478594598, 177.54409932578844, 189.3098956675889, 179.15727431501819, 193.18040987027854, 153.1468611803478, 193.80636068829529, 171.18629873328032, 184.14995551600776, 175.1827417582135, 178.93222080349727, 183.39800325228003, 159.41650183630006, 191.14609090935699, 172.0807161262224, 186.88750982089655, 181.11232313967736, 179.04734950070548, 177.2430106591813, 182.45056935043303, 190.39829203065486, 169.38127806402477, 181.98668374321795, 176.1319686030924, 184.44506715381587, 189.96248311186235, 186.31767002907543, 191.60591845864482, 173.44218985703384, 190.4910398656843, 175.60952028367757, 189.0512281833659, 174.68751999567078, 182.3553985305707, 180.29625730084288, 172.06034769202643, 185.45872002474252, 175.69026002720827, 186.00648955231256, 180.83283101508263, 174.55600760168113, 186.6804173586895, 179.59834175336044, 189.89830750005456, 173.39629937116268, 181.97762959560046, 172.70026570519488, 181.41922517630746, 176.85073047657121, 185.44360420524953, 175.87562988668512, 179.93176346142153, 193.0525184745512, 170.3027363655967, 181.2967952753871, 183.25619973475352, 183.67190426726074, 193.7310401089744, 170.69617477436077, 197.04859872090069, 178.43028707846082, 186.1565092653391, 178.58903675692164, 183.19653478613716, 180.09221984267336, 180.08913267186102, 185.39405368285236, 169.4753033542935, 183.22643620615918, 178.144441182819, 184.948849772679, 176.35255242606888, 182.03343233427967, 186.75542271734042, 183.04946693026724, 135.77577405137058, 186.88205582484093, 171.67904807259245, 178.99192803303004, 171.61248598694306, 181.46600270405122, 176.2639388998271, 179.5387188256963, 176.3466204089238, 176.25932999007094, 176.97207654986155, 182.39447350205754, 179.5366430791479, 184.31916211999706, 182.12361888325765]
Elapsed: 0.22535780106760067~0.021309390969970584
Time per graph: 0.005203376905588829~0.0005058272008849793
Speed: 193.90268789898738~17.893805282831813
Total Time: 0.2369
best val loss: 0.27788203954696655 test_score: 0.8140

Testing...
Test loss: 0.5523 score: 0.8372 time: 0.31s
test Score 0.8372
Epoch Time List: [0.7732664779759943, 0.7813887620577589, 0.8288674730574712, 0.7950316090136766, 0.7965285470709205, 0.8897204200038686, 0.7090583479730412, 0.7811067568836734, 0.7758594719925895, 0.7341011859243736, 0.8235915959812701, 0.737911434029229, 0.8137491289526224, 0.7548164819600061, 0.8023644528584555, 0.7700347149511799, 0.7656643980881199, 0.8406777619384229, 0.7675775809912011, 0.8128744401037693, 0.8440270129358396, 0.7188111690338701, 0.7738049409817904, 0.7840968408854678, 0.785351003985852, 0.7677554361289367, 0.736225773114711, 0.7842672599945217, 0.8653467500116676, 0.7812117339344695, 0.7350670748855919, 0.7577877099392936, 0.7985238520195708, 0.7334894760278985, 0.803243687027134, 0.8349421621533111, 0.7916383527917787, 0.737575912149623, 0.851187521009706, 0.7242460489505902, 0.7560415827902034, 0.8183124138740823, 0.7380108040524647, 0.7303960169665515, 0.7931049490580335, 0.7387767949840054, 0.9567796410992742, 0.7186160059645772, 0.8875752170570195, 0.7623694220092148, 0.7596321690361947, 0.7851218339055777, 0.7118690209463239, 0.7969890109961852, 0.7611447629751638, 0.816726791090332, 0.7826309739612043, 0.7794539500027895, 0.7711630179546773, 0.7414236011682078, 0.7703469479456544, 0.8183632269501686, 0.7269962569698691, 0.8690970919560641, 0.7338661990361288, 0.7606935399817303, 0.7453950489871204, 0.7991215400397778, 0.805327538982965, 0.762273985077627, 0.8432232819031924, 0.7705078560393304, 0.7762300530448556, 0.7308651158818975, 0.8054208350367844, 0.9440757369156927, 0.8434858899563551, 0.738293560105376, 0.9748748468700796, 0.8227267819456756, 0.792995274066925, 0.7344594849273562, 0.8107915088767186, 0.8026043430436403, 0.7909117179224268, 0.8250153720146045, 0.7692621680907905, 0.7962496160762385, 0.759355018963106, 0.8079231270821765, 0.7304338549729437, 0.7853603980038315, 0.7962508279597387, 0.7190112030366436, 0.8057455769740045, 0.72474109090399, 0.8615275900810957, 0.8660373939201236, 0.8607163380365819, 0.7651457510655746, 0.7937388708814979, 0.769650814938359, 0.7518147090449929, 0.8489000670379028, 0.8504795398330316, 0.7862286239396781, 0.7412116770865396, 0.8212429590057582, 0.7617201040266082, 0.7539216190343723, 0.7694299820577726, 0.723049662890844, 0.8997155219549313, 0.8584593860432506, 0.7913590649841353, 0.7286333000520244, 0.9512326279655099, 0.7441427300218493, 0.955275894026272, 0.7366696598473936, 0.820235259947367, 0.8832085480680689, 0.7989495259243995, 0.7682132769841701, 0.7629421029705554, 0.8255186141468585, 0.7300417899386957, 0.7782565668458119, 0.7699648708803579, 0.7377288189018145, 0.906755656003952, 0.7447551351506263, 0.8493402630556375, 0.7440091291209683, 0.7975087690865621, 0.7413135250099003, 0.7895126298535615, 0.75007045106031, 0.7997397780418396, 0.7698245899518952, 0.7455802869517356, 0.8314993060193956, 0.7452866779640317, 0.7528035989962518, 0.7973252580268309, 0.7093929649563506, 0.9289047050988302, 0.8442672158125788, 0.8132702020229772, 0.7741190451197326, 0.9496016580378637, 0.7644116439623758, 0.8528837249614298, 0.7725550329778343, 0.7454468799987808, 0.9002405379433185, 0.7594567449996248, 0.7621744051575661, 0.722983960993588, 0.7725384400691837, 0.8628053279826418, 0.7807934479787946, 0.9005882239434868, 0.8738047359511256, 0.8165225811535493, 0.7627214901149273, 0.7819654460763559, 0.7476258500246331, 0.7980920620029792, 0.7383388559101149, 0.8277270100079477, 0.7609620769508183, 0.8045150911202654, 0.7495339361485094, 0.7512870909413323, 0.8885110749397427, 0.7867105369223282, 0.8957934479694813, 0.7562049580737948, 0.7500790630001575, 0.7971631409600377, 0.8612333229975775, 0.7645560872042552, 0.7760440589627251, 0.7391999771352857, 0.7926183650270104, 0.7382275080308318, 0.7695347709814087, 0.8680282189743593, 0.8841431350447237, 0.8385549461236224, 0.7798323557944968, 0.7845685951178893, 0.757053637993522, 0.8253978381399065, 0.7362095090793446, 0.7435585770290345, 0.9271390830399469, 0.7937038859818131, 0.7973562371917069, 0.7863611920038238, 0.7960622949758545, 0.7821335120825097, 0.8025170300388709, 0.803438026108779, 0.8780482240254059, 0.8007267529610544, 0.7645099648507312, 0.8179825630504638, 0.7806767601286992, 0.7882291229907423, 0.8070150580024347, 0.7646480799885467, 0.8781135330209509, 0.760164197999984, 0.7838514940813184, 0.7538570589385927, 0.7823248129570857, 0.8700385160045698, 0.7754965340718627, 0.8036816939711571, 0.8314044920261949, 0.956483079935424, 0.764213275979273, 0.9129668299574405, 0.7883055250858888, 0.8094376699300483, 0.7278467259602621, 0.8805478868307546, 0.8437591649126261, 0.7655878548976034, 0.766587584046647, 0.834807067993097, 0.7677367279538885, 0.7221379530383274, 0.8077433317666873, 0.7587135649519041, 0.8963727119844407, 0.7629388849018142, 0.7932242270326242, 0.7923198338830844, 0.7451366310706362, 0.7748458910500631, 0.7579917780822143, 0.7694771338719875, 0.8616374750854447, 0.7679394769947976, 0.831302692880854, 0.7700712820515037, 0.8941219870466739, 0.7880200828658417, 0.9037509590853006, 0.7994594490155578, 0.9540544910123572, 0.7803177930181846, 0.7707445521373302, 0.8182984020095319, 0.7824799530208111, 0.7944337670924142, 0.8070059210294858, 0.7914273130008951, 0.8713523941114545, 0.7772646548692137, 0.7870927670737728, 0.7504915880272165, 0.7997970429714769, 0.7722966410219669, 0.796103987027891, 0.770646104007028, 0.8641397450119257, 0.7506247628480196, 0.7723178790183738, 0.7842955390224233, 0.7439725399017334, 0.7801429919200018, 0.7538752119289711, 0.752527195843868, 0.840762653038837, 0.7150777621427551, 0.781142724910751, 0.811122830840759, 0.8523491419618949, 0.7708763839909807, 0.8326756281312555, 0.7933470159769058, 0.8420092440210283, 0.7881223901640624, 0.7433921330375597, 0.7692194040864706, 0.777619338943623, 0.9112021790351719, 0.7837255958002061, 0.8471924040932208, 0.8646932659903541, 1.0207740869373083, 0.7544793979031965, 0.9860740309813991, 0.8097275570034981, 0.7864346829010174, 0.783748630201444, 0.7878008109983057, 0.839666545856744, 0.7739865659968928, 0.7720833549974486, 0.8295400600181893, 0.7664714028360322, 0.8270601420663297, 0.7651303159072995, 0.7879679278703406]
Total Epoch List: [100, 89, 120]
Total Time List: [0.19983986602164805, 0.22330781805794686, 0.2369171449681744]
========================training times:9========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcf970>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4884 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.20s
Epoch 9/1000, LR 0.000210
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.19s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.22s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.21s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.21s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.21s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4884 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.26s
Test loss: 0.6917 score: 0.5227 time: 0.19s
Epoch 24/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.24s
Test loss: 0.6915 score: 0.5227 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4884 time: 0.27s
Test loss: 0.6912 score: 0.5455 time: 0.21s
Epoch 26/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.31s
Val loss: 0.6915 score: 0.5349 time: 0.25s
Test loss: 0.6910 score: 0.5682 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.39s
Val loss: 0.6911 score: 0.5349 time: 0.33s
Test loss: 0.6906 score: 0.5682 time: 0.20s
Epoch 28/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.30s
Val loss: 0.6908 score: 0.5581 time: 0.25s
Test loss: 0.6903 score: 0.5909 time: 0.20s
Epoch 29/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.35s
Val loss: 0.6903 score: 0.5814 time: 0.23s
Test loss: 0.6899 score: 0.6136 time: 0.21s
Epoch 30/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.26s
Val loss: 0.6898 score: 0.6047 time: 0.26s
Test loss: 0.6894 score: 0.6136 time: 0.21s
Epoch 31/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.33s
Val loss: 0.6893 score: 0.6279 time: 0.25s
Test loss: 0.6889 score: 0.6136 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.28s
Val loss: 0.6887 score: 0.6279 time: 0.26s
Test loss: 0.6883 score: 0.6364 time: 0.19s
Epoch 33/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.37s
Val loss: 0.6879 score: 0.6279 time: 0.24s
Test loss: 0.6876 score: 0.6364 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.29s
Val loss: 0.6871 score: 0.6744 time: 0.26s
Test loss: 0.6869 score: 0.6818 time: 0.21s
Epoch 35/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.29s
Val loss: 0.6862 score: 0.6977 time: 0.26s
Test loss: 0.6860 score: 0.7045 time: 0.20s
Epoch 36/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.34s
Val loss: 0.6852 score: 0.7209 time: 0.32s
Test loss: 0.6851 score: 0.7273 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.29s
Val loss: 0.6840 score: 0.7442 time: 0.26s
Test loss: 0.6840 score: 0.7273 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.32s
Val loss: 0.6827 score: 0.7674 time: 0.24s
Test loss: 0.6827 score: 0.7273 time: 0.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.26s
Val loss: 0.6812 score: 0.7907 time: 0.25s
Test loss: 0.6813 score: 0.7500 time: 0.20s
Epoch 40/1000, LR 0.000269
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.41s
Val loss: 0.6796 score: 0.8140 time: 0.23s
Test loss: 0.6798 score: 0.7500 time: 0.21s
Epoch 41/1000, LR 0.000269
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.29s
Val loss: 0.6778 score: 0.8140 time: 0.34s
Test loss: 0.6781 score: 0.7955 time: 0.20s
Epoch 42/1000, LR 0.000269
Train loss: 0.6753;  Loss pred: 0.6753; Loss self: 0.0000; time: 0.40s
Val loss: 0.6758 score: 0.8140 time: 0.24s
Test loss: 0.6762 score: 0.8182 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6729;  Loss pred: 0.6729; Loss self: 0.0000; time: 0.31s
Val loss: 0.6736 score: 0.8372 time: 0.32s
Test loss: 0.6742 score: 0.8182 time: 0.20s
Epoch 44/1000, LR 0.000269
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 0.26s
Val loss: 0.6711 score: 0.9070 time: 0.26s
Test loss: 0.6719 score: 0.8182 time: 0.21s
Epoch 45/1000, LR 0.000269
Train loss: 0.6665;  Loss pred: 0.6665; Loss self: 0.0000; time: 0.35s
Val loss: 0.6685 score: 0.8837 time: 0.25s
Test loss: 0.6694 score: 0.8182 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 0.30s
Val loss: 0.6655 score: 0.8837 time: 0.25s
Test loss: 0.6666 score: 0.8182 time: 0.20s
Epoch 47/1000, LR 0.000269
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.37s
Val loss: 0.6623 score: 0.8837 time: 0.25s
Test loss: 0.6636 score: 0.8864 time: 0.21s
Epoch 48/1000, LR 0.000269
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.30s
Val loss: 0.6588 score: 0.8837 time: 0.24s
Test loss: 0.6603 score: 0.8864 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6541;  Loss pred: 0.6541; Loss self: 0.0000; time: 0.36s
Val loss: 0.6550 score: 0.8837 time: 0.26s
Test loss: 0.6567 score: 0.8636 time: 0.20s
Epoch 50/1000, LR 0.000269
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.30s
Val loss: 0.6509 score: 0.8837 time: 0.23s
Test loss: 0.6528 score: 0.8409 time: 0.30s
Epoch 51/1000, LR 0.000269
Train loss: 0.6463;  Loss pred: 0.6463; Loss self: 0.0000; time: 0.30s
Val loss: 0.6465 score: 0.8837 time: 0.26s
Test loss: 0.6485 score: 0.8409 time: 0.20s
Epoch 52/1000, LR 0.000269
Train loss: 0.6400;  Loss pred: 0.6400; Loss self: 0.0000; time: 0.33s
Val loss: 0.6417 score: 0.8605 time: 0.23s
Test loss: 0.6440 score: 0.8409 time: 0.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6330;  Loss pred: 0.6330; Loss self: 0.0000; time: 0.32s
Val loss: 0.6366 score: 0.8605 time: 0.26s
Test loss: 0.6391 score: 0.8636 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.35s
Val loss: 0.6312 score: 0.8605 time: 0.25s
Test loss: 0.6339 score: 0.8636 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.6242;  Loss pred: 0.6242; Loss self: 0.0000; time: 0.30s
Val loss: 0.6253 score: 0.8605 time: 0.25s
Test loss: 0.6283 score: 0.8636 time: 0.20s
Epoch 56/1000, LR 0.000269
Train loss: 0.6193;  Loss pred: 0.6193; Loss self: 0.0000; time: 0.32s
Val loss: 0.6191 score: 0.8605 time: 0.27s
Test loss: 0.6223 score: 0.8636 time: 0.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.6082;  Loss pred: 0.6082; Loss self: 0.0000; time: 0.32s
Val loss: 0.6124 score: 0.8605 time: 0.23s
Test loss: 0.6159 score: 0.8636 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6049;  Loss pred: 0.6049; Loss self: 0.0000; time: 0.50s
Val loss: 0.6054 score: 0.8605 time: 0.24s
Test loss: 0.6092 score: 0.8636 time: 0.21s
Epoch 59/1000, LR 0.000268
Train loss: 0.5921;  Loss pred: 0.5921; Loss self: 0.0000; time: 0.31s
Val loss: 0.5980 score: 0.8605 time: 0.25s
Test loss: 0.6022 score: 0.8636 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5889;  Loss pred: 0.5889; Loss self: 0.0000; time: 0.34s
Val loss: 0.5902 score: 0.8605 time: 0.25s
Test loss: 0.5948 score: 0.8636 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.32s
Val loss: 0.5822 score: 0.8605 time: 0.24s
Test loss: 0.5871 score: 0.8636 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.31s
Val loss: 0.5738 score: 0.8605 time: 0.30s
Test loss: 0.5791 score: 0.8636 time: 0.20s
Epoch 63/1000, LR 0.000268
Train loss: 0.5623;  Loss pred: 0.5623; Loss self: 0.0000; time: 0.31s
Val loss: 0.5651 score: 0.8837 time: 0.25s
Test loss: 0.5707 score: 0.8636 time: 0.22s
Epoch 64/1000, LR 0.000268
Train loss: 0.5529;  Loss pred: 0.5529; Loss self: 0.0000; time: 0.28s
Val loss: 0.5563 score: 0.8837 time: 0.26s
Test loss: 0.5622 score: 0.8636 time: 0.20s
Epoch 65/1000, LR 0.000268
Train loss: 0.5362;  Loss pred: 0.5362; Loss self: 0.0000; time: 0.41s
Val loss: 0.5472 score: 0.8837 time: 0.24s
Test loss: 0.5535 score: 0.8864 time: 0.21s
Epoch 66/1000, LR 0.000268
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 0.30s
Val loss: 0.5381 score: 0.8605 time: 0.25s
Test loss: 0.5447 score: 0.9091 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.5149;  Loss pred: 0.5149; Loss self: 0.0000; time: 0.27s
Val loss: 0.5289 score: 0.8605 time: 0.27s
Test loss: 0.5358 score: 0.9091 time: 0.20s
Epoch 68/1000, LR 0.000268
Train loss: 0.5081;  Loss pred: 0.5081; Loss self: 0.0000; time: 0.31s
Val loss: 0.5196 score: 0.8605 time: 0.24s
Test loss: 0.5269 score: 0.9091 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 0.28s
Val loss: 0.5103 score: 0.8605 time: 0.35s
Test loss: 0.5180 score: 0.9091 time: 0.20s
Epoch 70/1000, LR 0.000268
Train loss: 0.4945;  Loss pred: 0.4945; Loss self: 0.0000; time: 0.34s
Val loss: 0.5009 score: 0.8605 time: 0.23s
Test loss: 0.5089 score: 0.9091 time: 0.22s
Epoch 71/1000, LR 0.000268
Train loss: 0.4714;  Loss pred: 0.4714; Loss self: 0.0000; time: 0.28s
Val loss: 0.4914 score: 0.8605 time: 0.26s
Test loss: 0.4998 score: 0.9091 time: 0.19s
Epoch 72/1000, LR 0.000267
Train loss: 0.4694;  Loss pred: 0.4694; Loss self: 0.0000; time: 0.31s
Val loss: 0.4817 score: 0.8605 time: 0.25s
Test loss: 0.4903 score: 0.9091 time: 0.21s
Epoch 73/1000, LR 0.000267
Train loss: 0.4633;  Loss pred: 0.4633; Loss self: 0.0000; time: 0.35s
Val loss: 0.4720 score: 0.8605 time: 0.26s
Test loss: 0.4809 score: 0.9091 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.4520;  Loss pred: 0.4520; Loss self: 0.0000; time: 0.29s
Val loss: 0.4623 score: 0.8605 time: 0.25s
Test loss: 0.4716 score: 0.9091 time: 0.20s
Epoch 75/1000, LR 0.000267
Train loss: 0.4283;  Loss pred: 0.4283; Loss self: 0.0000; time: 0.33s
Val loss: 0.4527 score: 0.8605 time: 0.23s
Test loss: 0.4623 score: 0.9091 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.4115;  Loss pred: 0.4115; Loss self: 0.0000; time: 0.26s
Val loss: 0.4433 score: 0.8605 time: 0.27s
Test loss: 0.4534 score: 0.9091 time: 0.19s
Epoch 77/1000, LR 0.000267
Train loss: 0.4192;  Loss pred: 0.4192; Loss self: 0.0000; time: 0.34s
Val loss: 0.4344 score: 0.8605 time: 0.25s
Test loss: 0.4449 score: 0.9091 time: 0.21s
Epoch 78/1000, LR 0.000267
Train loss: 0.3981;  Loss pred: 0.3981; Loss self: 0.0000; time: 0.39s
Val loss: 0.4253 score: 0.8605 time: 0.25s
Test loss: 0.4359 score: 0.9091 time: 0.20s
Epoch 79/1000, LR 0.000267
Train loss: 0.3789;  Loss pred: 0.3789; Loss self: 0.0000; time: 0.33s
Val loss: 0.4171 score: 0.8605 time: 0.24s
Test loss: 0.4280 score: 0.9091 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.3746;  Loss pred: 0.3746; Loss self: 0.0000; time: 0.30s
Val loss: 0.4087 score: 0.8605 time: 0.24s
Test loss: 0.4198 score: 0.9091 time: 0.20s
Epoch 81/1000, LR 0.000267
Train loss: 0.3597;  Loss pred: 0.3597; Loss self: 0.0000; time: 0.25s
Val loss: 0.3993 score: 0.8605 time: 0.33s
Test loss: 0.4104 score: 0.9091 time: 0.20s
Epoch 82/1000, LR 0.000267
Train loss: 0.3513;  Loss pred: 0.3513; Loss self: 0.0000; time: 0.34s
Val loss: 0.3905 score: 0.8605 time: 0.23s
Test loss: 0.4017 score: 0.9091 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.3352;  Loss pred: 0.3352; Loss self: 0.0000; time: 0.27s
Val loss: 0.3819 score: 0.8605 time: 0.26s
Test loss: 0.3932 score: 0.9091 time: 0.19s
Epoch 84/1000, LR 0.000266
Train loss: 0.3389;  Loss pred: 0.3389; Loss self: 0.0000; time: 0.34s
Val loss: 0.3749 score: 0.8605 time: 0.24s
Test loss: 0.3860 score: 0.9091 time: 0.21s
Epoch 85/1000, LR 0.000266
Train loss: 0.3013;  Loss pred: 0.3013; Loss self: 0.0000; time: 0.33s
Val loss: 0.3680 score: 0.8605 time: 0.25s
Test loss: 0.3791 score: 0.9091 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.3062;  Loss pred: 0.3062; Loss self: 0.0000; time: 0.28s
Val loss: 0.3616 score: 0.8605 time: 0.33s
Test loss: 0.3726 score: 0.9091 time: 0.21s
Epoch 87/1000, LR 0.000266
Train loss: 0.2919;  Loss pred: 0.2919; Loss self: 0.0000; time: 0.33s
Val loss: 0.3543 score: 0.8605 time: 0.24s
Test loss: 0.3650 score: 0.9091 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.3015;  Loss pred: 0.3015; Loss self: 0.0000; time: 0.26s
Val loss: 0.3463 score: 0.8605 time: 0.27s
Test loss: 0.3564 score: 0.9091 time: 0.20s
Epoch 89/1000, LR 0.000266
Train loss: 0.2518;  Loss pred: 0.2518; Loss self: 0.0000; time: 0.41s
Val loss: 0.3383 score: 0.8605 time: 0.23s
Test loss: 0.3477 score: 0.9091 time: 0.27s
Epoch 90/1000, LR 0.000266
Train loss: 0.2888;  Loss pred: 0.2888; Loss self: 0.0000; time: 0.31s
Val loss: 0.3327 score: 0.8605 time: 0.28s
Test loss: 0.3414 score: 0.9091 time: 0.20s
Epoch 91/1000, LR 0.000266
Train loss: 0.2775;  Loss pred: 0.2775; Loss self: 0.0000; time: 0.35s
Val loss: 0.3293 score: 0.8605 time: 0.24s
Test loss: 0.3376 score: 0.9091 time: 0.23s
Epoch 92/1000, LR 0.000266
Train loss: 0.2827;  Loss pred: 0.2827; Loss self: 0.0000; time: 0.27s
Val loss: 0.3241 score: 0.8605 time: 0.26s
Test loss: 0.3315 score: 0.9091 time: 0.21s
Epoch 93/1000, LR 0.000265
Train loss: 0.2357;  Loss pred: 0.2357; Loss self: 0.0000; time: 0.33s
Val loss: 0.3214 score: 0.8605 time: 0.25s
Test loss: 0.3284 score: 0.9091 time: 0.32s
Epoch 94/1000, LR 0.000265
Train loss: 0.2283;  Loss pred: 0.2283; Loss self: 0.0000; time: 0.28s
Val loss: 0.3184 score: 0.8605 time: 0.27s
Test loss: 0.3249 score: 0.9091 time: 0.20s
Epoch 95/1000, LR 0.000265
Train loss: 0.2097;  Loss pred: 0.2097; Loss self: 0.0000; time: 0.32s
Val loss: 0.3169 score: 0.8605 time: 0.24s
Test loss: 0.3235 score: 0.8864 time: 0.23s
Epoch 96/1000, LR 0.000265
Train loss: 0.1944;  Loss pred: 0.1944; Loss self: 0.0000; time: 0.27s
Val loss: 0.3153 score: 0.8605 time: 0.26s
Test loss: 0.3225 score: 0.8864 time: 0.19s
Epoch 97/1000, LR 0.000265
Train loss: 0.1998;  Loss pred: 0.1998; Loss self: 0.0000; time: 0.36s
Val loss: 0.3119 score: 0.8605 time: 0.25s
Test loss: 0.3184 score: 0.8864 time: 0.21s
Epoch 98/1000, LR 0.000265
Train loss: 0.1951;  Loss pred: 0.1951; Loss self: 0.0000; time: 0.37s
Val loss: 0.3096 score: 0.8605 time: 0.26s
Test loss: 0.3161 score: 0.8636 time: 0.20s
Epoch 99/1000, LR 0.000265
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.35s
Val loss: 0.3107 score: 0.8605 time: 0.25s
Test loss: 0.3195 score: 0.8864 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 100/1000, LR 0.000265
Train loss: 0.1842;  Loss pred: 0.1842; Loss self: 0.0000; time: 0.33s
Val loss: 0.3134 score: 0.8605 time: 0.23s
Test loss: 0.3263 score: 0.8864 time: 0.31s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 097,   Train_Loss: 0.1951,   Val_Loss: 0.3096,   Val_Precision: 0.8333,   Val_Recall: 0.9091,   Val_accuracy: 0.8696,   Val_Score: 0.8605,   Val_Loss: 0.3096,   Test_Precision: 0.8077,   Test_Recall: 0.9545,   Test_accuracy: 0.8750,   Test_Score: 0.8636,   Test_loss: 0.3161


[0.21000363898929209, 0.21896358602680266, 0.23570401896722615, 0.2334318410139531, 0.2370636040577665, 0.22622855403460562, 0.23879036807920784, 0.20149155799299479, 0.23418466793373227, 0.19052168203052133, 0.22441150003578514, 0.2125962539575994, 0.2113999699940905, 0.21144898899365216, 0.23474077600985765, 0.212449571932666, 0.21736793802119792, 0.23096712806727737, 0.20310729707125574, 0.23237734800204635, 0.20029986696317792, 0.22170401900075376, 0.19601341302040964, 0.2122095429804176, 0.21065234707202762, 0.20147403795272112, 0.20580874290317297, 0.2033492639893666, 0.21524286502972245, 0.2184417420066893, 0.22482139605563134, 0.19556767190806568, 0.22609894908964634, 0.21688176493626088, 0.20031395298428833, 0.22074502299074084, 0.2012494009686634, 0.22444682696368545, 0.20040656509809196, 0.2093083841027692, 0.20876574399881065, 0.23400828195735812, 0.20515659404918551, 0.21028852101881057, 0.22175943898037076, 0.2069441539933905, 0.2149381039198488, 0.22784358193166554, 0.2006136740092188, 0.30937669693958014, 0.20323270501103252, 0.22286396310664713, 0.2341168139828369, 0.22029928700067103, 0.20903773501049727, 0.2067600640002638, 0.22632112598512322, 0.21547106502112, 0.23203869501594454, 0.20580600202083588, 0.21495272195897996, 0.20132685301359743, 0.22247615398373455, 0.20723992097191513, 0.2096125460229814, 0.21129505592398345, 0.20168845495209098, 0.22201196604873985, 0.20908700895961374, 0.22857367899268866, 0.1937701259739697, 0.21317247895058244, 0.2258366229943931, 0.20011368696577847, 0.22843012399971485, 0.1932438190560788, 0.21258020494133234, 0.20241560693830252, 0.21639134001452476, 0.20336082903668284, 0.19963096093852073, 0.2300804880214855, 0.19341301906388253, 0.21454502490814775, 0.2278501879191026, 0.2167616819497198, 0.22253256093245, 0.1996497140498832, 0.2776736009400338, 0.2042603649897501, 0.2361489520408213, 0.20926569204311818, 0.3234124280279502, 0.20709889801219106, 0.23405570001341403, 0.1962262619053945, 0.21679898200090975, 0.20627501199487597, 0.21014493599068373, 0.31567218096461147]
[0.004772809977029366, 0.004976445136972788, 0.005356909521982412, 0.005305269113953479, 0.005387809183131057, 0.005141558046241037, 0.005427053819981997, 0.004579353590749882, 0.005322378816675733, 0.004330038227966394, 0.005100261364449662, 0.004831733044490896, 0.004804544772592966, 0.004805658840764822, 0.005335017636587674, 0.004828399362106045, 0.00494018040957268, 0.00524925291061994, 0.0046160749334376305, 0.005281303363682871, 0.004552269703708589, 0.005038727704562585, 0.004454850295918401, 0.004822944158645855, 0.004787553342546083, 0.004578955408016389, 0.004677471429617567, 0.004621574181576513, 0.004891883296130056, 0.0049645850456065755, 0.005109577183082531, 0.004444719816092402, 0.005138612479310144, 0.004929131021278657, 0.0045525898405520075, 0.005016932340698655, 0.004573850022015077, 0.005101064249174669, 0.004554694661320272, 0.004757008729608391, 0.004744675999972969, 0.005318370044485412, 0.004662649864754217, 0.0047792845686093315, 0.005039987249553881, 0.004703276227122511, 0.004884956907269291, 0.005178263225719671, 0.0045594016820277, 0.007031288566808639, 0.004618925113887103, 0.005065090070605616, 0.005320836681428112, 0.005006801977287978, 0.004750857613874938, 0.0046990923636423595, 0.005143661954207346, 0.00489706965957091, 0.00527360670490783, 0.004677409136837179, 0.004885289135431362, 0.004575610295763578, 0.005056276226903058, 0.004709998203907162, 0.004763921500522305, 0.0048021603619087146, 0.004583828521638431, 0.005045726501107724, 0.004751977476354858, 0.005194856340742924, 0.004403866499408402, 0.004844829067058692, 0.0051326505225998435, 0.004548038340131329, 0.005191593727266247, 0.004391904978547245, 0.004831368294121189, 0.004600354703143239, 0.004917985000330108, 0.004621837023560974, 0.0045370672940572895, 0.005229102000488307, 0.004395750433270057, 0.004876023293366994, 0.005178413361797787, 0.004926401862493632, 0.005057558203010227, 0.004537493501133708, 0.006310763657728041, 0.0046422810224943205, 0.005367021637291394, 0.004756038455525413, 0.007350282455180687, 0.004706793136640706, 0.005319447727577592, 0.004459687770577148, 0.004927249590929767, 0.004688068454428999, 0.00477602127251554, 0.007174367749195715]
[209.52017884910805, 200.94665418301147, 186.67479745484547, 188.49185187795334, 185.60419755230876, 194.4935739335072, 184.26203851490777, 218.37143172782325, 187.88591237941662, 230.94484329059858, 196.06838327351167, 206.9650766695797, 208.1362641689589, 208.0880131392868, 187.44080490793075, 207.1079720223932, 202.42175732333203, 190.50329961752584, 216.63426491547256, 189.34719919263614, 219.6706401611776, 198.462798276338, 224.47443428485457, 207.34223061806534, 208.87495730088037, 218.39042115354462, 213.7907232672847, 216.3764900683428, 204.42024869871588, 201.42670350363986, 195.7109099576641, 224.98606017401454, 194.60506197467714, 202.8755161270986, 219.65519298324242, 199.324992264245, 218.63419114897764, 196.03752298587412, 219.5536856712431, 210.21613724941022, 210.76254732793072, 188.02753317943612, 214.47031816803883, 209.23633770796337, 198.4132003684168, 212.61774807808922, 204.71009652345197, 193.11494151806482, 219.32702353069942, 142.221442129473, 216.5005873321986, 197.42985535505653, 187.94036725284343, 199.72829054079497, 210.48831206380248, 212.80705349338575, 194.41402038134194, 204.20375234924103, 189.62354531849329, 213.79357048850997, 204.6961750426061, 218.55008083312305, 197.774005043331, 212.3143060161793, 209.91109947768084, 208.2396098081427, 218.15824812804357, 198.18751566904447, 210.43870788021474, 192.49810474200495, 227.07318674041005, 206.40563086100838, 194.83111028051636, 219.87501538325293, 192.61907855924872, 227.69162923255686, 206.98070176450847, 217.37454273183323, 203.33530906110477, 216.3641848256979, 220.40669339637373, 191.23742468718672, 227.492441889174, 205.0851564553293, 193.10934259848864, 202.98790636901532, 197.7238738260701, 220.38599058051466, 158.459428087664, 215.41134523189533, 186.3230796484502, 210.25902320832415, 136.04919349666238, 212.45888038192217, 187.98944010967605, 224.23094428213426, 202.9529824997765, 213.3074654776556, 209.379301921178, 139.3851047170122]
Elapsed: 0.21797004160587677~0.02196455251912002
Time per graph: 0.004953864581951744~0.0004991943754345459
Speed: 203.52521236915646~16.77479324671229
Total Time: 0.3161
best val loss: 0.30962496995925903 test_score: 0.8636

Testing...
Test loss: 0.6719 score: 0.8182 time: 0.29s
test Score 0.8182
Epoch Time List: [0.8368686391040683, 0.7383035930106416, 0.8292838400229812, 0.8196518970653415, 0.838839877047576, 0.8075794440228492, 0.8764903380069882, 0.8225866261636838, 0.7991897731553763, 0.6925968959694728, 0.8325671100756153, 0.7813390670344234, 0.8727483669063076, 0.7846036748960614, 0.8837972140172496, 0.7880818189587444, 0.7734641280258074, 0.79556767095346, 0.7935597690520808, 0.7652457249350846, 0.7804519570199773, 0.7732225671643391, 0.7366388400550932, 0.7731690799118951, 0.8243375108577311, 0.7595107068773359, 0.9191975231515244, 0.7539745081448928, 0.7922654909780249, 0.7351377140730619, 0.7982187969610095, 0.7300411571050063, 0.8325525700347498, 0.7641648279968649, 0.7427935739979148, 0.8776815470773727, 0.7476634569466114, 0.775722608086653, 0.7077108951052651, 0.8511541018960997, 0.8364465532358736, 0.8720421210164204, 0.829452923964709, 0.7214425179408863, 0.8154494020855054, 0.758217801922001, 0.8271387219429016, 0.7720118910074234, 0.8134768409654498, 0.8402605470037088, 0.7606590670766309, 0.7769599659368396, 0.805578600964509, 0.8111874921014532, 0.7574346620822325, 0.7923574720043689, 0.7770580038195476, 0.9568957610754296, 0.778582316939719, 0.7918602199060842, 0.7695492669008672, 0.8058116530301049, 0.7736595750320703, 0.7461145739071071, 0.850383830955252, 0.760938894120045, 0.7423859249101952, 0.7641751610208303, 0.8341947509907186, 0.7983638349687681, 0.7224953270051628, 0.76770489604678, 0.8305382641265169, 0.7335952189750969, 0.7833847480360419, 0.7148350598290563, 0.8015345809981227, 0.8400047241011634, 0.7812213968718424, 0.7414344832068309, 0.7714395489310846, 0.799991293111816, 0.7140763450879604, 0.789369965903461, 0.8018272669287398, 0.829650983097963, 0.7835261720465496, 0.7309464910067618, 0.9215434170328081, 0.7909464309923351, 0.8238376850495115, 0.7374784279381856, 0.9014909670222551, 0.7543774700025097, 0.787361147115007, 0.7218258139910176, 0.8260586890392005, 0.8355988051043823, 0.8033900939626619, 0.8686916469596326]
Total Epoch List: [100]
Total Time List: [0.31614621391054243]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcceb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.18s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.26s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.18s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.6930,   Val_Loss: 0.6932,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.5000,   Val_Loss: 0.6932,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.4884,   Test_loss: 0.6934


[0.21000363898929209, 0.21896358602680266, 0.23570401896722615, 0.2334318410139531, 0.2370636040577665, 0.22622855403460562, 0.23879036807920784, 0.20149155799299479, 0.23418466793373227, 0.19052168203052133, 0.22441150003578514, 0.2125962539575994, 0.2113999699940905, 0.21144898899365216, 0.23474077600985765, 0.212449571932666, 0.21736793802119792, 0.23096712806727737, 0.20310729707125574, 0.23237734800204635, 0.20029986696317792, 0.22170401900075376, 0.19601341302040964, 0.2122095429804176, 0.21065234707202762, 0.20147403795272112, 0.20580874290317297, 0.2033492639893666, 0.21524286502972245, 0.2184417420066893, 0.22482139605563134, 0.19556767190806568, 0.22609894908964634, 0.21688176493626088, 0.20031395298428833, 0.22074502299074084, 0.2012494009686634, 0.22444682696368545, 0.20040656509809196, 0.2093083841027692, 0.20876574399881065, 0.23400828195735812, 0.20515659404918551, 0.21028852101881057, 0.22175943898037076, 0.2069441539933905, 0.2149381039198488, 0.22784358193166554, 0.2006136740092188, 0.30937669693958014, 0.20323270501103252, 0.22286396310664713, 0.2341168139828369, 0.22029928700067103, 0.20903773501049727, 0.2067600640002638, 0.22632112598512322, 0.21547106502112, 0.23203869501594454, 0.20580600202083588, 0.21495272195897996, 0.20132685301359743, 0.22247615398373455, 0.20723992097191513, 0.2096125460229814, 0.21129505592398345, 0.20168845495209098, 0.22201196604873985, 0.20908700895961374, 0.22857367899268866, 0.1937701259739697, 0.21317247895058244, 0.2258366229943931, 0.20011368696577847, 0.22843012399971485, 0.1932438190560788, 0.21258020494133234, 0.20241560693830252, 0.21639134001452476, 0.20336082903668284, 0.19963096093852073, 0.2300804880214855, 0.19341301906388253, 0.21454502490814775, 0.2278501879191026, 0.2167616819497198, 0.22253256093245, 0.1996497140498832, 0.2776736009400338, 0.2042603649897501, 0.2361489520408213, 0.20926569204311818, 0.3234124280279502, 0.20709889801219106, 0.23405570001341403, 0.1962262619053945, 0.21679898200090975, 0.20627501199487597, 0.21014493599068373, 0.31567218096461147, 0.18443015799857676, 0.2668528539361432, 0.18811454600654542]
[0.004772809977029366, 0.004976445136972788, 0.005356909521982412, 0.005305269113953479, 0.005387809183131057, 0.005141558046241037, 0.005427053819981997, 0.004579353590749882, 0.005322378816675733, 0.004330038227966394, 0.005100261364449662, 0.004831733044490896, 0.004804544772592966, 0.004805658840764822, 0.005335017636587674, 0.004828399362106045, 0.00494018040957268, 0.00524925291061994, 0.0046160749334376305, 0.005281303363682871, 0.004552269703708589, 0.005038727704562585, 0.004454850295918401, 0.004822944158645855, 0.004787553342546083, 0.004578955408016389, 0.004677471429617567, 0.004621574181576513, 0.004891883296130056, 0.0049645850456065755, 0.005109577183082531, 0.004444719816092402, 0.005138612479310144, 0.004929131021278657, 0.0045525898405520075, 0.005016932340698655, 0.004573850022015077, 0.005101064249174669, 0.004554694661320272, 0.004757008729608391, 0.004744675999972969, 0.005318370044485412, 0.004662649864754217, 0.0047792845686093315, 0.005039987249553881, 0.004703276227122511, 0.004884956907269291, 0.005178263225719671, 0.0045594016820277, 0.007031288566808639, 0.004618925113887103, 0.005065090070605616, 0.005320836681428112, 0.005006801977287978, 0.004750857613874938, 0.0046990923636423595, 0.005143661954207346, 0.00489706965957091, 0.00527360670490783, 0.004677409136837179, 0.004885289135431362, 0.004575610295763578, 0.005056276226903058, 0.004709998203907162, 0.004763921500522305, 0.0048021603619087146, 0.004583828521638431, 0.005045726501107724, 0.004751977476354858, 0.005194856340742924, 0.004403866499408402, 0.004844829067058692, 0.0051326505225998435, 0.004548038340131329, 0.005191593727266247, 0.004391904978547245, 0.004831368294121189, 0.004600354703143239, 0.004917985000330108, 0.004621837023560974, 0.0045370672940572895, 0.005229102000488307, 0.004395750433270057, 0.004876023293366994, 0.005178413361797787, 0.004926401862493632, 0.005057558203010227, 0.004537493501133708, 0.006310763657728041, 0.0046422810224943205, 0.005367021637291394, 0.004756038455525413, 0.007350282455180687, 0.004706793136640706, 0.005319447727577592, 0.004459687770577148, 0.004927249590929767, 0.004688068454428999, 0.00477602127251554, 0.007174367749195715, 0.004289073441827366, 0.006205880324096354, 0.00437475688387315]
[209.52017884910805, 200.94665418301147, 186.67479745484547, 188.49185187795334, 185.60419755230876, 194.4935739335072, 184.26203851490777, 218.37143172782325, 187.88591237941662, 230.94484329059858, 196.06838327351167, 206.9650766695797, 208.1362641689589, 208.0880131392868, 187.44080490793075, 207.1079720223932, 202.42175732333203, 190.50329961752584, 216.63426491547256, 189.34719919263614, 219.6706401611776, 198.462798276338, 224.47443428485457, 207.34223061806534, 208.87495730088037, 218.39042115354462, 213.7907232672847, 216.3764900683428, 204.42024869871588, 201.42670350363986, 195.7109099576641, 224.98606017401454, 194.60506197467714, 202.8755161270986, 219.65519298324242, 199.324992264245, 218.63419114897764, 196.03752298587412, 219.5536856712431, 210.21613724941022, 210.76254732793072, 188.02753317943612, 214.47031816803883, 209.23633770796337, 198.4132003684168, 212.61774807808922, 204.71009652345197, 193.11494151806482, 219.32702353069942, 142.221442129473, 216.5005873321986, 197.42985535505653, 187.94036725284343, 199.72829054079497, 210.48831206380248, 212.80705349338575, 194.41402038134194, 204.20375234924103, 189.62354531849329, 213.79357048850997, 204.6961750426061, 218.55008083312305, 197.774005043331, 212.3143060161793, 209.91109947768084, 208.2396098081427, 218.15824812804357, 198.18751566904447, 210.43870788021474, 192.49810474200495, 227.07318674041005, 206.40563086100838, 194.83111028051636, 219.87501538325293, 192.61907855924872, 227.69162923255686, 206.98070176450847, 217.37454273183323, 203.33530906110477, 216.3641848256979, 220.40669339637373, 191.23742468718672, 227.492441889174, 205.0851564553293, 193.10934259848864, 202.98790636901532, 197.7238738260701, 220.38599058051466, 158.459428087664, 215.41134523189533, 186.3230796484502, 210.25902320832415, 136.04919349666238, 212.45888038192217, 187.98944010967605, 224.23094428213426, 202.9529824997765, 213.3074654776556, 209.379301921178, 139.3851047170122, 233.15058918038682, 161.13749343782445, 228.58413085452636]
Elapsed: 0.21782914289833924~0.022608511947569875
Time per graph: 0.004953943386844381~0.000514492475073398
Speed: 203.64459660571248~17.47126125475282
Total Time: 0.1886
best val loss: 0.6931840181350708 test_score: 0.4884

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.21s
test Score 0.4884
Epoch Time List: [0.8368686391040683, 0.7383035930106416, 0.8292838400229812, 0.8196518970653415, 0.838839877047576, 0.8075794440228492, 0.8764903380069882, 0.8225866261636838, 0.7991897731553763, 0.6925968959694728, 0.8325671100756153, 0.7813390670344234, 0.8727483669063076, 0.7846036748960614, 0.8837972140172496, 0.7880818189587444, 0.7734641280258074, 0.79556767095346, 0.7935597690520808, 0.7652457249350846, 0.7804519570199773, 0.7732225671643391, 0.7366388400550932, 0.7731690799118951, 0.8243375108577311, 0.7595107068773359, 0.9191975231515244, 0.7539745081448928, 0.7922654909780249, 0.7351377140730619, 0.7982187969610095, 0.7300411571050063, 0.8325525700347498, 0.7641648279968649, 0.7427935739979148, 0.8776815470773727, 0.7476634569466114, 0.775722608086653, 0.7077108951052651, 0.8511541018960997, 0.8364465532358736, 0.8720421210164204, 0.829452923964709, 0.7214425179408863, 0.8154494020855054, 0.758217801922001, 0.8271387219429016, 0.7720118910074234, 0.8134768409654498, 0.8402605470037088, 0.7606590670766309, 0.7769599659368396, 0.805578600964509, 0.8111874921014532, 0.7574346620822325, 0.7923574720043689, 0.7770580038195476, 0.9568957610754296, 0.778582316939719, 0.7918602199060842, 0.7695492669008672, 0.8058116530301049, 0.7736595750320703, 0.7461145739071071, 0.850383830955252, 0.760938894120045, 0.7423859249101952, 0.7641751610208303, 0.8341947509907186, 0.7983638349687681, 0.7224953270051628, 0.76770489604678, 0.8305382641265169, 0.7335952189750969, 0.7833847480360419, 0.7148350598290563, 0.8015345809981227, 0.8400047241011634, 0.7812213968718424, 0.7414344832068309, 0.7714395489310846, 0.799991293111816, 0.7140763450879604, 0.789369965903461, 0.8018272669287398, 0.829650983097963, 0.7835261720465496, 0.7309464910067618, 0.9215434170328081, 0.7909464309923351, 0.8238376850495115, 0.7374784279381856, 0.9014909670222551, 0.7543774700025097, 0.787361147115007, 0.7218258139910176, 0.8260586890392005, 0.8355988051043823, 0.8033900939626619, 0.8686916469596326, 0.7274732809746638, 0.763215619022958, 0.7230574128916487]
Total Epoch List: [100, 3]
Total Time List: [0.31614621391054243, 0.18860470899380744]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7858a3bcc8b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.33s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.22s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.23s
Epoch 18/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.4884 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.27s
Epoch 24/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.22s
Epoch 25/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.26s
Epoch 26/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4884 time: 0.23s
Epoch 27/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.4884 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4884 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6839;  Loss pred: 0.6839; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6861 score: 0.4884 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.4884 time: 0.33s
Epoch 31/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.4884 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 0.38s
Val loss: 0.6855 score: 0.5227 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6820 score: 0.4884 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.34s
Val loss: 0.6843 score: 0.5227 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.4884 time: 0.22s
Epoch 34/1000, LR 0.000270
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.39s
Val loss: 0.6829 score: 0.5909 time: 0.16s
Test loss: 0.6785 score: 0.5349 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.35s
Val loss: 0.6814 score: 0.6136 time: 0.17s
Test loss: 0.6764 score: 0.5814 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6741;  Loss pred: 0.6741; Loss self: 0.0000; time: 0.35s
Val loss: 0.6797 score: 0.6364 time: 0.20s
Test loss: 0.6740 score: 0.6279 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6697;  Loss pred: 0.6697; Loss self: 0.0000; time: 0.36s
Val loss: 0.6779 score: 0.6364 time: 0.19s
Test loss: 0.6715 score: 0.6512 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.35s
Val loss: 0.6759 score: 0.6818 time: 0.19s
Test loss: 0.6687 score: 0.6744 time: 0.32s
Epoch 39/1000, LR 0.000269
Train loss: 0.6644;  Loss pred: 0.6644; Loss self: 0.0000; time: 0.38s
Val loss: 0.6735 score: 0.7045 time: 0.16s
Test loss: 0.6655 score: 0.7209 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6618;  Loss pred: 0.6618; Loss self: 0.0000; time: 0.40s
Val loss: 0.6710 score: 0.7273 time: 0.24s
Test loss: 0.6620 score: 0.7209 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6579;  Loss pred: 0.6579; Loss self: 0.0000; time: 0.37s
Val loss: 0.6683 score: 0.7273 time: 0.17s
Test loss: 0.6583 score: 0.7209 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6523;  Loss pred: 0.6523; Loss self: 0.0000; time: 0.32s
Val loss: 0.6653 score: 0.7273 time: 0.19s
Test loss: 0.6543 score: 0.7209 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6488;  Loss pred: 0.6488; Loss self: 0.0000; time: 0.46s
Val loss: 0.6621 score: 0.7273 time: 0.19s
Test loss: 0.6499 score: 0.7674 time: 0.29s
Epoch 44/1000, LR 0.000269
Train loss: 0.6431;  Loss pred: 0.6431; Loss self: 0.0000; time: 0.34s
Val loss: 0.6585 score: 0.7273 time: 0.19s
Test loss: 0.6450 score: 0.7674 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6381;  Loss pred: 0.6381; Loss self: 0.0000; time: 0.47s
Val loss: 0.6546 score: 0.7955 time: 0.18s
Test loss: 0.6397 score: 0.7674 time: 0.28s
Epoch 46/1000, LR 0.000269
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.35s
Val loss: 0.6504 score: 0.7955 time: 0.19s
Test loss: 0.6341 score: 0.7907 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6264;  Loss pred: 0.6264; Loss self: 0.0000; time: 0.52s
Val loss: 0.6459 score: 0.8182 time: 0.16s
Test loss: 0.6280 score: 0.8372 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6210;  Loss pred: 0.6210; Loss self: 0.0000; time: 0.32s
Val loss: 0.6410 score: 0.8409 time: 0.19s
Test loss: 0.6214 score: 0.8372 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6140;  Loss pred: 0.6140; Loss self: 0.0000; time: 0.33s
Val loss: 0.6357 score: 0.8409 time: 0.18s
Test loss: 0.6144 score: 0.8140 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6058;  Loss pred: 0.6058; Loss self: 0.0000; time: 0.41s
Val loss: 0.6301 score: 0.8409 time: 0.16s
Test loss: 0.6071 score: 0.8140 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6055;  Loss pred: 0.6055; Loss self: 0.0000; time: 0.36s
Val loss: 0.6242 score: 0.8409 time: 0.17s
Test loss: 0.5997 score: 0.8140 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.31s
Val loss: 0.6183 score: 0.8409 time: 0.19s
Test loss: 0.5924 score: 0.8140 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.5860;  Loss pred: 0.5860; Loss self: 0.0000; time: 0.43s
Val loss: 0.6119 score: 0.8409 time: 0.17s
Test loss: 0.5847 score: 0.8140 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.5727;  Loss pred: 0.5727; Loss self: 0.0000; time: 0.33s
Val loss: 0.6052 score: 0.8409 time: 0.19s
Test loss: 0.5764 score: 0.8372 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.5678;  Loss pred: 0.5678; Loss self: 0.0000; time: 0.45s
Val loss: 0.5983 score: 0.8409 time: 0.27s
Test loss: 0.5680 score: 0.8372 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.31s
Val loss: 0.5907 score: 0.8409 time: 0.19s
Test loss: 0.5584 score: 0.8140 time: 0.23s
Epoch 57/1000, LR 0.000269
Train loss: 0.5422;  Loss pred: 0.5422; Loss self: 0.0000; time: 0.43s
Val loss: 0.5826 score: 0.8182 time: 0.17s
Test loss: 0.5482 score: 0.8140 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.5379;  Loss pred: 0.5379; Loss self: 0.0000; time: 0.34s
Val loss: 0.5744 score: 0.8182 time: 0.19s
Test loss: 0.5380 score: 0.8140 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.5239;  Loss pred: 0.5239; Loss self: 0.0000; time: 0.44s
Val loss: 0.5659 score: 0.8409 time: 0.16s
Test loss: 0.5274 score: 0.7907 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5140;  Loss pred: 0.5140; Loss self: 0.0000; time: 0.35s
Val loss: 0.5569 score: 0.8636 time: 0.18s
Test loss: 0.5159 score: 0.8372 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.4965;  Loss pred: 0.4965; Loss self: 0.0000; time: 0.36s
Val loss: 0.5478 score: 0.9091 time: 0.20s
Test loss: 0.5047 score: 0.8372 time: 0.22s
Epoch 62/1000, LR 0.000268
Train loss: 0.4866;  Loss pred: 0.4866; Loss self: 0.0000; time: 0.44s
Val loss: 0.5384 score: 0.9091 time: 0.19s
Test loss: 0.4933 score: 0.8372 time: 0.23s
Epoch 63/1000, LR 0.000268
Train loss: 0.4878;  Loss pred: 0.4878; Loss self: 0.0000; time: 0.35s
Val loss: 0.5286 score: 0.9318 time: 0.19s
Test loss: 0.4813 score: 0.8837 time: 0.22s
Epoch 64/1000, LR 0.000268
Train loss: 0.4610;  Loss pred: 0.4610; Loss self: 0.0000; time: 0.37s
Val loss: 0.5187 score: 0.9318 time: 0.17s
Test loss: 0.4695 score: 0.8837 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.4445;  Loss pred: 0.4445; Loss self: 0.0000; time: 0.31s
Val loss: 0.5088 score: 0.9318 time: 0.19s
Test loss: 0.4584 score: 0.8605 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.4295;  Loss pred: 0.4295; Loss self: 0.0000; time: 0.47s
Val loss: 0.4991 score: 0.9318 time: 0.17s
Test loss: 0.4479 score: 0.8605 time: 0.23s
Epoch 67/1000, LR 0.000268
Train loss: 0.4109;  Loss pred: 0.4109; Loss self: 0.0000; time: 0.35s
Val loss: 0.4892 score: 0.9091 time: 0.18s
Test loss: 0.4373 score: 0.8605 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.4017;  Loss pred: 0.4017; Loss self: 0.0000; time: 0.36s
Val loss: 0.4793 score: 0.9318 time: 0.19s
Test loss: 0.4268 score: 0.8605 time: 0.29s
Epoch 69/1000, LR 0.000268
Train loss: 0.3941;  Loss pred: 0.3941; Loss self: 0.0000; time: 0.35s
Val loss: 0.4695 score: 0.9091 time: 0.19s
Test loss: 0.4168 score: 0.8372 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.3867;  Loss pred: 0.3867; Loss self: 0.0000; time: 0.32s
Val loss: 0.4602 score: 0.9091 time: 0.19s
Test loss: 0.4075 score: 0.8372 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.3883;  Loss pred: 0.3883; Loss self: 0.0000; time: 0.37s
Val loss: 0.4503 score: 0.9091 time: 0.18s
Test loss: 0.3974 score: 0.8372 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.3646;  Loss pred: 0.3646; Loss self: 0.0000; time: 0.32s
Val loss: 0.4394 score: 0.9091 time: 0.19s
Test loss: 0.3855 score: 0.8605 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.3492;  Loss pred: 0.3492; Loss self: 0.0000; time: 0.38s
Val loss: 0.4283 score: 0.9545 time: 0.16s
Test loss: 0.3726 score: 0.8837 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.3296;  Loss pred: 0.3296; Loss self: 0.0000; time: 0.35s
Val loss: 0.4176 score: 0.9318 time: 0.18s
Test loss: 0.3602 score: 0.8837 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.3034;  Loss pred: 0.3034; Loss self: 0.0000; time: 0.32s
Val loss: 0.4078 score: 0.9318 time: 0.18s
Test loss: 0.3501 score: 0.8837 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.2946;  Loss pred: 0.2946; Loss self: 0.0000; time: 0.37s
Val loss: 0.3989 score: 0.9318 time: 0.17s
Test loss: 0.3428 score: 0.8837 time: 0.32s
Epoch 77/1000, LR 0.000267
Train loss: 0.3049;  Loss pred: 0.3049; Loss self: 0.0000; time: 0.30s
Val loss: 0.3905 score: 0.9318 time: 0.19s
Test loss: 0.3362 score: 0.8605 time: 0.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.3021;  Loss pred: 0.3021; Loss self: 0.0000; time: 0.37s
Val loss: 0.3805 score: 0.9091 time: 0.18s
Test loss: 0.3248 score: 0.8837 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.2539;  Loss pred: 0.2539; Loss self: 0.0000; time: 0.35s
Val loss: 0.3710 score: 0.9091 time: 0.18s
Test loss: 0.3124 score: 0.8837 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.2527;  Loss pred: 0.2527; Loss self: 0.0000; time: 0.38s
Val loss: 0.3627 score: 0.8636 time: 0.17s
Test loss: 0.3014 score: 0.8837 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.2403;  Loss pred: 0.2403; Loss self: 0.0000; time: 0.35s
Val loss: 0.3555 score: 0.8636 time: 0.17s
Test loss: 0.2923 score: 0.8837 time: 0.25s
Epoch 82/1000, LR 0.000267
Train loss: 0.2208;  Loss pred: 0.2208; Loss self: 0.0000; time: 0.31s
Val loss: 0.3485 score: 0.8636 time: 0.20s
Test loss: 0.2868 score: 0.8837 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.2266;  Loss pred: 0.2266; Loss self: 0.0000; time: 0.37s
Val loss: 0.3413 score: 0.8636 time: 0.18s
Test loss: 0.2858 score: 0.8837 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.2045;  Loss pred: 0.2045; Loss self: 0.0000; time: 0.40s
Val loss: 0.3347 score: 0.8636 time: 0.20s
Test loss: 0.2870 score: 0.8837 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.2056;  Loss pred: 0.2056; Loss self: 0.0000; time: 0.36s
Val loss: 0.3299 score: 0.8864 time: 0.16s
Test loss: 0.2950 score: 0.8605 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.1938;  Loss pred: 0.1938; Loss self: 0.0000; time: 0.34s
Val loss: 0.3290 score: 0.9091 time: 0.19s
Test loss: 0.3095 score: 0.8140 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.1710;  Loss pred: 0.1710; Loss self: 0.0000; time: 0.36s
Val loss: 0.3298 score: 0.8864 time: 0.19s
Test loss: 0.3237 score: 0.8140 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 88/1000, LR 0.000266
Train loss: 0.1543;  Loss pred: 0.1543; Loss self: 0.0000; time: 0.41s
Val loss: 0.3302 score: 0.9091 time: 0.18s
Test loss: 0.3347 score: 0.8140 time: 0.29s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 085,   Train_Loss: 0.1938,   Val_Loss: 0.3290,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.3290,   Test_Precision: 0.7826,   Test_Recall: 0.8571,   Test_accuracy: 0.8182,   Test_Score: 0.8140,   Test_loss: 0.3095


[0.21000363898929209, 0.21896358602680266, 0.23570401896722615, 0.2334318410139531, 0.2370636040577665, 0.22622855403460562, 0.23879036807920784, 0.20149155799299479, 0.23418466793373227, 0.19052168203052133, 0.22441150003578514, 0.2125962539575994, 0.2113999699940905, 0.21144898899365216, 0.23474077600985765, 0.212449571932666, 0.21736793802119792, 0.23096712806727737, 0.20310729707125574, 0.23237734800204635, 0.20029986696317792, 0.22170401900075376, 0.19601341302040964, 0.2122095429804176, 0.21065234707202762, 0.20147403795272112, 0.20580874290317297, 0.2033492639893666, 0.21524286502972245, 0.2184417420066893, 0.22482139605563134, 0.19556767190806568, 0.22609894908964634, 0.21688176493626088, 0.20031395298428833, 0.22074502299074084, 0.2012494009686634, 0.22444682696368545, 0.20040656509809196, 0.2093083841027692, 0.20876574399881065, 0.23400828195735812, 0.20515659404918551, 0.21028852101881057, 0.22175943898037076, 0.2069441539933905, 0.2149381039198488, 0.22784358193166554, 0.2006136740092188, 0.30937669693958014, 0.20323270501103252, 0.22286396310664713, 0.2341168139828369, 0.22029928700067103, 0.20903773501049727, 0.2067600640002638, 0.22632112598512322, 0.21547106502112, 0.23203869501594454, 0.20580600202083588, 0.21495272195897996, 0.20132685301359743, 0.22247615398373455, 0.20723992097191513, 0.2096125460229814, 0.21129505592398345, 0.20168845495209098, 0.22201196604873985, 0.20908700895961374, 0.22857367899268866, 0.1937701259739697, 0.21317247895058244, 0.2258366229943931, 0.20011368696577847, 0.22843012399971485, 0.1932438190560788, 0.21258020494133234, 0.20241560693830252, 0.21639134001452476, 0.20336082903668284, 0.19963096093852073, 0.2300804880214855, 0.19341301906388253, 0.21454502490814775, 0.2278501879191026, 0.2167616819497198, 0.22253256093245, 0.1996497140498832, 0.2776736009400338, 0.2042603649897501, 0.2361489520408213, 0.20926569204311818, 0.3234124280279502, 0.20709889801219106, 0.23405570001341403, 0.1962262619053945, 0.21679898200090975, 0.20627501199487597, 0.21014493599068373, 0.31567218096461147, 0.18443015799857676, 0.2668528539361432, 0.18811454600654542, 0.20976175996474922, 0.25803185591939837, 0.24678291601594537, 0.24099844601005316, 0.3356312479590997, 0.22083290992304683, 0.2477121149422601, 0.23999063100200146, 0.2391473059542477, 0.2348672340158373, 0.23902281501796097, 0.23859405994880944, 0.220428196946159, 0.25134502595756203, 0.2175721179228276, 0.23692247492726892, 0.23519538203254342, 0.22234753298107535, 0.2529494889313355, 0.224247410078533, 0.24272005097009242, 0.22934253397397697, 0.2756191329099238, 0.22921919997315854, 0.2647014510584995, 0.2316643469966948, 0.2410917520755902, 0.2519913240103051, 0.22602941596414894, 0.33158904407173395, 0.23530664097052068, 0.24276049993932247, 0.22860421205405146, 0.22156471200287342, 0.2473234690260142, 0.23360493103973567, 0.2472479969728738, 0.31986863899510354, 0.24553157796617597, 0.240147917997092, 0.24949988699518144, 0.2356462109601125, 0.29667839710600674, 0.23130578605923802, 0.28281670599244535, 0.23608997906558216, 0.24025021807756275, 0.23950679309200495, 0.22323171398602426, 0.2216585580026731, 0.250754177919589, 0.2318277769954875, 0.24818868900183588, 0.2271760740550235, 0.2499630789970979, 0.2313600069610402, 0.24190376210026443, 0.23317208292428404, 0.23876244004350156, 0.23572862800210714, 0.22541735402774066, 0.23387440596707165, 0.22723824391141534, 0.24888714298140258, 0.22444031399209052, 0.23604601400438696, 0.2388587259920314, 0.29797283699736, 0.23929373605642468, 0.23515754798427224, 0.24661095801275223, 0.22854560206178576, 0.23381821007933468, 0.23911298194434494, 0.23087836697231978, 0.32500907604116946, 0.22143574303481728, 0.2370763240614906, 0.23723801295273006, 0.22126744303386658, 0.25379499793052673, 0.2346541000297293, 0.24353464203886688, 0.23359012603759766, 0.23883794795256108, 0.2293562829727307, 0.2249259480740875, 0.292398722958751]
[0.004772809977029366, 0.004976445136972788, 0.005356909521982412, 0.005305269113953479, 0.005387809183131057, 0.005141558046241037, 0.005427053819981997, 0.004579353590749882, 0.005322378816675733, 0.004330038227966394, 0.005100261364449662, 0.004831733044490896, 0.004804544772592966, 0.004805658840764822, 0.005335017636587674, 0.004828399362106045, 0.00494018040957268, 0.00524925291061994, 0.0046160749334376305, 0.005281303363682871, 0.004552269703708589, 0.005038727704562585, 0.004454850295918401, 0.004822944158645855, 0.004787553342546083, 0.004578955408016389, 0.004677471429617567, 0.004621574181576513, 0.004891883296130056, 0.0049645850456065755, 0.005109577183082531, 0.004444719816092402, 0.005138612479310144, 0.004929131021278657, 0.0045525898405520075, 0.005016932340698655, 0.004573850022015077, 0.005101064249174669, 0.004554694661320272, 0.004757008729608391, 0.004744675999972969, 0.005318370044485412, 0.004662649864754217, 0.0047792845686093315, 0.005039987249553881, 0.004703276227122511, 0.004884956907269291, 0.005178263225719671, 0.0045594016820277, 0.007031288566808639, 0.004618925113887103, 0.005065090070605616, 0.005320836681428112, 0.005006801977287978, 0.004750857613874938, 0.0046990923636423595, 0.005143661954207346, 0.00489706965957091, 0.00527360670490783, 0.004677409136837179, 0.004885289135431362, 0.004575610295763578, 0.005056276226903058, 0.004709998203907162, 0.004763921500522305, 0.0048021603619087146, 0.004583828521638431, 0.005045726501107724, 0.004751977476354858, 0.005194856340742924, 0.004403866499408402, 0.004844829067058692, 0.0051326505225998435, 0.004548038340131329, 0.005191593727266247, 0.004391904978547245, 0.004831368294121189, 0.004600354703143239, 0.004917985000330108, 0.004621837023560974, 0.0045370672940572895, 0.005229102000488307, 0.004395750433270057, 0.004876023293366994, 0.005178413361797787, 0.004926401862493632, 0.005057558203010227, 0.004537493501133708, 0.006310763657728041, 0.0046422810224943205, 0.005367021637291394, 0.004756038455525413, 0.007350282455180687, 0.004706793136640706, 0.005319447727577592, 0.004459687770577148, 0.004927249590929767, 0.004688068454428999, 0.00477602127251554, 0.007174367749195715, 0.004289073441827366, 0.006205880324096354, 0.00437475688387315, 0.004878180464296494, 0.006000740835334846, 0.005739137581766172, 0.005604615023489608, 0.007805377859513947, 0.005135649067977833, 0.005760746859122328, 0.005581177465162825, 0.005561565254749947, 0.0054620286980427285, 0.005558670116696767, 0.005548699068576964, 0.005126237138282768, 0.005845233161803768, 0.0050598166958797115, 0.00550982499830858, 0.005469660047268452, 0.005170872860025008, 0.005882546254217105, 0.005215056048337977, 0.005644652348141684, 0.0053335473017203944, 0.006409747276974972, 0.005330679069143222, 0.006155847699034872, 0.005387542953411507, 0.005606784931990469, 0.005860263349076863, 0.0052564980456778826, 0.007711373117947301, 0.005472247464430713, 0.005645593021844708, 0.005316377024512825, 0.005152667720997056, 0.00575170858200033, 0.005432672814877574, 0.0057499534179738096, 0.007438805558025664, 0.005710036696887813, 0.005584835302257954, 0.005802322953376312, 0.005480144440932849, 0.006899497607116436, 0.0053792043269590235, 0.006577132697498729, 0.005490464629432143, 0.0055872143738968085, 0.005569925420744301, 0.005191435208977308, 0.005154850186108676, 0.005831492509757883, 0.005391343651057849, 0.005771829976786881, 0.005283164512907523, 0.005813094860397626, 0.005380465278163726, 0.005625668886052661, 0.005422606579634512, 0.0055526148847325945, 0.005482061116328073, 0.005242264047156759, 0.005438939673652829, 0.005284610323521287, 0.0057880730925907575, 0.00521954218586257, 0.005489442186148534, 0.005554854092837939, 0.006929600860403721, 0.005564970605963364, 0.00546878018568075, 0.0057351385584360984, 0.005315014001436878, 0.005437632792542667, 0.00556076702196151, 0.005369264348193483, 0.007558350605608592, 0.005149668442670169, 0.0055134028851509444, 0.005517163091923955, 0.005145754489159688, 0.005902209254198296, 0.005457072093714635, 0.005663596326485276, 0.005432328512502271, 0.0055543708826177, 0.005333867045877458, 0.0052308360017229655, 0.006799970301366303]
[209.52017884910805, 200.94665418301147, 186.67479745484547, 188.49185187795334, 185.60419755230876, 194.4935739335072, 184.26203851490777, 218.37143172782325, 187.88591237941662, 230.94484329059858, 196.06838327351167, 206.9650766695797, 208.1362641689589, 208.0880131392868, 187.44080490793075, 207.1079720223932, 202.42175732333203, 190.50329961752584, 216.63426491547256, 189.34719919263614, 219.6706401611776, 198.462798276338, 224.47443428485457, 207.34223061806534, 208.87495730088037, 218.39042115354462, 213.7907232672847, 216.3764900683428, 204.42024869871588, 201.42670350363986, 195.7109099576641, 224.98606017401454, 194.60506197467714, 202.8755161270986, 219.65519298324242, 199.324992264245, 218.63419114897764, 196.03752298587412, 219.5536856712431, 210.21613724941022, 210.76254732793072, 188.02753317943612, 214.47031816803883, 209.23633770796337, 198.4132003684168, 212.61774807808922, 204.71009652345197, 193.11494151806482, 219.32702353069942, 142.221442129473, 216.5005873321986, 197.42985535505653, 187.94036725284343, 199.72829054079497, 210.48831206380248, 212.80705349338575, 194.41402038134194, 204.20375234924103, 189.62354531849329, 213.79357048850997, 204.6961750426061, 218.55008083312305, 197.774005043331, 212.3143060161793, 209.91109947768084, 208.2396098081427, 218.15824812804357, 198.18751566904447, 210.43870788021474, 192.49810474200495, 227.07318674041005, 206.40563086100838, 194.83111028051636, 219.87501538325293, 192.61907855924872, 227.69162923255686, 206.98070176450847, 217.37454273183323, 203.33530906110477, 216.3641848256979, 220.40669339637373, 191.23742468718672, 227.492441889174, 205.0851564553293, 193.10934259848864, 202.98790636901532, 197.7238738260701, 220.38599058051466, 158.459428087664, 215.41134523189533, 186.3230796484502, 210.25902320832415, 136.04919349666238, 212.45888038192217, 187.98944010967605, 224.23094428213426, 202.9529824997765, 213.3074654776556, 209.379301921178, 139.3851047170122, 233.15058918038682, 161.13749343782445, 228.58413085452636, 204.9944661373275, 166.6460904479637, 174.24220725725456, 178.4243870112186, 128.11679562458383, 194.7173544694227, 173.58860308476633, 179.17366115696987, 179.80549614983542, 183.08215779941645, 179.89914476059047, 180.22242468746157, 195.07486154551736, 171.07957412795696, 197.63561806780783, 181.49396764996757, 182.82671891087637, 193.39094715145706, 169.9944134367181, 191.75249330612604, 177.15882898071075, 187.492477975669, 156.01239125170966, 187.5933604385465, 162.44716388236543, 185.61336933133475, 178.35533414066396, 170.64079554676067, 190.24072515773935, 129.67859092080752, 182.74027380887674, 177.1293106199228, 188.098021526537, 194.07422604120435, 173.8613814909621, 184.0714569192283, 173.9144523978394, 134.4301840126885, 175.13022298876606, 179.0563097887057, 172.34476743803276, 182.47694212778387, 144.93808925573904, 185.90109971995074, 152.04193772467113, 182.1339481251564, 178.9800664660284, 179.53561752831718, 192.6249601017357, 193.9920587206989, 171.48268617797106, 185.48251877874404, 173.25527675309138, 189.2804961035867, 172.02540540196833, 185.85753244397583, 177.7566401889084, 184.4131572730472, 180.09532819385484, 182.4131433014391, 190.7572741480599, 183.85936597976516, 189.22871106486267, 172.76906908450894, 191.58768420505487, 182.16787172352267, 180.0227302620484, 144.3084558757313, 179.6954684591524, 182.8561335521151, 174.3637036508997, 188.14625883010973, 183.90355475482457, 179.83130673351948, 186.24525356745661, 132.30399754914265, 194.18725906972898, 181.3761883234155, 181.25257189946115, 194.33496139519514, 169.42808310103385, 183.24844950312885, 176.56625620078074, 184.08312341540886, 180.0383915898525, 187.48123854585, 191.17403024499598, 147.05946580370684]
Elapsed: 0.22963091239993855~0.026647580929650135
Time per graph: 0.005279936419115194~0.0006433723024501554
Speed: 191.87749864267377~20.712009843029975
Total Time: 0.2930
best val loss: 0.3289705514907837 test_score: 0.8140

Testing...
Test loss: 0.3726 score: 0.8837 time: 0.23s
test Score 0.8837
Epoch Time List: [0.8368686391040683, 0.7383035930106416, 0.8292838400229812, 0.8196518970653415, 0.838839877047576, 0.8075794440228492, 0.8764903380069882, 0.8225866261636838, 0.7991897731553763, 0.6925968959694728, 0.8325671100756153, 0.7813390670344234, 0.8727483669063076, 0.7846036748960614, 0.8837972140172496, 0.7880818189587444, 0.7734641280258074, 0.79556767095346, 0.7935597690520808, 0.7652457249350846, 0.7804519570199773, 0.7732225671643391, 0.7366388400550932, 0.7731690799118951, 0.8243375108577311, 0.7595107068773359, 0.9191975231515244, 0.7539745081448928, 0.7922654909780249, 0.7351377140730619, 0.7982187969610095, 0.7300411571050063, 0.8325525700347498, 0.7641648279968649, 0.7427935739979148, 0.8776815470773727, 0.7476634569466114, 0.775722608086653, 0.7077108951052651, 0.8511541018960997, 0.8364465532358736, 0.8720421210164204, 0.829452923964709, 0.7214425179408863, 0.8154494020855054, 0.758217801922001, 0.8271387219429016, 0.7720118910074234, 0.8134768409654498, 0.8402605470037088, 0.7606590670766309, 0.7769599659368396, 0.805578600964509, 0.8111874921014532, 0.7574346620822325, 0.7923574720043689, 0.7770580038195476, 0.9568957610754296, 0.778582316939719, 0.7918602199060842, 0.7695492669008672, 0.8058116530301049, 0.7736595750320703, 0.7461145739071071, 0.850383830955252, 0.760938894120045, 0.7423859249101952, 0.7641751610208303, 0.8341947509907186, 0.7983638349687681, 0.7224953270051628, 0.76770489604678, 0.8305382641265169, 0.7335952189750969, 0.7833847480360419, 0.7148350598290563, 0.8015345809981227, 0.8400047241011634, 0.7812213968718424, 0.7414344832068309, 0.7714395489310846, 0.799991293111816, 0.7140763450879604, 0.789369965903461, 0.8018272669287398, 0.829650983097963, 0.7835261720465496, 0.7309464910067618, 0.9215434170328081, 0.7909464309923351, 0.8238376850495115, 0.7374784279381856, 0.9014909670222551, 0.7543774700025097, 0.787361147115007, 0.7218258139910176, 0.8260586890392005, 0.8355988051043823, 0.8033900939626619, 0.8686916469596326, 0.7274732809746638, 0.763215619022958, 0.7230574128916487, 0.650637416052632, 0.9124379260465503, 0.7607483030296862, 0.8961862950818613, 0.8656024520751089, 0.7625860830303282, 0.7731819121399894, 0.7838339670561254, 0.7672680838732049, 0.756831665057689, 0.8481558291241527, 0.782503042020835, 0.8058589469874278, 0.7863167129689828, 0.7462374849710613, 0.8114110450260341, 0.7860689120134339, 0.7653122829506174, 0.7997738029807806, 0.8061464979546145, 0.7576060858555138, 0.74963851692155, 0.9135691301198676, 0.7550071569858119, 0.9251716959988698, 0.7621928259031847, 0.8092078240588307, 0.7843449820065871, 0.7632707519223914, 0.874224852072075, 0.750498246983625, 0.7811057410435751, 0.7588309530401602, 0.7717828671447933, 0.7580396521370858, 0.7745375208323821, 0.7917885940987617, 0.8498294239398092, 0.7780668529449031, 0.874004143057391, 0.7909260269952938, 0.7452158470405266, 0.9422208869364113, 0.7531609879806638, 0.9262015840504318, 0.7650595360901207, 0.9184269619872794, 0.7472494670655578, 0.7257679441245273, 0.7790197811555117, 0.7807698921533301, 0.7257177719147876, 0.8407164819072932, 0.7410648190416396, 0.9648959239711985, 0.7296945219859481, 0.8433038558578119, 0.7616870480123907, 0.8341367819812149, 0.7616481280419976, 0.7730412770761177, 0.8558696748223156, 0.7681904999772087, 0.7867135460255668, 0.7200140110217035, 0.8686208568979055, 0.7695417559007183, 0.8432096181204543, 0.7700676969252527, 0.7412362931063399, 0.7944156568264589, 0.7360898611368611, 0.7739992130082101, 0.7679596601519734, 0.7332627470605075, 0.8571184249594808, 0.7110276779858395, 0.7772777461213991, 0.7684329111361876, 0.7742656709160656, 0.7690676930360496, 0.7433102079667151, 0.7844428319949657, 0.8301162859424949, 0.7515683819074184, 0.7543636569753289, 0.7745878478744999, 0.879022124921903]
Total Epoch List: [100, 3, 88]
Total Time List: [0.31614621391054243, 0.18860470899380744, 0.2929979639593512]
T-times Epoch Time: 0.7994595016290752 ~ 0.004894913777601908
T-times Total Epoch: 75.06666666666666 ~ 14.432063069583798
T-times Total Time: 0.22978208702212818 ~ 0.014142228447879835
T-times Inference Elapsed: 0.2253762980078983 ~ 0.00538309578277553
T-times Time Per Graph: 0.005203359973981352 ~ 0.00013854936262380806
T-times Speed: 194.47448428724198 ~ 5.021138398714921
T-times cross validation test micro f1 score:0.6765578579564472 ~ 0.0499067229937975
T-times cross validation test precision:0.6686216906162187 ~ 0.12621749526291462
T-times cross validation test recall:0.7121933621933623 ~ 0.15718036871557595
T-times cross validation test f1_score:0.6765578579564472 ~ 0.12848401636172818
