Namespace(seed=60, model='GPSTransformer', dataset='mining/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Volume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Processing...
Loading necessary files...
This might take a while.
Processing graphs...
  0%|          | 0/130 [00:00<?, ?it/s]100%|##########| 130/130 [00:00<00:00, 114912.44it/s]
Converting graphs into PyG objects...
  0%|          | 0/130 [00:00<?, ?it/s]100%|##########| 130/130 [00:00<00:00, 49461.13it/s]
Saving...
Done!
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 118], edge_attr=[118, 2], x=[28, 14887], y=[1, 1], num_nodes=32)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc25a7d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.26s
Val loss: 0.7039 score: 0.5349 time: 0.05s
Test loss: 0.6720 score: 0.5455 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.11s
Val loss: 0.6989 score: 0.5349 time: 0.04s
Test loss: 0.6709 score: 0.5682 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 0.11s
Val loss: 0.7026 score: 0.5349 time: 0.04s
Test loss: 0.6737 score: 0.5682 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.5600;  Loss pred: 0.5600; Loss self: 0.0000; time: 0.11s
Val loss: 0.7028 score: 0.5349 time: 0.04s
Test loss: 0.6729 score: 0.5682 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.5131;  Loss pred: 0.5131; Loss self: 0.0000; time: 0.11s
Val loss: 0.6986 score: 0.5349 time: 0.04s
Test loss: 0.6698 score: 0.5682 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.4488;  Loss pred: 0.4488; Loss self: 0.0000; time: 0.11s
Val loss: 0.6944 score: 0.5349 time: 0.04s
Test loss: 0.6659 score: 0.5682 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.3969;  Loss pred: 0.3969; Loss self: 0.0000; time: 0.12s
Val loss: 0.6901 score: 0.5116 time: 0.04s
Test loss: 0.6622 score: 0.5682 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.3589;  Loss pred: 0.3589; Loss self: 0.0000; time: 0.14s
Val loss: 0.6878 score: 0.4651 time: 0.04s
Test loss: 0.6596 score: 0.5682 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3249;  Loss pred: 0.3249; Loss self: 0.0000; time: 0.11s
Val loss: 0.6834 score: 0.5116 time: 0.04s
Test loss: 0.6595 score: 0.5455 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.2881;  Loss pred: 0.2881; Loss self: 0.0000; time: 0.11s
Val loss: 0.6797 score: 0.5581 time: 0.04s
Test loss: 0.6578 score: 0.5682 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.11s
Val loss: 0.6780 score: 0.5349 time: 0.05s
Test loss: 0.6563 score: 0.5909 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.2209;  Loss pred: 0.2209; Loss self: 0.0000; time: 0.12s
Val loss: 0.6768 score: 0.5116 time: 0.05s
Test loss: 0.6549 score: 0.5455 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.1957;  Loss pred: 0.1957; Loss self: 0.0000; time: 0.12s
Val loss: 0.6740 score: 0.5349 time: 0.05s
Test loss: 0.6519 score: 0.5909 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.1699;  Loss pred: 0.1699; Loss self: 0.0000; time: 0.11s
Val loss: 0.6723 score: 0.5349 time: 0.05s
Test loss: 0.6489 score: 0.6364 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.1578;  Loss pred: 0.1578; Loss self: 0.0000; time: 0.12s
Val loss: 0.6711 score: 0.5581 time: 0.04s
Test loss: 0.6472 score: 0.6364 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.1464;  Loss pred: 0.1464; Loss self: 0.0000; time: 0.11s
Val loss: 0.6707 score: 0.5814 time: 0.04s
Test loss: 0.6473 score: 0.6364 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.10s
Val loss: 0.6702 score: 0.5116 time: 0.04s
Test loss: 0.6473 score: 0.6136 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 0.1169;  Loss pred: 0.1169; Loss self: 0.0000; time: 0.10s
Val loss: 0.6679 score: 0.5581 time: 0.04s
Test loss: 0.6463 score: 0.6364 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 0.1021;  Loss pred: 0.1021; Loss self: 0.0000; time: 0.10s
Val loss: 0.6649 score: 0.6047 time: 0.04s
Test loss: 0.6459 score: 0.6364 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.10s
Val loss: 0.6624 score: 0.6047 time: 0.04s
Test loss: 0.6449 score: 0.6364 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 0.0844;  Loss pred: 0.0844; Loss self: 0.0000; time: 0.11s
Val loss: 0.6605 score: 0.6047 time: 0.05s
Test loss: 0.6436 score: 0.6136 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.10s
Val loss: 0.6587 score: 0.6047 time: 0.04s
Test loss: 0.6425 score: 0.6136 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 0.0729;  Loss pred: 0.0729; Loss self: 0.0000; time: 0.11s
Val loss: 0.6569 score: 0.6047 time: 0.04s
Test loss: 0.6415 score: 0.6364 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.11s
Val loss: 0.6551 score: 0.6047 time: 0.04s
Test loss: 0.6410 score: 0.6364 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.10s
Val loss: 0.6534 score: 0.6047 time: 0.04s
Test loss: 0.6406 score: 0.6136 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.0569;  Loss pred: 0.0569; Loss self: 0.0000; time: 0.10s
Val loss: 0.6526 score: 0.6047 time: 0.04s
Test loss: 0.6409 score: 0.6136 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.0442;  Loss pred: 0.0442; Loss self: 0.0000; time: 0.11s
Val loss: 0.6522 score: 0.6047 time: 0.04s
Test loss: 0.6424 score: 0.6136 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.0444;  Loss pred: 0.0444; Loss self: 0.0000; time: 0.11s
Val loss: 0.6522 score: 0.6047 time: 0.04s
Test loss: 0.6451 score: 0.6136 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.11s
Val loss: 0.6531 score: 0.6047 time: 0.04s
Test loss: 0.6468 score: 0.6136 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.11s
Val loss: 0.6541 score: 0.6047 time: 0.04s
Test loss: 0.6490 score: 0.6136 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.11s
Val loss: 0.6539 score: 0.6047 time: 0.05s
Test loss: 0.6507 score: 0.6136 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0255;  Loss pred: 0.0255; Loss self: 0.0000; time: 0.11s
Val loss: 0.6521 score: 0.6047 time: 0.05s
Test loss: 0.6517 score: 0.6136 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.0223;  Loss pred: 0.0223; Loss self: 0.0000; time: 0.11s
Val loss: 0.6489 score: 0.6047 time: 0.04s
Test loss: 0.6519 score: 0.6136 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.11s
Val loss: 0.6451 score: 0.6047 time: 0.04s
Test loss: 0.6518 score: 0.6136 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0201;  Loss pred: 0.0201; Loss self: 0.0000; time: 0.11s
Val loss: 0.6423 score: 0.6047 time: 0.04s
Test loss: 0.6534 score: 0.6136 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.11s
Val loss: 0.6405 score: 0.6047 time: 0.04s
Test loss: 0.6551 score: 0.6364 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.0166;  Loss pred: 0.0166; Loss self: 0.0000; time: 0.11s
Val loss: 0.6389 score: 0.6047 time: 0.04s
Test loss: 0.6574 score: 0.6364 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.11s
Val loss: 0.6386 score: 0.6047 time: 0.04s
Test loss: 0.6602 score: 0.6364 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.11s
Val loss: 0.6381 score: 0.6279 time: 0.04s
Test loss: 0.6621 score: 0.6364 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.12s
Val loss: 0.6368 score: 0.6279 time: 0.04s
Test loss: 0.6642 score: 0.6364 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0115;  Loss pred: 0.0115; Loss self: 0.0000; time: 0.11s
Val loss: 0.6350 score: 0.6279 time: 0.04s
Test loss: 0.6664 score: 0.6364 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0103;  Loss pred: 0.0103; Loss self: 0.0000; time: 0.12s
Val loss: 0.6337 score: 0.6512 time: 0.04s
Test loss: 0.6685 score: 0.6364 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.11s
Val loss: 0.6328 score: 0.6512 time: 0.04s
Test loss: 0.6705 score: 0.6364 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.11s
Val loss: 0.6326 score: 0.6512 time: 0.04s
Test loss: 0.6725 score: 0.6364 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.11s
Val loss: 0.6319 score: 0.6512 time: 0.04s
Test loss: 0.6747 score: 0.6364 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.11s
Val loss: 0.6325 score: 0.6512 time: 0.04s
Test loss: 0.6786 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.11s
Val loss: 0.6331 score: 0.6512 time: 0.04s
Test loss: 0.6835 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.11s
Val loss: 0.6342 score: 0.6512 time: 0.04s
Test loss: 0.6900 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0062;  Loss pred: 0.0062; Loss self: 0.0000; time: 0.11s
Val loss: 0.6360 score: 0.6512 time: 0.04s
Test loss: 0.6969 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.11s
Val loss: 0.6380 score: 0.6512 time: 0.04s
Test loss: 0.7053 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.11s
Val loss: 0.6373 score: 0.6512 time: 0.04s
Test loss: 0.7129 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.12s
Val loss: 0.6353 score: 0.6512 time: 0.05s
Test loss: 0.7199 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.12s
Val loss: 0.6328 score: 0.6512 time: 0.05s
Test loss: 0.7270 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.11s
Val loss: 0.6305 score: 0.6512 time: 0.04s
Test loss: 0.7335 score: 0.6136 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.12s
Val loss: 0.6266 score: 0.6744 time: 0.05s
Test loss: 0.7392 score: 0.6136 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.11s
Val loss: 0.6225 score: 0.6744 time: 0.04s
Test loss: 0.7440 score: 0.6136 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.11s
Val loss: 0.6183 score: 0.6744 time: 0.04s
Test loss: 0.7482 score: 0.6136 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.11s
Val loss: 0.6124 score: 0.7209 time: 0.04s
Test loss: 0.7504 score: 0.6364 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.11s
Val loss: 0.6063 score: 0.7209 time: 0.04s
Test loss: 0.7523 score: 0.6364 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.11s
Val loss: 0.6020 score: 0.7209 time: 0.04s
Test loss: 0.7562 score: 0.6364 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.11s
Val loss: 0.5982 score: 0.7209 time: 0.04s
Test loss: 0.7603 score: 0.6818 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.11s
Val loss: 0.5966 score: 0.7209 time: 0.04s
Test loss: 0.7677 score: 0.6818 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.11s
Val loss: 0.5941 score: 0.7209 time: 0.04s
Test loss: 0.7722 score: 0.6818 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.11s
Val loss: 0.5936 score: 0.6977 time: 0.04s
Test loss: 0.7765 score: 0.6818 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.11s
Val loss: 0.5924 score: 0.6977 time: 0.04s
Test loss: 0.7798 score: 0.6818 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.11s
Val loss: 0.5904 score: 0.7209 time: 0.04s
Test loss: 0.7818 score: 0.6591 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.11s
Val loss: 0.5902 score: 0.7209 time: 0.04s
Test loss: 0.7838 score: 0.6591 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.11s
Val loss: 0.5908 score: 0.7209 time: 0.04s
Test loss: 0.7856 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.11s
Val loss: 0.5901 score: 0.7209 time: 0.04s
Test loss: 0.7866 score: 0.6818 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.11s
Val loss: 0.5887 score: 0.7209 time: 0.04s
Test loss: 0.7871 score: 0.6818 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.11s
Val loss: 0.5862 score: 0.7209 time: 0.04s
Test loss: 0.7872 score: 0.6818 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.11s
Val loss: 0.5832 score: 0.7209 time: 0.04s
Test loss: 0.7871 score: 0.7045 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.11s
Val loss: 0.5781 score: 0.7209 time: 0.04s
Test loss: 0.7856 score: 0.7045 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.11s
Val loss: 0.5714 score: 0.7442 time: 0.04s
Test loss: 0.7824 score: 0.7045 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.11s
Val loss: 0.5644 score: 0.7674 time: 0.04s
Test loss: 0.7787 score: 0.6818 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.11s
Val loss: 0.5573 score: 0.7674 time: 0.04s
Test loss: 0.7746 score: 0.6591 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.12s
Val loss: 0.5515 score: 0.7674 time: 0.05s
Test loss: 0.7713 score: 0.6364 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.11s
Val loss: 0.5463 score: 0.7674 time: 0.05s
Test loss: 0.7677 score: 0.6364 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.12s
Val loss: 0.5421 score: 0.7907 time: 0.05s
Test loss: 0.7655 score: 0.6364 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.12s
Val loss: 0.5380 score: 0.7907 time: 0.05s
Test loss: 0.7637 score: 0.6364 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.12s
Val loss: 0.5351 score: 0.7907 time: 0.05s
Test loss: 0.7633 score: 0.6364 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.12s
Val loss: 0.5338 score: 0.7907 time: 0.05s
Test loss: 0.7633 score: 0.6364 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.11s
Val loss: 0.5336 score: 0.7907 time: 0.05s
Test loss: 0.7636 score: 0.6591 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.11s
Val loss: 0.5350 score: 0.8140 time: 0.05s
Test loss: 0.7653 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.11s
Val loss: 0.5380 score: 0.8140 time: 0.05s
Test loss: 0.7686 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.11s
Val loss: 0.5430 score: 0.8372 time: 0.05s
Test loss: 0.7744 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.12s
Val loss: 0.5496 score: 0.8372 time: 0.05s
Test loss: 0.7816 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.12s
Val loss: 0.5569 score: 0.8372 time: 0.04s
Test loss: 0.7892 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.11s
Val loss: 0.5660 score: 0.8372 time: 0.05s
Test loss: 0.7978 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.11s
Val loss: 0.5764 score: 0.8372 time: 0.05s
Test loss: 0.8063 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.11s
Val loss: 0.5873 score: 0.8372 time: 0.05s
Test loss: 0.8159 score: 0.6818 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.11s
Val loss: 0.5977 score: 0.8372 time: 0.05s
Test loss: 0.8247 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.12s
Val loss: 0.6074 score: 0.8372 time: 0.05s
Test loss: 0.8338 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.11s
Val loss: 0.6174 score: 0.8372 time: 0.05s
Test loss: 0.8436 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.12s
Val loss: 0.6270 score: 0.8372 time: 0.05s
Test loss: 0.8534 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.11s
Val loss: 0.6364 score: 0.8372 time: 0.05s
Test loss: 0.8634 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.12s
Val loss: 0.6474 score: 0.8372 time: 0.05s
Test loss: 0.8760 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0011;  Loss pred: 0.0011; Loss self: 0.0000; time: 0.11s
Val loss: 0.6580 score: 0.8372 time: 0.05s
Test loss: 0.8874 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.11s
Val loss: 0.6669 score: 0.8372 time: 0.05s
Test loss: 0.8964 score: 0.7045 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.12s
Val loss: 0.6748 score: 0.8372 time: 0.05s
Test loss: 0.9039 score: 0.7273 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0010;  Loss pred: 0.0010; Loss self: 0.0000; time: 0.12s
Val loss: 0.6824 score: 0.8372 time: 0.05s
Test loss: 0.9123 score: 0.7273 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0009;  Loss pred: 0.0009; Loss self: 0.0000; time: 0.11s
Val loss: 0.6904 score: 0.8372 time: 0.05s
Test loss: 0.9208 score: 0.7273 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0012;  Loss pred: 0.0012; Loss self: 0.0000; time: 0.12s
Val loss: 0.6983 score: 0.8372 time: 0.05s
Test loss: 0.9306 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 082,   Train_Loss: 0.0015,   Val_Loss: 0.5336,   Val_Precision: 0.7826,   Val_Recall: 0.8182,   Val_accuracy: 0.8000,   Val_Score: 0.7907,   Val_Loss: 0.5336,   Test_Precision: 0.6667,   Test_Recall: 0.6364,   Test_accuracy: 0.6512,   Test_Score: 0.6591,   Test_loss: 0.7636


[0.04664938198402524, 0.046703288913704455, 0.04678865405730903, 0.047170124016702175, 0.04711302404757589, 0.04693800292443484, 0.04828996502328664, 0.044818797963671386, 0.04485939897131175, 0.04477922304067761, 0.05405346897896379, 0.048318655928596854, 0.049058952019549906, 0.048568419995717704, 0.04467657604254782, 0.04496546392329037, 0.04506860708352178, 0.04562680597882718, 0.045558333047665656, 0.045264746993780136, 0.04583811992779374, 0.045046273968182504, 0.04664070100989193, 0.04561574407853186, 0.04516893799882382, 0.04505250102374703, 0.046471651061438024, 0.04687806696165353, 0.04692500305827707, 0.047406019060872495, 0.05243396293371916, 0.05180499900598079, 0.04701562400441617, 0.046596599044278264, 0.047243534005247056, 0.0469257109798491, 0.04699444002471864, 0.047257601050660014, 0.04743034893181175, 0.04713687999173999, 0.04691480693873018, 0.0470330030657351, 0.047004456049762666, 0.04705501301214099, 0.047220682026818395, 0.04708700196351856, 0.04691958904732019, 0.04730397101957351, 0.04718265309929848, 0.04728842794429511, 0.05175014410633594, 0.056990816025063396, 0.05039940704591572, 0.046779111959040165, 0.04830624896567315, 0.04861109901685268, 0.04662461008410901, 0.04686780704651028, 0.04670650698244572, 0.04659123800229281, 0.0467024811077863, 0.04677323310170323, 0.04660260688979179, 0.04676140798255801, 0.04707098891958594, 0.04719411791302264, 0.04721536405850202, 0.0469146219547838, 0.046916902996599674, 0.048341669025830925, 0.04719952307641506, 0.047018268960528076, 0.046720337006263435, 0.04675217706244439, 0.04687155003193766, 0.047467043972574174, 0.04915560397785157, 0.04796624195296317, 0.048560420982539654, 0.04944507800973952, 0.048034391016699374, 0.04782202607020736, 0.04790222004521638, 0.0480519849807024, 0.04792345897294581, 0.04779461701400578, 0.048055446008220315, 0.047928957967087626, 0.048188679036684334, 0.04792675795033574, 0.048396305995993316, 0.04810651496518403, 0.04806383990217, 0.04809001099783927, 0.04805484297685325, 0.04869856406003237, 0.048341909889131784, 0.048793121008202434, 0.04882135393563658, 0.04878331196960062, 0.04847352404613048, 0.04853701789397746, 0.049662714009173214]
[0.0010602132269096646, 0.001061438384402374, 0.001063378501302478, 0.0010720482731068676, 0.0010707505465358156, 0.0010667727937371555, 0.0010974992050746964, 0.0010186090446288952, 0.0010195317948025397, 0.0010177096145608548, 0.0012284879313400861, 0.001098151271104474, 0.0011149761822624978, 0.0011038277271754023, 0.0010153767282397232, 0.001021942361892963, 0.001024286524625495, 0.0010369728631551632, 0.0010354166601742195, 0.0010287442498586395, 0.0010417754529044032, 0.0010237789538223296, 0.0010600159320429984, 0.0010367214563302696, 0.0010265667727005414, 0.0010239204778124324, 0.001056173887759955, 0.0010654106127648529, 0.0010664773422335697, 0.0010774095241107386, 0.0011916809757663445, 0.0011773863410450178, 0.0010685369091912764, 0.0010590136146426878, 0.0010737166819374331, 0.0010664934313602068, 0.0010680554551072419, 0.0010740363875150003, 0.0010779624757229942, 0.0010712927270849998, 0.0010662456122438678, 0.001068931887857616, 0.0010682830920400606, 0.0010694321139122951, 0.0010731973187913272, 0.0010701591355345126, 0.0010663542965300042, 0.0010750902504448525, 0.0010723330249840562, 0.0010747369987339798, 0.0011761396387803622, 0.0012952458187514408, 0.0011454410692253573, 0.001063161635432731, 0.0010978692946743897, 0.00110479770492847, 0.0010596502291842955, 0.0010651774328752335, 0.0010615115223283117, 0.001058891772779382, 0.0010614200251769614, 0.0010630280250387098, 0.001059150156586177, 0.001062759272330864, 0.0010697952027178624, 0.0010725935889323327, 0.001073076455875046, 0.0010662414080632682, 0.00106629324992272, 0.001098674296041612, 0.0010727164335548878, 0.0010685970218301836, 0.0010618258410514418, 0.001062549478691918, 0.001065262500725856, 0.0010787964539221402, 0.0011171728176784447, 0.0010901418625673448, 0.0011036459314213557, 0.0011237517729486253, 0.0010916907049249858, 0.001086864228868349, 0.0010886868192094632, 0.0010920905677432363, 0.0010891695221124048, 0.0010862412957728586, 0.0010921692274595525, 0.0010892944992519915, 0.0010951972508337349, 0.0010892444988712668, 0.0010999160453634845, 0.0010933298855723644, 0.001092359997776591, 0.001092954795405438, 0.0010921555222012103, 0.0011067855468189175, 0.0010986797702075405, 0.001108934568368237, 0.0011095762258099223, 0.0011087116356727413, 0.00110167100104842, 0.0011031140430449423, 0.0011286980456630276]
[943.2064933907913, 942.1178041936312, 940.3989254768186, 932.7938163660607, 933.9243423552629, 937.406733533919, 911.162391167235, 981.7309253957441, 980.8423877488563, 982.598558265074, 814.0088107411507, 910.6213563767426, 896.8801449828377, 905.9384679155611, 984.8561348590482, 978.5287676573865, 976.289325260454, 964.3453898661658, 965.7947746675621, 972.0588962100258, 959.8997530725686, 976.7733515779458, 943.382047166661, 964.5792453642715, 974.1207553107788, 976.6383441578025, 946.8137885144134, 938.6052551183947, 937.6664279671021, 928.1521813401176, 839.1507629438503, 849.3388832016053, 935.8591092158447, 944.2749235451529, 931.3443823892003, 937.6522823254513, 936.2809723204789, 931.0671515642981, 927.6760764137874, 933.4516838558327, 937.8702135013182, 935.5133019786965, 936.0814632854828, 935.0757163460406, 931.7950972205516, 934.4404647823986, 937.7746244883843, 930.1544680423048, 932.5461183244527, 930.4601974045571, 850.2391782637169, 772.0542197649828, 873.0261441352748, 940.5907499596495, 910.8552401008571, 905.1430823389925, 943.7076239485046, 938.8107268670722, 942.0528924703589, 944.3835769685859, 942.1340998661475, 940.7089713966717, 944.1531909159811, 940.9468597783023, 934.7583513736606, 932.3195759499245, 931.9000473126071, 937.8739115154141, 937.8283132454186, 910.1878542192866, 932.2128092007392, 935.8064635883997, 941.774028601319, 941.1326437532855, 938.7357569787852, 926.9589238677389, 895.1166589230686, 917.3118053139839, 906.0876967236467, 889.8762378599754, 916.0103640057225, 920.0781233192367, 918.5378038526616, 915.6749719636, 918.1307222594113, 920.605765856565, 915.6090236364346, 918.0253831141998, 913.0775294027952, 918.0675238995958, 909.1602983839865, 914.6370305943794, 915.4491212012687, 914.950924049008, 915.6205134453074, 903.5174003438727, 910.1833192132955, 901.7664599196949, 901.2449769010341, 901.9477813933308, 907.7120111615325, 906.524584928395, 885.9765495674028]
Elapsed: 0.04753291662285122~0.0017927306998906135
Time per graph: 0.001080293559610255~4.0743879542968486e-05
Speed: 926.904265114419~32.723253667988004
Total Time: 0.0502
best val loss: 0.5336167216300964 test_score: 0.6591

Testing...
Test loss: 0.7744 score: 0.6591 time: 0.04s
test Score 0.6591
Epoch Time List: [0.3441138418857008, 0.1950901160016656, 0.20092373108491302, 0.19552739395294338, 0.19563883496448398, 0.20061134896241128, 0.20279942196793854, 0.21964536106679589, 0.1906130739953369, 0.1907715218840167, 0.2064600430894643, 0.20916516007855535, 0.20820258208550513, 0.20331831311341375, 0.20177148596849293, 0.19249879790004343, 0.18681801797356457, 0.18902222800534219, 0.18872436101082712, 0.18816109385807067, 0.19814793404657394, 0.18872316204942763, 0.18934238387737423, 0.19064730196259916, 0.18730703508481383, 0.18768095003906637, 0.1980663602007553, 0.20147660793736577, 0.2011338050942868, 0.1956611139466986, 0.20366825792007148, 0.20684987702406943, 0.19591468898579478, 0.1942613379796967, 0.19509483501315117, 0.19485604704823345, 0.20094562007579952, 0.20161761494819075, 0.19522095005959272, 0.2035598891088739, 0.20075422094669193, 0.20354284311179072, 0.20091752894222736, 0.19471306295599788, 0.2007422869792208, 0.19578393595293164, 0.20067987206857651, 0.19496597605757415, 0.20105283381417394, 0.201496192952618, 0.20558978407643735, 0.22088609787169844, 0.21799896901939064, 0.199370467918925, 0.20595013699494302, 0.20323498011566699, 0.2000330031150952, 0.1997889019548893, 0.19960507203359157, 0.19977073604241014, 0.20053704199381173, 0.19961461902130395, 0.19992803607601672, 0.2002224219031632, 0.20041608402971178, 0.20041008107364178, 0.20000288600567728, 0.20015437889378518, 0.19995300192385912, 0.2014604409923777, 0.20105557306669652, 0.2001083471113816, 0.20056632487103343, 0.20011935394722968, 0.1990925749996677, 0.2008088689763099, 0.20834908890537918, 0.20071389386430383, 0.2071437140693888, 0.20870618906337768, 0.21013470296747983, 0.20743860502261668, 0.19951007200870663, 0.20096610486507416, 0.19967496395111084, 0.2020885208621621, 0.20705822098534554, 0.20593356399331242, 0.19996599503792822, 0.20068104087840766, 0.2003224149812013, 0.2003873320063576, 0.2082030720775947, 0.2014931709272787, 0.20841177506372333, 0.20234542491380125, 0.20927221502643079, 0.2029490970307961, 0.2033659789012745, 0.20966396597214043, 0.2120379889383912, 0.20290385093539953, 0.2082310609985143]
Total Epoch List: [103]
Total Time List: [0.05022927199024707]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc25a650>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7866;  Loss pred: 0.7866; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7168 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6825 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7744;  Loss pred: 0.7744; Loss self: 0.0000; time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6715 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7503;  Loss pred: 0.7503; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6675 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.7128;  Loss pred: 0.7128; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6838 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6670 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6617 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5690;  Loss pred: 0.5690; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6829 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6588 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6568 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4694;  Loss pred: 0.4694; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6538 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.4318;  Loss pred: 0.4318; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6523 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3939;  Loss pred: 0.3939; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6529 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.3601;  Loss pred: 0.3601; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6522 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.3295;  Loss pred: 0.3295; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6510 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2994;  Loss pred: 0.2994; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6499 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.2756;  Loss pred: 0.2756; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6495 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2493;  Loss pred: 0.2493; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6491 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2300;  Loss pred: 0.2300; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6485 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.2097;  Loss pred: 0.2097; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6476 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1945;  Loss pred: 0.1945; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6463 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1798;  Loss pred: 0.1798; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6457 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1640;  Loss pred: 0.1640; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6843 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6450 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1524;  Loss pred: 0.1524; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6808 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6439 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1405;  Loss pred: 0.1405; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6766 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6428 score: 0.5116 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6706 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6422 score: 0.5116 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 0.1205;  Loss pred: 0.1205; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6634 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6413 score: 0.5116 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.1119;  Loss pred: 0.1119; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6561 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6405 score: 0.5116 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.1071;  Loss pred: 0.1071; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6494 score: 0.5000 time: 0.04s
Test loss: 0.6400 score: 0.5349 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.0983;  Loss pred: 0.0983; Loss self: 0.0000; time: 0.12s
Val loss: 0.6431 score: 0.5227 time: 0.04s
Test loss: 0.6403 score: 0.5581 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.0938;  Loss pred: 0.0938; Loss self: 0.0000; time: 0.12s
Val loss: 0.6365 score: 0.5455 time: 0.05s
Test loss: 0.6407 score: 0.5814 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.12s
Val loss: 0.6298 score: 0.6364 time: 0.04s
Test loss: 0.6411 score: 0.5814 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 0.0821;  Loss pred: 0.0821; Loss self: 0.0000; time: 0.12s
Val loss: 0.6235 score: 0.6364 time: 0.04s
Test loss: 0.6414 score: 0.5581 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 0.0762;  Loss pred: 0.0762; Loss self: 0.0000; time: 0.12s
Val loss: 0.6180 score: 0.6136 time: 0.04s
Test loss: 0.6422 score: 0.5581 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.0735;  Loss pred: 0.0735; Loss self: 0.0000; time: 0.12s
Val loss: 0.6118 score: 0.6591 time: 0.04s
Test loss: 0.6423 score: 0.5814 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 0.0641;  Loss pred: 0.0641; Loss self: 0.0000; time: 0.12s
Val loss: 0.6046 score: 0.6591 time: 0.04s
Test loss: 0.6445 score: 0.5814 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.12s
Val loss: 0.5975 score: 0.6818 time: 0.04s
Test loss: 0.6472 score: 0.5814 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.12s
Val loss: 0.5905 score: 0.7273 time: 0.04s
Test loss: 0.6501 score: 0.5814 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0570;  Loss pred: 0.0570; Loss self: 0.0000; time: 0.12s
Val loss: 0.5850 score: 0.7273 time: 0.04s
Test loss: 0.6531 score: 0.5814 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.0558;  Loss pred: 0.0558; Loss self: 0.0000; time: 0.12s
Val loss: 0.5823 score: 0.7045 time: 0.04s
Test loss: 0.6537 score: 0.5814 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.12s
Val loss: 0.5790 score: 0.7273 time: 0.04s
Test loss: 0.6542 score: 0.6047 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.12s
Val loss: 0.5747 score: 0.7500 time: 0.04s
Test loss: 0.6550 score: 0.6047 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.12s
Val loss: 0.5698 score: 0.7500 time: 0.04s
Test loss: 0.6565 score: 0.6279 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0419;  Loss pred: 0.0419; Loss self: 0.0000; time: 0.12s
Val loss: 0.5650 score: 0.7500 time: 0.04s
Test loss: 0.6588 score: 0.6047 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0396;  Loss pred: 0.0396; Loss self: 0.0000; time: 0.12s
Val loss: 0.5611 score: 0.7727 time: 0.04s
Test loss: 0.6622 score: 0.6047 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.12s
Val loss: 0.5575 score: 0.7500 time: 0.04s
Test loss: 0.6649 score: 0.5814 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0368;  Loss pred: 0.0368; Loss self: 0.0000; time: 0.12s
Val loss: 0.5547 score: 0.7500 time: 0.04s
Test loss: 0.6651 score: 0.5814 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 0.0330;  Loss pred: 0.0330; Loss self: 0.0000; time: 0.12s
Val loss: 0.5516 score: 0.7500 time: 0.04s
Test loss: 0.6666 score: 0.6047 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 0.0335;  Loss pred: 0.0335; Loss self: 0.0000; time: 0.12s
Val loss: 0.5489 score: 0.7500 time: 0.04s
Test loss: 0.6695 score: 0.6047 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.12s
Val loss: 0.5473 score: 0.7273 time: 0.04s
Test loss: 0.6755 score: 0.5814 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 0.0310;  Loss pred: 0.0310; Loss self: 0.0000; time: 0.12s
Val loss: 0.5464 score: 0.7273 time: 0.04s
Test loss: 0.6793 score: 0.5814 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 0.0291;  Loss pred: 0.0291; Loss self: 0.0000; time: 0.12s
Val loss: 0.5459 score: 0.7273 time: 0.04s
Test loss: 0.6814 score: 0.5814 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 0.0272;  Loss pred: 0.0272; Loss self: 0.0000; time: 0.12s
Val loss: 0.5450 score: 0.7273 time: 0.04s
Test loss: 0.6872 score: 0.5814 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.12s
Val loss: 0.5436 score: 0.7273 time: 0.04s
Test loss: 0.6942 score: 0.6279 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.12s
Val loss: 0.5425 score: 0.7045 time: 0.04s
Test loss: 0.7032 score: 0.6047 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.12s
Val loss: 0.5407 score: 0.7045 time: 0.04s
Test loss: 0.7128 score: 0.6047 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0217;  Loss pred: 0.0217; Loss self: 0.0000; time: 0.12s
Val loss: 0.5384 score: 0.7045 time: 0.04s
Test loss: 0.7219 score: 0.6047 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0205;  Loss pred: 0.0205; Loss self: 0.0000; time: 0.13s
Val loss: 0.5341 score: 0.7727 time: 0.05s
Test loss: 0.7283 score: 0.5814 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.14s
Val loss: 0.5290 score: 0.7955 time: 0.05s
Test loss: 0.7316 score: 0.5814 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0202;  Loss pred: 0.0202; Loss self: 0.0000; time: 0.14s
Val loss: 0.5228 score: 0.7955 time: 0.05s
Test loss: 0.7293 score: 0.5814 time: 0.06s
Epoch 58/1000, LR 0.000269
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.15s
Val loss: 0.5169 score: 0.7955 time: 0.05s
Test loss: 0.7209 score: 0.6047 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.14s
Val loss: 0.5117 score: 0.8182 time: 0.05s
Test loss: 0.7107 score: 0.6047 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.0145;  Loss pred: 0.0145; Loss self: 0.0000; time: 0.14s
Val loss: 0.5065 score: 0.8182 time: 0.05s
Test loss: 0.7004 score: 0.6047 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0158;  Loss pred: 0.0158; Loss self: 0.0000; time: 0.14s
Val loss: 0.5008 score: 0.8182 time: 0.05s
Test loss: 0.6928 score: 0.6279 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.14s
Val loss: 0.4942 score: 0.8409 time: 0.05s
Test loss: 0.6893 score: 0.6512 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.0136;  Loss pred: 0.0136; Loss self: 0.0000; time: 0.14s
Val loss: 0.4851 score: 0.8409 time: 0.05s
Test loss: 0.6858 score: 0.6512 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.14s
Val loss: 0.4777 score: 0.8409 time: 0.05s
Test loss: 0.6967 score: 0.6512 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0131;  Loss pred: 0.0131; Loss self: 0.0000; time: 0.14s
Val loss: 0.4701 score: 0.8409 time: 0.05s
Test loss: 0.7027 score: 0.6512 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.14s
Val loss: 0.4674 score: 0.7955 time: 0.05s
Test loss: 0.7128 score: 0.6977 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.14s
Val loss: 0.4665 score: 0.7955 time: 0.05s
Test loss: 0.7210 score: 0.6512 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.14s
Val loss: 0.4642 score: 0.8182 time: 0.05s
Test loss: 0.7252 score: 0.6512 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.14s
Val loss: 0.4598 score: 0.7955 time: 0.05s
Test loss: 0.7224 score: 0.6512 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.14s
Val loss: 0.4517 score: 0.7955 time: 0.05s
Test loss: 0.7108 score: 0.6512 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.14s
Val loss: 0.4431 score: 0.7955 time: 0.05s
Test loss: 0.6958 score: 0.6512 time: 0.05s
Epoch 72/1000, LR 0.000267
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.14s
Val loss: 0.4346 score: 0.7955 time: 0.05s
Test loss: 0.6791 score: 0.6977 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.14s
Val loss: 0.4274 score: 0.8182 time: 0.05s
Test loss: 0.6630 score: 0.7209 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.14s
Val loss: 0.4234 score: 0.8409 time: 0.05s
Test loss: 0.6506 score: 0.7209 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.14s
Val loss: 0.4200 score: 0.8409 time: 0.05s
Test loss: 0.6389 score: 0.7209 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.14s
Val loss: 0.4170 score: 0.8409 time: 0.05s
Test loss: 0.6304 score: 0.7209 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.14s
Val loss: 0.4141 score: 0.8182 time: 0.05s
Test loss: 0.6245 score: 0.7209 time: 0.05s
Epoch 78/1000, LR 0.000267
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.14s
Val loss: 0.4108 score: 0.8182 time: 0.05s
Test loss: 0.6212 score: 0.7209 time: 0.05s
Epoch 79/1000, LR 0.000267
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.14s
Val loss: 0.4067 score: 0.7955 time: 0.05s
Test loss: 0.6195 score: 0.7209 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.14s
Val loss: 0.4006 score: 0.7955 time: 0.05s
Test loss: 0.6177 score: 0.7209 time: 0.05s
Epoch 81/1000, LR 0.000267
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.14s
Val loss: 0.3949 score: 0.7955 time: 0.05s
Test loss: 0.6157 score: 0.7209 time: 0.05s
Epoch 82/1000, LR 0.000267
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.14s
Val loss: 0.3871 score: 0.8182 time: 0.05s
Test loss: 0.6079 score: 0.7209 time: 0.05s
Epoch 83/1000, LR 0.000266
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.14s
Val loss: 0.3808 score: 0.8182 time: 0.04s
Test loss: 0.5967 score: 0.7442 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.12s
Val loss: 0.3772 score: 0.8182 time: 0.04s
Test loss: 0.5889 score: 0.7442 time: 0.04s
Epoch 85/1000, LR 0.000266
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.13s
Val loss: 0.3739 score: 0.8182 time: 0.04s
Test loss: 0.5772 score: 0.7442 time: 0.04s
Epoch 86/1000, LR 0.000266
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.12s
Val loss: 0.3698 score: 0.8636 time: 0.04s
Test loss: 0.5636 score: 0.7442 time: 0.04s
Epoch 87/1000, LR 0.000266
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.12s
Val loss: 0.3664 score: 0.8636 time: 0.04s
Test loss: 0.5521 score: 0.7674 time: 0.04s
Epoch 88/1000, LR 0.000266
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.12s
Val loss: 0.3631 score: 0.8636 time: 0.04s
Test loss: 0.5403 score: 0.7907 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.12s
Val loss: 0.3593 score: 0.8636 time: 0.04s
Test loss: 0.5279 score: 0.8140 time: 0.04s
Epoch 90/1000, LR 0.000266
Train loss: 0.0037;  Loss pred: 0.0037; Loss self: 0.0000; time: 0.12s
Val loss: 0.3569 score: 0.8636 time: 0.04s
Test loss: 0.5172 score: 0.8140 time: 0.04s
Epoch 91/1000, LR 0.000266
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.12s
Val loss: 0.3550 score: 0.8636 time: 0.04s
Test loss: 0.5094 score: 0.7907 time: 0.04s
Epoch 92/1000, LR 0.000266
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.12s
Val loss: 0.3537 score: 0.8636 time: 0.04s
Test loss: 0.5009 score: 0.7907 time: 0.04s
Epoch 93/1000, LR 0.000265
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.12s
Val loss: 0.3532 score: 0.8636 time: 0.04s
Test loss: 0.4953 score: 0.7907 time: 0.04s
Epoch 94/1000, LR 0.000265
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.12s
Val loss: 0.3541 score: 0.8636 time: 0.04s
Test loss: 0.4909 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.12s
Val loss: 0.3561 score: 0.8636 time: 0.04s
Test loss: 0.4885 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.12s
Val loss: 0.3595 score: 0.8636 time: 0.04s
Test loss: 0.4895 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.12s
Val loss: 0.3625 score: 0.8864 time: 0.04s
Test loss: 0.4874 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.3683 score: 0.8864 time: 0.04s
Test loss: 0.4903 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.12s
Val loss: 0.3786 score: 0.8864 time: 0.04s
Test loss: 0.5039 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.13s
Val loss: 0.3780 score: 0.8864 time: 0.04s
Test loss: 0.4923 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.12s
Val loss: 0.3769 score: 0.8864 time: 0.04s
Test loss: 0.4816 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.12s
Val loss: 0.3763 score: 0.8864 time: 0.04s
Test loss: 0.4750 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.12s
Val loss: 0.3801 score: 0.8864 time: 0.04s
Test loss: 0.4767 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.3930 score: 0.8864 time: 0.04s
Test loss: 0.5011 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.13s
Val loss: 0.4028 score: 0.8864 time: 0.04s
Test loss: 0.5200 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.13s
Val loss: 0.4120 score: 0.8864 time: 0.04s
Test loss: 0.5368 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.13s
Val loss: 0.4206 score: 0.8864 time: 0.04s
Test loss: 0.5513 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.13s
Val loss: 0.4278 score: 0.8864 time: 0.04s
Test loss: 0.5618 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.13s
Val loss: 0.4327 score: 0.8864 time: 0.04s
Test loss: 0.5666 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4354 score: 0.8864 time: 0.04s
Test loss: 0.5675 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 111/1000, LR 0.000263
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.13s
Val loss: 0.4357 score: 0.8864 time: 0.04s
Test loss: 0.5620 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 112/1000, LR 0.000263
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.4359 score: 0.8864 time: 0.04s
Test loss: 0.5552 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 113/1000, LR 0.000263
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4370 score: 0.8864 time: 0.04s
Test loss: 0.5487 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.0033,   Val_Loss: 0.3532,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3532,   Test_Precision: 0.8824,   Test_Recall: 0.6818,   Test_accuracy: 0.7692,   Test_Score: 0.7907,   Test_loss: 0.4953


[0.04664938198402524, 0.046703288913704455, 0.04678865405730903, 0.047170124016702175, 0.04711302404757589, 0.04693800292443484, 0.04828996502328664, 0.044818797963671386, 0.04485939897131175, 0.04477922304067761, 0.05405346897896379, 0.048318655928596854, 0.049058952019549906, 0.048568419995717704, 0.04467657604254782, 0.04496546392329037, 0.04506860708352178, 0.04562680597882718, 0.045558333047665656, 0.045264746993780136, 0.04583811992779374, 0.045046273968182504, 0.04664070100989193, 0.04561574407853186, 0.04516893799882382, 0.04505250102374703, 0.046471651061438024, 0.04687806696165353, 0.04692500305827707, 0.047406019060872495, 0.05243396293371916, 0.05180499900598079, 0.04701562400441617, 0.046596599044278264, 0.047243534005247056, 0.0469257109798491, 0.04699444002471864, 0.047257601050660014, 0.04743034893181175, 0.04713687999173999, 0.04691480693873018, 0.0470330030657351, 0.047004456049762666, 0.04705501301214099, 0.047220682026818395, 0.04708700196351856, 0.04691958904732019, 0.04730397101957351, 0.04718265309929848, 0.04728842794429511, 0.05175014410633594, 0.056990816025063396, 0.05039940704591572, 0.046779111959040165, 0.04830624896567315, 0.04861109901685268, 0.04662461008410901, 0.04686780704651028, 0.04670650698244572, 0.04659123800229281, 0.0467024811077863, 0.04677323310170323, 0.04660260688979179, 0.04676140798255801, 0.04707098891958594, 0.04719411791302264, 0.04721536405850202, 0.0469146219547838, 0.046916902996599674, 0.048341669025830925, 0.04719952307641506, 0.047018268960528076, 0.046720337006263435, 0.04675217706244439, 0.04687155003193766, 0.047467043972574174, 0.04915560397785157, 0.04796624195296317, 0.048560420982539654, 0.04944507800973952, 0.048034391016699374, 0.04782202607020736, 0.04790222004521638, 0.0480519849807024, 0.04792345897294581, 0.04779461701400578, 0.048055446008220315, 0.047928957967087626, 0.048188679036684334, 0.04792675795033574, 0.048396305995993316, 0.04810651496518403, 0.04806383990217, 0.04809001099783927, 0.04805484297685325, 0.04869856406003237, 0.048341909889131784, 0.048793121008202434, 0.04882135393563658, 0.04878331196960062, 0.04847352404613048, 0.04853701789397746, 0.049662714009173214, 0.045015275944024324, 0.045787223032675683, 0.04573908296879381, 0.045704097021371126, 0.046562826028093696, 0.04655713390093297, 0.04811068600974977, 0.04700580006465316, 0.046718380064703524, 0.04706191807053983, 0.04796882497612387, 0.04749118001200259, 0.0482049600686878, 0.047238101018592715, 0.04705658904276788, 0.049086833954788744, 0.04881336505059153, 0.05284992197994143, 0.05264117894694209, 0.04727574891876429, 0.047588643967173994, 0.04689317394513637, 0.04653174907434732, 0.046917354920879006, 0.0476451349677518, 0.04741883499082178, 0.04668796097394079, 0.04775445908308029, 0.0469051799736917, 0.046953288023360074, 0.04693036200478673, 0.04712284088600427, 0.04736672097351402, 0.047270511044189334, 0.04697739391122013, 0.047212994075380266, 0.046963577973656356, 0.046940537984482944, 0.04710266902111471, 0.04720935900695622, 0.04769874201156199, 0.05144454294349998, 0.04744017799384892, 0.04788760107476264, 0.047266664914786816, 0.04755318898241967, 0.04729077499359846, 0.047500993008725345, 0.04808145994320512, 0.048294423962943256, 0.04826438508462161, 0.048375908052548766, 0.048176505020819604, 0.04824729007668793, 0.051695696893148124, 0.05446892709005624, 0.06295889592729509, 0.05390826507937163, 0.052632687962614, 0.05204726103693247, 0.05268922995310277, 0.05286947696004063, 0.052569514024071395, 0.052717800019308925, 0.053074512048624456, 0.05272903200238943, 0.052592218038626015, 0.05270797701086849, 0.0524707610020414, 0.05260110390372574, 0.05285246903076768, 0.05291082593612373, 0.05322269501630217, 0.05165360099636018, 0.05208971502725035, 0.05286740500014275, 0.052437584032304585, 0.05296167288906872, 0.05245352897327393, 0.052501916070468724, 0.0527414110256359, 0.052242992096580565, 0.047817126964218915, 0.04863608710002154, 0.04858879395760596, 0.04754860606044531, 0.04738716792780906, 0.0474524520104751, 0.04764579702168703, 0.04812514199875295, 0.047599427052773535, 0.048368290066719055, 0.04823341406881809, 0.048347542993724346, 0.047730796970427036, 0.047902101068757474, 0.047618121025152504, 0.04765744705218822, 0.048133388976566494, 0.04807703895494342, 0.04803634306881577, 0.047973360982723534, 0.04799345100764185, 0.048485393985174596, 0.049030070076696575, 0.04974715900607407, 0.04953237192239612, 0.05928319494705647, 0.04918066400568932, 0.04897524800617248, 0.048825543955899775, 0.048889460042119026, 0.04889763402752578]
[0.0010602132269096646, 0.001061438384402374, 0.001063378501302478, 0.0010720482731068676, 0.0010707505465358156, 0.0010667727937371555, 0.0010974992050746964, 0.0010186090446288952, 0.0010195317948025397, 0.0010177096145608548, 0.0012284879313400861, 0.001098151271104474, 0.0011149761822624978, 0.0011038277271754023, 0.0010153767282397232, 0.001021942361892963, 0.001024286524625495, 0.0010369728631551632, 0.0010354166601742195, 0.0010287442498586395, 0.0010417754529044032, 0.0010237789538223296, 0.0010600159320429984, 0.0010367214563302696, 0.0010265667727005414, 0.0010239204778124324, 0.001056173887759955, 0.0010654106127648529, 0.0010664773422335697, 0.0010774095241107386, 0.0011916809757663445, 0.0011773863410450178, 0.0010685369091912764, 0.0010590136146426878, 0.0010737166819374331, 0.0010664934313602068, 0.0010680554551072419, 0.0010740363875150003, 0.0010779624757229942, 0.0010712927270849998, 0.0010662456122438678, 0.001068931887857616, 0.0010682830920400606, 0.0010694321139122951, 0.0010731973187913272, 0.0010701591355345126, 0.0010663542965300042, 0.0010750902504448525, 0.0010723330249840562, 0.0010747369987339798, 0.0011761396387803622, 0.0012952458187514408, 0.0011454410692253573, 0.001063161635432731, 0.0010978692946743897, 0.00110479770492847, 0.0010596502291842955, 0.0010651774328752335, 0.0010615115223283117, 0.001058891772779382, 0.0010614200251769614, 0.0010630280250387098, 0.001059150156586177, 0.001062759272330864, 0.0010697952027178624, 0.0010725935889323327, 0.001073076455875046, 0.0010662414080632682, 0.00106629324992272, 0.001098674296041612, 0.0010727164335548878, 0.0010685970218301836, 0.0010618258410514418, 0.001062549478691918, 0.001065262500725856, 0.0010787964539221402, 0.0011171728176784447, 0.0010901418625673448, 0.0011036459314213557, 0.0011237517729486253, 0.0010916907049249858, 0.001086864228868349, 0.0010886868192094632, 0.0010920905677432363, 0.0010891695221124048, 0.0010862412957728586, 0.0010921692274595525, 0.0010892944992519915, 0.0010951972508337349, 0.0010892444988712668, 0.0010999160453634845, 0.0010933298855723644, 0.001092359997776591, 0.001092954795405438, 0.0010921555222012103, 0.0011067855468189175, 0.0010986797702075405, 0.001108934568368237, 0.0011095762258099223, 0.0011087116356727413, 0.00110167100104842, 0.0011031140430449423, 0.0011286980456630276, 0.0010468668824191703, 0.0010648191402947834, 0.0010636996039254374, 0.001062885977241189, 0.001082856419257993, 0.0010827240442077433, 0.0011188531630174365, 0.0010931581410384456, 0.0010864739549931051, 0.0010944632109427868, 0.001115554069212183, 0.001104446046790758, 0.0011210455829927394, 0.0010985604888044816, 0.0010943392800643694, 0.0011415542780183429, 0.0011351945360602682, 0.0012290679530218937, 0.0012242134638823742, 0.0010994360213666115, 0.0011067126503993953, 0.0010905389289566597, 0.001082133699403426, 0.0010911012772297443, 0.0011080263945988792, 0.0011027636044377157, 0.0010857665342776928, 0.0011105688158855882, 0.0010908181389230628, 0.0010919369307758157, 0.0010914037675531798, 0.0010958800206047504, 0.0011015516505468376, 0.0010993142103299844, 0.001092497532819073, 0.0010979766064041921, 0.0010921762319454967, 0.0010916404182437895, 0.0010954109074677839, 0.0010978920699292144, 0.0011092730700363253, 0.0011963847196162787, 0.0011032599533453238, 0.0011136651412735497, 0.0010992247654601586, 0.0011058881158702248, 0.001099785464967406, 0.0011046742560168684, 0.0011181734870512818, 0.001123126138673099, 0.0011224275601074793, 0.0011250211175011342, 0.0011203838376934792, 0.0011220300017834404, 0.0012022255091429796, 0.0012667192346524708, 0.0014641603704022114, 0.0012536805832412006, 0.0012240159991305582, 0.0012104014194635458, 0.0012253309291419248, 0.0012295227200009448, 0.0012225468377691023, 0.0012259953492862541, 0.0012342909778749874, 0.001226256558195103, 0.0012230748381075818, 0.0012257669072294997, 0.001220250255861428, 0.0012232814861331568, 0.001229127186762039, 0.0012304843240959007, 0.001237737093402376, 0.001201246534799074, 0.0012113887215639617, 0.0012294745348870408, 0.0012194786984256881, 0.0012316668113736913, 0.0012198495110063705, 0.001220974792336482, 0.0012265444424566488, 0.001214953304571641, 0.0011120262084702074, 0.0011310717930237567, 0.001129971952502464, 0.0011057815362894258, 0.0011020271611118385, 0.0011035453955924443, 0.001108041791202024, 0.0011191893488082083, 0.0011069634198319427, 0.001124843955039978, 0.0011217073039260022, 0.0011243614649703337, 0.0011100185341959775, 0.0011140023504362204, 0.0011073981633756397, 0.0011083127221439121, 0.0011193811389899186, 0.0011180706733707772, 0.0011171242574143203, 0.0011156595577377566, 0.001116126767619578, 0.0011275673019808046, 0.0011402341878301529, 0.001156910674559862, 0.0011519156261022355, 0.0013786789522571273, 0.001143736372225333, 0.0011389592559574995, 0.0011354777664162737, 0.0011369641870260239, 0.001137154279709902]
[943.2064933907913, 942.1178041936312, 940.3989254768186, 932.7938163660607, 933.9243423552629, 937.406733533919, 911.162391167235, 981.7309253957441, 980.8423877488563, 982.598558265074, 814.0088107411507, 910.6213563767426, 896.8801449828377, 905.9384679155611, 984.8561348590482, 978.5287676573865, 976.289325260454, 964.3453898661658, 965.7947746675621, 972.0588962100258, 959.8997530725686, 976.7733515779458, 943.382047166661, 964.5792453642715, 974.1207553107788, 976.6383441578025, 946.8137885144134, 938.6052551183947, 937.6664279671021, 928.1521813401176, 839.1507629438503, 849.3388832016053, 935.8591092158447, 944.2749235451529, 931.3443823892003, 937.6522823254513, 936.2809723204789, 931.0671515642981, 927.6760764137874, 933.4516838558327, 937.8702135013182, 935.5133019786965, 936.0814632854828, 935.0757163460406, 931.7950972205516, 934.4404647823986, 937.7746244883843, 930.1544680423048, 932.5461183244527, 930.4601974045571, 850.2391782637169, 772.0542197649828, 873.0261441352748, 940.5907499596495, 910.8552401008571, 905.1430823389925, 943.7076239485046, 938.8107268670722, 942.0528924703589, 944.3835769685859, 942.1340998661475, 940.7089713966717, 944.1531909159811, 940.9468597783023, 934.7583513736606, 932.3195759499245, 931.9000473126071, 937.8739115154141, 937.8283132454186, 910.1878542192866, 932.2128092007392, 935.8064635883997, 941.774028601319, 941.1326437532855, 938.7357569787852, 926.9589238677389, 895.1166589230686, 917.3118053139839, 906.0876967236467, 889.8762378599754, 916.0103640057225, 920.0781233192367, 918.5378038526616, 915.6749719636, 918.1307222594113, 920.605765856565, 915.6090236364346, 918.0253831141998, 913.0775294027952, 918.0675238995958, 909.1602983839865, 914.6370305943794, 915.4491212012687, 914.950924049008, 915.6205134453074, 903.5174003438727, 910.1833192132955, 901.7664599196949, 901.2449769010341, 901.9477813933308, 907.7120111615325, 906.524584928395, 885.9765495674028, 955.2312875626868, 939.1266198719542, 940.1150440496896, 940.8346910320382, 923.483466704876, 923.5963728243656, 893.7723313961045, 914.7807279283948, 920.4086259079685, 913.6899166657098, 896.4155369951825, 905.4312819587233, 892.02438792043, 910.282146673834, 913.7933895063876, 875.998644353494, 880.9062836670572, 813.6246637472833, 816.8510063830524, 909.557246229743, 903.5769127958515, 916.9778111054869, 924.1002295292109, 916.5052052170205, 902.505576468703, 906.812662274873, 921.0083092727212, 900.4394736246771, 916.7430979716515, 915.8038086407656, 916.251189275168, 912.508651675354, 907.8103595991846, 909.6580309826314, 915.3338748689043, 910.7662168458581, 915.6031515341596, 916.0525602457824, 912.8994363509294, 910.8363448371333, 901.4912802014143, 835.8515313708905, 906.4046936243656, 897.9359799808709, 909.7320506433268, 904.2506069550251, 909.2682453569585, 905.2442333595307, 894.3156062813515, 890.3719409303709, 890.9260922854068, 888.8722037690923, 892.5512546295635, 891.2417657375681, 831.7907018233722, 789.4409215901375, 682.9852932881222, 797.6513422698563, 816.9827851190827, 826.1721970246892, 816.1060626293669, 813.3237261359689, 817.9645712591227, 815.6637792975126, 810.1817301797397, 815.4900320956289, 817.611456668721, 815.8157918133211, 819.5040281258179, 817.473338177496, 813.5854537839635, 812.688126469836, 807.9260170276807, 832.4685824523644, 825.4988528446521, 813.3556016203914, 820.0225237972351, 811.9078883717661, 819.7732515177256, 819.0177277013065, 815.2986270901832, 823.0769003526209, 899.2593811036886, 884.1171764408028, 884.9777180622714, 904.3377621909048, 907.4186510894132, 906.1702436474258, 902.4930358584957, 893.5038571130708, 903.3722181640097, 889.0122007762926, 891.498162221086, 889.3936969161292, 900.8858583828354, 897.6641742348395, 903.0175713419445, 902.2724182626067, 893.3507678201163, 894.3978442661269, 895.1555687409257, 896.3307785644919, 895.9555751294743, 886.8650219311022, 877.012819535769, 864.3709682948879, 868.1191376695913, 725.3320277087231, 874.3273574961426, 877.9945329645008, 880.6865529002293, 879.5351792176644, 879.3881514961266]
Elapsed: 0.048417049793729176~0.0025429903568122674
Time per graph: 0.001113997902541521~6.411172081289137e-05
Speed: 900.4106809559042~47.87795744108154
Total Time: 0.0496
best val loss: 0.3532356917858124 test_score: 0.7907

Testing...
Test loss: 0.4874 score: 0.7907 time: 0.04s
test Score 0.7907
Epoch Time List: [0.3441138418857008, 0.1950901160016656, 0.20092373108491302, 0.19552739395294338, 0.19563883496448398, 0.20061134896241128, 0.20279942196793854, 0.21964536106679589, 0.1906130739953369, 0.1907715218840167, 0.2064600430894643, 0.20916516007855535, 0.20820258208550513, 0.20331831311341375, 0.20177148596849293, 0.19249879790004343, 0.18681801797356457, 0.18902222800534219, 0.18872436101082712, 0.18816109385807067, 0.19814793404657394, 0.18872316204942763, 0.18934238387737423, 0.19064730196259916, 0.18730703508481383, 0.18768095003906637, 0.1980663602007553, 0.20147660793736577, 0.2011338050942868, 0.1956611139466986, 0.20366825792007148, 0.20684987702406943, 0.19591468898579478, 0.1942613379796967, 0.19509483501315117, 0.19485604704823345, 0.20094562007579952, 0.20161761494819075, 0.19522095005959272, 0.2035598891088739, 0.20075422094669193, 0.20354284311179072, 0.20091752894222736, 0.19471306295599788, 0.2007422869792208, 0.19578393595293164, 0.20067987206857651, 0.19496597605757415, 0.20105283381417394, 0.201496192952618, 0.20558978407643735, 0.22088609787169844, 0.21799896901939064, 0.199370467918925, 0.20595013699494302, 0.20323498011566699, 0.2000330031150952, 0.1997889019548893, 0.19960507203359157, 0.19977073604241014, 0.20053704199381173, 0.19961461902130395, 0.19992803607601672, 0.2002224219031632, 0.20041608402971178, 0.20041008107364178, 0.20000288600567728, 0.20015437889378518, 0.19995300192385912, 0.2014604409923777, 0.20105557306669652, 0.2001083471113816, 0.20056632487103343, 0.20011935394722968, 0.1990925749996677, 0.2008088689763099, 0.20834908890537918, 0.20071389386430383, 0.2071437140693888, 0.20870618906337768, 0.21013470296747983, 0.20743860502261668, 0.19951007200870663, 0.20096610486507416, 0.19967496395111084, 0.2020885208621621, 0.20705822098534554, 0.20593356399331242, 0.19996599503792822, 0.20068104087840766, 0.2003224149812013, 0.2003873320063576, 0.2082030720775947, 0.2014931709272787, 0.20841177506372333, 0.20234542491380125, 0.20927221502643079, 0.2029490970307961, 0.2033659789012745, 0.20966396597214043, 0.2120379889383912, 0.20290385093539953, 0.2082310609985143, 0.19868628901895136, 0.1960695228772238, 0.20064436993561685, 0.19624863611534238, 0.2906196870608255, 0.19985546101815999, 0.20065009593963623, 0.20116747508291155, 0.20166447816882282, 0.2030768000986427, 0.2139252470806241, 0.20509520883206278, 0.29588500689715147, 0.2060446630930528, 0.2050796200055629, 0.2083055858965963, 0.21309774683322757, 0.23612300481181592, 0.23663849802687764, 0.3134113649139181, 0.2056823680177331, 0.20347587717697024, 0.20329672913067043, 0.20429866597987711, 0.20571523602120578, 0.20626207499299198, 0.20393884892109782, 0.2123550169635564, 0.2049756619380787, 0.20469553698785603, 0.20362186303827912, 0.20439256087411195, 0.20403858297504485, 0.2043572209076956, 0.20417349401395768, 0.2044085959205404, 0.20381092105526477, 0.20335583307314664, 0.20395090396050364, 0.205458867829293, 0.2055725009413436, 0.20860061410348862, 0.20632765791378915, 0.2056089781690389, 0.20455074787605554, 0.20527620997745544, 0.20780583401210606, 0.20562633499503136, 0.20966818800661713, 0.21034027903806418, 0.2091436720220372, 0.20960277412086725, 0.20979694498237222, 0.20943492802325636, 0.22341804602183402, 0.2287456028861925, 0.24922992300707847, 0.24721079494338483, 0.23140337283257395, 0.23345452698413283, 0.23386672511696815, 0.23507835110649467, 0.23470628901850432, 0.23609194590244442, 0.2348163459682837, 0.23376382689457387, 0.2344475541030988, 0.23396093596238643, 0.2332322810543701, 0.23427352192811668, 0.2334625399671495, 0.23383455001749098, 0.23626814782619476, 0.23466603516135365, 0.2316210650606081, 0.2361152188386768, 0.2346087850164622, 0.23582808184437454, 0.23514923395123333, 0.23521520500071347, 0.23542078107129782, 0.23540780309122056, 0.223627574974671, 0.21102545910980552, 0.21241260005626827, 0.21090190310496837, 0.20790383988060057, 0.2074976839357987, 0.20827368996106088, 0.20961148699279875, 0.20981431903783232, 0.21117048803716898, 0.20969018898904324, 0.21044695912860334, 0.2095831100596115, 0.21079597005154938, 0.20893395494204015, 0.20962894102558494, 0.21076088806148618, 0.21110858395695686, 0.21034896711353213, 0.2114922960754484, 0.2102468031225726, 0.21190923394169658, 0.21297021687496454, 0.2153109321370721, 0.2172092798864469, 0.2275579150300473, 0.22172905306797475, 0.21631598589010537, 0.21418273309245706, 0.21529363305307925, 0.21436399803496897]
Total Epoch List: [103, 113]
Total Time List: [0.05022927199024707, 0.0496497789863497]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc617b50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7448;  Loss pred: 0.7448; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0457 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8759 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7420;  Loss pred: 0.7420; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9581 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8205 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7087;  Loss pred: 0.7087; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9008 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7806 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6216;  Loss pred: 0.6216; Loss self: 0.0000; time: 0.13s
Val loss: 0.8625 score: 0.5227 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7591 score: 0.5116 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5499;  Loss pred: 0.5499; Loss self: 0.0000; time: 0.13s
Val loss: 0.8263 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7457 score: 0.5116 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.4738;  Loss pred: 0.4738; Loss self: 0.0000; time: 0.14s
Val loss: 0.7979 score: 0.4773 time: 0.04s
Test loss: 0.7317 score: 0.4884 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4122;  Loss pred: 0.4122; Loss self: 0.0000; time: 0.14s
Val loss: 0.7712 score: 0.4545 time: 0.04s
Test loss: 0.7168 score: 0.5349 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.3431;  Loss pred: 0.3431; Loss self: 0.0000; time: 0.14s
Val loss: 0.7466 score: 0.5455 time: 0.04s
Test loss: 0.7036 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.2964;  Loss pred: 0.2964; Loss self: 0.0000; time: 0.14s
Val loss: 0.7270 score: 0.5682 time: 0.04s
Test loss: 0.6923 score: 0.5349 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.2456;  Loss pred: 0.2456; Loss self: 0.0000; time: 0.14s
Val loss: 0.7127 score: 0.4545 time: 0.04s
Test loss: 0.6845 score: 0.5349 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.2025;  Loss pred: 0.2025; Loss self: 0.0000; time: 0.14s
Val loss: 0.7038 score: 0.4545 time: 0.04s
Test loss: 0.6848 score: 0.5116 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.1690;  Loss pred: 0.1690; Loss self: 0.0000; time: 0.14s
Val loss: 0.6978 score: 0.5000 time: 0.04s
Test loss: 0.6899 score: 0.5349 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.1413;  Loss pred: 0.1413; Loss self: 0.0000; time: 0.14s
Val loss: 0.6932 score: 0.5227 time: 0.05s
Test loss: 0.6990 score: 0.5349 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.1220;  Loss pred: 0.1220; Loss self: 0.0000; time: 0.14s
Val loss: 0.6904 score: 0.5455 time: 0.04s
Test loss: 0.7086 score: 0.5116 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.0998;  Loss pred: 0.0998; Loss self: 0.0000; time: 0.14s
Val loss: 0.6909 score: 0.5227 time: 0.04s
Test loss: 0.7200 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.0867;  Loss pred: 0.0867; Loss self: 0.0000; time: 0.14s
Val loss: 0.6932 score: 0.5000 time: 0.04s
Test loss: 0.7317 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.13s
Val loss: 0.6965 score: 0.5000 time: 0.04s
Test loss: 0.7431 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.13s
Val loss: 0.7014 score: 0.5000 time: 0.04s
Test loss: 0.7537 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.0573;  Loss pred: 0.0573; Loss self: 0.0000; time: 0.13s
Val loss: 0.7077 score: 0.4773 time: 0.04s
Test loss: 0.7645 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.13s
Val loss: 0.7150 score: 0.4773 time: 0.04s
Test loss: 0.7750 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.13s
Val loss: 0.7230 score: 0.4773 time: 0.04s
Test loss: 0.7857 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.13s
Val loss: 0.7304 score: 0.4773 time: 0.04s
Test loss: 0.7953 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.14s
Val loss: 0.7375 score: 0.5000 time: 0.04s
Test loss: 0.8042 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.13s
Val loss: 0.7446 score: 0.5000 time: 0.04s
Test loss: 0.8129 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.14s
Val loss: 0.7514 score: 0.5000 time: 0.04s
Test loss: 0.8211 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0229;  Loss pred: 0.0229; Loss self: 0.0000; time: 0.14s
Val loss: 0.7587 score: 0.5000 time: 0.04s
Test loss: 0.8292 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.14s
Val loss: 0.7668 score: 0.5000 time: 0.04s
Test loss: 0.8374 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0173;  Loss pred: 0.0173; Loss self: 0.0000; time: 0.14s
Val loss: 0.7739 score: 0.5000 time: 0.04s
Test loss: 0.8445 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.14s
Val loss: 0.7810 score: 0.5000 time: 0.04s
Test loss: 0.8513 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0148;  Loss pred: 0.0148; Loss self: 0.0000; time: 0.14s
Val loss: 0.7868 score: 0.5000 time: 0.04s
Test loss: 0.8572 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.14s
Val loss: 0.7921 score: 0.5000 time: 0.04s
Test loss: 0.8623 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0114;  Loss pred: 0.0114; Loss self: 0.0000; time: 0.14s
Val loss: 0.7970 score: 0.5000 time: 0.04s
Test loss: 0.8657 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.14s
Val loss: 0.8017 score: 0.5227 time: 0.04s
Test loss: 0.8678 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.14s
Val loss: 0.8065 score: 0.5227 time: 0.04s
Test loss: 0.8691 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 013,   Train_Loss: 0.1220,   Val_Loss: 0.6904,   Val_Precision: 0.5278,   Val_Recall: 0.8636,   Val_accuracy: 0.6552,   Val_Score: 0.5455,   Val_Loss: 0.6904,   Test_Precision: 0.5000,   Test_Recall: 0.9048,   Test_accuracy: 0.6441,   Test_Score: 0.5116,   Test_loss: 0.7086


[0.04664938198402524, 0.046703288913704455, 0.04678865405730903, 0.047170124016702175, 0.04711302404757589, 0.04693800292443484, 0.04828996502328664, 0.044818797963671386, 0.04485939897131175, 0.04477922304067761, 0.05405346897896379, 0.048318655928596854, 0.049058952019549906, 0.048568419995717704, 0.04467657604254782, 0.04496546392329037, 0.04506860708352178, 0.04562680597882718, 0.045558333047665656, 0.045264746993780136, 0.04583811992779374, 0.045046273968182504, 0.04664070100989193, 0.04561574407853186, 0.04516893799882382, 0.04505250102374703, 0.046471651061438024, 0.04687806696165353, 0.04692500305827707, 0.047406019060872495, 0.05243396293371916, 0.05180499900598079, 0.04701562400441617, 0.046596599044278264, 0.047243534005247056, 0.0469257109798491, 0.04699444002471864, 0.047257601050660014, 0.04743034893181175, 0.04713687999173999, 0.04691480693873018, 0.0470330030657351, 0.047004456049762666, 0.04705501301214099, 0.047220682026818395, 0.04708700196351856, 0.04691958904732019, 0.04730397101957351, 0.04718265309929848, 0.04728842794429511, 0.05175014410633594, 0.056990816025063396, 0.05039940704591572, 0.046779111959040165, 0.04830624896567315, 0.04861109901685268, 0.04662461008410901, 0.04686780704651028, 0.04670650698244572, 0.04659123800229281, 0.0467024811077863, 0.04677323310170323, 0.04660260688979179, 0.04676140798255801, 0.04707098891958594, 0.04719411791302264, 0.04721536405850202, 0.0469146219547838, 0.046916902996599674, 0.048341669025830925, 0.04719952307641506, 0.047018268960528076, 0.046720337006263435, 0.04675217706244439, 0.04687155003193766, 0.047467043972574174, 0.04915560397785157, 0.04796624195296317, 0.048560420982539654, 0.04944507800973952, 0.048034391016699374, 0.04782202607020736, 0.04790222004521638, 0.0480519849807024, 0.04792345897294581, 0.04779461701400578, 0.048055446008220315, 0.047928957967087626, 0.048188679036684334, 0.04792675795033574, 0.048396305995993316, 0.04810651496518403, 0.04806383990217, 0.04809001099783927, 0.04805484297685325, 0.04869856406003237, 0.048341909889131784, 0.048793121008202434, 0.04882135393563658, 0.04878331196960062, 0.04847352404613048, 0.04853701789397746, 0.049662714009173214, 0.045015275944024324, 0.045787223032675683, 0.04573908296879381, 0.045704097021371126, 0.046562826028093696, 0.04655713390093297, 0.04811068600974977, 0.04700580006465316, 0.046718380064703524, 0.04706191807053983, 0.04796882497612387, 0.04749118001200259, 0.0482049600686878, 0.047238101018592715, 0.04705658904276788, 0.049086833954788744, 0.04881336505059153, 0.05284992197994143, 0.05264117894694209, 0.04727574891876429, 0.047588643967173994, 0.04689317394513637, 0.04653174907434732, 0.046917354920879006, 0.0476451349677518, 0.04741883499082178, 0.04668796097394079, 0.04775445908308029, 0.0469051799736917, 0.046953288023360074, 0.04693036200478673, 0.04712284088600427, 0.04736672097351402, 0.047270511044189334, 0.04697739391122013, 0.047212994075380266, 0.046963577973656356, 0.046940537984482944, 0.04710266902111471, 0.04720935900695622, 0.04769874201156199, 0.05144454294349998, 0.04744017799384892, 0.04788760107476264, 0.047266664914786816, 0.04755318898241967, 0.04729077499359846, 0.047500993008725345, 0.04808145994320512, 0.048294423962943256, 0.04826438508462161, 0.048375908052548766, 0.048176505020819604, 0.04824729007668793, 0.051695696893148124, 0.05446892709005624, 0.06295889592729509, 0.05390826507937163, 0.052632687962614, 0.05204726103693247, 0.05268922995310277, 0.05286947696004063, 0.052569514024071395, 0.052717800019308925, 0.053074512048624456, 0.05272903200238943, 0.052592218038626015, 0.05270797701086849, 0.0524707610020414, 0.05260110390372574, 0.05285246903076768, 0.05291082593612373, 0.05322269501630217, 0.05165360099636018, 0.05208971502725035, 0.05286740500014275, 0.052437584032304585, 0.05296167288906872, 0.05245352897327393, 0.052501916070468724, 0.0527414110256359, 0.052242992096580565, 0.047817126964218915, 0.04863608710002154, 0.04858879395760596, 0.04754860606044531, 0.04738716792780906, 0.0474524520104751, 0.04764579702168703, 0.04812514199875295, 0.047599427052773535, 0.048368290066719055, 0.04823341406881809, 0.048347542993724346, 0.047730796970427036, 0.047902101068757474, 0.047618121025152504, 0.04765744705218822, 0.048133388976566494, 0.04807703895494342, 0.04803634306881577, 0.047973360982723534, 0.04799345100764185, 0.048485393985174596, 0.049030070076696575, 0.04974715900607407, 0.04953237192239612, 0.05928319494705647, 0.04918066400568932, 0.04897524800617248, 0.048825543955899775, 0.048889460042119026, 0.04889763402752578, 0.046628451091237366, 0.04626393795479089, 0.04605692008044571, 0.04649939900264144, 0.046528781997039914, 0.04726438398938626, 0.04726244101766497, 0.0474069369956851, 0.047208481933921576, 0.047566057997755706, 0.04694484104402363, 0.04771374596748501, 0.04779551096726209, 0.04705670499242842, 0.04665053705684841, 0.045858014025725424, 0.046333282021805644, 0.04584120202343911, 0.045709287049248815, 0.045712737017311156, 0.04611509304959327, 0.045839632977731526, 0.04653691500425339, 0.04635533189866692, 0.04704591596964747, 0.046650447067804635, 0.04656405502464622, 0.046516160015016794, 0.04676650802139193, 0.04634831298608333, 0.04651201795786619, 0.04643281304743141, 0.04664584808051586, 0.04707399709150195]
[0.0010602132269096646, 0.001061438384402374, 0.001063378501302478, 0.0010720482731068676, 0.0010707505465358156, 0.0010667727937371555, 0.0010974992050746964, 0.0010186090446288952, 0.0010195317948025397, 0.0010177096145608548, 0.0012284879313400861, 0.001098151271104474, 0.0011149761822624978, 0.0011038277271754023, 0.0010153767282397232, 0.001021942361892963, 0.001024286524625495, 0.0010369728631551632, 0.0010354166601742195, 0.0010287442498586395, 0.0010417754529044032, 0.0010237789538223296, 0.0010600159320429984, 0.0010367214563302696, 0.0010265667727005414, 0.0010239204778124324, 0.001056173887759955, 0.0010654106127648529, 0.0010664773422335697, 0.0010774095241107386, 0.0011916809757663445, 0.0011773863410450178, 0.0010685369091912764, 0.0010590136146426878, 0.0010737166819374331, 0.0010664934313602068, 0.0010680554551072419, 0.0010740363875150003, 0.0010779624757229942, 0.0010712927270849998, 0.0010662456122438678, 0.001068931887857616, 0.0010682830920400606, 0.0010694321139122951, 0.0010731973187913272, 0.0010701591355345126, 0.0010663542965300042, 0.0010750902504448525, 0.0010723330249840562, 0.0010747369987339798, 0.0011761396387803622, 0.0012952458187514408, 0.0011454410692253573, 0.001063161635432731, 0.0010978692946743897, 0.00110479770492847, 0.0010596502291842955, 0.0010651774328752335, 0.0010615115223283117, 0.001058891772779382, 0.0010614200251769614, 0.0010630280250387098, 0.001059150156586177, 0.001062759272330864, 0.0010697952027178624, 0.0010725935889323327, 0.001073076455875046, 0.0010662414080632682, 0.00106629324992272, 0.001098674296041612, 0.0010727164335548878, 0.0010685970218301836, 0.0010618258410514418, 0.001062549478691918, 0.001065262500725856, 0.0010787964539221402, 0.0011171728176784447, 0.0010901418625673448, 0.0011036459314213557, 0.0011237517729486253, 0.0010916907049249858, 0.001086864228868349, 0.0010886868192094632, 0.0010920905677432363, 0.0010891695221124048, 0.0010862412957728586, 0.0010921692274595525, 0.0010892944992519915, 0.0010951972508337349, 0.0010892444988712668, 0.0010999160453634845, 0.0010933298855723644, 0.001092359997776591, 0.001092954795405438, 0.0010921555222012103, 0.0011067855468189175, 0.0010986797702075405, 0.001108934568368237, 0.0011095762258099223, 0.0011087116356727413, 0.00110167100104842, 0.0011031140430449423, 0.0011286980456630276, 0.0010468668824191703, 0.0010648191402947834, 0.0010636996039254374, 0.001062885977241189, 0.001082856419257993, 0.0010827240442077433, 0.0011188531630174365, 0.0010931581410384456, 0.0010864739549931051, 0.0010944632109427868, 0.001115554069212183, 0.001104446046790758, 0.0011210455829927394, 0.0010985604888044816, 0.0010943392800643694, 0.0011415542780183429, 0.0011351945360602682, 0.0012290679530218937, 0.0012242134638823742, 0.0010994360213666115, 0.0011067126503993953, 0.0010905389289566597, 0.001082133699403426, 0.0010911012772297443, 0.0011080263945988792, 0.0011027636044377157, 0.0010857665342776928, 0.0011105688158855882, 0.0010908181389230628, 0.0010919369307758157, 0.0010914037675531798, 0.0010958800206047504, 0.0011015516505468376, 0.0010993142103299844, 0.001092497532819073, 0.0010979766064041921, 0.0010921762319454967, 0.0010916404182437895, 0.0010954109074677839, 0.0010978920699292144, 0.0011092730700363253, 0.0011963847196162787, 0.0011032599533453238, 0.0011136651412735497, 0.0010992247654601586, 0.0011058881158702248, 0.001099785464967406, 0.0011046742560168684, 0.0011181734870512818, 0.001123126138673099, 0.0011224275601074793, 0.0011250211175011342, 0.0011203838376934792, 0.0011220300017834404, 0.0012022255091429796, 0.0012667192346524708, 0.0014641603704022114, 0.0012536805832412006, 0.0012240159991305582, 0.0012104014194635458, 0.0012253309291419248, 0.0012295227200009448, 0.0012225468377691023, 0.0012259953492862541, 0.0012342909778749874, 0.001226256558195103, 0.0012230748381075818, 0.0012257669072294997, 0.001220250255861428, 0.0012232814861331568, 0.001229127186762039, 0.0012304843240959007, 0.001237737093402376, 0.001201246534799074, 0.0012113887215639617, 0.0012294745348870408, 0.0012194786984256881, 0.0012316668113736913, 0.0012198495110063705, 0.001220974792336482, 0.0012265444424566488, 0.001214953304571641, 0.0011120262084702074, 0.0011310717930237567, 0.001129971952502464, 0.0011057815362894258, 0.0011020271611118385, 0.0011035453955924443, 0.001108041791202024, 0.0011191893488082083, 0.0011069634198319427, 0.001124843955039978, 0.0011217073039260022, 0.0011243614649703337, 0.0011100185341959775, 0.0011140023504362204, 0.0011073981633756397, 0.0011083127221439121, 0.0011193811389899186, 0.0011180706733707772, 0.0011171242574143203, 0.0011156595577377566, 0.001116126767619578, 0.0011275673019808046, 0.0011402341878301529, 0.001156910674559862, 0.0011519156261022355, 0.0013786789522571273, 0.001143736372225333, 0.0011389592559574995, 0.0011354777664162737, 0.0011369641870260239, 0.001137154279709902, 0.001084382583517148, 0.0010759055338323462, 0.001071091164661528, 0.0010813813721544522, 0.0010820646976055794, 0.0010991717206834013, 0.0010991265352945341, 0.0011024869068763977, 0.0010978716728818972, 0.0011061873952966444, 0.0010917404893958985, 0.0011096219992438375, 0.0011115235108665603, 0.0010943419765681028, 0.0010848962106243816, 0.0010664654424587308, 0.0010775181865536195, 0.0010660744656613747, 0.001063006675563926, 0.0010630869073793292, 0.0010724440244091458, 0.0010660379762263146, 0.0010822538373082182, 0.0010780309743876027, 0.001094091069061569, 0.0010848941178559217, 0.001082885000573168, 0.0010817711631399254, 0.0010875932097998123, 0.001077867743862403, 0.0010816748362294463, 0.0010798328615681722, 0.0010847871646631597, 0.0010947441184070222]
[943.2064933907913, 942.1178041936312, 940.3989254768186, 932.7938163660607, 933.9243423552629, 937.406733533919, 911.162391167235, 981.7309253957441, 980.8423877488563, 982.598558265074, 814.0088107411507, 910.6213563767426, 896.8801449828377, 905.9384679155611, 984.8561348590482, 978.5287676573865, 976.289325260454, 964.3453898661658, 965.7947746675621, 972.0588962100258, 959.8997530725686, 976.7733515779458, 943.382047166661, 964.5792453642715, 974.1207553107788, 976.6383441578025, 946.8137885144134, 938.6052551183947, 937.6664279671021, 928.1521813401176, 839.1507629438503, 849.3388832016053, 935.8591092158447, 944.2749235451529, 931.3443823892003, 937.6522823254513, 936.2809723204789, 931.0671515642981, 927.6760764137874, 933.4516838558327, 937.8702135013182, 935.5133019786965, 936.0814632854828, 935.0757163460406, 931.7950972205516, 934.4404647823986, 937.7746244883843, 930.1544680423048, 932.5461183244527, 930.4601974045571, 850.2391782637169, 772.0542197649828, 873.0261441352748, 940.5907499596495, 910.8552401008571, 905.1430823389925, 943.7076239485046, 938.8107268670722, 942.0528924703589, 944.3835769685859, 942.1340998661475, 940.7089713966717, 944.1531909159811, 940.9468597783023, 934.7583513736606, 932.3195759499245, 931.9000473126071, 937.8739115154141, 937.8283132454186, 910.1878542192866, 932.2128092007392, 935.8064635883997, 941.774028601319, 941.1326437532855, 938.7357569787852, 926.9589238677389, 895.1166589230686, 917.3118053139839, 906.0876967236467, 889.8762378599754, 916.0103640057225, 920.0781233192367, 918.5378038526616, 915.6749719636, 918.1307222594113, 920.605765856565, 915.6090236364346, 918.0253831141998, 913.0775294027952, 918.0675238995958, 909.1602983839865, 914.6370305943794, 915.4491212012687, 914.950924049008, 915.6205134453074, 903.5174003438727, 910.1833192132955, 901.7664599196949, 901.2449769010341, 901.9477813933308, 907.7120111615325, 906.524584928395, 885.9765495674028, 955.2312875626868, 939.1266198719542, 940.1150440496896, 940.8346910320382, 923.483466704876, 923.5963728243656, 893.7723313961045, 914.7807279283948, 920.4086259079685, 913.6899166657098, 896.4155369951825, 905.4312819587233, 892.02438792043, 910.282146673834, 913.7933895063876, 875.998644353494, 880.9062836670572, 813.6246637472833, 816.8510063830524, 909.557246229743, 903.5769127958515, 916.9778111054869, 924.1002295292109, 916.5052052170205, 902.505576468703, 906.812662274873, 921.0083092727212, 900.4394736246771, 916.7430979716515, 915.8038086407656, 916.251189275168, 912.508651675354, 907.8103595991846, 909.6580309826314, 915.3338748689043, 910.7662168458581, 915.6031515341596, 916.0525602457824, 912.8994363509294, 910.8363448371333, 901.4912802014143, 835.8515313708905, 906.4046936243656, 897.9359799808709, 909.7320506433268, 904.2506069550251, 909.2682453569585, 905.2442333595307, 894.3156062813515, 890.3719409303709, 890.9260922854068, 888.8722037690923, 892.5512546295635, 891.2417657375681, 831.7907018233722, 789.4409215901375, 682.9852932881222, 797.6513422698563, 816.9827851190827, 826.1721970246892, 816.1060626293669, 813.3237261359689, 817.9645712591227, 815.6637792975126, 810.1817301797397, 815.4900320956289, 817.611456668721, 815.8157918133211, 819.5040281258179, 817.473338177496, 813.5854537839635, 812.688126469836, 807.9260170276807, 832.4685824523644, 825.4988528446521, 813.3556016203914, 820.0225237972351, 811.9078883717661, 819.7732515177256, 819.0177277013065, 815.2986270901832, 823.0769003526209, 899.2593811036886, 884.1171764408028, 884.9777180622714, 904.3377621909048, 907.4186510894132, 906.1702436474258, 902.4930358584957, 893.5038571130708, 903.3722181640097, 889.0122007762926, 891.498162221086, 889.3936969161292, 900.8858583828354, 897.6641742348395, 903.0175713419445, 902.2724182626067, 893.3507678201163, 894.3978442661269, 895.1555687409257, 896.3307785644919, 895.9555751294743, 886.8650219311022, 877.012819535769, 864.3709682948879, 868.1191376695913, 725.3320277087231, 874.3273574961426, 877.9945329645008, 880.6865529002293, 879.5351792176644, 879.3881514961266, 922.1837524875614, 929.4496296882378, 933.6273447050669, 924.7431347995991, 924.1591581472214, 909.7759532771256, 909.8133544124006, 907.0402503311655, 910.853267007987, 904.0059616045722, 915.9685930063271, 901.2077993059435, 899.6660801357095, 913.7911378818139, 921.7471590434242, 937.6768905840071, 928.058581728855, 938.0207782948978, 940.7278646387608, 940.6568673347243, 932.4495985242137, 938.0528858267475, 923.9976478043266, 927.6171313798012, 914.0006972707735, 921.7489371002415, 923.4590925820404, 924.4099251984327, 919.4614226987173, 927.7576081984105, 924.4922471209995, 926.069242371235, 921.8398157490303, 913.4554670685186]
Elapsed: 0.048175149831455204~0.002449646664359699
Time per graph: 0.0011100016019743204~6.062446808785376e-05
Speed: 903.3627694551367~45.30044895482972
Total Time: 0.0478
best val loss: 0.6903998851776123 test_score: 0.5116

Testing...
Test loss: 0.6923 score: 0.5349 time: 0.04s
test Score 0.5349
Epoch Time List: [0.3441138418857008, 0.1950901160016656, 0.20092373108491302, 0.19552739395294338, 0.19563883496448398, 0.20061134896241128, 0.20279942196793854, 0.21964536106679589, 0.1906130739953369, 0.1907715218840167, 0.2064600430894643, 0.20916516007855535, 0.20820258208550513, 0.20331831311341375, 0.20177148596849293, 0.19249879790004343, 0.18681801797356457, 0.18902222800534219, 0.18872436101082712, 0.18816109385807067, 0.19814793404657394, 0.18872316204942763, 0.18934238387737423, 0.19064730196259916, 0.18730703508481383, 0.18768095003906637, 0.1980663602007553, 0.20147660793736577, 0.2011338050942868, 0.1956611139466986, 0.20366825792007148, 0.20684987702406943, 0.19591468898579478, 0.1942613379796967, 0.19509483501315117, 0.19485604704823345, 0.20094562007579952, 0.20161761494819075, 0.19522095005959272, 0.2035598891088739, 0.20075422094669193, 0.20354284311179072, 0.20091752894222736, 0.19471306295599788, 0.2007422869792208, 0.19578393595293164, 0.20067987206857651, 0.19496597605757415, 0.20105283381417394, 0.201496192952618, 0.20558978407643735, 0.22088609787169844, 0.21799896901939064, 0.199370467918925, 0.20595013699494302, 0.20323498011566699, 0.2000330031150952, 0.1997889019548893, 0.19960507203359157, 0.19977073604241014, 0.20053704199381173, 0.19961461902130395, 0.19992803607601672, 0.2002224219031632, 0.20041608402971178, 0.20041008107364178, 0.20000288600567728, 0.20015437889378518, 0.19995300192385912, 0.2014604409923777, 0.20105557306669652, 0.2001083471113816, 0.20056632487103343, 0.20011935394722968, 0.1990925749996677, 0.2008088689763099, 0.20834908890537918, 0.20071389386430383, 0.2071437140693888, 0.20870618906337768, 0.21013470296747983, 0.20743860502261668, 0.19951007200870663, 0.20096610486507416, 0.19967496395111084, 0.2020885208621621, 0.20705822098534554, 0.20593356399331242, 0.19996599503792822, 0.20068104087840766, 0.2003224149812013, 0.2003873320063576, 0.2082030720775947, 0.2014931709272787, 0.20841177506372333, 0.20234542491380125, 0.20927221502643079, 0.2029490970307961, 0.2033659789012745, 0.20966396597214043, 0.2120379889383912, 0.20290385093539953, 0.2082310609985143, 0.19868628901895136, 0.1960695228772238, 0.20064436993561685, 0.19624863611534238, 0.2906196870608255, 0.19985546101815999, 0.20065009593963623, 0.20116747508291155, 0.20166447816882282, 0.2030768000986427, 0.2139252470806241, 0.20509520883206278, 0.29588500689715147, 0.2060446630930528, 0.2050796200055629, 0.2083055858965963, 0.21309774683322757, 0.23612300481181592, 0.23663849802687764, 0.3134113649139181, 0.2056823680177331, 0.20347587717697024, 0.20329672913067043, 0.20429866597987711, 0.20571523602120578, 0.20626207499299198, 0.20393884892109782, 0.2123550169635564, 0.2049756619380787, 0.20469553698785603, 0.20362186303827912, 0.20439256087411195, 0.20403858297504485, 0.2043572209076956, 0.20417349401395768, 0.2044085959205404, 0.20381092105526477, 0.20335583307314664, 0.20395090396050364, 0.205458867829293, 0.2055725009413436, 0.20860061410348862, 0.20632765791378915, 0.2056089781690389, 0.20455074787605554, 0.20527620997745544, 0.20780583401210606, 0.20562633499503136, 0.20966818800661713, 0.21034027903806418, 0.2091436720220372, 0.20960277412086725, 0.20979694498237222, 0.20943492802325636, 0.22341804602183402, 0.2287456028861925, 0.24922992300707847, 0.24721079494338483, 0.23140337283257395, 0.23345452698413283, 0.23386672511696815, 0.23507835110649467, 0.23470628901850432, 0.23609194590244442, 0.2348163459682837, 0.23376382689457387, 0.2344475541030988, 0.23396093596238643, 0.2332322810543701, 0.23427352192811668, 0.2334625399671495, 0.23383455001749098, 0.23626814782619476, 0.23466603516135365, 0.2316210650606081, 0.2361152188386768, 0.2346087850164622, 0.23582808184437454, 0.23514923395123333, 0.23521520500071347, 0.23542078107129782, 0.23540780309122056, 0.223627574974671, 0.21102545910980552, 0.21241260005626827, 0.21090190310496837, 0.20790383988060057, 0.2074976839357987, 0.20827368996106088, 0.20961148699279875, 0.20981431903783232, 0.21117048803716898, 0.20969018898904324, 0.21044695912860334, 0.2095831100596115, 0.21079597005154938, 0.20893395494204015, 0.20962894102558494, 0.21076088806148618, 0.21110858395695686, 0.21034896711353213, 0.2114922960754484, 0.2102468031225726, 0.21190923394169658, 0.21297021687496454, 0.2153109321370721, 0.2172092798864469, 0.2275579150300473, 0.22172905306797475, 0.21631598589010537, 0.21418273309245706, 0.21529363305307925, 0.21436399803496897, 0.2225921939825639, 0.2189920371165499, 0.21916303283069283, 0.21922026900574565, 0.21898593404330313, 0.22157003998290747, 0.2224944360787049, 0.22219876910094172, 0.22254275309387594, 0.22386397700756788, 0.2232801279751584, 0.22189612698275596, 0.23528560996055603, 0.2225023319479078, 0.22194501606281847, 0.2197909450624138, 0.21686547598801553, 0.2184976078569889, 0.21673714101780206, 0.21675970894284546, 0.2166014319518581, 0.21627046109642833, 0.2204309661174193, 0.21901094005443156, 0.22009416797664016, 0.22075069008860737, 0.22046371595934033, 0.22128574398811907, 0.22008819598704576, 0.2198646430624649, 0.22018857905641198, 0.22001431905664504, 0.22142010019160807, 0.22106635197997093]
Total Epoch List: [103, 113, 34]
Total Time List: [0.05022927199024707, 0.0496497789863497, 0.04784597107209265]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc617970>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9581 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8971 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.6976;  Loss pred: 0.6976; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8733 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8281 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.6818;  Loss pred: 0.6818; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8173 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7814 score: 0.5000 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7811 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7552 score: 0.5000 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6011;  Loss pred: 0.6011; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7655 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7429 score: 0.5000 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7649 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7402 score: 0.5000 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7666 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7460 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.5022;  Loss pred: 0.5022; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7629 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7487 score: 0.5000 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.4698;  Loss pred: 0.4698; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7606 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7496 score: 0.5000 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.4435;  Loss pred: 0.4435; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7605 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7511 score: 0.5000 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.4125;  Loss pred: 0.4125; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7588 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7515 score: 0.5000 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.3947;  Loss pred: 0.3947; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7569 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7488 score: 0.5000 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.3745;  Loss pred: 0.3745; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7632 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7488 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.3578;  Loss pred: 0.3578; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7718 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7505 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.3465;  Loss pred: 0.3465; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7853 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7560 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.3285;  Loss pred: 0.3285; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7965 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7621 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3183;  Loss pred: 0.3183; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8053 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7674 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3045;  Loss pred: 0.3045; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8135 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7721 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.2916;  Loss pred: 0.2916; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8199 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7756 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.2806;  Loss pred: 0.2806; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8244 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7789 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2673;  Loss pred: 0.2673; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8263 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7807 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8246 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7807 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2442;  Loss pred: 0.2442; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8223 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7795 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2312;  Loss pred: 0.2312; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8191 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7773 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2239;  Loss pred: 0.2239; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8225 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7805 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2130;  Loss pred: 0.2130; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8308 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7874 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2015;  Loss pred: 0.2015; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8449 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7984 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8624 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8107 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8773 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8228 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1714;  Loss pred: 0.1714; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8867 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8296 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1608;  Loss pred: 0.1608; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8962 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8365 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1536;  Loss pred: 0.1536; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9021 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8402 score: 0.5000 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 011,   Train_Loss: 0.3947,   Val_Loss: 0.7569,   Val_Precision: 0.5116,   Val_Recall: 1.0000,   Val_accuracy: 0.6769,   Val_Score: 0.5116,   Val_Loss: 0.7569,   Test_Precision: 0.5000,   Test_Recall: 1.0000,   Test_accuracy: 0.6667,   Test_Score: 0.5000,   Test_loss: 0.7488


[0.048919940018095076, 0.04924889700487256, 0.04939216107595712, 0.049359297030605376, 0.04903856199234724, 0.048506039078347385, 0.048594781081192195, 0.04893965495284647, 0.048655848018825054, 0.04875244898721576, 0.0487780780531466, 0.04954258992802352, 0.0489908829331398, 0.049344411003403366, 0.049139524111524224, 0.04826834099367261, 0.04889612307306379, 0.04891901696100831, 0.04955583205446601, 0.05438077903818339, 0.04936534399166703, 0.04900607711169869, 0.048830753890797496, 0.04874369001481682, 0.049260037019848824, 0.04926774790510535, 0.049283761996775866, 0.049395993002690375, 0.049509478034451604, 0.04950638092122972, 0.04904776101466268, 0.04881711699999869]
[0.00111181681859307, 0.0011192931137471037, 0.0011225491153626617, 0.0011218022052410313, 0.0011145127725533464, 0.0011024099790533496, 0.0011044268427543682, 0.0011122648852919651, 0.0011058147277005694, 0.0011080102042549036, 0.0011085926830260591, 0.0011259679529096254, 0.0011134291575713592, 0.0011214638864409856, 0.001116807366171005, 0.0010970077498561957, 0.0011112755243878134, 0.001111795840022916, 0.001126268910328773, 0.00123592679632235, 0.0011219396361742508, 0.0011137744798113338, 0.0011097898611544886, 0.0011078111367003823, 0.0011195462959056551, 0.001119721543297849, 0.0011200854999267242, 0.0011226362046065994, 0.0011252154098739002, 0.0011251450209370391, 0.0011147218412423338, 0.001109479931818152]
[899.4287397679713, 893.4210241428707, 890.8296183342767, 891.4227439810918, 897.2530639635489, 907.1035449612946, 905.4470258130141, 899.0664123479039, 904.3106181804972, 902.5187639607195, 902.0445609205717, 888.124744062111, 898.1262913764772, 891.691664877006, 895.409566851729, 911.5705883856224, 899.8668449490835, 899.4457111652695, 887.8874226476576, 809.1094092106599, 891.3135499962744, 897.8478301724003, 901.0714865963213, 902.6809416075217, 893.2189795608691, 893.0791820390986, 892.7889880419127, 890.7605116391429, 888.7187210776533, 888.774319213699, 897.0847820524638, 901.323197762809]
Elapsed: 0.04922679216542747~0.0009802872840913766
Time per graph: 0.0011187907310324426~2.227925645662221e-05
Speed: 894.1481515518608~16.37735424024366
Total Time: 0.0491
best val loss: 0.7568684816360474 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8971 score: 0.5000 time: 0.04s
test Score 0.5000
Epoch Time List: [0.21191933192312717, 0.2123097098665312, 0.21243662701454014, 0.21308073692489415, 0.21194575098343194, 0.21068064984865487, 0.20962241815868765, 0.20958971604704857, 0.20989280403591692, 0.20955727703403682, 0.20998796017374843, 0.21079494804143906, 0.21250662091188133, 0.21050791698507965, 0.2103257649578154, 0.20803794299717993, 0.20946690009441227, 0.21024671103805304, 0.21327064698562026, 0.22378621192183346, 0.21331507596187294, 0.21178053901530802, 0.21309993916656822, 0.21298517694231123, 0.2115853219293058, 0.21205857198219746, 0.21198387793265283, 0.21252977894619107, 0.21255349298007786, 0.211768341017887, 0.21084096294362098, 0.21073453198187053]
Total Epoch List: [32]
Total Time List: [0.04913591698277742]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc25a590>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7687;  Loss pred: 0.7687; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.7981 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.5945 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7744;  Loss pred: 0.7744; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4499 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.2819 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7425;  Loss pred: 0.7425; Loss self: 0.0000; time: 0.13s
Val loss: 1.2021 score: 0.5227 time: 0.04s
Test loss: 1.0654 score: 0.5349 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.13s
Val loss: 1.0248 score: 0.5227 time: 0.04s
Test loss: 0.9093 score: 0.5349 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6249;  Loss pred: 0.6249; Loss self: 0.0000; time: 0.13s
Val loss: 0.8865 score: 0.5455 time: 0.04s
Test loss: 0.7988 score: 0.5349 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.13s
Val loss: 0.7873 score: 0.5455 time: 0.04s
Test loss: 0.7249 score: 0.5349 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.4929;  Loss pred: 0.4929; Loss self: 0.0000; time: 0.13s
Val loss: 0.7274 score: 0.5455 time: 0.04s
Test loss: 0.6832 score: 0.5349 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.4233;  Loss pred: 0.4233; Loss self: 0.0000; time: 0.13s
Val loss: 0.6926 score: 0.5682 time: 0.04s
Test loss: 0.6648 score: 0.5581 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3663;  Loss pred: 0.3663; Loss self: 0.0000; time: 0.13s
Val loss: 0.6691 score: 0.5909 time: 0.04s
Test loss: 0.6593 score: 0.5581 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.3211;  Loss pred: 0.3211; Loss self: 0.0000; time: 0.13s
Val loss: 0.6534 score: 0.6136 time: 0.04s
Test loss: 0.6562 score: 0.5116 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.2836;  Loss pred: 0.2836; Loss self: 0.0000; time: 0.13s
Val loss: 0.6446 score: 0.6364 time: 0.04s
Test loss: 0.6561 score: 0.5349 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.2481;  Loss pred: 0.2481; Loss self: 0.0000; time: 0.13s
Val loss: 0.6412 score: 0.6591 time: 0.04s
Test loss: 0.6591 score: 0.5116 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.2231;  Loss pred: 0.2231; Loss self: 0.0000; time: 0.13s
Val loss: 0.6391 score: 0.6818 time: 0.04s
Test loss: 0.6597 score: 0.5349 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.1998;  Loss pred: 0.1998; Loss self: 0.0000; time: 0.13s
Val loss: 0.6376 score: 0.6818 time: 0.04s
Test loss: 0.6592 score: 0.5349 time: 0.04s
Epoch 15/1000, LR 0.000270
Train loss: 0.1820;  Loss pred: 0.1820; Loss self: 0.0000; time: 0.13s
Val loss: 0.6361 score: 0.6818 time: 0.04s
Test loss: 0.6582 score: 0.5581 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.1676;  Loss pred: 0.1676; Loss self: 0.0000; time: 0.13s
Val loss: 0.6341 score: 0.6591 time: 0.04s
Test loss: 0.6581 score: 0.5581 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.1553;  Loss pred: 0.1553; Loss self: 0.0000; time: 0.13s
Val loss: 0.6322 score: 0.6364 time: 0.04s
Test loss: 0.6591 score: 0.6047 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 0.1470;  Loss pred: 0.1470; Loss self: 0.0000; time: 0.13s
Val loss: 0.6304 score: 0.6818 time: 0.04s
Test loss: 0.6605 score: 0.6512 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 0.1344;  Loss pred: 0.1344; Loss self: 0.0000; time: 0.13s
Val loss: 0.6282 score: 0.7045 time: 0.04s
Test loss: 0.6621 score: 0.6279 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.1294;  Loss pred: 0.1294; Loss self: 0.0000; time: 0.13s
Val loss: 0.6251 score: 0.7045 time: 0.04s
Test loss: 0.6646 score: 0.6279 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 0.1213;  Loss pred: 0.1213; Loss self: 0.0000; time: 0.13s
Val loss: 0.6225 score: 0.7045 time: 0.04s
Test loss: 0.6668 score: 0.6279 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 0.13s
Val loss: 0.6194 score: 0.7045 time: 0.04s
Test loss: 0.6690 score: 0.6279 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 0.1081;  Loss pred: 0.1081; Loss self: 0.0000; time: 0.13s
Val loss: 0.6165 score: 0.7045 time: 0.04s
Test loss: 0.6720 score: 0.5814 time: 0.04s
Epoch 24/1000, LR 0.000270
Train loss: 0.1019;  Loss pred: 0.1019; Loss self: 0.0000; time: 0.13s
Val loss: 0.6137 score: 0.7045 time: 0.04s
Test loss: 0.6755 score: 0.5814 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.0969;  Loss pred: 0.0969; Loss self: 0.0000; time: 0.13s
Val loss: 0.6110 score: 0.7045 time: 0.04s
Test loss: 0.6779 score: 0.5814 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.0889;  Loss pred: 0.0889; Loss self: 0.0000; time: 0.13s
Val loss: 0.6080 score: 0.7045 time: 0.04s
Test loss: 0.6806 score: 0.5814 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.13s
Val loss: 0.6052 score: 0.7045 time: 0.04s
Test loss: 0.6833 score: 0.6047 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.13s
Val loss: 0.6028 score: 0.7045 time: 0.04s
Test loss: 0.6878 score: 0.6047 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 0.0799;  Loss pred: 0.0799; Loss self: 0.0000; time: 0.13s
Val loss: 0.6012 score: 0.7273 time: 0.05s
Test loss: 0.6929 score: 0.5814 time: 0.06s
Epoch 30/1000, LR 0.000270
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 0.13s
Val loss: 0.6001 score: 0.7273 time: 0.04s
Test loss: 0.6971 score: 0.5814 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 0.0704;  Loss pred: 0.0704; Loss self: 0.0000; time: 0.13s
Val loss: 0.5997 score: 0.7273 time: 0.04s
Test loss: 0.6997 score: 0.5814 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.0665;  Loss pred: 0.0665; Loss self: 0.0000; time: 0.13s
Val loss: 0.5996 score: 0.7273 time: 0.04s
Test loss: 0.7017 score: 0.5814 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.13s
Val loss: 0.5993 score: 0.7273 time: 0.04s
Test loss: 0.7037 score: 0.5814 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.13s
Val loss: 0.5990 score: 0.7500 time: 0.04s
Test loss: 0.7064 score: 0.5814 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0551;  Loss pred: 0.0551; Loss self: 0.0000; time: 0.13s
Val loss: 0.5985 score: 0.7500 time: 0.04s
Test loss: 0.7102 score: 0.5814 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 0.13s
Val loss: 0.5979 score: 0.7273 time: 0.04s
Test loss: 0.7132 score: 0.6047 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.13s
Val loss: 0.5973 score: 0.7273 time: 0.04s
Test loss: 0.7146 score: 0.5581 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0559;  Loss pred: 0.0559; Loss self: 0.0000; time: 0.13s
Val loss: 0.5968 score: 0.7273 time: 0.05s
Test loss: 0.7150 score: 0.5581 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.13s
Val loss: 0.5954 score: 0.6818 time: 0.04s
Test loss: 0.7160 score: 0.5581 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 0.0400;  Loss pred: 0.0400; Loss self: 0.0000; time: 0.13s
Val loss: 0.5944 score: 0.7045 time: 0.04s
Test loss: 0.7185 score: 0.5581 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0422;  Loss pred: 0.0422; Loss self: 0.0000; time: 0.13s
Val loss: 0.5940 score: 0.7045 time: 0.04s
Test loss: 0.7220 score: 0.5581 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0390;  Loss pred: 0.0390; Loss self: 0.0000; time: 0.13s
Val loss: 0.5927 score: 0.7045 time: 0.04s
Test loss: 0.7260 score: 0.5581 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0351;  Loss pred: 0.0351; Loss self: 0.0000; time: 0.13s
Val loss: 0.5911 score: 0.6818 time: 0.04s
Test loss: 0.7310 score: 0.5581 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.13s
Val loss: 0.5894 score: 0.6818 time: 0.04s
Test loss: 0.7361 score: 0.5581 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 0.0334;  Loss pred: 0.0334; Loss self: 0.0000; time: 0.13s
Val loss: 0.5882 score: 0.6818 time: 0.04s
Test loss: 0.7409 score: 0.5581 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 0.0276;  Loss pred: 0.0276; Loss self: 0.0000; time: 0.13s
Val loss: 0.5858 score: 0.6818 time: 0.04s
Test loss: 0.7463 score: 0.5581 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.13s
Val loss: 0.5826 score: 0.6818 time: 0.04s
Test loss: 0.7525 score: 0.5814 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 0.0266;  Loss pred: 0.0266; Loss self: 0.0000; time: 0.13s
Val loss: 0.5796 score: 0.6818 time: 0.04s
Test loss: 0.7608 score: 0.6047 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.13s
Val loss: 0.5764 score: 0.6818 time: 0.04s
Test loss: 0.7699 score: 0.6047 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 0.0267;  Loss pred: 0.0267; Loss self: 0.0000; time: 0.13s
Val loss: 0.5743 score: 0.6591 time: 0.04s
Test loss: 0.7817 score: 0.6047 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 0.0261;  Loss pred: 0.0261; Loss self: 0.0000; time: 0.13s
Val loss: 0.5724 score: 0.6591 time: 0.04s
Test loss: 0.7911 score: 0.5814 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0216;  Loss pred: 0.0216; Loss self: 0.0000; time: 0.13s
Val loss: 0.5707 score: 0.6591 time: 0.04s
Test loss: 0.8002 score: 0.5814 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.0195;  Loss pred: 0.0195; Loss self: 0.0000; time: 0.13s
Val loss: 0.5696 score: 0.6591 time: 0.04s
Test loss: 0.8082 score: 0.5814 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.13s
Val loss: 0.5703 score: 0.6591 time: 0.04s
Test loss: 0.8167 score: 0.5814 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.13s
Val loss: 0.5712 score: 0.6591 time: 0.04s
Test loss: 0.8245 score: 0.5581 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0162;  Loss pred: 0.0162; Loss self: 0.0000; time: 0.13s
Val loss: 0.5686 score: 0.6591 time: 0.04s
Test loss: 0.8333 score: 0.5581 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.13s
Val loss: 0.5631 score: 0.6591 time: 0.04s
Test loss: 0.8429 score: 0.5581 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0144;  Loss pred: 0.0144; Loss self: 0.0000; time: 0.12s
Val loss: 0.5575 score: 0.6591 time: 0.04s
Test loss: 0.8516 score: 0.5581 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0137;  Loss pred: 0.0137; Loss self: 0.0000; time: 0.13s
Val loss: 0.5525 score: 0.6818 time: 0.04s
Test loss: 0.8590 score: 0.5581 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 0.0123;  Loss pred: 0.0123; Loss self: 0.0000; time: 0.13s
Val loss: 0.5477 score: 0.6818 time: 0.04s
Test loss: 0.8647 score: 0.5814 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 0.0121;  Loss pred: 0.0121; Loss self: 0.0000; time: 0.13s
Val loss: 0.5394 score: 0.7045 time: 0.04s
Test loss: 0.8681 score: 0.5814 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0117;  Loss pred: 0.0117; Loss self: 0.0000; time: 0.13s
Val loss: 0.5332 score: 0.7045 time: 0.04s
Test loss: 0.8738 score: 0.5814 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.12s
Val loss: 0.5290 score: 0.7045 time: 0.04s
Test loss: 0.8772 score: 0.5814 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.12s
Val loss: 0.5251 score: 0.7045 time: 0.04s
Test loss: 0.8826 score: 0.5814 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 0.0095;  Loss pred: 0.0095; Loss self: 0.0000; time: 0.12s
Val loss: 0.5216 score: 0.7045 time: 0.04s
Test loss: 0.8842 score: 0.5814 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.12s
Val loss: 0.5194 score: 0.7273 time: 0.04s
Test loss: 0.8872 score: 0.6047 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.13s
Val loss: 0.5182 score: 0.7273 time: 0.04s
Test loss: 0.8933 score: 0.6279 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 0.0080;  Loss pred: 0.0080; Loss self: 0.0000; time: 0.13s
Val loss: 0.5170 score: 0.6818 time: 0.04s
Test loss: 0.8952 score: 0.6279 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.13s
Val loss: 0.5173 score: 0.7045 time: 0.04s
Test loss: 0.8957 score: 0.6279 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0087;  Loss pred: 0.0087; Loss self: 0.0000; time: 0.13s
Val loss: 0.5193 score: 0.6818 time: 0.05s
Test loss: 0.8969 score: 0.6279 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0085;  Loss pred: 0.0085; Loss self: 0.0000; time: 0.13s
Val loss: 0.5220 score: 0.6818 time: 0.04s
Test loss: 0.8951 score: 0.6279 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.13s
Val loss: 0.5196 score: 0.7045 time: 0.04s
Test loss: 0.8846 score: 0.6279 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.13s
Val loss: 0.5069 score: 0.7045 time: 0.04s
Test loss: 0.8578 score: 0.6279 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.13s
Val loss: 0.4925 score: 0.7273 time: 0.04s
Test loss: 0.8292 score: 0.6279 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.14s
Val loss: 0.4781 score: 0.7500 time: 0.05s
Test loss: 0.7979 score: 0.6512 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.14s
Val loss: 0.4621 score: 0.7955 time: 0.05s
Test loss: 0.7671 score: 0.6744 time: 0.05s
Epoch 77/1000, LR 0.000267
Train loss: 0.0056;  Loss pred: 0.0056; Loss self: 0.0000; time: 0.14s
Val loss: 0.4451 score: 0.8409 time: 0.05s
Test loss: 0.7398 score: 0.6744 time: 0.05s
Epoch 78/1000, LR 0.000267
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.4275 score: 0.8409 time: 0.05s
Test loss: 0.7185 score: 0.6744 time: 0.05s
Epoch 79/1000, LR 0.000267
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.15s
Val loss: 0.4115 score: 0.8409 time: 0.04s
Test loss: 0.6999 score: 0.6977 time: 0.05s
Epoch 80/1000, LR 0.000267
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.13s
Val loss: 0.3962 score: 0.8409 time: 0.04s
Test loss: 0.6831 score: 0.6744 time: 0.05s
Epoch 81/1000, LR 0.000267
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.13s
Val loss: 0.3835 score: 0.8636 time: 0.05s
Test loss: 0.6679 score: 0.6744 time: 0.05s
Epoch 82/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.3734 score: 0.8636 time: 0.05s
Test loss: 0.6530 score: 0.6977 time: 0.05s
Epoch 83/1000, LR 0.000266
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.13s
Val loss: 0.3647 score: 0.8636 time: 0.05s
Test loss: 0.6367 score: 0.7209 time: 0.05s
Epoch 84/1000, LR 0.000266
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.14s
Val loss: 0.3576 score: 0.8636 time: 0.05s
Test loss: 0.6189 score: 0.7209 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.14s
Val loss: 0.3549 score: 0.8864 time: 0.05s
Test loss: 0.6097 score: 0.7442 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.14s
Val loss: 0.3504 score: 0.9091 time: 0.13s
Test loss: 0.6007 score: 0.7442 time: 0.04s
Epoch 87/1000, LR 0.000266
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.13s
Val loss: 0.3468 score: 0.9091 time: 0.04s
Test loss: 0.5911 score: 0.7442 time: 0.04s
Epoch 88/1000, LR 0.000266
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.13s
Val loss: 0.3462 score: 0.9091 time: 0.04s
Test loss: 0.5827 score: 0.7442 time: 0.04s
Epoch 89/1000, LR 0.000266
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.13s
Val loss: 0.3479 score: 0.9091 time: 0.04s
Test loss: 0.5762 score: 0.6977 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.13s
Val loss: 0.3492 score: 0.9091 time: 0.04s
Test loss: 0.5682 score: 0.7209 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.13s
Val loss: 0.3542 score: 0.9091 time: 0.04s
Test loss: 0.5725 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.13s
Val loss: 0.3605 score: 0.9091 time: 0.04s
Test loss: 0.5753 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.13s
Val loss: 0.3689 score: 0.9091 time: 0.04s
Test loss: 0.5798 score: 0.7442 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.18s
Val loss: 0.3766 score: 0.8864 time: 0.05s
Test loss: 0.5828 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.14s
Val loss: 0.3874 score: 0.8864 time: 0.05s
Test loss: 0.5902 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.14s
Val loss: 0.3954 score: 0.8864 time: 0.06s
Test loss: 0.5923 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.14s
Val loss: 0.4020 score: 0.8636 time: 0.05s
Test loss: 0.5940 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.14s
Val loss: 0.4109 score: 0.8636 time: 0.05s
Test loss: 0.6006 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.13s
Val loss: 0.4195 score: 0.8409 time: 0.05s
Test loss: 0.6073 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.13s
Val loss: 0.4283 score: 0.8409 time: 0.05s
Test loss: 0.6162 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.4357 score: 0.8409 time: 0.05s
Test loss: 0.6221 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.4417 score: 0.8409 time: 0.05s
Test loss: 0.6284 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.4466 score: 0.8409 time: 0.05s
Test loss: 0.6326 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.14s
Val loss: 0.4515 score: 0.8636 time: 0.05s
Test loss: 0.6384 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.14s
Val loss: 0.4544 score: 0.8636 time: 0.05s
Test loss: 0.6420 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.4560 score: 0.8864 time: 0.05s
Test loss: 0.6427 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4594 score: 0.8864 time: 0.04s
Test loss: 0.6460 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.13s
Val loss: 0.4647 score: 0.8864 time: 0.04s
Test loss: 0.6508 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.0032,   Val_Loss: 0.3462,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.3462,   Test_Precision: 0.8667,   Test_Recall: 0.5909,   Test_accuracy: 0.7027,   Test_Score: 0.7442,   Test_loss: 0.5827


[0.048919940018095076, 0.04924889700487256, 0.04939216107595712, 0.049359297030605376, 0.04903856199234724, 0.048506039078347385, 0.048594781081192195, 0.04893965495284647, 0.048655848018825054, 0.04875244898721576, 0.0487780780531466, 0.04954258992802352, 0.0489908829331398, 0.049344411003403366, 0.049139524111524224, 0.04826834099367261, 0.04889612307306379, 0.04891901696100831, 0.04955583205446601, 0.05438077903818339, 0.04936534399166703, 0.04900607711169869, 0.048830753890797496, 0.04874369001481682, 0.049260037019848824, 0.04926774790510535, 0.049283761996775866, 0.049395993002690375, 0.049509478034451604, 0.04950638092122972, 0.04904776101466268, 0.04881711699999869, 0.04859536106232554, 0.04840893705841154, 0.04837554495315999, 0.048454768024384975, 0.04898076492827386, 0.06046971201431006, 0.04906162898987532, 0.04876062599942088, 0.048151674098335207, 0.048762122984044254, 0.048785238061100245, 0.04843312699813396, 0.048427838017232716, 0.04841079900506884, 0.048217335948720574, 0.048064099973998964, 0.048650593031197786, 0.04836927505675703, 0.048424758948385715, 0.04840753099415451, 0.048434213967993855, 0.048561252071522176, 0.04868116194847971, 0.048304411000572145, 0.048472148017026484, 0.04856768902391195, 0.04845709295477718, 0.048788870917633176, 0.06523360800929368, 0.04887635190971196, 0.04817081394139677, 0.04889612307306379, 0.04841361800208688, 0.04850049701053649, 0.04821358295157552, 0.04843343293759972, 0.04911666794214398, 0.04891537595540285, 0.04832150007132441, 0.0482409029500559, 0.04840436391532421, 0.048238982912153006, 0.04872989805880934, 0.048496682895347476, 0.048587145982310176, 0.048557557980529964, 0.04816806793678552, 0.04861212696414441, 0.048051026999019086, 0.04849591106176376, 0.049595235963352025, 0.048411834985017776, 0.0482192198978737, 0.047833519987761974, 0.04813609202392399, 0.04857596301008016, 0.048094838042743504, 0.04826586099807173, 0.04846041195560247, 0.04796458000782877, 0.048187072039581835, 0.048143082996830344, 0.04812715493608266, 0.0481812700163573, 0.04783951898571104, 0.04822874697856605, 0.04823540791403502, 0.048580709961242974, 0.048346969997510314, 0.09843771392479539, 0.04853802500292659, 0.04800975206308067, 0.047764702001586556, 0.049081209930591285, 0.051500231958925724, 0.05181325599551201, 0.0511323920218274, 0.05096227000467479, 0.05093237094115466, 0.05086531501729041, 0.05104637995827943, 0.051447184989228845, 0.051908933906815946, 0.0521968980319798, 0.0519540460081771, 0.047853035968728364, 0.04770458093844354, 0.04741221305448562, 0.04798458202276379, 0.04877917002886534, 0.04905449494253844, 0.04852214700076729, 0.06547510495875031, 0.05205149995163083, 0.051734988926909864, 0.051080400007776916, 0.051312658935785294, 0.051662895013578236, 0.051440650946460664, 0.05161500198300928, 0.05140953394584358, 0.051015747943893075, 0.05093738797586411, 0.0513445200631395, 0.051354275085031986, 0.048799406038597226, 0.04893143696244806, 0.048975378973409534]
[0.00111181681859307, 0.0011192931137471037, 0.0011225491153626617, 0.0011218022052410313, 0.0011145127725533464, 0.0011024099790533496, 0.0011044268427543682, 0.0011122648852919651, 0.0011058147277005694, 0.0011080102042549036, 0.0011085926830260591, 0.0011259679529096254, 0.0011134291575713592, 0.0011214638864409856, 0.001116807366171005, 0.0010970077498561957, 0.0011112755243878134, 0.001111795840022916, 0.001126268910328773, 0.00123592679632235, 0.0011219396361742508, 0.0011137744798113338, 0.0011097898611544886, 0.0011078111367003823, 0.0011195462959056551, 0.001119721543297849, 0.0011200854999267242, 0.0011226362046065994, 0.0011252154098739002, 0.0011251450209370391, 0.0011147218412423338, 0.001109479931818152, 0.0011301246758680357, 0.0011257892339165475, 0.0011250126733293021, 0.0011268550703345344, 0.001139087556471485, 0.0014062723724258153, 0.001140968116043612, 0.00113396804649816, 0.0011198063743798885, 0.0011340028600940525, 0.001134540420025587, 0.001126351790654278, 0.0011262287910984352, 0.001125832535001601, 0.0011213333941562924, 0.0011177697668371851, 0.0011314091402604136, 0.0011248668617850474, 0.0011261571848461794, 0.0011257565347477794, 0.001126377069023113, 0.0011293314435237714, 0.0011321200453134816, 0.0011233583953621428, 0.0011272592562099183, 0.0011294811400909756, 0.0011269091384831904, 0.0011346249050612366, 0.0015170606513789228, 0.0011366593467374874, 0.0011202514870092273, 0.0011371191412340416, 0.0011258980930717879, 0.0011279185351287557, 0.001121246115152919, 0.001126358905525575, 0.001142248091677767, 0.0011375668826837872, 0.0011237558156121956, 0.0011218814639547884, 0.001125682881751726, 0.001121836811910535, 0.0011332534432281242, 0.0011278298347755227, 0.0011299336274955855, 0.0011292455344309295, 0.0011201876264368726, 0.001130514580561498, 0.0011174657441632345, 0.001127811885157297, 0.0011533775805430703, 0.001125856627558553, 0.0011213772069272954, 0.0011124074415758598, 0.0011194440005563718, 0.0011296735583739572, 0.0011184846056451977, 0.0011224618836760867, 0.0011269863245488947, 0.0011154553490192738, 0.0011206295823158567, 0.001119606581321636, 0.001119236161304248, 0.0011204946515431932, 0.0011125469531560706, 0.0011215987669433965, 0.001121753672419419, 0.001129783952587046, 0.001124348139476984, 0.0022892491610417534, 0.0011287912791378277, 0.0011165058619321086, 0.0011108070232927105, 0.0011414234867579368, 0.0011976798129982728, 0.0012049594417560933, 0.0011891253958564512, 0.001185169069876158, 0.0011844737428175502, 0.001182914302727684, 0.001187125115308824, 0.0011964461625402058, 0.001207184509460836, 0.0012138813495809256, 0.001208233628097142, 0.0011128613015983341, 0.0011094088590335707, 0.0011026096059182703, 0.0011159205121572973, 0.0011343993029968684, 0.0011408022079660103, 0.001128422023273658, 0.001522676859505821, 0.0012104999988751357, 0.0012031392773699968, 0.001187916279250626, 0.0011933176496694254, 0.0012014626747343777, 0.0011962942080572248, 0.0012003488833257972, 0.0011955705568800832, 0.0011864127428812344, 0.0011845904180433514, 0.0011940586061195233, 0.001194285467093767, 0.0011348699078743542, 0.0011379403944755363, 0.0011389623017071984]
[899.4287397679713, 893.4210241428707, 890.8296183342767, 891.4227439810918, 897.2530639635489, 907.1035449612946, 905.4470258130141, 899.0664123479039, 904.3106181804972, 902.5187639607195, 902.0445609205717, 888.124744062111, 898.1262913764772, 891.691664877006, 895.409566851729, 911.5705883856224, 899.8668449490835, 899.4457111652695, 887.8874226476576, 809.1094092106599, 891.3135499962744, 897.8478301724003, 901.0714865963213, 902.6809416075217, 893.2189795608691, 893.0791820390986, 892.7889880419127, 890.7605116391429, 888.7187210776533, 888.774319213699, 897.0847820524638, 901.323197762809, 884.8581234914736, 888.2657338275169, 888.8788755069343, 887.4255672498556, 877.895640522725, 711.0997980249041, 876.4486806761709, 881.8590639199484, 893.0115267059153, 881.8319910736923, 881.4141676657472, 887.8220892418677, 887.9190515318636, 888.2315698920338, 891.7954331971131, 894.6386184961651, 883.853563150314, 888.9940969663766, 887.9755095081056, 888.2915347447176, 887.8021645693502, 885.479639953858, 883.2985549011298, 890.1878546762672, 887.1073752476511, 885.3622822949072, 887.382989320675, 881.3485368946922, 659.1694268064077, 879.7710614620504, 892.6567039600486, 879.4153257456952, 888.1798505153339, 886.5888527010036, 891.8648515126555, 887.816481136078, 875.4665534447698, 879.0691916423984, 889.8730365682012, 891.3597667215757, 888.3496553166509, 891.3952451755957, 882.4151437400043, 886.6585801917846, 885.0077346723685, 885.5470041809275, 892.7075932634883, 884.552943583731, 894.8820178365345, 886.6726917499448, 867.0187602650884, 888.2125623477689, 891.7605903013821, 898.9511959605186, 893.3006023552698, 885.2114777647728, 894.0668427198871, 890.8988488098844, 887.3222134263926, 896.4948716945202, 892.3555256621305, 893.1708840256666, 893.4664859601196, 892.4629837570017, 898.8384689412004, 891.5844323948572, 891.4613115044958, 885.1249813826272, 889.4042377881042, 436.82444751664195, 885.9033715815038, 895.6513656538303, 900.2463785615518, 876.0990216176192, 834.9476956588248, 829.9034518063215, 840.9542033872415, 843.761472871118, 844.2567900418494, 845.3697767404607, 842.3711933176105, 835.8086066128326, 828.3737839268906, 823.8037435415208, 827.6545005413474, 898.5845752420016, 901.380939819717, 906.9393143615729, 896.1211744972768, 881.5238138441986, 876.5761435393317, 886.193267567489, 656.7381606656492, 826.1049160919091, 831.1589678843756, 841.8101658063232, 837.9998404255743, 832.3188235715126, 835.9147718553236, 833.0911236650696, 836.4207317128678, 842.8769886367487, 844.1736356873046, 837.4798312871937, 837.3207474704093, 881.1582658606487, 878.7806504231609, 877.9921850803079]
Elapsed: 0.04979199606376434~0.004803296417962276
Time per graph: 0.0011520063364674367~0.00011290826880776315
Speed: 873.0309765019543~52.45265797246415
Total Time: 0.0495
best val loss: 0.3461838364601135 test_score: 0.7442

Testing...
Test loss: 0.6007 score: 0.7442 time: 0.04s
test Score 0.7442
Epoch Time List: [0.21191933192312717, 0.2123097098665312, 0.21243662701454014, 0.21308073692489415, 0.21194575098343194, 0.21068064984865487, 0.20962241815868765, 0.20958971604704857, 0.20989280403591692, 0.20955727703403682, 0.20998796017374843, 0.21079494804143906, 0.21250662091188133, 0.21050791698507965, 0.2103257649578154, 0.20803794299717993, 0.20946690009441227, 0.21024671103805304, 0.21327064698562026, 0.22378621192183346, 0.21331507596187294, 0.21178053901530802, 0.21309993916656822, 0.21298517694231123, 0.2115853219293058, 0.21205857198219746, 0.21198387793265283, 0.21252977894619107, 0.21255349298007786, 0.211768341017887, 0.21084096294362098, 0.21073453198187053, 0.21404095506295562, 0.21416308009065688, 0.21430163807235658, 0.21376202208921313, 0.21422891807742417, 0.2270918459398672, 0.21831702499184757, 0.2135449959896505, 0.21240545203909278, 0.21405222709290683, 0.2137814761372283, 0.21328516397625208, 0.21280704403761774, 0.21282172400970012, 0.212729346123524, 0.21337543008849025, 0.21556892292574048, 0.21282883000094444, 0.21474164293613285, 0.21464580297470093, 0.21396235609427094, 0.2151403728639707, 0.21396270010154694, 0.21457510604523122, 0.2132852990180254, 0.21414857590571046, 0.2140485019190237, 0.21487859706394374, 0.2397439880296588, 0.21530775690916926, 0.21390277903992683, 0.21483276295475662, 0.21295455598738045, 0.21378290990833193, 0.21240270184352994, 0.2139915480511263, 0.21549598698038608, 0.22437410708516836, 0.21214934100862592, 0.21219321188982576, 0.21258236293215305, 0.21204762905836105, 0.21206217200960964, 0.21246345003601164, 0.21297436417080462, 0.21243489300832152, 0.2120991259580478, 0.21259341307450086, 0.2116035498911515, 0.21259706700220704, 0.2144944240571931, 0.2141017682151869, 0.2118388480739668, 0.21113083406817168, 0.21228522690944374, 0.21169071004260331, 0.21197451499756426, 0.21104034094605595, 0.21242059697397053, 0.21131307689938694, 0.21173170197289437, 0.21184766094665974, 0.21101247507613152, 0.21085560391657054, 0.2106747489888221, 0.21093859209213406, 0.2110384259140119, 0.21300416975282133, 0.21298084105364978, 0.27171024796552956, 0.21331221086438745, 0.21232796704862267, 0.2126071669626981, 0.2159894600044936, 0.23043696593958884, 0.23340727400500327, 0.22996996203437448, 0.22669699997641146, 0.24010033707600087, 0.22545311995781958, 0.22590790584217757, 0.22678424883633852, 0.22780241398140788, 0.22960639791563153, 0.22885581001173705, 0.30790092307142913, 0.21097903396002948, 0.2102941470220685, 0.21140091412235051, 0.21381927409674972, 0.21354981313925236, 0.2147509700153023, 0.22810676111839712, 0.2769163609482348, 0.2305775108980015, 0.24343148607295007, 0.22965806198772043, 0.23019072914030403, 0.2283459680620581, 0.22799501102417707, 0.2277134859468788, 0.22649773804005235, 0.22719383204821497, 0.22872641100548208, 0.22893706196919084, 0.224469089997001, 0.21387688990216702, 0.2137692340184003]
Total Epoch List: [32, 108]
Total Time List: [0.04913591698277742, 0.049535689991898835]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc25a350>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7367;  Loss pred: 0.7367; Loss self: 0.0000; time: 0.14s
Val loss: 0.7218 score: 0.4773 time: 0.04s
Test loss: 0.7817 score: 0.5116 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7294;  Loss pred: 0.7294; Loss self: 0.0000; time: 0.13s
Val loss: 0.6998 score: 0.4773 time: 0.04s
Test loss: 0.7531 score: 0.5116 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.13s
Val loss: 0.6827 score: 0.5000 time: 0.04s
Test loss: 0.7288 score: 0.5349 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 0.13s
Val loss: 0.6737 score: 0.5000 time: 0.04s
Test loss: 0.7153 score: 0.5814 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5977;  Loss pred: 0.5977; Loss self: 0.0000; time: 0.14s
Val loss: 0.6685 score: 0.5227 time: 0.05s
Test loss: 0.7082 score: 0.5349 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5390;  Loss pred: 0.5390; Loss self: 0.0000; time: 0.14s
Val loss: 0.6656 score: 0.5227 time: 0.04s
Test loss: 0.7023 score: 0.5116 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4884;  Loss pred: 0.4884; Loss self: 0.0000; time: 0.14s
Val loss: 0.6650 score: 0.5455 time: 0.04s
Test loss: 0.6996 score: 0.5581 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.4485;  Loss pred: 0.4485; Loss self: 0.0000; time: 0.14s
Val loss: 0.6644 score: 0.5227 time: 0.04s
Test loss: 0.6983 score: 0.5116 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.4119;  Loss pred: 0.4119; Loss self: 0.0000; time: 0.14s
Val loss: 0.6632 score: 0.5227 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.4884 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.3687;  Loss pred: 0.3687; Loss self: 0.0000; time: 0.14s
Val loss: 0.6621 score: 0.5227 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.4884 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.3350;  Loss pred: 0.3350; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6616 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.4884 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.3046;  Loss pred: 0.3046; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6614 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6989 score: 0.4884 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.2731;  Loss pred: 0.2731; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6615 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.2465;  Loss pred: 0.2465; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6631 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7047 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2274;  Loss pred: 0.2274; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6675 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7106 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2036;  Loss pred: 0.2036; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6730 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7172 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1864;  Loss pred: 0.1864; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7248 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1712;  Loss pred: 0.1712; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6861 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7335 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1491;  Loss pred: 0.1491; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7430 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1375;  Loss pred: 0.1375; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7004 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7524 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1253;  Loss pred: 0.1253; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7076 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7610 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1112;  Loss pred: 0.1112; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7165 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7717 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0972;  Loss pred: 0.0972; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7259 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7822 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0885;  Loss pred: 0.0885; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7348 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7919 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0820;  Loss pred: 0.0820; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7440 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8020 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0709;  Loss pred: 0.0709; Loss self: 0.0000; time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7528 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8114 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0637;  Loss pred: 0.0637; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7606 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8187 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0555;  Loss pred: 0.0555; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7671 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8258 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7739 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8341 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0427;  Loss pred: 0.0427; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7811 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8429 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7867 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8494 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7905 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8535 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 011,   Train_Loss: 0.3046,   Val_Loss: 0.6614,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.6614,   Test_Precision: 0.4884,   Test_Recall: 1.0000,   Test_accuracy: 0.6562,   Test_Score: 0.4884,   Test_loss: 0.6989


[0.048919940018095076, 0.04924889700487256, 0.04939216107595712, 0.049359297030605376, 0.04903856199234724, 0.048506039078347385, 0.048594781081192195, 0.04893965495284647, 0.048655848018825054, 0.04875244898721576, 0.0487780780531466, 0.04954258992802352, 0.0489908829331398, 0.049344411003403366, 0.049139524111524224, 0.04826834099367261, 0.04889612307306379, 0.04891901696100831, 0.04955583205446601, 0.05438077903818339, 0.04936534399166703, 0.04900607711169869, 0.048830753890797496, 0.04874369001481682, 0.049260037019848824, 0.04926774790510535, 0.049283761996775866, 0.049395993002690375, 0.049509478034451604, 0.04950638092122972, 0.04904776101466268, 0.04881711699999869, 0.04859536106232554, 0.04840893705841154, 0.04837554495315999, 0.048454768024384975, 0.04898076492827386, 0.06046971201431006, 0.04906162898987532, 0.04876062599942088, 0.048151674098335207, 0.048762122984044254, 0.048785238061100245, 0.04843312699813396, 0.048427838017232716, 0.04841079900506884, 0.048217335948720574, 0.048064099973998964, 0.048650593031197786, 0.04836927505675703, 0.048424758948385715, 0.04840753099415451, 0.048434213967993855, 0.048561252071522176, 0.04868116194847971, 0.048304411000572145, 0.048472148017026484, 0.04856768902391195, 0.04845709295477718, 0.048788870917633176, 0.06523360800929368, 0.04887635190971196, 0.04817081394139677, 0.04889612307306379, 0.04841361800208688, 0.04850049701053649, 0.04821358295157552, 0.04843343293759972, 0.04911666794214398, 0.04891537595540285, 0.04832150007132441, 0.0482409029500559, 0.04840436391532421, 0.048238982912153006, 0.04872989805880934, 0.048496682895347476, 0.048587145982310176, 0.048557557980529964, 0.04816806793678552, 0.04861212696414441, 0.048051026999019086, 0.04849591106176376, 0.049595235963352025, 0.048411834985017776, 0.0482192198978737, 0.047833519987761974, 0.04813609202392399, 0.04857596301008016, 0.048094838042743504, 0.04826586099807173, 0.04846041195560247, 0.04796458000782877, 0.048187072039581835, 0.048143082996830344, 0.04812715493608266, 0.0481812700163573, 0.04783951898571104, 0.04822874697856605, 0.04823540791403502, 0.048580709961242974, 0.048346969997510314, 0.09843771392479539, 0.04853802500292659, 0.04800975206308067, 0.047764702001586556, 0.049081209930591285, 0.051500231958925724, 0.05181325599551201, 0.0511323920218274, 0.05096227000467479, 0.05093237094115466, 0.05086531501729041, 0.05104637995827943, 0.051447184989228845, 0.051908933906815946, 0.0521968980319798, 0.0519540460081771, 0.047853035968728364, 0.04770458093844354, 0.04741221305448562, 0.04798458202276379, 0.04877917002886534, 0.04905449494253844, 0.04852214700076729, 0.06547510495875031, 0.05205149995163083, 0.051734988926909864, 0.051080400007776916, 0.051312658935785294, 0.051662895013578236, 0.051440650946460664, 0.05161500198300928, 0.05140953394584358, 0.051015747943893075, 0.05093738797586411, 0.0513445200631395, 0.051354275085031986, 0.048799406038597226, 0.04893143696244806, 0.048975378973409534, 0.04607470496557653, 0.046253886073827744, 0.04650519497226924, 0.04700903198681772, 0.048375707934610546, 0.046874906052835286, 0.04606431198772043, 0.04659338900819421, 0.04682033997960389, 0.04667576402425766, 0.04637746699154377, 0.04636985098477453, 0.046353167039342225, 0.046131227049045265, 0.04654825699981302, 0.04644424200523645, 0.04619180504232645, 0.04669456300325692, 0.046269827987998724, 0.0461926570860669, 0.04657012096140534, 0.04628954501822591, 0.046541917021386325, 0.046465724939480424, 0.046174093964509666, 0.04618045606184751, 0.0467743300832808, 0.04679473803844303, 0.04620417603291571, 0.04663513298146427, 0.04657635802868754, 0.04658406297676265]
[0.00111181681859307, 0.0011192931137471037, 0.0011225491153626617, 0.0011218022052410313, 0.0011145127725533464, 0.0011024099790533496, 0.0011044268427543682, 0.0011122648852919651, 0.0011058147277005694, 0.0011080102042549036, 0.0011085926830260591, 0.0011259679529096254, 0.0011134291575713592, 0.0011214638864409856, 0.001116807366171005, 0.0010970077498561957, 0.0011112755243878134, 0.001111795840022916, 0.001126268910328773, 0.00123592679632235, 0.0011219396361742508, 0.0011137744798113338, 0.0011097898611544886, 0.0011078111367003823, 0.0011195462959056551, 0.001119721543297849, 0.0011200854999267242, 0.0011226362046065994, 0.0011252154098739002, 0.0011251450209370391, 0.0011147218412423338, 0.001109479931818152, 0.0011301246758680357, 0.0011257892339165475, 0.0011250126733293021, 0.0011268550703345344, 0.001139087556471485, 0.0014062723724258153, 0.001140968116043612, 0.00113396804649816, 0.0011198063743798885, 0.0011340028600940525, 0.001134540420025587, 0.001126351790654278, 0.0011262287910984352, 0.001125832535001601, 0.0011213333941562924, 0.0011177697668371851, 0.0011314091402604136, 0.0011248668617850474, 0.0011261571848461794, 0.0011257565347477794, 0.001126377069023113, 0.0011293314435237714, 0.0011321200453134816, 0.0011233583953621428, 0.0011272592562099183, 0.0011294811400909756, 0.0011269091384831904, 0.0011346249050612366, 0.0015170606513789228, 0.0011366593467374874, 0.0011202514870092273, 0.0011371191412340416, 0.0011258980930717879, 0.0011279185351287557, 0.001121246115152919, 0.001126358905525575, 0.001142248091677767, 0.0011375668826837872, 0.0011237558156121956, 0.0011218814639547884, 0.001125682881751726, 0.001121836811910535, 0.0011332534432281242, 0.0011278298347755227, 0.0011299336274955855, 0.0011292455344309295, 0.0011201876264368726, 0.001130514580561498, 0.0011174657441632345, 0.001127811885157297, 0.0011533775805430703, 0.001125856627558553, 0.0011213772069272954, 0.0011124074415758598, 0.0011194440005563718, 0.0011296735583739572, 0.0011184846056451977, 0.0011224618836760867, 0.0011269863245488947, 0.0011154553490192738, 0.0011206295823158567, 0.001119606581321636, 0.001119236161304248, 0.0011204946515431932, 0.0011125469531560706, 0.0011215987669433965, 0.001121753672419419, 0.001129783952587046, 0.001124348139476984, 0.0022892491610417534, 0.0011287912791378277, 0.0011165058619321086, 0.0011108070232927105, 0.0011414234867579368, 0.0011976798129982728, 0.0012049594417560933, 0.0011891253958564512, 0.001185169069876158, 0.0011844737428175502, 0.001182914302727684, 0.001187125115308824, 0.0011964461625402058, 0.001207184509460836, 0.0012138813495809256, 0.001208233628097142, 0.0011128613015983341, 0.0011094088590335707, 0.0011026096059182703, 0.0011159205121572973, 0.0011343993029968684, 0.0011408022079660103, 0.001128422023273658, 0.001522676859505821, 0.0012104999988751357, 0.0012031392773699968, 0.001187916279250626, 0.0011933176496694254, 0.0012014626747343777, 0.0011962942080572248, 0.0012003488833257972, 0.0011955705568800832, 0.0011864127428812344, 0.0011845904180433514, 0.0011940586061195233, 0.001194285467093767, 0.0011348699078743542, 0.0011379403944755363, 0.0011389623017071984, 0.0010715047666413147, 0.0010756717691587847, 0.0010815161621457962, 0.0010932333020190167, 0.0011250164635955942, 0.0010901140942519835, 0.0010712630694818704, 0.0010835671862370747, 0.0010888451158047416, 0.0010854828842850619, 0.00107854574398939, 0.001078368627552896, 0.0010779806288219122, 0.001072819233698727, 0.0010825176046468145, 0.0010800986512845686, 0.00107422802424015, 0.0010859200698431842, 0.0010760425113488076, 0.001074247839210858, 0.0010830260688698915, 0.0010765010469354862, 0.0010823701632880541, 0.0010805982544065216, 0.0010738161387095272, 0.00107396409446157, 0.0010877751182158325, 0.0010882497218242565, 0.0010745157216957143, 0.0010845379763131226, 0.0010831711169462218, 0.0010833503017851779]
[899.4287397679713, 893.4210241428707, 890.8296183342767, 891.4227439810918, 897.2530639635489, 907.1035449612946, 905.4470258130141, 899.0664123479039, 904.3106181804972, 902.5187639607195, 902.0445609205717, 888.124744062111, 898.1262913764772, 891.691664877006, 895.409566851729, 911.5705883856224, 899.8668449490835, 899.4457111652695, 887.8874226476576, 809.1094092106599, 891.3135499962744, 897.8478301724003, 901.0714865963213, 902.6809416075217, 893.2189795608691, 893.0791820390986, 892.7889880419127, 890.7605116391429, 888.7187210776533, 888.774319213699, 897.0847820524638, 901.323197762809, 884.8581234914736, 888.2657338275169, 888.8788755069343, 887.4255672498556, 877.895640522725, 711.0997980249041, 876.4486806761709, 881.8590639199484, 893.0115267059153, 881.8319910736923, 881.4141676657472, 887.8220892418677, 887.9190515318636, 888.2315698920338, 891.7954331971131, 894.6386184961651, 883.853563150314, 888.9940969663766, 887.9755095081056, 888.2915347447176, 887.8021645693502, 885.479639953858, 883.2985549011298, 890.1878546762672, 887.1073752476511, 885.3622822949072, 887.382989320675, 881.3485368946922, 659.1694268064077, 879.7710614620504, 892.6567039600486, 879.4153257456952, 888.1798505153339, 886.5888527010036, 891.8648515126555, 887.816481136078, 875.4665534447698, 879.0691916423984, 889.8730365682012, 891.3597667215757, 888.3496553166509, 891.3952451755957, 882.4151437400043, 886.6585801917846, 885.0077346723685, 885.5470041809275, 892.7075932634883, 884.552943583731, 894.8820178365345, 886.6726917499448, 867.0187602650884, 888.2125623477689, 891.7605903013821, 898.9511959605186, 893.3006023552698, 885.2114777647728, 894.0668427198871, 890.8988488098844, 887.3222134263926, 896.4948716945202, 892.3555256621305, 893.1708840256666, 893.4664859601196, 892.4629837570017, 898.8384689412004, 891.5844323948572, 891.4613115044958, 885.1249813826272, 889.4042377881042, 436.82444751664195, 885.9033715815038, 895.6513656538303, 900.2463785615518, 876.0990216176192, 834.9476956588248, 829.9034518063215, 840.9542033872415, 843.761472871118, 844.2567900418494, 845.3697767404607, 842.3711933176105, 835.8086066128326, 828.3737839268906, 823.8037435415208, 827.6545005413474, 898.5845752420016, 901.380939819717, 906.9393143615729, 896.1211744972768, 881.5238138441986, 876.5761435393317, 886.193267567489, 656.7381606656492, 826.1049160919091, 831.1589678843756, 841.8101658063232, 837.9998404255743, 832.3188235715126, 835.9147718553236, 833.0911236650696, 836.4207317128678, 842.8769886367487, 844.1736356873046, 837.4798312871937, 837.3207474704093, 881.1582658606487, 878.7806504231609, 877.9921850803079, 933.2669635568212, 929.6516174093117, 924.6278835222739, 914.717835756713, 888.8758808061912, 917.3351718621544, 933.4775261912672, 922.8777068016612, 918.4042665801205, 921.2489800413905, 927.1743971666326, 927.3266807374254, 927.6604544303041, 932.1234823058957, 923.7725055993552, 925.8413560749227, 930.90105399861, 920.8780901750977, 929.3313130784312, 930.8838831219799, 923.3388084956007, 928.9354644352047, 923.8983426540254, 925.4133031606764, 931.2581213407384, 931.1298256217293, 919.3076613484217, 918.9067361521384, 930.6518088184698, 922.0516218339272, 923.2151636569615, 923.0624649775509]
Elapsed: 0.04918308375703799~0.004520326050578766
Time per graph: 0.0011389520149834365~0.00010554313847945756
Speed: 882.5923434999162~51.491952138897666
Total Time: 0.0472
best val loss: 0.6613643169403076 test_score: 0.4884

Testing...
Test loss: 0.6996 score: 0.5581 time: 0.04s
test Score 0.5581
Epoch Time List: [0.21191933192312717, 0.2123097098665312, 0.21243662701454014, 0.21308073692489415, 0.21194575098343194, 0.21068064984865487, 0.20962241815868765, 0.20958971604704857, 0.20989280403591692, 0.20955727703403682, 0.20998796017374843, 0.21079494804143906, 0.21250662091188133, 0.21050791698507965, 0.2103257649578154, 0.20803794299717993, 0.20946690009441227, 0.21024671103805304, 0.21327064698562026, 0.22378621192183346, 0.21331507596187294, 0.21178053901530802, 0.21309993916656822, 0.21298517694231123, 0.2115853219293058, 0.21205857198219746, 0.21198387793265283, 0.21252977894619107, 0.21255349298007786, 0.211768341017887, 0.21084096294362098, 0.21073453198187053, 0.21404095506295562, 0.21416308009065688, 0.21430163807235658, 0.21376202208921313, 0.21422891807742417, 0.2270918459398672, 0.21831702499184757, 0.2135449959896505, 0.21240545203909278, 0.21405222709290683, 0.2137814761372283, 0.21328516397625208, 0.21280704403761774, 0.21282172400970012, 0.212729346123524, 0.21337543008849025, 0.21556892292574048, 0.21282883000094444, 0.21474164293613285, 0.21464580297470093, 0.21396235609427094, 0.2151403728639707, 0.21396270010154694, 0.21457510604523122, 0.2132852990180254, 0.21414857590571046, 0.2140485019190237, 0.21487859706394374, 0.2397439880296588, 0.21530775690916926, 0.21390277903992683, 0.21483276295475662, 0.21295455598738045, 0.21378290990833193, 0.21240270184352994, 0.2139915480511263, 0.21549598698038608, 0.22437410708516836, 0.21214934100862592, 0.21219321188982576, 0.21258236293215305, 0.21204762905836105, 0.21206217200960964, 0.21246345003601164, 0.21297436417080462, 0.21243489300832152, 0.2120991259580478, 0.21259341307450086, 0.2116035498911515, 0.21259706700220704, 0.2144944240571931, 0.2141017682151869, 0.2118388480739668, 0.21113083406817168, 0.21228522690944374, 0.21169071004260331, 0.21197451499756426, 0.21104034094605595, 0.21242059697397053, 0.21131307689938694, 0.21173170197289437, 0.21184766094665974, 0.21101247507613152, 0.21085560391657054, 0.2106747489888221, 0.21093859209213406, 0.2110384259140119, 0.21300416975282133, 0.21298084105364978, 0.27171024796552956, 0.21331221086438745, 0.21232796704862267, 0.2126071669626981, 0.2159894600044936, 0.23043696593958884, 0.23340727400500327, 0.22996996203437448, 0.22669699997641146, 0.24010033707600087, 0.22545311995781958, 0.22590790584217757, 0.22678424883633852, 0.22780241398140788, 0.22960639791563153, 0.22885581001173705, 0.30790092307142913, 0.21097903396002948, 0.2102941470220685, 0.21140091412235051, 0.21381927409674972, 0.21354981313925236, 0.2147509700153023, 0.22810676111839712, 0.2769163609482348, 0.2305775108980015, 0.24343148607295007, 0.22965806198772043, 0.23019072914030403, 0.2283459680620581, 0.22799501102417707, 0.2277134859468788, 0.22649773804005235, 0.22719383204821497, 0.22872641100548208, 0.22893706196919084, 0.224469089997001, 0.21387688990216702, 0.2137692340184003, 0.21985356893856078, 0.21839603898115456, 0.21865223394706845, 0.21916730317752808, 0.22870845708530396, 0.21973719308152795, 0.22077295591589063, 0.21971759700682014, 0.21983553899917752, 0.2215855469694361, 0.2191053939750418, 0.21805384592153132, 0.2175297400681302, 0.21790874295402318, 0.21911440510302782, 0.21935580007266253, 0.21829492505639791, 0.21830954705365002, 0.21887413901276886, 0.21917438006494194, 0.22146948997396976, 0.21938127791509032, 0.22096912993583828, 0.21927888807840645, 0.21954412397462875, 0.218329745111987, 0.22170736698899418, 0.23178253590594977, 0.21976628492120653, 0.21930267906282097, 0.22087546798866242, 0.21963698486797512]
Total Epoch List: [32, 108, 32]
Total Time List: [0.04913591698277742, 0.049535689991898835, 0.04719507496338338]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc25a380>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7122;  Loss pred: 0.7122; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9945 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9187 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7137;  Loss pred: 0.7137; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9423 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8781 score: 0.5000 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9056 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8481 score: 0.5000 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6299;  Loss pred: 0.6299; Loss self: 0.0000; time: 0.12s
Val loss: 0.8674 score: 0.5116 time: 0.05s
Test loss: 0.8150 score: 0.5455 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.12s
Val loss: 0.8267 score: 0.5116 time: 0.05s
Test loss: 0.7777 score: 0.5909 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5169;  Loss pred: 0.5169; Loss self: 0.0000; time: 0.12s
Val loss: 0.7846 score: 0.5581 time: 0.05s
Test loss: 0.7393 score: 0.5682 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4535;  Loss pred: 0.4535; Loss self: 0.0000; time: 0.12s
Val loss: 0.7499 score: 0.5814 time: 0.05s
Test loss: 0.7066 score: 0.6364 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.3959;  Loss pred: 0.3959; Loss self: 0.0000; time: 0.12s
Val loss: 0.7254 score: 0.5581 time: 0.05s
Test loss: 0.6853 score: 0.6136 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3645;  Loss pred: 0.3645; Loss self: 0.0000; time: 0.12s
Val loss: 0.7091 score: 0.5814 time: 0.05s
Test loss: 0.6686 score: 0.5909 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.3141;  Loss pred: 0.3141; Loss self: 0.0000; time: 0.12s
Val loss: 0.7010 score: 0.6279 time: 0.05s
Test loss: 0.6576 score: 0.6136 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.12s
Val loss: 0.6953 score: 0.6279 time: 0.05s
Test loss: 0.6511 score: 0.6136 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.2433;  Loss pred: 0.2433; Loss self: 0.0000; time: 0.12s
Val loss: 0.6871 score: 0.6279 time: 0.05s
Test loss: 0.6435 score: 0.6818 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.2188;  Loss pred: 0.2188; Loss self: 0.0000; time: 0.12s
Val loss: 0.6798 score: 0.6279 time: 0.05s
Test loss: 0.6383 score: 0.6818 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.1914;  Loss pred: 0.1914; Loss self: 0.0000; time: 0.12s
Val loss: 0.6763 score: 0.6279 time: 0.05s
Test loss: 0.6346 score: 0.6818 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.1699;  Loss pred: 0.1699; Loss self: 0.0000; time: 0.12s
Val loss: 0.6751 score: 0.6279 time: 0.05s
Test loss: 0.6337 score: 0.6818 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.1507;  Loss pred: 0.1507; Loss self: 0.0000; time: 0.12s
Val loss: 0.6768 score: 0.6279 time: 0.05s
Test loss: 0.6342 score: 0.6818 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1287;  Loss pred: 0.1287; Loss self: 0.0000; time: 0.12s
Val loss: 0.6797 score: 0.6047 time: 0.05s
Test loss: 0.6354 score: 0.6818 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1154;  Loss pred: 0.1154; Loss self: 0.0000; time: 0.12s
Val loss: 0.6840 score: 0.6047 time: 0.05s
Test loss: 0.6380 score: 0.6818 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1037;  Loss pred: 0.1037; Loss self: 0.0000; time: 0.12s
Val loss: 0.6889 score: 0.6047 time: 0.05s
Test loss: 0.6427 score: 0.6818 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.0975;  Loss pred: 0.0975; Loss self: 0.0000; time: 0.12s
Val loss: 0.6952 score: 0.6047 time: 0.05s
Test loss: 0.6476 score: 0.6818 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.0823;  Loss pred: 0.0823; Loss self: 0.0000; time: 0.12s
Val loss: 0.6995 score: 0.6047 time: 0.05s
Test loss: 0.6540 score: 0.6818 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 0.12s
Val loss: 0.7023 score: 0.6047 time: 0.05s
Test loss: 0.6573 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.12s
Val loss: 0.7032 score: 0.6047 time: 0.05s
Test loss: 0.6616 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.12s
Val loss: 0.7001 score: 0.6047 time: 0.05s
Test loss: 0.6653 score: 0.6591 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0528;  Loss pred: 0.0528; Loss self: 0.0000; time: 0.12s
Val loss: 0.6966 score: 0.6047 time: 0.05s
Test loss: 0.6673 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.0479;  Loss pred: 0.0479; Loss self: 0.0000; time: 0.12s
Val loss: 0.6917 score: 0.6047 time: 0.05s
Test loss: 0.6710 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.0443;  Loss pred: 0.0443; Loss self: 0.0000; time: 0.12s
Val loss: 0.6887 score: 0.6047 time: 0.05s
Test loss: 0.6749 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.0391;  Loss pred: 0.0391; Loss self: 0.0000; time: 0.12s
Val loss: 0.6871 score: 0.5814 time: 0.05s
Test loss: 0.6783 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.0342;  Loss pred: 0.0342; Loss self: 0.0000; time: 0.12s
Val loss: 0.6866 score: 0.6047 time: 0.05s
Test loss: 0.6812 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.0305;  Loss pred: 0.0305; Loss self: 0.0000; time: 0.12s
Val loss: 0.6861 score: 0.6047 time: 0.05s
Test loss: 0.6831 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.12s
Val loss: 0.6846 score: 0.6047 time: 0.05s
Test loss: 0.6843 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0254;  Loss pred: 0.0254; Loss self: 0.0000; time: 0.12s
Val loss: 0.6807 score: 0.6047 time: 0.05s
Test loss: 0.6844 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0253;  Loss pred: 0.0253; Loss self: 0.0000; time: 0.12s
Val loss: 0.6748 score: 0.6047 time: 0.05s
Test loss: 0.6828 score: 0.6364 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.0222;  Loss pred: 0.0222; Loss self: 0.0000; time: 0.12s
Val loss: 0.6689 score: 0.6047 time: 0.05s
Test loss: 0.6803 score: 0.6364 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.12s
Val loss: 0.6622 score: 0.6279 time: 0.05s
Test loss: 0.6769 score: 0.6364 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.12s
Val loss: 0.6545 score: 0.6047 time: 0.05s
Test loss: 0.6725 score: 0.6364 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.12s
Val loss: 0.6489 score: 0.6279 time: 0.05s
Test loss: 0.6690 score: 0.6364 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0157;  Loss pred: 0.0157; Loss self: 0.0000; time: 0.13s
Val loss: 0.6440 score: 0.6279 time: 0.05s
Test loss: 0.6657 score: 0.6364 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0171;  Loss pred: 0.0171; Loss self: 0.0000; time: 0.12s
Val loss: 0.6420 score: 0.6279 time: 0.05s
Test loss: 0.6634 score: 0.6364 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 0.0130;  Loss pred: 0.0130; Loss self: 0.0000; time: 0.12s
Val loss: 0.6407 score: 0.6279 time: 0.05s
Test loss: 0.6616 score: 0.6364 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.12s
Val loss: 0.6406 score: 0.6512 time: 0.05s
Test loss: 0.6611 score: 0.6364 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.12s
Val loss: 0.6415 score: 0.6279 time: 0.05s
Test loss: 0.6625 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0108;  Loss pred: 0.0108; Loss self: 0.0000; time: 0.12s
Val loss: 0.6425 score: 0.6279 time: 0.05s
Test loss: 0.6648 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0093;  Loss pred: 0.0093; Loss self: 0.0000; time: 0.12s
Val loss: 0.6433 score: 0.6047 time: 0.05s
Test loss: 0.6665 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.12s
Val loss: 0.6446 score: 0.6047 time: 0.05s
Test loss: 0.6687 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.12s
Val loss: 0.6470 score: 0.6047 time: 0.05s
Test loss: 0.6709 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.12s
Val loss: 0.6500 score: 0.6047 time: 0.05s
Test loss: 0.6724 score: 0.5909 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0075;  Loss pred: 0.0075; Loss self: 0.0000; time: 0.12s
Val loss: 0.6534 score: 0.6047 time: 0.05s
Test loss: 0.6733 score: 0.5909 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0072;  Loss pred: 0.0072; Loss self: 0.0000; time: 0.12s
Val loss: 0.6562 score: 0.6279 time: 0.05s
Test loss: 0.6745 score: 0.5909 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.12s
Val loss: 0.6573 score: 0.6279 time: 0.05s
Test loss: 0.6745 score: 0.5909 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0067;  Loss pred: 0.0067; Loss self: 0.0000; time: 0.12s
Val loss: 0.6544 score: 0.6512 time: 0.05s
Test loss: 0.6714 score: 0.6136 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0063;  Loss pred: 0.0063; Loss self: 0.0000; time: 0.14s
Val loss: 0.6474 score: 0.6512 time: 0.05s
Test loss: 0.6649 score: 0.6364 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.12s
Val loss: 0.6382 score: 0.6512 time: 0.05s
Test loss: 0.6562 score: 0.6364 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.12s
Val loss: 0.6277 score: 0.6744 time: 0.05s
Test loss: 0.6477 score: 0.6364 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.12s
Val loss: 0.6190 score: 0.6744 time: 0.05s
Test loss: 0.6420 score: 0.6364 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.12s
Val loss: 0.6125 score: 0.6977 time: 0.05s
Test loss: 0.6383 score: 0.6591 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 0.0049;  Loss pred: 0.0049; Loss self: 0.0000; time: 0.12s
Val loss: 0.6064 score: 0.6977 time: 0.05s
Test loss: 0.6356 score: 0.6591 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0041;  Loss pred: 0.0041; Loss self: 0.0000; time: 0.12s
Val loss: 0.6010 score: 0.6977 time: 0.05s
Test loss: 0.6339 score: 0.6591 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.12s
Val loss: 0.5964 score: 0.7209 time: 0.05s
Test loss: 0.6333 score: 0.6818 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.12s
Val loss: 0.5920 score: 0.7209 time: 0.05s
Test loss: 0.6333 score: 0.6818 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.12s
Val loss: 0.5866 score: 0.7209 time: 0.05s
Test loss: 0.6329 score: 0.6818 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.12s
Val loss: 0.5809 score: 0.7209 time: 0.05s
Test loss: 0.6326 score: 0.6818 time: 0.04s
Epoch 63/1000, LR 0.000268
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.12s
Val loss: 0.5750 score: 0.7442 time: 0.05s
Test loss: 0.6317 score: 0.6818 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.5688 score: 0.7442 time: 0.05s
Test loss: 0.6308 score: 0.7045 time: 0.04s
Epoch 65/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.5621 score: 0.7442 time: 0.05s
Test loss: 0.6295 score: 0.7045 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.12s
Val loss: 0.5551 score: 0.7442 time: 0.05s
Test loss: 0.6272 score: 0.7045 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.12s
Val loss: 0.5488 score: 0.7442 time: 0.05s
Test loss: 0.6257 score: 0.7045 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 0.0032;  Loss pred: 0.0032; Loss self: 0.0000; time: 0.12s
Val loss: 0.5427 score: 0.7442 time: 0.05s
Test loss: 0.6243 score: 0.7045 time: 0.04s
Epoch 69/1000, LR 0.000268
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.12s
Val loss: 0.5365 score: 0.7442 time: 0.05s
Test loss: 0.6223 score: 0.7045 time: 0.04s
Epoch 70/1000, LR 0.000268
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.12s
Val loss: 0.5277 score: 0.7442 time: 0.05s
Test loss: 0.6185 score: 0.7045 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.12s
Val loss: 0.5195 score: 0.7674 time: 0.05s
Test loss: 0.6162 score: 0.7045 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 0.0029;  Loss pred: 0.0029; Loss self: 0.0000; time: 0.12s
Val loss: 0.5113 score: 0.7674 time: 0.05s
Test loss: 0.6138 score: 0.7273 time: 0.04s
Epoch 73/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.12s
Val loss: 0.5031 score: 0.7907 time: 0.05s
Test loss: 0.6116 score: 0.7273 time: 0.04s
Epoch 74/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.12s
Val loss: 0.4937 score: 0.8140 time: 0.05s
Test loss: 0.6087 score: 0.7273 time: 0.04s
Epoch 75/1000, LR 0.000267
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.12s
Val loss: 0.4850 score: 0.8140 time: 0.05s
Test loss: 0.6060 score: 0.7273 time: 0.04s
Epoch 76/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.12s
Val loss: 0.4759 score: 0.8140 time: 0.05s
Test loss: 0.6038 score: 0.7045 time: 0.04s
Epoch 77/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.12s
Val loss: 0.4665 score: 0.8140 time: 0.05s
Test loss: 0.6020 score: 0.7045 time: 0.04s
Epoch 78/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.12s
Val loss: 0.4566 score: 0.8140 time: 0.05s
Test loss: 0.6007 score: 0.7045 time: 0.04s
Epoch 79/1000, LR 0.000267
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.12s
Val loss: 0.4475 score: 0.8140 time: 0.05s
Test loss: 0.5993 score: 0.7045 time: 0.04s
Epoch 80/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.12s
Val loss: 0.4376 score: 0.8140 time: 0.05s
Test loss: 0.5977 score: 0.7273 time: 0.04s
Epoch 81/1000, LR 0.000267
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.12s
Val loss: 0.4283 score: 0.8140 time: 0.05s
Test loss: 0.5962 score: 0.7045 time: 0.04s
Epoch 82/1000, LR 0.000267
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.12s
Val loss: 0.4203 score: 0.8140 time: 0.05s
Test loss: 0.5975 score: 0.7045 time: 0.04s
Epoch 83/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.12s
Val loss: 0.4130 score: 0.7907 time: 0.05s
Test loss: 0.5986 score: 0.7045 time: 0.04s
Epoch 84/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.12s
Val loss: 0.4083 score: 0.7907 time: 0.05s
Test loss: 0.6012 score: 0.7273 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.13s
Val loss: 0.4067 score: 0.7907 time: 0.05s
Test loss: 0.6075 score: 0.7273 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.13s
Val loss: 0.4063 score: 0.7907 time: 0.05s
Test loss: 0.6139 score: 0.7273 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4087 score: 0.7907 time: 0.05s
Test loss: 0.6209 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.13s
Val loss: 0.4137 score: 0.8140 time: 0.05s
Test loss: 0.6290 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4222 score: 0.8140 time: 0.05s
Test loss: 0.6413 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4293 score: 0.8140 time: 0.05s
Test loss: 0.6524 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.13s
Val loss: 0.4371 score: 0.8140 time: 0.05s
Test loss: 0.6662 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.4427 score: 0.8140 time: 0.05s
Test loss: 0.6788 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.13s
Val loss: 0.4432 score: 0.8140 time: 0.05s
Test loss: 0.6863 score: 0.7045 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.13s
Val loss: 0.4444 score: 0.8140 time: 0.05s
Test loss: 0.6921 score: 0.7045 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.13s
Val loss: 0.4460 score: 0.8372 time: 0.05s
Test loss: 0.6956 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.13s
Val loss: 0.4461 score: 0.8372 time: 0.05s
Test loss: 0.6972 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.13s
Val loss: 0.4479 score: 0.8372 time: 0.05s
Test loss: 0.7035 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.13s
Val loss: 0.4493 score: 0.8140 time: 0.05s
Test loss: 0.7086 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.13s
Val loss: 0.4495 score: 0.8140 time: 0.05s
Test loss: 0.7099 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.13s
Val loss: 0.4517 score: 0.8140 time: 0.05s
Test loss: 0.7137 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.13s
Val loss: 0.4557 score: 0.8140 time: 0.05s
Test loss: 0.7230 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.13s
Val loss: 0.4636 score: 0.8140 time: 0.05s
Test loss: 0.7403 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.13s
Val loss: 0.4715 score: 0.8140 time: 0.05s
Test loss: 0.7606 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.13s
Val loss: 0.4790 score: 0.8140 time: 0.05s
Test loss: 0.7802 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0014;  Loss pred: 0.0014; Loss self: 0.0000; time: 0.13s
Val loss: 0.4850 score: 0.8372 time: 0.05s
Test loss: 0.7959 score: 0.7500 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0013;  Loss pred: 0.0013; Loss self: 0.0000; time: 0.13s
Val loss: 0.4894 score: 0.8372 time: 0.05s
Test loss: 0.8077 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 085,   Train_Loss: 0.0024,   Val_Loss: 0.4063,   Val_Precision: 0.7826,   Val_Recall: 0.8182,   Val_accuracy: 0.8000,   Val_Score: 0.7907,   Val_Loss: 0.4063,   Test_Precision: 0.7778,   Test_Recall: 0.6364,   Test_accuracy: 0.7000,   Test_Score: 0.7273,   Test_loss: 0.6139


[0.048371284967288375, 0.048314770916476846, 0.04832497099414468, 0.048075893078930676, 0.04847581696230918, 0.04872640804387629, 0.04883767105638981, 0.048940648906864226, 0.04905551497358829, 0.049001332954503596, 0.04852818907238543, 0.048614457016810775, 0.049804407986812294, 0.04948871594388038, 0.048596497043035924, 0.04861798696219921, 0.048708030954003334, 0.04823659302201122, 0.04859163204673678, 0.04859647306147963, 0.048275682027451694, 0.048384136985987425, 0.048302454059012234, 0.04840387706644833, 0.04826082196086645, 0.04856935306452215, 0.048194234957918525, 0.04852310102432966, 0.04825108591467142, 0.04847721091937274, 0.04818993795197457, 0.04866418498568237, 0.048311269958503544, 0.04882302600890398, 0.04842600703705102, 0.04871758003719151, 0.0479917669435963, 0.048496788018383086, 0.04827001201920211, 0.04832533898297697, 0.04841918603051454, 0.04822260397486389, 0.04882081691175699, 0.048320069909095764, 0.048658134997822344, 0.04849197599105537, 0.04864228004589677, 0.04835628601722419, 0.04849041998386383, 0.0482396261068061, 0.061497073038481176, 0.04825057298876345, 0.048857738031074405, 0.047865530010312796, 0.04845367593225092, 0.048273756983689964, 0.048206876032054424, 0.0486928999889642, 0.04893237294163555, 0.049609470064751804, 0.04846287297550589, 0.048419211991131306, 0.04857749701477587, 0.04884292697533965, 0.04862694500479847, 0.04793623089790344, 0.04865748493466526, 0.04828831402119249, 0.0481859699357301, 0.04822733800392598, 0.04841510800179094, 0.04846596100833267, 0.048540977062657475, 0.048633021069690585, 0.048249538987874985, 0.04863227903842926, 0.048175286035984755, 0.04857876303140074, 0.04869130998849869, 0.04849462606944144, 0.04844939906615764, 0.04850833909586072, 0.048933561076410115, 0.0529460929101333, 0.052562044002115726, 0.0525511329760775, 0.05263005802407861, 0.05231981899123639, 0.05234612606000155, 0.052383208996616304, 0.05231634806841612, 0.052850884036161005, 0.05215331306681037, 0.051935587893240154, 0.05246181599795818, 0.052930900012142956, 0.052523241960443556, 0.05245814600493759, 0.052489230991341174, 0.05217524303589016, 0.05277115397620946, 0.05241584195755422, 0.052613653941079974, 0.05279064702335745, 0.05988537799566984, 0.052807055064477026]
[0.0010993473856201904, 0.0010980629753744738, 0.00109829479532147, 0.001092633933612061, 0.001101723112779754, 0.001107418364633552, 0.0011099470694634047, 0.001112287475156005, 0.001114898067581552, 0.0011136666580569, 0.0011029133880087597, 0.0011048740231093359, 0.001131918363336643, 0.0011247435441790994, 0.0011044658418871802, 0.001104954249140891, 0.0011070007035000758, 0.0010962862050457095, 0.0011043552737894722, 0.0011044652968518096, 0.001097174591532993, 0.0010996394769542596, 0.0010977830467957326, 0.001100088115146553, 0.0010968368627469647, 0.0011038489332845943, 0.0010953235217708755, 0.0011027977505529468, 0.001096615588969805, 0.0011017547936221076, 0.0010952258625448767, 0.001106004204220054, 0.0010979834081478077, 0.0011096142274750905, 0.0011005910690238868, 0.0011072177281179888, 0.001090721975990825, 0.0011021997276905247, 0.0010970457277091389, 0.001098303158704022, 0.0011004360461480576, 0.0010959682721559975, 0.0011095640207217498, 0.0010981834070249038, 0.0011058667044959623, 0.0011020903634330766, 0.001105506364679472, 0.001099006500391459, 0.0011020549996332688, 0.0010963551387910477, 0.0013976607508745722, 0.0010966039315628057, 0.0011104031370698729, 0.0010878529547798362, 0.0011012199075511571, 0.0010971308405384082, 0.001095610818910328, 0.0011066568179310045, 0.0011120993850371715, 0.0011274879560170864, 0.0011014289312614974, 0.001100436636162075, 0.001104034023063088, 0.0011100665221668103, 0.0011051578410181471, 0.001089459793134169, 0.0011058519303333014, 0.0010974616822998294, 0.0010951356803575022, 0.0010960758637255904, 0.001100343363677067, 0.0011014991138257426, 0.0011032040241513062, 0.0011052959334020588, 0.0010965804315426133, 0.0011052790690552104, 0.001094892864454199, 0.0011040627961681987, 0.0011066206815567884, 0.0011021505924873054, 0.0011011227060490373, 0.0011024622521786528, 0.0011121263881002299, 0.0012033202934121205, 0.0011945919091389937, 0.0011943439312744886, 0.001196137682365423, 0.0011890867952553724, 0.0011896846831818534, 0.001190527477195825, 0.0011890079106458209, 0.0012011564553672956, 0.0011853025697002356, 0.0011803542703009125, 0.0011923139999535952, 0.0012029750002759763, 0.0011937100445555354, 0.0011922305910213088, 0.0011929370679850267, 0.0011858009780884129, 0.0011993444085502151, 0.0011912691353989596, 0.0011957648622972722, 0.001199787432349033, 0.0013610313180834055, 0.001200160342374478]
[909.6305799971097, 910.6945798431723, 910.5023571629517, 915.2196076266559, 907.6690761954727, 903.0010987138568, 900.9438625604392, 899.0481528705013, 896.9429843655662, 897.9347570167694, 906.6895105928822, 905.0805603934832, 883.455938511522, 889.0915668512289, 905.4150541145924, 905.0148463409288, 903.3417926820058, 912.1705585616714, 905.5057043089143, 905.4155009219577, 911.4319705515429, 909.3889597068329, 910.9268019021182, 909.0180924886919, 911.7126110218046, 905.9210638763919, 912.9722681233392, 906.7845844794264, 911.8965752980324, 907.6429762673592, 913.0536761398156, 904.1556950546972, 910.7605748677965, 901.2141113903019, 908.6026846346291, 903.1647295783152, 916.823922147152, 907.2765805298582, 911.5390313658204, 910.4954238499887, 908.7306831690748, 912.4351729934591, 901.2548905015137, 910.5947090469223, 904.2681147144087, 907.3666127384881, 904.5628609202424, 909.9127253968076, 907.3957291902584, 912.1132054916999, 715.4812062757433, 911.9062691804029, 900.5738246010326, 919.2418843063066, 908.0838378809865, 911.4683163124445, 912.7328634766309, 903.6224995835573, 899.200209490787, 886.9274342695023, 907.9115062418707, 908.7301959408023, 905.769187461768, 900.846913253483, 904.8481247517834, 917.8860994247339, 904.2801957207798, 911.1935442742842, 913.1288642458936, 912.3456077218724, 908.8072260083026, 907.8536582083899, 906.4506456720904, 904.7350757204346, 911.925811582513, 904.748880167248, 913.3313700957374, 905.7455821087688, 903.6520071116013, 907.317028014498, 908.1639988953839, 907.0605347474076, 899.1783763967981, 831.0339362468592, 837.1059542172468, 837.2797598870005, 836.0241590436724, 840.9815027718279, 840.5588591133786, 839.9638136495644, 841.0372976045555, 832.5310125350948, 843.666440589006, 847.2032720693812, 838.7052404307256, 831.2724701432603, 837.7243741568237, 838.7639165871117, 838.2671867922471, 843.3118360317632, 833.7888540363602, 839.4408704839793, 836.2848178017425, 833.4809759109793, 734.7369503651046, 833.2219993384657]
Elapsed: 0.049579022793793386~0.002258639536576308
Time per graph: 0.0011267959725862133~5.1332716740370636e-05
Speed: 889.1294283586647~36.49693406807678
Total Time: 0.0533
best val loss: 0.406337171792984 test_score: 0.7273

Testing...
Test loss: 0.6956 score: 0.7273 time: 0.05s
test Score 0.7273
Epoch Time List: [0.20951852214056998, 0.20800187706481665, 0.2091198309790343, 0.20812104817014188, 0.2089419630356133, 0.21016171597875655, 0.20849381596781313, 0.2127057999605313, 0.21119072602596134, 0.21246902004349977, 0.20914729696232826, 0.20835418393835425, 0.21199621306732297, 0.21569255704525858, 0.2084684701403603, 0.207627696916461, 0.20780514006037265, 0.20914397609885782, 0.20882781012915075, 0.2086428429465741, 0.2075390461832285, 0.20813527808059007, 0.21017259603831917, 0.2085739771137014, 0.20779073296580464, 0.20784640102647245, 0.2074596369639039, 0.21002104692161083, 0.20773024205118418, 0.20821985986549407, 0.20729632303118706, 0.20817588304635137, 0.20965170417912304, 0.2086198558099568, 0.20838132197968662, 0.20931448286864907, 0.2096033999696374, 0.22443798009771854, 0.20825321204029024, 0.2083706361008808, 0.20774203294422477, 0.20848835993092507, 0.2095085319597274, 0.20892105298116803, 0.20838093291968107, 0.20836284395772964, 0.2096320919226855, 0.20902743004262447, 0.2082503760466352, 0.20823867805302143, 0.22686147107742727, 0.23099057585932314, 0.2095973389223218, 0.2075115729821846, 0.20769286691211164, 0.20805089897476137, 0.20779363997280598, 0.20771000511012971, 0.20935786503832787, 0.21370171988382936, 0.2137542839627713, 0.20867142907809466, 0.20965590712148696, 0.2112697750562802, 0.20834063191432506, 0.20753215404693037, 0.20814963313750923, 0.20861071988474578, 0.20783535484224558, 0.20880642486736178, 0.2077665909891948, 0.2083816429367289, 0.2100248959613964, 0.20875887107104063, 0.20814838516525924, 0.2089825621806085, 0.20901070605032146, 0.2096980249043554, 0.20849268499296159, 0.20919109601527452, 0.20869736489839852, 0.20905206794850528, 0.21086815907619894, 0.22176489990670234, 0.2275525570148602, 0.22826252889353782, 0.22751170094124973, 0.2277223818236962, 0.22680933203082532, 0.22716120991390198, 0.22729921899735928, 0.2274032379500568, 0.22652331599965692, 0.22609258093871176, 0.22708475193940103, 0.22736276197247207, 0.22713678795844316, 0.2271889919647947, 0.22702375403605402, 0.2272732281126082, 0.22657388192601502, 0.2278958138776943, 0.22789511096198112, 0.2278597690165043, 0.23680454096756876, 0.22966578195337206]
Total Epoch List: [106]
Total Time List: [0.05331585509702563]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc259f60>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7256;  Loss pred: 0.7256; Loss self: 0.0000; time: 0.13s
Val loss: 0.6838 score: 0.5455 time: 0.04s
Test loss: 0.6592 score: 0.6744 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7247;  Loss pred: 0.7247; Loss self: 0.0000; time: 0.13s
Val loss: 0.6759 score: 0.5682 time: 0.04s
Test loss: 0.6648 score: 0.6512 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.13s
Val loss: 0.6719 score: 0.5227 time: 0.04s
Test loss: 0.6741 score: 0.5581 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6456;  Loss pred: 0.6456; Loss self: 0.0000; time: 0.13s
Val loss: 0.6695 score: 0.5682 time: 0.04s
Test loss: 0.6811 score: 0.5349 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 0.13s
Val loss: 0.6680 score: 0.5909 time: 0.04s
Test loss: 0.6860 score: 0.5349 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.5247;  Loss pred: 0.5247; Loss self: 0.0000; time: 0.13s
Val loss: 0.6658 score: 0.5909 time: 0.04s
Test loss: 0.6867 score: 0.5349 time: 0.04s
Epoch 7/1000, LR 0.000150
Train loss: 0.4615;  Loss pred: 0.4615; Loss self: 0.0000; time: 0.13s
Val loss: 0.6633 score: 0.5909 time: 0.04s
Test loss: 0.6853 score: 0.5814 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.4102;  Loss pred: 0.4102; Loss self: 0.0000; time: 0.15s
Val loss: 0.6610 score: 0.5909 time: 0.04s
Test loss: 0.6839 score: 0.5349 time: 0.04s
Epoch 9/1000, LR 0.000210
Train loss: 0.3718;  Loss pred: 0.3718; Loss self: 0.0000; time: 0.13s
Val loss: 0.6588 score: 0.5909 time: 0.04s
Test loss: 0.6804 score: 0.5581 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.13s
Val loss: 0.6567 score: 0.6136 time: 0.04s
Test loss: 0.6762 score: 0.5349 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.2934;  Loss pred: 0.2934; Loss self: 0.0000; time: 0.13s
Val loss: 0.6557 score: 0.6136 time: 0.04s
Test loss: 0.6721 score: 0.6047 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.2578;  Loss pred: 0.2578; Loss self: 0.0000; time: 0.13s
Val loss: 0.6543 score: 0.5909 time: 0.04s
Test loss: 0.6691 score: 0.6047 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.2334;  Loss pred: 0.2334; Loss self: 0.0000; time: 0.13s
Val loss: 0.6534 score: 0.5909 time: 0.04s
Test loss: 0.6671 score: 0.6047 time: 0.04s
Epoch 14/1000, LR 0.000270
Train loss: 0.2067;  Loss pred: 0.2067; Loss self: 0.0000; time: 0.13s
Val loss: 0.6539 score: 0.5909 time: 0.04s
Test loss: 0.6659 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1824;  Loss pred: 0.1824; Loss self: 0.0000; time: 0.13s
Val loss: 0.6547 score: 0.6364 time: 0.04s
Test loss: 0.6658 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1713;  Loss pred: 0.1713; Loss self: 0.0000; time: 0.13s
Val loss: 0.6549 score: 0.6364 time: 0.04s
Test loss: 0.6664 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1557;  Loss pred: 0.1557; Loss self: 0.0000; time: 0.13s
Val loss: 0.6553 score: 0.5909 time: 0.05s
Test loss: 0.6663 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1376;  Loss pred: 0.1376; Loss self: 0.0000; time: 0.13s
Val loss: 0.6556 score: 0.5909 time: 0.04s
Test loss: 0.6663 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1237;  Loss pred: 0.1237; Loss self: 0.0000; time: 0.13s
Val loss: 0.6558 score: 0.5909 time: 0.04s
Test loss: 0.6662 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1164;  Loss pred: 0.1164; Loss self: 0.0000; time: 0.13s
Val loss: 0.6561 score: 0.5909 time: 0.04s
Test loss: 0.6650 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1054;  Loss pred: 0.1054; Loss self: 0.0000; time: 0.13s
Val loss: 0.6560 score: 0.5909 time: 0.04s
Test loss: 0.6641 score: 0.6047 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.0982;  Loss pred: 0.0982; Loss self: 0.0000; time: 0.13s
Val loss: 0.6555 score: 0.5909 time: 0.04s
Test loss: 0.6643 score: 0.5814 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.0933;  Loss pred: 0.0933; Loss self: 0.0000; time: 0.13s
Val loss: 0.6551 score: 0.5909 time: 0.04s
Test loss: 0.6639 score: 0.5814 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.0855;  Loss pred: 0.0855; Loss self: 0.0000; time: 0.13s
Val loss: 0.6541 score: 0.5909 time: 0.04s
Test loss: 0.6643 score: 0.5814 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.0831;  Loss pred: 0.0831; Loss self: 0.0000; time: 0.13s
Val loss: 0.6529 score: 0.5909 time: 0.04s
Test loss: 0.6651 score: 0.5814 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.0769;  Loss pred: 0.0769; Loss self: 0.0000; time: 0.13s
Val loss: 0.6514 score: 0.5909 time: 0.04s
Test loss: 0.6652 score: 0.5814 time: 0.04s
Epoch 27/1000, LR 0.000270
Train loss: 0.0714;  Loss pred: 0.0714; Loss self: 0.0000; time: 0.13s
Val loss: 0.6498 score: 0.5909 time: 0.04s
Test loss: 0.6657 score: 0.5814 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.0681;  Loss pred: 0.0681; Loss self: 0.0000; time: 0.13s
Val loss: 0.6482 score: 0.6136 time: 0.04s
Test loss: 0.6670 score: 0.5814 time: 0.04s
Epoch 29/1000, LR 0.000270
Train loss: 0.0649;  Loss pred: 0.0649; Loss self: 0.0000; time: 0.13s
Val loss: 0.6457 score: 0.6136 time: 0.04s
Test loss: 0.6687 score: 0.5814 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 0.0604;  Loss pred: 0.0604; Loss self: 0.0000; time: 0.13s
Val loss: 0.6424 score: 0.6136 time: 0.04s
Test loss: 0.6704 score: 0.5814 time: 0.04s
Epoch 31/1000, LR 0.000270
Train loss: 0.0574;  Loss pred: 0.0574; Loss self: 0.0000; time: 0.13s
Val loss: 0.6394 score: 0.6136 time: 0.04s
Test loss: 0.6720 score: 0.5814 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.0520;  Loss pred: 0.0520; Loss self: 0.0000; time: 0.13s
Val loss: 0.6361 score: 0.6364 time: 0.04s
Test loss: 0.6739 score: 0.5814 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.13s
Val loss: 0.6328 score: 0.6364 time: 0.04s
Test loss: 0.6768 score: 0.6047 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.13s
Val loss: 0.6296 score: 0.6364 time: 0.04s
Test loss: 0.6804 score: 0.6047 time: 0.04s
Epoch 35/1000, LR 0.000270
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.13s
Val loss: 0.6257 score: 0.6591 time: 0.04s
Test loss: 0.6848 score: 0.6047 time: 0.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.0437;  Loss pred: 0.0437; Loss self: 0.0000; time: 0.13s
Val loss: 0.6217 score: 0.6818 time: 0.04s
Test loss: 0.6888 score: 0.6047 time: 0.04s
Epoch 37/1000, LR 0.000270
Train loss: 0.0405;  Loss pred: 0.0405; Loss self: 0.0000; time: 0.13s
Val loss: 0.6181 score: 0.6818 time: 0.04s
Test loss: 0.6928 score: 0.6047 time: 0.04s
Epoch 38/1000, LR 0.000270
Train loss: 0.0381;  Loss pred: 0.0381; Loss self: 0.0000; time: 0.13s
Val loss: 0.6149 score: 0.6818 time: 0.04s
Test loss: 0.6970 score: 0.6047 time: 0.04s
Epoch 39/1000, LR 0.000269
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.13s
Val loss: 0.6112 score: 0.6818 time: 0.04s
Test loss: 0.7010 score: 0.6047 time: 0.04s
Epoch 40/1000, LR 0.000269
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.13s
Val loss: 0.6082 score: 0.6818 time: 0.06s
Test loss: 0.7054 score: 0.6047 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.0318;  Loss pred: 0.0318; Loss self: 0.0000; time: 0.13s
Val loss: 0.6052 score: 0.6818 time: 0.04s
Test loss: 0.7096 score: 0.6047 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.13s
Val loss: 0.6024 score: 0.6818 time: 0.04s
Test loss: 0.7124 score: 0.6047 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.13s
Val loss: 0.5995 score: 0.6818 time: 0.04s
Test loss: 0.7158 score: 0.6047 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0262;  Loss pred: 0.0262; Loss self: 0.0000; time: 0.13s
Val loss: 0.5970 score: 0.6818 time: 0.04s
Test loss: 0.7207 score: 0.6047 time: 0.04s
Epoch 45/1000, LR 0.000269
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.13s
Val loss: 0.5941 score: 0.6818 time: 0.04s
Test loss: 0.7259 score: 0.6047 time: 0.04s
Epoch 46/1000, LR 0.000269
Train loss: 0.0256;  Loss pred: 0.0256; Loss self: 0.0000; time: 0.13s
Val loss: 0.5911 score: 0.6818 time: 0.04s
Test loss: 0.7307 score: 0.6279 time: 0.04s
Epoch 47/1000, LR 0.000269
Train loss: 0.0208;  Loss pred: 0.0208; Loss self: 0.0000; time: 0.13s
Val loss: 0.5880 score: 0.6818 time: 0.04s
Test loss: 0.7347 score: 0.6279 time: 0.04s
Epoch 48/1000, LR 0.000269
Train loss: 0.0221;  Loss pred: 0.0221; Loss self: 0.0000; time: 0.13s
Val loss: 0.5848 score: 0.6818 time: 0.05s
Test loss: 0.7380 score: 0.6279 time: 0.05s
Epoch 49/1000, LR 0.000269
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.13s
Val loss: 0.5812 score: 0.6818 time: 0.04s
Test loss: 0.7429 score: 0.6279 time: 0.04s
Epoch 50/1000, LR 0.000269
Train loss: 0.0183;  Loss pred: 0.0183; Loss self: 0.0000; time: 0.13s
Val loss: 0.5778 score: 0.7045 time: 0.04s
Test loss: 0.7478 score: 0.6279 time: 0.04s
Epoch 51/1000, LR 0.000269
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.13s
Val loss: 0.5752 score: 0.6818 time: 0.04s
Test loss: 0.7510 score: 0.6279 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0164;  Loss pred: 0.0164; Loss self: 0.0000; time: 0.13s
Val loss: 0.5719 score: 0.6818 time: 0.04s
Test loss: 0.7515 score: 0.6279 time: 0.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.0149;  Loss pred: 0.0149; Loss self: 0.0000; time: 0.13s
Val loss: 0.5685 score: 0.6818 time: 0.04s
Test loss: 0.7500 score: 0.6279 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.13s
Val loss: 0.5657 score: 0.7273 time: 0.04s
Test loss: 0.7496 score: 0.6279 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0138;  Loss pred: 0.0138; Loss self: 0.0000; time: 0.13s
Val loss: 0.5630 score: 0.7273 time: 0.04s
Test loss: 0.7515 score: 0.6279 time: 0.04s
Epoch 56/1000, LR 0.000269
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.13s
Val loss: 0.5600 score: 0.7273 time: 0.04s
Test loss: 0.7542 score: 0.6279 time: 0.04s
Epoch 57/1000, LR 0.000269
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.13s
Val loss: 0.5566 score: 0.7273 time: 0.04s
Test loss: 0.7560 score: 0.6279 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0104;  Loss pred: 0.0104; Loss self: 0.0000; time: 0.13s
Val loss: 0.5524 score: 0.7273 time: 0.04s
Test loss: 0.7596 score: 0.6279 time: 0.04s
Epoch 59/1000, LR 0.000268
Train loss: 0.0111;  Loss pred: 0.0111; Loss self: 0.0000; time: 0.13s
Val loss: 0.5463 score: 0.7273 time: 0.04s
Test loss: 0.7636 score: 0.6279 time: 0.04s
Epoch 60/1000, LR 0.000268
Train loss: 0.0098;  Loss pred: 0.0098; Loss self: 0.0000; time: 0.13s
Val loss: 0.5396 score: 0.7273 time: 0.04s
Test loss: 0.7664 score: 0.6279 time: 0.04s
Epoch 61/1000, LR 0.000268
Train loss: 0.0090;  Loss pred: 0.0090; Loss self: 0.0000; time: 0.13s
Val loss: 0.5336 score: 0.7273 time: 0.04s
Test loss: 0.7672 score: 0.6279 time: 0.04s
Epoch 62/1000, LR 0.000268
Train loss: 0.0101;  Loss pred: 0.0101; Loss self: 0.0000; time: 0.13s
Val loss: 0.5288 score: 0.7273 time: 0.04s
Test loss: 0.7687 score: 0.6279 time: 0.05s
Epoch 63/1000, LR 0.000268
Train loss: 0.0070;  Loss pred: 0.0070; Loss self: 0.0000; time: 0.13s
Val loss: 0.5237 score: 0.7500 time: 0.04s
Test loss: 0.7681 score: 0.6512 time: 0.04s
Epoch 64/1000, LR 0.000268
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.13s
Val loss: 0.5159 score: 0.7500 time: 0.04s
Test loss: 0.7657 score: 0.6512 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.13s
Val loss: 0.5052 score: 0.7500 time: 0.04s
Test loss: 0.7654 score: 0.6744 time: 0.04s
Epoch 66/1000, LR 0.000268
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.13s
Val loss: 0.4954 score: 0.7273 time: 0.04s
Test loss: 0.7741 score: 0.6512 time: 0.04s
Epoch 67/1000, LR 0.000268
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.13s
Val loss: 0.4856 score: 0.7273 time: 0.04s
Test loss: 0.7827 score: 0.6744 time: 0.04s
Epoch 68/1000, LR 0.000268
Train loss: 0.0058;  Loss pred: 0.0058; Loss self: 0.0000; time: 0.13s
Val loss: 0.4814 score: 0.7273 time: 0.04s
Test loss: 0.7904 score: 0.6744 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.13s
Val loss: 0.4770 score: 0.7273 time: 0.04s
Test loss: 0.7958 score: 0.6744 time: 0.05s
Epoch 70/1000, LR 0.000268
Train loss: 0.0054;  Loss pred: 0.0054; Loss self: 0.0000; time: 0.13s
Val loss: 0.4730 score: 0.7500 time: 0.04s
Test loss: 0.8018 score: 0.6512 time: 0.04s
Epoch 71/1000, LR 0.000268
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.13s
Val loss: 0.4673 score: 0.7500 time: 0.04s
Test loss: 0.8047 score: 0.6512 time: 0.04s
Epoch 72/1000, LR 0.000267
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.13s
Val loss: 0.4624 score: 0.7727 time: 0.04s
Test loss: 0.8074 score: 0.6512 time: 0.05s
Epoch 73/1000, LR 0.000267
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.13s
Val loss: 0.4586 score: 0.7727 time: 0.04s
Test loss: 0.8107 score: 0.6512 time: 0.05s
Epoch 74/1000, LR 0.000267
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.13s
Val loss: 0.4555 score: 0.7727 time: 0.04s
Test loss: 0.8134 score: 0.6512 time: 0.05s
Epoch 75/1000, LR 0.000267
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.14s
Val loss: 0.4563 score: 0.7727 time: 0.04s
Test loss: 0.8136 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0040;  Loss pred: 0.0040; Loss self: 0.0000; time: 0.13s
Val loss: 0.4578 score: 0.7727 time: 0.04s
Test loss: 0.8079 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0043;  Loss pred: 0.0043; Loss self: 0.0000; time: 0.13s
Val loss: 0.4591 score: 0.7727 time: 0.04s
Test loss: 0.8024 score: 0.6744 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0039;  Loss pred: 0.0039; Loss self: 0.0000; time: 0.13s
Val loss: 0.4595 score: 0.7727 time: 0.04s
Test loss: 0.7983 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0035;  Loss pred: 0.0035; Loss self: 0.0000; time: 0.13s
Val loss: 0.4587 score: 0.7955 time: 0.04s
Test loss: 0.7934 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.13s
Val loss: 0.4584 score: 0.7955 time: 0.04s
Test loss: 0.7918 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.13s
Val loss: 0.4572 score: 0.7955 time: 0.04s
Test loss: 0.7885 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.13s
Val loss: 0.4558 score: 0.7955 time: 0.12s
Test loss: 0.7852 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.13s
Val loss: 0.4548 score: 0.7955 time: 0.04s
Test loss: 0.7828 score: 0.6977 time: 0.05s
Epoch 84/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.13s
Val loss: 0.4524 score: 0.7955 time: 0.04s
Test loss: 0.7811 score: 0.6744 time: 0.05s
Epoch 85/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.13s
Val loss: 0.4470 score: 0.7955 time: 0.05s
Test loss: 0.7829 score: 0.7209 time: 0.05s
Epoch 86/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.13s
Val loss: 0.4403 score: 0.7955 time: 0.04s
Test loss: 0.7882 score: 0.6977 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.13s
Val loss: 0.4333 score: 0.8182 time: 0.05s
Test loss: 0.7972 score: 0.6977 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.13s
Val loss: 0.4255 score: 0.8182 time: 0.04s
Test loss: 0.8016 score: 0.6744 time: 0.05s
Epoch 89/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.13s
Val loss: 0.4192 score: 0.8182 time: 0.04s
Test loss: 0.8062 score: 0.6744 time: 0.05s
Epoch 90/1000, LR 0.000266
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.19s
Val loss: 0.4173 score: 0.8636 time: 0.04s
Test loss: 0.8143 score: 0.6744 time: 0.05s
Epoch 91/1000, LR 0.000266
Train loss: 0.0025;  Loss pred: 0.0025; Loss self: 0.0000; time: 0.13s
Val loss: 0.4176 score: 0.8636 time: 0.04s
Test loss: 0.8185 score: 0.6744 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.13s
Val loss: 0.4191 score: 0.8409 time: 0.04s
Test loss: 0.8205 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0022;  Loss pred: 0.0022; Loss self: 0.0000; time: 0.13s
Val loss: 0.4250 score: 0.8409 time: 0.04s
Test loss: 0.8311 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.13s
Val loss: 0.4338 score: 0.8409 time: 0.05s
Test loss: 0.8441 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.4420 score: 0.8409 time: 0.04s
Test loss: 0.8596 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.4481 score: 0.8409 time: 0.04s
Test loss: 0.8724 score: 0.7209 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.13s
Val loss: 0.4548 score: 0.8409 time: 0.05s
Test loss: 0.8855 score: 0.7209 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.14s
Val loss: 0.4620 score: 0.8409 time: 0.05s
Test loss: 0.8948 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.14s
Val loss: 0.4695 score: 0.8409 time: 0.05s
Test loss: 0.9033 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.14s
Val loss: 0.4743 score: 0.8636 time: 0.05s
Test loss: 0.9085 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0023;  Loss pred: 0.0023; Loss self: 0.0000; time: 0.14s
Val loss: 0.4831 score: 0.8182 time: 0.05s
Test loss: 0.9133 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.4909 score: 0.8182 time: 0.04s
Test loss: 0.9178 score: 0.7209 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0021;  Loss pred: 0.0021; Loss self: 0.0000; time: 0.13s
Val loss: 0.5004 score: 0.8182 time: 0.04s
Test loss: 0.9197 score: 0.7209 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.13s
Val loss: 0.5058 score: 0.8182 time: 0.04s
Test loss: 0.9176 score: 0.6977 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.5076 score: 0.8182 time: 0.04s
Test loss: 0.9149 score: 0.6977 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.13s
Val loss: 0.5072 score: 0.8182 time: 0.05s
Test loss: 0.9083 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4991 score: 0.8182 time: 0.04s
Test loss: 0.8907 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0017;  Loss pred: 0.0017; Loss self: 0.0000; time: 0.13s
Val loss: 0.4952 score: 0.8182 time: 0.04s
Test loss: 0.8830 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 109/1000, LR 0.000264
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.13s
Val loss: 0.4960 score: 0.8182 time: 0.04s
Test loss: 0.8787 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 110/1000, LR 0.000263
Train loss: 0.0015;  Loss pred: 0.0015; Loss self: 0.0000; time: 0.13s
Val loss: 0.4960 score: 0.8182 time: 0.04s
Test loss: 0.8640 score: 0.7442 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.0027,   Val_Loss: 0.4173,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.4173,   Test_Precision: 0.8333,   Test_Recall: 0.4545,   Test_accuracy: 0.5882,   Test_Score: 0.6744,   Test_loss: 0.8143


[0.048371284967288375, 0.048314770916476846, 0.04832497099414468, 0.048075893078930676, 0.04847581696230918, 0.04872640804387629, 0.04883767105638981, 0.048940648906864226, 0.04905551497358829, 0.049001332954503596, 0.04852818907238543, 0.048614457016810775, 0.049804407986812294, 0.04948871594388038, 0.048596497043035924, 0.04861798696219921, 0.048708030954003334, 0.04823659302201122, 0.04859163204673678, 0.04859647306147963, 0.048275682027451694, 0.048384136985987425, 0.048302454059012234, 0.04840387706644833, 0.04826082196086645, 0.04856935306452215, 0.048194234957918525, 0.04852310102432966, 0.04825108591467142, 0.04847721091937274, 0.04818993795197457, 0.04866418498568237, 0.048311269958503544, 0.04882302600890398, 0.04842600703705102, 0.04871758003719151, 0.0479917669435963, 0.048496788018383086, 0.04827001201920211, 0.04832533898297697, 0.04841918603051454, 0.04822260397486389, 0.04882081691175699, 0.048320069909095764, 0.048658134997822344, 0.04849197599105537, 0.04864228004589677, 0.04835628601722419, 0.04849041998386383, 0.0482396261068061, 0.061497073038481176, 0.04825057298876345, 0.048857738031074405, 0.047865530010312796, 0.04845367593225092, 0.048273756983689964, 0.048206876032054424, 0.0486928999889642, 0.04893237294163555, 0.049609470064751804, 0.04846287297550589, 0.048419211991131306, 0.04857749701477587, 0.04884292697533965, 0.04862694500479847, 0.04793623089790344, 0.04865748493466526, 0.04828831402119249, 0.0481859699357301, 0.04822733800392598, 0.04841510800179094, 0.04846596100833267, 0.048540977062657475, 0.048633021069690585, 0.048249538987874985, 0.04863227903842926, 0.048175286035984755, 0.04857876303140074, 0.04869130998849869, 0.04849462606944144, 0.04844939906615764, 0.04850833909586072, 0.048933561076410115, 0.0529460929101333, 0.052562044002115726, 0.0525511329760775, 0.05263005802407861, 0.05231981899123639, 0.05234612606000155, 0.052383208996616304, 0.05231634806841612, 0.052850884036161005, 0.05215331306681037, 0.051935587893240154, 0.05246181599795818, 0.052930900012142956, 0.052523241960443556, 0.05245814600493759, 0.052489230991341174, 0.05217524303589016, 0.05277115397620946, 0.05241584195755422, 0.052613653941079974, 0.05279064702335745, 0.05988537799566984, 0.052807055064477026, 0.04853805503807962, 0.04816513403784484, 0.04827979498077184, 0.048416333040222526, 0.048828328028321266, 0.048882341012358665, 0.04864067700691521, 0.04840554599650204, 0.051448542973957956, 0.048857441986911, 0.048554053995758295, 0.04845496895723045, 0.04868172004353255, 0.048404398025013506, 0.048877564957365394, 0.049005187000148, 0.050715371035039425, 0.048332882928662, 0.04846877895761281, 0.0484792860224843, 0.04860793496482074, 0.048137137899175286, 0.04813893395476043, 0.0484212840674445, 0.04872371500823647, 0.04828474100213498, 0.04826966498512775, 0.0488154289778322, 0.048450081958435476, 0.048432365991175175, 0.048401455977000296, 0.04848657303955406, 0.04859508899971843, 0.04895231209229678, 0.048547231941483915, 0.04878699197433889, 0.04904934798832983, 0.048958884086459875, 0.049293731921352446, 0.0489381990628317, 0.04907544003799558, 0.04829305503517389, 0.048052764032036066, 0.04797064093872905, 0.0480174400145188, 0.04833667597267777, 0.04928288399241865, 0.05126218392979354, 0.048248198931105435, 0.04864665202330798, 0.048669703071936965, 0.0484270571032539, 0.04823533201124519, 0.04875035700388253, 0.048632321995683014, 0.048196811927482486, 0.04872753005474806, 0.04870261892210692, 0.048718032077886164, 0.04872348497156054, 0.048338282969780266, 0.04942844598554075, 0.048817228991538286, 0.04944138100836426, 0.048762883991003036, 0.04861138702835888, 0.04892698803450912, 0.05095382104627788, 0.04939281498081982, 0.04857705009635538, 0.048955942038446665, 0.05032618495170027, 0.04950738907791674, 0.0501067079603672, 0.04952541505917907, 0.04956464597489685, 0.04935415100771934, 0.050395107944495976, 0.04984629398677498, 0.04975226102396846, 0.04969579493626952, 0.050033173989504576, 0.04973179299850017, 0.05052708298899233, 0.05081371497362852, 0.050084372982382774, 0.05068809795193374, 0.05104300705716014, 0.05017897207289934, 0.05036156298592687, 0.04929268592968583, 0.04995064705144614, 0.05086573900189251, 0.05020407703705132, 0.049442721996456385, 0.0488786599598825, 0.10253582999575883, 0.054212923045270145, 0.053857996012084186, 0.05412796197924763, 0.053827053983695805, 0.04916738695465028, 0.04890847101341933, 0.049056124058552086, 0.04955221409909427, 0.051826154929585755, 0.04875234188511968, 0.049595647025853395, 0.04925812594592571, 0.04872199194505811]
[0.0010993473856201904, 0.0010980629753744738, 0.00109829479532147, 0.001092633933612061, 0.001101723112779754, 0.001107418364633552, 0.0011099470694634047, 0.001112287475156005, 0.001114898067581552, 0.0011136666580569, 0.0011029133880087597, 0.0011048740231093359, 0.001131918363336643, 0.0011247435441790994, 0.0011044658418871802, 0.001104954249140891, 0.0011070007035000758, 0.0010962862050457095, 0.0011043552737894722, 0.0011044652968518096, 0.001097174591532993, 0.0010996394769542596, 0.0010977830467957326, 0.001100088115146553, 0.0010968368627469647, 0.0011038489332845943, 0.0010953235217708755, 0.0011027977505529468, 0.001096615588969805, 0.0011017547936221076, 0.0010952258625448767, 0.001106004204220054, 0.0010979834081478077, 0.0011096142274750905, 0.0011005910690238868, 0.0011072177281179888, 0.001090721975990825, 0.0011021997276905247, 0.0010970457277091389, 0.001098303158704022, 0.0011004360461480576, 0.0010959682721559975, 0.0011095640207217498, 0.0010981834070249038, 0.0011058667044959623, 0.0011020903634330766, 0.001105506364679472, 0.001099006500391459, 0.0011020549996332688, 0.0010963551387910477, 0.0013976607508745722, 0.0010966039315628057, 0.0011104031370698729, 0.0010878529547798362, 0.0011012199075511571, 0.0010971308405384082, 0.001095610818910328, 0.0011066568179310045, 0.0011120993850371715, 0.0011274879560170864, 0.0011014289312614974, 0.001100436636162075, 0.001104034023063088, 0.0011100665221668103, 0.0011051578410181471, 0.001089459793134169, 0.0011058519303333014, 0.0010974616822998294, 0.0010951356803575022, 0.0010960758637255904, 0.001100343363677067, 0.0011014991138257426, 0.0011032040241513062, 0.0011052959334020588, 0.0010965804315426133, 0.0011052790690552104, 0.001094892864454199, 0.0011040627961681987, 0.0011066206815567884, 0.0011021505924873054, 0.0011011227060490373, 0.0011024622521786528, 0.0011121263881002299, 0.0012033202934121205, 0.0011945919091389937, 0.0011943439312744886, 0.001196137682365423, 0.0011890867952553724, 0.0011896846831818534, 0.001190527477195825, 0.0011890079106458209, 0.0012011564553672956, 0.0011853025697002356, 0.0011803542703009125, 0.0011923139999535952, 0.0012029750002759763, 0.0011937100445555354, 0.0011922305910213088, 0.0011929370679850267, 0.0011858009780884129, 0.0011993444085502151, 0.0011912691353989596, 0.0011957648622972722, 0.001199787432349033, 0.0013610313180834055, 0.001200160342374478, 0.0011287919776297587, 0.0011201193962289498, 0.0011227859297853915, 0.0011259612334935472, 0.0011355425122865411, 0.0011367986281943876, 0.0011311785350445398, 0.0011257103720116754, 0.0011964777435804176, 0.0011362195810909535, 0.0011291640464129837, 0.0011268597431914058, 0.0011321330242681988, 0.001125683675000314, 0.0011366875571480325, 0.0011396555116313489, 0.00117942723337301, 0.0011240205332246978, 0.0011271809059909957, 0.0011274252563368442, 0.0011304170922051336, 0.0011194683232366345, 0.0011195100919711729, 0.0011260763736615001, 0.0011331096513543366, 0.0011229009535380227, 0.001122550348491343, 0.0011352425343681906, 0.0011267460920566389, 0.0011263340928180272, 0.0011256152552790767, 0.0011275947218500944, 0.001130118348830661, 0.001138425862611553, 0.001129005393987998, 0.0011345812087055556, 0.0011406825113565076, 0.0011385786996851133, 0.0011463658586361033, 0.001138097652623993, 0.0011412893032091995, 0.0011230943031435789, 0.0011175061402799084, 0.0011155963009006756, 0.001116684651500437, 0.0011241087435506457, 0.0011461135812190382, 0.00119214381232078, 0.0011220511379326846, 0.001131317488914139, 0.0011318535598124875, 0.0011262106303082302, 0.0011217519072382603, 0.0011337292326484309, 0.0011309842324577445, 0.001120856091336802, 0.001133198373366234, 0.001132619044700161, 0.0011329774901833992, 0.0011331043016641986, 0.0011241461155762852, 0.0011494987438497848, 0.001135284395152053, 0.0011497995583340526, 0.0011340205579303032, 0.001130497372752532, 0.001137836931035096, 0.0011849725824715786, 0.001148670115833019, 0.001129698839450125, 0.001138510279963876, 0.0011703763942255876, 0.001151334629718994, 0.0011652722781480744, 0.0011517538385855597, 0.0011526661854627173, 0.0011477709536678916, 0.0011719792545231622, 0.0011592161392273252, 0.0011570293261388013, 0.0011557161613085934, 0.001163562185802432, 0.0011565533255465156, 0.001175048441604473, 0.0011817143017122912, 0.0011647528600554134, 0.001178792975626366, 0.0011870466757479103, 0.0011669528389046358, 0.0011711991392076015, 0.0011463415332485078, 0.001161642954684794, 0.0011829241628347094, 0.0011675366752802632, 0.001149830744103637, 0.0011367130223228488, 0.0023845541859478796, 0.0012607656522155848, 0.0012525115351647484, 0.001258789813470875, 0.0012517919531092048, 0.0011434276035965183, 0.0011374063026376587, 0.0011408400943849323, 0.0011523770720719597, 0.0012052594169671106, 0.0011337753926772018, 0.0011533871401361254, 0.0011455378126959469, 0.0011330695801176304]
[909.6305799971097, 910.6945798431723, 910.5023571629517, 915.2196076266559, 907.6690761954727, 903.0010987138568, 900.9438625604392, 899.0481528705013, 896.9429843655662, 897.9347570167694, 906.6895105928822, 905.0805603934832, 883.455938511522, 889.0915668512289, 905.4150541145924, 905.0148463409288, 903.3417926820058, 912.1705585616714, 905.5057043089143, 905.4155009219577, 911.4319705515429, 909.3889597068329, 910.9268019021182, 909.0180924886919, 911.7126110218046, 905.9210638763919, 912.9722681233392, 906.7845844794264, 911.8965752980324, 907.6429762673592, 913.0536761398156, 904.1556950546972, 910.7605748677965, 901.2141113903019, 908.6026846346291, 903.1647295783152, 916.823922147152, 907.2765805298582, 911.5390313658204, 910.4954238499887, 908.7306831690748, 912.4351729934591, 901.2548905015137, 910.5947090469223, 904.2681147144087, 907.3666127384881, 904.5628609202424, 909.9127253968076, 907.3957291902584, 912.1132054916999, 715.4812062757433, 911.9062691804029, 900.5738246010326, 919.2418843063066, 908.0838378809865, 911.4683163124445, 912.7328634766309, 903.6224995835573, 899.200209490787, 886.9274342695023, 907.9115062418707, 908.7301959408023, 905.769187461768, 900.846913253483, 904.8481247517834, 917.8860994247339, 904.2801957207798, 911.1935442742842, 913.1288642458936, 912.3456077218724, 908.8072260083026, 907.8536582083899, 906.4506456720904, 904.7350757204346, 911.925811582513, 904.748880167248, 913.3313700957374, 905.7455821087688, 903.6520071116013, 907.317028014498, 908.1639988953839, 907.0605347474076, 899.1783763967981, 831.0339362468592, 837.1059542172468, 837.2797598870005, 836.0241590436724, 840.9815027718279, 840.5588591133786, 839.9638136495644, 841.0372976045555, 832.5310125350948, 843.666440589006, 847.2032720693812, 838.7052404307256, 831.2724701432603, 837.7243741568237, 838.7639165871117, 838.2671867922471, 843.3118360317632, 833.7888540363602, 839.4408704839793, 836.2848178017425, 833.4809759109793, 734.7369503651046, 833.2219993384657, 885.9028233880645, 892.7619710600944, 890.6417273960133, 888.1300441377327, 880.6363382964754, 879.6632712236211, 884.0337480065649, 888.3279614924153, 835.7865454375567, 880.1115705468121, 885.6109111662746, 887.4218872775388, 883.2884286247117, 888.3490293129826, 879.7492272274153, 877.4581351943445, 847.869178957424, 889.66346293613, 887.1690379822566, 886.9767591060839, 884.6292283578925, 893.2811936194632, 893.2478654473351, 888.0392337408201, 882.5271224234666, 890.5504949917552, 890.828639752288, 880.8690387526233, 887.5113985749083, 887.836039392232, 888.4030269757383, 886.8434559176154, 884.8630774243289, 878.4059048922136, 885.7353608096496, 881.3824804492406, 876.668126357783, 878.2879921050351, 872.3218617045668, 878.6592237444689, 876.2020262418065, 890.3971796499779, 894.8496692371856, 896.3816025498211, 895.5079651684534, 889.5936498468717, 872.5138733077155, 838.824972008429, 891.22497735927, 883.9251667185132, 883.5065201947755, 887.933369734143, 891.4627143019422, 882.0448226989483, 884.1856246102544, 892.1751933447035, 882.458026329, 882.9093989539357, 882.6300687034183, 882.5312890713526, 889.5640754737273, 869.9444043330584, 880.8365589012311, 869.7168065092167, 881.8182289614717, 884.566407761924, 878.8605578923291, 843.9013820169843, 870.5719651066198, 885.1916679729836, 878.3407735516707, 854.4259820462954, 868.5572154153565, 858.1685317265628, 868.2410828585344, 867.5538613103045, 871.2539699705197, 853.257424259498, 862.6518956737002, 864.2823283807038, 865.2643559710376, 859.4297857061813, 864.638039519243, 851.0287445124794, 846.2282283890538, 858.5512294448667, 848.3253808571753, 842.4268568630129, 856.932659711127, 853.8257641450882, 872.3403723898895, 860.8497094284405, 845.3627302731244, 856.5041434436768, 869.6932180044994, 879.7295186752777, 419.3656012905791, 793.1688163003705, 798.3958406167217, 794.4138006985368, 798.854791737714, 874.563458897281, 879.1932994225441, 876.5470331222325, 867.7715170104975, 829.6968983792543, 882.0089115170194, 867.0115741727254, 872.9524149417352, 882.5583331750768]
Elapsed: 0.04967986028727696~0.004036600613854174
Time per graph: 0.0011424859441168912~9.478126649774138e-05
Speed: 878.7442522638522~43.827749385305104
Total Time: 0.0495
best val loss: 0.41732385754585266 test_score: 0.6744

Testing...
Test loss: 0.8143 score: 0.6744 time: 0.04s
test Score 0.6744
Epoch Time List: [0.20951852214056998, 0.20800187706481665, 0.2091198309790343, 0.20812104817014188, 0.2089419630356133, 0.21016171597875655, 0.20849381596781313, 0.2127057999605313, 0.21119072602596134, 0.21246902004349977, 0.20914729696232826, 0.20835418393835425, 0.21199621306732297, 0.21569255704525858, 0.2084684701403603, 0.207627696916461, 0.20780514006037265, 0.20914397609885782, 0.20882781012915075, 0.2086428429465741, 0.2075390461832285, 0.20813527808059007, 0.21017259603831917, 0.2085739771137014, 0.20779073296580464, 0.20784640102647245, 0.2074596369639039, 0.21002104692161083, 0.20773024205118418, 0.20821985986549407, 0.20729632303118706, 0.20817588304635137, 0.20965170417912304, 0.2086198558099568, 0.20838132197968662, 0.20931448286864907, 0.2096033999696374, 0.22443798009771854, 0.20825321204029024, 0.2083706361008808, 0.20774203294422477, 0.20848835993092507, 0.2095085319597274, 0.20892105298116803, 0.20838093291968107, 0.20836284395772964, 0.2096320919226855, 0.20902743004262447, 0.2082503760466352, 0.20823867805302143, 0.22686147107742727, 0.23099057585932314, 0.2095973389223218, 0.2075115729821846, 0.20769286691211164, 0.20805089897476137, 0.20779363997280598, 0.20771000511012971, 0.20935786503832787, 0.21370171988382936, 0.2137542839627713, 0.20867142907809466, 0.20965590712148696, 0.2112697750562802, 0.20834063191432506, 0.20753215404693037, 0.20814963313750923, 0.20861071988474578, 0.20783535484224558, 0.20880642486736178, 0.2077665909891948, 0.2083816429367289, 0.2100248959613964, 0.20875887107104063, 0.20814838516525924, 0.2089825621806085, 0.20901070605032146, 0.2096980249043554, 0.20849268499296159, 0.20919109601527452, 0.20869736489839852, 0.20905206794850528, 0.21086815907619894, 0.22176489990670234, 0.2275525570148602, 0.22826252889353782, 0.22751170094124973, 0.2277223818236962, 0.22680933203082532, 0.22716120991390198, 0.22729921899735928, 0.2274032379500568, 0.22652331599965692, 0.22609258093871176, 0.22708475193940103, 0.22736276197247207, 0.22713678795844316, 0.2271889919647947, 0.22702375403605402, 0.2272732281126082, 0.22657388192601502, 0.2278958138776943, 0.22789511096198112, 0.2278597690165043, 0.23680454096756876, 0.22966578195337206, 0.2155544088454917, 0.21335494692903012, 0.21188790502492338, 0.21178725396748632, 0.21412081504240632, 0.2146444469690323, 0.21337286289781332, 0.23299355199560523, 0.21973446803167462, 0.2141218709293753, 0.21299239504151046, 0.21276290388777852, 0.21261098387185484, 0.21333082800265402, 0.21464773314073682, 0.21426410612184554, 0.22071037313435227, 0.21298183116596192, 0.21283788699656725, 0.21271715394686908, 0.2129784469725564, 0.2122992171207443, 0.21194444899447262, 0.2125215280102566, 0.2135801638942212, 0.21294473705347627, 0.2129173099528998, 0.2134095240617171, 0.212537306942977, 0.21338441595435143, 0.21228913008235395, 0.2126493731047958, 0.21436047891620547, 0.21534907503519207, 0.2142366130137816, 0.21346128289587796, 0.21447788598015904, 0.2144652649294585, 0.21582451288122684, 0.23335868900176138, 0.21430763707030565, 0.2126296750502661, 0.212028774083592, 0.21143536595627666, 0.21230999496765435, 0.21176106692291796, 0.2128225100459531, 0.2186967059969902, 0.2165926059242338, 0.21357358596287668, 0.2116783030796796, 0.21169664501212537, 0.2123030920047313, 0.21285612788051367, 0.2130876659648493, 0.2151906779035926, 0.21248485089745373, 0.21295239811297506, 0.21275114198215306, 0.21350842190440744, 0.21309517603367567, 0.2146742680342868, 0.21682468603830785, 0.2162772020092234, 0.21654540905728936, 0.21591379784513265, 0.21593074407428503, 0.21730640495661646, 0.21639045292977244, 0.21600866504013538, 0.21529913798440248, 0.21801341301761568, 0.2193434878718108, 0.2186125338776037, 0.2332126758992672, 0.21806461515370756, 0.21678972512017936, 0.2202394399791956, 0.22079821000806987, 0.21988807385787368, 0.21868884400464594, 0.3015850310912356, 0.21991987305227667, 0.22006016690284014, 0.2251992980018258, 0.22288379701785743, 0.2237101149512455, 0.22350324783474207, 0.2216876979218796, 0.2825280281249434, 0.21796792407985777, 0.22080714697949588, 0.22359009610954672, 0.22254218894522637, 0.21769694797694683, 0.21455064497422427, 0.2781893589999527, 0.24180067516863346, 0.2397765489295125, 0.24037607398349792, 0.24127717805095017, 0.2216096610063687, 0.21788412681780756, 0.21776505711022764, 0.2175981568871066, 0.22537709202151746, 0.21640115196350962, 0.21625002089422196, 0.21932935598306358, 0.21723312104586512]
Total Epoch List: [106, 110]
Total Time List: [0.05331585509702563, 0.04946517990902066]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSTransformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=multihead)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039906
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7912cc6179d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8142;  Loss pred: 0.8142; Loss self: 0.0000; time: 0.15s
Val loss: 0.6574 score: 0.5000 time: 0.05s
Test loss: 0.6836 score: 0.5581 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8112;  Loss pred: 0.8112; Loss self: 0.0000; time: 0.15s
Val loss: 0.6647 score: 0.5455 time: 0.05s
Test loss: 0.6887 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7919;  Loss pred: 0.7919; Loss self: 0.0000; time: 0.15s
Val loss: 0.6816 score: 0.5455 time: 0.05s
Test loss: 0.6931 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7152;  Loss pred: 0.7152; Loss self: 0.0000; time: 0.15s
Val loss: 0.6989 score: 0.5000 time: 0.04s
Test loss: 0.7008 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.14s
Val loss: 0.7203 score: 0.5000 time: 0.04s
Test loss: 0.7134 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.5646;  Loss pred: 0.5646; Loss self: 0.0000; time: 0.14s
Val loss: 0.7377 score: 0.5227 time: 0.04s
Test loss: 0.7258 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.4891;  Loss pred: 0.4891; Loss self: 0.0000; time: 0.14s
Val loss: 0.7520 score: 0.5227 time: 0.04s
Test loss: 0.7338 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.4368;  Loss pred: 0.4368; Loss self: 0.0000; time: 0.14s
Val loss: 0.7621 score: 0.5227 time: 0.04s
Test loss: 0.7392 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.3710;  Loss pred: 0.3710; Loss self: 0.0000; time: 0.14s
Val loss: 0.7672 score: 0.5227 time: 0.04s
Test loss: 0.7444 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.3301;  Loss pred: 0.3301; Loss self: 0.0000; time: 0.14s
Val loss: 0.7743 score: 0.5227 time: 0.04s
Test loss: 0.7483 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.2910;  Loss pred: 0.2910; Loss self: 0.0000; time: 0.14s
Val loss: 0.7786 score: 0.5227 time: 0.04s
Test loss: 0.7483 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.2565;  Loss pred: 0.2565; Loss self: 0.0000; time: 0.14s
Val loss: 0.7801 score: 0.5227 time: 0.04s
Test loss: 0.7443 score: 0.5116 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.2322;  Loss pred: 0.2322; Loss self: 0.0000; time: 0.14s
Val loss: 0.7809 score: 0.5227 time: 0.05s
Test loss: 0.7381 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.2085;  Loss pred: 0.2085; Loss self: 0.0000; time: 0.14s
Val loss: 0.7800 score: 0.5000 time: 0.04s
Test loss: 0.7288 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.1897;  Loss pred: 0.1897; Loss self: 0.0000; time: 0.14s
Val loss: 0.7748 score: 0.5000 time: 0.04s
Test loss: 0.7192 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.1767;  Loss pred: 0.1767; Loss self: 0.0000; time: 0.14s
Val loss: 0.7686 score: 0.5000 time: 0.04s
Test loss: 0.7087 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.1614;  Loss pred: 0.1614; Loss self: 0.0000; time: 0.14s
Val loss: 0.7617 score: 0.5227 time: 0.04s
Test loss: 0.6960 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 0.14s
Val loss: 0.7523 score: 0.5455 time: 0.04s
Test loss: 0.6818 score: 0.4884 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1332;  Loss pred: 0.1332; Loss self: 0.0000; time: 0.14s
Val loss: 0.7385 score: 0.5227 time: 0.04s
Test loss: 0.6657 score: 0.5349 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1248;  Loss pred: 0.1248; Loss self: 0.0000; time: 0.14s
Val loss: 0.7220 score: 0.5682 time: 0.04s
Test loss: 0.6499 score: 0.6279 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1182;  Loss pred: 0.1182; Loss self: 0.0000; time: 0.14s
Val loss: 0.7069 score: 0.5909 time: 0.04s
Test loss: 0.6365 score: 0.6279 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.8142,   Val_Loss: 0.6574,   Val_Precision: 0.5000,   Val_Recall: 0.8636,   Val_accuracy: 0.6333,   Val_Score: 0.5000,   Val_Loss: 0.6574,   Test_Precision: 0.5250,   Test_Recall: 1.0000,   Test_accuracy: 0.6885,   Test_Score: 0.5581,   Test_loss: 0.6836


[0.048371284967288375, 0.048314770916476846, 0.04832497099414468, 0.048075893078930676, 0.04847581696230918, 0.04872640804387629, 0.04883767105638981, 0.048940648906864226, 0.04905551497358829, 0.049001332954503596, 0.04852818907238543, 0.048614457016810775, 0.049804407986812294, 0.04948871594388038, 0.048596497043035924, 0.04861798696219921, 0.048708030954003334, 0.04823659302201122, 0.04859163204673678, 0.04859647306147963, 0.048275682027451694, 0.048384136985987425, 0.048302454059012234, 0.04840387706644833, 0.04826082196086645, 0.04856935306452215, 0.048194234957918525, 0.04852310102432966, 0.04825108591467142, 0.04847721091937274, 0.04818993795197457, 0.04866418498568237, 0.048311269958503544, 0.04882302600890398, 0.04842600703705102, 0.04871758003719151, 0.0479917669435963, 0.048496788018383086, 0.04827001201920211, 0.04832533898297697, 0.04841918603051454, 0.04822260397486389, 0.04882081691175699, 0.048320069909095764, 0.048658134997822344, 0.04849197599105537, 0.04864228004589677, 0.04835628601722419, 0.04849041998386383, 0.0482396261068061, 0.061497073038481176, 0.04825057298876345, 0.048857738031074405, 0.047865530010312796, 0.04845367593225092, 0.048273756983689964, 0.048206876032054424, 0.0486928999889642, 0.04893237294163555, 0.049609470064751804, 0.04846287297550589, 0.048419211991131306, 0.04857749701477587, 0.04884292697533965, 0.04862694500479847, 0.04793623089790344, 0.04865748493466526, 0.04828831402119249, 0.0481859699357301, 0.04822733800392598, 0.04841510800179094, 0.04846596100833267, 0.048540977062657475, 0.048633021069690585, 0.048249538987874985, 0.04863227903842926, 0.048175286035984755, 0.04857876303140074, 0.04869130998849869, 0.04849462606944144, 0.04844939906615764, 0.04850833909586072, 0.048933561076410115, 0.0529460929101333, 0.052562044002115726, 0.0525511329760775, 0.05263005802407861, 0.05231981899123639, 0.05234612606000155, 0.052383208996616304, 0.05231634806841612, 0.052850884036161005, 0.05215331306681037, 0.051935587893240154, 0.05246181599795818, 0.052930900012142956, 0.052523241960443556, 0.05245814600493759, 0.052489230991341174, 0.05217524303589016, 0.05277115397620946, 0.05241584195755422, 0.052613653941079974, 0.05279064702335745, 0.05988537799566984, 0.052807055064477026, 0.04853805503807962, 0.04816513403784484, 0.04827979498077184, 0.048416333040222526, 0.048828328028321266, 0.048882341012358665, 0.04864067700691521, 0.04840554599650204, 0.051448542973957956, 0.048857441986911, 0.048554053995758295, 0.04845496895723045, 0.04868172004353255, 0.048404398025013506, 0.048877564957365394, 0.049005187000148, 0.050715371035039425, 0.048332882928662, 0.04846877895761281, 0.0484792860224843, 0.04860793496482074, 0.048137137899175286, 0.04813893395476043, 0.0484212840674445, 0.04872371500823647, 0.04828474100213498, 0.04826966498512775, 0.0488154289778322, 0.048450081958435476, 0.048432365991175175, 0.048401455977000296, 0.04848657303955406, 0.04859508899971843, 0.04895231209229678, 0.048547231941483915, 0.04878699197433889, 0.04904934798832983, 0.048958884086459875, 0.049293731921352446, 0.0489381990628317, 0.04907544003799558, 0.04829305503517389, 0.048052764032036066, 0.04797064093872905, 0.0480174400145188, 0.04833667597267777, 0.04928288399241865, 0.05126218392979354, 0.048248198931105435, 0.04864665202330798, 0.048669703071936965, 0.0484270571032539, 0.04823533201124519, 0.04875035700388253, 0.048632321995683014, 0.048196811927482486, 0.04872753005474806, 0.04870261892210692, 0.048718032077886164, 0.04872348497156054, 0.048338282969780266, 0.04942844598554075, 0.048817228991538286, 0.04944138100836426, 0.048762883991003036, 0.04861138702835888, 0.04892698803450912, 0.05095382104627788, 0.04939281498081982, 0.04857705009635538, 0.048955942038446665, 0.05032618495170027, 0.04950738907791674, 0.0501067079603672, 0.04952541505917907, 0.04956464597489685, 0.04935415100771934, 0.050395107944495976, 0.04984629398677498, 0.04975226102396846, 0.04969579493626952, 0.050033173989504576, 0.04973179299850017, 0.05052708298899233, 0.05081371497362852, 0.050084372982382774, 0.05068809795193374, 0.05104300705716014, 0.05017897207289934, 0.05036156298592687, 0.04929268592968583, 0.04995064705144614, 0.05086573900189251, 0.05020407703705132, 0.049442721996456385, 0.0488786599598825, 0.10253582999575883, 0.054212923045270145, 0.053857996012084186, 0.05412796197924763, 0.053827053983695805, 0.04916738695465028, 0.04890847101341933, 0.049056124058552086, 0.04955221409909427, 0.051826154929585755, 0.04875234188511968, 0.049595647025853395, 0.04925812594592571, 0.04872199194505811, 0.05057231092359871, 0.05058971303515136, 0.050646253977902234, 0.045935441041365266, 0.04626853403169662, 0.04660768003668636, 0.046453048940747976, 0.0464198449626565, 0.046754118986427784, 0.046329749980941415, 0.04679575399495661, 0.047199624008499086, 0.0516432230360806, 0.046620778972283006, 0.04647813597694039, 0.046402545063756406, 0.04640300606843084, 0.04652001103386283, 0.0461546879960224, 0.046753471018746495, 0.046339637017808855]
[0.0010993473856201904, 0.0010980629753744738, 0.00109829479532147, 0.001092633933612061, 0.001101723112779754, 0.001107418364633552, 0.0011099470694634047, 0.001112287475156005, 0.001114898067581552, 0.0011136666580569, 0.0011029133880087597, 0.0011048740231093359, 0.001131918363336643, 0.0011247435441790994, 0.0011044658418871802, 0.001104954249140891, 0.0011070007035000758, 0.0010962862050457095, 0.0011043552737894722, 0.0011044652968518096, 0.001097174591532993, 0.0010996394769542596, 0.0010977830467957326, 0.001100088115146553, 0.0010968368627469647, 0.0011038489332845943, 0.0010953235217708755, 0.0011027977505529468, 0.001096615588969805, 0.0011017547936221076, 0.0010952258625448767, 0.001106004204220054, 0.0010979834081478077, 0.0011096142274750905, 0.0011005910690238868, 0.0011072177281179888, 0.001090721975990825, 0.0011021997276905247, 0.0010970457277091389, 0.001098303158704022, 0.0011004360461480576, 0.0010959682721559975, 0.0011095640207217498, 0.0010981834070249038, 0.0011058667044959623, 0.0011020903634330766, 0.001105506364679472, 0.001099006500391459, 0.0011020549996332688, 0.0010963551387910477, 0.0013976607508745722, 0.0010966039315628057, 0.0011104031370698729, 0.0010878529547798362, 0.0011012199075511571, 0.0010971308405384082, 0.001095610818910328, 0.0011066568179310045, 0.0011120993850371715, 0.0011274879560170864, 0.0011014289312614974, 0.001100436636162075, 0.001104034023063088, 0.0011100665221668103, 0.0011051578410181471, 0.001089459793134169, 0.0011058519303333014, 0.0010974616822998294, 0.0010951356803575022, 0.0010960758637255904, 0.001100343363677067, 0.0011014991138257426, 0.0011032040241513062, 0.0011052959334020588, 0.0010965804315426133, 0.0011052790690552104, 0.001094892864454199, 0.0011040627961681987, 0.0011066206815567884, 0.0011021505924873054, 0.0011011227060490373, 0.0011024622521786528, 0.0011121263881002299, 0.0012033202934121205, 0.0011945919091389937, 0.0011943439312744886, 0.001196137682365423, 0.0011890867952553724, 0.0011896846831818534, 0.001190527477195825, 0.0011890079106458209, 0.0012011564553672956, 0.0011853025697002356, 0.0011803542703009125, 0.0011923139999535952, 0.0012029750002759763, 0.0011937100445555354, 0.0011922305910213088, 0.0011929370679850267, 0.0011858009780884129, 0.0011993444085502151, 0.0011912691353989596, 0.0011957648622972722, 0.001199787432349033, 0.0013610313180834055, 0.001200160342374478, 0.0011287919776297587, 0.0011201193962289498, 0.0011227859297853915, 0.0011259612334935472, 0.0011355425122865411, 0.0011367986281943876, 0.0011311785350445398, 0.0011257103720116754, 0.0011964777435804176, 0.0011362195810909535, 0.0011291640464129837, 0.0011268597431914058, 0.0011321330242681988, 0.001125683675000314, 0.0011366875571480325, 0.0011396555116313489, 0.00117942723337301, 0.0011240205332246978, 0.0011271809059909957, 0.0011274252563368442, 0.0011304170922051336, 0.0011194683232366345, 0.0011195100919711729, 0.0011260763736615001, 0.0011331096513543366, 0.0011229009535380227, 0.001122550348491343, 0.0011352425343681906, 0.0011267460920566389, 0.0011263340928180272, 0.0011256152552790767, 0.0011275947218500944, 0.001130118348830661, 0.001138425862611553, 0.001129005393987998, 0.0011345812087055556, 0.0011406825113565076, 0.0011385786996851133, 0.0011463658586361033, 0.001138097652623993, 0.0011412893032091995, 0.0011230943031435789, 0.0011175061402799084, 0.0011155963009006756, 0.001116684651500437, 0.0011241087435506457, 0.0011461135812190382, 0.00119214381232078, 0.0011220511379326846, 0.001131317488914139, 0.0011318535598124875, 0.0011262106303082302, 0.0011217519072382603, 0.0011337292326484309, 0.0011309842324577445, 0.001120856091336802, 0.001133198373366234, 0.001132619044700161, 0.0011329774901833992, 0.0011331043016641986, 0.0011241461155762852, 0.0011494987438497848, 0.001135284395152053, 0.0011497995583340526, 0.0011340205579303032, 0.001130497372752532, 0.001137836931035096, 0.0011849725824715786, 0.001148670115833019, 0.001129698839450125, 0.001138510279963876, 0.0011703763942255876, 0.001151334629718994, 0.0011652722781480744, 0.0011517538385855597, 0.0011526661854627173, 0.0011477709536678916, 0.0011719792545231622, 0.0011592161392273252, 0.0011570293261388013, 0.0011557161613085934, 0.001163562185802432, 0.0011565533255465156, 0.001175048441604473, 0.0011817143017122912, 0.0011647528600554134, 0.001178792975626366, 0.0011870466757479103, 0.0011669528389046358, 0.0011711991392076015, 0.0011463415332485078, 0.001161642954684794, 0.0011829241628347094, 0.0011675366752802632, 0.001149830744103637, 0.0011367130223228488, 0.0023845541859478796, 0.0012607656522155848, 0.0012525115351647484, 0.001258789813470875, 0.0012517919531092048, 0.0011434276035965183, 0.0011374063026376587, 0.0011408400943849323, 0.0011523770720719597, 0.0012052594169671106, 0.0011337753926772018, 0.0011533871401361254, 0.0011455378126959469, 0.0011330695801176304, 0.0011761002540371793, 0.0011765049543058456, 0.0011778198599512147, 0.0010682660707294247, 0.0010760124193417819, 0.001083899535736892, 0.001080303463738325, 0.0010795312782013139, 0.0010873050927076228, 0.001077436046068405, 0.0010882733487199213, 0.0010976656746162579, 0.0012010051868855953, 0.0010842041621461165, 0.0010808868831846603, 0.0010791289549710792, 0.0010791396760100195, 0.0010818607217177403, 0.0010733648371168, 0.0010872900236917789, 0.0010776659771583456]
[909.6305799971097, 910.6945798431723, 910.5023571629517, 915.2196076266559, 907.6690761954727, 903.0010987138568, 900.9438625604392, 899.0481528705013, 896.9429843655662, 897.9347570167694, 906.6895105928822, 905.0805603934832, 883.455938511522, 889.0915668512289, 905.4150541145924, 905.0148463409288, 903.3417926820058, 912.1705585616714, 905.5057043089143, 905.4155009219577, 911.4319705515429, 909.3889597068329, 910.9268019021182, 909.0180924886919, 911.7126110218046, 905.9210638763919, 912.9722681233392, 906.7845844794264, 911.8965752980324, 907.6429762673592, 913.0536761398156, 904.1556950546972, 910.7605748677965, 901.2141113903019, 908.6026846346291, 903.1647295783152, 916.823922147152, 907.2765805298582, 911.5390313658204, 910.4954238499887, 908.7306831690748, 912.4351729934591, 901.2548905015137, 910.5947090469223, 904.2681147144087, 907.3666127384881, 904.5628609202424, 909.9127253968076, 907.3957291902584, 912.1132054916999, 715.4812062757433, 911.9062691804029, 900.5738246010326, 919.2418843063066, 908.0838378809865, 911.4683163124445, 912.7328634766309, 903.6224995835573, 899.200209490787, 886.9274342695023, 907.9115062418707, 908.7301959408023, 905.769187461768, 900.846913253483, 904.8481247517834, 917.8860994247339, 904.2801957207798, 911.1935442742842, 913.1288642458936, 912.3456077218724, 908.8072260083026, 907.8536582083899, 906.4506456720904, 904.7350757204346, 911.925811582513, 904.748880167248, 913.3313700957374, 905.7455821087688, 903.6520071116013, 907.317028014498, 908.1639988953839, 907.0605347474076, 899.1783763967981, 831.0339362468592, 837.1059542172468, 837.2797598870005, 836.0241590436724, 840.9815027718279, 840.5588591133786, 839.9638136495644, 841.0372976045555, 832.5310125350948, 843.666440589006, 847.2032720693812, 838.7052404307256, 831.2724701432603, 837.7243741568237, 838.7639165871117, 838.2671867922471, 843.3118360317632, 833.7888540363602, 839.4408704839793, 836.2848178017425, 833.4809759109793, 734.7369503651046, 833.2219993384657, 885.9028233880645, 892.7619710600944, 890.6417273960133, 888.1300441377327, 880.6363382964754, 879.6632712236211, 884.0337480065649, 888.3279614924153, 835.7865454375567, 880.1115705468121, 885.6109111662746, 887.4218872775388, 883.2884286247117, 888.3490293129826, 879.7492272274153, 877.4581351943445, 847.869178957424, 889.66346293613, 887.1690379822566, 886.9767591060839, 884.6292283578925, 893.2811936194632, 893.2478654473351, 888.0392337408201, 882.5271224234666, 890.5504949917552, 890.828639752288, 880.8690387526233, 887.5113985749083, 887.836039392232, 888.4030269757383, 886.8434559176154, 884.8630774243289, 878.4059048922136, 885.7353608096496, 881.3824804492406, 876.668126357783, 878.2879921050351, 872.3218617045668, 878.6592237444689, 876.2020262418065, 890.3971796499779, 894.8496692371856, 896.3816025498211, 895.5079651684534, 889.5936498468717, 872.5138733077155, 838.824972008429, 891.22497735927, 883.9251667185132, 883.5065201947755, 887.933369734143, 891.4627143019422, 882.0448226989483, 884.1856246102544, 892.1751933447035, 882.458026329, 882.9093989539357, 882.6300687034183, 882.5312890713526, 889.5640754737273, 869.9444043330584, 880.8365589012311, 869.7168065092167, 881.8182289614717, 884.566407761924, 878.8605578923291, 843.9013820169843, 870.5719651066198, 885.1916679729836, 878.3407735516707, 854.4259820462954, 868.5572154153565, 858.1685317265628, 868.2410828585344, 867.5538613103045, 871.2539699705197, 853.257424259498, 862.6518956737002, 864.2823283807038, 865.2643559710376, 859.4297857061813, 864.638039519243, 851.0287445124794, 846.2282283890538, 858.5512294448667, 848.3253808571753, 842.4268568630129, 856.932659711127, 853.8257641450882, 872.3403723898895, 860.8497094284405, 845.3627302731244, 856.5041434436768, 869.6932180044994, 879.7295186752777, 419.3656012905791, 793.1688163003705, 798.3958406167217, 794.4138006985368, 798.854791737714, 874.563458897281, 879.1932994225441, 876.5470331222325, 867.7715170104975, 829.6968983792543, 882.0089115170194, 867.0115741727254, 872.9524149417352, 882.5583331750768, 850.2676507103175, 849.9751712393034, 849.0262679400057, 936.0963784211438, 929.3573029684173, 922.5947304425656, 925.6658277661726, 926.3279538006284, 919.7050641138685, 928.1293341252399, 918.8867862805309, 911.0242062999723, 832.6358711182297, 922.3355110725278, 925.1661904284192, 926.6733094255636, 926.6641031097769, 924.3334007100613, 931.6496734569135, 919.7178105291587, 927.9313082119008]
Elapsed: 0.04947146578968939~0.003945442605250909
Time per graph: 0.0011387790225750411~9.205736560297315e-05
Speed: 881.4891238023747~43.787189372023754
Total Time: 0.0503
best val loss: 0.6574411988258362 test_score: 0.5581

Testing...
Test loss: 0.6365 score: 0.6279 time: 0.04s
test Score 0.6279
Epoch Time List: [0.20951852214056998, 0.20800187706481665, 0.2091198309790343, 0.20812104817014188, 0.2089419630356133, 0.21016171597875655, 0.20849381596781313, 0.2127057999605313, 0.21119072602596134, 0.21246902004349977, 0.20914729696232826, 0.20835418393835425, 0.21199621306732297, 0.21569255704525858, 0.2084684701403603, 0.207627696916461, 0.20780514006037265, 0.20914397609885782, 0.20882781012915075, 0.2086428429465741, 0.2075390461832285, 0.20813527808059007, 0.21017259603831917, 0.2085739771137014, 0.20779073296580464, 0.20784640102647245, 0.2074596369639039, 0.21002104692161083, 0.20773024205118418, 0.20821985986549407, 0.20729632303118706, 0.20817588304635137, 0.20965170417912304, 0.2086198558099568, 0.20838132197968662, 0.20931448286864907, 0.2096033999696374, 0.22443798009771854, 0.20825321204029024, 0.2083706361008808, 0.20774203294422477, 0.20848835993092507, 0.2095085319597274, 0.20892105298116803, 0.20838093291968107, 0.20836284395772964, 0.2096320919226855, 0.20902743004262447, 0.2082503760466352, 0.20823867805302143, 0.22686147107742727, 0.23099057585932314, 0.2095973389223218, 0.2075115729821846, 0.20769286691211164, 0.20805089897476137, 0.20779363997280598, 0.20771000511012971, 0.20935786503832787, 0.21370171988382936, 0.2137542839627713, 0.20867142907809466, 0.20965590712148696, 0.2112697750562802, 0.20834063191432506, 0.20753215404693037, 0.20814963313750923, 0.20861071988474578, 0.20783535484224558, 0.20880642486736178, 0.2077665909891948, 0.2083816429367289, 0.2100248959613964, 0.20875887107104063, 0.20814838516525924, 0.2089825621806085, 0.20901070605032146, 0.2096980249043554, 0.20849268499296159, 0.20919109601527452, 0.20869736489839852, 0.20905206794850528, 0.21086815907619894, 0.22176489990670234, 0.2275525570148602, 0.22826252889353782, 0.22751170094124973, 0.2277223818236962, 0.22680933203082532, 0.22716120991390198, 0.22729921899735928, 0.2274032379500568, 0.22652331599965692, 0.22609258093871176, 0.22708475193940103, 0.22736276197247207, 0.22713678795844316, 0.2271889919647947, 0.22702375403605402, 0.2272732281126082, 0.22657388192601502, 0.2278958138776943, 0.22789511096198112, 0.2278597690165043, 0.23680454096756876, 0.22966578195337206, 0.2155544088454917, 0.21335494692903012, 0.21188790502492338, 0.21178725396748632, 0.21412081504240632, 0.2146444469690323, 0.21337286289781332, 0.23299355199560523, 0.21973446803167462, 0.2141218709293753, 0.21299239504151046, 0.21276290388777852, 0.21261098387185484, 0.21333082800265402, 0.21464773314073682, 0.21426410612184554, 0.22071037313435227, 0.21298183116596192, 0.21283788699656725, 0.21271715394686908, 0.2129784469725564, 0.2122992171207443, 0.21194444899447262, 0.2125215280102566, 0.2135801638942212, 0.21294473705347627, 0.2129173099528998, 0.2134095240617171, 0.212537306942977, 0.21338441595435143, 0.21228913008235395, 0.2126493731047958, 0.21436047891620547, 0.21534907503519207, 0.2142366130137816, 0.21346128289587796, 0.21447788598015904, 0.2144652649294585, 0.21582451288122684, 0.23335868900176138, 0.21430763707030565, 0.2126296750502661, 0.212028774083592, 0.21143536595627666, 0.21230999496765435, 0.21176106692291796, 0.2128225100459531, 0.2186967059969902, 0.2165926059242338, 0.21357358596287668, 0.2116783030796796, 0.21169664501212537, 0.2123030920047313, 0.21285612788051367, 0.2130876659648493, 0.2151906779035926, 0.21248485089745373, 0.21295239811297506, 0.21275114198215306, 0.21350842190440744, 0.21309517603367567, 0.2146742680342868, 0.21682468603830785, 0.2162772020092234, 0.21654540905728936, 0.21591379784513265, 0.21593074407428503, 0.21730640495661646, 0.21639045292977244, 0.21600866504013538, 0.21529913798440248, 0.21801341301761568, 0.2193434878718108, 0.2186125338776037, 0.2332126758992672, 0.21806461515370756, 0.21678972512017936, 0.2202394399791956, 0.22079821000806987, 0.21988807385787368, 0.21868884400464594, 0.3015850310912356, 0.21991987305227667, 0.22006016690284014, 0.2251992980018258, 0.22288379701785743, 0.2237101149512455, 0.22350324783474207, 0.2216876979218796, 0.2825280281249434, 0.21796792407985777, 0.22080714697949588, 0.22359009610954672, 0.22254218894522637, 0.21769694797694683, 0.21455064497422427, 0.2781893589999527, 0.24180067516863346, 0.2397765489295125, 0.24037607398349792, 0.24127717805095017, 0.2216096610063687, 0.21788412681780756, 0.21776505711022764, 0.2175981568871066, 0.22537709202151746, 0.21640115196350962, 0.21625002089422196, 0.21932935598306358, 0.21723312104586512, 0.24215298588387668, 0.24268506793305278, 0.2415176269132644, 0.23334992909803987, 0.21921874315012246, 0.2199569270014763, 0.22149918600916862, 0.2212538932217285, 0.22148788603954017, 0.22078369092196226, 0.22208928898908198, 0.22347873402759433, 0.23562111193314195, 0.22249297599773854, 0.22243926010560244, 0.2214897299418226, 0.22160163999069482, 0.22189350496046245, 0.2211537630064413, 0.22168653190601617, 0.22255336586385965]
Total Epoch List: [106, 110, 21]
Total Time List: [0.05331585509702563, 0.04946517990902066, 0.050336258951574564]
T-times Epoch Time: 0.2156300715267981 ~ 0.0028242642657317473
T-times Total Epoch: 73.22222222222221 ~ 11.373566929128168
T-times Total Time: 0.049634333104929984 ~ 0.0010250046610242255
T-times Inference Elapsed: 0.04894323312606086 ~ 0.000555730756019045
T-times Time Per Graph: 0.0011292442131775994 ~ 1.3606764152037459e-05
T-times Speed: 889.1480789191424 ~ 10.061389688667013
T-times cross validation test micro f1 score:0.6740934011321738 ~ 0.03583769164961716
T-times cross validation test precision:0.6711299420715745 ~ 0.0391601911313814
T-times cross validation test recall:0.7671957671957671 ~ 0.07052113386748454
T-times cross validation test f1_score:0.6740934011321738 ~ 0.011960582114715144
