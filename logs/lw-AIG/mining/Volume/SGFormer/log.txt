Namespace(seed=60, model='SGFormer', dataset='mining/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=32, abs_pe='lap', abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Volume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 118], edge_attr=[118, 2], x=[28, 14887], y=[1, 1], num_nodes=32)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f39b490>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.92s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7434 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7321 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000015
Train loss: 0.7043;  Loss pred: 0.7043; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7454 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7341 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000045
Train loss: 0.7131;  Loss pred: 0.7131; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7470 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7359 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000075
Train loss: 0.7237;  Loss pred: 0.7237; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7467 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7360 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000105
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7469 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7365 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000135
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7455 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7356 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000165
Train loss: 0.6977;  Loss pred: 0.6977; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7441 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7347 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6594;  Loss pred: 0.6594; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7433 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7344 score: 0.5000 time: 0.06s
Epoch 9/1000, LR 0.000225
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7417 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7333 score: 0.5000 time: 0.06s
Epoch 10/1000, LR 0.000255
Train loss: 0.6143;  Loss pred: 0.6143; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7405 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7327 score: 0.5000 time: 0.06s
Epoch 11/1000, LR 0.000285
Train loss: 0.6103;  Loss pred: 0.6103; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7385 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7318 score: 0.5000 time: 0.06s
Epoch 12/1000, LR 0.000285
Train loss: 0.5865;  Loss pred: 0.5865; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7342 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7288 score: 0.5000 time: 0.05s
Epoch 13/1000, LR 0.000285
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7297 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7258 score: 0.5000 time: 0.06s
Epoch 14/1000, LR 0.000285
Train loss: 0.5837;  Loss pred: 0.5837; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7253 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7228 score: 0.5000 time: 0.06s
Epoch 15/1000, LR 0.000285
Train loss: 0.6132;  Loss pred: 0.6132; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7203 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7190 score: 0.5000 time: 0.06s
Epoch 16/1000, LR 0.000285
Train loss: 0.5714;  Loss pred: 0.5714; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7116 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7117 score: 0.5000 time: 0.06s
Epoch 17/1000, LR 0.000285
Train loss: 0.5712;  Loss pred: 0.5712; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7036 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7051 score: 0.5000 time: 0.06s
Epoch 18/1000, LR 0.000285
Train loss: 0.5581;  Loss pred: 0.5581; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.5000 time: 0.06s
Epoch 19/1000, LR 0.000285
Train loss: 0.5425;  Loss pred: 0.5425; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5000 time: 0.18s
Epoch 20/1000, LR 0.000285
Train loss: 0.5435;  Loss pred: 0.5435; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6739 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.5000 time: 0.07s
Epoch 21/1000, LR 0.000285
Train loss: 0.5152;  Loss pred: 0.5152; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6630 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6726 score: 0.5000 time: 0.27s
Epoch 22/1000, LR 0.000285
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6523 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6644 score: 0.5000 time: 0.06s
Epoch 23/1000, LR 0.000285
Train loss: 0.5308;  Loss pred: 0.5308; Loss self: 0.0000; time: 0.17s
Val loss: 0.6408 score: 0.5116 time: 0.07s
Test loss: 0.6555 score: 0.5227 time: 0.06s
Epoch 24/1000, LR 0.000285
Train loss: 0.4840;  Loss pred: 0.4840; Loss self: 0.0000; time: 0.17s
Val loss: 0.6288 score: 0.5581 time: 0.06s
Test loss: 0.6461 score: 0.5682 time: 0.06s
Epoch 25/1000, LR 0.000285
Train loss: 0.4711;  Loss pred: 0.4711; Loss self: 0.0000; time: 0.17s
Val loss: 0.6175 score: 0.5814 time: 0.06s
Test loss: 0.6372 score: 0.5909 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4720;  Loss pred: 0.4720; Loss self: 0.0000; time: 0.31s
Val loss: 0.6067 score: 0.6279 time: 0.07s
Test loss: 0.6286 score: 0.6136 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.4828;  Loss pred: 0.4828; Loss self: 0.0000; time: 0.16s
Val loss: 0.5963 score: 0.6744 time: 0.06s
Test loss: 0.6203 score: 0.6136 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.4694;  Loss pred: 0.4694; Loss self: 0.0000; time: 0.16s
Val loss: 0.5857 score: 0.7209 time: 0.06s
Test loss: 0.6114 score: 0.6364 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.4625;  Loss pred: 0.4625; Loss self: 0.0000; time: 0.31s
Val loss: 0.5762 score: 0.7907 time: 0.07s
Test loss: 0.6036 score: 0.6591 time: 0.06s
Epoch 30/1000, LR 0.000285
Train loss: 0.4350;  Loss pred: 0.4350; Loss self: 0.0000; time: 0.17s
Val loss: 0.5673 score: 0.8140 time: 0.07s
Test loss: 0.5964 score: 0.7045 time: 0.06s
Epoch 31/1000, LR 0.000285
Train loss: 0.4288;  Loss pred: 0.4288; Loss self: 0.0000; time: 0.18s
Val loss: 0.5603 score: 0.8140 time: 0.07s
Test loss: 0.5908 score: 0.7273 time: 0.06s
Epoch 32/1000, LR 0.000285
Train loss: 0.4167;  Loss pred: 0.4167; Loss self: 0.0000; time: 0.19s
Val loss: 0.5539 score: 0.8140 time: 0.06s
Test loss: 0.5855 score: 0.7500 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.4195;  Loss pred: 0.4195; Loss self: 0.0000; time: 0.17s
Val loss: 0.5477 score: 0.8140 time: 0.21s
Test loss: 0.5802 score: 0.7500 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.4244;  Loss pred: 0.4244; Loss self: 0.0000; time: 0.16s
Val loss: 0.5421 score: 0.8140 time: 0.06s
Test loss: 0.5757 score: 0.7500 time: 0.06s
Epoch 35/1000, LR 0.000285
Train loss: 0.3860;  Loss pred: 0.3860; Loss self: 0.0000; time: 0.16s
Val loss: 0.5359 score: 0.8372 time: 0.06s
Test loss: 0.5704 score: 0.7955 time: 0.06s
Epoch 36/1000, LR 0.000285
Train loss: 0.3736;  Loss pred: 0.3736; Loss self: 0.0000; time: 0.16s
Val loss: 0.5300 score: 0.8837 time: 0.07s
Test loss: 0.5654 score: 0.7955 time: 0.28s
Epoch 37/1000, LR 0.000285
Train loss: 0.3756;  Loss pred: 0.3756; Loss self: 0.0000; time: 0.17s
Val loss: 0.5246 score: 0.9070 time: 0.07s
Test loss: 0.5610 score: 0.7955 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3771;  Loss pred: 0.3771; Loss self: 0.0000; time: 0.17s
Val loss: 0.5181 score: 0.9070 time: 0.07s
Test loss: 0.5557 score: 0.7955 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.3745;  Loss pred: 0.3745; Loss self: 0.0000; time: 0.30s
Val loss: 0.5123 score: 0.8837 time: 0.06s
Test loss: 0.5508 score: 0.7955 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.3716;  Loss pred: 0.3716; Loss self: 0.0000; time: 0.17s
Val loss: 0.5051 score: 0.8837 time: 0.06s
Test loss: 0.5444 score: 0.8409 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3522;  Loss pred: 0.3522; Loss self: 0.0000; time: 0.18s
Val loss: 0.4983 score: 0.8837 time: 0.06s
Test loss: 0.5388 score: 0.8182 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3348;  Loss pred: 0.3348; Loss self: 0.0000; time: 0.17s
Val loss: 0.4916 score: 0.8837 time: 0.06s
Test loss: 0.5334 score: 0.8182 time: 0.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.3474;  Loss pred: 0.3474; Loss self: 0.0000; time: 0.17s
Val loss: 0.4854 score: 0.8837 time: 0.07s
Test loss: 0.5278 score: 0.8182 time: 0.06s
Epoch 44/1000, LR 0.000284
Train loss: 0.3496;  Loss pred: 0.3496; Loss self: 0.0000; time: 0.17s
Val loss: 0.4781 score: 0.8837 time: 0.28s
Test loss: 0.5213 score: 0.8182 time: 0.08s
Epoch 45/1000, LR 0.000284
Train loss: 0.3349;  Loss pred: 0.3349; Loss self: 0.0000; time: 0.31s
Val loss: 0.4704 score: 0.8605 time: 0.06s
Test loss: 0.5146 score: 0.8182 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.3289;  Loss pred: 0.3289; Loss self: 0.0000; time: 0.17s
Val loss: 0.4623 score: 0.8605 time: 0.06s
Test loss: 0.5070 score: 0.8182 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3112;  Loss pred: 0.3112; Loss self: 0.0000; time: 0.17s
Val loss: 0.4551 score: 0.8605 time: 0.06s
Test loss: 0.5007 score: 0.8182 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3115;  Loss pred: 0.3115; Loss self: 0.0000; time: 0.17s
Val loss: 0.4478 score: 0.8605 time: 0.06s
Test loss: 0.4942 score: 0.8182 time: 0.06s
Epoch 49/1000, LR 0.000284
Train loss: 0.3130;  Loss pred: 0.3130; Loss self: 0.0000; time: 0.17s
Val loss: 0.4416 score: 0.8605 time: 0.27s
Test loss: 0.4889 score: 0.8182 time: 0.06s
Epoch 50/1000, LR 0.000284
Train loss: 0.3038;  Loss pred: 0.3038; Loss self: 0.0000; time: 0.17s
Val loss: 0.4350 score: 0.8605 time: 0.06s
Test loss: 0.4834 score: 0.8182 time: 0.06s
Epoch 51/1000, LR 0.000284
Train loss: 0.3131;  Loss pred: 0.3131; Loss self: 0.0000; time: 0.18s
Val loss: 0.4286 score: 0.8837 time: 0.06s
Test loss: 0.4781 score: 0.8182 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.2905;  Loss pred: 0.2905; Loss self: 0.0000; time: 0.18s
Val loss: 0.4227 score: 0.8837 time: 0.06s
Test loss: 0.4730 score: 0.8182 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.2921;  Loss pred: 0.2921; Loss self: 0.0000; time: 0.30s
Val loss: 0.4174 score: 0.8837 time: 0.06s
Test loss: 0.4682 score: 0.8409 time: 0.06s
Epoch 54/1000, LR 0.000284
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 0.16s
Val loss: 0.4131 score: 0.8837 time: 0.06s
Test loss: 0.4644 score: 0.8409 time: 0.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.2875;  Loss pred: 0.2875; Loss self: 0.0000; time: 0.16s
Val loss: 0.4098 score: 0.8605 time: 0.06s
Test loss: 0.4619 score: 0.8409 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.2807;  Loss pred: 0.2807; Loss self: 0.0000; time: 0.17s
Val loss: 0.4065 score: 0.8837 time: 0.07s
Test loss: 0.4598 score: 0.8409 time: 0.06s
Epoch 57/1000, LR 0.000283
Train loss: 0.2616;  Loss pred: 0.2616; Loss self: 0.0000; time: 0.38s
Val loss: 0.4037 score: 0.8837 time: 0.07s
Test loss: 0.4586 score: 0.8409 time: 0.06s
Epoch 58/1000, LR 0.000283
Train loss: 0.2853;  Loss pred: 0.2853; Loss self: 0.0000; time: 0.16s
Val loss: 0.4011 score: 0.8837 time: 0.06s
Test loss: 0.4575 score: 0.8409 time: 0.06s
Epoch 59/1000, LR 0.000283
Train loss: 0.2770;  Loss pred: 0.2770; Loss self: 0.0000; time: 0.17s
Val loss: 0.3989 score: 0.8837 time: 0.07s
Test loss: 0.4567 score: 0.8409 time: 0.20s
Epoch 60/1000, LR 0.000283
Train loss: 0.2522;  Loss pred: 0.2522; Loss self: 0.0000; time: 0.18s
Val loss: 0.3972 score: 0.8837 time: 0.07s
Test loss: 0.4564 score: 0.8409 time: 0.06s
Epoch 61/1000, LR 0.000283
Train loss: 0.2706;  Loss pred: 0.2706; Loss self: 0.0000; time: 0.17s
Val loss: 0.3950 score: 0.8837 time: 0.06s
Test loss: 0.4559 score: 0.8409 time: 0.07s
Epoch 62/1000, LR 0.000283
Train loss: 0.2558;  Loss pred: 0.2558; Loss self: 0.0000; time: 0.17s
Val loss: 0.3929 score: 0.8837 time: 0.06s
Test loss: 0.4553 score: 0.8182 time: 0.06s
Epoch 63/1000, LR 0.000283
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 0.17s
Val loss: 0.3917 score: 0.8837 time: 0.07s
Test loss: 0.4550 score: 0.8182 time: 0.06s
Epoch 64/1000, LR 0.000283
Train loss: 0.2426;  Loss pred: 0.2426; Loss self: 0.0000; time: 0.37s
Val loss: 0.3909 score: 0.8837 time: 0.07s
Test loss: 0.4548 score: 0.8182 time: 0.06s
Epoch 65/1000, LR 0.000283
Train loss: 0.2486;  Loss pred: 0.2486; Loss self: 0.0000; time: 0.17s
Val loss: 0.3894 score: 0.8837 time: 0.06s
Test loss: 0.4534 score: 0.8182 time: 0.06s
Epoch 66/1000, LR 0.000283
Train loss: 0.2390;  Loss pred: 0.2390; Loss self: 0.0000; time: 0.17s
Val loss: 0.3880 score: 0.8605 time: 0.06s
Test loss: 0.4524 score: 0.8182 time: 0.06s
Epoch 67/1000, LR 0.000283
Train loss: 0.2445;  Loss pred: 0.2445; Loss self: 0.0000; time: 0.29s
Val loss: 0.3864 score: 0.8605 time: 0.06s
Test loss: 0.4510 score: 0.8182 time: 0.06s
Epoch 68/1000, LR 0.000283
Train loss: 0.2271;  Loss pred: 0.2271; Loss self: 0.0000; time: 0.17s
Val loss: 0.3860 score: 0.8605 time: 0.06s
Test loss: 0.4517 score: 0.8182 time: 0.06s
Epoch 69/1000, LR 0.000283
Train loss: 0.2349;  Loss pred: 0.2349; Loss self: 0.0000; time: 0.17s
Val loss: 0.3861 score: 0.8605 time: 0.06s
Test loss: 0.4521 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.2424;  Loss pred: 0.2424; Loss self: 0.0000; time: 0.17s
Val loss: 0.3844 score: 0.8605 time: 0.07s
Test loss: 0.4510 score: 0.8182 time: 0.06s
Epoch 71/1000, LR 0.000282
Train loss: 0.2299;  Loss pred: 0.2299; Loss self: 0.0000; time: 0.17s
Val loss: 0.3838 score: 0.8605 time: 0.26s
Test loss: 0.4503 score: 0.8182 time: 0.06s
Epoch 72/1000, LR 0.000282
Train loss: 0.2391;  Loss pred: 0.2391; Loss self: 0.0000; time: 0.16s
Val loss: 0.3821 score: 0.8605 time: 0.06s
Test loss: 0.4492 score: 0.8182 time: 0.06s
Epoch 73/1000, LR 0.000282
Train loss: 0.2053;  Loss pred: 0.2053; Loss self: 0.0000; time: 0.16s
Val loss: 0.3816 score: 0.8605 time: 0.06s
Test loss: 0.4483 score: 0.8182 time: 0.06s
Epoch 74/1000, LR 0.000282
Train loss: 0.2177;  Loss pred: 0.2177; Loss self: 0.0000; time: 0.15s
Val loss: 0.3806 score: 0.8605 time: 0.06s
Test loss: 0.4466 score: 0.8182 time: 0.06s
Epoch 75/1000, LR 0.000282
Train loss: 0.2186;  Loss pred: 0.2186; Loss self: 0.0000; time: 0.29s
Val loss: 0.3799 score: 0.8605 time: 0.07s
Test loss: 0.4456 score: 0.8182 time: 0.06s
Epoch 76/1000, LR 0.000282
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.17s
Val loss: 0.3791 score: 0.8605 time: 0.07s
Test loss: 0.4450 score: 0.8182 time: 0.06s
Epoch 77/1000, LR 0.000282
Train loss: 0.2111;  Loss pred: 0.2111; Loss self: 0.0000; time: 0.17s
Val loss: 0.3779 score: 0.8605 time: 0.06s
Test loss: 0.4441 score: 0.8182 time: 0.07s
Epoch 78/1000, LR 0.000282
Train loss: 0.2014;  Loss pred: 0.2014; Loss self: 0.0000; time: 0.17s
Val loss: 0.3773 score: 0.8605 time: 0.06s
Test loss: 0.4439 score: 0.8182 time: 0.07s
Epoch 79/1000, LR 0.000282
Train loss: 0.2050;  Loss pred: 0.2050; Loss self: 0.0000; time: 0.18s
Val loss: 0.3767 score: 0.8605 time: 0.26s
Test loss: 0.4447 score: 0.8182 time: 0.06s
Epoch 80/1000, LR 0.000282
Train loss: 0.2113;  Loss pred: 0.2113; Loss self: 0.0000; time: 0.18s
Val loss: 0.3761 score: 0.8605 time: 0.06s
Test loss: 0.4461 score: 0.8182 time: 0.19s
Epoch 81/1000, LR 0.000281
Train loss: 0.2038;  Loss pred: 0.2038; Loss self: 0.0000; time: 0.15s
Val loss: 0.3757 score: 0.8605 time: 0.06s
Test loss: 0.4470 score: 0.8182 time: 0.06s
Epoch 82/1000, LR 0.000281
Train loss: 0.1988;  Loss pred: 0.1988; Loss self: 0.0000; time: 0.16s
Val loss: 0.3750 score: 0.8605 time: 0.06s
Test loss: 0.4471 score: 0.8182 time: 0.06s
Epoch 83/1000, LR 0.000281
Train loss: 0.2032;  Loss pred: 0.2032; Loss self: 0.0000; time: 0.16s
Val loss: 0.3754 score: 0.8605 time: 0.06s
Test loss: 0.4483 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.1856;  Loss pred: 0.1856; Loss self: 0.0000; time: 0.17s
Val loss: 0.3749 score: 0.8605 time: 0.06s
Test loss: 0.4494 score: 0.8182 time: 0.06s
Epoch 85/1000, LR 0.000281
Train loss: 0.1969;  Loss pred: 0.1969; Loss self: 0.0000; time: 0.17s
Val loss: 0.3761 score: 0.8605 time: 0.06s
Test loss: 0.4528 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.1941;  Loss pred: 0.1941; Loss self: 0.0000; time: 0.16s
Val loss: 0.3758 score: 0.8605 time: 0.06s
Test loss: 0.4542 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.1813;  Loss pred: 0.1813; Loss self: 0.0000; time: 0.48s
Val loss: 0.3759 score: 0.8605 time: 0.07s
Test loss: 0.4557 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.1856;  Loss pred: 0.1856; Loss self: 0.0000; time: 0.17s
Val loss: 0.3742 score: 0.8605 time: 0.06s
Test loss: 0.4540 score: 0.7955 time: 0.06s
Epoch 89/1000, LR 0.000281
Train loss: 0.1845;  Loss pred: 0.1845; Loss self: 0.0000; time: 0.16s
Val loss: 0.3735 score: 0.8605 time: 0.06s
Test loss: 0.4536 score: 0.7955 time: 0.06s
Epoch 90/1000, LR 0.000281
Train loss: 0.1837;  Loss pred: 0.1837; Loss self: 0.0000; time: 0.15s
Val loss: 0.3724 score: 0.8605 time: 0.06s
Test loss: 0.4522 score: 0.8182 time: 0.05s
Epoch 91/1000, LR 0.000280
Train loss: 0.1755;  Loss pred: 0.1755; Loss self: 0.0000; time: 0.15s
Val loss: 0.3711 score: 0.8605 time: 0.07s
Test loss: 0.4499 score: 0.8182 time: 0.07s
Epoch 92/1000, LR 0.000280
Train loss: 0.1723;  Loss pred: 0.1723; Loss self: 0.0000; time: 0.20s
Val loss: 0.3713 score: 0.8605 time: 0.06s
Test loss: 0.4492 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1651;  Loss pred: 0.1651; Loss self: 0.0000; time: 0.15s
Val loss: 0.3700 score: 0.8605 time: 0.06s
Test loss: 0.4478 score: 0.8182 time: 0.07s
Epoch 94/1000, LR 0.000280
Train loss: 0.1811;  Loss pred: 0.1811; Loss self: 0.0000; time: 0.17s
Val loss: 0.3682 score: 0.8605 time: 0.28s
Test loss: 0.4450 score: 0.8182 time: 0.20s
Epoch 95/1000, LR 0.000280
Train loss: 0.1629;  Loss pred: 0.1629; Loss self: 0.0000; time: 0.16s
Val loss: 0.3664 score: 0.8605 time: 0.06s
Test loss: 0.4438 score: 0.8182 time: 0.06s
Epoch 96/1000, LR 0.000280
Train loss: 0.1706;  Loss pred: 0.1706; Loss self: 0.0000; time: 0.17s
Val loss: 0.3641 score: 0.8605 time: 0.06s
Test loss: 0.4414 score: 0.8182 time: 0.06s
Epoch 97/1000, LR 0.000280
Train loss: 0.1625;  Loss pred: 0.1625; Loss self: 0.0000; time: 0.17s
Val loss: 0.3619 score: 0.8605 time: 0.07s
Test loss: 0.4387 score: 0.8182 time: 0.06s
Epoch 98/1000, LR 0.000280
Train loss: 0.1503;  Loss pred: 0.1503; Loss self: 0.0000; time: 0.15s
Val loss: 0.3614 score: 0.8605 time: 0.06s
Test loss: 0.4381 score: 0.8182 time: 0.06s
Epoch 99/1000, LR 0.000279
Train loss: 0.1532;  Loss pred: 0.1532; Loss self: 0.0000; time: 0.15s
Val loss: 0.3608 score: 0.8605 time: 0.06s
Test loss: 0.4375 score: 0.8182 time: 0.06s
Epoch 100/1000, LR 0.000279
Train loss: 0.1521;  Loss pred: 0.1521; Loss self: 0.0000; time: 0.15s
Val loss: 0.3617 score: 0.8605 time: 0.06s
Test loss: 0.4391 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1596;  Loss pred: 0.1596; Loss self: 0.0000; time: 0.17s
Val loss: 0.3621 score: 0.8605 time: 0.06s
Test loss: 0.4397 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1592;  Loss pred: 0.1592; Loss self: 0.0000; time: 0.27s
Val loss: 0.3629 score: 0.8605 time: 0.31s
Test loss: 0.4396 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1523;  Loss pred: 0.1523; Loss self: 0.0000; time: 0.17s
Val loss: 0.3633 score: 0.8605 time: 0.06s
Test loss: 0.4385 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1624;  Loss pred: 0.1624; Loss self: 0.0000; time: 0.16s
Val loss: 0.3639 score: 0.8837 time: 0.06s
Test loss: 0.4380 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1441;  Loss pred: 0.1441; Loss self: 0.0000; time: 0.16s
Val loss: 0.3649 score: 0.8837 time: 0.06s
Test loss: 0.4387 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1531;  Loss pred: 0.1531; Loss self: 0.0000; time: 0.16s
Val loss: 0.3655 score: 0.8605 time: 0.06s
Test loss: 0.4378 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 0.18s
Val loss: 0.3658 score: 0.8605 time: 0.06s
Test loss: 0.4366 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1412;  Loss pred: 0.1412; Loss self: 0.0000; time: 0.18s
Val loss: 0.3670 score: 0.8605 time: 0.07s
Test loss: 0.4350 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1473;  Loss pred: 0.1473; Loss self: 0.0000; time: 0.16s
Val loss: 0.3668 score: 0.8605 time: 0.06s
Test loss: 0.4331 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1366;  Loss pred: 0.1366; Loss self: 0.0000; time: 0.29s
Val loss: 0.3682 score: 0.8605 time: 0.06s
Test loss: 0.4334 score: 0.7955 time: 0.29s
     INFO: Early stopping counter 11 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1273;  Loss pred: 0.1273; Loss self: 0.0000; time: 0.16s
Val loss: 0.3682 score: 0.8605 time: 0.06s
Test loss: 0.4333 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1355;  Loss pred: 0.1355; Loss self: 0.0000; time: 0.16s
Val loss: 0.3698 score: 0.8605 time: 0.06s
Test loss: 0.4356 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1422;  Loss pred: 0.1422; Loss self: 0.0000; time: 0.16s
Val loss: 0.3715 score: 0.8605 time: 0.06s
Test loss: 0.4363 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1228;  Loss pred: 0.1228; Loss self: 0.0000; time: 0.17s
Val loss: 0.3736 score: 0.8605 time: 0.06s
Test loss: 0.4362 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1357;  Loss pred: 0.1357; Loss self: 0.0000; time: 0.18s
Val loss: 0.3768 score: 0.8605 time: 0.06s
Test loss: 0.4367 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 0.30s
Val loss: 0.3790 score: 0.8605 time: 0.06s
Test loss: 0.4380 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1242;  Loss pred: 0.1242; Loss self: 0.0000; time: 0.17s
Val loss: 0.3823 score: 0.8605 time: 0.06s
Test loss: 0.4434 score: 0.8182 time: 0.30s
     INFO: Early stopping counter 18 of 20
Epoch 118/1000, LR 0.000277
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.21s
Val loss: 0.3824 score: 0.8605 time: 0.06s
Test loss: 0.4417 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 119/1000, LR 0.000277
Train loss: 0.1257;  Loss pred: 0.1257; Loss self: 0.0000; time: 0.16s
Val loss: 0.3810 score: 0.8605 time: 0.06s
Test loss: 0.4372 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 098,   Train_Loss: 0.1532,   Val_Loss: 0.3608,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8605,   Val_Loss: 0.3608,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8182,   Test_loss: 0.4375


[0.07191935099945113, 0.06566255500001716, 0.06481734499993763, 0.0645866439999736, 0.0654895409998062, 0.06699546399977407, 0.07489186499969946, 0.06789828700038925, 0.065364120000595, 0.06464336099998036, 0.06451441399985924, 0.05992405399956624, 0.06385051400047814, 0.06252894699991884, 0.06415656399985892, 0.06309215600049356, 0.0617179880000549, 0.0641192979992411, 0.1894068010005867, 0.07646090099933645, 0.2771502909999981, 0.06651674099975935, 0.06608974400023726, 0.06725618300060887, 0.06670381500043732, 0.06598288900022453, 0.06562872199992853, 0.06523706200005108, 0.06973811700027, 0.06990561699967657, 0.06867506599974149, 0.06882964399937919, 0.062270883000564936, 0.06334112999957142, 0.06483782600025734, 0.28924775599989516, 0.06909718600036285, 0.06811032900077407, 0.06529857799978345, 0.07043848400007846, 0.06744881400027225, 0.06771488899994438, 0.06791518099998939, 0.08430060699993192, 0.06500819899974886, 0.06531481099955272, 0.06504587800009176, 0.06363612099994498, 0.06740145299954747, 0.06567490599991288, 0.0671864790001564, 0.0656816250002521, 0.06673767299980682, 0.06430500599981315, 0.06567406500016659, 0.06758690900005604, 0.06828165099977923, 0.06732209099936881, 0.20218850400033261, 0.06953205599984358, 0.07159061499987729, 0.06690466399959405, 0.0673232930002996, 0.06922149599995464, 0.06753499700062093, 0.06784073900053045, 0.06801344900031836, 0.06668652699954691, 0.06984229599947867, 0.06936718699944322, 0.06119231799948466, 0.0631043059993317, 0.0639366900004461, 0.06529062899971905, 0.06836912100061454, 0.06744043300022895, 0.07002063199979602, 0.07175901200025692, 0.06500699499974871, 0.19943561499985663, 0.06787225999960356, 0.06755850900026417, 0.07186840099984693, 0.06693687000006321, 0.065379216999645, 0.071644446999926, 0.0654276570003276, 0.06368186799954856, 0.06381112999952165, 0.059397782999440096, 0.07724504999987403, 0.06215368599987414, 0.07222140700014279, 0.20523750899974402, 0.06538299700059724, 0.06806568899992271, 0.06354714300050546, 0.06045466100022168, 0.06061913000030472, 0.0611227310000686, 0.061042645999805245, 0.06966101199941477, 0.06683107800017751, 0.06522892599969055, 0.06565365999995265, 0.0660666260000653, 0.07000669399985782, 0.0637244469999132, 0.06090805100029684, 0.29635152199989534, 0.06483579199993983, 0.0650263610004913, 0.065247575000285, 0.06462665199978801, 0.06737577100011549, 0.06559359999937442, 0.3106766989994867, 0.06436933699933434, 0.06667949300026521]
[0.00163453070453298, 0.0014923307954549355, 0.00147312147727131, 0.0014678782727266728, 0.0014883986590865045, 0.0015226241818130472, 0.0017020878409022605, 0.001543142886372483, 0.0014855481818317044, 0.001469167295454099, 0.0014662366818149828, 0.00136191031817196, 0.0014511480454654122, 0.0014211124318163374, 0.001458103727269521, 0.0014339126363748537, 0.0014026815454557932, 0.001457256772710025, 0.004304700022740607, 0.0017377477499849192, 0.006298870249999957, 0.0015117441136308944, 0.0015020396363690286, 0.0015285496136502015, 0.0015159957954644844, 0.0014996111136414665, 0.0014915618636347392, 0.001482660500001161, 0.0015849572045515908, 0.001588764022719922, 0.0015607969545395793, 0.0015643100908949816, 0.0014152473409219303, 0.001439571136353896, 0.001473586954551303, 0.006573812636361254, 0.0015703905909173375, 0.0015479620227448652, 0.0014840585909041693, 0.0016008746363654195, 0.0015329275909152784, 0.0015389747499987359, 0.0015435268409088496, 0.001915922886362089, 0.0014774590681761104, 0.0014844275227171072, 0.0014783154090929945, 0.0014462754772714768, 0.00153185120453517, 0.00149261149999802, 0.0015269654318217363, 0.001492764204551184, 0.001516765295450155, 0.0014614774090866624, 0.0014925923863674225, 0.0015360661136376373, 0.001551855704540437, 0.0015300475227129275, 0.004595193272734832, 0.001580273999996445, 0.0016270594318153928, 0.0015205605454453193, 0.0015300748409159, 0.0015732158181807872, 0.0015348862954686576, 0.001541834977284783, 0.00154576020455269, 0.001515602886353339, 0.0015873249090790605, 0.0015765269772600732, 0.0013907344999882878, 0.001434188772712084, 0.0014531065909192296, 0.0014838779318117965, 0.001553843659104876, 0.001532737113641567, 0.0015913779999953642, 0.0016308866363694754, 0.0014774317045397436, 0.004532627613633105, 0.0015425513636273536, 0.001535420659096913, 0.001633372749996521, 0.0015212925000014366, 0.0014858912954464772, 0.0016282828863619545, 0.0014869922045529, 0.0014473151818079218, 0.001450252954534583, 0.0013499496136236385, 0.0017555693181789552, 0.0014125837727244123, 0.0016413956136396089, 0.004664488840903273, 0.0014859772045590282, 0.0015469474772709707, 0.0014442532500114876, 0.0013739695681868563, 0.0013777075000069256, 0.0013891529772742863, 0.0013873328636319375, 0.0015832048181685175, 0.0015188881363676708, 0.0014824755909020578, 0.0014921286363625602, 0.0015015142272742114, 0.001591061227269496, 0.0014482828863616635, 0.0013842738863703826, 0.006735261863633985, 0.0014735407272713599, 0.001477871840920257, 0.001482899431824659, 0.0014687875454497275, 0.0015312675227298976, 0.0014907636363494187, 0.007060834068170152, 0.0014629394772575986, 0.0015154430227333003]
[611.7963995578297, 670.0927187494989, 678.8306432489998, 681.2554001105551, 671.8630078676259, 656.7608816045871, 587.5137439850986, 648.0281306617906, 673.1521819554746, 680.657678056272, 682.0181300894401, 734.2627386377847, 689.1095661292642, 703.6740919378846, 685.8222644232749, 697.3925570027404, 712.9201943517805, 686.2208628753355, 232.3042243866613, 575.4575139048106, 158.7586281841584, 661.487609565225, 665.7613925670834, 654.2149440684385, 659.6324363113494, 666.8395498695165, 670.4381657782078, 674.4632368632043, 630.931861837188, 629.4200936700634, 640.6983285631735, 639.259444671788, 706.5902694779649, 694.6513268755659, 678.6162139339059, 152.11872551231113, 636.7842534103913, 646.010680692791, 673.8278435427172, 624.658531832556, 652.3465334738488, 649.7832404338156, 647.8669327260849, 521.9416747501656, 676.8377016592935, 673.660373912761, 676.4456311887731, 691.4312077575886, 652.80491802299, 669.9666993060997, 654.8936728757221, 669.898164057773, 659.2977852273538, 684.239108851461, 669.975278671853, 651.0136452602607, 644.3898083270169, 653.5744708287883, 217.61870298979815, 632.8016533855836, 614.6056993654184, 657.6522079277908, 653.5628018047809, 635.640697508601, 651.5140586975291, 648.5778405163888, 646.9308739186869, 659.8034412603156, 629.9907437225207, 634.3056696295494, 719.0445049061641, 697.2582821918045, 688.1807613076779, 673.9098806995617, 643.565389696973, 652.4276022938735, 628.3862162245005, 613.1634030836777, 676.8502374270658, 220.6225803752837, 648.2766302500758, 651.2873160037909, 612.2301232233303, 657.3357851951914, 672.9967414604998, 614.1438986896702, 672.4984817930999, 690.9345058834002, 689.534882086085, 740.7683886183893, 569.6157876792332, 707.9226162079767, 609.2376461166564, 214.38576317964802, 672.9578333583895, 646.4343584335121, 692.3993420073979, 727.8181578065364, 725.8434754800807, 719.8631226073753, 720.8075482203111, 631.6302151965529, 658.3763320394612, 674.5473626257275, 670.183505383123, 665.9943554549995, 628.5113249325752, 690.4728416091227, 722.4003933369263, 148.47232672560912, 678.6375031871413, 676.6486594516269, 674.3545641321966, 680.8336597746752, 653.0537513244127, 670.7971509479528, 141.62632776033422, 683.5552772658668, 659.8730437231278]
Elapsed: 0.07861990127727621~0.04681901119187477
Time per graph: 0.001786815938119914~0.001064068436178972
Speed: 630.9944332958116~124.34040813622975
Total Time: 0.0674
best val loss: 0.3607848640098128 test_score: 0.8182

Testing...
Test loss: 0.5610 score: 0.7955 time: 0.06s
test Score 0.7955
Epoch Time List: [1.0520186630010357, 0.2974265689999811, 0.5067151310004192, 0.2826955199989243, 0.28970633700009785, 0.2876284209987716, 0.2977158160001636, 0.2991115760014509, 0.28664642899911996, 0.28319891099909, 0.3958080930005963, 0.49973489399963, 0.27024119599991536, 0.2803650949990697, 0.26860879299965745, 0.2680863119994683, 0.2715254859995184, 0.2731704860007085, 0.3968236210012037, 0.318864922001012, 0.5113738180007203, 0.2907970429996567, 0.2939924439997412, 0.2894198750000214, 0.28993831899970246, 0.44691500200133305, 0.27910435699959635, 0.2846669580003436, 0.44561562099988805, 0.3026070769992657, 0.3151263689996995, 0.31315806600105134, 0.4358365740008594, 0.27664776100118615, 0.2789770089993908, 0.5086380420007117, 0.30138823300057993, 0.3040189500006818, 0.42312929299987445, 0.30363877700074227, 0.30681966400061356, 0.29851695699926495, 0.2988277999993443, 0.5251918239991937, 0.42877742200107605, 0.29507682799976465, 0.29226279800150223, 0.2900989190002292, 0.5045724860010523, 0.2917673400006606, 0.30351892299950123, 0.298129550998965, 0.4218481650004833, 0.28249260199936543, 0.28484595800000534, 0.2968283150003117, 0.5056092730001183, 0.2911291150012403, 0.4304211550006585, 0.30792823400133784, 0.3033947919984712, 0.2901176799996392, 0.2955366109999886, 0.5054509479987246, 0.29627474200060533, 0.2917242760013323, 0.4169394500004273, 0.2918066279989944, 0.3022027649994925, 0.3063403930000277, 0.49125498800094647, 0.2726601660015149, 0.27735730200129183, 0.27387424600146915, 0.4163954380001087, 0.3012869739995949, 0.3015169869986494, 0.3050076950003131, 0.4972433180000735, 0.4362038469998879, 0.2693942580008297, 0.2799999239996396, 0.28418706899992685, 0.29868690700004663, 0.2894643720001113, 0.2919398900003216, 0.6124269189995175, 0.2851433319992793, 0.27903715199954604, 0.254870354000559, 0.28602403000058985, 0.31237043700002687, 0.27823058600006334, 0.645783076999578, 0.2834840109999277, 0.3020887330003461, 0.2926792590005789, 0.2681505880009354, 0.26286586000060197, 0.2634253930000341, 0.27866222800093965, 0.6421565639993787, 0.2927732329999344, 0.2878333999997267, 0.28381621399967116, 0.289090130000659, 0.3048068099988086, 0.3120301079998171, 0.2682489690014336, 0.6453079869997964, 0.28434719300003053, 0.2866854939993573, 0.28682344499975443, 0.29318224000053306, 0.3013063840007817, 0.42220615400128736, 0.5308118800003285, 0.3246516600001996, 0.28324070099915843]
Total Epoch List: [119]
Total Time List: [0.06739704700066795]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f39b9d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7218;  Loss pred: 0.7218; Loss self: 0.0000; time: 0.19s
Val loss: 0.6948 score: 0.6364 time: 0.07s
Test loss: 0.7001 score: 0.5349 time: 0.07s
Epoch 2/1000, LR 0.000015
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.19s
Val loss: 0.6937 score: 0.5909 time: 0.07s
Test loss: 0.6992 score: 0.4651 time: 0.06s
Epoch 3/1000, LR 0.000045
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.21s
Val loss: 0.6917 score: 0.5909 time: 0.07s
Test loss: 0.6976 score: 0.4651 time: 0.06s
Epoch 4/1000, LR 0.000075
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.21s
Val loss: 0.6891 score: 0.5227 time: 0.08s
Test loss: 0.6958 score: 0.4419 time: 0.08s
Epoch 5/1000, LR 0.000105
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.19s
Val loss: 0.6861 score: 0.5000 time: 0.07s
Test loss: 0.6934 score: 0.4884 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6421;  Loss pred: 0.6421; Loss self: 0.0000; time: 0.18s
Val loss: 0.6832 score: 0.5227 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.06s
Epoch 7/1000, LR 0.000165
Train loss: 0.6532;  Loss pred: 0.6532; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6800 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.4884 time: 0.07s
Epoch 8/1000, LR 0.000195
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4884 time: 0.06s
Epoch 9/1000, LR 0.000225
Train loss: 0.6340;  Loss pred: 0.6340; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6750 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.4884 time: 0.06s
Epoch 10/1000, LR 0.000255
Train loss: 0.6030;  Loss pred: 0.6030; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6734 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.4884 time: 0.06s
Epoch 11/1000, LR 0.000285
Train loss: 0.5738;  Loss pred: 0.5738; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6714 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6845 score: 0.4884 time: 0.07s
Epoch 12/1000, LR 0.000285
Train loss: 0.6008;  Loss pred: 0.6008; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6707 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.4884 time: 0.07s
Epoch 13/1000, LR 0.000285
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6706 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.4884 time: 0.07s
Epoch 14/1000, LR 0.000285
Train loss: 0.5471;  Loss pred: 0.5471; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6704 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.06s
Epoch 15/1000, LR 0.000285
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6672 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.4884 time: 0.07s
Epoch 16/1000, LR 0.000285
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.18s
Val loss: 0.6628 score: 0.5227 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.4884 time: 0.07s
Epoch 17/1000, LR 0.000285
Train loss: 0.5242;  Loss pred: 0.5242; Loss self: 0.0000; time: 0.17s
Val loss: 0.6583 score: 0.5227 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4884 time: 0.06s
Epoch 18/1000, LR 0.000285
Train loss: 0.5107;  Loss pred: 0.5107; Loss self: 0.0000; time: 0.18s
Val loss: 0.6515 score: 0.5455 time: 0.07s
Test loss: 0.6766 score: 0.5116 time: 0.06s
Epoch 19/1000, LR 0.000285
Train loss: 0.5085;  Loss pred: 0.5085; Loss self: 0.0000; time: 0.18s
Val loss: 0.6447 score: 0.5682 time: 0.07s
Test loss: 0.6701 score: 0.5116 time: 0.07s
Epoch 20/1000, LR 0.000285
Train loss: 0.4787;  Loss pred: 0.4787; Loss self: 0.0000; time: 0.18s
Val loss: 0.6375 score: 0.5909 time: 0.06s
Test loss: 0.6636 score: 0.5349 time: 0.06s
Epoch 21/1000, LR 0.000285
Train loss: 0.4652;  Loss pred: 0.4652; Loss self: 0.0000; time: 0.18s
Val loss: 0.6300 score: 0.5909 time: 0.06s
Test loss: 0.6567 score: 0.5349 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.4589;  Loss pred: 0.4589; Loss self: 0.0000; time: 0.17s
Val loss: 0.6224 score: 0.6364 time: 0.07s
Test loss: 0.6492 score: 0.5581 time: 0.06s
Epoch 23/1000, LR 0.000285
Train loss: 0.4585;  Loss pred: 0.4585; Loss self: 0.0000; time: 0.40s
Val loss: 0.6159 score: 0.6364 time: 0.06s
Test loss: 0.6424 score: 0.5814 time: 0.06s
Epoch 24/1000, LR 0.000285
Train loss: 0.4447;  Loss pred: 0.4447; Loss self: 0.0000; time: 0.17s
Val loss: 0.6095 score: 0.6591 time: 0.07s
Test loss: 0.6357 score: 0.5814 time: 0.06s
Epoch 25/1000, LR 0.000285
Train loss: 0.4393;  Loss pred: 0.4393; Loss self: 0.0000; time: 0.18s
Val loss: 0.6024 score: 0.6591 time: 0.07s
Test loss: 0.6285 score: 0.6047 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4302;  Loss pred: 0.4302; Loss self: 0.0000; time: 0.18s
Val loss: 0.5968 score: 0.6818 time: 0.07s
Test loss: 0.6226 score: 0.6047 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.4236;  Loss pred: 0.4236; Loss self: 0.0000; time: 0.18s
Val loss: 0.5919 score: 0.6818 time: 0.07s
Test loss: 0.6179 score: 0.6279 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 0.18s
Val loss: 0.5879 score: 0.6818 time: 0.07s
Test loss: 0.6144 score: 0.6279 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.4242;  Loss pred: 0.4242; Loss self: 0.0000; time: 0.18s
Val loss: 0.5840 score: 0.7045 time: 0.07s
Test loss: 0.6117 score: 0.6279 time: 0.07s
Epoch 30/1000, LR 0.000285
Train loss: 0.4007;  Loss pred: 0.4007; Loss self: 0.0000; time: 0.33s
Val loss: 0.5796 score: 0.7045 time: 0.06s
Test loss: 0.6083 score: 0.6512 time: 0.07s
Epoch 31/1000, LR 0.000285
Train loss: 0.3669;  Loss pred: 0.3669; Loss self: 0.0000; time: 0.18s
Val loss: 0.5756 score: 0.7045 time: 0.07s
Test loss: 0.6055 score: 0.6512 time: 0.07s
Epoch 32/1000, LR 0.000285
Train loss: 0.3724;  Loss pred: 0.3724; Loss self: 0.0000; time: 0.20s
Val loss: 0.5729 score: 0.7045 time: 0.07s
Test loss: 0.6039 score: 0.6512 time: 0.07s
Epoch 33/1000, LR 0.000285
Train loss: 0.3779;  Loss pred: 0.3779; Loss self: 0.0000; time: 0.20s
Val loss: 0.5687 score: 0.7045 time: 0.07s
Test loss: 0.6004 score: 0.6512 time: 0.08s
Epoch 34/1000, LR 0.000285
Train loss: 0.3825;  Loss pred: 0.3825; Loss self: 0.0000; time: 0.20s
Val loss: 0.5637 score: 0.7045 time: 0.08s
Test loss: 0.5953 score: 0.6744 time: 0.07s
Epoch 35/1000, LR 0.000285
Train loss: 0.3554;  Loss pred: 0.3554; Loss self: 0.0000; time: 0.20s
Val loss: 0.5599 score: 0.7045 time: 0.07s
Test loss: 0.5919 score: 0.6744 time: 0.06s
Epoch 36/1000, LR 0.000285
Train loss: 0.3718;  Loss pred: 0.3718; Loss self: 0.0000; time: 0.18s
Val loss: 0.5568 score: 0.7045 time: 0.06s
Test loss: 0.5895 score: 0.6744 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.3414;  Loss pred: 0.3414; Loss self: 0.0000; time: 0.19s
Val loss: 0.5531 score: 0.7045 time: 0.06s
Test loss: 0.5864 score: 0.6744 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3656;  Loss pred: 0.3656; Loss self: 0.0000; time: 0.38s
Val loss: 0.5510 score: 0.7045 time: 0.06s
Test loss: 0.5849 score: 0.6744 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.3236;  Loss pred: 0.3236; Loss self: 0.0000; time: 0.17s
Val loss: 0.5498 score: 0.7045 time: 0.06s
Test loss: 0.5846 score: 0.6744 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.3545;  Loss pred: 0.3545; Loss self: 0.0000; time: 0.18s
Val loss: 0.5492 score: 0.7045 time: 0.06s
Test loss: 0.5847 score: 0.6744 time: 0.07s
Epoch 41/1000, LR 0.000284
Train loss: 0.3195;  Loss pred: 0.3195; Loss self: 0.0000; time: 0.21s
Val loss: 0.5480 score: 0.7045 time: 0.07s
Test loss: 0.5846 score: 0.6744 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3232;  Loss pred: 0.3232; Loss self: 0.0000; time: 0.19s
Val loss: 0.5491 score: 0.7045 time: 0.07s
Test loss: 0.5870 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000284
Train loss: 0.3253;  Loss pred: 0.3253; Loss self: 0.0000; time: 0.19s
Val loss: 0.5466 score: 0.7045 time: 0.07s
Test loss: 0.5852 score: 0.6512 time: 0.07s
Epoch 44/1000, LR 0.000284
Train loss: 0.3302;  Loss pred: 0.3302; Loss self: 0.0000; time: 0.18s
Val loss: 0.5458 score: 0.7045 time: 0.27s
Test loss: 0.5854 score: 0.6512 time: 0.07s
Epoch 45/1000, LR 0.000284
Train loss: 0.3272;  Loss pred: 0.3272; Loss self: 0.0000; time: 0.18s
Val loss: 0.5480 score: 0.7045 time: 0.07s
Test loss: 0.5901 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 46/1000, LR 0.000284
Train loss: 0.3271;  Loss pred: 0.3271; Loss self: 0.0000; time: 0.20s
Val loss: 0.5519 score: 0.7045 time: 0.07s
Test loss: 0.5966 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 47/1000, LR 0.000284
Train loss: 0.3053;  Loss pred: 0.3053; Loss self: 0.0000; time: 0.19s
Val loss: 0.5538 score: 0.7045 time: 0.07s
Test loss: 0.6014 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 48/1000, LR 0.000284
Train loss: 0.3192;  Loss pred: 0.3192; Loss self: 0.0000; time: 0.19s
Val loss: 0.5549 score: 0.7045 time: 0.07s
Test loss: 0.6051 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 49/1000, LR 0.000284
Train loss: 0.3005;  Loss pred: 0.3005; Loss self: 0.0000; time: 0.18s
Val loss: 0.5570 score: 0.7045 time: 0.07s
Test loss: 0.6102 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.2949;  Loss pred: 0.2949; Loss self: 0.0000; time: 0.32s
Val loss: 0.5595 score: 0.7045 time: 0.07s
Test loss: 0.6147 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.2880;  Loss pred: 0.2880; Loss self: 0.0000; time: 0.18s
Val loss: 0.5626 score: 0.6818 time: 0.07s
Test loss: 0.6195 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.2783;  Loss pred: 0.2783; Loss self: 0.0000; time: 0.17s
Val loss: 0.5665 score: 0.6818 time: 0.25s
Test loss: 0.6248 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.2896;  Loss pred: 0.2896; Loss self: 0.0000; time: 0.19s
Val loss: 0.5674 score: 0.6818 time: 0.07s
Test loss: 0.6249 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.2774;  Loss pred: 0.2774; Loss self: 0.0000; time: 0.19s
Val loss: 0.5697 score: 0.6818 time: 0.07s
Test loss: 0.6268 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.2806;  Loss pred: 0.2806; Loss self: 0.0000; time: 0.18s
Val loss: 0.5729 score: 0.6818 time: 0.07s
Test loss: 0.6290 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.2817;  Loss pred: 0.2817; Loss self: 0.0000; time: 0.17s
Val loss: 0.5769 score: 0.6818 time: 0.06s
Test loss: 0.6310 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.2686;  Loss pred: 0.2686; Loss self: 0.0000; time: 0.18s
Val loss: 0.5814 score: 0.6818 time: 0.06s
Test loss: 0.6347 score: 0.6279 time: 0.18s
     INFO: Early stopping counter 13 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.2632;  Loss pred: 0.2632; Loss self: 0.0000; time: 0.17s
Val loss: 0.5887 score: 0.6818 time: 0.06s
Test loss: 0.6417 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.2603;  Loss pred: 0.2603; Loss self: 0.0000; time: 0.16s
Val loss: 0.5929 score: 0.6818 time: 0.06s
Test loss: 0.6455 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.2696;  Loss pred: 0.2696; Loss self: 0.0000; time: 0.17s
Val loss: 0.5968 score: 0.6818 time: 0.07s
Test loss: 0.6494 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.2526;  Loss pred: 0.2526; Loss self: 0.0000; time: 0.17s
Val loss: 0.5989 score: 0.6818 time: 0.27s
Test loss: 0.6526 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.2562;  Loss pred: 0.2562; Loss self: 0.0000; time: 0.18s
Val loss: 0.6041 score: 0.6818 time: 0.06s
Test loss: 0.6592 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.2448;  Loss pred: 0.2448; Loss self: 0.0000; time: 0.31s
Val loss: 0.6069 score: 0.6818 time: 0.07s
Test loss: 0.6631 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.2482;  Loss pred: 0.2482; Loss self: 0.0000; time: 0.18s
Val loss: 0.6072 score: 0.6818 time: 0.07s
Test loss: 0.6639 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 043,   Train_Loss: 0.3302,   Val_Loss: 0.5458,   Val_Precision: 1.0000,   Val_Recall: 0.4091,   Val_accuracy: 0.5806,   Val_Score: 0.7045,   Val_Loss: 0.5458,   Test_Precision: 1.0000,   Test_Recall: 0.3182,   Test_accuracy: 0.4828,   Test_Score: 0.6512,   Test_loss: 0.5854


[0.07191935099945113, 0.06566255500001716, 0.06481734499993763, 0.0645866439999736, 0.0654895409998062, 0.06699546399977407, 0.07489186499969946, 0.06789828700038925, 0.065364120000595, 0.06464336099998036, 0.06451441399985924, 0.05992405399956624, 0.06385051400047814, 0.06252894699991884, 0.06415656399985892, 0.06309215600049356, 0.0617179880000549, 0.0641192979992411, 0.1894068010005867, 0.07646090099933645, 0.2771502909999981, 0.06651674099975935, 0.06608974400023726, 0.06725618300060887, 0.06670381500043732, 0.06598288900022453, 0.06562872199992853, 0.06523706200005108, 0.06973811700027, 0.06990561699967657, 0.06867506599974149, 0.06882964399937919, 0.062270883000564936, 0.06334112999957142, 0.06483782600025734, 0.28924775599989516, 0.06909718600036285, 0.06811032900077407, 0.06529857799978345, 0.07043848400007846, 0.06744881400027225, 0.06771488899994438, 0.06791518099998939, 0.08430060699993192, 0.06500819899974886, 0.06531481099955272, 0.06504587800009176, 0.06363612099994498, 0.06740145299954747, 0.06567490599991288, 0.0671864790001564, 0.0656816250002521, 0.06673767299980682, 0.06430500599981315, 0.06567406500016659, 0.06758690900005604, 0.06828165099977923, 0.06732209099936881, 0.20218850400033261, 0.06953205599984358, 0.07159061499987729, 0.06690466399959405, 0.0673232930002996, 0.06922149599995464, 0.06753499700062093, 0.06784073900053045, 0.06801344900031836, 0.06668652699954691, 0.06984229599947867, 0.06936718699944322, 0.06119231799948466, 0.0631043059993317, 0.0639366900004461, 0.06529062899971905, 0.06836912100061454, 0.06744043300022895, 0.07002063199979602, 0.07175901200025692, 0.06500699499974871, 0.19943561499985663, 0.06787225999960356, 0.06755850900026417, 0.07186840099984693, 0.06693687000006321, 0.065379216999645, 0.071644446999926, 0.0654276570003276, 0.06368186799954856, 0.06381112999952165, 0.059397782999440096, 0.07724504999987403, 0.06215368599987414, 0.07222140700014279, 0.20523750899974402, 0.06538299700059724, 0.06806568899992271, 0.06354714300050546, 0.06045466100022168, 0.06061913000030472, 0.0611227310000686, 0.061042645999805245, 0.06966101199941477, 0.06683107800017751, 0.06522892599969055, 0.06565365999995265, 0.0660666260000653, 0.07000669399985782, 0.0637244469999132, 0.06090805100029684, 0.29635152199989534, 0.06483579199993983, 0.0650263610004913, 0.065247575000285, 0.06462665199978801, 0.06737577100011549, 0.06559359999937442, 0.3106766989994867, 0.06436933699933434, 0.06667949300026521, 0.07503842599999189, 0.06862229299986211, 0.06836183600080403, 0.08654296100030479, 0.06978251800046564, 0.06794946399986657, 0.07251243399969098, 0.06189062199973705, 0.06257518499933212, 0.06919449999986682, 0.07385849000002054, 0.07149127899992891, 0.07576360700022633, 0.06811446899973816, 0.07178769600068335, 0.07157901599930483, 0.06631399700017937, 0.06684833400049683, 0.06988835900028789, 0.0667835650001507, 0.06803682200006733, 0.06766274599976896, 0.06950735800000984, 0.06831102399974043, 0.07035781400009, 0.06749960300021485, 0.06739035899954615, 0.06747527300012734, 0.07041527799992764, 0.07261062900033721, 0.07162366800002928, 0.07317872700059525, 0.08491048299947579, 0.07283579499926418, 0.06259298799977842, 0.06357226299951435, 0.06655724699976417, 0.062344345999918005, 0.06443906200001948, 0.07508069100003922, 0.067774639000163, 0.07344398300028843, 0.07457832199997938, 0.07258729500063055, 0.06712794800023403, 0.06815647900020849, 0.07629938399986713, 0.07244966500002192, 0.06759041499935847, 0.06710172100065392, 0.06806636300007085, 0.07032599700050923, 0.0717779819997304, 0.07357447300000786, 0.07481395699960558, 0.06551039999976638, 0.18484424800044508, 0.06748363499991683, 0.06329096899935394, 0.070221746000243, 0.06305758300004527, 0.07337050600017392, 0.07338463799987949, 0.07119937399966147]
[0.00163453070453298, 0.0014923307954549355, 0.00147312147727131, 0.0014678782727266728, 0.0014883986590865045, 0.0015226241818130472, 0.0017020878409022605, 0.001543142886372483, 0.0014855481818317044, 0.001469167295454099, 0.0014662366818149828, 0.00136191031817196, 0.0014511480454654122, 0.0014211124318163374, 0.001458103727269521, 0.0014339126363748537, 0.0014026815454557932, 0.001457256772710025, 0.004304700022740607, 0.0017377477499849192, 0.006298870249999957, 0.0015117441136308944, 0.0015020396363690286, 0.0015285496136502015, 0.0015159957954644844, 0.0014996111136414665, 0.0014915618636347392, 0.001482660500001161, 0.0015849572045515908, 0.001588764022719922, 0.0015607969545395793, 0.0015643100908949816, 0.0014152473409219303, 0.001439571136353896, 0.001473586954551303, 0.006573812636361254, 0.0015703905909173375, 0.0015479620227448652, 0.0014840585909041693, 0.0016008746363654195, 0.0015329275909152784, 0.0015389747499987359, 0.0015435268409088496, 0.001915922886362089, 0.0014774590681761104, 0.0014844275227171072, 0.0014783154090929945, 0.0014462754772714768, 0.00153185120453517, 0.00149261149999802, 0.0015269654318217363, 0.001492764204551184, 0.001516765295450155, 0.0014614774090866624, 0.0014925923863674225, 0.0015360661136376373, 0.001551855704540437, 0.0015300475227129275, 0.004595193272734832, 0.001580273999996445, 0.0016270594318153928, 0.0015205605454453193, 0.0015300748409159, 0.0015732158181807872, 0.0015348862954686576, 0.001541834977284783, 0.00154576020455269, 0.001515602886353339, 0.0015873249090790605, 0.0015765269772600732, 0.0013907344999882878, 0.001434188772712084, 0.0014531065909192296, 0.0014838779318117965, 0.001553843659104876, 0.001532737113641567, 0.0015913779999953642, 0.0016308866363694754, 0.0014774317045397436, 0.004532627613633105, 0.0015425513636273536, 0.001535420659096913, 0.001633372749996521, 0.0015212925000014366, 0.0014858912954464772, 0.0016282828863619545, 0.0014869922045529, 0.0014473151818079218, 0.001450252954534583, 0.0013499496136236385, 0.0017555693181789552, 0.0014125837727244123, 0.0016413956136396089, 0.004664488840903273, 0.0014859772045590282, 0.0015469474772709707, 0.0014442532500114876, 0.0013739695681868563, 0.0013777075000069256, 0.0013891529772742863, 0.0013873328636319375, 0.0015832048181685175, 0.0015188881363676708, 0.0014824755909020578, 0.0014921286363625602, 0.0015015142272742114, 0.001591061227269496, 0.0014482828863616635, 0.0013842738863703826, 0.006735261863633985, 0.0014735407272713599, 0.001477871840920257, 0.001482899431824659, 0.0014687875454497275, 0.0015312675227298976, 0.0014907636363494187, 0.007060834068170152, 0.0014629394772575986, 0.0015154430227333003, 0.001745079674418416, 0.0015958672790665608, 0.0015898101395535821, 0.002012627000007088, 0.0016228492558247824, 0.0015802200930201528, 0.0016863356744114182, 0.0014393167906915593, 0.0014552368604495844, 0.001609174418601554, 0.0017176393023260591, 0.0016625878837192769, 0.0017619443488424728, 0.0015840574185985619, 0.0016694813023414733, 0.0016646282790536008, 0.0015421859767483575, 0.0015546124186162054, 0.0016253106744252997, 0.0015531061627942024, 0.0015822516744201705, 0.0015735522325527665, 0.0016164501860467405, 0.0015886284651102426, 0.0016362282325602326, 0.001569758209307322, 0.0015672176511522362, 0.0015691923953517987, 0.00163756460464948, 0.0016886192790776097, 0.0016656666976750997, 0.0017018308604789592, 0.001974662395336646, 0.0016938556976573065, 0.0014556508837157772, 0.0014784247209189384, 0.0015478429534828877, 0.001449868511626, 0.0014985828372097553, 0.001746062581396261, 0.001576154395352628, 0.0017079996046578704, 0.0017343795813948694, 0.0016880766279216406, 0.0015611150697728844, 0.001585034395353686, 0.0017744042790666776, 0.001684875930233068, 0.0015718701162641506, 0.0015605051395500913, 0.0015829386744202524, 0.001635488302337424, 0.0016692553953425674, 0.001711034255814136, 0.0017398594651071065, 0.0015234976744131715, 0.004298703441870816, 0.0015693868604631821, 0.0014718829999849753, 0.0016330638604707675, 0.001466455418605704, 0.0017062908372133469, 0.0017066194883692905, 0.0016557993953409645]
[611.7963995578297, 670.0927187494989, 678.8306432489998, 681.2554001105551, 671.8630078676259, 656.7608816045871, 587.5137439850986, 648.0281306617906, 673.1521819554746, 680.657678056272, 682.0181300894401, 734.2627386377847, 689.1095661292642, 703.6740919378846, 685.8222644232749, 697.3925570027404, 712.9201943517805, 686.2208628753355, 232.3042243866613, 575.4575139048106, 158.7586281841584, 661.487609565225, 665.7613925670834, 654.2149440684385, 659.6324363113494, 666.8395498695165, 670.4381657782078, 674.4632368632043, 630.931861837188, 629.4200936700634, 640.6983285631735, 639.259444671788, 706.5902694779649, 694.6513268755659, 678.6162139339059, 152.11872551231113, 636.7842534103913, 646.010680692791, 673.8278435427172, 624.658531832556, 652.3465334738488, 649.7832404338156, 647.8669327260849, 521.9416747501656, 676.8377016592935, 673.660373912761, 676.4456311887731, 691.4312077575886, 652.80491802299, 669.9666993060997, 654.8936728757221, 669.898164057773, 659.2977852273538, 684.239108851461, 669.975278671853, 651.0136452602607, 644.3898083270169, 653.5744708287883, 217.61870298979815, 632.8016533855836, 614.6056993654184, 657.6522079277908, 653.5628018047809, 635.640697508601, 651.5140586975291, 648.5778405163888, 646.9308739186869, 659.8034412603156, 629.9907437225207, 634.3056696295494, 719.0445049061641, 697.2582821918045, 688.1807613076779, 673.9098806995617, 643.565389696973, 652.4276022938735, 628.3862162245005, 613.1634030836777, 676.8502374270658, 220.6225803752837, 648.2766302500758, 651.2873160037909, 612.2301232233303, 657.3357851951914, 672.9967414604998, 614.1438986896702, 672.4984817930999, 690.9345058834002, 689.534882086085, 740.7683886183893, 569.6157876792332, 707.9226162079767, 609.2376461166564, 214.38576317964802, 672.9578333583895, 646.4343584335121, 692.3993420073979, 727.8181578065364, 725.8434754800807, 719.8631226073753, 720.8075482203111, 631.6302151965529, 658.3763320394612, 674.5473626257275, 670.183505383123, 665.9943554549995, 628.5113249325752, 690.4728416091227, 722.4003933369263, 148.47232672560912, 678.6375031871413, 676.6486594516269, 674.3545641321966, 680.8336597746752, 653.0537513244127, 670.7971509479528, 141.62632776033422, 683.5552772658668, 659.8730437231278, 573.0397383335926, 626.6185246839012, 629.0059266327232, 496.86305509986613, 616.2001778111972, 632.8232405201082, 593.0017464340426, 694.7740806383024, 687.17335794467, 621.436674881425, 582.1944098774297, 601.4719641544357, 567.5548155973031, 631.2902476001875, 598.9884394616965, 600.7347181248987, 648.4302250682264, 643.2471450923581, 615.2669860201306, 643.8709883173049, 632.0107073777997, 635.5048020094666, 618.6395402914596, 629.4738020639743, 611.1616827655417, 637.0407837785822, 638.0734668632584, 637.2704857365875, 610.662930281184, 592.1998003873551, 600.3602049532344, 587.6024599286926, 506.4156801494754, 590.3690623605386, 686.9779087739385, 676.3956161247318, 646.060375666565, 689.7177171456182, 667.2971124251778, 572.717158396658, 634.4556110420091, 585.4802291949652, 576.5750535391758, 592.3901696519545, 640.5677706675928, 630.9011356039748, 563.5694254107593, 593.5155117692675, 636.1848791786251, 640.8181393675574, 631.7364128880405, 611.4381855075391, 599.069502959299, 584.4418348738346, 574.7590653469472, 656.3843298186753, 232.6282828118972, 637.1915205820344, 679.4018274619707, 612.3459248628081, 681.916400125408, 586.0665592233761, 585.9536978307451, 603.9378941759298]
Elapsed: 0.07620273880325601~0.038918328891133766
Time per graph: 0.0017451353544744668~0.0008842379555942083
Speed: 624.2076758790689~107.0437204736548
Total Time: 0.0720
best val loss: 0.5457824129949916 test_score: 0.6512

Testing...
Test loss: 0.6117 score: 0.6279 time: 0.07s
test Score 0.6279
Epoch Time List: [1.0520186630010357, 0.2974265689999811, 0.5067151310004192, 0.2826955199989243, 0.28970633700009785, 0.2876284209987716, 0.2977158160001636, 0.2991115760014509, 0.28664642899911996, 0.28319891099909, 0.3958080930005963, 0.49973489399963, 0.27024119599991536, 0.2803650949990697, 0.26860879299965745, 0.2680863119994683, 0.2715254859995184, 0.2731704860007085, 0.3968236210012037, 0.318864922001012, 0.5113738180007203, 0.2907970429996567, 0.2939924439997412, 0.2894198750000214, 0.28993831899970246, 0.44691500200133305, 0.27910435699959635, 0.2846669580003436, 0.44561562099988805, 0.3026070769992657, 0.3151263689996995, 0.31315806600105134, 0.4358365740008594, 0.27664776100118615, 0.2789770089993908, 0.5086380420007117, 0.30138823300057993, 0.3040189500006818, 0.42312929299987445, 0.30363877700074227, 0.30681966400061356, 0.29851695699926495, 0.2988277999993443, 0.5251918239991937, 0.42877742200107605, 0.29507682799976465, 0.29226279800150223, 0.2900989190002292, 0.5045724860010523, 0.2917673400006606, 0.30351892299950123, 0.298129550998965, 0.4218481650004833, 0.28249260199936543, 0.28484595800000534, 0.2968283150003117, 0.5056092730001183, 0.2911291150012403, 0.4304211550006585, 0.30792823400133784, 0.3033947919984712, 0.2901176799996392, 0.2955366109999886, 0.5054509479987246, 0.29627474200060533, 0.2917242760013323, 0.4169394500004273, 0.2918066279989944, 0.3022027649994925, 0.3063403930000277, 0.49125498800094647, 0.2726601660015149, 0.27735730200129183, 0.27387424600146915, 0.4163954380001087, 0.3012869739995949, 0.3015169869986494, 0.3050076950003131, 0.4972433180000735, 0.4362038469998879, 0.2693942580008297, 0.2799999239996396, 0.28418706899992685, 0.29868690700004663, 0.2894643720001113, 0.2919398900003216, 0.6124269189995175, 0.2851433319992793, 0.27903715199954604, 0.254870354000559, 0.28602403000058985, 0.31237043700002687, 0.27823058600006334, 0.645783076999578, 0.2834840109999277, 0.3020887330003461, 0.2926792590005789, 0.2681505880009354, 0.26286586000060197, 0.2634253930000341, 0.27866222800093965, 0.6421565639993787, 0.2927732329999344, 0.2878333999997267, 0.28381621399967116, 0.289090130000659, 0.3048068099988086, 0.3120301079998171, 0.2682489690014336, 0.6453079869997964, 0.28434719300003053, 0.2866854939993573, 0.28682344499975443, 0.29318224000053306, 0.3013063840007817, 0.42220615400128736, 0.5308118800003285, 0.3246516600001996, 0.28324070099915843, 0.3309496570000192, 0.3315559079992454, 0.34074211299957824, 0.37318763400071475, 0.32173636199968314, 0.3147598369996558, 0.31527681199986546, 0.4275812539999606, 0.2900637230013672, 0.31761635200018645, 0.3187234239994723, 0.3072149209992858, 0.31387469500077714, 0.30793551500028116, 0.3176293570004418, 0.5296896110003217, 0.299783462999585, 0.3079042790004678, 0.3104298709995419, 0.29877699700045923, 0.30000869300056365, 0.303672602000006, 0.529081784000482, 0.30485452199900465, 0.3083071849996486, 0.30605311000090296, 0.30830467300074815, 0.30721929500123224, 0.31297511299908365, 0.46326517699981196, 0.3112107470005867, 0.33849732399994537, 0.35182817399891064, 0.3437438300006761, 0.3209373999998206, 0.29915073000029224, 0.31062015399947995, 0.4959637930005556, 0.28459729200039874, 0.3036895300001561, 0.3370862869996927, 0.33258963000116637, 0.329024097999536, 0.5152158929995494, 0.3053366799986179, 0.3314196119999906, 0.32867643299960037, 0.32653196699902765, 0.31209146099899954, 0.4454127810004138, 0.30962212499980524, 0.4920858990008128, 0.3289643719999731, 0.33124257100007526, 0.31690738099950977, 0.2935282290000032, 0.4241746779998721, 0.29028115299934143, 0.28371846200025175, 0.30239426200023445, 0.5005521000002773, 0.31654846999936126, 0.4466316310008551, 0.3177860609994241]
Total Epoch List: [119, 64]
Total Time List: [0.06739704700066795, 0.07204406500022742]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f4d7190>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7432;  Loss pred: 0.7432; Loss self: 0.0000; time: 0.21s
Val loss: 0.6857 score: 0.6136 time: 0.20s
Test loss: 0.6939 score: 0.4419 time: 0.06s
Epoch 2/1000, LR 0.000015
Train loss: 0.7147;  Loss pred: 0.7147; Loss self: 0.0000; time: 0.18s
Val loss: 0.6848 score: 0.5682 time: 0.06s
Test loss: 0.6927 score: 0.4186 time: 0.05s
Epoch 3/1000, LR 0.000045
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.17s
Val loss: 0.6836 score: 0.5455 time: 0.06s
Test loss: 0.6910 score: 0.4884 time: 0.05s
Epoch 4/1000, LR 0.000075
Train loss: 0.7020;  Loss pred: 0.7020; Loss self: 0.0000; time: 0.18s
Val loss: 0.6821 score: 0.5227 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000105
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.5116 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6849 score: 0.5116 time: 0.05s
Epoch 7/1000, LR 0.000165
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6803 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6842 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 8/1000, LR 0.000195
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6830 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.6241;  Loss pred: 0.6241; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.6188;  Loss pred: 0.6188; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6867 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.5116 time: 0.28s
     INFO: Early stopping counter 7 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5930;  Loss pred: 0.5930; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6856 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5726;  Loss pred: 0.5726; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6834 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5731;  Loss pred: 0.5731; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6804 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5453;  Loss pred: 0.5453; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6740 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5486;  Loss pred: 0.5486; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6766 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6673 score: 0.5116 time: 0.06s
Epoch 19/1000, LR 0.000285
Train loss: 0.5224;  Loss pred: 0.5224; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6693 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6599 score: 0.5116 time: 0.06s
Epoch 20/1000, LR 0.000285
Train loss: 0.5222;  Loss pred: 0.5222; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6608 score: 0.5000 time: 0.07s
Test loss: 0.6515 score: 0.5349 time: 0.08s
Epoch 21/1000, LR 0.000285
Train loss: 0.5116;  Loss pred: 0.5116; Loss self: 0.0000; time: 0.52s
Val loss: 0.6521 score: 0.5455 time: 0.07s
Test loss: 0.6430 score: 0.5581 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.5306;  Loss pred: 0.5306; Loss self: 0.0000; time: 0.19s
Val loss: 0.6418 score: 0.5682 time: 0.06s
Test loss: 0.6331 score: 0.5581 time: 0.05s
Epoch 23/1000, LR 0.000285
Train loss: 0.5112;  Loss pred: 0.5112; Loss self: 0.0000; time: 0.17s
Val loss: 0.6318 score: 0.5682 time: 0.06s
Test loss: 0.6234 score: 0.5814 time: 0.06s
Epoch 24/1000, LR 0.000285
Train loss: 0.4865;  Loss pred: 0.4865; Loss self: 0.0000; time: 0.18s
Val loss: 0.6221 score: 0.5909 time: 0.06s
Test loss: 0.6141 score: 0.5814 time: 0.05s
Epoch 25/1000, LR 0.000285
Train loss: 0.4735;  Loss pred: 0.4735; Loss self: 0.0000; time: 0.18s
Val loss: 0.6128 score: 0.5909 time: 0.06s
Test loss: 0.6054 score: 0.6047 time: 0.05s
Epoch 26/1000, LR 0.000285
Train loss: 0.4662;  Loss pred: 0.4662; Loss self: 0.0000; time: 0.18s
Val loss: 0.6027 score: 0.6364 time: 0.19s
Test loss: 0.5961 score: 0.6744 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.4401;  Loss pred: 0.4401; Loss self: 0.0000; time: 0.19s
Val loss: 0.5929 score: 0.6818 time: 0.06s
Test loss: 0.5872 score: 0.6977 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.4451;  Loss pred: 0.4451; Loss self: 0.0000; time: 0.19s
Val loss: 0.5849 score: 0.6818 time: 0.06s
Test loss: 0.5798 score: 0.7209 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.4345;  Loss pred: 0.4345; Loss self: 0.0000; time: 0.34s
Val loss: 0.5769 score: 0.6818 time: 0.06s
Test loss: 0.5723 score: 0.7209 time: 0.05s
Epoch 30/1000, LR 0.000285
Train loss: 0.4260;  Loss pred: 0.4260; Loss self: 0.0000; time: 0.17s
Val loss: 0.5690 score: 0.6818 time: 0.06s
Test loss: 0.5651 score: 0.7442 time: 0.05s
Epoch 31/1000, LR 0.000285
Train loss: 0.4373;  Loss pred: 0.4373; Loss self: 0.0000; time: 0.17s
Val loss: 0.5620 score: 0.6818 time: 0.05s
Test loss: 0.5586 score: 0.7674 time: 0.06s
Epoch 32/1000, LR 0.000285
Train loss: 0.4302;  Loss pred: 0.4302; Loss self: 0.0000; time: 0.18s
Val loss: 0.5550 score: 0.7273 time: 0.06s
Test loss: 0.5520 score: 0.7674 time: 0.05s
Epoch 33/1000, LR 0.000285
Train loss: 0.4155;  Loss pred: 0.4155; Loss self: 0.0000; time: 0.18s
Val loss: 0.5482 score: 0.7273 time: 0.06s
Test loss: 0.5455 score: 0.7907 time: 0.05s
Epoch 34/1000, LR 0.000285
Train loss: 0.4092;  Loss pred: 0.4092; Loss self: 0.0000; time: 0.17s
Val loss: 0.5420 score: 0.7273 time: 0.06s
Test loss: 0.5396 score: 0.8140 time: 0.05s
Epoch 35/1000, LR 0.000285
Train loss: 0.3996;  Loss pred: 0.3996; Loss self: 0.0000; time: 0.18s
Val loss: 0.5351 score: 0.7273 time: 0.06s
Test loss: 0.5333 score: 0.8140 time: 0.17s
Epoch 36/1000, LR 0.000285
Train loss: 0.3826;  Loss pred: 0.3826; Loss self: 0.0000; time: 0.19s
Val loss: 0.5296 score: 0.7273 time: 0.06s
Test loss: 0.5279 score: 0.8140 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.3853;  Loss pred: 0.3853; Loss self: 0.0000; time: 0.40s
Val loss: 0.5251 score: 0.7273 time: 0.06s
Test loss: 0.5232 score: 0.8140 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3551;  Loss pred: 0.3551; Loss self: 0.0000; time: 0.19s
Val loss: 0.5208 score: 0.7273 time: 0.06s
Test loss: 0.5187 score: 0.8605 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.3719;  Loss pred: 0.3719; Loss self: 0.0000; time: 0.19s
Val loss: 0.5162 score: 0.7273 time: 0.06s
Test loss: 0.5133 score: 0.8605 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.3667;  Loss pred: 0.3667; Loss self: 0.0000; time: 0.18s
Val loss: 0.5118 score: 0.7273 time: 0.07s
Test loss: 0.5080 score: 0.8605 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3514;  Loss pred: 0.3514; Loss self: 0.0000; time: 0.18s
Val loss: 0.5085 score: 0.7273 time: 0.06s
Test loss: 0.5032 score: 0.8605 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3719;  Loss pred: 0.3719; Loss self: 0.0000; time: 0.18s
Val loss: 0.5047 score: 0.7273 time: 0.06s
Test loss: 0.4983 score: 0.8605 time: 0.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.3494;  Loss pred: 0.3494; Loss self: 0.0000; time: 0.34s
Val loss: 0.5006 score: 0.7273 time: 0.07s
Test loss: 0.4931 score: 0.8605 time: 0.06s
Epoch 44/1000, LR 0.000284
Train loss: 0.3436;  Loss pred: 0.3436; Loss self: 0.0000; time: 0.19s
Val loss: 0.4954 score: 0.7273 time: 0.07s
Test loss: 0.4874 score: 0.8605 time: 0.29s
Epoch 45/1000, LR 0.000284
Train loss: 0.3392;  Loss pred: 0.3392; Loss self: 0.0000; time: 0.18s
Val loss: 0.4892 score: 0.7273 time: 0.07s
Test loss: 0.4811 score: 0.8605 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.3536;  Loss pred: 0.3536; Loss self: 0.0000; time: 0.19s
Val loss: 0.4821 score: 0.7500 time: 0.06s
Test loss: 0.4746 score: 0.8372 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3344;  Loss pred: 0.3344; Loss self: 0.0000; time: 0.19s
Val loss: 0.4754 score: 0.7727 time: 0.06s
Test loss: 0.4686 score: 0.8372 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3227;  Loss pred: 0.3227; Loss self: 0.0000; time: 0.19s
Val loss: 0.4682 score: 0.7955 time: 0.20s
Test loss: 0.4620 score: 0.8372 time: 0.05s
Epoch 49/1000, LR 0.000284
Train loss: 0.3161;  Loss pred: 0.3161; Loss self: 0.0000; time: 0.19s
Val loss: 0.4618 score: 0.8409 time: 0.06s
Test loss: 0.4554 score: 0.8605 time: 0.06s
Epoch 50/1000, LR 0.000284
Train loss: 0.2983;  Loss pred: 0.2983; Loss self: 0.0000; time: 0.19s
Val loss: 0.4577 score: 0.8409 time: 0.06s
Test loss: 0.4503 score: 0.8605 time: 0.06s
Epoch 51/1000, LR 0.000284
Train loss: 0.3148;  Loss pred: 0.3148; Loss self: 0.0000; time: 0.18s
Val loss: 0.4524 score: 0.8409 time: 0.06s
Test loss: 0.4450 score: 0.8837 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.3141;  Loss pred: 0.3141; Loss self: 0.0000; time: 0.40s
Val loss: 0.4486 score: 0.8409 time: 0.06s
Test loss: 0.4407 score: 0.8605 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.2934;  Loss pred: 0.2934; Loss self: 0.0000; time: 0.17s
Val loss: 0.4450 score: 0.8409 time: 0.06s
Test loss: 0.4368 score: 0.8605 time: 0.05s
Epoch 54/1000, LR 0.000284
Train loss: 0.2988;  Loss pred: 0.2988; Loss self: 0.0000; time: 0.18s
Val loss: 0.4431 score: 0.8409 time: 0.06s
Test loss: 0.4334 score: 0.8837 time: 0.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.2978;  Loss pred: 0.2978; Loss self: 0.0000; time: 0.31s
Val loss: 0.4424 score: 0.8409 time: 0.07s
Test loss: 0.4308 score: 0.9070 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.2896;  Loss pred: 0.2896; Loss self: 0.0000; time: 0.18s
Val loss: 0.4401 score: 0.8409 time: 0.07s
Test loss: 0.4278 score: 0.9070 time: 0.06s
Epoch 57/1000, LR 0.000283
Train loss: 0.2814;  Loss pred: 0.2814; Loss self: 0.0000; time: 0.18s
Val loss: 0.4382 score: 0.8409 time: 0.06s
Test loss: 0.4255 score: 0.9070 time: 0.06s
Epoch 58/1000, LR 0.000283
Train loss: 0.2816;  Loss pred: 0.2816; Loss self: 0.0000; time: 0.18s
Val loss: 0.4373 score: 0.8409 time: 0.06s
Test loss: 0.4235 score: 0.9070 time: 0.06s
Epoch 59/1000, LR 0.000283
Train loss: 0.2820;  Loss pred: 0.2820; Loss self: 0.0000; time: 0.19s
Val loss: 0.4355 score: 0.8409 time: 0.07s
Test loss: 0.4221 score: 0.9070 time: 0.06s
Epoch 60/1000, LR 0.000283
Train loss: 0.2724;  Loss pred: 0.2724; Loss self: 0.0000; time: 0.19s
Val loss: 0.4346 score: 0.8409 time: 0.07s
Test loss: 0.4202 score: 0.9070 time: 0.26s
Epoch 61/1000, LR 0.000283
Train loss: 0.2688;  Loss pred: 0.2688; Loss self: 0.0000; time: 0.19s
Val loss: 0.4328 score: 0.8409 time: 0.06s
Test loss: 0.4185 score: 0.9070 time: 0.06s
Epoch 62/1000, LR 0.000283
Train loss: 0.2667;  Loss pred: 0.2667; Loss self: 0.0000; time: 0.29s
Val loss: 0.4305 score: 0.8409 time: 0.06s
Test loss: 0.4170 score: 0.9070 time: 0.05s
Epoch 63/1000, LR 0.000283
Train loss: 0.2576;  Loss pred: 0.2576; Loss self: 0.0000; time: 0.17s
Val loss: 0.4278 score: 0.8409 time: 0.06s
Test loss: 0.4152 score: 0.9070 time: 0.05s
Epoch 64/1000, LR 0.000283
Train loss: 0.2768;  Loss pred: 0.2768; Loss self: 0.0000; time: 0.17s
Val loss: 0.4257 score: 0.8409 time: 0.06s
Test loss: 0.4133 score: 0.8837 time: 0.05s
Epoch 65/1000, LR 0.000283
Train loss: 0.2673;  Loss pred: 0.2673; Loss self: 0.0000; time: 0.18s
Val loss: 0.4228 score: 0.8409 time: 0.06s
Test loss: 0.4110 score: 0.8837 time: 0.05s
Epoch 66/1000, LR 0.000283
Train loss: 0.2586;  Loss pred: 0.2586; Loss self: 0.0000; time: 0.18s
Val loss: 0.4205 score: 0.8409 time: 0.06s
Test loss: 0.4093 score: 0.8837 time: 0.05s
Epoch 67/1000, LR 0.000283
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 0.17s
Val loss: 0.4182 score: 0.8409 time: 0.06s
Test loss: 0.4073 score: 0.8837 time: 0.05s
Epoch 68/1000, LR 0.000283
Train loss: 0.2527;  Loss pred: 0.2527; Loss self: 0.0000; time: 0.17s
Val loss: 0.4150 score: 0.8409 time: 0.41s
Test loss: 0.4055 score: 0.8837 time: 0.07s
Epoch 69/1000, LR 0.000283
Train loss: 0.2599;  Loss pred: 0.2599; Loss self: 0.0000; time: 0.18s
Val loss: 0.4143 score: 0.8636 time: 0.07s
Test loss: 0.4045 score: 0.8837 time: 0.06s
Epoch 70/1000, LR 0.000283
Train loss: 0.2421;  Loss pred: 0.2421; Loss self: 0.0000; time: 0.19s
Val loss: 0.4127 score: 0.8636 time: 0.06s
Test loss: 0.4027 score: 0.8837 time: 0.06s
Epoch 71/1000, LR 0.000282
Train loss: 0.2392;  Loss pred: 0.2392; Loss self: 0.0000; time: 0.18s
Val loss: 0.4109 score: 0.8636 time: 0.06s
Test loss: 0.4016 score: 0.8837 time: 0.06s
Epoch 72/1000, LR 0.000282
Train loss: 0.2446;  Loss pred: 0.2446; Loss self: 0.0000; time: 0.19s
Val loss: 0.4080 score: 0.8636 time: 0.06s
Test loss: 0.3996 score: 0.8837 time: 0.06s
Epoch 73/1000, LR 0.000282
Train loss: 0.2353;  Loss pred: 0.2353; Loss self: 0.0000; time: 0.19s
Val loss: 0.4065 score: 0.8636 time: 0.07s
Test loss: 0.3985 score: 0.8837 time: 0.06s
Epoch 74/1000, LR 0.000282
Train loss: 0.2215;  Loss pred: 0.2215; Loss self: 0.0000; time: 0.19s
Val loss: 0.4051 score: 0.8636 time: 0.07s
Test loss: 0.3974 score: 0.8837 time: 0.06s
Epoch 75/1000, LR 0.000282
Train loss: 0.2226;  Loss pred: 0.2226; Loss self: 0.0000; time: 0.20s
Val loss: 0.4031 score: 0.8636 time: 0.27s
Test loss: 0.3961 score: 0.8837 time: 0.06s
Epoch 76/1000, LR 0.000282
Train loss: 0.2157;  Loss pred: 0.2157; Loss self: 0.0000; time: 0.25s
Val loss: 0.4011 score: 0.8636 time: 0.07s
Test loss: 0.3947 score: 0.8837 time: 0.06s
Epoch 77/1000, LR 0.000282
Train loss: 0.2162;  Loss pred: 0.2162; Loss self: 0.0000; time: 0.19s
Val loss: 0.3988 score: 0.8636 time: 0.07s
Test loss: 0.3929 score: 0.8837 time: 0.06s
Epoch 78/1000, LR 0.000282
Train loss: 0.2239;  Loss pred: 0.2239; Loss self: 0.0000; time: 0.19s
Val loss: 0.3985 score: 0.8636 time: 0.06s
Test loss: 0.3916 score: 0.8837 time: 0.06s
Epoch 79/1000, LR 0.000282
Train loss: 0.2322;  Loss pred: 0.2322; Loss self: 0.0000; time: 0.20s
Val loss: 0.3994 score: 0.8636 time: 0.07s
Test loss: 0.3913 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.2199;  Loss pred: 0.2199; Loss self: 0.0000; time: 0.19s
Val loss: 0.4012 score: 0.8636 time: 0.07s
Test loss: 0.3906 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.2124;  Loss pred: 0.2124; Loss self: 0.0000; time: 0.19s
Val loss: 0.4001 score: 0.8636 time: 0.06s
Test loss: 0.3893 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.2067;  Loss pred: 0.2067; Loss self: 0.0000; time: 0.29s
Val loss: 0.3999 score: 0.8636 time: 0.05s
Test loss: 0.3882 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.19s
Val loss: 0.3959 score: 0.8636 time: 0.26s
Test loss: 0.3855 score: 0.8837 time: 0.06s
Epoch 84/1000, LR 0.000281
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.19s
Val loss: 0.3930 score: 0.8636 time: 0.07s
Test loss: 0.3841 score: 0.8837 time: 0.06s
Epoch 85/1000, LR 0.000281
Train loss: 0.2034;  Loss pred: 0.2034; Loss self: 0.0000; time: 0.19s
Val loss: 0.3904 score: 0.8409 time: 0.07s
Test loss: 0.3831 score: 0.8837 time: 0.06s
Epoch 86/1000, LR 0.000281
Train loss: 0.2007;  Loss pred: 0.2007; Loss self: 0.0000; time: 0.20s
Val loss: 0.3874 score: 0.8409 time: 0.07s
Test loss: 0.3814 score: 0.8605 time: 0.06s
Epoch 87/1000, LR 0.000281
Train loss: 0.2013;  Loss pred: 0.2013; Loss self: 0.0000; time: 0.19s
Val loss: 0.3841 score: 0.8409 time: 0.06s
Test loss: 0.3799 score: 0.8605 time: 0.06s
Epoch 88/1000, LR 0.000281
Train loss: 0.1896;  Loss pred: 0.1896; Loss self: 0.0000; time: 0.19s
Val loss: 0.3818 score: 0.8409 time: 0.06s
Test loss: 0.3789 score: 0.8605 time: 0.06s
Epoch 89/1000, LR 0.000281
Train loss: 0.1975;  Loss pred: 0.1975; Loss self: 0.0000; time: 0.19s
Val loss: 0.3797 score: 0.8409 time: 0.17s
Test loss: 0.3785 score: 0.8605 time: 0.05s
Epoch 90/1000, LR 0.000281
Train loss: 0.2060;  Loss pred: 0.2060; Loss self: 0.0000; time: 0.19s
Val loss: 0.3765 score: 0.8409 time: 0.06s
Test loss: 0.3776 score: 0.8605 time: 0.06s
Epoch 91/1000, LR 0.000280
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 0.40s
Val loss: 0.3749 score: 0.8409 time: 0.07s
Test loss: 0.3770 score: 0.8605 time: 0.06s
Epoch 92/1000, LR 0.000280
Train loss: 0.2009;  Loss pred: 0.2009; Loss self: 0.0000; time: 0.21s
Val loss: 0.3717 score: 0.8409 time: 0.07s
Test loss: 0.3752 score: 0.8605 time: 0.06s
Epoch 93/1000, LR 0.000280
Train loss: 0.1904;  Loss pred: 0.1904; Loss self: 0.0000; time: 0.21s
Val loss: 0.3690 score: 0.8409 time: 0.07s
Test loss: 0.3741 score: 0.8837 time: 0.06s
Epoch 94/1000, LR 0.000280
Train loss: 0.1904;  Loss pred: 0.1904; Loss self: 0.0000; time: 0.21s
Val loss: 0.3701 score: 0.8409 time: 0.07s
Test loss: 0.3744 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1891;  Loss pred: 0.1891; Loss self: 0.0000; time: 0.21s
Val loss: 0.3689 score: 0.8409 time: 0.07s
Test loss: 0.3737 score: 0.8837 time: 0.06s
Epoch 96/1000, LR 0.000280
Train loss: 0.1911;  Loss pred: 0.1911; Loss self: 0.0000; time: 0.35s
Val loss: 0.3679 score: 0.8409 time: 0.07s
Test loss: 0.3735 score: 0.8837 time: 0.06s
Epoch 97/1000, LR 0.000280
Train loss: 0.1920;  Loss pred: 0.1920; Loss self: 0.0000; time: 0.20s
Val loss: 0.3647 score: 0.8409 time: 0.07s
Test loss: 0.3719 score: 0.8837 time: 0.06s
Epoch 98/1000, LR 0.000280
Train loss: 0.1836;  Loss pred: 0.1836; Loss self: 0.0000; time: 0.20s
Val loss: 0.3637 score: 0.8409 time: 0.07s
Test loss: 0.3705 score: 0.8605 time: 0.06s
Epoch 99/1000, LR 0.000279
Train loss: 0.1816;  Loss pred: 0.1816; Loss self: 0.0000; time: 0.45s
Val loss: 0.3645 score: 0.8409 time: 0.07s
Test loss: 0.3696 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.1777;  Loss pred: 0.1777; Loss self: 0.0000; time: 0.20s
Val loss: 0.3642 score: 0.8409 time: 0.07s
Test loss: 0.3681 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1722;  Loss pred: 0.1722; Loss self: 0.0000; time: 0.20s
Val loss: 0.3643 score: 0.8409 time: 0.08s
Test loss: 0.3670 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1754;  Loss pred: 0.1754; Loss self: 0.0000; time: 0.21s
Val loss: 0.3645 score: 0.8409 time: 0.06s
Test loss: 0.3656 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1644;  Loss pred: 0.1644; Loss self: 0.0000; time: 0.20s
Val loss: 0.3676 score: 0.8409 time: 0.07s
Test loss: 0.3652 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1663;  Loss pred: 0.1663; Loss self: 0.0000; time: 0.20s
Val loss: 0.3701 score: 0.8409 time: 0.07s
Test loss: 0.3652 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.20s
Val loss: 0.3708 score: 0.8409 time: 0.07s
Test loss: 0.3639 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1655;  Loss pred: 0.1655; Loss self: 0.0000; time: 0.21s
Val loss: 0.3700 score: 0.8409 time: 0.07s
Test loss: 0.3621 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1632;  Loss pred: 0.1632; Loss self: 0.0000; time: 0.21s
Val loss: 0.3709 score: 0.8409 time: 0.07s
Test loss: 0.3607 score: 0.8837 time: 0.28s
     INFO: Early stopping counter 9 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1604;  Loss pred: 0.1604; Loss self: 0.0000; time: 0.20s
Val loss: 0.3699 score: 0.8409 time: 0.07s
Test loss: 0.3586 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1669;  Loss pred: 0.1669; Loss self: 0.0000; time: 0.20s
Val loss: 0.3694 score: 0.8409 time: 0.06s
Test loss: 0.3572 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1656;  Loss pred: 0.1656; Loss self: 0.0000; time: 0.21s
Val loss: 0.3684 score: 0.8409 time: 0.06s
Test loss: 0.3554 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1545;  Loss pred: 0.1545; Loss self: 0.0000; time: 0.20s
Val loss: 0.3675 score: 0.8409 time: 0.07s
Test loss: 0.3547 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1526;  Loss pred: 0.1526; Loss self: 0.0000; time: 0.20s
Val loss: 0.3668 score: 0.8409 time: 0.07s
Test loss: 0.3549 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1454;  Loss pred: 0.1454; Loss self: 0.0000; time: 0.20s
Val loss: 0.3670 score: 0.8409 time: 0.07s
Test loss: 0.3558 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1541;  Loss pred: 0.1541; Loss self: 0.0000; time: 0.21s
Val loss: 0.3669 score: 0.8409 time: 0.06s
Test loss: 0.3562 score: 0.8837 time: 0.27s
     INFO: Early stopping counter 16 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1454;  Loss pred: 0.1454; Loss self: 0.0000; time: 0.20s
Val loss: 0.3654 score: 0.8409 time: 0.07s
Test loss: 0.3559 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1458;  Loss pred: 0.1458; Loss self: 0.0000; time: 0.20s
Val loss: 0.3639 score: 0.8409 time: 0.07s
Test loss: 0.3554 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1502;  Loss pred: 0.1502; Loss self: 0.0000; time: 0.20s
Val loss: 0.3619 score: 0.8409 time: 0.07s
Test loss: 0.3534 score: 0.8837 time: 0.06s
Epoch 118/1000, LR 0.000277
Train loss: 0.1301;  Loss pred: 0.1301; Loss self: 0.0000; time: 0.21s
Val loss: 0.3600 score: 0.8409 time: 0.07s
Test loss: 0.3516 score: 0.8837 time: 0.06s
Epoch 119/1000, LR 0.000277
Train loss: 0.1378;  Loss pred: 0.1378; Loss self: 0.0000; time: 0.20s
Val loss: 0.3595 score: 0.8409 time: 0.07s
Test loss: 0.3489 score: 0.8837 time: 0.06s
Epoch 120/1000, LR 0.000277
Train loss: 0.1300;  Loss pred: 0.1300; Loss self: 0.0000; time: 0.19s
Val loss: 0.3556 score: 0.8409 time: 0.06s
Test loss: 0.3462 score: 0.8837 time: 0.05s
Epoch 121/1000, LR 0.000276
Train loss: 0.1385;  Loss pred: 0.1385; Loss self: 0.0000; time: 0.19s
Val loss: 0.3520 score: 0.8409 time: 0.06s
Test loss: 0.3437 score: 0.8837 time: 0.06s
Epoch 122/1000, LR 0.000276
Train loss: 0.1262;  Loss pred: 0.1262; Loss self: 0.0000; time: 0.18s
Val loss: 0.3501 score: 0.8636 time: 0.06s
Test loss: 0.3437 score: 0.8837 time: 0.06s
Epoch 123/1000, LR 0.000276
Train loss: 0.1268;  Loss pred: 0.1268; Loss self: 0.0000; time: 0.38s
Val loss: 0.3638 score: 0.8636 time: 0.06s
Test loss: 0.3496 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 124/1000, LR 0.000276
Train loss: 0.1335;  Loss pred: 0.1335; Loss self: 0.0000; time: 0.17s
Val loss: 0.3590 score: 0.8636 time: 0.06s
Test loss: 0.3489 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 125/1000, LR 0.000276
Train loss: 0.1200;  Loss pred: 0.1200; Loss self: 0.0000; time: 0.17s
Val loss: 0.3558 score: 0.8636 time: 0.06s
Test loss: 0.3426 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 126/1000, LR 0.000276
Train loss: 0.1074;  Loss pred: 0.1074; Loss self: 0.0000; time: 0.17s
Val loss: 0.3618 score: 0.8636 time: 0.06s
Test loss: 0.3442 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 127/1000, LR 0.000275
Train loss: 0.1125;  Loss pred: 0.1125; Loss self: 0.0000; time: 0.17s
Val loss: 0.3656 score: 0.8409 time: 0.06s
Test loss: 0.3466 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1123;  Loss pred: 0.1123; Loss self: 0.0000; time: 0.17s
Val loss: 0.3612 score: 0.8636 time: 0.06s
Test loss: 0.3518 score: 0.9070 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 129/1000, LR 0.000275
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.18s
Val loss: 0.3597 score: 0.8636 time: 0.06s
Test loss: 0.3519 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 130/1000, LR 0.000275
Train loss: 0.1030;  Loss pred: 0.1030; Loss self: 0.0000; time: 0.17s
Val loss: 0.3612 score: 0.8636 time: 0.27s
Test loss: 0.3505 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 131/1000, LR 0.000275
Train loss: 0.1024;  Loss pred: 0.1024; Loss self: 0.0000; time: 0.20s
Val loss: 0.3606 score: 0.8636 time: 0.07s
Test loss: 0.3479 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 132/1000, LR 0.000275
Train loss: 0.0940;  Loss pred: 0.0940; Loss self: 0.0000; time: 0.20s
Val loss: 0.3592 score: 0.8636 time: 0.07s
Test loss: 0.3430 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 133/1000, LR 0.000274
Train loss: 0.0937;  Loss pred: 0.0937; Loss self: 0.0000; time: 0.19s
Val loss: 0.3570 score: 0.8636 time: 0.07s
Test loss: 0.3416 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 134/1000, LR 0.000274
Train loss: 0.0951;  Loss pred: 0.0951; Loss self: 0.0000; time: 0.19s
Val loss: 0.3524 score: 0.8636 time: 0.06s
Test loss: 0.3374 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 135/1000, LR 0.000274
Train loss: 0.0917;  Loss pred: 0.0917; Loss self: 0.0000; time: 0.19s
Val loss: 0.3499 score: 0.8636 time: 0.07s
Test loss: 0.3355 score: 0.9070 time: 0.06s
Epoch 136/1000, LR 0.000274
Train loss: 0.0911;  Loss pred: 0.0911; Loss self: 0.0000; time: 0.20s
Val loss: 0.3480 score: 0.8636 time: 0.06s
Test loss: 0.3355 score: 0.9070 time: 0.06s
Epoch 137/1000, LR 0.000274
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.19s
Val loss: 0.3493 score: 0.8636 time: 0.07s
Test loss: 0.3399 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 138/1000, LR 0.000274
Train loss: 0.0825;  Loss pred: 0.0825; Loss self: 0.0000; time: 0.32s
Val loss: 0.3478 score: 0.8636 time: 0.06s
Test loss: 0.3416 score: 0.9070 time: 0.06s
Epoch 139/1000, LR 0.000273
Train loss: 0.0810;  Loss pred: 0.0810; Loss self: 0.0000; time: 0.19s
Val loss: 0.3512 score: 0.8636 time: 0.06s
Test loss: 0.3421 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0817;  Loss pred: 0.0817; Loss self: 0.0000; time: 0.19s
Val loss: 0.3498 score: 0.8636 time: 0.07s
Test loss: 0.3430 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.19s
Val loss: 0.3428 score: 0.8636 time: 0.07s
Test loss: 0.3424 score: 0.9070 time: 0.06s
Epoch 142/1000, LR 0.000273
Train loss: 0.0804;  Loss pred: 0.0804; Loss self: 0.0000; time: 0.19s
Val loss: 0.3434 score: 0.8636 time: 0.06s
Test loss: 0.3432 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 143/1000, LR 0.000273
Train loss: 0.0692;  Loss pred: 0.0692; Loss self: 0.0000; time: 0.19s
Val loss: 0.3450 score: 0.8636 time: 0.07s
Test loss: 0.3453 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 144/1000, LR 0.000272
Train loss: 0.0747;  Loss pred: 0.0747; Loss self: 0.0000; time: 0.19s
Val loss: 0.3407 score: 0.8864 time: 0.06s
Test loss: 0.3431 score: 0.8837 time: 0.06s
Epoch 145/1000, LR 0.000272
Train loss: 0.0700;  Loss pred: 0.0700; Loss self: 0.0000; time: 0.19s
Val loss: 0.3455 score: 0.8636 time: 0.06s
Test loss: 0.3424 score: 0.9070 time: 0.28s
     INFO: Early stopping counter 1 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0757;  Loss pred: 0.0757; Loss self: 0.0000; time: 0.32s
Val loss: 0.3360 score: 0.8864 time: 0.07s
Test loss: 0.3402 score: 0.8605 time: 0.06s
Epoch 147/1000, LR 0.000272
Train loss: 0.0715;  Loss pred: 0.0715; Loss self: 0.0000; time: 0.19s
Val loss: 0.3358 score: 0.8864 time: 0.07s
Test loss: 0.3428 score: 0.8605 time: 0.06s
Epoch 148/1000, LR 0.000272
Train loss: 0.0720;  Loss pred: 0.0720; Loss self: 0.0000; time: 0.19s
Val loss: 0.3425 score: 0.8864 time: 0.07s
Test loss: 0.3464 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 149/1000, LR 0.000272
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.20s
Val loss: 0.3506 score: 0.8636 time: 0.06s
Test loss: 0.3497 score: 0.9070 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0605;  Loss pred: 0.0605; Loss self: 0.0000; time: 0.19s
Val loss: 0.3498 score: 0.8636 time: 0.07s
Test loss: 0.3491 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 151/1000, LR 0.000271
Train loss: 0.0705;  Loss pred: 0.0705; Loss self: 0.0000; time: 0.19s
Val loss: 0.3399 score: 0.8864 time: 0.07s
Test loss: 0.3476 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 152/1000, LR 0.000271
Train loss: 0.0630;  Loss pred: 0.0630; Loss self: 0.0000; time: 0.19s
Val loss: 0.3393 score: 0.8864 time: 0.34s
Test loss: 0.3505 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 153/1000, LR 0.000271
Train loss: 0.0621;  Loss pred: 0.0621; Loss self: 0.0000; time: 0.19s
Val loss: 0.3468 score: 0.8864 time: 0.07s
Test loss: 0.3530 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 154/1000, LR 0.000271
Train loss: 0.0599;  Loss pred: 0.0599; Loss self: 0.0000; time: 0.19s
Val loss: 0.3444 score: 0.8864 time: 0.07s
Test loss: 0.3522 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 155/1000, LR 0.000270
Train loss: 0.0575;  Loss pred: 0.0575; Loss self: 0.0000; time: 0.19s
Val loss: 0.3464 score: 0.8864 time: 0.07s
Test loss: 0.3538 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.19s
Val loss: 0.3447 score: 0.8864 time: 0.07s
Test loss: 0.3555 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 157/1000, LR 0.000270
Train loss: 0.0565;  Loss pred: 0.0565; Loss self: 0.0000; time: 0.19s
Val loss: 0.3419 score: 0.8864 time: 0.07s
Test loss: 0.3559 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 158/1000, LR 0.000270
Train loss: 0.0590;  Loss pred: 0.0590; Loss self: 0.0000; time: 0.31s
Val loss: 0.3441 score: 0.8864 time: 0.07s
Test loss: 0.3569 score: 0.8605 time: 0.27s
     INFO: Early stopping counter 11 of 20
Epoch 159/1000, LR 0.000270
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 0.19s
Val loss: 0.3452 score: 0.8864 time: 0.07s
Test loss: 0.3554 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 160/1000, LR 0.000269
Train loss: 0.0524;  Loss pred: 0.0524; Loss self: 0.0000; time: 0.19s
Val loss: 0.3478 score: 0.8864 time: 0.06s
Test loss: 0.3541 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 161/1000, LR 0.000269
Train loss: 0.0551;  Loss pred: 0.0551; Loss self: 0.0000; time: 0.20s
Val loss: 0.3469 score: 0.8864 time: 0.08s
Test loss: 0.3529 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 162/1000, LR 0.000269
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.21s
Val loss: 0.3444 score: 0.8864 time: 0.07s
Test loss: 0.3528 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 163/1000, LR 0.000269
Train loss: 0.0492;  Loss pred: 0.0492; Loss self: 0.0000; time: 0.18s
Val loss: 0.3465 score: 0.8864 time: 0.07s
Test loss: 0.3565 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 164/1000, LR 0.000269
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.19s
Val loss: 0.3437 score: 0.8864 time: 0.06s
Test loss: 0.3572 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 165/1000, LR 0.000268
Train loss: 0.0525;  Loss pred: 0.0525; Loss self: 0.0000; time: 0.45s
Val loss: 0.3511 score: 0.8636 time: 0.07s
Test loss: 0.3606 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 166/1000, LR 0.000268
Train loss: 0.0513;  Loss pred: 0.0513; Loss self: 0.0000; time: 0.19s
Val loss: 0.3572 score: 0.8636 time: 0.07s
Test loss: 0.3648 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 167/1000, LR 0.000268
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.20s
Val loss: 0.3430 score: 0.8864 time: 0.07s
Test loss: 0.3614 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 146,   Train_Loss: 0.0715,   Val_Loss: 0.3358,   Val_Precision: 1.0000,   Val_Recall: 0.7727,   Val_accuracy: 0.8718,   Val_Score: 0.8864,   Val_Loss: 0.3358,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.3428


[0.07191935099945113, 0.06566255500001716, 0.06481734499993763, 0.0645866439999736, 0.0654895409998062, 0.06699546399977407, 0.07489186499969946, 0.06789828700038925, 0.065364120000595, 0.06464336099998036, 0.06451441399985924, 0.05992405399956624, 0.06385051400047814, 0.06252894699991884, 0.06415656399985892, 0.06309215600049356, 0.0617179880000549, 0.0641192979992411, 0.1894068010005867, 0.07646090099933645, 0.2771502909999981, 0.06651674099975935, 0.06608974400023726, 0.06725618300060887, 0.06670381500043732, 0.06598288900022453, 0.06562872199992853, 0.06523706200005108, 0.06973811700027, 0.06990561699967657, 0.06867506599974149, 0.06882964399937919, 0.062270883000564936, 0.06334112999957142, 0.06483782600025734, 0.28924775599989516, 0.06909718600036285, 0.06811032900077407, 0.06529857799978345, 0.07043848400007846, 0.06744881400027225, 0.06771488899994438, 0.06791518099998939, 0.08430060699993192, 0.06500819899974886, 0.06531481099955272, 0.06504587800009176, 0.06363612099994498, 0.06740145299954747, 0.06567490599991288, 0.0671864790001564, 0.0656816250002521, 0.06673767299980682, 0.06430500599981315, 0.06567406500016659, 0.06758690900005604, 0.06828165099977923, 0.06732209099936881, 0.20218850400033261, 0.06953205599984358, 0.07159061499987729, 0.06690466399959405, 0.0673232930002996, 0.06922149599995464, 0.06753499700062093, 0.06784073900053045, 0.06801344900031836, 0.06668652699954691, 0.06984229599947867, 0.06936718699944322, 0.06119231799948466, 0.0631043059993317, 0.0639366900004461, 0.06529062899971905, 0.06836912100061454, 0.06744043300022895, 0.07002063199979602, 0.07175901200025692, 0.06500699499974871, 0.19943561499985663, 0.06787225999960356, 0.06755850900026417, 0.07186840099984693, 0.06693687000006321, 0.065379216999645, 0.071644446999926, 0.0654276570003276, 0.06368186799954856, 0.06381112999952165, 0.059397782999440096, 0.07724504999987403, 0.06215368599987414, 0.07222140700014279, 0.20523750899974402, 0.06538299700059724, 0.06806568899992271, 0.06354714300050546, 0.06045466100022168, 0.06061913000030472, 0.0611227310000686, 0.061042645999805245, 0.06966101199941477, 0.06683107800017751, 0.06522892599969055, 0.06565365999995265, 0.0660666260000653, 0.07000669399985782, 0.0637244469999132, 0.06090805100029684, 0.29635152199989534, 0.06483579199993983, 0.0650263610004913, 0.065247575000285, 0.06462665199978801, 0.06737577100011549, 0.06559359999937442, 0.3106766989994867, 0.06436933699933434, 0.06667949300026521, 0.07503842599999189, 0.06862229299986211, 0.06836183600080403, 0.08654296100030479, 0.06978251800046564, 0.06794946399986657, 0.07251243399969098, 0.06189062199973705, 0.06257518499933212, 0.06919449999986682, 0.07385849000002054, 0.07149127899992891, 0.07576360700022633, 0.06811446899973816, 0.07178769600068335, 0.07157901599930483, 0.06631399700017937, 0.06684833400049683, 0.06988835900028789, 0.0667835650001507, 0.06803682200006733, 0.06766274599976896, 0.06950735800000984, 0.06831102399974043, 0.07035781400009, 0.06749960300021485, 0.06739035899954615, 0.06747527300012734, 0.07041527799992764, 0.07261062900033721, 0.07162366800002928, 0.07317872700059525, 0.08491048299947579, 0.07283579499926418, 0.06259298799977842, 0.06357226299951435, 0.06655724699976417, 0.062344345999918005, 0.06443906200001948, 0.07508069100003922, 0.067774639000163, 0.07344398300028843, 0.07457832199997938, 0.07258729500063055, 0.06712794800023403, 0.06815647900020849, 0.07629938399986713, 0.07244966500002192, 0.06759041499935847, 0.06710172100065392, 0.06806636300007085, 0.07032599700050923, 0.0717779819997304, 0.07357447300000786, 0.07481395699960558, 0.06551039999976638, 0.18484424800044508, 0.06748363499991683, 0.06329096899935394, 0.070221746000243, 0.06305758300004527, 0.07337050600017392, 0.07338463799987949, 0.07119937399966147, 0.06228765000014391, 0.05855494400020689, 0.05877285399947141, 0.05956566999975621, 0.06066611999995075, 0.06071471199993539, 0.06479382699944836, 0.06351696599995194, 0.06466677800017351, 0.06458180000026914, 0.06227831099931791, 0.06492185600018274, 0.2824407030002476, 0.06508024800041312, 0.06568158799927915, 0.06443414100067457, 0.06628108599943516, 0.06651780900028825, 0.06618482700014283, 0.08492191799996363, 0.0670898260004833, 0.05960616299944377, 0.0605452019999575, 0.05945295499986969, 0.05995556699963345, 0.06225547200028814, 0.06332335099978081, 0.0619014949998018, 0.05784378200041829, 0.05946926700016775, 0.06600638100007927, 0.05942027299988695, 0.05944515500050329, 0.05925366900009976, 0.1793402060002336, 0.06063864099996863, 0.06514647499989223, 0.06761656399976346, 0.06289238199951797, 0.06375519699940924, 0.06332682099946396, 0.06141730099989218, 0.06506629400064412, 0.2919764200005375, 0.06254808100038645, 0.06399390899969148, 0.06613637199916411, 0.05986259600013, 0.06067776399959257, 0.060354780000125174, 0.06234871899960126, 0.059951771000669396, 0.05978715200035367, 0.05992908700045518, 0.06183642100040743, 0.06288870200023666, 0.06294006000007357, 0.06314714800009824, 0.06316426099965611, 0.260026537000158, 0.06324659600068117, 0.05845977200078778, 0.059304911000253924, 0.06035710700052732, 0.05783836000045994, 0.05866752500060102, 0.058946694000042044, 0.07710775499981537, 0.06304027199985285, 0.06278254499920877, 0.0622295489993121, 0.06345478799994453, 0.06480823599940777, 0.06454352000037034, 0.0638122079999448, 0.06354099700001825, 0.06374472700008482, 0.06578850299956684, 0.06659249699987413, 0.06487827799992374, 0.06038536800042493, 0.060257696000007854, 0.06670066500009852, 0.06435849599984067, 0.06620768099946872, 0.06712927399985347, 0.06248369399963849, 0.0649493099999745, 0.058904709999296756, 0.06470010099928913, 0.0691035899999406, 0.0684374749998824, 0.06591682100042817, 0.07138385599955654, 0.06946548399992025, 0.06610941300004924, 0.06655322999995406, 0.06770158899962553, 0.06863846300075238, 0.06774209800005337, 0.06550710999999865, 0.06643074800012982, 0.0675737659994411, 0.06857861499975115, 0.0660449940005492, 0.06885996399978467, 0.28354936799951247, 0.06574267699943448, 0.06576730399956432, 0.0633607560002929, 0.06827350900039164, 0.06605093799953465, 0.06603317499957484, 0.2723550239998076, 0.0647789520007791, 0.06732159200055321, 0.06551425899942842, 0.06604844899993623, 0.06258948699996836, 0.05937699300011445, 0.060601646999202785, 0.06308050900042872, 0.06282765000014479, 0.05774454899983539, 0.05903453399969294, 0.05784724999921309, 0.058324838000771706, 0.05900569599998562, 0.05806308299997909, 0.07002184000066336, 0.06760328900054446, 0.06571011599953636, 0.0642975339997065, 0.06520104199989873, 0.06563647799976025, 0.06451341899992258, 0.06385856799988687, 0.06282161899980565, 0.06583721700008027, 0.06543463900015922, 0.06453644900011568, 0.06518255500031955, 0.06510795799931657, 0.06355632700069691, 0.2869673559998773, 0.06428241300000082, 0.06673502800003916, 0.0651962830006596, 0.06571500100017147, 0.06552037000074051, 0.06512960899999598, 0.06695051499991678, 0.0678875829999015, 0.06552818000000116, 0.06649514099990483, 0.06967896800051676, 0.06530381199991098, 0.2783370360002664, 0.06532245299968054, 0.06364848599969264, 0.07770287899984396, 0.06481252300000051, 0.06262544600031106, 0.06183214400061843, 0.06880134800030646, 0.06564183900081844, 0.06644058399979258]
[0.00163453070453298, 0.0014923307954549355, 0.00147312147727131, 0.0014678782727266728, 0.0014883986590865045, 0.0015226241818130472, 0.0017020878409022605, 0.001543142886372483, 0.0014855481818317044, 0.001469167295454099, 0.0014662366818149828, 0.00136191031817196, 0.0014511480454654122, 0.0014211124318163374, 0.001458103727269521, 0.0014339126363748537, 0.0014026815454557932, 0.001457256772710025, 0.004304700022740607, 0.0017377477499849192, 0.006298870249999957, 0.0015117441136308944, 0.0015020396363690286, 0.0015285496136502015, 0.0015159957954644844, 0.0014996111136414665, 0.0014915618636347392, 0.001482660500001161, 0.0015849572045515908, 0.001588764022719922, 0.0015607969545395793, 0.0015643100908949816, 0.0014152473409219303, 0.001439571136353896, 0.001473586954551303, 0.006573812636361254, 0.0015703905909173375, 0.0015479620227448652, 0.0014840585909041693, 0.0016008746363654195, 0.0015329275909152784, 0.0015389747499987359, 0.0015435268409088496, 0.001915922886362089, 0.0014774590681761104, 0.0014844275227171072, 0.0014783154090929945, 0.0014462754772714768, 0.00153185120453517, 0.00149261149999802, 0.0015269654318217363, 0.001492764204551184, 0.001516765295450155, 0.0014614774090866624, 0.0014925923863674225, 0.0015360661136376373, 0.001551855704540437, 0.0015300475227129275, 0.004595193272734832, 0.001580273999996445, 0.0016270594318153928, 0.0015205605454453193, 0.0015300748409159, 0.0015732158181807872, 0.0015348862954686576, 0.001541834977284783, 0.00154576020455269, 0.001515602886353339, 0.0015873249090790605, 0.0015765269772600732, 0.0013907344999882878, 0.001434188772712084, 0.0014531065909192296, 0.0014838779318117965, 0.001553843659104876, 0.001532737113641567, 0.0015913779999953642, 0.0016308866363694754, 0.0014774317045397436, 0.004532627613633105, 0.0015425513636273536, 0.001535420659096913, 0.001633372749996521, 0.0015212925000014366, 0.0014858912954464772, 0.0016282828863619545, 0.0014869922045529, 0.0014473151818079218, 0.001450252954534583, 0.0013499496136236385, 0.0017555693181789552, 0.0014125837727244123, 0.0016413956136396089, 0.004664488840903273, 0.0014859772045590282, 0.0015469474772709707, 0.0014442532500114876, 0.0013739695681868563, 0.0013777075000069256, 0.0013891529772742863, 0.0013873328636319375, 0.0015832048181685175, 0.0015188881363676708, 0.0014824755909020578, 0.0014921286363625602, 0.0015015142272742114, 0.001591061227269496, 0.0014482828863616635, 0.0013842738863703826, 0.006735261863633985, 0.0014735407272713599, 0.001477871840920257, 0.001482899431824659, 0.0014687875454497275, 0.0015312675227298976, 0.0014907636363494187, 0.007060834068170152, 0.0014629394772575986, 0.0015154430227333003, 0.001745079674418416, 0.0015958672790665608, 0.0015898101395535821, 0.002012627000007088, 0.0016228492558247824, 0.0015802200930201528, 0.0016863356744114182, 0.0014393167906915593, 0.0014552368604495844, 0.001609174418601554, 0.0017176393023260591, 0.0016625878837192769, 0.0017619443488424728, 0.0015840574185985619, 0.0016694813023414733, 0.0016646282790536008, 0.0015421859767483575, 0.0015546124186162054, 0.0016253106744252997, 0.0015531061627942024, 0.0015822516744201705, 0.0015735522325527665, 0.0016164501860467405, 0.0015886284651102426, 0.0016362282325602326, 0.001569758209307322, 0.0015672176511522362, 0.0015691923953517987, 0.00163756460464948, 0.0016886192790776097, 0.0016656666976750997, 0.0017018308604789592, 0.001974662395336646, 0.0016938556976573065, 0.0014556508837157772, 0.0014784247209189384, 0.0015478429534828877, 0.001449868511626, 0.0014985828372097553, 0.001746062581396261, 0.001576154395352628, 0.0017079996046578704, 0.0017343795813948694, 0.0016880766279216406, 0.0015611150697728844, 0.001585034395353686, 0.0017744042790666776, 0.001684875930233068, 0.0015718701162641506, 0.0015605051395500913, 0.0015829386744202524, 0.001635488302337424, 0.0016692553953425674, 0.001711034255814136, 0.0017398594651071065, 0.0015234976744131715, 0.004298703441870816, 0.0015693868604631821, 0.0014718829999849753, 0.0016330638604707675, 0.001466455418605704, 0.0017062908372133469, 0.0017066194883692905, 0.0016557993953409645, 0.0014485500000033468, 0.0013617428837257417, 0.0013668105581272421, 0.001385248139529214, 0.0014108399999988547, 0.0014119700465101254, 0.0015068331860336828, 0.0014771387441849288, 0.00150387855814357, 0.0015019023255876543, 0.0014483328139376257, 0.0015098106046554126, 0.006568388441866223, 0.0015134941395444912, 0.0015274787906809104, 0.001498468395364525, 0.001541420604638027, 0.001546925790704378, 0.0015391820232591356, 0.0019749283255805494, 0.0015602285116391465, 0.0013861898371963666, 0.0014080279534873837, 0.0013826268604620858, 0.0013943155116193824, 0.0014478016744253056, 0.0014726360697623446, 0.0014395696511581813, 0.0013452042325678671, 0.0013830062093062266, 0.0015350321162809133, 0.0013818668139508592, 0.0013824454651279836, 0.0013779923023279014, 0.004170702465121711, 0.0014102009534876425, 0.0015150343023230752, 0.0015724782325526386, 0.0014626135348725108, 0.0014826789999862614, 0.0014727167674293946, 0.0014283093255788879, 0.0015131696279219564, 0.0067901493023380805, 0.001454606534892708, 0.0014882304418532904, 0.0015380551627712585, 0.0013921533953518603, 0.0014111107906881993, 0.001403599534886632, 0.0014499702092930526, 0.0013942272325737069, 0.0013903988837291551, 0.0013936996976850044, 0.0014380563023350564, 0.0014625279534938758, 0.0014637223255831062, 0.00146853832558368, 0.0014689363023175838, 0.006047128767445535, 0.001470851069783283, 0.0013595295814136693, 0.0013791839767500912, 0.001403653651175054, 0.0013450781395455802, 0.0013643610465256051, 0.001370853348838187, 0.001793203604646869, 0.0014660528372058802, 0.0014600591860281108, 0.0014471988139374908, 0.0014756927441847564, 0.0015071682790559946, 0.0015010120930318684, 0.0014840048372080188, 0.0014776976046515873, 0.0014824355116298796, 0.0015299651860364383, 0.0015486627209273054, 0.0015087971627889241, 0.0014043108837308125, 0.001401341767442043, 0.0015511782558162448, 0.0014967092092986202, 0.0015397135116155516, 0.0015611459069733364, 0.0014531091627822904, 0.0015104490697668488, 0.0013698769767278315, 0.0015046535116113751, 0.0016070602325567581, 0.0015915691860437767, 0.0015329493255913527, 0.0016600896744082918, 0.0016154763720911687, 0.0015374282093034708, 0.0015477495348826524, 0.0015744555581308263, 0.0015962433255988925, 0.001575397627908218, 0.0015234211627906662, 0.001544901116282089, 0.0015714829302195604, 0.0015948515116221199, 0.001535930093036028, 0.0016013945116228993, 0.006594171348825871, 0.0015288994651031275, 0.0015294721860363795, 0.0014735059534951836, 0.001587756023264922, 0.0015360683255705733, 0.001535655232548252, 0.006333837767437386, 0.001506487255832072, 0.0015656184186175165, 0.0015235874185913587, 0.001536010441858982, 0.0014555694651155432, 0.0013808603023282432, 0.0014093406278884368, 0.001466988581405319, 0.0014611081395382508, 0.0013428964883682648, 0.0013728961395277428, 0.00134528488370263, 0.0013563915814132955, 0.0013722254883717585, 0.001350304255813467, 0.0016284148837363572, 0.0015721695116405689, 0.0015281422325473572, 0.0014952914883652674, 0.0015163033023232264, 0.001526429720924657, 0.0015003120697656414, 0.0014850829767415552, 0.0014609678837164105, 0.0015310980697693087, 0.0015217357907013772, 0.001500847651165481, 0.0015158733721004547, 0.0015141385581236412, 0.001478054116295277, 0.006673659441857612, 0.0014949398372093212, 0.0015519773953497481, 0.0015161926279223163, 0.00152825583721329, 0.0015237295349009423, 0.0015146420697673484, 0.0015569887209282971, 0.0015787809999977093, 0.0015239111627907247, 0.0015463986279047636, 0.0016204411162910875, 0.001518693302323511, 0.00647295432558759, 0.001519126813946059, 0.0014801973488300613, 0.0018070436976707898, 0.001507267976744198, 0.0014564057209374664, 0.0014379568372236843, 0.0016000313488443363, 0.0015265543953678707, 0.0015451298604602925]
[611.7963995578297, 670.0927187494989, 678.8306432489998, 681.2554001105551, 671.8630078676259, 656.7608816045871, 587.5137439850986, 648.0281306617906, 673.1521819554746, 680.657678056272, 682.0181300894401, 734.2627386377847, 689.1095661292642, 703.6740919378846, 685.8222644232749, 697.3925570027404, 712.9201943517805, 686.2208628753355, 232.3042243866613, 575.4575139048106, 158.7586281841584, 661.487609565225, 665.7613925670834, 654.2149440684385, 659.6324363113494, 666.8395498695165, 670.4381657782078, 674.4632368632043, 630.931861837188, 629.4200936700634, 640.6983285631735, 639.259444671788, 706.5902694779649, 694.6513268755659, 678.6162139339059, 152.11872551231113, 636.7842534103913, 646.010680692791, 673.8278435427172, 624.658531832556, 652.3465334738488, 649.7832404338156, 647.8669327260849, 521.9416747501656, 676.8377016592935, 673.660373912761, 676.4456311887731, 691.4312077575886, 652.80491802299, 669.9666993060997, 654.8936728757221, 669.898164057773, 659.2977852273538, 684.239108851461, 669.975278671853, 651.0136452602607, 644.3898083270169, 653.5744708287883, 217.61870298979815, 632.8016533855836, 614.6056993654184, 657.6522079277908, 653.5628018047809, 635.640697508601, 651.5140586975291, 648.5778405163888, 646.9308739186869, 659.8034412603156, 629.9907437225207, 634.3056696295494, 719.0445049061641, 697.2582821918045, 688.1807613076779, 673.9098806995617, 643.565389696973, 652.4276022938735, 628.3862162245005, 613.1634030836777, 676.8502374270658, 220.6225803752837, 648.2766302500758, 651.2873160037909, 612.2301232233303, 657.3357851951914, 672.9967414604998, 614.1438986896702, 672.4984817930999, 690.9345058834002, 689.534882086085, 740.7683886183893, 569.6157876792332, 707.9226162079767, 609.2376461166564, 214.38576317964802, 672.9578333583895, 646.4343584335121, 692.3993420073979, 727.8181578065364, 725.8434754800807, 719.8631226073753, 720.8075482203111, 631.6302151965529, 658.3763320394612, 674.5473626257275, 670.183505383123, 665.9943554549995, 628.5113249325752, 690.4728416091227, 722.4003933369263, 148.47232672560912, 678.6375031871413, 676.6486594516269, 674.3545641321966, 680.8336597746752, 653.0537513244127, 670.7971509479528, 141.62632776033422, 683.5552772658668, 659.8730437231278, 573.0397383335926, 626.6185246839012, 629.0059266327232, 496.86305509986613, 616.2001778111972, 632.8232405201082, 593.0017464340426, 694.7740806383024, 687.17335794467, 621.436674881425, 582.1944098774297, 601.4719641544357, 567.5548155973031, 631.2902476001875, 598.9884394616965, 600.7347181248987, 648.4302250682264, 643.2471450923581, 615.2669860201306, 643.8709883173049, 632.0107073777997, 635.5048020094666, 618.6395402914596, 629.4738020639743, 611.1616827655417, 637.0407837785822, 638.0734668632584, 637.2704857365875, 610.662930281184, 592.1998003873551, 600.3602049532344, 587.6024599286926, 506.4156801494754, 590.3690623605386, 686.9779087739385, 676.3956161247318, 646.060375666565, 689.7177171456182, 667.2971124251778, 572.717158396658, 634.4556110420091, 585.4802291949652, 576.5750535391758, 592.3901696519545, 640.5677706675928, 630.9011356039748, 563.5694254107593, 593.5155117692675, 636.1848791786251, 640.8181393675574, 631.7364128880405, 611.4381855075391, 599.069502959299, 584.4418348738346, 574.7590653469472, 656.3843298186753, 232.6282828118972, 637.1915205820344, 679.4018274619707, 612.3459248628081, 681.916400125408, 586.0665592233761, 585.9536978307451, 603.9378941759298, 690.3455179301299, 734.3530206407178, 731.6302863289017, 721.8923248941209, 708.7975957591306, 708.2303215083316, 663.6434671526054, 676.9844768724082, 664.9473088002718, 665.8222595192577, 690.449039320783, 662.334730539419, 152.24434560326918, 660.7227434002256, 654.6735745864111, 667.348075604047, 648.7521945606992, 646.4434208861819, 649.6957376636673, 506.3474897024631, 640.9317561755227, 721.4019127586063, 710.2131726314199, 723.2609379986958, 717.1977874925758, 690.7023369737038, 679.0543981184577, 694.6520435433374, 743.3815444448127, 723.0625526270352, 651.4521679343147, 723.6587418587224, 723.3558395067847, 725.6934587447674, 239.76776295185024, 709.1187944007888, 660.0510618582376, 635.9388507252517, 683.707607072818, 674.454821312817, 679.0171892627293, 700.1284540340757, 660.8644408051629, 147.27216670414975, 687.4711312043983, 671.938949692967, 650.1717390930285, 718.3116482269934, 708.6615782395793, 712.453926597211, 689.6693418877619, 717.2431986958297, 719.2180687875153, 717.5146853092122, 695.3830655839005, 683.7476149505866, 683.1896887284463, 680.94920137855, 680.7647128212916, 165.3677370628286, 679.8784870499101, 735.5485409594233, 725.0664283066859, 712.4264587370684, 743.4512320138064, 732.9438219791867, 729.4726316622495, 557.6611587265504, 682.1036558995236, 684.9037419642979, 690.9900632652075, 677.6478395930909, 663.4959174076724, 666.217150842614, 673.852250967984, 676.7284435273757, 674.5655997545143, 653.6096436224292, 645.7183907682764, 662.7796132328072, 712.0930355131297, 713.6017945325091, 644.671233786597, 668.1324560491042, 649.4714714497409, 640.5551175794611, 688.1795432941078, 662.0547623988135, 729.992559177583, 664.6048357864611, 622.2542128424437, 628.3107318040867, 652.337284283183, 602.3770977049367, 619.0124580439023, 650.4368750024746, 646.0993703841224, 635.1401885151889, 626.4709045062483, 634.7603819410217, 656.4172957714192, 647.2906190957833, 636.3416240609655, 627.017620582685, 651.0712984490912, 624.4557432550279, 151.64907720786715, 654.0652428919176, 653.8203238540059, 678.6535185880867, 629.8196859890904, 651.0127078029225, 651.1878309694613, 157.88216192417428, 663.7958576341716, 638.7252398850974, 656.3456666796026, 651.0372408599862, 687.0163355072991, 724.186218051108, 709.551672755126, 681.6685642106622, 684.4120383286802, 744.6590326668337, 728.3872182377803, 743.3369779995581, 737.2502260431663, 728.7432047240063, 740.5738341523389, 614.0941169154172, 636.0637276043432, 654.3893485183225, 668.7659281022547, 659.4986626144224, 655.1235122664118, 666.5279978426132, 673.3630481672589, 684.4777432452518, 653.1260274860581, 657.1442993655909, 666.2901455876961, 659.6857088493877, 660.4415392731464, 676.5652143417365, 149.8428274190824, 668.9232403270154, 644.3392816134694, 659.5468026845175, 654.340703728937, 656.2844501567072, 660.2219890495988, 642.2654105058557, 633.4000725885674, 656.206230662888, 646.6637915702975, 617.1159136524683, 658.4607955207671, 154.48896279817697, 658.2728912554815, 675.5855905230433, 553.390048779098, 663.4520307132568, 686.6218565499146, 695.4311660221571, 624.9877545976057, 655.0700080091276, 647.1947928713907]
Elapsed: 0.07505925304284444~0.04149291904607155
Time per graph: 0.0017314357121833413~0.0009544976949661361
Speed: 635.9248965719213~111.48122524890573
Total Time: 0.0676
best val loss: 0.3358164957978509 test_score: 0.8605

Testing...
Test loss: 0.3431 score: 0.8837 time: 0.06s
test Score 0.8837
Epoch Time List: [1.0520186630010357, 0.2974265689999811, 0.5067151310004192, 0.2826955199989243, 0.28970633700009785, 0.2876284209987716, 0.2977158160001636, 0.2991115760014509, 0.28664642899911996, 0.28319891099909, 0.3958080930005963, 0.49973489399963, 0.27024119599991536, 0.2803650949990697, 0.26860879299965745, 0.2680863119994683, 0.2715254859995184, 0.2731704860007085, 0.3968236210012037, 0.318864922001012, 0.5113738180007203, 0.2907970429996567, 0.2939924439997412, 0.2894198750000214, 0.28993831899970246, 0.44691500200133305, 0.27910435699959635, 0.2846669580003436, 0.44561562099988805, 0.3026070769992657, 0.3151263689996995, 0.31315806600105134, 0.4358365740008594, 0.27664776100118615, 0.2789770089993908, 0.5086380420007117, 0.30138823300057993, 0.3040189500006818, 0.42312929299987445, 0.30363877700074227, 0.30681966400061356, 0.29851695699926495, 0.2988277999993443, 0.5251918239991937, 0.42877742200107605, 0.29507682799976465, 0.29226279800150223, 0.2900989190002292, 0.5045724860010523, 0.2917673400006606, 0.30351892299950123, 0.298129550998965, 0.4218481650004833, 0.28249260199936543, 0.28484595800000534, 0.2968283150003117, 0.5056092730001183, 0.2911291150012403, 0.4304211550006585, 0.30792823400133784, 0.3033947919984712, 0.2901176799996392, 0.2955366109999886, 0.5054509479987246, 0.29627474200060533, 0.2917242760013323, 0.4169394500004273, 0.2918066279989944, 0.3022027649994925, 0.3063403930000277, 0.49125498800094647, 0.2726601660015149, 0.27735730200129183, 0.27387424600146915, 0.4163954380001087, 0.3012869739995949, 0.3015169869986494, 0.3050076950003131, 0.4972433180000735, 0.4362038469998879, 0.2693942580008297, 0.2799999239996396, 0.28418706899992685, 0.29868690700004663, 0.2894643720001113, 0.2919398900003216, 0.6124269189995175, 0.2851433319992793, 0.27903715199954604, 0.254870354000559, 0.28602403000058985, 0.31237043700002687, 0.27823058600006334, 0.645783076999578, 0.2834840109999277, 0.3020887330003461, 0.2926792590005789, 0.2681505880009354, 0.26286586000060197, 0.2634253930000341, 0.27866222800093965, 0.6421565639993787, 0.2927732329999344, 0.2878333999997267, 0.28381621399967116, 0.289090130000659, 0.3048068099988086, 0.3120301079998171, 0.2682489690014336, 0.6453079869997964, 0.28434719300003053, 0.2866854939993573, 0.28682344499975443, 0.29318224000053306, 0.3013063840007817, 0.42220615400128736, 0.5308118800003285, 0.3246516600001996, 0.28324070099915843, 0.3309496570000192, 0.3315559079992454, 0.34074211299957824, 0.37318763400071475, 0.32173636199968314, 0.3147598369996558, 0.31527681199986546, 0.4275812539999606, 0.2900637230013672, 0.31761635200018645, 0.3187234239994723, 0.3072149209992858, 0.31387469500077714, 0.30793551500028116, 0.3176293570004418, 0.5296896110003217, 0.299783462999585, 0.3079042790004678, 0.3104298709995419, 0.29877699700045923, 0.30000869300056365, 0.303672602000006, 0.529081784000482, 0.30485452199900465, 0.3083071849996486, 0.30605311000090296, 0.30830467300074815, 0.30721929500123224, 0.31297511299908365, 0.46326517699981196, 0.3112107470005867, 0.33849732399994537, 0.35182817399891064, 0.3437438300006761, 0.3209373999998206, 0.29915073000029224, 0.31062015399947995, 0.4959637930005556, 0.28459729200039874, 0.3036895300001561, 0.3370862869996927, 0.33258963000116637, 0.329024097999536, 0.5152158929995494, 0.3053366799986179, 0.3314196119999906, 0.32867643299960037, 0.32653196699902765, 0.31209146099899954, 0.4454127810004138, 0.30962212499980524, 0.4920858990008128, 0.3289643719999731, 0.33124257100007526, 0.31690738099950977, 0.2935282290000032, 0.4241746779998721, 0.29028115299934143, 0.28371846200025175, 0.30239426200023445, 0.5005521000002773, 0.31654846999936126, 0.4466316310008551, 0.3177860609994241, 0.4679122870002175, 0.2953603910000311, 0.28255025199996453, 0.29367204800109903, 0.3103123310002047, 0.29693801299981715, 0.4292746190003527, 0.49932545599858713, 0.31038220999926125, 0.3213558469997224, 0.31675511699995695, 0.3058133609993092, 0.5327186670001538, 0.314902213998721, 0.3222332900004403, 0.31737999099914305, 0.30596375100049045, 0.3194482059998336, 0.3208615049998116, 0.3337361800004146, 0.6491259660006108, 0.3007203200004369, 0.2900013330008733, 0.29373317700083135, 0.29398704600043857, 0.42014206599924364, 0.30681053399894154, 0.2999757170009616, 0.45611211499999627, 0.28420959199956997, 0.28810118100136606, 0.2937077929991574, 0.2937037159990723, 0.2836562699985734, 0.4154559509997853, 0.306533567999395, 0.5194002700000055, 0.3105454890010151, 0.30551788699904137, 0.30654089200015733, 0.3011155650010551, 0.3034909060006612, 0.4721240629987733, 0.5410955889992692, 0.30901153000013437, 0.30668930200044997, 0.3156815190004636, 0.44225822899989, 0.3058349809998617, 0.30551445099899865, 0.2982959859991752, 0.5109280139995462, 0.2895039369986989, 0.2943456529992545, 0.4285537059995477, 0.30749170499984757, 0.30219748400122626, 0.304710704000172, 0.3138063639999018, 0.5095479299989165, 0.3080437789994903, 0.39923573300075077, 0.2895439879994228, 0.289084501000616, 0.2853523869998753, 0.285502180999174, 0.28523032700013573, 0.6535221590002038, 0.3099469569997382, 0.30180703600035486, 0.2954202720002286, 0.3101493620006295, 0.31368646000009903, 0.3181705340002736, 0.5255014149997805, 0.37489921399901505, 0.31262765699921147, 0.3150593159998607, 0.3265826360002393, 0.3165664509997441, 0.30841861499902734, 0.40388721199997235, 0.5096500199997536, 0.31447572599972773, 0.32291514800090226, 0.33273008599826426, 0.31046219500058214, 0.30681101699974533, 0.4144645819997095, 0.30927824699938355, 0.5334225449996666, 0.34640891599974566, 0.3430468610004027, 0.3418388540003434, 0.34515951800040057, 0.4817116939993866, 0.3260003199993662, 0.3276806640005816, 0.578720214999521, 0.32980853299886803, 0.3364177480007129, 0.32789670700003626, 0.3323893359993235, 0.3274455250002575, 0.33171130499977153, 0.3431051950001347, 0.5588991269996768, 0.33623731499938003, 0.32699174400113407, 0.32495656599985523, 0.32667912500073726, 0.3300285580007767, 0.3310998090000794, 0.5402267829995253, 0.3288478569993458, 0.332717350999701, 0.3300634409997656, 0.3346262650002245, 0.3283578080008738, 0.3082762019994334, 0.31033912700058863, 0.2934472030001416, 0.5058561459991324, 0.2801982800001497, 0.28485025400004815, 0.288001993000762, 0.2859152629980599, 0.28234010900087014, 0.2931617459998961, 0.5082426750004743, 0.3293926369988185, 0.32809225599976344, 0.3211821540016899, 0.31193463699946733, 0.3220156400011547, 0.3162147780003579, 0.31758100900151476, 0.44614382199961256, 0.3141336990011041, 0.32225530100095057, 0.3161488890009423, 0.31461769900033687, 0.319037425998431, 0.306116084999303, 0.5311111719993278, 0.44543704699935915, 0.31559813000058057, 0.3188142399994831, 0.3211169209998843, 0.32028927600003954, 0.3188709870009916, 0.5999036080002043, 0.31977342600021075, 0.3179766289995314, 0.326451743000689, 0.3248215200001141, 0.3173818859995663, 0.6591652290007914, 0.3200054309991174, 0.30985680800040427, 0.348249709000811, 0.33999694399881264, 0.3049703190008586, 0.3028261139997994, 0.5811167479996584, 0.3255699689998437, 0.3301290859999426]
Total Epoch List: [119, 64, 167]
Total Time List: [0.06739704700066795, 0.07204406500022742, 0.06759007500022562]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f39bb50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7420;  Loss pred: 0.7420; Loss self: 0.0000; time: 0.17s
Val loss: 0.6925 score: 0.5349 time: 0.06s
Test loss: 0.6954 score: 0.4545 time: 0.06s
Epoch 2/1000, LR 0.000015
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.17s
Val loss: 0.6917 score: 0.5349 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000045
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 0.17s
Val loss: 0.6900 score: 0.5349 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.06s
Epoch 4/1000, LR 0.000075
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.19s
Val loss: 0.6873 score: 0.5349 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5000 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.7234;  Loss pred: 0.7234; Loss self: 0.0000; time: 0.16s
Val loss: 0.6834 score: 0.5581 time: 0.06s
Test loss: 0.6867 score: 0.5227 time: 0.17s
Epoch 6/1000, LR 0.000135
Train loss: 0.6655;  Loss pred: 0.6655; Loss self: 0.0000; time: 0.16s
Val loss: 0.6789 score: 0.6512 time: 0.06s
Test loss: 0.6820 score: 0.5909 time: 0.06s
Epoch 7/1000, LR 0.000165
Train loss: 0.6531;  Loss pred: 0.6531; Loss self: 0.0000; time: 0.15s
Val loss: 0.6743 score: 0.7674 time: 0.06s
Test loss: 0.6770 score: 0.6818 time: 0.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6430;  Loss pred: 0.6430; Loss self: 0.0000; time: 0.16s
Val loss: 0.6704 score: 0.6047 time: 0.06s
Test loss: 0.6728 score: 0.5909 time: 0.06s
Epoch 9/1000, LR 0.000225
Train loss: 0.6238;  Loss pred: 0.6238; Loss self: 0.0000; time: 0.17s
Val loss: 0.6680 score: 0.5116 time: 0.06s
Test loss: 0.6704 score: 0.5682 time: 0.06s
Epoch 10/1000, LR 0.000255
Train loss: 0.5989;  Loss pred: 0.5989; Loss self: 0.0000; time: 0.17s
Val loss: 0.6673 score: 0.5116 time: 0.06s
Test loss: 0.6700 score: 0.5227 time: 0.06s
Epoch 11/1000, LR 0.000285
Train loss: 0.5656;  Loss pred: 0.5656; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6694 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6723 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5813;  Loss pred: 0.5813; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6717 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6753 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5780;  Loss pred: 0.5780; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6748 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6791 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5786;  Loss pred: 0.5786; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6765 score: 0.4884 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6816 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5536;  Loss pred: 0.5536; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5456;  Loss pred: 0.5456; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6870 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5385;  Loss pred: 0.5385; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6791 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6875 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5190;  Loss pred: 0.5190; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6762 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6719 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6831 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6646 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6779 score: 0.5000 time: 0.06s
Epoch 21/1000, LR 0.000285
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6556 score: 0.4884 time: 0.06s
Test loss: 0.6718 score: 0.5227 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.5111;  Loss pred: 0.5111; Loss self: 0.0000; time: 0.17s
Val loss: 0.6472 score: 0.5349 time: 0.06s
Test loss: 0.6661 score: 0.5227 time: 0.06s
Epoch 23/1000, LR 0.000285
Train loss: 0.5364;  Loss pred: 0.5364; Loss self: 0.0000; time: 0.18s
Val loss: 0.6357 score: 0.5349 time: 0.07s
Test loss: 0.6580 score: 0.5682 time: 0.06s
Epoch 24/1000, LR 0.000285
Train loss: 0.4769;  Loss pred: 0.4769; Loss self: 0.0000; time: 0.17s
Val loss: 0.6262 score: 0.5349 time: 0.07s
Test loss: 0.6516 score: 0.5682 time: 0.06s
Epoch 25/1000, LR 0.000285
Train loss: 0.4566;  Loss pred: 0.4566; Loss self: 0.0000; time: 0.16s
Val loss: 0.6174 score: 0.5814 time: 0.06s
Test loss: 0.6459 score: 0.5682 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4918;  Loss pred: 0.4918; Loss self: 0.0000; time: 0.16s
Val loss: 0.6057 score: 0.6047 time: 0.21s
Test loss: 0.6371 score: 0.5682 time: 0.23s
Epoch 27/1000, LR 0.000285
Train loss: 0.4335;  Loss pred: 0.4335; Loss self: 0.0000; time: 0.18s
Val loss: 0.5943 score: 0.6279 time: 0.06s
Test loss: 0.6276 score: 0.5909 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.4397;  Loss pred: 0.4397; Loss self: 0.0000; time: 0.18s
Val loss: 0.5824 score: 0.6512 time: 0.06s
Test loss: 0.6171 score: 0.6136 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.4516;  Loss pred: 0.4516; Loss self: 0.0000; time: 0.18s
Val loss: 0.5713 score: 0.6744 time: 0.06s
Test loss: 0.6070 score: 0.6364 time: 0.06s
Epoch 30/1000, LR 0.000285
Train loss: 0.4154;  Loss pred: 0.4154; Loss self: 0.0000; time: 0.18s
Val loss: 0.5610 score: 0.7674 time: 0.06s
Test loss: 0.5974 score: 0.6591 time: 0.06s
Epoch 31/1000, LR 0.000285
Train loss: 0.4215;  Loss pred: 0.4215; Loss self: 0.0000; time: 0.18s
Val loss: 0.5506 score: 0.8140 time: 0.06s
Test loss: 0.5871 score: 0.6591 time: 0.07s
Epoch 32/1000, LR 0.000285
Train loss: 0.4077;  Loss pred: 0.4077; Loss self: 0.0000; time: 0.17s
Val loss: 0.5418 score: 0.8140 time: 0.06s
Test loss: 0.5787 score: 0.6818 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.3944;  Loss pred: 0.3944; Loss self: 0.0000; time: 0.17s
Val loss: 0.5339 score: 0.8140 time: 0.06s
Test loss: 0.5711 score: 0.7045 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.4053;  Loss pred: 0.4053; Loss self: 0.0000; time: 0.37s
Val loss: 0.5268 score: 0.8140 time: 0.06s
Test loss: 0.5648 score: 0.7273 time: 0.06s
Epoch 35/1000, LR 0.000285
Train loss: 0.3742;  Loss pred: 0.3742; Loss self: 0.0000; time: 0.17s
Val loss: 0.5201 score: 0.8372 time: 0.06s
Test loss: 0.5585 score: 0.7727 time: 0.07s
Epoch 36/1000, LR 0.000285
Train loss: 0.3872;  Loss pred: 0.3872; Loss self: 0.0000; time: 0.17s
Val loss: 0.5135 score: 0.8372 time: 0.06s
Test loss: 0.5520 score: 0.7955 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.3748;  Loss pred: 0.3748; Loss self: 0.0000; time: 0.17s
Val loss: 0.5070 score: 0.9070 time: 0.07s
Test loss: 0.5461 score: 0.7955 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3700;  Loss pred: 0.3700; Loss self: 0.0000; time: 0.16s
Val loss: 0.5002 score: 0.9070 time: 0.06s
Test loss: 0.5400 score: 0.8182 time: 0.07s
Epoch 39/1000, LR 0.000284
Train loss: 0.3517;  Loss pred: 0.3517; Loss self: 0.0000; time: 0.17s
Val loss: 0.4932 score: 0.9070 time: 0.06s
Test loss: 0.5339 score: 0.8409 time: 0.07s
Epoch 40/1000, LR 0.000284
Train loss: 0.3677;  Loss pred: 0.3677; Loss self: 0.0000; time: 0.17s
Val loss: 0.4871 score: 0.9070 time: 0.06s
Test loss: 0.5289 score: 0.8409 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3513;  Loss pred: 0.3513; Loss self: 0.0000; time: 0.17s
Val loss: 0.4798 score: 0.8837 time: 0.06s
Test loss: 0.5229 score: 0.8409 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3393;  Loss pred: 0.3393; Loss self: 0.0000; time: 0.17s
Val loss: 0.4740 score: 0.8837 time: 0.06s
Test loss: 0.5184 score: 0.8182 time: 0.28s
Epoch 43/1000, LR 0.000284
Train loss: 0.3442;  Loss pred: 0.3442; Loss self: 0.0000; time: 0.17s
Val loss: 0.4682 score: 0.8837 time: 0.06s
Test loss: 0.5136 score: 0.8182 time: 0.06s
Epoch 44/1000, LR 0.000284
Train loss: 0.3217;  Loss pred: 0.3217; Loss self: 0.0000; time: 0.17s
Val loss: 0.4622 score: 0.8837 time: 0.06s
Test loss: 0.5095 score: 0.8182 time: 0.06s
Epoch 45/1000, LR 0.000284
Train loss: 0.3355;  Loss pred: 0.3355; Loss self: 0.0000; time: 0.17s
Val loss: 0.4550 score: 0.8837 time: 0.06s
Test loss: 0.5039 score: 0.8182 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.3359;  Loss pred: 0.3359; Loss self: 0.0000; time: 0.17s
Val loss: 0.4473 score: 0.8837 time: 0.06s
Test loss: 0.4983 score: 0.8182 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3296;  Loss pred: 0.3296; Loss self: 0.0000; time: 0.17s
Val loss: 0.4393 score: 0.8837 time: 0.06s
Test loss: 0.4923 score: 0.8182 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3124;  Loss pred: 0.3124; Loss self: 0.0000; time: 0.15s
Val loss: 0.4327 score: 0.8837 time: 0.06s
Test loss: 0.4871 score: 0.8182 time: 0.06s
Epoch 49/1000, LR 0.000284
Train loss: 0.3090;  Loss pred: 0.3090; Loss self: 0.0000; time: 0.19s
Val loss: 0.4265 score: 0.8837 time: 0.21s
Test loss: 0.4830 score: 0.8182 time: 0.31s
Epoch 50/1000, LR 0.000284
Train loss: 0.2970;  Loss pred: 0.2970; Loss self: 0.0000; time: 0.17s
Val loss: 0.4199 score: 0.9070 time: 0.06s
Test loss: 0.4795 score: 0.8182 time: 0.06s
Epoch 51/1000, LR 0.000284
Train loss: 0.3181;  Loss pred: 0.3181; Loss self: 0.0000; time: 0.16s
Val loss: 0.4134 score: 0.9070 time: 0.06s
Test loss: 0.4759 score: 0.8182 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.2950;  Loss pred: 0.2950; Loss self: 0.0000; time: 0.17s
Val loss: 0.4078 score: 0.9070 time: 0.06s
Test loss: 0.4730 score: 0.8182 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.2909;  Loss pred: 0.2909; Loss self: 0.0000; time: 0.16s
Val loss: 0.4025 score: 0.8837 time: 0.06s
Test loss: 0.4695 score: 0.8182 time: 0.06s
Epoch 54/1000, LR 0.000284
Train loss: 0.2706;  Loss pred: 0.2706; Loss self: 0.0000; time: 0.16s
Val loss: 0.3970 score: 0.8837 time: 0.06s
Test loss: 0.4662 score: 0.8182 time: 0.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.2808;  Loss pred: 0.2808; Loss self: 0.0000; time: 0.17s
Val loss: 0.3921 score: 0.8837 time: 0.06s
Test loss: 0.4633 score: 0.8182 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.18s
Val loss: 0.3881 score: 0.8605 time: 0.07s
Test loss: 0.4613 score: 0.8182 time: 0.13s
Epoch 57/1000, LR 0.000283
Train loss: 0.2877;  Loss pred: 0.2877; Loss self: 0.0000; time: 0.44s
Val loss: 0.3835 score: 0.8605 time: 0.06s
Test loss: 0.4599 score: 0.8182 time: 0.06s
Epoch 58/1000, LR 0.000283
Train loss: 0.2702;  Loss pred: 0.2702; Loss self: 0.0000; time: 0.19s
Val loss: 0.3807 score: 0.8837 time: 0.07s
Test loss: 0.4605 score: 0.8182 time: 0.06s
Epoch 59/1000, LR 0.000283
Train loss: 0.2752;  Loss pred: 0.2752; Loss self: 0.0000; time: 0.15s
Val loss: 0.3789 score: 0.8837 time: 0.06s
Test loss: 0.4625 score: 0.8182 time: 0.06s
Epoch 60/1000, LR 0.000283
Train loss: 0.2716;  Loss pred: 0.2716; Loss self: 0.0000; time: 0.15s
Val loss: 0.3787 score: 0.8837 time: 0.06s
Test loss: 0.4651 score: 0.8182 time: 0.06s
Epoch 61/1000, LR 0.000283
Train loss: 0.2576;  Loss pred: 0.2576; Loss self: 0.0000; time: 0.16s
Val loss: 0.3788 score: 0.8605 time: 0.06s
Test loss: 0.4678 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.2553;  Loss pred: 0.2553; Loss self: 0.0000; time: 0.17s
Val loss: 0.3763 score: 0.8605 time: 0.06s
Test loss: 0.4665 score: 0.8182 time: 0.06s
Epoch 63/1000, LR 0.000283
Train loss: 0.2668;  Loss pred: 0.2668; Loss self: 0.0000; time: 0.28s
Val loss: 0.3738 score: 0.8605 time: 0.29s
Test loss: 0.4645 score: 0.8182 time: 0.06s
Epoch 64/1000, LR 0.000283
Train loss: 0.2472;  Loss pred: 0.2472; Loss self: 0.0000; time: 0.17s
Val loss: 0.3721 score: 0.8605 time: 0.06s
Test loss: 0.4629 score: 0.8182 time: 0.06s
Epoch 65/1000, LR 0.000283
Train loss: 0.2604;  Loss pred: 0.2604; Loss self: 0.0000; time: 0.17s
Val loss: 0.3696 score: 0.8605 time: 0.06s
Test loss: 0.4603 score: 0.8182 time: 0.06s
Epoch 66/1000, LR 0.000283
Train loss: 0.2613;  Loss pred: 0.2613; Loss self: 0.0000; time: 0.17s
Val loss: 0.3659 score: 0.8372 time: 0.06s
Test loss: 0.4563 score: 0.8182 time: 0.06s
Epoch 67/1000, LR 0.000283
Train loss: 0.2403;  Loss pred: 0.2403; Loss self: 0.0000; time: 0.17s
Val loss: 0.3619 score: 0.8605 time: 0.06s
Test loss: 0.4516 score: 0.8182 time: 0.06s
Epoch 68/1000, LR 0.000283
Train loss: 0.2327;  Loss pred: 0.2327; Loss self: 0.0000; time: 0.17s
Val loss: 0.3594 score: 0.8605 time: 0.06s
Test loss: 0.4488 score: 0.8182 time: 0.06s
Epoch 69/1000, LR 0.000283
Train loss: 0.2234;  Loss pred: 0.2234; Loss self: 0.0000; time: 0.27s
Val loss: 0.3578 score: 0.8837 time: 0.06s
Test loss: 0.4467 score: 0.8182 time: 0.06s
Epoch 70/1000, LR 0.000283
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 0.17s
Val loss: 0.3555 score: 0.8837 time: 0.06s
Test loss: 0.4436 score: 0.8182 time: 0.06s
Epoch 71/1000, LR 0.000282
Train loss: 0.2345;  Loss pred: 0.2345; Loss self: 0.0000; time: 0.16s
Val loss: 0.3538 score: 0.8837 time: 0.06s
Test loss: 0.4412 score: 0.8182 time: 0.12s
Epoch 72/1000, LR 0.000282
Train loss: 0.2209;  Loss pred: 0.2209; Loss self: 0.0000; time: 0.28s
Val loss: 0.3532 score: 0.8837 time: 0.06s
Test loss: 0.4404 score: 0.8182 time: 0.06s
Epoch 73/1000, LR 0.000282
Train loss: 0.2229;  Loss pred: 0.2229; Loss self: 0.0000; time: 0.17s
Val loss: 0.3523 score: 0.8837 time: 0.06s
Test loss: 0.4383 score: 0.8182 time: 0.06s
Epoch 74/1000, LR 0.000282
Train loss: 0.2165;  Loss pred: 0.2165; Loss self: 0.0000; time: 0.17s
Val loss: 0.3505 score: 0.8837 time: 0.06s
Test loss: 0.4353 score: 0.8182 time: 0.07s
Epoch 75/1000, LR 0.000282
Train loss: 0.2162;  Loss pred: 0.2162; Loss self: 0.0000; time: 0.17s
Val loss: 0.3483 score: 0.8837 time: 0.06s
Test loss: 0.4333 score: 0.8182 time: 0.06s
Epoch 76/1000, LR 0.000282
Train loss: 0.2119;  Loss pred: 0.2119; Loss self: 0.0000; time: 0.28s
Val loss: 0.3474 score: 0.8837 time: 0.07s
Test loss: 0.4322 score: 0.8182 time: 0.06s
Epoch 77/1000, LR 0.000282
Train loss: 0.2089;  Loss pred: 0.2089; Loss self: 0.0000; time: 0.17s
Val loss: 0.3472 score: 0.8837 time: 0.07s
Test loss: 0.4326 score: 0.8182 time: 0.37s
Epoch 78/1000, LR 0.000282
Train loss: 0.2083;  Loss pred: 0.2083; Loss self: 0.0000; time: 0.19s
Val loss: 0.3463 score: 0.8837 time: 0.07s
Test loss: 0.4301 score: 0.8182 time: 0.06s
Epoch 79/1000, LR 0.000282
Train loss: 0.2100;  Loss pred: 0.2100; Loss self: 0.0000; time: 0.17s
Val loss: 0.3463 score: 0.8837 time: 0.06s
Test loss: 0.4289 score: 0.8182 time: 0.06s
Epoch 80/1000, LR 0.000282
Train loss: 0.2133;  Loss pred: 0.2133; Loss self: 0.0000; time: 0.17s
Val loss: 0.3460 score: 0.8837 time: 0.06s
Test loss: 0.4280 score: 0.8182 time: 0.06s
Epoch 81/1000, LR 0.000281
Train loss: 0.2029;  Loss pred: 0.2029; Loss self: 0.0000; time: 0.16s
Val loss: 0.3459 score: 0.8837 time: 0.06s
Test loss: 0.4286 score: 0.8182 time: 0.06s
Epoch 82/1000, LR 0.000281
Train loss: 0.1970;  Loss pred: 0.1970; Loss self: 0.0000; time: 0.28s
Val loss: 0.3459 score: 0.8837 time: 0.06s
Test loss: 0.4309 score: 0.8182 time: 0.06s
Epoch 83/1000, LR 0.000281
Train loss: 0.1889;  Loss pred: 0.1889; Loss self: 0.0000; time: 0.16s
Val loss: 0.3476 score: 0.8837 time: 0.06s
Test loss: 0.4349 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2005;  Loss pred: 0.2005; Loss self: 0.0000; time: 0.16s
Val loss: 0.3496 score: 0.8837 time: 0.29s
Test loss: 0.4387 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.1884;  Loss pred: 0.1884; Loss self: 0.0000; time: 0.17s
Val loss: 0.3493 score: 0.8837 time: 0.07s
Test loss: 0.4400 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.1845;  Loss pred: 0.1845; Loss self: 0.0000; time: 0.16s
Val loss: 0.3482 score: 0.8837 time: 0.06s
Test loss: 0.4402 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.1759;  Loss pred: 0.1759; Loss self: 0.0000; time: 0.16s
Val loss: 0.3486 score: 0.8837 time: 0.06s
Test loss: 0.4421 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.1807;  Loss pred: 0.1807; Loss self: 0.0000; time: 0.17s
Val loss: 0.3481 score: 0.8837 time: 0.16s
Test loss: 0.4431 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.1807;  Loss pred: 0.1807; Loss self: 0.0000; time: 0.16s
Val loss: 0.3464 score: 0.8837 time: 0.06s
Test loss: 0.4425 score: 0.7955 time: 0.19s
     INFO: Early stopping counter 7 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.1773;  Loss pred: 0.1773; Loss self: 0.0000; time: 0.16s
Val loss: 0.3478 score: 0.8837 time: 0.06s
Test loss: 0.4451 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.1641;  Loss pred: 0.1641; Loss self: 0.0000; time: 0.16s
Val loss: 0.3478 score: 0.8837 time: 0.06s
Test loss: 0.4459 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.1799;  Loss pred: 0.1799; Loss self: 0.0000; time: 0.51s
Val loss: 0.3466 score: 0.8837 time: 0.07s
Test loss: 0.4450 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1685;  Loss pred: 0.1685; Loss self: 0.0000; time: 0.17s
Val loss: 0.3444 score: 0.8837 time: 0.07s
Test loss: 0.4411 score: 0.7955 time: 0.07s
Epoch 94/1000, LR 0.000280
Train loss: 0.1775;  Loss pred: 0.1775; Loss self: 0.0000; time: 0.18s
Val loss: 0.3447 score: 0.8837 time: 0.06s
Test loss: 0.4410 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1667;  Loss pred: 0.1667; Loss self: 0.0000; time: 0.17s
Val loss: 0.3471 score: 0.8837 time: 0.06s
Test loss: 0.4426 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.1567;  Loss pred: 0.1567; Loss self: 0.0000; time: 0.19s
Val loss: 0.3485 score: 0.8837 time: 0.06s
Test loss: 0.4398 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.1566;  Loss pred: 0.1566; Loss self: 0.0000; time: 0.19s
Val loss: 0.3510 score: 0.8837 time: 0.06s
Test loss: 0.4400 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.1667;  Loss pred: 0.1667; Loss self: 0.0000; time: 0.18s
Val loss: 0.3535 score: 0.8837 time: 0.07s
Test loss: 0.4408 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 99/1000, LR 0.000279
Train loss: 0.1599;  Loss pred: 0.1599; Loss self: 0.0000; time: 0.19s
Val loss: 0.3571 score: 0.8837 time: 0.07s
Test loss: 0.4427 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 100/1000, LR 0.000279
Train loss: 0.1547;  Loss pred: 0.1547; Loss self: 0.0000; time: 0.39s
Val loss: 0.3603 score: 0.8605 time: 0.07s
Test loss: 0.4473 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1507;  Loss pred: 0.1507; Loss self: 0.0000; time: 0.28s
Val loss: 0.3635 score: 0.8605 time: 0.06s
Test loss: 0.4493 score: 0.7955 time: 0.19s
     INFO: Early stopping counter 8 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1506;  Loss pred: 0.1506; Loss self: 0.0000; time: 0.17s
Val loss: 0.3646 score: 0.8605 time: 0.06s
Test loss: 0.4482 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1616;  Loss pred: 0.1616; Loss self: 0.0000; time: 0.17s
Val loss: 0.3656 score: 0.8605 time: 0.07s
Test loss: 0.4471 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1323;  Loss pred: 0.1323; Loss self: 0.0000; time: 0.17s
Val loss: 0.3686 score: 0.8605 time: 0.06s
Test loss: 0.4537 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1416;  Loss pred: 0.1416; Loss self: 0.0000; time: 0.18s
Val loss: 0.3744 score: 0.8372 time: 0.06s
Test loss: 0.4631 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1437;  Loss pred: 0.1437; Loss self: 0.0000; time: 0.19s
Val loss: 0.3812 score: 0.8605 time: 0.07s
Test loss: 0.4731 score: 0.7955 time: 0.26s
     INFO: Early stopping counter 13 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1341;  Loss pred: 0.1341; Loss self: 0.0000; time: 0.18s
Val loss: 0.3753 score: 0.8372 time: 0.07s
Test loss: 0.4638 score: 0.7727 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1309;  Loss pred: 0.1309; Loss self: 0.0000; time: 0.17s
Val loss: 0.3690 score: 0.8372 time: 0.07s
Test loss: 0.4523 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1348;  Loss pred: 0.1348; Loss self: 0.0000; time: 0.18s
Val loss: 0.3665 score: 0.8605 time: 0.07s
Test loss: 0.4444 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1427;  Loss pred: 0.1427; Loss self: 0.0000; time: 0.18s
Val loss: 0.3657 score: 0.8605 time: 0.06s
Test loss: 0.4392 score: 0.8182 time: 0.43s
     INFO: Early stopping counter 17 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1268;  Loss pred: 0.1268; Loss self: 0.0000; time: 1.12s
Val loss: 0.3707 score: 0.8372 time: 0.07s
Test loss: 0.4514 score: 0.7955 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1297;  Loss pred: 0.1297; Loss self: 0.0000; time: 0.20s
Val loss: 0.3697 score: 0.8372 time: 0.07s
Test loss: 0.4568 score: 0.7727 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1240;  Loss pred: 0.1240; Loss self: 0.0000; time: 0.18s
Val loss: 0.3683 score: 0.8372 time: 0.06s
Test loss: 0.4550 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.1685,   Val_Loss: 0.3444,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8837,   Val_Loss: 0.3444,   Test_Precision: 0.9333,   Test_Recall: 0.6364,   Test_accuracy: 0.7568,   Test_Score: 0.7955,   Test_loss: 0.4411


[0.06591607199970895, 0.06817726799999946, 0.07005674799984263, 0.06766368199987483, 0.17381003600075928, 0.06136203999994905, 0.062295979999362316, 0.06546464299935906, 0.06661496800006717, 0.06489528700058145, 0.06424749300003896, 0.06385992899959092, 0.06769902100040781, 0.07471809500020754, 0.06556483599979401, 0.0659469940001145, 0.06724114999997255, 0.06917075500041392, 0.06610225499935041, 0.06726669100044091, 0.06828087499980029, 0.0682031359992834, 0.06798686399997678, 0.06657617699966067, 0.06352621899986843, 0.23693184800049494, 0.06788920400049392, 0.06893622400002641, 0.06677409100029763, 0.06739371399999072, 0.07208560700019007, 0.06850123299955158, 0.06800084499991499, 0.066757389000486, 0.07255278599950543, 0.06737409700053831, 0.06752685799983738, 0.07118423200063262, 0.07145388400022057, 0.06755969900041237, 0.06918337299975974, 0.2810418179997214, 0.061408673000187264, 0.06243992999952752, 0.06164740299936966, 0.06451223300064157, 0.06310639000002993, 0.06574583600013284, 0.31267133300025307, 0.06756266100001085, 0.06637742600014462, 0.06825081200076966, 0.06569890700029646, 0.06663645399930829, 0.06673422200037749, 0.1342714430002161, 0.06847112800005561, 0.06709129299997585, 0.0632612009994773, 0.061709057000371104, 0.06250218600052904, 0.061246041000231344, 0.06287892999989708, 0.0634588379998604, 0.0634538229996906, 0.06116769700020086, 0.06244881299971894, 0.06421606399999291, 0.06356517499989423, 0.06453649200011569, 0.12779223999950773, 0.06608748799953901, 0.0689109049999388, 0.0710869949998596, 0.06532244300069578, 0.06934541200007516, 0.3758377059994018, 0.06988696199914557, 0.06432876399958332, 0.06256094200034568, 0.06271078299960209, 0.06389529399984895, 0.0662608310003634, 0.06837720200019248, 0.0645508620000328, 0.062243012999715575, 0.06345873700047377, 0.06277281500024401, 0.1991693399995711, 0.0649816989998726, 0.06629433199941559, 0.06958622999991348, 0.07466290199954528, 0.06707709700003761, 0.0679971879999357, 0.06982141799926467, 0.06945081400044728, 0.07040255700030684, 0.07290697299958993, 0.0705223479999404, 0.19827589099986653, 0.06165821099966706, 0.06830467999952816, 0.06943916199998057, 0.07216930699996738, 0.26977394000005006, 0.06847857399952773, 0.06944954600021447, 0.07272309100062557, 0.4375212590002775, 0.07378885399975843, 0.0703821010001775, 0.06879922299958707]
[0.0014980925454479307, 0.0015494833636363512, 0.0015921988181782415, 0.0015378109545426096, 0.003950228090926347, 0.00139459181818066, 0.0014158177272582345, 0.0014878327954399786, 0.001513976545456072, 0.001474892886376851, 0.001460170295455431, 0.0014513620227179754, 0.001538614113645632, 0.0016981385227319895, 0.0014901099090862276, 0.0014987953181844205, 0.0015282079545448307, 0.0015720626136457709, 0.0015023239772579639, 0.0015287884318282026, 0.0015518380681772794, 0.0015500712727109865, 0.0015451559999994722, 0.00151309493181047, 0.0014437777045424643, 0.005384814727283976, 0.00154293645455668, 0.001566732363636964, 0.0015175929772794916, 0.0015316753181816073, 0.0016383092500043197, 0.001556846204535263, 0.0015454737499980679, 0.001517213386374682, 0.0016489269545342143, 0.0015312294772849616, 0.0015347013181781222, 0.0016178234545598323, 0.001623951909095922, 0.0015354477045548265, 0.001572349386358176, 0.006387314045448214, 0.001395651659095165, 0.00141908931817108, 0.001401077340894765, 0.0014661871136509449, 0.0014342361363643167, 0.0014942235454575646, 0.007106166659096661, 0.0015355150227275192, 0.0015085778636396506, 0.0015511548181993103, 0.0014931569772794649, 0.001514464863620643, 0.0015166868636449428, 0.003051623704550366, 0.001556162000001264, 0.0015248021136358147, 0.0014377545681699385, 0.0014024785681902524, 0.0014205042272847509, 0.001391955477277985, 0.0014290665909067518, 0.0014422463181786456, 0.001442132340902059, 0.0013901749318227469, 0.0014192912045390667, 0.001459455999999839, 0.0014446630681794143, 0.0014667384545480838, 0.002904369090897903, 0.0015019883636258865, 0.0015661569318167908, 0.0016156135227240818, 0.0014846009772885407, 0.001576032090910799, 0.00854176604544095, 0.0015883400454351265, 0.0014620173636268935, 0.0014218395909169471, 0.0014252450681727748, 0.0014521657727238396, 0.0015059279772809862, 0.0015540273181861928, 0.0014670650454552908, 0.0014146139318117175, 0.0014422440227380403, 0.0014266548863691821, 0.004526575909081162, 0.0014768567954516502, 0.0015066893636230816, 0.001581505227270761, 0.001696884136353302, 0.001524479477273582, 0.0015453906363621752, 0.001586850409074197, 0.0015784275909192563, 0.0016000581136433373, 0.0016569766590815891, 0.0016027806363622817, 0.004506270249996966, 0.0014013229772651605, 0.0015523790908983674, 0.0015781627727268312, 0.0016402115227265313, 0.006131225909092047, 0.0015563312272619937, 0.001578398772732147, 0.0016527975227414902, 0.009943664977279033, 0.0016770194090854188, 0.0015995932045494885, 0.0015636187045360698]
[667.515503657352, 645.3764031729806, 628.0622674649248, 650.2749879925452, 253.1499389356768, 717.0556911086485, 706.3056075279721, 672.1185358091816, 660.5122140110558, 678.0153387657531, 684.8516252606669, 689.0079693054757, 649.9355433771332, 588.8801099636946, 671.0914368814746, 667.2025111549983, 654.3612058987385, 636.1069790222284, 665.6353856677415, 654.1127465257892, 644.3971317023792, 645.13162562587, 647.183844220481, 660.8970653305049, 692.6274016102096, 185.7074106808473, 648.114831331354, 638.2711069289659, 658.938210028256, 652.8798813492614, 610.385371380503, 642.3242045918798, 647.050782972697, 659.1030694696534, 606.455002297223, 653.0699773185597, 651.5925855769265, 618.1144161196959, 615.781781713422, 651.2758441942058, 635.9909627440802, 156.56033081896595, 716.5111677281452, 704.6772794321349, 713.7364732209384, 682.041187437465, 697.2352562074797, 669.2439046620549, 140.7228465040138, 651.2472917547302, 662.8759602685427, 644.6809746308046, 669.7219483392845, 660.2992410198889, 659.3318792230936, 327.69440036426204, 642.6066180765163, 655.8228055019872, 695.5290020555183, 713.0233735338946, 703.975377751229, 718.413782857145, 699.7574545252602, 693.362837814597, 693.417636951749, 719.3339320892777, 704.5770429647403, 685.1868093317718, 692.2029240078898, 681.7848109860251, 344.30885631372837, 665.7841193828841, 638.5056182332692, 618.959909616195, 673.5816662510821, 634.5048465492182, 117.07180864942282, 629.5881054400095, 683.9864045932082, 703.3142179949414, 701.6337206358784, 688.6266146627953, 664.0423812336234, 643.4893314276904, 681.6330353571057, 706.9066531242802, 693.3639413540724, 700.9403672565738, 220.9175368060905, 677.1137208968061, 663.706815846457, 632.3090071132565, 589.315427362681, 655.961601915708, 647.0855824220497, 630.1791235529389, 633.5418905200539, 624.9773001825521, 603.5088029268131, 623.9156983263971, 221.91301109840742, 713.6113631360078, 644.1725515777828, 633.6481998445245, 609.677463024827, 163.0995195458532, 642.5367444173627, 633.5534576405169, 605.0347887388547, 100.5665418419636, 596.2960205364355, 625.1589448841409, 639.5421064604768]
Elapsed: 0.08460115670794167~0.06163172427869067
Time per graph: 0.001922753561544129~0.0014007210063338787
Speed: 615.7701228092509~140.96100532044971
Total Time: 0.0694
best val loss: 0.34441249280474906 test_score: 0.7955

Testing...
Test loss: 0.5461 score: 0.7955 time: 0.06s
test Score 0.7955
Epoch Time List: [0.28946014600023773, 0.2929908019996219, 0.29777926599945204, 0.5267754069991497, 0.39736853599879396, 0.271057472999928, 0.27063809399896854, 0.2791777929996897, 0.28923020000002, 0.2862319440000647, 0.27968796999903134, 0.4477909599991108, 0.2909866399995735, 0.5017175469993163, 0.28507440599878464, 0.28539106200150854, 0.29058691099999123, 0.295430973999828, 0.5097417519991723, 0.374978949999786, 0.2941008259995215, 0.29732383800001116, 0.3073254379996797, 0.29596549500092806, 0.2782364120002967, 0.6027024229997551, 0.3113775500005431, 0.3124169639986576, 0.3092012850001993, 0.30435634900095465, 0.31090348299858306, 0.29883839300055115, 0.2963336019993221, 0.4937979410015032, 0.2985385190004308, 0.2998377289995915, 0.29509157100073935, 0.29301920699981565, 0.30014524800026265, 0.2942576729992652, 0.2989058000002842, 0.5113060809999297, 0.2818638920007288, 0.2822085130001142, 0.28362122800081124, 0.29023633700217033, 0.29148358500060567, 0.27413094300027296, 0.7029576459999589, 0.29270030400039104, 0.28774286799944093, 0.2931235269998069, 0.28681857100036723, 0.289351999000246, 0.28904176500145695, 0.37740212000062456, 0.5685437600004661, 0.32303841700013436, 0.2708085589993061, 0.2706425799997305, 0.2786599370001568, 0.2846582820002368, 0.6333631770003194, 0.28818735200002266, 0.2909317630010264, 0.28728936099832936, 0.286456965999605, 0.2889930960000129, 0.3889500280001812, 0.2939744060004159, 0.3383712580007341, 0.4049582369998461, 0.2935332039996865, 0.29447582800003147, 0.29161565599952155, 0.41294112200012023, 0.6103350839994164, 0.3198269659988, 0.29462957499890763, 0.2880557369999224, 0.276596691001032, 0.3993376159996842, 0.2822034410000924, 0.5145550980005282, 0.29104779299996153, 0.2765295929993954, 0.2752145959993868, 0.3839714029991228, 0.4113264079996952, 0.28443816500112007, 0.28560387899869966, 0.6389193560007698, 0.3099507189999713, 0.30414823400133173, 0.2977245420006511, 0.3190402570007791, 0.3139368190004461, 0.3155934980004531, 0.31993006900029286, 0.5218939979995412, 0.5386832990006951, 0.28287206499953754, 0.3065953100003753, 0.2943921779997254, 0.31279106499914633, 0.5231791889991655, 0.31140595599936205, 0.30261627600066277, 0.31317889300044044, 0.6722106009992785, 1.2544176379997225, 0.3324633510010244, 0.3040275589992234]
Total Epoch List: [113]
Total Time List: [0.06943193200004316]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f465300>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7110 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7136 score: 0.4884 time: 0.08s
Epoch 2/1000, LR 0.000015
Train loss: 0.7186;  Loss pred: 0.7186; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7077 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7106 score: 0.4884 time: 0.10s
Epoch 3/1000, LR 0.000045
Train loss: 0.7145;  Loss pred: 0.7145; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7050 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7088 score: 0.4884 time: 0.06s
Epoch 4/1000, LR 0.000075
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7027 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7076 score: 0.4884 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7001 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7060 score: 0.4884 time: 0.07s
Epoch 6/1000, LR 0.000135
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7050 score: 0.4884 time: 0.07s
Epoch 7/1000, LR 0.000165
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7048 score: 0.4884 time: 0.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7053 score: 0.4884 time: 0.07s
Epoch 9/1000, LR 0.000225
Train loss: 0.6513;  Loss pred: 0.6513; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7055 score: 0.4884 time: 0.07s
Epoch 10/1000, LR 0.000255
Train loss: 0.6372;  Loss pred: 0.6372; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7057 score: 0.4884 time: 0.07s
Epoch 11/1000, LR 0.000285
Train loss: 0.6217;  Loss pred: 0.6217; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7043 score: 0.4884 time: 0.07s
Epoch 12/1000, LR 0.000285
Train loss: 0.6280;  Loss pred: 0.6280; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7026 score: 0.4884 time: 0.07s
Epoch 13/1000, LR 0.000285
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6827 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7021 score: 0.4884 time: 0.07s
Epoch 14/1000, LR 0.000285
Train loss: 0.6031;  Loss pred: 0.6031; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6788 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7005 score: 0.4884 time: 0.07s
Epoch 15/1000, LR 0.000285
Train loss: 0.5806;  Loss pred: 0.5806; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6737 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6976 score: 0.4884 time: 0.06s
Epoch 16/1000, LR 0.000285
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6670 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.07s
Epoch 17/1000, LR 0.000285
Train loss: 0.5761;  Loss pred: 0.5761; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6602 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4884 time: 0.07s
Epoch 18/1000, LR 0.000285
Train loss: 0.5592;  Loss pred: 0.5592; Loss self: 0.0000; time: 0.19s
Val loss: 0.6539 score: 0.5682 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.4884 time: 0.07s
Epoch 19/1000, LR 0.000285
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.20s
Val loss: 0.6473 score: 0.5682 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6786 score: 0.4884 time: 0.07s
Epoch 20/1000, LR 0.000285
Train loss: 0.5477;  Loss pred: 0.5477; Loss self: 0.0000; time: 0.19s
Val loss: 0.6400 score: 0.5909 time: 0.07s
Test loss: 0.6731 score: 0.5116 time: 0.07s
Epoch 21/1000, LR 0.000285
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.32s
Val loss: 0.6319 score: 0.5909 time: 0.07s
Test loss: 0.6666 score: 0.5116 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.5274;  Loss pred: 0.5274; Loss self: 0.0000; time: 0.18s
Val loss: 0.6233 score: 0.6364 time: 0.07s
Test loss: 0.6592 score: 0.5349 time: 0.08s
Epoch 23/1000, LR 0.000285
Train loss: 0.5002;  Loss pred: 0.5002; Loss self: 0.0000; time: 0.24s
Val loss: 0.6163 score: 0.6364 time: 0.07s
Test loss: 0.6529 score: 0.5814 time: 0.07s
Epoch 24/1000, LR 0.000285
Train loss: 0.4865;  Loss pred: 0.4865; Loss self: 0.0000; time: 0.20s
Val loss: 0.6090 score: 0.6591 time: 0.08s
Test loss: 0.6465 score: 0.5814 time: 0.29s
Epoch 25/1000, LR 0.000285
Train loss: 0.4950;  Loss pred: 0.4950; Loss self: 0.0000; time: 0.20s
Val loss: 0.6025 score: 0.6818 time: 0.06s
Test loss: 0.6412 score: 0.5814 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4648;  Loss pred: 0.4648; Loss self: 0.0000; time: 0.18s
Val loss: 0.5970 score: 0.6818 time: 0.07s
Test loss: 0.6367 score: 0.5814 time: 0.07s
Epoch 27/1000, LR 0.000285
Train loss: 0.4610;  Loss pred: 0.4610; Loss self: 0.0000; time: 0.20s
Val loss: 0.5912 score: 0.7045 time: 0.07s
Test loss: 0.6321 score: 0.6047 time: 0.26s
Epoch 28/1000, LR 0.000285
Train loss: 0.4642;  Loss pred: 0.4642; Loss self: 0.0000; time: 0.21s
Val loss: 0.5867 score: 0.7045 time: 0.07s
Test loss: 0.6287 score: 0.6279 time: 0.07s
Epoch 29/1000, LR 0.000285
Train loss: 0.4661;  Loss pred: 0.4661; Loss self: 0.0000; time: 0.21s
Val loss: 0.5824 score: 0.7045 time: 0.07s
Test loss: 0.6257 score: 0.6279 time: 0.07s
Epoch 30/1000, LR 0.000285
Train loss: 0.4546;  Loss pred: 0.4546; Loss self: 0.0000; time: 0.20s
Val loss: 0.5784 score: 0.7045 time: 0.07s
Test loss: 0.6241 score: 0.6512 time: 0.07s
Epoch 31/1000, LR 0.000285
Train loss: 0.4432;  Loss pred: 0.4432; Loss self: 0.0000; time: 0.24s
Val loss: 0.5748 score: 0.7045 time: 0.31s
Test loss: 0.6228 score: 0.6512 time: 0.07s
Epoch 32/1000, LR 0.000285
Train loss: 0.4115;  Loss pred: 0.4115; Loss self: 0.0000; time: 0.21s
Val loss: 0.5721 score: 0.7045 time: 0.07s
Test loss: 0.6224 score: 0.6512 time: 0.07s
Epoch 33/1000, LR 0.000285
Train loss: 0.4285;  Loss pred: 0.4285; Loss self: 0.0000; time: 0.34s
Val loss: 0.5695 score: 0.7045 time: 0.07s
Test loss: 0.6218 score: 0.6512 time: 0.07s
Epoch 34/1000, LR 0.000285
Train loss: 0.4247;  Loss pred: 0.4247; Loss self: 0.0000; time: 0.20s
Val loss: 0.5667 score: 0.7045 time: 0.07s
Test loss: 0.6209 score: 0.6512 time: 0.07s
Epoch 35/1000, LR 0.000285
Train loss: 0.4015;  Loss pred: 0.4015; Loss self: 0.0000; time: 0.19s
Val loss: 0.5640 score: 0.7045 time: 0.07s
Test loss: 0.6201 score: 0.6512 time: 0.07s
Epoch 36/1000, LR 0.000285
Train loss: 0.4061;  Loss pred: 0.4061; Loss self: 0.0000; time: 0.20s
Val loss: 0.5632 score: 0.7045 time: 0.07s
Test loss: 0.6218 score: 0.6279 time: 0.07s
Epoch 37/1000, LR 0.000285
Train loss: 0.3896;  Loss pred: 0.3896; Loss self: 0.0000; time: 0.18s
Val loss: 0.5620 score: 0.7045 time: 0.06s
Test loss: 0.6225 score: 0.6279 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.4044;  Loss pred: 0.4044; Loss self: 0.0000; time: 0.20s
Val loss: 0.5598 score: 0.7045 time: 0.07s
Test loss: 0.6221 score: 0.6279 time: 0.07s
Epoch 39/1000, LR 0.000284
Train loss: 0.3796;  Loss pred: 0.3796; Loss self: 0.0000; time: 0.20s
Val loss: 0.5581 score: 0.7045 time: 0.07s
Test loss: 0.6220 score: 0.6279 time: 0.32s
Epoch 40/1000, LR 0.000284
Train loss: 0.4021;  Loss pred: 0.4021; Loss self: 0.0000; time: 0.35s
Val loss: 0.5563 score: 0.7045 time: 0.07s
Test loss: 0.6218 score: 0.6279 time: 0.09s
Epoch 41/1000, LR 0.000284
Train loss: 0.3922;  Loss pred: 0.3922; Loss self: 0.0000; time: 0.20s
Val loss: 0.5511 score: 0.7045 time: 0.07s
Test loss: 0.6175 score: 0.6279 time: 0.07s
Epoch 42/1000, LR 0.000284
Train loss: 0.3908;  Loss pred: 0.3908; Loss self: 0.0000; time: 0.20s
Val loss: 0.5472 score: 0.7045 time: 0.07s
Test loss: 0.6148 score: 0.6279 time: 0.07s
Epoch 43/1000, LR 0.000284
Train loss: 0.3606;  Loss pred: 0.3606; Loss self: 0.0000; time: 0.21s
Val loss: 0.5427 score: 0.7045 time: 0.08s
Test loss: 0.6113 score: 0.6279 time: 0.07s
Epoch 44/1000, LR 0.000284
Train loss: 0.3615;  Loss pred: 0.3615; Loss self: 0.0000; time: 0.42s
Val loss: 0.5403 score: 0.7045 time: 0.07s
Test loss: 0.6102 score: 0.6279 time: 0.07s
Epoch 45/1000, LR 0.000284
Train loss: 0.3534;  Loss pred: 0.3534; Loss self: 0.0000; time: 0.19s
Val loss: 0.5395 score: 0.7045 time: 0.07s
Test loss: 0.6113 score: 0.6279 time: 0.07s
Epoch 46/1000, LR 0.000284
Train loss: 0.3652;  Loss pred: 0.3652; Loss self: 0.0000; time: 0.20s
Val loss: 0.5372 score: 0.7045 time: 0.18s
Test loss: 0.6109 score: 0.6279 time: 0.07s
Epoch 47/1000, LR 0.000284
Train loss: 0.3506;  Loss pred: 0.3506; Loss self: 0.0000; time: 0.20s
Val loss: 0.5351 score: 0.7045 time: 0.07s
Test loss: 0.6112 score: 0.6047 time: 0.07s
Epoch 48/1000, LR 0.000284
Train loss: 0.3431;  Loss pred: 0.3431; Loss self: 0.0000; time: 0.21s
Val loss: 0.5347 score: 0.7045 time: 0.07s
Test loss: 0.6130 score: 0.6047 time: 0.07s
Epoch 49/1000, LR 0.000284
Train loss: 0.3411;  Loss pred: 0.3411; Loss self: 0.0000; time: 0.21s
Val loss: 0.5372 score: 0.7045 time: 0.15s
Test loss: 0.6181 score: 0.6047 time: 0.21s
     INFO: Early stopping counter 1 of 20
Epoch 50/1000, LR 0.000284
Train loss: 0.3367;  Loss pred: 0.3367; Loss self: 0.0000; time: 0.22s
Val loss: 0.5369 score: 0.7045 time: 0.07s
Test loss: 0.6194 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 51/1000, LR 0.000284
Train loss: 0.3347;  Loss pred: 0.3347; Loss self: 0.0000; time: 0.24s
Val loss: 0.5383 score: 0.7045 time: 0.07s
Test loss: 0.6228 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 3 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.3310;  Loss pred: 0.3310; Loss self: 0.0000; time: 0.30s
Val loss: 0.5363 score: 0.7045 time: 0.19s
Test loss: 0.6218 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 53/1000, LR 0.000284
Train loss: 0.3398;  Loss pred: 0.3398; Loss self: 0.0000; time: 0.21s
Val loss: 0.5356 score: 0.7045 time: 0.08s
Test loss: 0.6219 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.3102;  Loss pred: 0.3102; Loss self: 0.0000; time: 0.20s
Val loss: 0.5368 score: 0.7045 time: 0.31s
Test loss: 0.6250 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.3188;  Loss pred: 0.3188; Loss self: 0.0000; time: 0.19s
Val loss: 0.5379 score: 0.7045 time: 0.07s
Test loss: 0.6270 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.3145;  Loss pred: 0.3145; Loss self: 0.0000; time: 0.19s
Val loss: 0.5396 score: 0.7045 time: 0.07s
Test loss: 0.6290 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.3021;  Loss pred: 0.3021; Loss self: 0.0000; time: 0.19s
Val loss: 0.5391 score: 0.7045 time: 0.07s
Test loss: 0.6293 score: 0.6047 time: 0.21s
     INFO: Early stopping counter 9 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.3142;  Loss pred: 0.3142; Loss self: 0.0000; time: 0.19s
Val loss: 0.5423 score: 0.7045 time: 0.07s
Test loss: 0.6332 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.2983;  Loss pred: 0.2983; Loss self: 0.0000; time: 0.19s
Val loss: 0.5448 score: 0.7045 time: 0.07s
Test loss: 0.6353 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.2966;  Loss pred: 0.2966; Loss self: 0.0000; time: 0.20s
Val loss: 0.5469 score: 0.7045 time: 0.07s
Test loss: 0.6372 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.2971;  Loss pred: 0.2971; Loss self: 0.0000; time: 0.21s
Val loss: 0.5497 score: 0.7045 time: 0.29s
Test loss: 0.6407 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.2931;  Loss pred: 0.2931; Loss self: 0.0000; time: 0.20s
Val loss: 0.5476 score: 0.7045 time: 0.07s
Test loss: 0.6375 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.2851;  Loss pred: 0.2851; Loss self: 0.0000; time: 0.20s
Val loss: 0.5456 score: 0.7273 time: 0.07s
Test loss: 0.6343 score: 0.6047 time: 0.09s
     INFO: Early stopping counter 15 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.2895;  Loss pred: 0.2895; Loss self: 0.0000; time: 0.21s
Val loss: 0.5457 score: 0.7273 time: 0.07s
Test loss: 0.6342 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.2724;  Loss pred: 0.2724; Loss self: 0.0000; time: 0.33s
Val loss: 0.5470 score: 0.7273 time: 0.07s
Test loss: 0.6345 score: 0.6047 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.2990;  Loss pred: 0.2990; Loss self: 0.0000; time: 0.20s
Val loss: 0.5477 score: 0.7273 time: 0.07s
Test loss: 0.6358 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.2833;  Loss pred: 0.2833; Loss self: 0.0000; time: 0.21s
Val loss: 0.5483 score: 0.7273 time: 0.20s
Test loss: 0.6365 score: 0.6512 time: 0.10s
     INFO: Early stopping counter 19 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.2727;  Loss pred: 0.2727; Loss self: 0.0000; time: 0.19s
Val loss: 0.5505 score: 0.7273 time: 0.07s
Test loss: 0.6383 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 047,   Train_Loss: 0.3431,   Val_Loss: 0.5347,   Val_Precision: 1.0000,   Val_Recall: 0.4091,   Val_accuracy: 0.5806,   Val_Score: 0.7045,   Val_Loss: 0.5347,   Test_Precision: 1.0000,   Test_Recall: 0.2273,   Test_accuracy: 0.3704,   Test_Score: 0.6047,   Test_loss: 0.6130


[0.06591607199970895, 0.06817726799999946, 0.07005674799984263, 0.06766368199987483, 0.17381003600075928, 0.06136203999994905, 0.062295979999362316, 0.06546464299935906, 0.06661496800006717, 0.06489528700058145, 0.06424749300003896, 0.06385992899959092, 0.06769902100040781, 0.07471809500020754, 0.06556483599979401, 0.0659469940001145, 0.06724114999997255, 0.06917075500041392, 0.06610225499935041, 0.06726669100044091, 0.06828087499980029, 0.0682031359992834, 0.06798686399997678, 0.06657617699966067, 0.06352621899986843, 0.23693184800049494, 0.06788920400049392, 0.06893622400002641, 0.06677409100029763, 0.06739371399999072, 0.07208560700019007, 0.06850123299955158, 0.06800084499991499, 0.066757389000486, 0.07255278599950543, 0.06737409700053831, 0.06752685799983738, 0.07118423200063262, 0.07145388400022057, 0.06755969900041237, 0.06918337299975974, 0.2810418179997214, 0.061408673000187264, 0.06243992999952752, 0.06164740299936966, 0.06451223300064157, 0.06310639000002993, 0.06574583600013284, 0.31267133300025307, 0.06756266100001085, 0.06637742600014462, 0.06825081200076966, 0.06569890700029646, 0.06663645399930829, 0.06673422200037749, 0.1342714430002161, 0.06847112800005561, 0.06709129299997585, 0.0632612009994773, 0.061709057000371104, 0.06250218600052904, 0.061246041000231344, 0.06287892999989708, 0.0634588379998604, 0.0634538229996906, 0.06116769700020086, 0.06244881299971894, 0.06421606399999291, 0.06356517499989423, 0.06453649200011569, 0.12779223999950773, 0.06608748799953901, 0.0689109049999388, 0.0710869949998596, 0.06532244300069578, 0.06934541200007516, 0.3758377059994018, 0.06988696199914557, 0.06432876399958332, 0.06256094200034568, 0.06271078299960209, 0.06389529399984895, 0.0662608310003634, 0.06837720200019248, 0.0645508620000328, 0.062243012999715575, 0.06345873700047377, 0.06277281500024401, 0.1991693399995711, 0.0649816989998726, 0.06629433199941559, 0.06958622999991348, 0.07466290199954528, 0.06707709700003761, 0.0679971879999357, 0.06982141799926467, 0.06945081400044728, 0.07040255700030684, 0.07290697299958993, 0.0705223479999404, 0.19827589099986653, 0.06165821099966706, 0.06830467999952816, 0.06943916199998057, 0.07216930699996738, 0.26977394000005006, 0.06847857399952773, 0.06944954600021447, 0.07272309100062557, 0.4375212590002775, 0.07378885399975843, 0.0703821010001775, 0.06879922299958707, 0.08837106000009953, 0.10109133500009193, 0.06485065699962433, 0.06738126200070838, 0.07322028199996566, 0.07090012900061993, 0.0698657569992065, 0.0730730669993136, 0.07111409600020124, 0.07607479499984038, 0.07173776600029669, 0.07505411300007836, 0.07652822399995785, 0.07748461600021983, 0.07013477700002113, 0.08058685700052592, 0.07436394800060953, 0.07654399199964246, 0.07040857599986339, 0.07066148100057035, 0.06671318600001541, 0.08719595700040372, 0.07411004200002935, 0.2970378259997233, 0.06704289499975857, 0.07553532000019914, 0.2650830890006546, 0.07949305600050138, 0.07513159200061637, 0.07389512499958073, 0.07559007400050177, 0.07143001900021773, 0.07531674099936936, 0.07304941099937423, 0.0753747190001377, 0.07174838699938846, 0.0701000839999324, 0.07903885500036267, 0.32152356400001736, 0.09845714299990505, 0.07617841899991618, 0.07932534999963536, 0.07754755000041769, 0.07142530500004796, 0.0715199530004611, 0.07265401800032123, 0.07342315799996868, 0.07260862400016777, 0.2148123350007154, 0.08217332399999577, 0.07399009900018427, 0.0748927630002072, 0.07737829600046098, 0.07205890899967926, 0.07127893899996707, 0.07027425400065113, 0.21344456800034095, 0.07520197100075166, 0.07312045799972111, 0.07484958199984248, 0.07646205499986536, 0.07494926100025623, 0.09401583899943944, 0.07121029399968393, 0.07219407299999148, 0.07445424300021841, 0.10830919500040181, 0.07355686200025957]
[0.0014980925454479307, 0.0015494833636363512, 0.0015921988181782415, 0.0015378109545426096, 0.003950228090926347, 0.00139459181818066, 0.0014158177272582345, 0.0014878327954399786, 0.001513976545456072, 0.001474892886376851, 0.001460170295455431, 0.0014513620227179754, 0.001538614113645632, 0.0016981385227319895, 0.0014901099090862276, 0.0014987953181844205, 0.0015282079545448307, 0.0015720626136457709, 0.0015023239772579639, 0.0015287884318282026, 0.0015518380681772794, 0.0015500712727109865, 0.0015451559999994722, 0.00151309493181047, 0.0014437777045424643, 0.005384814727283976, 0.00154293645455668, 0.001566732363636964, 0.0015175929772794916, 0.0015316753181816073, 0.0016383092500043197, 0.001556846204535263, 0.0015454737499980679, 0.001517213386374682, 0.0016489269545342143, 0.0015312294772849616, 0.0015347013181781222, 0.0016178234545598323, 0.001623951909095922, 0.0015354477045548265, 0.001572349386358176, 0.006387314045448214, 0.001395651659095165, 0.00141908931817108, 0.001401077340894765, 0.0014661871136509449, 0.0014342361363643167, 0.0014942235454575646, 0.007106166659096661, 0.0015355150227275192, 0.0015085778636396506, 0.0015511548181993103, 0.0014931569772794649, 0.001514464863620643, 0.0015166868636449428, 0.003051623704550366, 0.001556162000001264, 0.0015248021136358147, 0.0014377545681699385, 0.0014024785681902524, 0.0014205042272847509, 0.001391955477277985, 0.0014290665909067518, 0.0014422463181786456, 0.001442132340902059, 0.0013901749318227469, 0.0014192912045390667, 0.001459455999999839, 0.0014446630681794143, 0.0014667384545480838, 0.002904369090897903, 0.0015019883636258865, 0.0015661569318167908, 0.0016156135227240818, 0.0014846009772885407, 0.001576032090910799, 0.00854176604544095, 0.0015883400454351265, 0.0014620173636268935, 0.0014218395909169471, 0.0014252450681727748, 0.0014521657727238396, 0.0015059279772809862, 0.0015540273181861928, 0.0014670650454552908, 0.0014146139318117175, 0.0014422440227380403, 0.0014266548863691821, 0.004526575909081162, 0.0014768567954516502, 0.0015066893636230816, 0.001581505227270761, 0.001696884136353302, 0.001524479477273582, 0.0015453906363621752, 0.001586850409074197, 0.0015784275909192563, 0.0016000581136433373, 0.0016569766590815891, 0.0016027806363622817, 0.004506270249996966, 0.0014013229772651605, 0.0015523790908983674, 0.0015781627727268312, 0.0016402115227265313, 0.006131225909092047, 0.0015563312272619937, 0.001578398772732147, 0.0016527975227414902, 0.009943664977279033, 0.0016770194090854188, 0.0015995932045494885, 0.0015636187045360698, 0.0020551409302348726, 0.0023509612790719053, 0.001508154813944752, 0.0015670060930397297, 0.001702797255813155, 0.0016488402093167425, 0.0016247850464931742, 0.0016993736511468278, 0.0016538161860511916, 0.0017691812790660553, 0.0016683201395417833, 0.0017454444883739154, 0.0017797261395339034, 0.0018019678139586006, 0.0016310413255818867, 0.001874112953500603, 0.0017293941395490587, 0.0017800928372009874, 0.0016374087441828695, 0.0016432902558272174, 0.0015514694418608235, 0.002027812953497761, 0.0017234893488378919, 0.0069078564185982155, 0.0015591370930176413, 0.0017566353488418403, 0.006164723000015223, 0.0018486757209418927, 0.0017472463255957295, 0.001718491279060017, 0.0017579086976860878, 0.0016611632325632029, 0.0017515521162644037, 0.0016988235116133542, 0.0017529004418636676, 0.0016685671395206619, 0.0016302345116263348, 0.0018381129069851784, 0.007477292186046916, 0.0022897009999977916, 0.0017715911395329343, 0.0018447755813868689, 0.0018034313953585509, 0.0016610536046522781, 0.0016632547209409559, 0.0016896283255888659, 0.001707515302324853, 0.0016885726511666921, 0.004995635697691056, 0.0019110075348836226, 0.0017206999767484714, 0.0017416921627955162, 0.0017994952558246739, 0.0016757885813878898, 0.0016576497441852806, 0.0016342849767593285, 0.004963827162798627, 0.0017488830465291082, 0.0017004757674353747, 0.001740687953484709, 0.001778187325578264, 0.0017430060697734007, 0.00218641486045208, 0.0016560533488298587, 0.00167893193023236, 0.0017314940232608933, 0.0025188184883814376, 0.001710624697680455]
[667.515503657352, 645.3764031729806, 628.0622674649248, 650.2749879925452, 253.1499389356768, 717.0556911086485, 706.3056075279721, 672.1185358091816, 660.5122140110558, 678.0153387657531, 684.8516252606669, 689.0079693054757, 649.9355433771332, 588.8801099636946, 671.0914368814746, 667.2025111549983, 654.3612058987385, 636.1069790222284, 665.6353856677415, 654.1127465257892, 644.3971317023792, 645.13162562587, 647.183844220481, 660.8970653305049, 692.6274016102096, 185.7074106808473, 648.114831331354, 638.2711069289659, 658.938210028256, 652.8798813492614, 610.385371380503, 642.3242045918798, 647.050782972697, 659.1030694696534, 606.455002297223, 653.0699773185597, 651.5925855769265, 618.1144161196959, 615.781781713422, 651.2758441942058, 635.9909627440802, 156.56033081896595, 716.5111677281452, 704.6772794321349, 713.7364732209384, 682.041187437465, 697.2352562074797, 669.2439046620549, 140.7228465040138, 651.2472917547302, 662.8759602685427, 644.6809746308046, 669.7219483392845, 660.2992410198889, 659.3318792230936, 327.69440036426204, 642.6066180765163, 655.8228055019872, 695.5290020555183, 713.0233735338946, 703.975377751229, 718.413782857145, 699.7574545252602, 693.362837814597, 693.417636951749, 719.3339320892777, 704.5770429647403, 685.1868093317718, 692.2029240078898, 681.7848109860251, 344.30885631372837, 665.7841193828841, 638.5056182332692, 618.959909616195, 673.5816662510821, 634.5048465492182, 117.07180864942282, 629.5881054400095, 683.9864045932082, 703.3142179949414, 701.6337206358784, 688.6266146627953, 664.0423812336234, 643.4893314276904, 681.6330353571057, 706.9066531242802, 693.3639413540724, 700.9403672565738, 220.9175368060905, 677.1137208968061, 663.706815846457, 632.3090071132565, 589.315427362681, 655.961601915708, 647.0855824220497, 630.1791235529389, 633.5418905200539, 624.9773001825521, 603.5088029268131, 623.9156983263971, 221.91301109840742, 713.6113631360078, 644.1725515777828, 633.6481998445245, 609.677463024827, 163.0995195458532, 642.5367444173627, 633.5534576405169, 605.0347887388547, 100.5665418419636, 596.2960205364355, 625.1589448841409, 639.5421064604768, 486.5846352861623, 425.35792014182914, 663.0619023682226, 638.1596117856615, 587.2689755554364, 606.4869077970792, 615.46602866535, 588.4521037060606, 604.662119305831, 565.2332024041632, 599.405339717746, 572.919967756532, 561.8842010501078, 554.9488688164629, 613.1052501954493, 533.5857682068352, 578.2371855734118, 561.7684533647116, 610.7210576178026, 608.5352216103838, 644.550239288378, 493.1421304292917, 580.2182651574124, 144.76270776382555, 641.3804177184599, 569.2701109876365, 162.21329003712424, 540.9277509689499, 572.329147499593, 581.9057752489623, 568.8577576959981, 601.987800113408, 570.9222070609779, 588.6426654469309, 570.4830554648097, 599.3166090321516, 613.4086800814885, 544.0362211699891, 133.73825378471383, 436.738246609913, 564.4643268331325, 542.0713555023414, 554.498498015326, 602.0275307185756, 601.2308201561985, 591.8461385000054, 585.6462888727606, 592.2161532754104, 200.17472460255505, 523.2841743143092, 581.1588385615339, 574.1542744240994, 555.7113844913803, 596.7339860806302, 603.2637494789291, 611.8883880233234, 201.45745756308642, 571.793523863493, 588.070714767186, 574.4855061460529, 562.3704463616067, 573.7214673784841, 457.36974171188734, 603.8452811357703, 595.6167620575318, 577.5359236393533, 397.01153719996233, 584.581762063861]
Elapsed: 0.08643952640885705~0.05779908322682174
Time per graph: 0.001982305425613342~0.00132363131821825
Speed: 588.314534174969~136.67554029761854
Total Time: 0.0743
best val loss: 0.534674187952822 test_score: 0.6047

Testing...
Test loss: 0.6343 score: 0.6047 time: 0.07s
test Score 0.6047
Epoch Time List: [0.28946014600023773, 0.2929908019996219, 0.29777926599945204, 0.5267754069991497, 0.39736853599879396, 0.271057472999928, 0.27063809399896854, 0.2791777929996897, 0.28923020000002, 0.2862319440000647, 0.27968796999903134, 0.4477909599991108, 0.2909866399995735, 0.5017175469993163, 0.28507440599878464, 0.28539106200150854, 0.29058691099999123, 0.295430973999828, 0.5097417519991723, 0.374978949999786, 0.2941008259995215, 0.29732383800001116, 0.3073254379996797, 0.29596549500092806, 0.2782364120002967, 0.6027024229997551, 0.3113775500005431, 0.3124169639986576, 0.3092012850001993, 0.30435634900095465, 0.31090348299858306, 0.29883839300055115, 0.2963336019993221, 0.4937979410015032, 0.2985385190004308, 0.2998377289995915, 0.29509157100073935, 0.29301920699981565, 0.30014524800026265, 0.2942576729992652, 0.2989058000002842, 0.5113060809999297, 0.2818638920007288, 0.2822085130001142, 0.28362122800081124, 0.29023633700217033, 0.29148358500060567, 0.27413094300027296, 0.7029576459999589, 0.29270030400039104, 0.28774286799944093, 0.2931235269998069, 0.28681857100036723, 0.289351999000246, 0.28904176500145695, 0.37740212000062456, 0.5685437600004661, 0.32303841700013436, 0.2708085589993061, 0.2706425799997305, 0.2786599370001568, 0.2846582820002368, 0.6333631770003194, 0.28818735200002266, 0.2909317630010264, 0.28728936099832936, 0.286456965999605, 0.2889930960000129, 0.3889500280001812, 0.2939744060004159, 0.3383712580007341, 0.4049582369998461, 0.2935332039996865, 0.29447582800003147, 0.29161565599952155, 0.41294112200012023, 0.6103350839994164, 0.3198269659988, 0.29462957499890763, 0.2880557369999224, 0.276596691001032, 0.3993376159996842, 0.2822034410000924, 0.5145550980005282, 0.29104779299996153, 0.2765295929993954, 0.2752145959993868, 0.3839714029991228, 0.4113264079996952, 0.28443816500112007, 0.28560387899869966, 0.6389193560007698, 0.3099507189999713, 0.30414823400133173, 0.2977245420006511, 0.3190402570007791, 0.3139368190004461, 0.3155934980004531, 0.31993006900029286, 0.5218939979995412, 0.5386832990006951, 0.28287206499953754, 0.3065953100003753, 0.2943921779997254, 0.31279106499914633, 0.5231791889991655, 0.31140595599936205, 0.30261627600066277, 0.31317889300044044, 0.6722106009992785, 1.2544176379997225, 0.3324633510010244, 0.3040275589992234, 1.1174616279995462, 0.4412181489997238, 0.5593521890004922, 0.30754121100017073, 0.34168522099935217, 0.3279745979998552, 0.31424817999959487, 0.32697865899990575, 0.3268408650010315, 0.5150854320008875, 0.3245178510005644, 0.3299991409994618, 0.3384837570010859, 0.3501108459995521, 0.5227461619997484, 0.3237543680006638, 0.6250344610007232, 0.3342567660001805, 0.3355705719995967, 0.3209188990003895, 0.44380688299952453, 0.32764394099922356, 0.3754979880004612, 0.5668967889996566, 0.32028515499951027, 0.320369419000599, 0.5276791460000823, 0.3603724249996958, 0.3494158569992578, 0.3409379810000246, 0.6183525510004984, 0.3495382340006472, 0.48521707999952923, 0.33534279800005606, 0.3373264209994886, 0.33579771099994105, 0.3078494810006305, 0.3403106479991038, 0.5938219480003681, 0.5116233930002636, 0.3378958529992815, 0.3455902480000077, 0.36669183600042743, 0.559398578998298, 0.32197873200038885, 0.44760629599932145, 0.339421339999717, 0.34670996500062756, 0.5639536580001732, 0.3584128869997585, 0.3789912430002005, 0.5551128460001564, 0.35577549500067107, 0.5703798230006214, 0.32215359799920407, 0.32444035399930726, 0.466197760999421, 0.33174908999990294, 0.334722478999538, 0.3394982670006357, 0.5649390720000156, 0.3388705769984881, 0.3556810989994119, 0.342360627000744, 0.46340029499970115, 0.34327232600026036, 0.5179717049995816, 0.32841140600066865]
Total Epoch List: [113, 68]
Total Time List: [0.06943193200004316, 0.07428210499983834]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f39b160>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7725;  Loss pred: 0.7725; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7159 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7176 score: 0.4884 time: 0.07s
Epoch 2/1000, LR 0.000015
Train loss: 0.7436;  Loss pred: 0.7436; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7145 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7158 score: 0.4884 time: 0.06s
Epoch 3/1000, LR 0.000045
Train loss: 0.7171;  Loss pred: 0.7171; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7116 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7123 score: 0.4884 time: 0.06s
Epoch 4/1000, LR 0.000075
Train loss: 0.7474;  Loss pred: 0.7474; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7066 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7065 score: 0.4884 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.7259;  Loss pred: 0.7259; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7001 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4884 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6865;  Loss pred: 0.6865; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4884 time: 0.06s
Epoch 7/1000, LR 0.000165
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.20s
Val loss: 0.6842 score: 0.5227 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6792 score: 0.4884 time: 0.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6503;  Loss pred: 0.6503; Loss self: 0.0000; time: 0.18s
Val loss: 0.6766 score: 0.6364 time: 0.06s
Test loss: 0.6692 score: 0.7442 time: 0.06s
Epoch 9/1000, LR 0.000225
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.19s
Val loss: 0.6710 score: 0.6591 time: 0.06s
Test loss: 0.6616 score: 0.7674 time: 0.06s
Epoch 10/1000, LR 0.000255
Train loss: 0.6181;  Loss pred: 0.6181; Loss self: 0.0000; time: 0.18s
Val loss: 0.6693 score: 0.5227 time: 0.07s
Test loss: 0.6580 score: 0.5581 time: 0.06s
Epoch 11/1000, LR 0.000285
Train loss: 0.5929;  Loss pred: 0.5929; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6709 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6587 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5904;  Loss pred: 0.5904; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6738 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6610 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5815;  Loss pred: 0.5815; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6786 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6651 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.6069;  Loss pred: 0.6069; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6694 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5661;  Loss pred: 0.5661; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6727 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5697;  Loss pred: 0.5697; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6750 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6758 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5266;  Loss pred: 0.5266; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6752 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5320;  Loss pred: 0.5320; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6732 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5288;  Loss pred: 0.5288; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6714 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 21/1000, LR 0.000285
Train loss: 0.5157;  Loss pred: 0.5157; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6686 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 22/1000, LR 0.000285
Train loss: 0.5174;  Loss pred: 0.5174; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6658 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 23/1000, LR 0.000285
Train loss: 0.5172;  Loss pred: 0.5172; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6796 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6603 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 24/1000, LR 0.000285
Train loss: 0.4942;  Loss pred: 0.4942; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6556 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 25/1000, LR 0.000285
Train loss: 0.5019;  Loss pred: 0.5019; Loss self: 0.0000; time: 0.19s
Val loss: 0.6714 score: 0.5227 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6508 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 26/1000, LR 0.000285
Train loss: 0.4911;  Loss pred: 0.4911; Loss self: 0.0000; time: 0.28s
Val loss: 0.6648 score: 0.5227 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6438 score: 0.5116 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.4720;  Loss pred: 0.4720; Loss self: 0.0000; time: 0.18s
Val loss: 0.6577 score: 0.5227 time: 0.06s
Test loss: 0.6361 score: 0.5581 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.4657;  Loss pred: 0.4657; Loss self: 0.0000; time: 0.19s
Val loss: 0.6505 score: 0.5682 time: 0.13s
Test loss: 0.6285 score: 0.5814 time: 0.16s
Epoch 29/1000, LR 0.000285
Train loss: 0.4491;  Loss pred: 0.4491; Loss self: 0.0000; time: 0.19s
Val loss: 0.6443 score: 0.5682 time: 0.06s
Test loss: 0.6218 score: 0.5814 time: 0.06s
Epoch 30/1000, LR 0.000285
Train loss: 0.4627;  Loss pred: 0.4627; Loss self: 0.0000; time: 0.18s
Val loss: 0.6373 score: 0.5909 time: 0.06s
Test loss: 0.6143 score: 0.5814 time: 0.06s
Epoch 31/1000, LR 0.000285
Train loss: 0.4470;  Loss pred: 0.4470; Loss self: 0.0000; time: 0.19s
Val loss: 0.6311 score: 0.6136 time: 0.06s
Test loss: 0.6074 score: 0.5814 time: 0.06s
Epoch 32/1000, LR 0.000285
Train loss: 0.4392;  Loss pred: 0.4392; Loss self: 0.0000; time: 0.18s
Val loss: 0.6258 score: 0.6136 time: 0.06s
Test loss: 0.6015 score: 0.6047 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.4418;  Loss pred: 0.4418; Loss self: 0.0000; time: 0.18s
Val loss: 0.6188 score: 0.6136 time: 0.17s
Test loss: 0.5943 score: 0.6279 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.4342;  Loss pred: 0.4342; Loss self: 0.0000; time: 0.20s
Val loss: 0.6125 score: 0.6364 time: 0.07s
Test loss: 0.5877 score: 0.6512 time: 0.06s
Epoch 35/1000, LR 0.000285
Train loss: 0.4257;  Loss pred: 0.4257; Loss self: 0.0000; time: 0.30s
Val loss: 0.6068 score: 0.6364 time: 0.06s
Test loss: 0.5818 score: 0.6977 time: 0.06s
Epoch 36/1000, LR 0.000285
Train loss: 0.4090;  Loss pred: 0.4090; Loss self: 0.0000; time: 0.18s
Val loss: 0.6008 score: 0.6818 time: 0.06s
Test loss: 0.5761 score: 0.6977 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.4076;  Loss pred: 0.4076; Loss self: 0.0000; time: 0.18s
Val loss: 0.5938 score: 0.6818 time: 0.06s
Test loss: 0.5696 score: 0.6977 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.4111;  Loss pred: 0.4111; Loss self: 0.0000; time: 0.19s
Val loss: 0.5867 score: 0.6818 time: 0.06s
Test loss: 0.5635 score: 0.6977 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.4000;  Loss pred: 0.4000; Loss self: 0.0000; time: 0.18s
Val loss: 0.5789 score: 0.6818 time: 0.06s
Test loss: 0.5569 score: 0.7209 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.3901;  Loss pred: 0.3901; Loss self: 0.0000; time: 0.31s
Val loss: 0.5722 score: 0.6818 time: 0.07s
Test loss: 0.5519 score: 0.7209 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3923;  Loss pred: 0.3923; Loss self: 0.0000; time: 0.19s
Val loss: 0.5652 score: 0.6818 time: 0.06s
Test loss: 0.5464 score: 0.7209 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3796;  Loss pred: 0.3796; Loss self: 0.0000; time: 0.19s
Val loss: 0.5576 score: 0.6818 time: 0.21s
Test loss: 0.5406 score: 0.7442 time: 0.10s
Epoch 43/1000, LR 0.000284
Train loss: 0.3784;  Loss pred: 0.3784; Loss self: 0.0000; time: 0.17s
Val loss: 0.5514 score: 0.6818 time: 0.06s
Test loss: 0.5358 score: 0.7442 time: 0.05s
Epoch 44/1000, LR 0.000284
Train loss: 0.3755;  Loss pred: 0.3755; Loss self: 0.0000; time: 0.18s
Val loss: 0.5462 score: 0.6818 time: 0.06s
Test loss: 0.5316 score: 0.7442 time: 0.06s
Epoch 45/1000, LR 0.000284
Train loss: 0.3601;  Loss pred: 0.3601; Loss self: 0.0000; time: 0.18s
Val loss: 0.5410 score: 0.6818 time: 0.06s
Test loss: 0.5272 score: 0.7442 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.3480;  Loss pred: 0.3480; Loss self: 0.0000; time: 0.18s
Val loss: 0.5341 score: 0.6818 time: 0.06s
Test loss: 0.5218 score: 0.7674 time: 0.05s
Epoch 47/1000, LR 0.000284
Train loss: 0.3492;  Loss pred: 0.3492; Loss self: 0.0000; time: 0.29s
Val loss: 0.5274 score: 0.6818 time: 0.07s
Test loss: 0.5166 score: 0.7674 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3471;  Loss pred: 0.3471; Loss self: 0.0000; time: 0.20s
Val loss: 0.5216 score: 0.7045 time: 0.06s
Test loss: 0.5122 score: 0.7907 time: 0.06s
Epoch 49/1000, LR 0.000284
Train loss: 0.3531;  Loss pred: 0.3531; Loss self: 0.0000; time: 0.19s
Val loss: 0.5160 score: 0.7045 time: 0.07s
Test loss: 0.5082 score: 0.7907 time: 0.06s
Epoch 50/1000, LR 0.000284
Train loss: 0.3510;  Loss pred: 0.3510; Loss self: 0.0000; time: 0.19s
Val loss: 0.5108 score: 0.7045 time: 0.06s
Test loss: 0.5040 score: 0.7907 time: 0.27s
Epoch 51/1000, LR 0.000284
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 0.19s
Val loss: 0.5058 score: 0.7045 time: 0.05s
Test loss: 0.5008 score: 0.7907 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.3358;  Loss pred: 0.3358; Loss self: 0.0000; time: 0.18s
Val loss: 0.5015 score: 0.7045 time: 0.06s
Test loss: 0.4985 score: 0.7907 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.3375;  Loss pred: 0.3375; Loss self: 0.0000; time: 0.18s
Val loss: 0.4961 score: 0.7045 time: 0.18s
Test loss: 0.4953 score: 0.7907 time: 0.06s
Epoch 54/1000, LR 0.000284
Train loss: 0.3324;  Loss pred: 0.3324; Loss self: 0.0000; time: 0.19s
Val loss: 0.4902 score: 0.7045 time: 0.07s
Test loss: 0.4913 score: 0.7907 time: 0.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.3159;  Loss pred: 0.3159; Loss self: 0.0000; time: 0.19s
Val loss: 0.4854 score: 0.7045 time: 0.07s
Test loss: 0.4878 score: 0.7907 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.3245;  Loss pred: 0.3245; Loss self: 0.0000; time: 0.19s
Val loss: 0.4814 score: 0.7045 time: 0.07s
Test loss: 0.4845 score: 0.8140 time: 0.06s
Epoch 57/1000, LR 0.000283
Train loss: 0.3184;  Loss pred: 0.3184; Loss self: 0.0000; time: 0.19s
Val loss: 0.4784 score: 0.7045 time: 0.07s
Test loss: 0.4824 score: 0.8140 time: 0.06s
Epoch 58/1000, LR 0.000283
Train loss: 0.3118;  Loss pred: 0.3118; Loss self: 0.0000; time: 0.20s
Val loss: 0.4767 score: 0.7045 time: 0.26s
Test loss: 0.4812 score: 0.8140 time: 0.06s
Epoch 59/1000, LR 0.000283
Train loss: 0.3141;  Loss pred: 0.3141; Loss self: 0.0000; time: 0.18s
Val loss: 0.4749 score: 0.7273 time: 0.06s
Test loss: 0.4803 score: 0.8140 time: 0.06s
Epoch 60/1000, LR 0.000283
Train loss: 0.3085;  Loss pred: 0.3085; Loss self: 0.0000; time: 0.18s
Val loss: 0.4728 score: 0.7273 time: 0.06s
Test loss: 0.4786 score: 0.8140 time: 0.06s
Epoch 61/1000, LR 0.000283
Train loss: 0.3091;  Loss pred: 0.3091; Loss self: 0.0000; time: 0.19s
Val loss: 0.4699 score: 0.7273 time: 0.06s
Test loss: 0.4761 score: 0.8372 time: 0.06s
Epoch 62/1000, LR 0.000283
Train loss: 0.3002;  Loss pred: 0.3002; Loss self: 0.0000; time: 0.18s
Val loss: 0.4682 score: 0.7045 time: 0.06s
Test loss: 0.4740 score: 0.8372 time: 0.06s
Epoch 63/1000, LR 0.000283
Train loss: 0.2952;  Loss pred: 0.2952; Loss self: 0.0000; time: 0.18s
Val loss: 0.4655 score: 0.7273 time: 0.06s
Test loss: 0.4717 score: 0.8605 time: 0.06s
Epoch 64/1000, LR 0.000283
Train loss: 0.2908;  Loss pred: 0.2908; Loss self: 0.0000; time: 0.18s
Val loss: 0.4651 score: 0.7045 time: 0.06s
Test loss: 0.4712 score: 0.8605 time: 0.06s
Epoch 65/1000, LR 0.000283
Train loss: 0.3017;  Loss pred: 0.3017; Loss self: 0.0000; time: 0.19s
Val loss: 0.4610 score: 0.7273 time: 0.07s
Test loss: 0.4687 score: 0.8605 time: 0.06s
Epoch 66/1000, LR 0.000283
Train loss: 0.2885;  Loss pred: 0.2885; Loss self: 0.0000; time: 0.20s
Val loss: 0.4595 score: 0.7273 time: 0.07s
Test loss: 0.4678 score: 0.8605 time: 0.26s
Epoch 67/1000, LR 0.000283
Train loss: 0.2964;  Loss pred: 0.2964; Loss self: 0.0000; time: 0.19s
Val loss: 0.4555 score: 0.7273 time: 0.07s
Test loss: 0.4650 score: 0.8605 time: 0.06s
Epoch 68/1000, LR 0.000283
Train loss: 0.2686;  Loss pred: 0.2686; Loss self: 0.0000; time: 0.20s
Val loss: 0.4507 score: 0.7273 time: 0.07s
Test loss: 0.4624 score: 0.8605 time: 0.06s
Epoch 69/1000, LR 0.000283
Train loss: 0.2791;  Loss pred: 0.2791; Loss self: 0.0000; time: 0.20s
Val loss: 0.4490 score: 0.7273 time: 0.06s
Test loss: 0.4615 score: 0.8605 time: 0.06s
Epoch 70/1000, LR 0.000283
Train loss: 0.2709;  Loss pred: 0.2709; Loss self: 0.0000; time: 0.19s
Val loss: 0.4474 score: 0.7273 time: 0.07s
Test loss: 0.4610 score: 0.8605 time: 0.06s
Epoch 71/1000, LR 0.000282
Train loss: 0.2652;  Loss pred: 0.2652; Loss self: 0.0000; time: 0.19s
Val loss: 0.4460 score: 0.7273 time: 0.07s
Test loss: 0.4605 score: 0.8605 time: 0.06s
Epoch 72/1000, LR 0.000282
Train loss: 0.2664;  Loss pred: 0.2664; Loss self: 0.0000; time: 0.20s
Val loss: 0.4433 score: 0.7273 time: 0.06s
Test loss: 0.4592 score: 0.8605 time: 0.06s
Epoch 73/1000, LR 0.000282
Train loss: 0.2663;  Loss pred: 0.2663; Loss self: 0.0000; time: 0.19s
Val loss: 0.4426 score: 0.7273 time: 0.07s
Test loss: 0.4584 score: 0.8605 time: 0.06s
Epoch 74/1000, LR 0.000282
Train loss: 0.2671;  Loss pred: 0.2671; Loss self: 0.0000; time: 0.20s
Val loss: 0.4425 score: 0.7273 time: 0.06s
Test loss: 0.4587 score: 0.8605 time: 0.27s
Epoch 75/1000, LR 0.000282
Train loss: 0.2575;  Loss pred: 0.2575; Loss self: 0.0000; time: 0.20s
Val loss: 0.4407 score: 0.7273 time: 0.07s
Test loss: 0.4578 score: 0.8605 time: 0.06s
Epoch 76/1000, LR 0.000282
Train loss: 0.2643;  Loss pred: 0.2643; Loss self: 0.0000; time: 0.20s
Val loss: 0.4392 score: 0.7273 time: 0.06s
Test loss: 0.4566 score: 0.8605 time: 0.06s
Epoch 77/1000, LR 0.000282
Train loss: 0.2485;  Loss pred: 0.2485; Loss self: 0.0000; time: 0.18s
Val loss: 0.4400 score: 0.7273 time: 0.06s
Test loss: 0.4569 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000282
Train loss: 0.2478;  Loss pred: 0.2478; Loss self: 0.0000; time: 0.20s
Val loss: 0.4385 score: 0.7500 time: 0.06s
Test loss: 0.4562 score: 0.8605 time: 0.06s
Epoch 79/1000, LR 0.000282
Train loss: 0.2439;  Loss pred: 0.2439; Loss self: 0.0000; time: 0.20s
Val loss: 0.4377 score: 0.7273 time: 0.06s
Test loss: 0.4551 score: 0.8605 time: 0.06s
Epoch 80/1000, LR 0.000282
Train loss: 0.2412;  Loss pred: 0.2412; Loss self: 0.0000; time: 0.19s
Val loss: 0.4355 score: 0.7500 time: 0.06s
Test loss: 0.4530 score: 0.8605 time: 0.06s
Epoch 81/1000, LR 0.000281
Train loss: 0.2399;  Loss pred: 0.2399; Loss self: 0.0000; time: 0.20s
Val loss: 0.4329 score: 0.7500 time: 0.06s
Test loss: 0.4511 score: 0.8605 time: 0.28s
Epoch 82/1000, LR 0.000281
Train loss: 0.2466;  Loss pred: 0.2466; Loss self: 0.0000; time: 0.19s
Val loss: 0.4304 score: 0.7500 time: 0.07s
Test loss: 0.4496 score: 0.8605 time: 0.06s
Epoch 83/1000, LR 0.000281
Train loss: 0.2433;  Loss pred: 0.2433; Loss self: 0.0000; time: 0.19s
Val loss: 0.4293 score: 0.7500 time: 0.07s
Test loss: 0.4486 score: 0.8605 time: 0.06s
Epoch 84/1000, LR 0.000281
Train loss: 0.2426;  Loss pred: 0.2426; Loss self: 0.0000; time: 0.19s
Val loss: 0.4295 score: 0.7500 time: 0.07s
Test loss: 0.4482 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.2310;  Loss pred: 0.2310; Loss self: 0.0000; time: 0.19s
Val loss: 0.4307 score: 0.7500 time: 0.07s
Test loss: 0.4486 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.2256;  Loss pred: 0.2256; Loss self: 0.0000; time: 0.19s
Val loss: 0.4310 score: 0.7500 time: 0.07s
Test loss: 0.4486 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.2284;  Loss pred: 0.2284; Loss self: 0.0000; time: 0.19s
Val loss: 0.4298 score: 0.7500 time: 0.07s
Test loss: 0.4473 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.2330;  Loss pred: 0.2330; Loss self: 0.0000; time: 0.20s
Val loss: 0.4273 score: 0.7500 time: 0.07s
Test loss: 0.4453 score: 0.8605 time: 0.06s
Epoch 89/1000, LR 0.000281
Train loss: 0.2181;  Loss pred: 0.2181; Loss self: 0.0000; time: 0.42s
Val loss: 0.4239 score: 0.7500 time: 0.07s
Test loss: 0.4427 score: 0.8605 time: 0.06s
Epoch 90/1000, LR 0.000281
Train loss: 0.2205;  Loss pred: 0.2205; Loss self: 0.0000; time: 0.18s
Val loss: 0.4242 score: 0.7500 time: 0.06s
Test loss: 0.4420 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.2215;  Loss pred: 0.2215; Loss self: 0.0000; time: 0.18s
Val loss: 0.4252 score: 0.7500 time: 0.06s
Test loss: 0.4420 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.2144;  Loss pred: 0.2144; Loss self: 0.0000; time: 0.19s
Val loss: 0.4246 score: 0.7500 time: 0.06s
Test loss: 0.4411 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.2112;  Loss pred: 0.2112; Loss self: 0.0000; time: 0.19s
Val loss: 0.4234 score: 0.7500 time: 0.06s
Test loss: 0.4384 score: 0.8605 time: 0.05s
Epoch 94/1000, LR 0.000280
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.18s
Val loss: 0.4246 score: 0.7500 time: 0.06s
Test loss: 0.4388 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.2090;  Loss pred: 0.2090; Loss self: 0.0000; time: 0.19s
Val loss: 0.4236 score: 0.7500 time: 0.28s
Test loss: 0.4382 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.2113;  Loss pred: 0.2113; Loss self: 0.0000; time: 0.18s
Val loss: 0.4199 score: 0.7955 time: 0.06s
Test loss: 0.4354 score: 0.8605 time: 0.06s
Epoch 97/1000, LR 0.000280
Train loss: 0.2075;  Loss pred: 0.2075; Loss self: 0.0000; time: 0.18s
Val loss: 0.4186 score: 0.7955 time: 0.06s
Test loss: 0.4342 score: 0.8605 time: 0.06s
Epoch 98/1000, LR 0.000280
Train loss: 0.2138;  Loss pred: 0.2138; Loss self: 0.0000; time: 0.18s
Val loss: 0.4164 score: 0.7955 time: 0.06s
Test loss: 0.4327 score: 0.8605 time: 0.06s
Epoch 99/1000, LR 0.000279
Train loss: 0.2067;  Loss pred: 0.2067; Loss self: 0.0000; time: 0.31s
Val loss: 0.4133 score: 0.7955 time: 0.06s
Test loss: 0.4309 score: 0.8605 time: 0.06s
Epoch 100/1000, LR 0.000279
Train loss: 0.2051;  Loss pred: 0.2051; Loss self: 0.0000; time: 0.18s
Val loss: 0.4101 score: 0.7955 time: 0.06s
Test loss: 0.4287 score: 0.8605 time: 0.06s
Epoch 101/1000, LR 0.000279
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 0.18s
Val loss: 0.4085 score: 0.7955 time: 0.06s
Test loss: 0.4270 score: 0.8605 time: 0.06s
Epoch 102/1000, LR 0.000279
Train loss: 0.1910;  Loss pred: 0.1910; Loss self: 0.0000; time: 0.40s
Val loss: 0.4078 score: 0.7955 time: 0.06s
Test loss: 0.4266 score: 0.8605 time: 0.06s
Epoch 103/1000, LR 0.000279
Train loss: 0.1959;  Loss pred: 0.1959; Loss self: 0.0000; time: 0.18s
Val loss: 0.4052 score: 0.7955 time: 0.06s
Test loss: 0.4250 score: 0.8605 time: 0.06s
Epoch 104/1000, LR 0.000279
Train loss: 0.1812;  Loss pred: 0.1812; Loss self: 0.0000; time: 0.19s
Val loss: 0.4020 score: 0.7955 time: 0.06s
Test loss: 0.4233 score: 0.8605 time: 0.07s
Epoch 105/1000, LR 0.000279
Train loss: 0.1950;  Loss pred: 0.1950; Loss self: 0.0000; time: 0.19s
Val loss: 0.3998 score: 0.8182 time: 0.06s
Test loss: 0.4219 score: 0.8837 time: 0.06s
Epoch 106/1000, LR 0.000279
Train loss: 0.1853;  Loss pred: 0.1853; Loss self: 0.0000; time: 0.19s
Val loss: 0.3982 score: 0.8409 time: 0.06s
Test loss: 0.4211 score: 0.8837 time: 0.07s
Epoch 107/1000, LR 0.000278
Train loss: 0.1814;  Loss pred: 0.1814; Loss self: 0.0000; time: 0.29s
Val loss: 0.3984 score: 0.8409 time: 0.06s
Test loss: 0.4206 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1788;  Loss pred: 0.1788; Loss self: 0.0000; time: 0.17s
Val loss: 0.3985 score: 0.8409 time: 0.06s
Test loss: 0.4206 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1819;  Loss pred: 0.1819; Loss self: 0.0000; time: 0.18s
Val loss: 0.3990 score: 0.8409 time: 0.06s
Test loss: 0.4205 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1715;  Loss pred: 0.1715; Loss self: 0.0000; time: 0.19s
Val loss: 0.4038 score: 0.8182 time: 0.26s
Test loss: 0.4227 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1885;  Loss pred: 0.1885; Loss self: 0.0000; time: 0.21s
Val loss: 0.4054 score: 0.7955 time: 0.07s
Test loss: 0.4227 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.19s
Val loss: 0.4072 score: 0.7955 time: 0.18s
Test loss: 0.4227 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1634;  Loss pred: 0.1634; Loss self: 0.0000; time: 0.21s
Val loss: 0.4091 score: 0.7955 time: 0.06s
Test loss: 0.4229 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1785;  Loss pred: 0.1785; Loss self: 0.0000; time: 0.20s
Val loss: 0.4095 score: 0.7955 time: 0.07s
Test loss: 0.4223 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1714;  Loss pred: 0.1714; Loss self: 0.0000; time: 0.20s
Val loss: 0.4072 score: 0.7955 time: 0.06s
Test loss: 0.4196 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1639;  Loss pred: 0.1639; Loss self: 0.0000; time: 0.20s
Val loss: 0.4012 score: 0.7955 time: 0.06s
Test loss: 0.4145 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1681;  Loss pred: 0.1681; Loss self: 0.0000; time: 0.20s
Val loss: 0.3933 score: 0.8409 time: 0.06s
Test loss: 0.4084 score: 0.8837 time: 0.06s
Epoch 118/1000, LR 0.000277
Train loss: 0.1625;  Loss pred: 0.1625; Loss self: 0.0000; time: 0.20s
Val loss: 0.3842 score: 0.8636 time: 0.06s
Test loss: 0.4017 score: 0.8837 time: 0.07s
Epoch 119/1000, LR 0.000277
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.43s
Val loss: 0.3772 score: 0.8636 time: 0.06s
Test loss: 0.3965 score: 0.8837 time: 0.19s
Epoch 120/1000, LR 0.000277
Train loss: 0.1546;  Loss pred: 0.1546; Loss self: 0.0000; time: 0.19s
Val loss: 0.3742 score: 0.8636 time: 0.06s
Test loss: 0.3936 score: 0.8837 time: 0.05s
Epoch 121/1000, LR 0.000276
Train loss: 0.1550;  Loss pred: 0.1550; Loss self: 0.0000; time: 0.18s
Val loss: 0.3725 score: 0.8636 time: 0.06s
Test loss: 0.3919 score: 0.8837 time: 0.06s
Epoch 122/1000, LR 0.000276
Train loss: 0.1503;  Loss pred: 0.1503; Loss self: 0.0000; time: 0.18s
Val loss: 0.3686 score: 0.8636 time: 0.06s
Test loss: 0.3888 score: 0.8837 time: 0.06s
Epoch 123/1000, LR 0.000276
Train loss: 0.1447;  Loss pred: 0.1447; Loss self: 0.0000; time: 0.20s
Val loss: 0.3628 score: 0.8636 time: 0.06s
Test loss: 0.3853 score: 0.8837 time: 0.06s
Epoch 124/1000, LR 0.000276
Train loss: 0.1469;  Loss pred: 0.1469; Loss self: 0.0000; time: 0.19s
Val loss: 0.3568 score: 0.8636 time: 0.06s
Test loss: 0.3817 score: 0.8837 time: 0.06s
Epoch 125/1000, LR 0.000276
Train loss: 0.1412;  Loss pred: 0.1412; Loss self: 0.0000; time: 0.20s
Val loss: 0.3526 score: 0.8636 time: 0.07s
Test loss: 0.3795 score: 0.8605 time: 0.06s
Epoch 126/1000, LR 0.000276
Train loss: 0.1438;  Loss pred: 0.1438; Loss self: 0.0000; time: 0.50s
Val loss: 0.3499 score: 0.8636 time: 0.07s
Test loss: 0.3780 score: 0.8605 time: 0.06s
Epoch 127/1000, LR 0.000275
Train loss: 0.1381;  Loss pred: 0.1381; Loss self: 0.0000; time: 0.21s
Val loss: 0.3512 score: 0.8636 time: 0.06s
Test loss: 0.3789 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1274;  Loss pred: 0.1274; Loss self: 0.0000; time: 0.20s
Val loss: 0.3498 score: 0.8636 time: 0.06s
Test loss: 0.3776 score: 0.8605 time: 0.06s
Epoch 129/1000, LR 0.000275
Train loss: 0.1429;  Loss pred: 0.1429; Loss self: 0.0000; time: 0.21s
Val loss: 0.3419 score: 0.8636 time: 0.06s
Test loss: 0.3728 score: 0.8605 time: 0.06s
Epoch 130/1000, LR 0.000275
Train loss: 0.1258;  Loss pred: 0.1258; Loss self: 0.0000; time: 0.18s
Val loss: 0.3352 score: 0.8636 time: 0.06s
Test loss: 0.3687 score: 0.8372 time: 0.06s
Epoch 131/1000, LR 0.000275
Train loss: 0.1249;  Loss pred: 0.1249; Loss self: 0.0000; time: 0.18s
Val loss: 0.3351 score: 0.8636 time: 0.06s
Test loss: 0.3678 score: 0.8372 time: 0.06s
Epoch 132/1000, LR 0.000275
Train loss: 0.1219;  Loss pred: 0.1219; Loss self: 0.0000; time: 0.18s
Val loss: 0.3301 score: 0.8864 time: 0.06s
Test loss: 0.3642 score: 0.8372 time: 0.06s
Epoch 133/1000, LR 0.000274
Train loss: 0.1120;  Loss pred: 0.1120; Loss self: 0.0000; time: 0.30s
Val loss: 0.3283 score: 0.8864 time: 0.30s
Test loss: 0.3619 score: 0.8372 time: 0.06s
Epoch 134/1000, LR 0.000274
Train loss: 0.1104;  Loss pred: 0.1104; Loss self: 0.0000; time: 0.19s
Val loss: 0.3326 score: 0.8864 time: 0.07s
Test loss: 0.3644 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 135/1000, LR 0.000274
Train loss: 0.1010;  Loss pred: 0.1010; Loss self: 0.0000; time: 0.20s
Val loss: 0.3381 score: 0.8636 time: 0.06s
Test loss: 0.3678 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 136/1000, LR 0.000274
Train loss: 0.1016;  Loss pred: 0.1016; Loss self: 0.0000; time: 0.20s
Val loss: 0.3332 score: 0.8864 time: 0.07s
Test loss: 0.3691 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 137/1000, LR 0.000274
Train loss: 0.0948;  Loss pred: 0.0948; Loss self: 0.0000; time: 0.20s
Val loss: 0.3314 score: 0.8864 time: 0.06s
Test loss: 0.3693 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 138/1000, LR 0.000274
Train loss: 0.0954;  Loss pred: 0.0954; Loss self: 0.0000; time: 0.19s
Val loss: 0.3526 score: 0.8636 time: 0.06s
Test loss: 0.3692 score: 0.8605 time: 0.19s
     INFO: Early stopping counter 5 of 20
Epoch 139/1000, LR 0.000273
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.24s
Val loss: 0.3353 score: 0.8864 time: 0.08s
Test loss: 0.3638 score: 0.8372 time: 0.20s
     INFO: Early stopping counter 6 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.24s
Val loss: 0.3425 score: 0.8636 time: 0.06s
Test loss: 0.3630 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0878;  Loss pred: 0.0878; Loss self: 0.0000; time: 0.20s
Val loss: 0.3382 score: 0.8864 time: 0.06s
Test loss: 0.3637 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0883;  Loss pred: 0.0883; Loss self: 0.0000; time: 0.18s
Val loss: 0.3294 score: 0.8864 time: 0.06s
Test loss: 0.3633 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 143/1000, LR 0.000273
Train loss: 0.0863;  Loss pred: 0.0863; Loss self: 0.0000; time: 0.18s
Val loss: 0.3320 score: 0.8864 time: 0.06s
Test loss: 0.3637 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 144/1000, LR 0.000272
Train loss: 0.0793;  Loss pred: 0.0793; Loss self: 0.0000; time: 0.19s
Val loss: 0.3259 score: 0.8864 time: 0.17s
Test loss: 0.3610 score: 0.8372 time: 0.06s
Epoch 145/1000, LR 0.000272
Train loss: 0.0756;  Loss pred: 0.0756; Loss self: 0.0000; time: 0.42s
Val loss: 0.3342 score: 0.8864 time: 0.07s
Test loss: 0.3623 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0828;  Loss pred: 0.0828; Loss self: 0.0000; time: 0.19s
Val loss: 0.3402 score: 0.8636 time: 0.07s
Test loss: 0.3638 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 147/1000, LR 0.000272
Train loss: 0.0768;  Loss pred: 0.0768; Loss self: 0.0000; time: 0.23s
Val loss: 0.3291 score: 0.8864 time: 0.08s
Test loss: 0.3639 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 148/1000, LR 0.000272
Train loss: 0.0738;  Loss pred: 0.0738; Loss self: 0.0000; time: 0.19s
Val loss: 0.3245 score: 0.8636 time: 0.07s
Test loss: 0.3641 score: 0.8837 time: 0.06s
Epoch 149/1000, LR 0.000272
Train loss: 0.0679;  Loss pred: 0.0679; Loss self: 0.0000; time: 0.20s
Val loss: 0.3299 score: 0.8864 time: 0.07s
Test loss: 0.3684 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0669;  Loss pred: 0.0669; Loss self: 0.0000; time: 0.20s
Val loss: 0.3408 score: 0.8636 time: 0.06s
Test loss: 0.3708 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 151/1000, LR 0.000271
Train loss: 0.0640;  Loss pred: 0.0640; Loss self: 0.0000; time: 0.20s
Val loss: 0.3344 score: 0.8864 time: 0.20s
Test loss: 0.3719 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 152/1000, LR 0.000271
Train loss: 0.0677;  Loss pred: 0.0677; Loss self: 0.0000; time: 0.19s
Val loss: 0.3246 score: 0.8864 time: 0.07s
Test loss: 0.3692 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 153/1000, LR 0.000271
Train loss: 0.0674;  Loss pred: 0.0674; Loss self: 0.0000; time: 0.41s
Val loss: 0.3229 score: 0.8864 time: 0.06s
Test loss: 0.3694 score: 0.8605 time: 0.06s
Epoch 154/1000, LR 0.000271
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.19s
Val loss: 0.3193 score: 0.9091 time: 0.06s
Test loss: 0.3663 score: 0.8837 time: 0.06s
Epoch 155/1000, LR 0.000270
Train loss: 0.0681;  Loss pred: 0.0681; Loss self: 0.0000; time: 0.18s
Val loss: 0.3246 score: 0.8864 time: 0.06s
Test loss: 0.3647 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.18s
Val loss: 0.3410 score: 0.8636 time: 0.06s
Test loss: 0.3660 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 157/1000, LR 0.000270
Train loss: 0.0711;  Loss pred: 0.0711; Loss self: 0.0000; time: 0.31s
Val loss: 0.3190 score: 0.9091 time: 0.06s
Test loss: 0.3671 score: 0.8837 time: 0.06s
Epoch 158/1000, LR 0.000270
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.18s
Val loss: 0.3212 score: 0.8864 time: 0.06s
Test loss: 0.3669 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 159/1000, LR 0.000270
Train loss: 0.0610;  Loss pred: 0.0610; Loss self: 0.0000; time: 0.18s
Val loss: 0.3215 score: 0.8864 time: 0.06s
Test loss: 0.3703 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 160/1000, LR 0.000269
Train loss: 0.0584;  Loss pred: 0.0584; Loss self: 0.0000; time: 0.43s
Val loss: 0.3333 score: 0.8636 time: 0.07s
Test loss: 0.3716 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 161/1000, LR 0.000269
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 0.21s
Val loss: 0.3193 score: 0.9091 time: 0.07s
Test loss: 0.3703 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 162/1000, LR 0.000269
Train loss: 0.0568;  Loss pred: 0.0568; Loss self: 0.0000; time: 0.21s
Val loss: 0.3256 score: 0.8864 time: 0.07s
Test loss: 0.3710 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 163/1000, LR 0.000269
Train loss: 0.0521;  Loss pred: 0.0521; Loss self: 0.0000; time: 0.20s
Val loss: 0.3357 score: 0.8636 time: 0.07s
Test loss: 0.3722 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 164/1000, LR 0.000269
Train loss: 0.0617;  Loss pred: 0.0617; Loss self: 0.0000; time: 0.19s
Val loss: 0.3261 score: 0.8864 time: 0.07s
Test loss: 0.3747 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 165/1000, LR 0.000268
Train loss: 0.0544;  Loss pred: 0.0544; Loss self: 0.0000; time: 0.28s
Val loss: 0.3325 score: 0.8864 time: 0.07s
Test loss: 0.3744 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 166/1000, LR 0.000268
Train loss: 0.0539;  Loss pred: 0.0539; Loss self: 0.0000; time: 0.20s
Val loss: 0.3321 score: 0.8636 time: 0.08s
Test loss: 0.3749 score: 0.8372 time: 0.27s
     INFO: Early stopping counter 9 of 20
Epoch 167/1000, LR 0.000268
Train loss: 0.0529;  Loss pred: 0.0529; Loss self: 0.0000; time: 0.19s
Val loss: 0.3269 score: 0.8636 time: 0.07s
Test loss: 0.3767 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 168/1000, LR 0.000268
Train loss: 0.0548;  Loss pred: 0.0548; Loss self: 0.0000; time: 0.19s
Val loss: 0.3198 score: 0.8864 time: 0.07s
Test loss: 0.3769 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 169/1000, LR 0.000267
Train loss: 0.0502;  Loss pred: 0.0502; Loss self: 0.0000; time: 0.19s
Val loss: 0.3257 score: 0.8636 time: 0.07s
Test loss: 0.3772 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 170/1000, LR 0.000267
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.21s
Val loss: 0.3298 score: 0.8636 time: 0.07s
Test loss: 0.3768 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 171/1000, LR 0.000267
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.19s
Val loss: 0.3262 score: 0.8864 time: 0.07s
Test loss: 0.3773 score: 0.8605 time: 0.20s
     INFO: Early stopping counter 14 of 20
Epoch 172/1000, LR 0.000267
Train loss: 0.0523;  Loss pred: 0.0523; Loss self: 0.0000; time: 0.23s
Val loss: 0.3264 score: 0.8864 time: 0.07s
Test loss: 0.3774 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 15 of 20
Epoch 173/1000, LR 0.000267
Train loss: 0.0464;  Loss pred: 0.0464; Loss self: 0.0000; time: 0.32s
Val loss: 0.3269 score: 0.8864 time: 0.08s
Test loss: 0.3764 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 16 of 20
Epoch 174/1000, LR 0.000266
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.21s
Val loss: 0.3220 score: 0.8636 time: 0.07s
Test loss: 0.3776 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 175/1000, LR 0.000266
Train loss: 0.0514;  Loss pred: 0.0514; Loss self: 0.0000; time: 0.20s
Val loss: 0.3251 score: 0.8864 time: 0.06s
Test loss: 0.3774 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 176/1000, LR 0.000266
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.19s
Val loss: 0.3227 score: 0.8864 time: 0.07s
Test loss: 0.3780 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 177/1000, LR 0.000266
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.33s
Val loss: 0.3224 score: 0.8636 time: 0.07s
Test loss: 0.3788 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 156,   Train_Loss: 0.0711,   Val_Loss: 0.3190,   Val_Precision: 1.0000,   Val_Recall: 0.8182,   Val_accuracy: 0.9000,   Val_Score: 0.9091,   Val_Loss: 0.3190,   Test_Precision: 0.9000,   Test_Recall: 0.8571,   Test_accuracy: 0.8780,   Test_Score: 0.8837,   Test_loss: 0.3671


[0.06591607199970895, 0.06817726799999946, 0.07005674799984263, 0.06766368199987483, 0.17381003600075928, 0.06136203999994905, 0.062295979999362316, 0.06546464299935906, 0.06661496800006717, 0.06489528700058145, 0.06424749300003896, 0.06385992899959092, 0.06769902100040781, 0.07471809500020754, 0.06556483599979401, 0.0659469940001145, 0.06724114999997255, 0.06917075500041392, 0.06610225499935041, 0.06726669100044091, 0.06828087499980029, 0.0682031359992834, 0.06798686399997678, 0.06657617699966067, 0.06352621899986843, 0.23693184800049494, 0.06788920400049392, 0.06893622400002641, 0.06677409100029763, 0.06739371399999072, 0.07208560700019007, 0.06850123299955158, 0.06800084499991499, 0.066757389000486, 0.07255278599950543, 0.06737409700053831, 0.06752685799983738, 0.07118423200063262, 0.07145388400022057, 0.06755969900041237, 0.06918337299975974, 0.2810418179997214, 0.061408673000187264, 0.06243992999952752, 0.06164740299936966, 0.06451223300064157, 0.06310639000002993, 0.06574583600013284, 0.31267133300025307, 0.06756266100001085, 0.06637742600014462, 0.06825081200076966, 0.06569890700029646, 0.06663645399930829, 0.06673422200037749, 0.1342714430002161, 0.06847112800005561, 0.06709129299997585, 0.0632612009994773, 0.061709057000371104, 0.06250218600052904, 0.061246041000231344, 0.06287892999989708, 0.0634588379998604, 0.0634538229996906, 0.06116769700020086, 0.06244881299971894, 0.06421606399999291, 0.06356517499989423, 0.06453649200011569, 0.12779223999950773, 0.06608748799953901, 0.0689109049999388, 0.0710869949998596, 0.06532244300069578, 0.06934541200007516, 0.3758377059994018, 0.06988696199914557, 0.06432876399958332, 0.06256094200034568, 0.06271078299960209, 0.06389529399984895, 0.0662608310003634, 0.06837720200019248, 0.0645508620000328, 0.062243012999715575, 0.06345873700047377, 0.06277281500024401, 0.1991693399995711, 0.0649816989998726, 0.06629433199941559, 0.06958622999991348, 0.07466290199954528, 0.06707709700003761, 0.0679971879999357, 0.06982141799926467, 0.06945081400044728, 0.07040255700030684, 0.07290697299958993, 0.0705223479999404, 0.19827589099986653, 0.06165821099966706, 0.06830467999952816, 0.06943916199998057, 0.07216930699996738, 0.26977394000005006, 0.06847857399952773, 0.06944954600021447, 0.07272309100062557, 0.4375212590002775, 0.07378885399975843, 0.0703821010001775, 0.06879922299958707, 0.08837106000009953, 0.10109133500009193, 0.06485065699962433, 0.06738126200070838, 0.07322028199996566, 0.07090012900061993, 0.0698657569992065, 0.0730730669993136, 0.07111409600020124, 0.07607479499984038, 0.07173776600029669, 0.07505411300007836, 0.07652822399995785, 0.07748461600021983, 0.07013477700002113, 0.08058685700052592, 0.07436394800060953, 0.07654399199964246, 0.07040857599986339, 0.07066148100057035, 0.06671318600001541, 0.08719595700040372, 0.07411004200002935, 0.2970378259997233, 0.06704289499975857, 0.07553532000019914, 0.2650830890006546, 0.07949305600050138, 0.07513159200061637, 0.07389512499958073, 0.07559007400050177, 0.07143001900021773, 0.07531674099936936, 0.07304941099937423, 0.0753747190001377, 0.07174838699938846, 0.0701000839999324, 0.07903885500036267, 0.32152356400001736, 0.09845714299990505, 0.07617841899991618, 0.07932534999963536, 0.07754755000041769, 0.07142530500004796, 0.0715199530004611, 0.07265401800032123, 0.07342315799996868, 0.07260862400016777, 0.2148123350007154, 0.08217332399999577, 0.07399009900018427, 0.0748927630002072, 0.07737829600046098, 0.07205890899967926, 0.07127893899996707, 0.07027425400065113, 0.21344456800034095, 0.07520197100075166, 0.07312045799972111, 0.07484958199984248, 0.07646205499986536, 0.07494926100025623, 0.09401583899943944, 0.07121029399968393, 0.07219407299999148, 0.07445424300021841, 0.10830919500040181, 0.07355686200025957, 0.0757065159996273, 0.06769611300023826, 0.06760570400001598, 0.06830041899956996, 0.06780714500018803, 0.06707895300041855, 0.06052319499940495, 0.06252649999987625, 0.0616633269992235, 0.06482219999998051, 0.06266408399915235, 0.06505545700019866, 0.06644948400025896, 0.06579186700037098, 0.0646573569993052, 0.05995752299986634, 0.06086450899965712, 0.06191295799999352, 0.06104427700029191, 0.059665819000656484, 0.06085462000010011, 0.0631810310005676, 0.06690401200012275, 0.06753620200015575, 0.0653568109992193, 0.061132216999794764, 0.063696197000354, 0.1685129159995995, 0.06208403099935822, 0.061288197999601834, 0.06050937099917064, 0.06265818900010345, 0.0664415139999619, 0.06688072300039494, 0.06203657999958523, 0.06181028199989669, 0.06170888900032878, 0.060944001999814645, 0.06046502499975759, 0.06480678000025364, 0.06784592799976963, 0.10637220500029798, 0.059380658000009134, 0.062267614000120375, 0.06139147900012176, 0.060118653000245104, 0.06684619400039082, 0.06431387500015262, 0.06454229299924918, 0.2768424510004479, 0.06226396600050066, 0.060906867000085185, 0.06314240499978041, 0.06337737400008336, 0.0646953460000077, 0.06680251599937037, 0.06518662100006622, 0.06215926799995941, 0.06236450899996271, 0.06477563499993266, 0.06247241599976405, 0.06148101700000552, 0.06261607699980232, 0.06643860500025767, 0.06618968100065103, 0.26742206400012947, 0.06653679000010015, 0.06641453399970487, 0.06570572600048763, 0.06545661600011954, 0.06487462999939453, 0.06551740700069786, 0.06663344200023857, 0.2735480800001824, 0.06413233700004639, 0.060860514000523835, 0.06167324699981691, 0.061469526999644586, 0.06340584699955798, 0.06087699900035659, 0.2804149209996467, 0.06704204400011804, 0.06563687900052173, 0.06504797100024007, 0.06439751500056445, 0.06492158800028847, 0.06649507400015864, 0.06878409499950067, 0.06078744599926722, 0.06227135700009967, 0.06106000499949005, 0.060930844999347755, 0.06058468900027947, 0.061274652000065544, 0.05965214099978766, 0.06061482400036766, 0.06167162600013398, 0.06377963900013128, 0.06257933800043247, 0.060495063999951526, 0.06330553899988445, 0.06346421199941688, 0.06299707099969964, 0.07284339499983616, 0.06403850599963334, 0.07352270399951522, 0.0597458350002853, 0.06046028099990508, 0.06316417400012142, 0.06516438800008473, 0.06642690999979095, 0.07073139600015566, 0.06737916599922755, 0.06562511100037227, 0.0660885369998141, 0.06422411800031114, 0.06349501099975896, 0.07301929100049165, 0.19192977999955474, 0.05954598399966926, 0.0616329919994314, 0.062173827000151505, 0.06768483099949663, 0.06349524599954748, 0.06664979699962714, 0.06467221599996265, 0.06508656699952553, 0.06590426900038437, 0.06340266000006523, 0.06024841999987984, 0.061749598999995214, 0.06168937499933236, 0.06536018499991769, 0.06541109700083325, 0.06914398000026267, 0.065936660999796, 0.06477029500001663, 0.19048686100086343, 0.20860260500012373, 0.06047524199948384, 0.06065820000003441, 0.059651256999131874, 0.05914198299979034, 0.06555645299977186, 0.06477103399993211, 0.06533728900012648, 0.06600034900020546, 0.06547199899978295, 0.06736722599998757, 0.06756121699982032, 0.06588109400036046, 0.06742029500037461, 0.06189776800056279, 0.06104326399963611, 0.061663989999942714, 0.059965385000396054, 0.06156312999974034, 0.06091211900002236, 0.06299619499986875, 0.0687119280000843, 0.06779205599923444, 0.06776949499999319, 0.06702615599988349, 0.10098704400024872, 0.06850632700025017, 0.2743341830000645, 0.06449164900004689, 0.06611737100047321, 0.06709233399942605, 0.0656322619997809, 0.20512010999937047, 0.07229460800044762, 0.07295323699963774, 0.06755418099965027, 0.06635058800020488, 0.06654828800037649, 0.06442013800005952]
[0.0014980925454479307, 0.0015494833636363512, 0.0015921988181782415, 0.0015378109545426096, 0.003950228090926347, 0.00139459181818066, 0.0014158177272582345, 0.0014878327954399786, 0.001513976545456072, 0.001474892886376851, 0.001460170295455431, 0.0014513620227179754, 0.001538614113645632, 0.0016981385227319895, 0.0014901099090862276, 0.0014987953181844205, 0.0015282079545448307, 0.0015720626136457709, 0.0015023239772579639, 0.0015287884318282026, 0.0015518380681772794, 0.0015500712727109865, 0.0015451559999994722, 0.00151309493181047, 0.0014437777045424643, 0.005384814727283976, 0.00154293645455668, 0.001566732363636964, 0.0015175929772794916, 0.0015316753181816073, 0.0016383092500043197, 0.001556846204535263, 0.0015454737499980679, 0.001517213386374682, 0.0016489269545342143, 0.0015312294772849616, 0.0015347013181781222, 0.0016178234545598323, 0.001623951909095922, 0.0015354477045548265, 0.001572349386358176, 0.006387314045448214, 0.001395651659095165, 0.00141908931817108, 0.001401077340894765, 0.0014661871136509449, 0.0014342361363643167, 0.0014942235454575646, 0.007106166659096661, 0.0015355150227275192, 0.0015085778636396506, 0.0015511548181993103, 0.0014931569772794649, 0.001514464863620643, 0.0015166868636449428, 0.003051623704550366, 0.001556162000001264, 0.0015248021136358147, 0.0014377545681699385, 0.0014024785681902524, 0.0014205042272847509, 0.001391955477277985, 0.0014290665909067518, 0.0014422463181786456, 0.001442132340902059, 0.0013901749318227469, 0.0014192912045390667, 0.001459455999999839, 0.0014446630681794143, 0.0014667384545480838, 0.002904369090897903, 0.0015019883636258865, 0.0015661569318167908, 0.0016156135227240818, 0.0014846009772885407, 0.001576032090910799, 0.00854176604544095, 0.0015883400454351265, 0.0014620173636268935, 0.0014218395909169471, 0.0014252450681727748, 0.0014521657727238396, 0.0015059279772809862, 0.0015540273181861928, 0.0014670650454552908, 0.0014146139318117175, 0.0014422440227380403, 0.0014266548863691821, 0.004526575909081162, 0.0014768567954516502, 0.0015066893636230816, 0.001581505227270761, 0.001696884136353302, 0.001524479477273582, 0.0015453906363621752, 0.001586850409074197, 0.0015784275909192563, 0.0016000581136433373, 0.0016569766590815891, 0.0016027806363622817, 0.004506270249996966, 0.0014013229772651605, 0.0015523790908983674, 0.0015781627727268312, 0.0016402115227265313, 0.006131225909092047, 0.0015563312272619937, 0.001578398772732147, 0.0016527975227414902, 0.009943664977279033, 0.0016770194090854188, 0.0015995932045494885, 0.0015636187045360698, 0.0020551409302348726, 0.0023509612790719053, 0.001508154813944752, 0.0015670060930397297, 0.001702797255813155, 0.0016488402093167425, 0.0016247850464931742, 0.0016993736511468278, 0.0016538161860511916, 0.0017691812790660553, 0.0016683201395417833, 0.0017454444883739154, 0.0017797261395339034, 0.0018019678139586006, 0.0016310413255818867, 0.001874112953500603, 0.0017293941395490587, 0.0017800928372009874, 0.0016374087441828695, 0.0016432902558272174, 0.0015514694418608235, 0.002027812953497761, 0.0017234893488378919, 0.0069078564185982155, 0.0015591370930176413, 0.0017566353488418403, 0.006164723000015223, 0.0018486757209418927, 0.0017472463255957295, 0.001718491279060017, 0.0017579086976860878, 0.0016611632325632029, 0.0017515521162644037, 0.0016988235116133542, 0.0017529004418636676, 0.0016685671395206619, 0.0016302345116263348, 0.0018381129069851784, 0.007477292186046916, 0.0022897009999977916, 0.0017715911395329343, 0.0018447755813868689, 0.0018034313953585509, 0.0016610536046522781, 0.0016632547209409559, 0.0016896283255888659, 0.001707515302324853, 0.0016885726511666921, 0.004995635697691056, 0.0019110075348836226, 0.0017206999767484714, 0.0017416921627955162, 0.0017994952558246739, 0.0016757885813878898, 0.0016576497441852806, 0.0016342849767593285, 0.004963827162798627, 0.0017488830465291082, 0.0017004757674353747, 0.001740687953484709, 0.001778187325578264, 0.0017430060697734007, 0.00218641486045208, 0.0016560533488298587, 0.00167893193023236, 0.0017314940232608933, 0.0025188184883814376, 0.001710624697680455, 0.0017606166511541232, 0.0015743282093078665, 0.0015722256744189762, 0.0015883818371993012, 0.001576910348841582, 0.0015599756511725244, 0.0014075161627768592, 0.0014541046511599128, 0.0014340308604470582, 0.0015074930232553607, 0.0014573042790500547, 0.0015129176046557828, 0.0015453368372153246, 0.0015300434186132786, 0.0015036594651001208, 0.0013943609999968916, 0.0014154536976664447, 0.0014398362325579888, 0.001419634348843998, 0.0013875771860617787, 0.0014152237209325608, 0.0014693263023387814, 0.001555907255816808, 0.0015706093488408316, 0.0015199258371911466, 0.001421679465111506, 0.0014813069069849769, 0.0039189050232465, 0.0014438146744036795, 0.0014253069302232984, 0.0014071946743993173, 0.0014571671860489175, 0.001545151488371207, 0.0015553656511719752, 0.0014427111627810518, 0.0014374484186022486, 0.0014350904418681113, 0.0014173023720887128, 0.0014061633720873858, 0.0015071344186105497, 0.0015778122790644099, 0.002473772209309255, 0.0013809455348839333, 0.0014480840465144274, 0.00142770881395632, 0.0013981082093080257, 0.0015545626511718797, 0.0014956715116314562, 0.001500983558122074, 0.006438196534894137, 0.0014479992093139689, 0.0014164387674438414, 0.0014684280232507073, 0.0014738924186065898, 0.0015045429302327372, 0.0015535468837062877, 0.0015159679302340983, 0.0014455643720920792, 0.001450337418603784, 0.0015064101162775038, 0.001452846883715443, 0.0014297910930233843, 0.0014561878372047051, 0.0015450838372152945, 0.0015392949069918845, 0.006219117767444871, 0.0015473672093046548, 0.0015445240465047645, 0.001528040139546224, 0.0015222468837237102, 0.0015087123255673146, 0.001523660627923206, 0.0015496149302381063, 0.006361583255818195, 0.0014914496976754974, 0.0014153607907098566, 0.001434261558135277, 0.0014295238837126647, 0.0014745545813850693, 0.0014157441627989905, 0.0065212772325499236, 0.0015591173023283265, 0.0015264390465237613, 0.00151274351163349, 0.0014976166279201035, 0.0015098043720997317, 0.001546397069771131, 0.0015996301162674574, 0.0014136615348666795, 0.0014481710930255736, 0.0014200001162672103, 0.0014169963953336688, 0.0014089462558204528, 0.0014249919069782685, 0.0013872590930183177, 0.001409647069775992, 0.0014342238604682321, 0.0014832474186077043, 0.0014553334418705227, 0.0014068619534872448, 0.0014722218372066151, 0.001475911906963183, 0.0014650481627837128, 0.001694032441856655, 0.001489267581386822, 0.0017098303255701213, 0.0013894380232624488, 0.0014060530465094204, 0.001468934279072591, 0.0015154508837229006, 0.0015448118604602546, 0.0016449161860501314, 0.0015669573488192453, 0.0015261653721016807, 0.0015369427209259092, 0.0014935841395421195, 0.001476628162785092, 0.0016981230465230616, 0.004463483255803599, 0.0013847903255737037, 0.001433325395335614, 0.0014459029534918955, 0.001574065837197596, 0.001476633627896453, 0.0015499952790610963, 0.0015040050232549452, 0.0015136410930122217, 0.0015326574186135902, 0.0014744804651177959, 0.0014011260465088336, 0.0014360371860464003, 0.0014346366278914502, 0.0015200043023236673, 0.0015211883023449593, 0.0016079995348898296, 0.0015334107209254884, 0.001506285930232945, 0.00442992700002008, 0.004851223372095901, 0.0014064009767321825, 0.0014106558139542888, 0.001387238534863532, 0.0013753949534834964, 0.001524568674413299, 0.001506303116277491, 0.0015194718372122436, 0.0015348918372140804, 0.0015226046279019291, 0.0015666796744183155, 0.0015711910930190772, 0.001532118465124662, 0.001567913837218014, 0.0014394829767572742, 0.0014196107906892118, 0.0014340462790684353, 0.0013945438372185129, 0.00143170069766838, 0.0014165609069772642, 0.001465027790694622, 0.0015979518139554488, 0.0015765594418426614, 0.001576034767441702, 0.0015587478139507788, 0.0023485359069825283, 0.0015931703953546551, 0.0063798647209317326, 0.0014998057906987648, 0.0015376132790807723, 0.0015602868371959546, 0.001526331674413509, 0.00477023511626443, 0.0016812699534987819, 0.0016965869069683196, 0.0015710274651081458, 0.0015430369302373227, 0.0015476346046599183, 0.0014981427441874307]
[667.515503657352, 645.3764031729806, 628.0622674649248, 650.2749879925452, 253.1499389356768, 717.0556911086485, 706.3056075279721, 672.1185358091816, 660.5122140110558, 678.0153387657531, 684.8516252606669, 689.0079693054757, 649.9355433771332, 588.8801099636946, 671.0914368814746, 667.2025111549983, 654.3612058987385, 636.1069790222284, 665.6353856677415, 654.1127465257892, 644.3971317023792, 645.13162562587, 647.183844220481, 660.8970653305049, 692.6274016102096, 185.7074106808473, 648.114831331354, 638.2711069289659, 658.938210028256, 652.8798813492614, 610.385371380503, 642.3242045918798, 647.050782972697, 659.1030694696534, 606.455002297223, 653.0699773185597, 651.5925855769265, 618.1144161196959, 615.781781713422, 651.2758441942058, 635.9909627440802, 156.56033081896595, 716.5111677281452, 704.6772794321349, 713.7364732209384, 682.041187437465, 697.2352562074797, 669.2439046620549, 140.7228465040138, 651.2472917547302, 662.8759602685427, 644.6809746308046, 669.7219483392845, 660.2992410198889, 659.3318792230936, 327.69440036426204, 642.6066180765163, 655.8228055019872, 695.5290020555183, 713.0233735338946, 703.975377751229, 718.413782857145, 699.7574545252602, 693.362837814597, 693.417636951749, 719.3339320892777, 704.5770429647403, 685.1868093317718, 692.2029240078898, 681.7848109860251, 344.30885631372837, 665.7841193828841, 638.5056182332692, 618.959909616195, 673.5816662510821, 634.5048465492182, 117.07180864942282, 629.5881054400095, 683.9864045932082, 703.3142179949414, 701.6337206358784, 688.6266146627953, 664.0423812336234, 643.4893314276904, 681.6330353571057, 706.9066531242802, 693.3639413540724, 700.9403672565738, 220.9175368060905, 677.1137208968061, 663.706815846457, 632.3090071132565, 589.315427362681, 655.961601915708, 647.0855824220497, 630.1791235529389, 633.5418905200539, 624.9773001825521, 603.5088029268131, 623.9156983263971, 221.91301109840742, 713.6113631360078, 644.1725515777828, 633.6481998445245, 609.677463024827, 163.0995195458532, 642.5367444173627, 633.5534576405169, 605.0347887388547, 100.5665418419636, 596.2960205364355, 625.1589448841409, 639.5421064604768, 486.5846352861623, 425.35792014182914, 663.0619023682226, 638.1596117856615, 587.2689755554364, 606.4869077970792, 615.46602866535, 588.4521037060606, 604.662119305831, 565.2332024041632, 599.405339717746, 572.919967756532, 561.8842010501078, 554.9488688164629, 613.1052501954493, 533.5857682068352, 578.2371855734118, 561.7684533647116, 610.7210576178026, 608.5352216103838, 644.550239288378, 493.1421304292917, 580.2182651574124, 144.76270776382555, 641.3804177184599, 569.2701109876365, 162.21329003712424, 540.9277509689499, 572.329147499593, 581.9057752489623, 568.8577576959981, 601.987800113408, 570.9222070609779, 588.6426654469309, 570.4830554648097, 599.3166090321516, 613.4086800814885, 544.0362211699891, 133.73825378471383, 436.738246609913, 564.4643268331325, 542.0713555023414, 554.498498015326, 602.0275307185756, 601.2308201561985, 591.8461385000054, 585.6462888727606, 592.2161532754104, 200.17472460255505, 523.2841743143092, 581.1588385615339, 574.1542744240994, 555.7113844913803, 596.7339860806302, 603.2637494789291, 611.8883880233234, 201.45745756308642, 571.793523863493, 588.070714767186, 574.4855061460529, 562.3704463616067, 573.7214673784841, 457.36974171188734, 603.8452811357703, 595.6167620575318, 577.5359236393533, 397.01153719996233, 584.581762063861, 567.9828140580619, 635.1915655755399, 636.0410062439382, 629.5715404069592, 634.1514599955618, 641.0356464527956, 710.4714151396463, 687.7084116348285, 697.3350627114473, 663.3529870941271, 686.1984929131279, 660.9745282377878, 647.1081099716654, 653.5762239390095, 665.0441959831745, 717.1743902778614, 706.4872568058051, 694.5234307817194, 704.4067374210096, 720.6806295498413, 706.6020624223643, 680.584018953627, 642.7118301951923, 636.6955606994428, 657.9268379620548, 703.3934332881198, 675.0795498789514, 255.1733185846846, 692.6096664123581, 701.6032678963633, 710.6337297835962, 686.2630517445854, 647.1857339076388, 642.9356333325835, 693.1394348348587, 695.6771366950222, 696.820193923292, 705.565742140314, 711.1549197271049, 663.5108240192108, 633.7889578302475, 404.2409386914518, 724.1415209645098, 690.5676520689692, 700.4229365432736, 715.252219636802, 643.2677378722354, 668.5960066921479, 666.2298161687595, 155.3229999395231, 690.608111915875, 705.9959265338646, 681.0003515094102, 678.4755707919271, 664.6536831257519, 643.6883305473897, 659.6445611125681, 691.7713381056559, 689.4947252775727, 663.8298489863465, 688.3037787455252, 699.4028742236997, 686.7245931126563, 647.2140707926247, 649.6480924205853, 160.79451095695373, 646.2590094883639, 647.4486443011266, 654.4330702575431, 656.9236637580145, 662.8168823529525, 656.3141303736576, 645.3216089279321, 157.19357269834, 670.4885867478819, 706.5336319642304, 697.2228979629961, 699.5336079330597, 678.1708948750384, 706.3423083609645, 153.34419383501412, 641.3885590947121, 655.1195098666742, 661.0505960261436, 667.7276289251705, 662.3374646937, 646.66444314202, 625.1445192425975, 707.3829027216962, 690.5261435033634, 704.225294451894, 705.7180973029391, 709.7502803026957, 701.7583714707022, 720.8458787783168, 709.3974239657807, 697.2412240258848, 674.1963528503429, 687.1277545266272, 710.8017936807944, 679.2454606551646, 677.5472135444634, 682.5714166965796, 590.3074671368174, 671.4710052768282, 584.8533536019503, 719.7154412486606, 711.2107203085528, 680.7656504764448, 659.8696208110493, 647.3280181199957, 607.933710228275, 638.1794633743754, 655.2369869478175, 650.6423345416309, 669.5304091181398, 677.2185613159944, 588.8854768489942, 224.04027139561018, 722.1309836821042, 697.6782824432197, 691.6093487360079, 635.297442056401, 677.2160548886834, 645.1632553395557, 664.8913963304557, 660.6585964245658, 652.461527184972, 678.2049838280565, 713.7116624815349, 696.3607974199692, 697.0406168074385, 657.8928746920492, 657.3808110793836, 621.8907271440934, 652.1409993771604, 663.8845785709168, 225.7373541359637, 206.13357153413543, 711.0347735419894, 708.8901418105978, 720.8565613399529, 727.0638862439299, 655.9232239143512, 663.8770040330847, 658.1234186180691, 651.511706398191, 656.7693159963322, 638.2925727119584, 636.4598198418238, 652.6910436515327, 637.7901490902863, 694.6938700537479, 704.4184269087631, 697.3275650836079, 717.0803622742687, 698.4700095687364, 705.935053744957, 682.58090826104, 625.8010981724635, 634.2926079788109, 634.5037689893413, 641.5405949891375, 425.79719434003925, 627.6792507039967, 156.74313543343553, 666.7529930885894, 650.3585873021515, 640.9077973106115, 655.1655952394807, 209.63327291571738, 594.7884799338528, 589.418671034618, 636.5261093199044, 648.0726289850999, 646.1473509244405, 667.4931370057026]
Elapsed: 0.08043080653351099~0.05041041872048806
Time per graph: 0.0018563698575121766~0.0011588463592097668
Speed: 614.1745675050245~130.3653002521247
Total Time: 0.0656
best val loss: 0.31899947266687045 test_score: 0.8837

Testing...
Test loss: 0.3663 score: 0.8837 time: 0.06s
test Score 0.8837
Epoch Time List: [0.28946014600023773, 0.2929908019996219, 0.29777926599945204, 0.5267754069991497, 0.39736853599879396, 0.271057472999928, 0.27063809399896854, 0.2791777929996897, 0.28923020000002, 0.2862319440000647, 0.27968796999903134, 0.4477909599991108, 0.2909866399995735, 0.5017175469993163, 0.28507440599878464, 0.28539106200150854, 0.29058691099999123, 0.295430973999828, 0.5097417519991723, 0.374978949999786, 0.2941008259995215, 0.29732383800001116, 0.3073254379996797, 0.29596549500092806, 0.2782364120002967, 0.6027024229997551, 0.3113775500005431, 0.3124169639986576, 0.3092012850001993, 0.30435634900095465, 0.31090348299858306, 0.29883839300055115, 0.2963336019993221, 0.4937979410015032, 0.2985385190004308, 0.2998377289995915, 0.29509157100073935, 0.29301920699981565, 0.30014524800026265, 0.2942576729992652, 0.2989058000002842, 0.5113060809999297, 0.2818638920007288, 0.2822085130001142, 0.28362122800081124, 0.29023633700217033, 0.29148358500060567, 0.27413094300027296, 0.7029576459999589, 0.29270030400039104, 0.28774286799944093, 0.2931235269998069, 0.28681857100036723, 0.289351999000246, 0.28904176500145695, 0.37740212000062456, 0.5685437600004661, 0.32303841700013436, 0.2708085589993061, 0.2706425799997305, 0.2786599370001568, 0.2846582820002368, 0.6333631770003194, 0.28818735200002266, 0.2909317630010264, 0.28728936099832936, 0.286456965999605, 0.2889930960000129, 0.3889500280001812, 0.2939744060004159, 0.3383712580007341, 0.4049582369998461, 0.2935332039996865, 0.29447582800003147, 0.29161565599952155, 0.41294112200012023, 0.6103350839994164, 0.3198269659988, 0.29462957499890763, 0.2880557369999224, 0.276596691001032, 0.3993376159996842, 0.2822034410000924, 0.5145550980005282, 0.29104779299996153, 0.2765295929993954, 0.2752145959993868, 0.3839714029991228, 0.4113264079996952, 0.28443816500112007, 0.28560387899869966, 0.6389193560007698, 0.3099507189999713, 0.30414823400133173, 0.2977245420006511, 0.3190402570007791, 0.3139368190004461, 0.3155934980004531, 0.31993006900029286, 0.5218939979995412, 0.5386832990006951, 0.28287206499953754, 0.3065953100003753, 0.2943921779997254, 0.31279106499914633, 0.5231791889991655, 0.31140595599936205, 0.30261627600066277, 0.31317889300044044, 0.6722106009992785, 1.2544176379997225, 0.3324633510010244, 0.3040275589992234, 1.1174616279995462, 0.4412181489997238, 0.5593521890004922, 0.30754121100017073, 0.34168522099935217, 0.3279745979998552, 0.31424817999959487, 0.32697865899990575, 0.3268408650010315, 0.5150854320008875, 0.3245178510005644, 0.3299991409994618, 0.3384837570010859, 0.3501108459995521, 0.5227461619997484, 0.3237543680006638, 0.6250344610007232, 0.3342567660001805, 0.3355705719995967, 0.3209188990003895, 0.44380688299952453, 0.32764394099922356, 0.3754979880004612, 0.5668967889996566, 0.32028515499951027, 0.320369419000599, 0.5276791460000823, 0.3603724249996958, 0.3494158569992578, 0.3409379810000246, 0.6183525510004984, 0.3495382340006472, 0.48521707999952923, 0.33534279800005606, 0.3373264209994886, 0.33579771099994105, 0.3078494810006305, 0.3403106479991038, 0.5938219480003681, 0.5116233930002636, 0.3378958529992815, 0.3455902480000077, 0.36669183600042743, 0.559398578998298, 0.32197873200038885, 0.44760629599932145, 0.339421339999717, 0.34670996500062756, 0.5639536580001732, 0.3584128869997585, 0.3789912430002005, 0.5551128460001564, 0.35577549500067107, 0.5703798230006214, 0.32215359799920407, 0.32444035399930726, 0.466197760999421, 0.33174908999990294, 0.334722478999538, 0.3394982670006357, 0.5649390720000156, 0.3388705769984881, 0.3556810989994119, 0.342360627000744, 0.46340029499970115, 0.34327232600026036, 0.5179717049995816, 0.32841140600066865, 0.3280198919992472, 0.3480130830002963, 0.32827695600008155, 0.44980827099971066, 0.32870412499960366, 0.32639792500049225, 0.5295789649999278, 0.3016752899993662, 0.29803946599986375, 0.3102510699991399, 0.3184322429997337, 0.42084803100078716, 0.31967778299986094, 0.3205563620003886, 0.4941518250006993, 0.28843668999979855, 0.2942598129993712, 0.30297993600015616, 0.29777536199890164, 0.4351908779999576, 0.2946196869997948, 0.5164789640011804, 0.3160503380004229, 0.3244802730005176, 0.31571517100019264, 0.4014665380000224, 0.30207478900047136, 0.48779189999913797, 0.30076982799982943, 0.30022181699951034, 0.3022670139989714, 0.30291175399906933, 0.41118690999974206, 0.32786406600007467, 0.417170234999503, 0.30395456200039916, 0.2960109220011873, 0.296169720000762, 0.29663903399978153, 0.4388586580007541, 0.31309899000098085, 0.5004730559994641, 0.28926152500116586, 0.2978262149999864, 0.29785200099831854, 0.2976908969994838, 0.41699769899969397, 0.3168501969994395, 0.3133093430005829, 0.5265407729993967, 0.3055039269993358, 0.2987351829997351, 0.4201820959997349, 0.31167215300047246, 0.31521914300083154, 0.32301534400085075, 0.32202536999920994, 0.5147490869994726, 0.30613029199867015, 0.3077130509991548, 0.3059504660004677, 0.2961638429997038, 0.3005231420002019, 0.3094704710001679, 0.3198167000000467, 0.5320465349996084, 0.3219397730008495, 0.32720008799969946, 0.32621429199934937, 0.3219939450000311, 0.3183723980009745, 0.31983881300038774, 0.32043078999959107, 0.5271712619996833, 0.33206214500114584, 0.31395608999991964, 0.305009286999848, 0.31603730200004065, 0.3147706999998263, 0.3125548689995412, 0.5277690749990143, 0.32215717300005053, 0.3200701099995058, 0.31800173000010545, 0.3161131180013399, 0.3177962239988119, 0.32070806399951834, 0.33242331099972944, 0.5536776279996047, 0.30057431400109635, 0.2978668880004989, 0.2982998059997044, 0.29862530599984893, 0.2953412839997327, 0.5191578909998498, 0.29782431199873827, 0.29837159999988216, 0.30346991700025683, 0.4319863949995124, 0.29654993599979207, 0.3018085189996782, 0.5209591120010373, 0.2978676409993568, 0.3138942950008641, 0.3099319500006459, 0.31611427899952105, 0.40546810700016067, 0.29074257699903683, 0.29870221899909666, 0.5106658069998957, 0.3360366830002022, 0.4414936260000104, 0.3319717819995276, 0.33243004199903226, 0.32563573100014764, 0.3221102930001507, 0.31651972599956935, 0.32397115600088, 0.6799187000005986, 0.30178074200011906, 0.29866430400124955, 0.3020684099983555, 0.31964878200051317, 0.30855112800054485, 0.3279227560005893, 0.6317798050004058, 0.3323974160011858, 0.32597210299991275, 0.3294930210004168, 0.30155860999911965, 0.30447137500050303, 0.3009842870005741, 0.662602789999255, 0.31860837299973355, 0.33038072699946497, 0.3308352879994345, 0.3188707789995533, 0.44118560700007947, 0.5094524830001319, 0.3491410269998596, 0.317226808999294, 0.2967649780011925, 0.28600577500037616, 0.4261790630007454, 0.5501191930015921, 0.31945841800097696, 0.36355518599975767, 0.32144599799994467, 0.33181619999959366, 0.3263832539996656, 0.4653187160001835, 0.3245275039998887, 0.5273924650000481, 0.301272922998578, 0.29882140799963963, 0.29461833700042916, 0.4284584229999382, 0.3008182509993276, 0.302663802999632, 0.5637249810015419, 0.34249579700099275, 0.3375341339997249, 0.3301964339998449, 0.35548040100093203, 0.41039778100002877, 0.5483087059992613, 0.31933910000134347, 0.3226909120012351, 0.3243660420002925, 0.33725170599973353, 0.45372554100049456, 0.3624460380005985, 0.46011194699985936, 0.3463611369998034, 0.3209997570002088, 0.3228446470011477, 0.4508857499995429]
Total Epoch List: [113, 68, 177]
Total Time List: [0.06943193200004316, 0.07428210499983834, 0.06560368900045432]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f398670>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8080;  Loss pred: 0.8080; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5116 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7090 score: 0.5000 time: 0.06s
Epoch 2/1000, LR 0.000015
Train loss: 0.7024;  Loss pred: 0.7024; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.5116 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7069 score: 0.5000 time: 0.06s
Epoch 3/1000, LR 0.000045
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.18s
Val loss: 0.6969 score: 0.5116 time: 0.06s
Test loss: 0.7040 score: 0.5000 time: 0.06s
Epoch 4/1000, LR 0.000075
Train loss: 0.7690;  Loss pred: 0.7690; Loss self: 0.0000; time: 0.18s
Val loss: 0.6939 score: 0.5116 time: 0.06s
Test loss: 0.7006 score: 0.4091 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.6454;  Loss pred: 0.6454; Loss self: 0.0000; time: 0.25s
Val loss: 0.6911 score: 0.5116 time: 0.18s
Test loss: 0.6972 score: 0.4091 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.06s
Epoch 7/1000, LR 0.000165
Train loss: 0.6274;  Loss pred: 0.6274; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.07s
Epoch 8/1000, LR 0.000195
Train loss: 0.6486;  Loss pred: 0.6486; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.17s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000225
Train loss: 0.6265;  Loss pred: 0.6265; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.4884 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000255
Train loss: 0.5974;  Loss pred: 0.5974; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7002 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7034 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7036 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5723;  Loss pred: 0.5723; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7057 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7057 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5862;  Loss pred: 0.5862; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7073 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7078 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7062 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7071 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5497;  Loss pred: 0.5497; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7045 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7061 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5736;  Loss pred: 0.5736; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7005 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7028 score: 0.5000 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.5492;  Loss pred: 0.5492; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 20/1000, LR 0.000285
Train loss: 0.5334;  Loss pred: 0.5334; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6812 score: 0.4884 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6874 score: 0.5000 time: 0.06s
Epoch 21/1000, LR 0.000285
Train loss: 0.5212;  Loss pred: 0.5212; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6704 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6787 score: 0.5000 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.5100;  Loss pred: 0.5100; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6599 score: 0.4884 time: 0.06s
Test loss: 0.6711 score: 0.5227 time: 0.06s
Epoch 23/1000, LR 0.000285
Train loss: 0.4995;  Loss pred: 0.4995; Loss self: 0.0000; time: 0.16s
Val loss: 0.6480 score: 0.5116 time: 0.06s
Test loss: 0.6617 score: 0.5455 time: 0.06s
Epoch 24/1000, LR 0.000285
Train loss: 0.5912;  Loss pred: 0.5912; Loss self: 0.0000; time: 0.17s
Val loss: 0.6377 score: 0.5349 time: 0.06s
Test loss: 0.6543 score: 0.5682 time: 0.06s
Epoch 25/1000, LR 0.000285
Train loss: 0.4905;  Loss pred: 0.4905; Loss self: 0.0000; time: 0.17s
Val loss: 0.6266 score: 0.5814 time: 0.06s
Test loss: 0.6466 score: 0.5682 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4901;  Loss pred: 0.4901; Loss self: 0.0000; time: 0.17s
Val loss: 0.6162 score: 0.6047 time: 0.06s
Test loss: 0.6393 score: 0.5909 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.4794;  Loss pred: 0.4794; Loss self: 0.0000; time: 0.20s
Val loss: 0.6057 score: 0.6047 time: 0.07s
Test loss: 0.6318 score: 0.6136 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.4639;  Loss pred: 0.4639; Loss self: 0.0000; time: 0.18s
Val loss: 0.5926 score: 0.6279 time: 0.28s
Test loss: 0.6221 score: 0.6364 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.4604;  Loss pred: 0.4604; Loss self: 0.0000; time: 0.17s
Val loss: 0.5802 score: 0.6512 time: 0.07s
Test loss: 0.6136 score: 0.6364 time: 0.06s
Epoch 30/1000, LR 0.000285
Train loss: 0.4236;  Loss pred: 0.4236; Loss self: 0.0000; time: 0.34s
Val loss: 0.5692 score: 0.6977 time: 0.07s
Test loss: 0.6057 score: 0.6364 time: 0.06s
Epoch 31/1000, LR 0.000285
Train loss: 0.4297;  Loss pred: 0.4297; Loss self: 0.0000; time: 0.18s
Val loss: 0.5570 score: 0.7442 time: 0.07s
Test loss: 0.5957 score: 0.6364 time: 0.06s
Epoch 32/1000, LR 0.000285
Train loss: 0.4200;  Loss pred: 0.4200; Loss self: 0.0000; time: 0.17s
Val loss: 0.5462 score: 0.7907 time: 0.06s
Test loss: 0.5863 score: 0.6591 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.4474;  Loss pred: 0.4474; Loss self: 0.0000; time: 0.17s
Val loss: 0.5369 score: 0.7907 time: 0.06s
Test loss: 0.5774 score: 0.6591 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.4178;  Loss pred: 0.4178; Loss self: 0.0000; time: 0.17s
Val loss: 0.5280 score: 0.8140 time: 0.08s
Test loss: 0.5692 score: 0.6818 time: 0.07s
Epoch 35/1000, LR 0.000285
Train loss: 0.3884;  Loss pred: 0.3884; Loss self: 0.0000; time: 0.17s
Val loss: 0.5196 score: 0.8140 time: 0.07s
Test loss: 0.5617 score: 0.6818 time: 0.06s
Epoch 36/1000, LR 0.000285
Train loss: 0.3879;  Loss pred: 0.3879; Loss self: 0.0000; time: 0.33s
Val loss: 0.5125 score: 0.8140 time: 0.06s
Test loss: 0.5553 score: 0.7273 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.3818;  Loss pred: 0.3818; Loss self: 0.0000; time: 0.17s
Val loss: 0.5048 score: 0.8140 time: 0.06s
Test loss: 0.5478 score: 0.7273 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3636;  Loss pred: 0.3636; Loss self: 0.0000; time: 0.17s
Val loss: 0.4988 score: 0.8140 time: 0.07s
Test loss: 0.5424 score: 0.7273 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.3816;  Loss pred: 0.3816; Loss self: 0.0000; time: 0.17s
Val loss: 0.4936 score: 0.8140 time: 0.06s
Test loss: 0.5381 score: 0.7500 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.3742;  Loss pred: 0.3742; Loss self: 0.0000; time: 0.17s
Val loss: 0.4883 score: 0.8140 time: 0.06s
Test loss: 0.5339 score: 0.7500 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3690;  Loss pred: 0.3690; Loss self: 0.0000; time: 0.17s
Val loss: 0.4828 score: 0.8605 time: 0.06s
Test loss: 0.5291 score: 0.7500 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3559;  Loss pred: 0.3559; Loss self: 0.0000; time: 0.17s
Val loss: 0.4771 score: 0.8837 time: 0.06s
Test loss: 0.5239 score: 0.7727 time: 0.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.3677;  Loss pred: 0.3677; Loss self: 0.0000; time: 0.17s
Val loss: 0.4706 score: 0.8837 time: 0.06s
Test loss: 0.5175 score: 0.8182 time: 0.06s
Epoch 44/1000, LR 0.000284
Train loss: 0.3541;  Loss pred: 0.3541; Loss self: 0.0000; time: 0.17s
Val loss: 0.4641 score: 0.8837 time: 0.11s
Test loss: 0.5111 score: 0.8182 time: 0.18s
Epoch 45/1000, LR 0.000284
Train loss: 0.3556;  Loss pred: 0.3556; Loss self: 0.0000; time: 0.17s
Val loss: 0.4575 score: 0.9070 time: 0.06s
Test loss: 0.5043 score: 0.8409 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 0.17s
Val loss: 0.4506 score: 0.8837 time: 0.06s
Test loss: 0.4972 score: 0.8409 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3118;  Loss pred: 0.3118; Loss self: 0.0000; time: 0.16s
Val loss: 0.4423 score: 0.8837 time: 0.06s
Test loss: 0.4898 score: 0.8409 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3225;  Loss pred: 0.3225; Loss self: 0.0000; time: 0.15s
Val loss: 0.4340 score: 0.8837 time: 0.06s
Test loss: 0.4820 score: 0.8409 time: 0.06s
Epoch 49/1000, LR 0.000284
Train loss: 0.3320;  Loss pred: 0.3320; Loss self: 0.0000; time: 0.17s
Val loss: 0.4256 score: 0.8837 time: 0.06s
Test loss: 0.4740 score: 0.8409 time: 0.06s
Epoch 50/1000, LR 0.000284
Train loss: 0.2969;  Loss pred: 0.2969; Loss self: 0.0000; time: 0.17s
Val loss: 0.4182 score: 0.8837 time: 0.06s
Test loss: 0.4674 score: 0.8182 time: 0.06s
Epoch 51/1000, LR 0.000284
Train loss: 0.3010;  Loss pred: 0.3010; Loss self: 0.0000; time: 0.16s
Val loss: 0.4104 score: 0.8605 time: 0.06s
Test loss: 0.4599 score: 0.8182 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.2921;  Loss pred: 0.2921; Loss self: 0.0000; time: 0.16s
Val loss: 0.4032 score: 0.8605 time: 0.06s
Test loss: 0.4531 score: 0.8182 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.2933;  Loss pred: 0.2933; Loss self: 0.0000; time: 0.16s
Val loss: 0.3949 score: 0.8605 time: 0.23s
Test loss: 0.4468 score: 0.8182 time: 0.08s
Epoch 54/1000, LR 0.000284
Train loss: 0.2799;  Loss pred: 0.2799; Loss self: 0.0000; time: 0.16s
Val loss: 0.3879 score: 0.8605 time: 0.06s
Test loss: 0.4414 score: 0.8182 time: 0.06s
Epoch 55/1000, LR 0.000284
Train loss: 0.2962;  Loss pred: 0.2962; Loss self: 0.0000; time: 0.15s
Val loss: 0.3820 score: 0.8605 time: 0.06s
Test loss: 0.4371 score: 0.8182 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.2982;  Loss pred: 0.2982; Loss self: 0.0000; time: 0.17s
Val loss: 0.3775 score: 0.8372 time: 0.06s
Test loss: 0.4343 score: 0.8182 time: 0.06s
Epoch 57/1000, LR 0.000283
Train loss: 0.2688;  Loss pred: 0.2688; Loss self: 0.0000; time: 0.17s
Val loss: 0.3730 score: 0.8605 time: 0.06s
Test loss: 0.4311 score: 0.8182 time: 0.06s
Epoch 58/1000, LR 0.000283
Train loss: 0.2778;  Loss pred: 0.2778; Loss self: 0.0000; time: 0.17s
Val loss: 0.3693 score: 0.8837 time: 0.06s
Test loss: 0.4297 score: 0.8182 time: 0.06s
Epoch 59/1000, LR 0.000283
Train loss: 0.2854;  Loss pred: 0.2854; Loss self: 0.0000; time: 0.17s
Val loss: 0.3658 score: 0.9070 time: 0.07s
Test loss: 0.4281 score: 0.8182 time: 0.06s
Epoch 60/1000, LR 0.000283
Train loss: 0.2604;  Loss pred: 0.2604; Loss self: 0.0000; time: 0.17s
Val loss: 0.3631 score: 0.9070 time: 0.27s
Test loss: 0.4261 score: 0.8182 time: 0.06s
Epoch 61/1000, LR 0.000283
Train loss: 0.2748;  Loss pred: 0.2748; Loss self: 0.0000; time: 0.17s
Val loss: 0.3610 score: 0.9070 time: 0.07s
Test loss: 0.4253 score: 0.8182 time: 0.06s
Epoch 62/1000, LR 0.000283
Train loss: 0.2811;  Loss pred: 0.2811; Loss self: 0.0000; time: 0.18s
Val loss: 0.3602 score: 0.8837 time: 0.07s
Test loss: 0.4244 score: 0.8182 time: 0.06s
Epoch 63/1000, LR 0.000283
Train loss: 0.2885;  Loss pred: 0.2885; Loss self: 0.0000; time: 0.17s
Val loss: 0.3589 score: 0.8837 time: 0.07s
Test loss: 0.4234 score: 0.8182 time: 0.07s
Epoch 64/1000, LR 0.000283
Train loss: 0.2526;  Loss pred: 0.2526; Loss self: 0.0000; time: 0.18s
Val loss: 0.3587 score: 0.8837 time: 0.07s
Test loss: 0.4232 score: 0.8182 time: 0.07s
Epoch 65/1000, LR 0.000283
Train loss: 0.2692;  Loss pred: 0.2692; Loss self: 0.0000; time: 0.18s
Val loss: 0.3582 score: 0.8837 time: 0.06s
Test loss: 0.4228 score: 0.8182 time: 0.06s
Epoch 66/1000, LR 0.000283
Train loss: 0.2548;  Loss pred: 0.2548; Loss self: 0.0000; time: 0.15s
Val loss: 0.3580 score: 0.8837 time: 0.06s
Test loss: 0.4226 score: 0.8182 time: 0.06s
Epoch 67/1000, LR 0.000283
Train loss: 0.2616;  Loss pred: 0.2616; Loss self: 0.0000; time: 0.16s
Val loss: 0.3575 score: 0.8837 time: 0.06s
Test loss: 0.4224 score: 0.8182 time: 0.06s
Epoch 68/1000, LR 0.000283
Train loss: 0.2302;  Loss pred: 0.2302; Loss self: 0.0000; time: 0.35s
Val loss: 0.3571 score: 0.8837 time: 0.06s
Test loss: 0.4233 score: 0.8182 time: 0.06s
Epoch 69/1000, LR 0.000283
Train loss: 0.2448;  Loss pred: 0.2448; Loss self: 0.0000; time: 0.16s
Val loss: 0.3559 score: 0.8837 time: 0.06s
Test loss: 0.4227 score: 0.8182 time: 0.06s
Epoch 70/1000, LR 0.000283
Train loss: 0.2345;  Loss pred: 0.2345; Loss self: 0.0000; time: 0.16s
Val loss: 0.3549 score: 0.8837 time: 0.06s
Test loss: 0.4219 score: 0.8182 time: 0.06s
Epoch 71/1000, LR 0.000282
Train loss: 0.2340;  Loss pred: 0.2340; Loss self: 0.0000; time: 0.16s
Val loss: 0.3536 score: 0.8837 time: 0.06s
Test loss: 0.4213 score: 0.8182 time: 0.06s
Epoch 72/1000, LR 0.000282
Train loss: 0.2160;  Loss pred: 0.2160; Loss self: 0.0000; time: 0.16s
Val loss: 0.3528 score: 0.8837 time: 0.06s
Test loss: 0.4213 score: 0.8182 time: 0.06s
Epoch 73/1000, LR 0.000282
Train loss: 0.2195;  Loss pred: 0.2195; Loss self: 0.0000; time: 0.16s
Val loss: 0.3527 score: 0.8837 time: 0.06s
Test loss: 0.4225 score: 0.8182 time: 0.06s
Epoch 74/1000, LR 0.000282
Train loss: 0.2407;  Loss pred: 0.2407; Loss self: 0.0000; time: 0.16s
Val loss: 0.3526 score: 0.8837 time: 0.06s
Test loss: 0.4232 score: 0.8182 time: 0.06s
Epoch 75/1000, LR 0.000282
Train loss: 0.2219;  Loss pred: 0.2219; Loss self: 0.0000; time: 0.16s
Val loss: 0.3526 score: 0.8837 time: 0.06s
Test loss: 0.4239 score: 0.8182 time: 0.06s
Epoch 76/1000, LR 0.000282
Train loss: 0.2102;  Loss pred: 0.2102; Loss self: 0.0000; time: 0.16s
Val loss: 0.3511 score: 0.8837 time: 0.06s
Test loss: 0.4231 score: 0.8182 time: 0.06s
Epoch 77/1000, LR 0.000282
Train loss: 0.2348;  Loss pred: 0.2348; Loss self: 0.0000; time: 0.16s
Val loss: 0.3494 score: 0.8837 time: 0.26s
Test loss: 0.4211 score: 0.8182 time: 0.06s
Epoch 78/1000, LR 0.000282
Train loss: 0.2116;  Loss pred: 0.2116; Loss self: 0.0000; time: 0.17s
Val loss: 0.3475 score: 0.8837 time: 0.06s
Test loss: 0.4191 score: 0.8182 time: 0.06s
Epoch 79/1000, LR 0.000282
Train loss: 0.2004;  Loss pred: 0.2004; Loss self: 0.0000; time: 0.16s
Val loss: 0.3465 score: 0.8837 time: 0.06s
Test loss: 0.4179 score: 0.8182 time: 0.06s
Epoch 80/1000, LR 0.000282
Train loss: 0.1966;  Loss pred: 0.1966; Loss self: 0.0000; time: 0.16s
Val loss: 0.3443 score: 0.8837 time: 0.06s
Test loss: 0.4155 score: 0.8182 time: 0.06s
Epoch 81/1000, LR 0.000281
Train loss: 0.2124;  Loss pred: 0.2124; Loss self: 0.0000; time: 0.16s
Val loss: 0.3428 score: 0.8837 time: 0.06s
Test loss: 0.4141 score: 0.8182 time: 0.06s
Epoch 82/1000, LR 0.000281
Train loss: 0.1969;  Loss pred: 0.1969; Loss self: 0.0000; time: 0.16s
Val loss: 0.3415 score: 0.8605 time: 0.06s
Test loss: 0.4133 score: 0.8182 time: 0.06s
Epoch 83/1000, LR 0.000281
Train loss: 0.1932;  Loss pred: 0.1932; Loss self: 0.0000; time: 0.16s
Val loss: 0.3410 score: 0.8605 time: 0.06s
Test loss: 0.4140 score: 0.8182 time: 0.06s
Epoch 84/1000, LR 0.000281
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.16s
Val loss: 0.3396 score: 0.8837 time: 0.07s
Test loss: 0.4138 score: 0.8182 time: 0.06s
Epoch 85/1000, LR 0.000281
Train loss: 0.2105;  Loss pred: 0.2105; Loss self: 0.0000; time: 0.36s
Val loss: 0.3391 score: 0.8837 time: 0.06s
Test loss: 0.4150 score: 0.8182 time: 0.06s
Epoch 86/1000, LR 0.000281
Train loss: 0.1886;  Loss pred: 0.1886; Loss self: 0.0000; time: 0.15s
Val loss: 0.3382 score: 0.8837 time: 0.07s
Test loss: 0.4147 score: 0.8182 time: 0.06s
Epoch 87/1000, LR 0.000281
Train loss: 0.1921;  Loss pred: 0.1921; Loss self: 0.0000; time: 0.17s
Val loss: 0.3365 score: 0.8837 time: 0.06s
Test loss: 0.4144 score: 0.8182 time: 0.06s
Epoch 88/1000, LR 0.000281
Train loss: 0.2064;  Loss pred: 0.2064; Loss self: 0.0000; time: 0.17s
Val loss: 0.3359 score: 0.8837 time: 0.06s
Test loss: 0.4144 score: 0.8182 time: 0.07s
Epoch 89/1000, LR 0.000281
Train loss: 0.1711;  Loss pred: 0.1711; Loss self: 0.0000; time: 0.17s
Val loss: 0.3349 score: 0.8837 time: 0.06s
Test loss: 0.4147 score: 0.8182 time: 0.06s
Epoch 90/1000, LR 0.000281
Train loss: 0.1788;  Loss pred: 0.1788; Loss self: 0.0000; time: 0.18s
Val loss: 0.3348 score: 0.8837 time: 0.06s
Test loss: 0.4154 score: 0.8182 time: 0.06s
Epoch 91/1000, LR 0.000280
Train loss: 0.1677;  Loss pred: 0.1677; Loss self: 0.0000; time: 0.16s
Val loss: 0.3356 score: 0.8837 time: 0.06s
Test loss: 0.4160 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 92/1000, LR 0.000280
Train loss: 0.1802;  Loss pred: 0.1802; Loss self: 0.0000; time: 0.29s
Val loss: 0.3359 score: 0.8837 time: 0.06s
Test loss: 0.4156 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 93/1000, LR 0.000280
Train loss: 0.1619;  Loss pred: 0.1619; Loss self: 0.0000; time: 0.16s
Val loss: 0.3369 score: 0.9070 time: 0.06s
Test loss: 0.4159 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 94/1000, LR 0.000280
Train loss: 0.1682;  Loss pred: 0.1682; Loss self: 0.0000; time: 0.29s
Val loss: 0.3375 score: 0.8837 time: 0.06s
Test loss: 0.4171 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.17s
Val loss: 0.3365 score: 0.8837 time: 0.06s
Test loss: 0.4159 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.1771;  Loss pred: 0.1771; Loss self: 0.0000; time: 0.17s
Val loss: 0.3360 score: 0.9070 time: 0.06s
Test loss: 0.4154 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 97/1000, LR 0.000280
Train loss: 0.1856;  Loss pred: 0.1856; Loss self: 0.0000; time: 0.17s
Val loss: 0.3350 score: 0.9070 time: 0.06s
Test loss: 0.4144 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 98/1000, LR 0.000280
Train loss: 0.1668;  Loss pred: 0.1668; Loss self: 0.0000; time: 0.17s
Val loss: 0.3344 score: 0.8837 time: 0.06s
Test loss: 0.4140 score: 0.8182 time: 0.06s
Epoch 99/1000, LR 0.000279
Train loss: 0.1741;  Loss pred: 0.1741; Loss self: 0.0000; time: 0.17s
Val loss: 0.3337 score: 0.9070 time: 0.06s
Test loss: 0.4124 score: 0.8182 time: 0.07s
Epoch 100/1000, LR 0.000279
Train loss: 0.1568;  Loss pred: 0.1568; Loss self: 0.0000; time: 0.29s
Val loss: 0.3339 score: 0.9070 time: 0.06s
Test loss: 0.4114 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.18s
Val loss: 0.3340 score: 0.8837 time: 0.06s
Test loss: 0.4105 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.18s
Val loss: 0.3350 score: 0.8837 time: 0.06s
Test loss: 0.4113 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1541;  Loss pred: 0.1541; Loss self: 0.0000; time: 0.17s
Val loss: 0.3370 score: 0.8837 time: 0.07s
Test loss: 0.4152 score: 0.8182 time: 0.29s
     INFO: Early stopping counter 4 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1508;  Loss pred: 0.1508; Loss self: 0.0000; time: 0.15s
Val loss: 0.3373 score: 0.8605 time: 0.06s
Test loss: 0.4161 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1476;  Loss pred: 0.1476; Loss self: 0.0000; time: 0.15s
Val loss: 0.3376 score: 0.8605 time: 0.06s
Test loss: 0.4165 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1570;  Loss pred: 0.1570; Loss self: 0.0000; time: 0.16s
Val loss: 0.3385 score: 0.8605 time: 0.16s
Test loss: 0.4182 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1486;  Loss pred: 0.1486; Loss self: 0.0000; time: 0.17s
Val loss: 0.3393 score: 0.8605 time: 0.06s
Test loss: 0.4187 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1571;  Loss pred: 0.1571; Loss self: 0.0000; time: 0.17s
Val loss: 0.3398 score: 0.8605 time: 0.07s
Test loss: 0.4190 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1485;  Loss pred: 0.1485; Loss self: 0.0000; time: 0.17s
Val loss: 0.3390 score: 0.8837 time: 0.06s
Test loss: 0.4182 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1477;  Loss pred: 0.1477; Loss self: 0.0000; time: 0.17s
Val loss: 0.3384 score: 0.8837 time: 0.06s
Test loss: 0.4182 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1415;  Loss pred: 0.1415; Loss self: 0.0000; time: 0.17s
Val loss: 0.3373 score: 0.8837 time: 0.06s
Test loss: 0.4160 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 112/1000, LR 0.000278
Train loss: 0.1521;  Loss pred: 0.1521; Loss self: 0.0000; time: 0.17s
Val loss: 0.3370 score: 0.8837 time: 0.06s
Test loss: 0.4137 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 113/1000, LR 0.000278
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.38s
Val loss: 0.3389 score: 0.8837 time: 0.06s
Test loss: 0.4142 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 114/1000, LR 0.000277
Train loss: 0.1384;  Loss pred: 0.1384; Loss self: 0.0000; time: 0.28s
Val loss: 0.3414 score: 0.8837 time: 0.06s
Test loss: 0.4152 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 115/1000, LR 0.000277
Train loss: 0.1343;  Loss pred: 0.1343; Loss self: 0.0000; time: 0.17s
Val loss: 0.3422 score: 0.8837 time: 0.06s
Test loss: 0.4124 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 116/1000, LR 0.000277
Train loss: 0.1349;  Loss pred: 0.1349; Loss self: 0.0000; time: 0.17s
Val loss: 0.3426 score: 0.9070 time: 0.06s
Test loss: 0.4098 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1362;  Loss pred: 0.1362; Loss self: 0.0000; time: 0.17s
Val loss: 0.3421 score: 0.9070 time: 0.06s
Test loss: 0.4051 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 118/1000, LR 0.000277
Train loss: 0.1374;  Loss pred: 0.1374; Loss self: 0.0000; time: 0.17s
Val loss: 0.3419 score: 0.9070 time: 0.06s
Test loss: 0.4008 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 119/1000, LR 0.000277
Train loss: 0.1269;  Loss pred: 0.1269; Loss self: 0.0000; time: 0.17s
Val loss: 0.3425 score: 0.9070 time: 0.06s
Test loss: 0.3981 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 098,   Train_Loss: 0.1741,   Val_Loss: 0.3337,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9070,   Val_Loss: 0.3337,   Test_Precision: 0.9375,   Test_Recall: 0.6818,   Test_accuracy: 0.7895,   Test_Score: 0.8182,   Test_loss: 0.4124


[0.06446969199987507, 0.06739953799933573, 0.06648357499943813, 0.06758560599973862, 0.06738812200001121, 0.06710440599999856, 0.08056942100029119, 0.17509985200013034, 0.07131669000045804, 0.06412443899989739, 0.06837282000014966, 0.06592467199970997, 0.06553784399966389, 0.06337089499993454, 0.062299672000335704, 0.06690161500046088, 0.06560062500011554, 0.07608798299952468, 0.073636236999846, 0.06533113499972387, 0.0689830230003281, 0.061442332999831706, 0.06847192399982305, 0.06883335100064869, 0.06696296599966445, 0.06939569799942547, 0.06422895200012135, 0.06721065100009582, 0.0684434839995447, 0.06812691599952814, 0.06882696600041527, 0.06779346799976338, 0.06794366899976012, 0.07018917000004876, 0.06816678099949058, 0.06644392799989873, 0.06804566899972997, 0.06908714400015015, 0.06838531599987618, 0.06672030800018547, 0.0680080789998101, 0.06776118499965378, 0.0678581000001941, 0.18484589800027607, 0.06670466600007785, 0.062036368000008224, 0.06299412599946663, 0.0682391930004087, 0.0675870989998657, 0.06669125699954748, 0.06246435000048223, 0.06730252400029713, 0.0799949350002862, 0.06102383400047984, 0.06101523300003464, 0.06725154099967767, 0.06652604499959125, 0.0687209099996835, 0.06839494699943316, 0.06975308100027178, 0.06908792899957916, 0.06938369599993166, 0.07001804599985917, 0.07115857400003733, 0.060237153000343824, 0.06167264699979569, 0.06376186400029837, 0.06241424600011669, 0.0635969760005537, 0.06397705499966833, 0.06351719000031153, 0.06311881400051789, 0.06462976900002104, 0.06561224200049764, 0.06461545599995588, 0.06779296300010174, 0.06337684000027366, 0.06453995799984114, 0.06428873999993812, 0.061633469999833324, 0.06195763799951237, 0.06303466200006369, 0.06267207599921676, 0.06887229499989189, 0.06676640800014866, 0.06911353100076667, 0.06855855899993912, 0.07268926399956399, 0.06795819499984646, 0.06685699300032866, 0.06734974199935095, 0.06483118899996043, 0.06628180799998518, 0.06668122199971549, 0.06829893899976014, 0.06640270899970346, 0.06704590399931476, 0.06804066099994088, 0.08546103000026051, 0.06689234100031172, 0.06716357599998446, 0.07005777100039268, 0.300154499999735, 0.06084551399999327, 0.060525646999849414, 0.06564192400037427, 0.06757724700037215, 0.0691543119992275, 0.06777195399990887, 0.06726434899974265, 0.06675026199991407, 0.07078908299990871, 0.06981862099928549, 0.06817521900029533, 0.06866262899984577, 0.06782407799983048, 0.06748675599919807, 0.06859721399996488, 0.0671858229998179]
[0.0014652202727244332, 0.001531807681803085, 0.0015109903408963212, 0.0015360364999940596, 0.001531548227272982, 0.0015251001363636037, 0.0018311232045520724, 0.003979542090912053, 0.0016208338636467736, 0.0014573736136340317, 0.0015539277272761285, 0.0014982879999934084, 0.0014894964545378157, 0.0014402476136348757, 0.001415901636371266, 0.0015204912500104745, 0.0014909232954571714, 0.0017292723408982883, 0.001673550840905591, 0.0014847985227209972, 0.0015677959772801842, 0.0013964166590870843, 0.0015561800909050692, 0.0015643943409238339, 0.0015218855909014646, 0.001577174954532397, 0.0014597489090936672, 0.0015275147954567233, 0.0015555337272623794, 0.0015483389999892759, 0.0015642492272821653, 0.0015407606363582586, 0.0015441742954490937, 0.0015952084090920173, 0.001549245022715695, 0.0015100892727249711, 0.0015464924772665902, 0.001570162363639776, 0.0015542117272699133, 0.0015163706363678516, 0.001545638159086593, 0.001540026931810313, 0.0015422295454589569, 0.004201043136369911, 0.0015160151363654056, 0.0014099174545456415, 0.0014316846818060599, 0.0015508907500092887, 0.0015360704318151295, 0.0015157103863533518, 0.001419644318192778, 0.001529602818188571, 0.0018180667045519592, 0.0013869053181927236, 0.0013867098409098783, 0.001528444113629038, 0.0015119555681725285, 0.0015618388636291704, 0.001554430613623481, 0.0015852972954607221, 0.00157018020453589, 0.0015769021818166286, 0.0015913192272695267, 0.0016172403181826667, 0.0013690262045532688, 0.0014016510681771747, 0.001449133272734054, 0.001418505590911743, 0.0014453858181944022, 0.0014540239772651892, 0.0014435725000070802, 0.0014345185000117701, 0.0014688583863641145, 0.0014911873181931283, 0.0014685330909080883, 0.0015407491590932213, 0.0014403827272789467, 0.0014668172272691168, 0.0014611077272713208, 0.0014007606818143938, 0.0014081281363525538, 0.001432605954546902, 0.0014243653636185627, 0.0015652794318157248, 0.0015174183636397422, 0.0015707620681992425, 0.0015581490681804346, 0.001652028727262818, 0.0015445044318146922, 0.001519477113643833, 0.0015306759545307034, 0.0014734361136354644, 0.0015064047272723903, 0.001515482318175352, 0.0015522486136309124, 0.0015091524772659877, 0.0015237705454389718, 0.0015463786590895654, 0.001942296136369557, 0.0015202804772798117, 0.001526444909090556, 0.0015922220681907427, 0.006821693181812158, 0.001382852590908938, 0.001375582886360214, 0.0014918619090994152, 0.0015358465227357308, 0.0015716889090733523, 0.0015402716818161107, 0.0015287352045396058, 0.0015170514090889562, 0.0016088427954524707, 0.0015867868408928518, 0.0015494367954612574, 0.0015605142954510402, 0.0015414563181779654, 0.0015337899090726833, 0.0015590275909082927, 0.001526950522723134]
[682.491239450706, 652.823465947699, 661.8175993148963, 651.0261963201183, 652.9340586163343, 655.6946499161462, 546.1128980912123, 251.28519240534393, 616.9663791143056, 686.1658470036737, 643.5305725272658, 667.4284249786418, 671.3678283378631, 694.3250525346921, 706.2637504698725, 657.6821800145914, 670.7253170213318, 578.277912824616, 597.5318918060956, 673.4920493909369, 637.8380953207969, 716.1186408745339, 642.5991476464676, 639.2250175294436, 657.0796162198145, 634.045067179298, 685.0493216815505, 654.6581433936307, 642.8661638599914, 645.8533951588937, 639.2843177154507, 649.0300805994108, 647.5952895648797, 626.877337343773, 645.4756899893632, 662.2125049570678, 646.6245485833141, 636.8768116960281, 643.412980647478, 659.4693777474422, 646.981956366138, 649.339293582673, 648.4119066091467, 238.0361180637846, 659.6240209035549, 709.261380356667, 698.4778231604093, 644.7907436381387, 651.0118151407479, 659.7566454670146, 704.4017907760237, 653.7644858580006, 550.0348240778317, 721.0297537131808, 721.1313935320876, 654.2601008980729, 661.3950972174935, 640.2709160894791, 643.3223787769681, 630.7965091868639, 636.8695752953894, 634.1547443659291, 628.4094246230251, 618.3372927059628, 730.446208169049, 713.4443248421908, 690.0676554843833, 704.9672601975795, 691.8567951975723, 687.7465678941944, 692.7258589333721, 697.098015809343, 680.8008241524997, 670.6065614960434, 680.9516286634275, 649.0349153190718, 694.2599220757932, 681.7481969868699, 684.4122314427434, 713.8978220781499, 710.1626437138597, 698.0286496968214, 702.0670577523216, 638.8635662579426, 659.0140359191112, 636.6336571562508, 641.7871180757908, 605.3163504346932, 647.4568666825159, 658.1211332640058, 653.3061403624089, 678.6856862987173, 663.832223768094, 659.8559336568208, 644.2266987508331, 662.6235685685126, 656.2667870128158, 646.6721421186403, 514.854548322971, 657.7733615242283, 655.1169937707036, 628.0530963474898, 146.5911722131064, 723.1428762357873, 726.9645543831934, 670.3033262667487, 651.1067253118152, 636.2582278382223, 649.2361132166731, 654.1355213319368, 659.1734426459126, 621.5647686813055, 630.204369124539, 645.3957998992184, 640.8143795382323, 648.7371638153338, 651.9797751209562, 641.4254666380842, 654.9000672376858]
Elapsed: 0.07091701848733793~0.025885613695922956
Time per graph: 0.001611750420166771~0.0005883094021800672
Speed: 646.795973461312~78.1622905903153
Total Time: 0.0678
best val loss: 0.3337004108484401 test_score: 0.8182

Testing...
Test loss: 0.5043 score: 0.8409 time: 0.06s
test Score 0.8409
Epoch Time List: [0.2903120949995355, 0.3980348620007135, 0.30279004399926635, 0.3042066350008099, 0.4868346109997219, 0.29824983599974075, 0.310318843999994, 0.43608431900065625, 0.36730222500045784, 0.29387197100004414, 0.29760491799970623, 0.29845263400056865, 0.5031798919999346, 0.2784005949997663, 0.2730238439999084, 0.42297254799996153, 0.29083148099925893, 0.30617148199962685, 0.3189013840010375, 0.3225448150005832, 0.5247720550005397, 0.39632135500050936, 0.29003623599965067, 0.3036314580003818, 0.2934266460006256, 0.3004526899994744, 0.3251651419996051, 0.5294976730001508, 0.2959876010008884, 0.47337431400046626, 0.3070974260017465, 0.30001463899952796, 0.2955653769995479, 0.3137485619990912, 0.3008091700003206, 0.45015781999973115, 0.29322713400051725, 0.3004105750005692, 0.29942723700060014, 0.29620807399987825, 0.29807307299870445, 0.30179881700041733, 0.2978595459990174, 0.46511933699912333, 0.2931461339994712, 0.2813477319996309, 0.27237540800069837, 0.28173125100056495, 0.2993500269994911, 0.2982135489992288, 0.2799033359997338, 0.2806151540007704, 0.4630859680000867, 0.2699940639995475, 0.2642221639998752, 0.2957152870003483, 0.29535603300064395, 0.2966215549995468, 0.301226211001449, 0.510928702999081, 0.3045476580000468, 0.3051510400009647, 0.3049188270006198, 0.307775944000241, 0.29399757699957263, 0.26335352800106193, 0.2729243060002773, 0.4717341060004401, 0.2754923000011331, 0.2800423040007445, 0.277669899999637, 0.27604659300050116, 0.2781622860002244, 0.28070408800067526, 0.2795754470007523, 0.2855379159991571, 0.47519074300089414, 0.291433101999246, 0.28083213200079626, 0.27418556599877775, 0.2707319250002911, 0.27410356100062927, 0.273878438999418, 0.2913270889994237, 0.4849043719996189, 0.2861967849994471, 0.2978630059997158, 0.3025898050009346, 0.295970991998729, 0.3019773300002271, 0.28894363600011275, 0.40762626200012164, 0.2868598839995684, 0.4137685590003457, 0.29408404500009055, 0.29099297399989155, 0.29434484600005817, 0.2935690860003888, 0.3019691360004799, 0.4139528639998389, 0.3066005810005663, 0.3101512819994241, 0.534819979000531, 0.2672785240001758, 0.2637353229993096, 0.3789664209998591, 0.29177950999928726, 0.29935186000057, 0.29535366799973417, 0.2922522980006761, 0.29214395899907686, 0.29855259399937495, 0.5081580810001469, 0.41097782500037283, 0.29650085600133025, 0.2988588100006382, 0.2971635590010919, 0.29952708600012556, 0.29568360700068297]
Total Epoch List: [119]
Total Time List: [0.06781762899936439]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f39b8b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.16s
Val loss: 0.6854 score: 0.5227 time: 0.06s
Test loss: 0.6896 score: 0.5349 time: 0.06s
Epoch 2/1000, LR 0.000015
Train loss: 0.7318;  Loss pred: 0.7318; Loss self: 0.0000; time: 0.16s
Val loss: 0.6841 score: 0.5227 time: 0.06s
Test loss: 0.6886 score: 0.5349 time: 0.06s
Epoch 3/1000, LR 0.000045
Train loss: 0.6768;  Loss pred: 0.6768; Loss self: 0.0000; time: 0.36s
Val loss: 0.6818 score: 0.5682 time: 0.07s
Test loss: 0.6867 score: 0.5581 time: 0.06s
Epoch 4/1000, LR 0.000075
Train loss: 0.7020;  Loss pred: 0.7020; Loss self: 0.0000; time: 0.29s
Val loss: 0.6789 score: 0.6136 time: 0.06s
Test loss: 0.6845 score: 0.6047 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.16s
Val loss: 0.6755 score: 0.6591 time: 0.06s
Test loss: 0.6818 score: 0.5814 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.17s
Val loss: 0.6720 score: 0.7273 time: 0.06s
Test loss: 0.6792 score: 0.5814 time: 0.06s
Epoch 7/1000, LR 0.000165
Train loss: 0.6360;  Loss pred: 0.6360; Loss self: 0.0000; time: 0.16s
Val loss: 0.6689 score: 0.7727 time: 0.06s
Test loss: 0.6772 score: 0.6512 time: 0.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6237;  Loss pred: 0.6237; Loss self: 0.0000; time: 0.17s
Val loss: 0.6664 score: 0.6364 time: 0.06s
Test loss: 0.6758 score: 0.5814 time: 0.06s
Epoch 9/1000, LR 0.000225
Train loss: 0.6358;  Loss pred: 0.6358; Loss self: 0.0000; time: 0.17s
Val loss: 0.6650 score: 0.5455 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6755 score: 0.4884 time: 0.06s
Epoch 10/1000, LR 0.000255
Train loss: 0.6119;  Loss pred: 0.6119; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6645 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6760 score: 0.4884 time: 0.06s
Epoch 11/1000, LR 0.000285
Train loss: 0.5934;  Loss pred: 0.5934; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6655 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6781 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6672 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6813 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5523;  Loss pred: 0.5523; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6700 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5404;  Loss pred: 0.5404; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6732 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5329;  Loss pred: 0.5329; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6747 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5333;  Loss pred: 0.5333; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6743 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5427;  Loss pred: 0.5427; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6736 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 18/1000, LR 0.000285
Train loss: 0.4979;  Loss pred: 0.4979; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6700 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 19/1000, LR 0.000285
Train loss: 0.4702;  Loss pred: 0.4702; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6637 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.06s
Epoch 20/1000, LR 0.000285
Train loss: 0.4929;  Loss pred: 0.4929; Loss self: 0.0000; time: 0.19s
Val loss: 0.6571 score: 0.5455 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.06s
Epoch 21/1000, LR 0.000285
Train loss: 0.4712;  Loss pred: 0.4712; Loss self: 0.0000; time: 0.19s
Val loss: 0.6490 score: 0.5682 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6846 score: 0.4884 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.4644;  Loss pred: 0.4644; Loss self: 0.0000; time: 0.16s
Val loss: 0.6406 score: 0.5909 time: 0.06s
Test loss: 0.6793 score: 0.5116 time: 0.06s
Epoch 23/1000, LR 0.000285
Train loss: 0.4486;  Loss pred: 0.4486; Loss self: 0.0000; time: 0.18s
Val loss: 0.6299 score: 0.5909 time: 0.07s
Test loss: 0.6713 score: 0.5349 time: 0.06s
Epoch 24/1000, LR 0.000285
Train loss: 0.4320;  Loss pred: 0.4320; Loss self: 0.0000; time: 0.16s
Val loss: 0.6191 score: 0.6364 time: 0.07s
Test loss: 0.6642 score: 0.5581 time: 0.17s
Epoch 25/1000, LR 0.000285
Train loss: 0.4168;  Loss pred: 0.4168; Loss self: 0.0000; time: 0.18s
Val loss: 0.6091 score: 0.6364 time: 0.06s
Test loss: 0.6577 score: 0.5581 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4548;  Loss pred: 0.4548; Loss self: 0.0000; time: 0.18s
Val loss: 0.6033 score: 0.6364 time: 0.06s
Test loss: 0.6551 score: 0.5814 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.3970;  Loss pred: 0.3970; Loss self: 0.0000; time: 0.18s
Val loss: 0.5967 score: 0.6591 time: 0.25s
Test loss: 0.6507 score: 0.5814 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.3901;  Loss pred: 0.3901; Loss self: 0.0000; time: 0.16s
Val loss: 0.5907 score: 0.6591 time: 0.06s
Test loss: 0.6467 score: 0.5814 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.3900;  Loss pred: 0.3900; Loss self: 0.0000; time: 0.17s
Val loss: 0.5863 score: 0.6818 time: 0.06s
Test loss: 0.6434 score: 0.5814 time: 0.06s
Epoch 30/1000, LR 0.000285
Train loss: 0.3655;  Loss pred: 0.3655; Loss self: 0.0000; time: 0.19s
Val loss: 0.5828 score: 0.6818 time: 0.07s
Test loss: 0.6409 score: 0.5814 time: 0.06s
Epoch 31/1000, LR 0.000285
Train loss: 0.3539;  Loss pred: 0.3539; Loss self: 0.0000; time: 0.19s
Val loss: 0.5792 score: 0.6818 time: 0.06s
Test loss: 0.6386 score: 0.5814 time: 0.06s
Epoch 32/1000, LR 0.000285
Train loss: 0.3668;  Loss pred: 0.3668; Loss self: 0.0000; time: 0.18s
Val loss: 0.5781 score: 0.7045 time: 0.19s
Test loss: 0.6392 score: 0.5814 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.3807;  Loss pred: 0.3807; Loss self: 0.0000; time: 0.17s
Val loss: 0.5767 score: 0.7045 time: 0.06s
Test loss: 0.6394 score: 0.5814 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.3412;  Loss pred: 0.3412; Loss self: 0.0000; time: 0.16s
Val loss: 0.5747 score: 0.7045 time: 0.06s
Test loss: 0.6386 score: 0.5814 time: 0.06s
Epoch 35/1000, LR 0.000285
Train loss: 0.3491;  Loss pred: 0.3491; Loss self: 0.0000; time: 0.19s
Val loss: 0.5719 score: 0.7045 time: 0.06s
Test loss: 0.6370 score: 0.5814 time: 0.28s
Epoch 36/1000, LR 0.000285
Train loss: 0.3393;  Loss pred: 0.3393; Loss self: 0.0000; time: 0.17s
Val loss: 0.5694 score: 0.7045 time: 0.07s
Test loss: 0.6361 score: 0.6047 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.3417;  Loss pred: 0.3417; Loss self: 0.0000; time: 0.17s
Val loss: 0.5655 score: 0.7045 time: 0.07s
Test loss: 0.6339 score: 0.6047 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3338;  Loss pred: 0.3338; Loss self: 0.0000; time: 0.30s
Val loss: 0.5630 score: 0.7045 time: 0.07s
Test loss: 0.6332 score: 0.6047 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.3503;  Loss pred: 0.3503; Loss self: 0.0000; time: 0.16s
Val loss: 0.5601 score: 0.7045 time: 0.06s
Test loss: 0.6322 score: 0.6279 time: 0.06s
Epoch 40/1000, LR 0.000284
Train loss: 0.3105;  Loss pred: 0.3105; Loss self: 0.0000; time: 0.17s
Val loss: 0.5582 score: 0.7045 time: 0.06s
Test loss: 0.6323 score: 0.6279 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.2961;  Loss pred: 0.2961; Loss self: 0.0000; time: 0.17s
Val loss: 0.5557 score: 0.7045 time: 0.06s
Test loss: 0.6314 score: 0.6279 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3055;  Loss pred: 0.3055; Loss self: 0.0000; time: 0.17s
Val loss: 0.5553 score: 0.7045 time: 0.06s
Test loss: 0.6333 score: 0.6279 time: 0.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.2962;  Loss pred: 0.2962; Loss self: 0.0000; time: 0.34s
Val loss: 0.5524 score: 0.7045 time: 0.06s
Test loss: 0.6319 score: 0.6279 time: 0.06s
Epoch 44/1000, LR 0.000284
Train loss: 0.2869;  Loss pred: 0.2869; Loss self: 0.0000; time: 0.18s
Val loss: 0.5506 score: 0.7045 time: 0.07s
Test loss: 0.6317 score: 0.6279 time: 0.06s
Epoch 45/1000, LR 0.000284
Train loss: 0.2981;  Loss pred: 0.2981; Loss self: 0.0000; time: 0.18s
Val loss: 0.5499 score: 0.7045 time: 0.06s
Test loss: 0.6327 score: 0.6512 time: 0.06s
Epoch 46/1000, LR 0.000284
Train loss: 0.2701;  Loss pred: 0.2701; Loss self: 0.0000; time: 0.30s
Val loss: 0.5473 score: 0.7045 time: 0.07s
Test loss: 0.6317 score: 0.6512 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3065;  Loss pred: 0.3065; Loss self: 0.0000; time: 0.17s
Val loss: 0.5435 score: 0.7045 time: 0.07s
Test loss: 0.6286 score: 0.6512 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.2918;  Loss pred: 0.2918; Loss self: 0.0000; time: 0.17s
Val loss: 0.5424 score: 0.7045 time: 0.07s
Test loss: 0.6283 score: 0.6512 time: 0.06s
Epoch 49/1000, LR 0.000284
Train loss: 0.2669;  Loss pred: 0.2669; Loss self: 0.0000; time: 0.18s
Val loss: 0.5409 score: 0.7045 time: 0.07s
Test loss: 0.6274 score: 0.6512 time: 0.06s
Epoch 50/1000, LR 0.000284
Train loss: 0.2600;  Loss pred: 0.2600; Loss self: 0.0000; time: 0.18s
Val loss: 0.5402 score: 0.7045 time: 0.07s
Test loss: 0.6273 score: 0.6512 time: 0.06s
Epoch 51/1000, LR 0.000284
Train loss: 0.2627;  Loss pred: 0.2627; Loss self: 0.0000; time: 0.18s
Val loss: 0.5405 score: 0.7273 time: 0.07s
Test loss: 0.6280 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 52/1000, LR 0.000284
Train loss: 0.2494;  Loss pred: 0.2494; Loss self: 0.0000; time: 0.29s
Val loss: 0.5401 score: 0.7273 time: 0.19s
Test loss: 0.6278 score: 0.6512 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.2620;  Loss pred: 0.2620; Loss self: 0.0000; time: 0.18s
Val loss: 0.5425 score: 0.7273 time: 0.06s
Test loss: 0.6309 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 54/1000, LR 0.000284
Train loss: 0.2446;  Loss pred: 0.2446; Loss self: 0.0000; time: 0.17s
Val loss: 0.5445 score: 0.7273 time: 0.06s
Test loss: 0.6333 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 55/1000, LR 0.000284
Train loss: 0.2444;  Loss pred: 0.2444; Loss self: 0.0000; time: 0.17s
Val loss: 0.5414 score: 0.7273 time: 0.06s
Test loss: 0.6296 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 56/1000, LR 0.000284
Train loss: 0.2450;  Loss pred: 0.2450; Loss self: 0.0000; time: 0.17s
Val loss: 0.5444 score: 0.7273 time: 0.06s
Test loss: 0.6336 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 57/1000, LR 0.000283
Train loss: 0.2308;  Loss pred: 0.2308; Loss self: 0.0000; time: 0.17s
Val loss: 0.5458 score: 0.7500 time: 0.06s
Test loss: 0.6355 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 58/1000, LR 0.000283
Train loss: 0.2342;  Loss pred: 0.2342; Loss self: 0.0000; time: 0.16s
Val loss: 0.5431 score: 0.7500 time: 0.06s
Test loss: 0.6327 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 59/1000, LR 0.000283
Train loss: 0.2420;  Loss pred: 0.2420; Loss self: 0.0000; time: 0.17s
Val loss: 0.5452 score: 0.7500 time: 0.06s
Test loss: 0.6350 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 60/1000, LR 0.000283
Train loss: 0.2268;  Loss pred: 0.2268; Loss self: 0.0000; time: 0.17s
Val loss: 0.5471 score: 0.7500 time: 0.12s
Test loss: 0.6372 score: 0.6512 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 61/1000, LR 0.000283
Train loss: 0.2266;  Loss pred: 0.2266; Loss self: 0.0000; time: 0.31s
Val loss: 0.5459 score: 0.7500 time: 0.07s
Test loss: 0.6351 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 62/1000, LR 0.000283
Train loss: 0.2420;  Loss pred: 0.2420; Loss self: 0.0000; time: 0.18s
Val loss: 0.5467 score: 0.7500 time: 0.06s
Test loss: 0.6351 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 63/1000, LR 0.000283
Train loss: 0.2253;  Loss pred: 0.2253; Loss self: 0.0000; time: 0.19s
Val loss: 0.5478 score: 0.7500 time: 0.07s
Test loss: 0.6357 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 64/1000, LR 0.000283
Train loss: 0.2046;  Loss pred: 0.2046; Loss self: 0.0000; time: 0.18s
Val loss: 0.5463 score: 0.7500 time: 0.07s
Test loss: 0.6337 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 65/1000, LR 0.000283
Train loss: 0.2297;  Loss pred: 0.2297; Loss self: 0.0000; time: 0.19s
Val loss: 0.5453 score: 0.7500 time: 0.07s
Test loss: 0.6325 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 13 of 20
Epoch 66/1000, LR 0.000283
Train loss: 0.2285;  Loss pred: 0.2285; Loss self: 0.0000; time: 0.19s
Val loss: 0.5458 score: 0.7500 time: 0.06s
Test loss: 0.6333 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 67/1000, LR 0.000283
Train loss: 0.2175;  Loss pred: 0.2175; Loss self: 0.0000; time: 0.19s
Val loss: 0.5490 score: 0.7500 time: 0.25s
Test loss: 0.6367 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 68/1000, LR 0.000283
Train loss: 0.2124;  Loss pred: 0.2124; Loss self: 0.0000; time: 0.18s
Val loss: 0.5509 score: 0.7500 time: 0.07s
Test loss: 0.6385 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 69/1000, LR 0.000283
Train loss: 0.2170;  Loss pred: 0.2170; Loss self: 0.0000; time: 0.18s
Val loss: 0.5527 score: 0.7500 time: 0.07s
Test loss: 0.6402 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 70/1000, LR 0.000283
Train loss: 0.2002;  Loss pred: 0.2002; Loss self: 0.0000; time: 0.17s
Val loss: 0.5523 score: 0.7500 time: 0.07s
Test loss: 0.6397 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 71/1000, LR 0.000282
Train loss: 0.2065;  Loss pred: 0.2065; Loss self: 0.0000; time: 0.17s
Val loss: 0.5513 score: 0.7500 time: 0.06s
Test loss: 0.6386 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 72/1000, LR 0.000282
Train loss: 0.1983;  Loss pred: 0.1983; Loss self: 0.0000; time: 0.18s
Val loss: 0.5515 score: 0.7500 time: 0.07s
Test loss: 0.6390 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 051,   Train_Loss: 0.2494,   Val_Loss: 0.5401,   Val_Precision: 1.0000,   Val_Recall: 0.4545,   Val_accuracy: 0.6250,   Val_Score: 0.7273,   Val_Loss: 0.5401,   Test_Precision: 1.0000,   Test_Recall: 0.3182,   Test_accuracy: 0.4828,   Test_Score: 0.6512,   Test_loss: 0.6278


[0.06446969199987507, 0.06739953799933573, 0.06648357499943813, 0.06758560599973862, 0.06738812200001121, 0.06710440599999856, 0.08056942100029119, 0.17509985200013034, 0.07131669000045804, 0.06412443899989739, 0.06837282000014966, 0.06592467199970997, 0.06553784399966389, 0.06337089499993454, 0.062299672000335704, 0.06690161500046088, 0.06560062500011554, 0.07608798299952468, 0.073636236999846, 0.06533113499972387, 0.0689830230003281, 0.061442332999831706, 0.06847192399982305, 0.06883335100064869, 0.06696296599966445, 0.06939569799942547, 0.06422895200012135, 0.06721065100009582, 0.0684434839995447, 0.06812691599952814, 0.06882696600041527, 0.06779346799976338, 0.06794366899976012, 0.07018917000004876, 0.06816678099949058, 0.06644392799989873, 0.06804566899972997, 0.06908714400015015, 0.06838531599987618, 0.06672030800018547, 0.0680080789998101, 0.06776118499965378, 0.0678581000001941, 0.18484589800027607, 0.06670466600007785, 0.062036368000008224, 0.06299412599946663, 0.0682391930004087, 0.0675870989998657, 0.06669125699954748, 0.06246435000048223, 0.06730252400029713, 0.0799949350002862, 0.06102383400047984, 0.06101523300003464, 0.06725154099967767, 0.06652604499959125, 0.0687209099996835, 0.06839494699943316, 0.06975308100027178, 0.06908792899957916, 0.06938369599993166, 0.07001804599985917, 0.07115857400003733, 0.060237153000343824, 0.06167264699979569, 0.06376186400029837, 0.06241424600011669, 0.0635969760005537, 0.06397705499966833, 0.06351719000031153, 0.06311881400051789, 0.06462976900002104, 0.06561224200049764, 0.06461545599995588, 0.06779296300010174, 0.06337684000027366, 0.06453995799984114, 0.06428873999993812, 0.061633469999833324, 0.06195763799951237, 0.06303466200006369, 0.06267207599921676, 0.06887229499989189, 0.06676640800014866, 0.06911353100076667, 0.06855855899993912, 0.07268926399956399, 0.06795819499984646, 0.06685699300032866, 0.06734974199935095, 0.06483118899996043, 0.06628180799998518, 0.06668122199971549, 0.06829893899976014, 0.06640270899970346, 0.06704590399931476, 0.06804066099994088, 0.08546103000026051, 0.06689234100031172, 0.06716357599998446, 0.07005777100039268, 0.300154499999735, 0.06084551399999327, 0.060525646999849414, 0.06564192400037427, 0.06757724700037215, 0.0691543119992275, 0.06777195399990887, 0.06726434899974265, 0.06675026199991407, 0.07078908299990871, 0.06981862099928549, 0.06817521900029533, 0.06866262899984577, 0.06782407799983048, 0.06748675599919807, 0.06859721399996488, 0.0671858229998179, 0.06536242299989681, 0.0629014019996248, 0.060527473000547616, 0.06280239799980336, 0.061916339000163134, 0.0629511300003287, 0.06415727800049353, 0.06484735199956049, 0.0613989120001861, 0.06261458200060588, 0.06823806500051433, 0.0639425539993681, 0.06238242400013405, 0.061805293999896094, 0.06407321200003935, 0.06392565399983141, 0.06319622099999833, 0.06979081499957829, 0.0685262689994488, 0.07066979499995796, 0.06477853499927733, 0.06504720900011307, 0.06442563899963716, 0.17092731600041589, 0.0633834129994284, 0.06261844399978145, 0.0622781580004812, 0.0667546479999146, 0.06820491399957973, 0.06772521499988216, 0.06414695100011159, 0.06975227800012362, 0.06206525699963095, 0.06657000700033677, 0.2855716570002187, 0.06550212999991345, 0.06532986599995638, 0.07029703599982895, 0.06448903100044845, 0.0628892149998137, 0.06423149800048122, 0.06254450800042832, 0.0618987890002245, 0.06766208599947277, 0.06787331900068239, 0.06667413600007421, 0.06632038000043394, 0.0670805809995727, 0.0667233820004185, 0.06915260799996759, 0.06899299799988512, 0.06685222800024349, 0.06713050100006512, 0.06602022999959445, 0.06370894199972099, 0.06321600699993724, 0.0628563310001482, 0.06269178900038241, 0.06282329199984815, 0.12756734300000971, 0.0670878430000812, 0.06929198999932851, 0.07065578999936406, 0.07127165700057958, 0.0717947950006419, 0.07529028999942966, 0.06767957499960175, 0.06715813000027993, 0.06653007599925331, 0.06677520900029776, 0.06752103300004819, 0.06791726099982043]
[0.0014652202727244332, 0.001531807681803085, 0.0015109903408963212, 0.0015360364999940596, 0.001531548227272982, 0.0015251001363636037, 0.0018311232045520724, 0.003979542090912053, 0.0016208338636467736, 0.0014573736136340317, 0.0015539277272761285, 0.0014982879999934084, 0.0014894964545378157, 0.0014402476136348757, 0.001415901636371266, 0.0015204912500104745, 0.0014909232954571714, 0.0017292723408982883, 0.001673550840905591, 0.0014847985227209972, 0.0015677959772801842, 0.0013964166590870843, 0.0015561800909050692, 0.0015643943409238339, 0.0015218855909014646, 0.001577174954532397, 0.0014597489090936672, 0.0015275147954567233, 0.0015555337272623794, 0.0015483389999892759, 0.0015642492272821653, 0.0015407606363582586, 0.0015441742954490937, 0.0015952084090920173, 0.001549245022715695, 0.0015100892727249711, 0.0015464924772665902, 0.001570162363639776, 0.0015542117272699133, 0.0015163706363678516, 0.001545638159086593, 0.001540026931810313, 0.0015422295454589569, 0.004201043136369911, 0.0015160151363654056, 0.0014099174545456415, 0.0014316846818060599, 0.0015508907500092887, 0.0015360704318151295, 0.0015157103863533518, 0.001419644318192778, 0.001529602818188571, 0.0018180667045519592, 0.0013869053181927236, 0.0013867098409098783, 0.001528444113629038, 0.0015119555681725285, 0.0015618388636291704, 0.001554430613623481, 0.0015852972954607221, 0.00157018020453589, 0.0015769021818166286, 0.0015913192272695267, 0.0016172403181826667, 0.0013690262045532688, 0.0014016510681771747, 0.001449133272734054, 0.001418505590911743, 0.0014453858181944022, 0.0014540239772651892, 0.0014435725000070802, 0.0014345185000117701, 0.0014688583863641145, 0.0014911873181931283, 0.0014685330909080883, 0.0015407491590932213, 0.0014403827272789467, 0.0014668172272691168, 0.0014611077272713208, 0.0014007606818143938, 0.0014081281363525538, 0.001432605954546902, 0.0014243653636185627, 0.0015652794318157248, 0.0015174183636397422, 0.0015707620681992425, 0.0015581490681804346, 0.001652028727262818, 0.0015445044318146922, 0.001519477113643833, 0.0015306759545307034, 0.0014734361136354644, 0.0015064047272723903, 0.001515482318175352, 0.0015522486136309124, 0.0015091524772659877, 0.0015237705454389718, 0.0015463786590895654, 0.001942296136369557, 0.0015202804772798117, 0.001526444909090556, 0.0015922220681907427, 0.006821693181812158, 0.001382852590908938, 0.001375582886360214, 0.0014918619090994152, 0.0015358465227357308, 0.0015716889090733523, 0.0015402716818161107, 0.0015287352045396058, 0.0015170514090889562, 0.0016088427954524707, 0.0015867868408928518, 0.0015494367954612574, 0.0015605142954510402, 0.0015414563181779654, 0.0015337899090726833, 0.0015590275909082927, 0.001526950522723134, 0.0015200563488348094, 0.0014628233023168556, 0.001407615651175526, 0.0014605208837163571, 0.0014399148604689101, 0.0014639797674495048, 0.0014920297209417102, 0.0015080779534781508, 0.0014278816744229326, 0.001456153069781532, 0.0015869317441980077, 0.0014870361395201883, 0.0014507540465147455, 0.0014373324186022348, 0.0014900746976753336, 0.0014866431162751492, 0.001469679558139496, 0.0016230422092925183, 0.001593634162777879, 0.001643483604650185, 0.0015064775581227286, 0.001512725790700304, 0.0014982706744101665, 0.003975053860474788, 0.0014740328604518232, 0.0014562428837158476, 0.0014483292558251442, 0.0015524336744166185, 0.0015861607906879007, 0.0015750049999972593, 0.0014917895581421298, 0.001622146000002875, 0.0014433780697588592, 0.0015481396976822505, 0.006641201325586482, 0.0015233053488351966, 0.0015192992093013113, 0.0016348147906936966, 0.0014997449069871732, 0.0014625398837165978, 0.0014937557674530517, 0.0014545234418704262, 0.0014395067209354534, 0.001573536883708669, 0.001578449279085637, 0.0015505613023273073, 0.001542334418614743, 0.0015600135116179698, 0.0015517065581492674, 0.0016082001860457577, 0.0016044883255787238, 0.0015547029767498486, 0.0015611744418619795, 0.0015353541860370801, 0.001481603302319093, 0.001470139697672959, 0.0014617751395383303, 0.0014579485814042422, 0.0014610067906941431, 0.002966682395349063, 0.0015601823953507255, 0.0016114416278913606, 0.0016431579069619549, 0.0016574803953623158, 0.001669646395363765, 0.0017509369767309223, 0.0015739436046419012, 0.001561816976750696, 0.001547211069750077, 0.001552911837216227, 0.0015702565813964695, 0.0015794711860423354]
[682.491239450706, 652.823465947699, 661.8175993148963, 651.0261963201183, 652.9340586163343, 655.6946499161462, 546.1128980912123, 251.28519240534393, 616.9663791143056, 686.1658470036737, 643.5305725272658, 667.4284249786418, 671.3678283378631, 694.3250525346921, 706.2637504698725, 657.6821800145914, 670.7253170213318, 578.277912824616, 597.5318918060956, 673.4920493909369, 637.8380953207969, 716.1186408745339, 642.5991476464676, 639.2250175294436, 657.0796162198145, 634.045067179298, 685.0493216815505, 654.6581433936307, 642.8661638599914, 645.8533951588937, 639.2843177154507, 649.0300805994108, 647.5952895648797, 626.877337343773, 645.4756899893632, 662.2125049570678, 646.6245485833141, 636.8768116960281, 643.412980647478, 659.4693777474422, 646.981956366138, 649.339293582673, 648.4119066091467, 238.0361180637846, 659.6240209035549, 709.261380356667, 698.4778231604093, 644.7907436381387, 651.0118151407479, 659.7566454670146, 704.4017907760237, 653.7644858580006, 550.0348240778317, 721.0297537131808, 721.1313935320876, 654.2601008980729, 661.3950972174935, 640.2709160894791, 643.3223787769681, 630.7965091868639, 636.8695752953894, 634.1547443659291, 628.4094246230251, 618.3372927059628, 730.446208169049, 713.4443248421908, 690.0676554843833, 704.9672601975795, 691.8567951975723, 687.7465678941944, 692.7258589333721, 697.098015809343, 680.8008241524997, 670.6065614960434, 680.9516286634275, 649.0349153190718, 694.2599220757932, 681.7481969868699, 684.4122314427434, 713.8978220781499, 710.1626437138597, 698.0286496968214, 702.0670577523216, 638.8635662579426, 659.0140359191112, 636.6336571562508, 641.7871180757908, 605.3163504346932, 647.4568666825159, 658.1211332640058, 653.3061403624089, 678.6856862987173, 663.832223768094, 659.8559336568208, 644.2266987508331, 662.6235685685126, 656.2667870128158, 646.6721421186403, 514.854548322971, 657.7733615242283, 655.1169937707036, 628.0530963474898, 146.5911722131064, 723.1428762357873, 726.9645543831934, 670.3033262667487, 651.1067253118152, 636.2582278382223, 649.2361132166731, 654.1355213319368, 659.1734426459126, 621.5647686813055, 630.204369124539, 645.3957998992184, 640.8143795382323, 648.7371638153338, 651.9797751209562, 641.4254666380842, 654.9000672376858, 657.8703485344765, 683.6095640643509, 710.4211999666823, 684.6872312126463, 694.4855056738207, 683.0695493436809, 670.2279357872574, 663.0956958780898, 700.3381427975412, 686.7409894964071, 630.146824938191, 672.4786126063237, 689.296716009426, 695.7332813605302, 671.1072951980867, 672.656395507716, 680.4204321014881, 616.1269215764256, 627.4965882112431, 608.4636300420226, 663.8001307142823, 661.05833995007, 667.4361429343708, 251.56891833424316, 678.4109274833114, 686.6986346730379, 690.4507355478908, 644.1499024915091, 630.4531078254121, 634.9186193070752, 670.3358356023064, 616.4673216826523, 692.8191725727598, 645.9365401566274, 150.5751671986379, 656.4672019071261, 658.1981968251505, 611.6900860529116, 666.7800606230381, 683.7420374881032, 669.4534821479307, 687.5104045858915, 694.6824113125064, 635.5110009516269, 633.5331855447895, 644.9277422950354, 648.3678169473493, 641.020088962466, 644.4517455624522, 621.8131353776297, 623.2516522918978, 643.2096773176114, 640.5434096188003, 651.3155134458657, 674.944499944581, 680.2074670746397, 684.0997448594098, 685.895245384331, 684.4595154310591, 337.0768645702429, 640.9507010077513, 620.5623478329414, 608.5842363433629, 603.3253864106222, 598.9292120635702, 571.1227835664564, 635.346779294241, 640.2798886720159, 646.3242278647419, 643.9515599241067, 636.8385981294052, 633.1232939460514]
Elapsed: 0.07098903826174033~0.02721757203650173
Time per graph: 0.0016275548530369608~0.0006252270452563213
Speed: 643.6375099489761~82.09063067466187
Total Time: 0.0687
best val loss: 0.5400783961469476 test_score: 0.6512

Testing...
Test loss: 0.6772 score: 0.6512 time: 0.06s
test Score 0.6512
Epoch Time List: [0.2903120949995355, 0.3980348620007135, 0.30279004399926635, 0.3042066350008099, 0.4868346109997219, 0.29824983599974075, 0.310318843999994, 0.43608431900065625, 0.36730222500045784, 0.29387197100004414, 0.29760491799970623, 0.29845263400056865, 0.5031798919999346, 0.2784005949997663, 0.2730238439999084, 0.42297254799996153, 0.29083148099925893, 0.30617148199962685, 0.3189013840010375, 0.3225448150005832, 0.5247720550005397, 0.39632135500050936, 0.29003623599965067, 0.3036314580003818, 0.2934266460006256, 0.3004526899994744, 0.3251651419996051, 0.5294976730001508, 0.2959876010008884, 0.47337431400046626, 0.3070974260017465, 0.30001463899952796, 0.2955653769995479, 0.3137485619990912, 0.3008091700003206, 0.45015781999973115, 0.29322713400051725, 0.3004105750005692, 0.29942723700060014, 0.29620807399987825, 0.29807307299870445, 0.30179881700041733, 0.2978595459990174, 0.46511933699912333, 0.2931461339994712, 0.2813477319996309, 0.27237540800069837, 0.28173125100056495, 0.2993500269994911, 0.2982135489992288, 0.2799033359997338, 0.2806151540007704, 0.4630859680000867, 0.2699940639995475, 0.2642221639998752, 0.2957152870003483, 0.29535603300064395, 0.2966215549995468, 0.301226211001449, 0.510928702999081, 0.3045476580000468, 0.3051510400009647, 0.3049188270006198, 0.307775944000241, 0.29399757699957263, 0.26335352800106193, 0.2729243060002773, 0.4717341060004401, 0.2754923000011331, 0.2800423040007445, 0.277669899999637, 0.27604659300050116, 0.2781622860002244, 0.28070408800067526, 0.2795754470007523, 0.2855379159991571, 0.47519074300089414, 0.291433101999246, 0.28083213200079626, 0.27418556599877775, 0.2707319250002911, 0.27410356100062927, 0.273878438999418, 0.2913270889994237, 0.4849043719996189, 0.2861967849994471, 0.2978630059997158, 0.3025898050009346, 0.295970991998729, 0.3019773300002271, 0.28894363600011275, 0.40762626200012164, 0.2868598839995684, 0.4137685590003457, 0.29408404500009055, 0.29099297399989155, 0.29434484600005817, 0.2935690860003888, 0.3019691360004799, 0.4139528639998389, 0.3066005810005663, 0.3101512819994241, 0.534819979000531, 0.2672785240001758, 0.2637353229993096, 0.3789664209998591, 0.29177950999928726, 0.29935186000057, 0.29535366799973417, 0.2922522980006761, 0.29214395899907686, 0.29855259399937495, 0.5081580810001469, 0.41097782500037283, 0.29650085600133025, 0.2988588100006382, 0.2971635590010919, 0.29952708600012556, 0.29568360700068297, 0.2759674620001533, 0.28395227799956047, 0.47846048199971847, 0.4067353930004174, 0.2804292630007694, 0.2844048049992125, 0.2832687780010019, 0.288318846999573, 0.2849268590007341, 0.27939025100113213, 0.633094624999103, 0.29216120400087675, 0.2765926420006508, 0.2801642720005475, 0.28328552299990406, 0.2848107589998108, 0.28273712700047327, 0.4266155360000994, 0.5167160040000454, 0.3136212669996894, 0.3160634799996842, 0.2868938490000801, 0.3142810979998103, 0.39580781899985595, 0.29710261799937143, 0.29866252899955725, 0.4889817849989413, 0.2872428859991487, 0.29603562700140174, 0.3151126539996767, 0.30645502200150077, 0.4331352910003261, 0.29013606300031825, 0.2861828470004184, 0.5272148270005346, 0.29946655099865893, 0.2984198860003744, 0.4361406279995208, 0.28603941399978794, 0.28573714300091524, 0.28585426499921596, 0.28642492099879746, 0.45586502400055906, 0.30937441200057947, 0.30312168299860787, 0.4261700300003213, 0.30184755599930213, 0.3017575769999894, 0.3050954649997948, 0.31234976300129347, 0.30837125400012155, 0.5397257930007981, 0.301038160000644, 0.292466949998925, 0.2876617180008907, 0.28527020499950595, 0.28302319400063425, 0.27891543799978535, 0.28602954000052705, 0.4104514549990199, 0.4367798280009083, 0.30667892600013147, 0.32031298599940783, 0.3196448930002589, 0.327583541000422, 0.3232178680009383, 0.5031732610004838, 0.304730226999709, 0.30203237799923954, 0.30153256300036446, 0.2941003889991407, 0.304556536999371]
Total Epoch List: [119, 72]
Total Time List: [0.06781762899936439, 0.06866784999965603]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: SGFormer
SGFormer(
  (trans_conv): SGModule(
    (attns): ModuleList(
      (0-1): 2 x SGFormerAttention(heads=1, head_channels=64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (graph_conv): GraphModule(
    (convs): ModuleList(
      (0-2): 3 x GCNConv(64, 64)
    )
    (fcs): ModuleList(
      (0): Linear(in_features=14887, out_features=64, bias=True)
    )
    (bns): ModuleList(
      (0-3): 4 x BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (classifier): Linear(in_features=64, out_features=2, bias=True)
)
Total number of parameters: 1943746
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x743a0f399090>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7061;  Loss pred: 0.7061; Loss self: 0.0000; time: 0.17s
Val loss: 0.6980 score: 0.4773 time: 0.06s
Test loss: 0.6920 score: 0.5116 time: 0.05s
Epoch 2/1000, LR 0.000015
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.17s
Val loss: 0.6972 score: 0.5227 time: 0.06s
Test loss: 0.6912 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000045
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.17s
Val loss: 0.6956 score: 0.5455 time: 0.06s
Test loss: 0.6893 score: 0.5349 time: 0.05s
Epoch 4/1000, LR 0.000075
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.31s
Val loss: 0.6935 score: 0.5455 time: 0.07s
Test loss: 0.6867 score: 0.5581 time: 0.06s
Epoch 5/1000, LR 0.000105
Train loss: 0.6584;  Loss pred: 0.6584; Loss self: 0.0000; time: 0.18s
Val loss: 0.6910 score: 0.5227 time: 0.06s
Test loss: 0.6836 score: 0.6047 time: 0.06s
Epoch 6/1000, LR 0.000135
Train loss: 0.6545;  Loss pred: 0.6545; Loss self: 0.0000; time: 0.18s
Val loss: 0.6890 score: 0.5455 time: 0.07s
Test loss: 0.6807 score: 0.5581 time: 0.28s
Epoch 7/1000, LR 0.000165
Train loss: 0.6377;  Loss pred: 0.6377; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5000 time: 0.07s
Test loss: 0.6783 score: 0.5349 time: 0.06s
Epoch 8/1000, LR 0.000195
Train loss: 0.6330;  Loss pred: 0.6330; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6857 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6766 score: 0.5116 time: 0.06s
Epoch 9/1000, LR 0.000225
Train loss: 0.6287;  Loss pred: 0.6287; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6762 score: 0.5116 time: 0.06s
Epoch 10/1000, LR 0.000255
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6857 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6758 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 11/1000, LR 0.000285
Train loss: 0.5854;  Loss pred: 0.5854; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6869 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 12/1000, LR 0.000285
Train loss: 0.5828;  Loss pred: 0.5828; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 13/1000, LR 0.000285
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6756 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 14/1000, LR 0.000285
Train loss: 0.5760;  Loss pred: 0.5760; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6745 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 15/1000, LR 0.000285
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6722 score: 0.5116 time: 0.27s
     INFO: Early stopping counter 6 of 20
Epoch 16/1000, LR 0.000285
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6685 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 17/1000, LR 0.000285
Train loss: 0.5625;  Loss pred: 0.5625; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6836 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6638 score: 0.5116 time: 0.06s
Epoch 18/1000, LR 0.000285
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6786 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6579 score: 0.5116 time: 0.06s
Epoch 19/1000, LR 0.000285
Train loss: 0.5261;  Loss pred: 0.5261; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6725 score: 0.5000 time: 0.07s
Test loss: 0.6507 score: 0.5349 time: 0.06s
Epoch 20/1000, LR 0.000285
Train loss: 0.5125;  Loss pred: 0.5125; Loss self: 0.0000; time: 0.20s
Val loss: 0.6660 score: 0.5227 time: 0.06s
Test loss: 0.6432 score: 0.5581 time: 0.06s
Epoch 21/1000, LR 0.000285
Train loss: 0.4804;  Loss pred: 0.4804; Loss self: 0.0000; time: 0.19s
Val loss: 0.6587 score: 0.5455 time: 0.07s
Test loss: 0.6350 score: 0.5581 time: 0.06s
Epoch 22/1000, LR 0.000285
Train loss: 0.4997;  Loss pred: 0.4997; Loss self: 0.0000; time: 0.19s
Val loss: 0.6499 score: 0.5455 time: 0.06s
Test loss: 0.6257 score: 0.5581 time: 0.05s
Epoch 23/1000, LR 0.000285
Train loss: 0.4764;  Loss pred: 0.4764; Loss self: 0.0000; time: 0.18s
Val loss: 0.6414 score: 0.5682 time: 0.05s
Test loss: 0.6173 score: 0.5814 time: 0.05s
Epoch 24/1000, LR 0.000285
Train loss: 0.4710;  Loss pred: 0.4710; Loss self: 0.0000; time: 0.18s
Val loss: 0.6323 score: 0.6136 time: 0.26s
Test loss: 0.6084 score: 0.6047 time: 0.06s
Epoch 25/1000, LR 0.000285
Train loss: 0.4843;  Loss pred: 0.4843; Loss self: 0.0000; time: 0.18s
Val loss: 0.6239 score: 0.6364 time: 0.07s
Test loss: 0.6003 score: 0.6977 time: 0.06s
Epoch 26/1000, LR 0.000285
Train loss: 0.4796;  Loss pred: 0.4796; Loss self: 0.0000; time: 0.19s
Val loss: 0.6150 score: 0.6591 time: 0.07s
Test loss: 0.5924 score: 0.6977 time: 0.06s
Epoch 27/1000, LR 0.000285
Train loss: 0.4691;  Loss pred: 0.4691; Loss self: 0.0000; time: 0.19s
Val loss: 0.6076 score: 0.6591 time: 0.07s
Test loss: 0.5860 score: 0.7209 time: 0.06s
Epoch 28/1000, LR 0.000285
Train loss: 0.4359;  Loss pred: 0.4359; Loss self: 0.0000; time: 0.20s
Val loss: 0.5999 score: 0.6591 time: 0.06s
Test loss: 0.5793 score: 0.7209 time: 0.06s
Epoch 29/1000, LR 0.000285
Train loss: 0.4493;  Loss pred: 0.4493; Loss self: 0.0000; time: 0.19s
Val loss: 0.5924 score: 0.6818 time: 0.06s
Test loss: 0.5733 score: 0.7209 time: 0.06s
Epoch 30/1000, LR 0.000285
Train loss: 0.4198;  Loss pred: 0.4198; Loss self: 0.0000; time: 0.18s
Val loss: 0.5847 score: 0.6818 time: 0.08s
Test loss: 0.5670 score: 0.7442 time: 0.07s
Epoch 31/1000, LR 0.000285
Train loss: 0.4268;  Loss pred: 0.4268; Loss self: 0.0000; time: 0.22s
Val loss: 0.5782 score: 0.6818 time: 0.21s
Test loss: 0.5611 score: 0.7442 time: 0.12s
Epoch 32/1000, LR 0.000285
Train loss: 0.4200;  Loss pred: 0.4200; Loss self: 0.0000; time: 0.18s
Val loss: 0.5719 score: 0.6818 time: 0.07s
Test loss: 0.5551 score: 0.7442 time: 0.06s
Epoch 33/1000, LR 0.000285
Train loss: 0.4133;  Loss pred: 0.4133; Loss self: 0.0000; time: 0.19s
Val loss: 0.5667 score: 0.6818 time: 0.07s
Test loss: 0.5497 score: 0.7442 time: 0.06s
Epoch 34/1000, LR 0.000285
Train loss: 0.3949;  Loss pred: 0.3949; Loss self: 0.0000; time: 0.19s
Val loss: 0.5621 score: 0.7045 time: 0.07s
Test loss: 0.5452 score: 0.7442 time: 0.06s
Epoch 35/1000, LR 0.000285
Train loss: 0.3993;  Loss pred: 0.3993; Loss self: 0.0000; time: 0.19s
Val loss: 0.5580 score: 0.7045 time: 0.07s
Test loss: 0.5406 score: 0.7442 time: 0.06s
Epoch 36/1000, LR 0.000285
Train loss: 0.4041;  Loss pred: 0.4041; Loss self: 0.0000; time: 0.19s
Val loss: 0.5546 score: 0.7045 time: 0.07s
Test loss: 0.5365 score: 0.7674 time: 0.06s
Epoch 37/1000, LR 0.000285
Train loss: 0.3857;  Loss pred: 0.3857; Loss self: 0.0000; time: 0.19s
Val loss: 0.5501 score: 0.7045 time: 0.07s
Test loss: 0.5312 score: 0.7907 time: 0.06s
Epoch 38/1000, LR 0.000284
Train loss: 0.3969;  Loss pred: 0.3969; Loss self: 0.0000; time: 0.19s
Val loss: 0.5453 score: 0.7045 time: 0.07s
Test loss: 0.5261 score: 0.7907 time: 0.06s
Epoch 39/1000, LR 0.000284
Train loss: 0.3729;  Loss pred: 0.3729; Loss self: 0.0000; time: 0.22s
Val loss: 0.5420 score: 0.6818 time: 0.08s
Test loss: 0.5222 score: 0.7907 time: 0.07s
Epoch 40/1000, LR 0.000284
Train loss: 0.3720;  Loss pred: 0.3720; Loss self: 0.0000; time: 0.36s
Val loss: 0.5365 score: 0.6818 time: 0.07s
Test loss: 0.5170 score: 0.8140 time: 0.06s
Epoch 41/1000, LR 0.000284
Train loss: 0.3780;  Loss pred: 0.3780; Loss self: 0.0000; time: 0.18s
Val loss: 0.5308 score: 0.6818 time: 0.06s
Test loss: 0.5120 score: 0.8140 time: 0.06s
Epoch 42/1000, LR 0.000284
Train loss: 0.3426;  Loss pred: 0.3426; Loss self: 0.0000; time: 0.18s
Val loss: 0.5251 score: 0.6818 time: 0.06s
Test loss: 0.5070 score: 0.8372 time: 0.06s
Epoch 43/1000, LR 0.000284
Train loss: 0.3578;  Loss pred: 0.3578; Loss self: 0.0000; time: 0.18s
Val loss: 0.5211 score: 0.6818 time: 0.06s
Test loss: 0.5035 score: 0.8372 time: 0.05s
Epoch 44/1000, LR 0.000284
Train loss: 0.3427;  Loss pred: 0.3427; Loss self: 0.0000; time: 0.18s
Val loss: 0.5158 score: 0.6818 time: 0.06s
Test loss: 0.4993 score: 0.8372 time: 0.05s
Epoch 45/1000, LR 0.000284
Train loss: 0.3474;  Loss pred: 0.3474; Loss self: 0.0000; time: 0.18s
Val loss: 0.5114 score: 0.6818 time: 0.06s
Test loss: 0.4957 score: 0.8372 time: 0.07s
Epoch 46/1000, LR 0.000284
Train loss: 0.3393;  Loss pred: 0.3393; Loss self: 0.0000; time: 0.19s
Val loss: 0.5079 score: 0.6818 time: 0.05s
Test loss: 0.4926 score: 0.8372 time: 0.06s
Epoch 47/1000, LR 0.000284
Train loss: 0.3435;  Loss pred: 0.3435; Loss self: 0.0000; time: 0.18s
Val loss: 0.5040 score: 0.6818 time: 0.06s
Test loss: 0.4898 score: 0.8372 time: 0.06s
Epoch 48/1000, LR 0.000284
Train loss: 0.3247;  Loss pred: 0.3247; Loss self: 0.0000; time: 0.40s
Val loss: 0.5011 score: 0.6818 time: 0.06s
Test loss: 0.4878 score: 0.8372 time: 0.05s
Epoch 49/1000, LR 0.000284
Train loss: 0.3343;  Loss pred: 0.3343; Loss self: 0.0000; time: 0.17s
Val loss: 0.4984 score: 0.7045 time: 0.06s
Test loss: 0.4865 score: 0.8372 time: 0.05s
Epoch 50/1000, LR 0.000284
Train loss: 0.3169;  Loss pred: 0.3169; Loss self: 0.0000; time: 0.17s
Val loss: 0.4945 score: 0.7045 time: 0.06s
Test loss: 0.4845 score: 0.8372 time: 0.05s
Epoch 51/1000, LR 0.000284
Train loss: 0.3196;  Loss pred: 0.3196; Loss self: 0.0000; time: 0.18s
Val loss: 0.4896 score: 0.7273 time: 0.06s
Test loss: 0.4807 score: 0.8372 time: 0.06s
Epoch 52/1000, LR 0.000284
Train loss: 0.3022;  Loss pred: 0.3022; Loss self: 0.0000; time: 0.18s
Val loss: 0.4855 score: 0.7273 time: 0.06s
Test loss: 0.4773 score: 0.8372 time: 0.06s
Epoch 53/1000, LR 0.000284
Train loss: 0.3037;  Loss pred: 0.3037; Loss self: 0.0000; time: 0.18s
Val loss: 0.4832 score: 0.7273 time: 0.06s
Test loss: 0.4749 score: 0.8372 time: 0.06s
Epoch 54/1000, LR 0.000284
Train loss: 0.3061;  Loss pred: 0.3061; Loss self: 0.0000; time: 0.18s
Val loss: 0.4785 score: 0.7273 time: 0.06s
Test loss: 0.4703 score: 0.8372 time: 0.05s
Epoch 55/1000, LR 0.000284
Train loss: 0.2952;  Loss pred: 0.2952; Loss self: 0.0000; time: 0.18s
Val loss: 0.4737 score: 0.7273 time: 0.06s
Test loss: 0.4665 score: 0.8372 time: 0.06s
Epoch 56/1000, LR 0.000284
Train loss: 0.3097;  Loss pred: 0.3097; Loss self: 0.0000; time: 0.36s
Val loss: 0.4694 score: 0.7500 time: 0.14s
Test loss: 0.4634 score: 0.8372 time: 0.11s
Epoch 57/1000, LR 0.000283
Train loss: 0.2859;  Loss pred: 0.2859; Loss self: 0.0000; time: 0.19s
Val loss: 0.4676 score: 0.7500 time: 0.06s
Test loss: 0.4623 score: 0.8372 time: 0.06s
Epoch 58/1000, LR 0.000283
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.17s
Val loss: 0.4655 score: 0.7500 time: 0.06s
Test loss: 0.4607 score: 0.8372 time: 0.05s
Epoch 59/1000, LR 0.000283
Train loss: 0.2711;  Loss pred: 0.2711; Loss self: 0.0000; time: 0.19s
Val loss: 0.4612 score: 0.7500 time: 0.07s
Test loss: 0.4576 score: 0.8372 time: 0.06s
Epoch 60/1000, LR 0.000283
Train loss: 0.2740;  Loss pred: 0.2740; Loss self: 0.0000; time: 0.20s
Val loss: 0.4578 score: 0.7500 time: 0.07s
Test loss: 0.4547 score: 0.8372 time: 0.06s
Epoch 61/1000, LR 0.000283
Train loss: 0.2725;  Loss pred: 0.2725; Loss self: 0.0000; time: 0.21s
Val loss: 0.4555 score: 0.7500 time: 0.07s
Test loss: 0.4535 score: 0.8372 time: 0.06s
Epoch 62/1000, LR 0.000283
Train loss: 0.2840;  Loss pred: 0.2840; Loss self: 0.0000; time: 0.19s
Val loss: 0.4536 score: 0.7500 time: 0.07s
Test loss: 0.4520 score: 0.8372 time: 0.06s
Epoch 63/1000, LR 0.000283
Train loss: 0.2698;  Loss pred: 0.2698; Loss self: 0.0000; time: 0.54s
Val loss: 0.4508 score: 0.7727 time: 0.07s
Test loss: 0.4500 score: 0.8372 time: 0.06s
Epoch 64/1000, LR 0.000283
Train loss: 0.2632;  Loss pred: 0.2632; Loss self: 0.0000; time: 0.18s
Val loss: 0.4461 score: 0.8182 time: 0.07s
Test loss: 0.4473 score: 0.8372 time: 0.06s
Epoch 65/1000, LR 0.000283
Train loss: 0.2571;  Loss pred: 0.2571; Loss self: 0.0000; time: 0.18s
Val loss: 0.4427 score: 0.8182 time: 0.07s
Test loss: 0.4449 score: 0.8372 time: 0.06s
Epoch 66/1000, LR 0.000283
Train loss: 0.2525;  Loss pred: 0.2525; Loss self: 0.0000; time: 0.18s
Val loss: 0.4380 score: 0.8182 time: 0.07s
Test loss: 0.4420 score: 0.8372 time: 0.06s
Epoch 67/1000, LR 0.000283
Train loss: 0.2689;  Loss pred: 0.2689; Loss self: 0.0000; time: 0.18s
Val loss: 0.4358 score: 0.8182 time: 0.07s
Test loss: 0.4419 score: 0.8372 time: 0.06s
Epoch 68/1000, LR 0.000283
Train loss: 0.2517;  Loss pred: 0.2517; Loss self: 0.0000; time: 0.19s
Val loss: 0.4345 score: 0.8182 time: 0.07s
Test loss: 0.4430 score: 0.8372 time: 0.06s
Epoch 69/1000, LR 0.000283
Train loss: 0.2488;  Loss pred: 0.2488; Loss self: 0.0000; time: 0.32s
Val loss: 0.4322 score: 0.8182 time: 0.07s
Test loss: 0.4427 score: 0.8372 time: 0.06s
Epoch 70/1000, LR 0.000283
Train loss: 0.2492;  Loss pred: 0.2492; Loss self: 0.0000; time: 0.43s
Val loss: 0.4272 score: 0.8182 time: 0.07s
Test loss: 0.4398 score: 0.8605 time: 0.06s
Epoch 71/1000, LR 0.000282
Train loss: 0.2340;  Loss pred: 0.2340; Loss self: 0.0000; time: 0.18s
Val loss: 0.4238 score: 0.8182 time: 0.07s
Test loss: 0.4380 score: 0.8605 time: 0.06s
Epoch 72/1000, LR 0.000282
Train loss: 0.2480;  Loss pred: 0.2480; Loss self: 0.0000; time: 0.19s
Val loss: 0.4198 score: 0.8182 time: 0.07s
Test loss: 0.4355 score: 0.8605 time: 0.06s
Epoch 73/1000, LR 0.000282
Train loss: 0.2378;  Loss pred: 0.2378; Loss self: 0.0000; time: 0.19s
Val loss: 0.4164 score: 0.8409 time: 0.07s
Test loss: 0.4335 score: 0.8837 time: 0.06s
Epoch 74/1000, LR 0.000282
Train loss: 0.2265;  Loss pred: 0.2265; Loss self: 0.0000; time: 0.19s
Val loss: 0.4144 score: 0.8409 time: 0.07s
Test loss: 0.4325 score: 0.8837 time: 0.06s
Epoch 75/1000, LR 0.000282
Train loss: 0.2397;  Loss pred: 0.2397; Loss self: 0.0000; time: 0.19s
Val loss: 0.4127 score: 0.8409 time: 0.07s
Test loss: 0.4315 score: 0.8837 time: 0.06s
Epoch 76/1000, LR 0.000282
Train loss: 0.2270;  Loss pred: 0.2270; Loss self: 0.0000; time: 0.49s
Val loss: 0.4110 score: 0.8409 time: 0.06s
Test loss: 0.4308 score: 0.8837 time: 0.05s
Epoch 77/1000, LR 0.000282
Train loss: 0.2144;  Loss pred: 0.2144; Loss self: 0.0000; time: 0.20s
Val loss: 0.4100 score: 0.8409 time: 0.06s
Test loss: 0.4300 score: 0.8837 time: 0.05s
Epoch 78/1000, LR 0.000282
Train loss: 0.2217;  Loss pred: 0.2217; Loss self: 0.0000; time: 0.18s
Val loss: 0.4104 score: 0.8409 time: 0.06s
Test loss: 0.4303 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 79/1000, LR 0.000282
Train loss: 0.2283;  Loss pred: 0.2283; Loss self: 0.0000; time: 0.19s
Val loss: 0.4134 score: 0.8409 time: 0.05s
Test loss: 0.4321 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 80/1000, LR 0.000282
Train loss: 0.2255;  Loss pred: 0.2255; Loss self: 0.0000; time: 0.18s
Val loss: 0.4150 score: 0.8409 time: 0.06s
Test loss: 0.4326 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 81/1000, LR 0.000281
Train loss: 0.2162;  Loss pred: 0.2162; Loss self: 0.0000; time: 0.18s
Val loss: 0.4156 score: 0.8409 time: 0.06s
Test loss: 0.4317 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 82/1000, LR 0.000281
Train loss: 0.2097;  Loss pred: 0.2097; Loss self: 0.0000; time: 0.18s
Val loss: 0.4191 score: 0.8409 time: 0.06s
Test loss: 0.4331 score: 0.8837 time: 0.30s
     INFO: Early stopping counter 5 of 20
Epoch 83/1000, LR 0.000281
Train loss: 0.2236;  Loss pred: 0.2236; Loss self: 0.0000; time: 0.20s
Val loss: 0.4225 score: 0.8409 time: 0.07s
Test loss: 0.4348 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 84/1000, LR 0.000281
Train loss: 0.2180;  Loss pred: 0.2180; Loss self: 0.0000; time: 0.31s
Val loss: 0.4236 score: 0.8409 time: 0.06s
Test loss: 0.4349 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 85/1000, LR 0.000281
Train loss: 0.2160;  Loss pred: 0.2160; Loss self: 0.0000; time: 0.19s
Val loss: 0.4253 score: 0.8409 time: 0.06s
Test loss: 0.4346 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 86/1000, LR 0.000281
Train loss: 0.1999;  Loss pred: 0.1999; Loss self: 0.0000; time: 0.19s
Val loss: 0.4245 score: 0.8409 time: 0.06s
Test loss: 0.4328 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 87/1000, LR 0.000281
Train loss: 0.2123;  Loss pred: 0.2123; Loss self: 0.0000; time: 0.19s
Val loss: 0.4236 score: 0.8409 time: 0.06s
Test loss: 0.4312 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 88/1000, LR 0.000281
Train loss: 0.2138;  Loss pred: 0.2138; Loss self: 0.0000; time: 0.18s
Val loss: 0.4208 score: 0.8409 time: 0.29s
Test loss: 0.4282 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 89/1000, LR 0.000281
Train loss: 0.2040;  Loss pred: 0.2040; Loss self: 0.0000; time: 0.18s
Val loss: 0.4164 score: 0.8409 time: 0.06s
Test loss: 0.4241 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 90/1000, LR 0.000281
Train loss: 0.2087;  Loss pred: 0.2087; Loss self: 0.0000; time: 0.32s
Val loss: 0.4120 score: 0.8409 time: 0.07s
Test loss: 0.4199 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 91/1000, LR 0.000280
Train loss: 0.2098;  Loss pred: 0.2098; Loss self: 0.0000; time: 0.19s
Val loss: 0.4091 score: 0.8636 time: 0.06s
Test loss: 0.4171 score: 0.8837 time: 0.06s
Epoch 92/1000, LR 0.000280
Train loss: 0.1886;  Loss pred: 0.1886; Loss self: 0.0000; time: 0.19s
Val loss: 0.4077 score: 0.8636 time: 0.07s
Test loss: 0.4151 score: 0.8837 time: 0.06s
Epoch 93/1000, LR 0.000280
Train loss: 0.2028;  Loss pred: 0.2028; Loss self: 0.0000; time: 0.18s
Val loss: 0.4071 score: 0.8409 time: 0.06s
Test loss: 0.4145 score: 0.8837 time: 0.05s
Epoch 94/1000, LR 0.000280
Train loss: 0.1964;  Loss pred: 0.1964; Loss self: 0.0000; time: 0.35s
Val loss: 0.4076 score: 0.8409 time: 0.07s
Test loss: 0.4141 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 95/1000, LR 0.000280
Train loss: 0.1946;  Loss pred: 0.1946; Loss self: 0.0000; time: 0.19s
Val loss: 0.4078 score: 0.8409 time: 0.06s
Test loss: 0.4140 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 96/1000, LR 0.000280
Train loss: 0.1990;  Loss pred: 0.1990; Loss self: 0.0000; time: 0.18s
Val loss: 0.4066 score: 0.8409 time: 0.06s
Test loss: 0.4136 score: 0.8837 time: 0.06s
Epoch 97/1000, LR 0.000280
Train loss: 0.1731;  Loss pred: 0.1731; Loss self: 0.0000; time: 0.31s
Val loss: 0.4055 score: 0.8409 time: 0.06s
Test loss: 0.4131 score: 0.8837 time: 0.06s
Epoch 98/1000, LR 0.000280
Train loss: 0.1892;  Loss pred: 0.1892; Loss self: 0.0000; time: 0.20s
Val loss: 0.4027 score: 0.8409 time: 0.06s
Test loss: 0.4125 score: 0.8837 time: 0.06s
Epoch 99/1000, LR 0.000279
Train loss: 0.1906;  Loss pred: 0.1906; Loss self: 0.0000; time: 0.20s
Val loss: 0.3992 score: 0.8636 time: 0.07s
Test loss: 0.4125 score: 0.8837 time: 0.06s
Epoch 100/1000, LR 0.000279
Train loss: 0.1745;  Loss pred: 0.1745; Loss self: 0.0000; time: 0.19s
Val loss: 0.3999 score: 0.8636 time: 0.08s
Test loss: 0.4148 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 101/1000, LR 0.000279
Train loss: 0.1701;  Loss pred: 0.1701; Loss self: 0.0000; time: 0.42s
Val loss: 0.3996 score: 0.8636 time: 0.07s
Test loss: 0.4160 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 102/1000, LR 0.000279
Train loss: 0.1793;  Loss pred: 0.1793; Loss self: 0.0000; time: 0.19s
Val loss: 0.4003 score: 0.8636 time: 0.07s
Test loss: 0.4164 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 103/1000, LR 0.000279
Train loss: 0.1820;  Loss pred: 0.1820; Loss self: 0.0000; time: 0.33s
Val loss: 0.4013 score: 0.8636 time: 0.07s
Test loss: 0.4173 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 104/1000, LR 0.000279
Train loss: 0.1633;  Loss pred: 0.1633; Loss self: 0.0000; time: 0.19s
Val loss: 0.4046 score: 0.8409 time: 0.07s
Test loss: 0.4186 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 105/1000, LR 0.000279
Train loss: 0.1769;  Loss pred: 0.1769; Loss self: 0.0000; time: 0.19s
Val loss: 0.4074 score: 0.8409 time: 0.07s
Test loss: 0.4192 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 106/1000, LR 0.000279
Train loss: 0.1713;  Loss pred: 0.1713; Loss self: 0.0000; time: 0.19s
Val loss: 0.4073 score: 0.8409 time: 0.06s
Test loss: 0.4174 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 107/1000, LR 0.000278
Train loss: 0.1630;  Loss pred: 0.1630; Loss self: 0.0000; time: 0.19s
Val loss: 0.4067 score: 0.8409 time: 0.06s
Test loss: 0.4158 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 108/1000, LR 0.000278
Train loss: 0.1815;  Loss pred: 0.1815; Loss self: 0.0000; time: 0.33s
Val loss: 0.4046 score: 0.8636 time: 0.07s
Test loss: 0.4144 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 109/1000, LR 0.000278
Train loss: 0.1668;  Loss pred: 0.1668; Loss self: 0.0000; time: 0.19s
Val loss: 0.4020 score: 0.8636 time: 0.06s
Test loss: 0.4127 score: 0.8837 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 110/1000, LR 0.000278
Train loss: 0.1758;  Loss pred: 0.1758; Loss self: 0.0000; time: 0.31s
Val loss: 0.4009 score: 0.8636 time: 0.07s
Test loss: 0.4117 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 111/1000, LR 0.000278
Train loss: 0.1664;  Loss pred: 0.1664; Loss self: 0.0000; time: 0.19s
Val loss: 0.3976 score: 0.8636 time: 0.07s
Test loss: 0.4090 score: 0.8837 time: 0.06s
Epoch 112/1000, LR 0.000278
Train loss: 0.1594;  Loss pred: 0.1594; Loss self: 0.0000; time: 0.19s
Val loss: 0.3931 score: 0.8636 time: 0.07s
Test loss: 0.4066 score: 0.8837 time: 0.06s
Epoch 113/1000, LR 0.000278
Train loss: 0.1517;  Loss pred: 0.1517; Loss self: 0.0000; time: 0.19s
Val loss: 0.3904 score: 0.8636 time: 0.07s
Test loss: 0.4046 score: 0.8837 time: 0.06s
Epoch 114/1000, LR 0.000277
Train loss: 0.1481;  Loss pred: 0.1481; Loss self: 0.0000; time: 0.19s
Val loss: 0.3868 score: 0.8636 time: 0.07s
Test loss: 0.4030 score: 0.8837 time: 0.06s
Epoch 115/1000, LR 0.000277
Train loss: 0.1501;  Loss pred: 0.1501; Loss self: 0.0000; time: 0.30s
Val loss: 0.3863 score: 0.8636 time: 0.07s
Test loss: 0.4037 score: 0.8837 time: 0.06s
Epoch 116/1000, LR 0.000277
Train loss: 0.1449;  Loss pred: 0.1449; Loss self: 0.0000; time: 0.19s
Val loss: 0.3895 score: 0.8636 time: 0.07s
Test loss: 0.4057 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 117/1000, LR 0.000277
Train loss: 0.1518;  Loss pred: 0.1518; Loss self: 0.0000; time: 0.33s
Val loss: 0.3924 score: 0.8636 time: 0.07s
Test loss: 0.4072 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 118/1000, LR 0.000277
Train loss: 0.1440;  Loss pred: 0.1440; Loss self: 0.0000; time: 0.19s
Val loss: 0.3930 score: 0.8636 time: 0.07s
Test loss: 0.4065 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 119/1000, LR 0.000277
Train loss: 0.1502;  Loss pred: 0.1502; Loss self: 0.0000; time: 0.19s
Val loss: 0.3931 score: 0.8636 time: 0.07s
Test loss: 0.4077 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 120/1000, LR 0.000277
Train loss: 0.1387;  Loss pred: 0.1387; Loss self: 0.0000; time: 0.19s
Val loss: 0.3916 score: 0.8636 time: 0.07s
Test loss: 0.4078 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 121/1000, LR 0.000276
Train loss: 0.1423;  Loss pred: 0.1423; Loss self: 0.0000; time: 0.19s
Val loss: 0.3866 score: 0.8636 time: 0.07s
Test loss: 0.4044 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 122/1000, LR 0.000276
Train loss: 0.1350;  Loss pred: 0.1350; Loss self: 0.0000; time: 0.20s
Val loss: 0.3846 score: 0.8864 time: 0.28s
Test loss: 0.4026 score: 0.8837 time: 0.08s
Epoch 123/1000, LR 0.000276
Train loss: 0.1298;  Loss pred: 0.1298; Loss self: 0.0000; time: 0.20s
Val loss: 0.3840 score: 0.8864 time: 0.06s
Test loss: 0.4017 score: 0.8837 time: 0.05s
Epoch 124/1000, LR 0.000276
Train loss: 0.1405;  Loss pred: 0.1405; Loss self: 0.0000; time: 0.17s
Val loss: 0.3825 score: 0.8636 time: 0.19s
Test loss: 0.4003 score: 0.8837 time: 0.05s
Epoch 125/1000, LR 0.000276
Train loss: 0.1259;  Loss pred: 0.1259; Loss self: 0.0000; time: 0.19s
Val loss: 0.3819 score: 0.8636 time: 0.06s
Test loss: 0.3986 score: 0.8837 time: 0.06s
Epoch 126/1000, LR 0.000276
Train loss: 0.1276;  Loss pred: 0.1276; Loss self: 0.0000; time: 0.19s
Val loss: 0.3799 score: 0.8864 time: 0.06s
Test loss: 0.3948 score: 0.8837 time: 0.05s
Epoch 127/1000, LR 0.000275
Train loss: 0.1253;  Loss pred: 0.1253; Loss self: 0.0000; time: 0.19s
Val loss: 0.3804 score: 0.8864 time: 0.06s
Test loss: 0.3945 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 128/1000, LR 0.000275
Train loss: 0.1230;  Loss pred: 0.1230; Loss self: 0.0000; time: 0.19s
Val loss: 0.3797 score: 0.8864 time: 0.06s
Test loss: 0.3913 score: 0.8837 time: 0.06s
Epoch 129/1000, LR 0.000275
Train loss: 0.1276;  Loss pred: 0.1276; Loss self: 0.0000; time: 0.17s
Val loss: 0.3760 score: 0.8864 time: 0.06s
Test loss: 0.3880 score: 0.8837 time: 0.06s
Epoch 130/1000, LR 0.000275
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.18s
Val loss: 0.3698 score: 0.8636 time: 0.26s
Test loss: 0.3853 score: 0.8837 time: 0.05s
Epoch 131/1000, LR 0.000275
Train loss: 0.1023;  Loss pred: 0.1023; Loss self: 0.0000; time: 0.17s
Val loss: 0.3771 score: 0.8864 time: 0.06s
Test loss: 0.3917 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 132/1000, LR 0.000275
Train loss: 0.1031;  Loss pred: 0.1031; Loss self: 0.0000; time: 0.17s
Val loss: 0.3864 score: 0.8864 time: 0.19s
Test loss: 0.3963 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 133/1000, LR 0.000274
Train loss: 0.1041;  Loss pred: 0.1041; Loss self: 0.0000; time: 0.18s
Val loss: 0.3857 score: 0.8864 time: 0.06s
Test loss: 0.3941 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 134/1000, LR 0.000274
Train loss: 0.0959;  Loss pred: 0.0959; Loss self: 0.0000; time: 0.17s
Val loss: 0.3756 score: 0.8864 time: 0.06s
Test loss: 0.3894 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 135/1000, LR 0.000274
Train loss: 0.0940;  Loss pred: 0.0940; Loss self: 0.0000; time: 0.17s
Val loss: 0.3742 score: 0.8864 time: 0.06s
Test loss: 0.3826 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 136/1000, LR 0.000274
Train loss: 0.0865;  Loss pred: 0.0865; Loss self: 0.0000; time: 0.17s
Val loss: 0.3767 score: 0.8864 time: 0.06s
Test loss: 0.3805 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 137/1000, LR 0.000274
Train loss: 0.0920;  Loss pred: 0.0920; Loss self: 0.0000; time: 0.17s
Val loss: 0.3784 score: 0.8409 time: 0.06s
Test loss: 0.3846 score: 0.8837 time: 0.26s
     INFO: Early stopping counter 7 of 20
Epoch 138/1000, LR 0.000274
Train loss: 0.0861;  Loss pred: 0.0861; Loss self: 0.0000; time: 0.18s
Val loss: 0.3830 score: 0.8409 time: 0.06s
Test loss: 0.3842 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 139/1000, LR 0.000273
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.30s
Val loss: 0.3806 score: 0.8409 time: 0.07s
Test loss: 0.3854 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 140/1000, LR 0.000273
Train loss: 0.0837;  Loss pred: 0.0837; Loss self: 0.0000; time: 0.17s
Val loss: 0.3709 score: 0.8864 time: 0.06s
Test loss: 0.3851 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 141/1000, LR 0.000273
Train loss: 0.0848;  Loss pred: 0.0848; Loss self: 0.0000; time: 0.18s
Val loss: 0.3699 score: 0.8864 time: 0.06s
Test loss: 0.3842 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 142/1000, LR 0.000273
Train loss: 0.0806;  Loss pred: 0.0806; Loss self: 0.0000; time: 0.18s
Val loss: 0.3743 score: 0.8636 time: 0.06s
Test loss: 0.3880 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 143/1000, LR 0.000273
Train loss: 0.0741;  Loss pred: 0.0741; Loss self: 0.0000; time: 0.18s
Val loss: 0.3664 score: 0.8864 time: 0.06s
Test loss: 0.3872 score: 0.8837 time: 0.05s
Epoch 144/1000, LR 0.000272
Train loss: 0.0778;  Loss pred: 0.0778; Loss self: 0.0000; time: 0.17s
Val loss: 0.3573 score: 0.8864 time: 0.06s
Test loss: 0.3827 score: 0.8605 time: 0.05s
Epoch 145/1000, LR 0.000272
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.18s
Val loss: 0.3584 score: 0.8864 time: 0.29s
Test loss: 0.3806 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 146/1000, LR 0.000272
Train loss: 0.0722;  Loss pred: 0.0722; Loss self: 0.0000; time: 0.18s
Val loss: 0.3729 score: 0.8636 time: 0.06s
Test loss: 0.3828 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 147/1000, LR 0.000272
Train loss: 0.0726;  Loss pred: 0.0726; Loss self: 0.0000; time: 0.31s
Val loss: 0.3631 score: 0.8864 time: 0.07s
Test loss: 0.3831 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 148/1000, LR 0.000272
Train loss: 0.0682;  Loss pred: 0.0682; Loss self: 0.0000; time: 0.19s
Val loss: 0.3608 score: 0.8864 time: 0.07s
Test loss: 0.3832 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 149/1000, LR 0.000272
Train loss: 0.0608;  Loss pred: 0.0608; Loss self: 0.0000; time: 0.19s
Val loss: 0.3715 score: 0.8409 time: 0.07s
Test loss: 0.3839 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 150/1000, LR 0.000271
Train loss: 0.0660;  Loss pred: 0.0660; Loss self: 0.0000; time: 0.20s
Val loss: 0.3683 score: 0.8636 time: 0.07s
Test loss: 0.3850 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 151/1000, LR 0.000271
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.20s
Val loss: 0.3642 score: 0.8864 time: 0.16s
Test loss: 0.3851 score: 0.8605 time: 0.13s
     INFO: Early stopping counter 7 of 20
Epoch 152/1000, LR 0.000271
Train loss: 0.0595;  Loss pred: 0.0595; Loss self: 0.0000; time: 0.18s
Val loss: 0.3642 score: 0.8864 time: 0.16s
Test loss: 0.3861 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 153/1000, LR 0.000271
Train loss: 0.0577;  Loss pred: 0.0577; Loss self: 0.0000; time: 0.19s
Val loss: 0.3665 score: 0.8864 time: 0.06s
Test loss: 0.3848 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 154/1000, LR 0.000271
Train loss: 0.0557;  Loss pred: 0.0557; Loss self: 0.0000; time: 0.19s
Val loss: 0.3739 score: 0.8636 time: 0.06s
Test loss: 0.3858 score: 0.8837 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 155/1000, LR 0.000270
Train loss: 0.0537;  Loss pred: 0.0537; Loss self: 0.0000; time: 0.20s
Val loss: 0.3707 score: 0.8636 time: 0.06s
Test loss: 0.3857 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 156/1000, LR 0.000270
Train loss: 0.0578;  Loss pred: 0.0578; Loss self: 0.0000; time: 0.18s
Val loss: 0.3567 score: 0.8864 time: 0.06s
Test loss: 0.3845 score: 0.8837 time: 0.06s
Epoch 157/1000, LR 0.000270
Train loss: 0.0515;  Loss pred: 0.0515; Loss self: 0.0000; time: 0.18s
Val loss: 0.3623 score: 0.8864 time: 0.06s
Test loss: 0.3892 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 158/1000, LR 0.000270
Train loss: 0.0561;  Loss pred: 0.0561; Loss self: 0.0000; time: 0.17s
Val loss: 0.3701 score: 0.8864 time: 0.06s
Test loss: 0.3929 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 159/1000, LR 0.000270
Train loss: 0.0556;  Loss pred: 0.0556; Loss self: 0.0000; time: 0.19s
Val loss: 0.3639 score: 0.8864 time: 0.26s
Test loss: 0.3954 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 160/1000, LR 0.000269
Train loss: 0.0564;  Loss pred: 0.0564; Loss self: 0.0000; time: 0.18s
Val loss: 0.3507 score: 0.8864 time: 0.18s
Test loss: 0.3942 score: 0.8605 time: 0.05s
Epoch 161/1000, LR 0.000269
Train loss: 0.0495;  Loss pred: 0.0495; Loss self: 0.0000; time: 0.17s
Val loss: 0.3553 score: 0.8864 time: 0.06s
Test loss: 0.3969 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 162/1000, LR 0.000269
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.17s
Val loss: 0.3458 score: 0.8636 time: 0.06s
Test loss: 0.3934 score: 0.8605 time: 0.06s
Epoch 163/1000, LR 0.000269
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.18s
Val loss: 0.3452 score: 0.8864 time: 0.06s
Test loss: 0.3920 score: 0.8372 time: 0.05s
Epoch 164/1000, LR 0.000269
Train loss: 0.0449;  Loss pred: 0.0449; Loss self: 0.0000; time: 0.17s
Val loss: 0.3494 score: 0.8864 time: 0.06s
Test loss: 0.3938 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 165/1000, LR 0.000268
Train loss: 0.0485;  Loss pred: 0.0485; Loss self: 0.0000; time: 0.36s
Val loss: 0.3584 score: 0.8864 time: 0.06s
Test loss: 0.3972 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 166/1000, LR 0.000268
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.17s
Val loss: 0.3563 score: 0.8864 time: 0.06s
Test loss: 0.3958 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 167/1000, LR 0.000268
Train loss: 0.0491;  Loss pred: 0.0491; Loss self: 0.0000; time: 0.17s
Val loss: 0.3517 score: 0.8864 time: 0.19s
Test loss: 0.3938 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 168/1000, LR 0.000268
Train loss: 0.0482;  Loss pred: 0.0482; Loss self: 0.0000; time: 0.19s
Val loss: 0.3631 score: 0.8636 time: 0.06s
Test loss: 0.3956 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 169/1000, LR 0.000267
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.17s
Val loss: 0.3575 score: 0.8864 time: 0.07s
Test loss: 0.3954 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 170/1000, LR 0.000267
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.19s
Val loss: 0.3543 score: 0.8864 time: 0.07s
Test loss: 0.3945 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 171/1000, LR 0.000267
Train loss: 0.0430;  Loss pred: 0.0430; Loss self: 0.0000; time: 0.19s
Val loss: 0.3426 score: 0.8864 time: 0.07s
Test loss: 0.3920 score: 0.8372 time: 0.06s
Epoch 172/1000, LR 0.000267
Train loss: 0.0457;  Loss pred: 0.0457; Loss self: 0.0000; time: 0.40s
Val loss: 0.3487 score: 0.8864 time: 0.06s
Test loss: 0.3936 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 173/1000, LR 0.000267
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.18s
Val loss: 0.3515 score: 0.8864 time: 0.06s
Test loss: 0.3932 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 174/1000, LR 0.000266
Train loss: 0.0398;  Loss pred: 0.0398; Loss self: 0.0000; time: 0.18s
Val loss: 0.3562 score: 0.8864 time: 0.19s
Test loss: 0.3944 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 175/1000, LR 0.000266
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.19s
Val loss: 0.3601 score: 0.8864 time: 0.06s
Test loss: 0.3951 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 176/1000, LR 0.000266
Train loss: 0.0407;  Loss pred: 0.0407; Loss self: 0.0000; time: 0.19s
Val loss: 0.3554 score: 0.8864 time: 0.06s
Test loss: 0.3939 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 177/1000, LR 0.000266
Train loss: 0.0425;  Loss pred: 0.0425; Loss self: 0.0000; time: 0.19s
Val loss: 0.3483 score: 0.8864 time: 0.06s
Test loss: 0.3925 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 178/1000, LR 0.000265
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.18s
Val loss: 0.3444 score: 0.8864 time: 0.06s
Test loss: 0.3914 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 179/1000, LR 0.000265
Train loss: 0.0434;  Loss pred: 0.0434; Loss self: 0.0000; time: 0.18s
Val loss: 0.3517 score: 0.8864 time: 0.26s
Test loss: 0.3957 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 180/1000, LR 0.000265
Train loss: 0.0350;  Loss pred: 0.0350; Loss self: 0.0000; time: 0.18s
Val loss: 0.3670 score: 0.8864 time: 0.06s
Test loss: 0.4030 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 181/1000, LR 0.000265
Train loss: 0.0429;  Loss pred: 0.0429; Loss self: 0.0000; time: 0.18s
Val loss: 0.3568 score: 0.8864 time: 0.06s
Test loss: 0.4049 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 182/1000, LR 0.000265
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.18s
Val loss: 0.3468 score: 0.8636 time: 0.06s
Test loss: 0.4049 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 183/1000, LR 0.000264
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.18s
Val loss: 0.3534 score: 0.8864 time: 0.06s
Test loss: 0.4070 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 184/1000, LR 0.000264
Train loss: 0.0397;  Loss pred: 0.0397; Loss self: 0.0000; time: 0.18s
Val loss: 0.3564 score: 0.8864 time: 0.06s
Test loss: 0.4063 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 185/1000, LR 0.000264
Train loss: 0.0376;  Loss pred: 0.0376; Loss self: 0.0000; time: 0.17s
Val loss: 0.3500 score: 0.8864 time: 0.06s
Test loss: 0.4018 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 186/1000, LR 0.000264
Train loss: 0.0403;  Loss pred: 0.0403; Loss self: 0.0000; time: 0.17s
Val loss: 0.3428 score: 0.8636 time: 0.06s
Test loss: 0.3988 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 187/1000, LR 0.000263
Train loss: 0.0366;  Loss pred: 0.0366; Loss self: 0.0000; time: 0.18s
Val loss: 0.3395 score: 0.8864 time: 0.06s
Test loss: 0.3976 score: 0.8605 time: 0.28s
Epoch 188/1000, LR 0.000263
Train loss: 0.0369;  Loss pred: 0.0369; Loss self: 0.0000; time: 0.20s
Val loss: 0.3460 score: 0.8864 time: 0.07s
Test loss: 0.3989 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 189/1000, LR 0.000263
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.19s
Val loss: 0.3629 score: 0.8864 time: 0.07s
Test loss: 0.4035 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 190/1000, LR 0.000263
Train loss: 0.0421;  Loss pred: 0.0421; Loss self: 0.0000; time: 0.19s
Val loss: 0.3401 score: 0.8864 time: 0.06s
Test loss: 0.3976 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 191/1000, LR 0.000262
Train loss: 0.0340;  Loss pred: 0.0340; Loss self: 0.0000; time: 0.19s
Val loss: 0.3344 score: 0.8864 time: 0.07s
Test loss: 0.3953 score: 0.8605 time: 0.06s
Epoch 192/1000, LR 0.000262
Train loss: 0.0377;  Loss pred: 0.0377; Loss self: 0.0000; time: 0.19s
Val loss: 0.3373 score: 0.8864 time: 0.07s
Test loss: 0.3962 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 193/1000, LR 0.000262
Train loss: 0.0332;  Loss pred: 0.0332; Loss self: 0.0000; time: 0.19s
Val loss: 0.3540 score: 0.8864 time: 0.07s
Test loss: 0.3980 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 194/1000, LR 0.000262
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.19s
Val loss: 0.3611 score: 0.8864 time: 0.07s
Test loss: 0.3997 score: 0.8372 time: 0.27s
     INFO: Early stopping counter 3 of 20
Epoch 195/1000, LR 0.000261
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.19s
Val loss: 0.3461 score: 0.8864 time: 0.07s
Test loss: 0.3976 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 196/1000, LR 0.000261
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.19s
Val loss: 0.3402 score: 0.8636 time: 0.07s
Test loss: 0.3947 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 197/1000, LR 0.000261
Train loss: 0.0346;  Loss pred: 0.0346; Loss self: 0.0000; time: 0.23s
Val loss: 0.3392 score: 0.8864 time: 0.08s
Test loss: 0.3938 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 198/1000, LR 0.000261
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.22s
Val loss: 0.3417 score: 0.8864 time: 0.06s
Test loss: 0.3960 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 199/1000, LR 0.000260
Train loss: 0.0320;  Loss pred: 0.0320; Loss self: 0.0000; time: 0.17s
Val loss: 0.3452 score: 0.8864 time: 0.06s
Test loss: 0.3980 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 200/1000, LR 0.000260
Train loss: 0.0348;  Loss pred: 0.0348; Loss self: 0.0000; time: 0.40s
Val loss: 0.3451 score: 0.8864 time: 0.07s
Test loss: 0.3970 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 201/1000, LR 0.000260
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.19s
Val loss: 0.3500 score: 0.8864 time: 0.07s
Test loss: 0.3979 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 10 of 20
Epoch 202/1000, LR 0.000260
Train loss: 0.0338;  Loss pred: 0.0338; Loss self: 0.0000; time: 0.20s
Val loss: 0.3407 score: 0.8864 time: 0.07s
Test loss: 0.3954 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 203/1000, LR 0.000259
Train loss: 0.0321;  Loss pred: 0.0321; Loss self: 0.0000; time: 0.21s
Val loss: 0.3377 score: 0.8636 time: 0.07s
Test loss: 0.3968 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 204/1000, LR 0.000259
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.21s
Val loss: 0.3415 score: 0.8636 time: 0.07s
Test loss: 0.3983 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 205/1000, LR 0.000259
Train loss: 0.0331;  Loss pred: 0.0331; Loss self: 0.0000; time: 0.20s
Val loss: 0.3506 score: 0.8864 time: 0.06s
Test loss: 0.3973 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 206/1000, LR 0.000259
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.18s
Val loss: 0.3508 score: 0.8864 time: 0.07s
Test loss: 0.3980 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 207/1000, LR 0.000258
Train loss: 0.0324;  Loss pred: 0.0324; Loss self: 0.0000; time: 0.18s
Val loss: 0.3457 score: 0.8864 time: 0.06s
Test loss: 0.3981 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 208/1000, LR 0.000258
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.38s
Val loss: 0.3445 score: 0.8636 time: 0.06s
Test loss: 0.3990 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 209/1000, LR 0.000258
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.18s
Val loss: 0.3429 score: 0.8864 time: 0.06s
Test loss: 0.3973 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 210/1000, LR 0.000258
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.18s
Val loss: 0.3371 score: 0.8636 time: 0.06s
Test loss: 0.3938 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 211/1000, LR 0.000257
Train loss: 0.0328;  Loss pred: 0.0328; Loss self: 0.0000; time: 0.18s
Val loss: 0.3325 score: 0.8864 time: 0.06s
Test loss: 0.3931 score: 0.8605 time: 0.06s
Epoch 212/1000, LR 0.000257
Train loss: 0.0314;  Loss pred: 0.0314; Loss self: 0.0000; time: 0.18s
Val loss: 0.3463 score: 0.8864 time: 0.06s
Test loss: 0.3984 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 213/1000, LR 0.000257
Train loss: 0.0370;  Loss pred: 0.0370; Loss self: 0.0000; time: 0.19s
Val loss: 0.3616 score: 0.8864 time: 0.06s
Test loss: 0.4043 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 214/1000, LR 0.000256
Train loss: 0.0326;  Loss pred: 0.0326; Loss self: 0.0000; time: 0.19s
Val loss: 0.3467 score: 0.8864 time: 0.07s
Test loss: 0.4070 score: 0.8372 time: 0.27s
     INFO: Early stopping counter 3 of 20
Epoch 215/1000, LR 0.000256
Train loss: 0.0299;  Loss pred: 0.0299; Loss self: 0.0000; time: 0.21s
Val loss: 0.3466 score: 0.8864 time: 0.07s
Test loss: 0.4069 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 216/1000, LR 0.000256
Train loss: 0.0295;  Loss pred: 0.0295; Loss self: 0.0000; time: 0.20s
Val loss: 0.3472 score: 0.8864 time: 0.06s
Test loss: 0.4068 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 217/1000, LR 0.000256
Train loss: 0.0327;  Loss pred: 0.0327; Loss self: 0.0000; time: 0.19s
Val loss: 0.3366 score: 0.8864 time: 0.06s
Test loss: 0.4048 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 218/1000, LR 0.000255
Train loss: 0.0306;  Loss pred: 0.0306; Loss self: 0.0000; time: 0.18s
Val loss: 0.3363 score: 0.8864 time: 0.06s
Test loss: 0.4044 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 219/1000, LR 0.000255
Train loss: 0.0279;  Loss pred: 0.0279; Loss self: 0.0000; time: 0.18s
Val loss: 0.3430 score: 0.8636 time: 0.06s
Test loss: 0.4079 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 220/1000, LR 0.000255
Train loss: 0.0281;  Loss pred: 0.0281; Loss self: 0.0000; time: 0.19s
Val loss: 0.3643 score: 0.8864 time: 0.06s
Test loss: 0.4153 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 221/1000, LR 0.000255
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.19s
Val loss: 0.3546 score: 0.8864 time: 0.06s
Test loss: 0.4148 score: 0.8372 time: 0.28s
     INFO: Early stopping counter 10 of 20
Epoch 222/1000, LR 0.000254
Train loss: 0.0286;  Loss pred: 0.0286; Loss self: 0.0000; time: 0.19s
Val loss: 0.3462 score: 0.8636 time: 0.09s
Test loss: 0.4134 score: 0.8605 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 223/1000, LR 0.000254
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.17s
Val loss: 0.3437 score: 0.8636 time: 0.06s
Test loss: 0.4129 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 224/1000, LR 0.000254
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.18s
Val loss: 0.3423 score: 0.8636 time: 0.06s
Test loss: 0.4105 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 225/1000, LR 0.000253
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.18s
Val loss: 0.3416 score: 0.8636 time: 0.06s
Test loss: 0.4082 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 226/1000, LR 0.000253
Train loss: 0.0268;  Loss pred: 0.0268; Loss self: 0.0000; time: 0.17s
Val loss: 0.3415 score: 0.8864 time: 0.06s
Test loss: 0.4066 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 227/1000, LR 0.000253
Train loss: 0.0271;  Loss pred: 0.0271; Loss self: 0.0000; time: 0.18s
Val loss: 0.3421 score: 0.8864 time: 0.06s
Test loss: 0.4060 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 16 of 20
Epoch 228/1000, LR 0.000253
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.18s
Val loss: 0.3424 score: 0.8864 time: 0.06s
Test loss: 0.4053 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 229/1000, LR 0.000252
Train loss: 0.0246;  Loss pred: 0.0246; Loss self: 0.0000; time: 0.18s
Val loss: 0.3466 score: 0.8864 time: 0.29s
Test loss: 0.4076 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 230/1000, LR 0.000252
Train loss: 0.0264;  Loss pred: 0.0264; Loss self: 0.0000; time: 0.19s
Val loss: 0.3467 score: 0.8864 time: 0.07s
Test loss: 0.4067 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 231/1000, LR 0.000252
Train loss: 0.0289;  Loss pred: 0.0289; Loss self: 0.0000; time: 0.31s
Val loss: 0.3341 score: 0.8864 time: 0.07s
Test loss: 0.4048 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 210,   Train_Loss: 0.0328,   Val_Loss: 0.3325,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8864,   Val_Loss: 0.3325,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.3931


[0.06446969199987507, 0.06739953799933573, 0.06648357499943813, 0.06758560599973862, 0.06738812200001121, 0.06710440599999856, 0.08056942100029119, 0.17509985200013034, 0.07131669000045804, 0.06412443899989739, 0.06837282000014966, 0.06592467199970997, 0.06553784399966389, 0.06337089499993454, 0.062299672000335704, 0.06690161500046088, 0.06560062500011554, 0.07608798299952468, 0.073636236999846, 0.06533113499972387, 0.0689830230003281, 0.061442332999831706, 0.06847192399982305, 0.06883335100064869, 0.06696296599966445, 0.06939569799942547, 0.06422895200012135, 0.06721065100009582, 0.0684434839995447, 0.06812691599952814, 0.06882696600041527, 0.06779346799976338, 0.06794366899976012, 0.07018917000004876, 0.06816678099949058, 0.06644392799989873, 0.06804566899972997, 0.06908714400015015, 0.06838531599987618, 0.06672030800018547, 0.0680080789998101, 0.06776118499965378, 0.0678581000001941, 0.18484589800027607, 0.06670466600007785, 0.062036368000008224, 0.06299412599946663, 0.0682391930004087, 0.0675870989998657, 0.06669125699954748, 0.06246435000048223, 0.06730252400029713, 0.0799949350002862, 0.06102383400047984, 0.06101523300003464, 0.06725154099967767, 0.06652604499959125, 0.0687209099996835, 0.06839494699943316, 0.06975308100027178, 0.06908792899957916, 0.06938369599993166, 0.07001804599985917, 0.07115857400003733, 0.060237153000343824, 0.06167264699979569, 0.06376186400029837, 0.06241424600011669, 0.0635969760005537, 0.06397705499966833, 0.06351719000031153, 0.06311881400051789, 0.06462976900002104, 0.06561224200049764, 0.06461545599995588, 0.06779296300010174, 0.06337684000027366, 0.06453995799984114, 0.06428873999993812, 0.061633469999833324, 0.06195763799951237, 0.06303466200006369, 0.06267207599921676, 0.06887229499989189, 0.06676640800014866, 0.06911353100076667, 0.06855855899993912, 0.07268926399956399, 0.06795819499984646, 0.06685699300032866, 0.06734974199935095, 0.06483118899996043, 0.06628180799998518, 0.06668122199971549, 0.06829893899976014, 0.06640270899970346, 0.06704590399931476, 0.06804066099994088, 0.08546103000026051, 0.06689234100031172, 0.06716357599998446, 0.07005777100039268, 0.300154499999735, 0.06084551399999327, 0.060525646999849414, 0.06564192400037427, 0.06757724700037215, 0.0691543119992275, 0.06777195399990887, 0.06726434899974265, 0.06675026199991407, 0.07078908299990871, 0.06981862099928549, 0.06817521900029533, 0.06866262899984577, 0.06782407799983048, 0.06748675599919807, 0.06859721399996488, 0.0671858229998179, 0.06536242299989681, 0.0629014019996248, 0.060527473000547616, 0.06280239799980336, 0.061916339000163134, 0.0629511300003287, 0.06415727800049353, 0.06484735199956049, 0.0613989120001861, 0.06261458200060588, 0.06823806500051433, 0.0639425539993681, 0.06238242400013405, 0.061805293999896094, 0.06407321200003935, 0.06392565399983141, 0.06319622099999833, 0.06979081499957829, 0.0685262689994488, 0.07066979499995796, 0.06477853499927733, 0.06504720900011307, 0.06442563899963716, 0.17092731600041589, 0.0633834129994284, 0.06261844399978145, 0.0622781580004812, 0.0667546479999146, 0.06820491399957973, 0.06772521499988216, 0.06414695100011159, 0.06975227800012362, 0.06206525699963095, 0.06657000700033677, 0.2855716570002187, 0.06550212999991345, 0.06532986599995638, 0.07029703599982895, 0.06448903100044845, 0.0628892149998137, 0.06423149800048122, 0.06254450800042832, 0.0618987890002245, 0.06766208599947277, 0.06787331900068239, 0.06667413600007421, 0.06632038000043394, 0.0670805809995727, 0.0667233820004185, 0.06915260799996759, 0.06899299799988512, 0.06685222800024349, 0.06713050100006512, 0.06602022999959445, 0.06370894199972099, 0.06321600699993724, 0.0628563310001482, 0.06269178900038241, 0.06282329199984815, 0.12756734300000971, 0.0670878430000812, 0.06929198999932851, 0.07065578999936406, 0.07127165700057958, 0.0717947950006419, 0.07529028999942966, 0.06767957499960175, 0.06715813000027993, 0.06653007599925331, 0.06677520900029776, 0.06752103300004819, 0.06791726099982043, 0.05973532400003023, 0.057456085000012536, 0.05958210800054076, 0.06409911999980977, 0.06052233300033549, 0.2813432040002226, 0.06322725399968476, 0.06470384699969145, 0.06406702899948868, 0.06513543599976401, 0.06480815700069797, 0.06334394199984672, 0.0644616829995357, 0.06509572899994964, 0.2805564219997905, 0.06565473499995278, 0.06546396999965509, 0.0654863949994251, 0.06600266399982502, 0.06410358999983146, 0.06360433000008925, 0.05991598699984024, 0.059810537999510416, 0.060316623999824515, 0.06469909700081189, 0.06342879699968762, 0.0662733750004918, 0.06502709800042794, 0.06103804800022772, 0.07736964899959275, 0.12872505499944964, 0.0650866269998005, 0.06489126199994644, 0.06638531900080125, 0.06383441300022241, 0.06481423100012762, 0.0649150670005838, 0.06455990800077416, 0.07371690400032094, 0.06923690900021029, 0.06231894699976692, 0.05999836799946934, 0.05955096400066395, 0.05867165200015734, 0.07492373100012628, 0.060739735000424844, 0.060708707999765466, 0.059835664000274846, 0.05811661500047194, 0.05946234699968045, 0.06113394300064101, 0.06133646900070744, 0.06025085800047236, 0.059805513999890536, 0.060175634999723115, 0.11542306800038205, 0.06094141700032196, 0.058557696000207216, 0.06529584899999463, 0.06604184200023155, 0.06519763699998293, 0.06606432900025538, 0.06512739200024953, 0.06322394500057271, 0.06413423000049079, 0.06293092300074932, 0.06393559500065749, 0.0690650780006763, 0.06503354600044986, 0.06625618700036284, 0.06370601200069359, 0.06743428400022822, 0.06505420200028311, 0.06349963699994987, 0.0632339759995375, 0.0594808900004864, 0.05995607800014113, 0.05859801999940828, 0.06011483299971587, 0.061007815999801096, 0.06087496899999678, 0.3097376269997767, 0.06933801200011658, 0.05853801300054329, 0.05925863699940237, 0.06127727000057348, 0.06215178599995852, 0.06117434399948252, 0.06155651199969725, 0.06560405199979868, 0.06609438000032242, 0.06520729500061861, 0.059185539000282006, 0.06316314800005784, 0.06376746499972796, 0.07107211200036545, 0.06193453099967883, 0.06437177299994801, 0.06523278599979676, 0.07003820199952315, 0.06786904100044922, 0.06429139100055181, 0.06500730500010832, 0.06455303200073104, 0.06479922099970281, 0.06477388399980555, 0.06418283599941788, 0.06511576899993088, 0.07221861600010016, 0.06500094700004411, 0.06640795099974639, 0.06547933999991074, 0.06686195400016004, 0.06468320299973129, 0.065639538999676, 0.06774514900007489, 0.06795324500035349, 0.06485229899953993, 0.06611184800021874, 0.0636467120002635, 0.06680882299951918, 0.08298066200040921, 0.059069382000416226, 0.05952627599981497, 0.060309148000669666, 0.05794996500026173, 0.06041085899960308, 0.06122832900018693, 0.06030780700075411, 0.06018159100040066, 0.05955631300003006, 0.05815059199994721, 0.05713944400031323, 0.05724953899971297, 0.05879928799913614, 0.0568541330003427, 0.2655112669999653, 0.060040145999664674, 0.060340263999933086, 0.06152325700077199, 0.06218153600002552, 0.05917848100034462, 0.05717048299993621, 0.05898217800040584, 0.06499551300021267, 0.05827149300057499, 0.06528392300060659, 0.06562681300056283, 0.06821540199962328, 0.0683496350002315, 0.13586085099996126, 0.05937731499943766, 0.06198117399981129, 0.06173307699918951, 0.06083956600014062, 0.06098579200079257, 0.058801290000701556, 0.06191227099952812, 0.06235664300038479, 0.06056932999945275, 0.057238697000684624, 0.0603671150001901, 0.05970515700028045, 0.061638150999897334, 0.05955462399924727, 0.05925435399967682, 0.06451154000023962, 0.05718449199957831, 0.0639655449995189, 0.06529783699988911, 0.06838472299932619, 0.06188396599918633, 0.0610837399999582, 0.0640310259996113, 0.0591434220004885, 0.05981240999972215, 0.061264824000318185, 0.059600780000437226, 0.05935622900051385, 0.06027346200062311, 0.06011090200081526, 0.060258759999669564, 0.061012307000055443, 0.06045289500070794, 0.057811365000816295, 0.06093617100032134, 0.2859846000001198, 0.06455287599965231, 0.06467494400021678, 0.06427088500004174, 0.06367556999975932, 0.06448514600015187, 0.06550723600048514, 0.272520383000483, 0.06425068099997588, 0.08055891999993037, 0.08089693800047826, 0.05824457399921812, 0.0597972169998684, 0.06570006600031775, 0.06594383900028333, 0.06587671699981001, 0.06583147299988923, 0.062395038999966346, 0.063467128999946, 0.06231669899989356, 0.06785701500029973, 0.06054468900038046, 0.06124487099987164, 0.060943399000279896, 0.06068984099965746, 0.062242011000307684, 0.06291826099914033, 0.2792144069999267, 0.06487213900072675, 0.05955491599979723, 0.05875091300003987, 0.060822678000477026, 0.06116400499922747, 0.059399986999778775, 0.28483608399983495, 0.16096916500009684, 0.06017479399997683, 0.06232955499945092, 0.05942293200041604, 0.06093486500049039, 0.06180937499993888, 0.06186464599977626, 0.06409952600006363, 0.06401758699939819, 0.06687569600035204]
[0.0014652202727244332, 0.001531807681803085, 0.0015109903408963212, 0.0015360364999940596, 0.001531548227272982, 0.0015251001363636037, 0.0018311232045520724, 0.003979542090912053, 0.0016208338636467736, 0.0014573736136340317, 0.0015539277272761285, 0.0014982879999934084, 0.0014894964545378157, 0.0014402476136348757, 0.001415901636371266, 0.0015204912500104745, 0.0014909232954571714, 0.0017292723408982883, 0.001673550840905591, 0.0014847985227209972, 0.0015677959772801842, 0.0013964166590870843, 0.0015561800909050692, 0.0015643943409238339, 0.0015218855909014646, 0.001577174954532397, 0.0014597489090936672, 0.0015275147954567233, 0.0015555337272623794, 0.0015483389999892759, 0.0015642492272821653, 0.0015407606363582586, 0.0015441742954490937, 0.0015952084090920173, 0.001549245022715695, 0.0015100892727249711, 0.0015464924772665902, 0.001570162363639776, 0.0015542117272699133, 0.0015163706363678516, 0.001545638159086593, 0.001540026931810313, 0.0015422295454589569, 0.004201043136369911, 0.0015160151363654056, 0.0014099174545456415, 0.0014316846818060599, 0.0015508907500092887, 0.0015360704318151295, 0.0015157103863533518, 0.001419644318192778, 0.001529602818188571, 0.0018180667045519592, 0.0013869053181927236, 0.0013867098409098783, 0.001528444113629038, 0.0015119555681725285, 0.0015618388636291704, 0.001554430613623481, 0.0015852972954607221, 0.00157018020453589, 0.0015769021818166286, 0.0015913192272695267, 0.0016172403181826667, 0.0013690262045532688, 0.0014016510681771747, 0.001449133272734054, 0.001418505590911743, 0.0014453858181944022, 0.0014540239772651892, 0.0014435725000070802, 0.0014345185000117701, 0.0014688583863641145, 0.0014911873181931283, 0.0014685330909080883, 0.0015407491590932213, 0.0014403827272789467, 0.0014668172272691168, 0.0014611077272713208, 0.0014007606818143938, 0.0014081281363525538, 0.001432605954546902, 0.0014243653636185627, 0.0015652794318157248, 0.0015174183636397422, 0.0015707620681992425, 0.0015581490681804346, 0.001652028727262818, 0.0015445044318146922, 0.001519477113643833, 0.0015306759545307034, 0.0014734361136354644, 0.0015064047272723903, 0.001515482318175352, 0.0015522486136309124, 0.0015091524772659877, 0.0015237705454389718, 0.0015463786590895654, 0.001942296136369557, 0.0015202804772798117, 0.001526444909090556, 0.0015922220681907427, 0.006821693181812158, 0.001382852590908938, 0.001375582886360214, 0.0014918619090994152, 0.0015358465227357308, 0.0015716889090733523, 0.0015402716818161107, 0.0015287352045396058, 0.0015170514090889562, 0.0016088427954524707, 0.0015867868408928518, 0.0015494367954612574, 0.0015605142954510402, 0.0015414563181779654, 0.0015337899090726833, 0.0015590275909082927, 0.001526950522723134, 0.0015200563488348094, 0.0014628233023168556, 0.001407615651175526, 0.0014605208837163571, 0.0014399148604689101, 0.0014639797674495048, 0.0014920297209417102, 0.0015080779534781508, 0.0014278816744229326, 0.001456153069781532, 0.0015869317441980077, 0.0014870361395201883, 0.0014507540465147455, 0.0014373324186022348, 0.0014900746976753336, 0.0014866431162751492, 0.001469679558139496, 0.0016230422092925183, 0.001593634162777879, 0.001643483604650185, 0.0015064775581227286, 0.001512725790700304, 0.0014982706744101665, 0.003975053860474788, 0.0014740328604518232, 0.0014562428837158476, 0.0014483292558251442, 0.0015524336744166185, 0.0015861607906879007, 0.0015750049999972593, 0.0014917895581421298, 0.001622146000002875, 0.0014433780697588592, 0.0015481396976822505, 0.006641201325586482, 0.0015233053488351966, 0.0015192992093013113, 0.0016348147906936966, 0.0014997449069871732, 0.0014625398837165978, 0.0014937557674530517, 0.0014545234418704262, 0.0014395067209354534, 0.001573536883708669, 0.001578449279085637, 0.0015505613023273073, 0.001542334418614743, 0.0015600135116179698, 0.0015517065581492674, 0.0016082001860457577, 0.0016044883255787238, 0.0015547029767498486, 0.0015611744418619795, 0.0015353541860370801, 0.001481603302319093, 0.001470139697672959, 0.0014617751395383303, 0.0014579485814042422, 0.0014610067906941431, 0.002966682395349063, 0.0015601823953507255, 0.0016114416278913606, 0.0016431579069619549, 0.0016574803953623158, 0.001669646395363765, 0.0017509369767309223, 0.0015739436046419012, 0.001561816976750696, 0.001547211069750077, 0.001552911837216227, 0.0015702565813964695, 0.0015794711860423354, 0.001389193581396052, 0.0013361880232561055, 0.001385630418617227, 0.0014906772092979016, 0.001407496116286872, 0.006542865209307503, 0.0014704012558066223, 0.0015047406278998012, 0.001489930906964853, 0.0015147775813898609, 0.001507166441876697, 0.0014731149302289934, 0.0014991089069659464, 0.0015138541627895267, 0.006524567953483499, 0.0015268543023244832, 0.001522417906968723, 0.0015229394185912815, 0.0015349456744145353, 0.001490781162786778, 0.0014791704651183546, 0.0013933950465079127, 0.0013909427441746607, 0.0014027121860424306, 0.0015046301628095788, 0.0014750883023183167, 0.0015412412790812046, 0.001512258093033208, 0.0014194894883773888, 0.0017992941627812268, 0.002993605930219759, 0.0015136424883674534, 0.001509099116277824, 0.0015438446279256103, 0.0014845212325633119, 0.0015073076976773866, 0.0015096527209438094, 0.0015013932093203293, 0.0017143466046586264, 0.0016101606744234952, 0.0014492778372038819, 0.0013953108837085892, 0.0013849061395503244, 0.001364457023259473, 0.001742412348840146, 0.001412551976754066, 0.001411830418599197, 0.0013915270697738337, 0.001351549186057487, 0.0013828452790623361, 0.00142171960466607, 0.001426429511644359, 0.0014011827441970316, 0.0013908259069741985, 0.001399433372086584, 0.002684257395357722, 0.001417242255821441, 0.0013618068837257492, 0.001518508116278945, 0.0015358567907030594, 0.001516224116278673, 0.0015363797441919856, 0.00151459051163371, 0.0014703243023389003, 0.0014914937209416462, 0.0014635098372267283, 0.0014868743023408717, 0.0016061646046668905, 0.0015124080465220898, 0.0015408415581479732, 0.0014815351628068277, 0.0015682391627960051, 0.001512888418611235, 0.0014767357441848807, 0.0014705575813845932, 0.0013832765116392185, 0.0013943273953521194, 0.0013627446511490298, 0.0013980193720864156, 0.0014187864186000255, 0.0014156969534882971, 0.007203200627901783, 0.0016125119069794554, 0.0013613491395475183, 0.0013781078371954039, 0.0014250527907110112, 0.0014453903720920585, 0.001422659162778663, 0.0014315467906906337, 0.001525675627902295, 0.001537078604658661, 0.001516448720944619, 0.0013764078837274886, 0.0014689104186059964, 0.001482964302319255, 0.0016528398139619871, 0.001440337930225089, 0.0014970179767429769, 0.0015170415348789944, 0.0016287953953377477, 0.0015783497907081213, 0.0014951486279198096, 0.0015117977907001935, 0.0015012333023425822, 0.0015069586279000654, 0.001506369395344315, 0.001492624093009718, 0.001514320209300718, 0.0016795026976767478, 0.0015116499302335838, 0.0015443709534824742, 0.0015227753488351334, 0.0015549291627944195, 0.0015042605348774718, 0.0015265009069692094, 0.0015754685813970904, 0.0015803080232640347, 0.0015081929999893006, 0.0015374848372143894, 0.001480156093029384, 0.001553693558128353, 0.001929782837218819, 0.0013737065581492145, 0.001384331999995697, 0.001402538325596969, 0.0013476736046572494, 0.0014049036976651879, 0.001423914627911324, 0.0014025071395524212, 0.001399571883730248, 0.00138503053488442, 0.0013523393488359817, 0.001328824279077052, 0.0013313846279003015, 0.0013674253023054916, 0.0013221891395428535, 0.00617468062790617, 0.0013962824651084808, 0.001403261953486816, 0.0014307734186226044, 0.001446082232558733, 0.0013762437441940608, 0.0013295461162775863, 0.0013716785581489729, 0.0015115235581444806, 0.0013551510000133719, 0.0015182307674559672, 0.0015262049535014614, 0.0015864046976656577, 0.001589526395354221, 0.0031595546744177037, 0.0013808677906845966, 0.001441422651158402, 0.0014356529534695233, 0.001414873627910247, 0.0014182742325765715, 0.0013674718604814314, 0.0014398202558029796, 0.0014501544883810417, 0.001408589069754715, 0.0013311324883880146, 0.001403886395353258, 0.0013884920232623361, 0.0014334453720906356, 0.0013849912557964482, 0.0013780082325506237, 0.0015002683720985958, 0.0013298719069669376, 0.0014875708139423, 0.0015185543488346305, 0.001590342395333167, 0.0014391619999810775, 0.0014205520930222837, 0.0014890936278979373, 0.0013754284186160116, 0.0013909862790633058, 0.001424763348844609, 0.0013860646511729587, 0.0013803774186166011, 0.0014017084186191422, 0.0013979279535073317, 0.0014013665116202223, 0.0014188908604664057, 0.0014058812790862312, 0.001344450348856193, 0.0014171202558214266, 0.006650804651165577, 0.001501229674410519, 0.0015040684651213205, 0.0014946717441870173, 0.0014808272092967283, 0.0014996545581430667, 0.001523424093034538, 0.006337683325592628, 0.0014942018837203694, 0.0018734632558123341, 0.001881324139546006, 0.0013545249767260028, 0.0013906329534853115, 0.0015279085116352965, 0.0015335776511693799, 0.0015320166744141864, 0.001530964488369517, 0.0014510474186038686, 0.0014759797441847907, 0.0014492255581370595, 0.00157807011628604, 0.001408016023264662, 0.0014242993255784101, 0.0014172883488437185, 0.0014113916511548246, 0.0014474886279141322, 0.001463215372073031, 0.006493358302323877, 0.0015086543953657383, 0.0013849980465069124, 0.0013663003023265086, 0.0014144808837320238, 0.001422418720912267, 0.0013813950465064832, 0.006624094976740348, 0.003743468953490624, 0.0013994138139529495, 0.0014495245348709516, 0.0013819286511724661, 0.0014170898837323346, 0.001437427325579974, 0.0014387126976692153, 0.0014906866511642705, 0.0014887810930092602, 0.0015552487441942333]
[682.491239450706, 652.823465947699, 661.8175993148963, 651.0261963201183, 652.9340586163343, 655.6946499161462, 546.1128980912123, 251.28519240534393, 616.9663791143056, 686.1658470036737, 643.5305725272658, 667.4284249786418, 671.3678283378631, 694.3250525346921, 706.2637504698725, 657.6821800145914, 670.7253170213318, 578.277912824616, 597.5318918060956, 673.4920493909369, 637.8380953207969, 716.1186408745339, 642.5991476464676, 639.2250175294436, 657.0796162198145, 634.045067179298, 685.0493216815505, 654.6581433936307, 642.8661638599914, 645.8533951588937, 639.2843177154507, 649.0300805994108, 647.5952895648797, 626.877337343773, 645.4756899893632, 662.2125049570678, 646.6245485833141, 636.8768116960281, 643.412980647478, 659.4693777474422, 646.981956366138, 649.339293582673, 648.4119066091467, 238.0361180637846, 659.6240209035549, 709.261380356667, 698.4778231604093, 644.7907436381387, 651.0118151407479, 659.7566454670146, 704.4017907760237, 653.7644858580006, 550.0348240778317, 721.0297537131808, 721.1313935320876, 654.2601008980729, 661.3950972174935, 640.2709160894791, 643.3223787769681, 630.7965091868639, 636.8695752953894, 634.1547443659291, 628.4094246230251, 618.3372927059628, 730.446208169049, 713.4443248421908, 690.0676554843833, 704.9672601975795, 691.8567951975723, 687.7465678941944, 692.7258589333721, 697.098015809343, 680.8008241524997, 670.6065614960434, 680.9516286634275, 649.0349153190718, 694.2599220757932, 681.7481969868699, 684.4122314427434, 713.8978220781499, 710.1626437138597, 698.0286496968214, 702.0670577523216, 638.8635662579426, 659.0140359191112, 636.6336571562508, 641.7871180757908, 605.3163504346932, 647.4568666825159, 658.1211332640058, 653.3061403624089, 678.6856862987173, 663.832223768094, 659.8559336568208, 644.2266987508331, 662.6235685685126, 656.2667870128158, 646.6721421186403, 514.854548322971, 657.7733615242283, 655.1169937707036, 628.0530963474898, 146.5911722131064, 723.1428762357873, 726.9645543831934, 670.3033262667487, 651.1067253118152, 636.2582278382223, 649.2361132166731, 654.1355213319368, 659.1734426459126, 621.5647686813055, 630.204369124539, 645.3957998992184, 640.8143795382323, 648.7371638153338, 651.9797751209562, 641.4254666380842, 654.9000672376858, 657.8703485344765, 683.6095640643509, 710.4211999666823, 684.6872312126463, 694.4855056738207, 683.0695493436809, 670.2279357872574, 663.0956958780898, 700.3381427975412, 686.7409894964071, 630.146824938191, 672.4786126063237, 689.296716009426, 695.7332813605302, 671.1072951980867, 672.656395507716, 680.4204321014881, 616.1269215764256, 627.4965882112431, 608.4636300420226, 663.8001307142823, 661.05833995007, 667.4361429343708, 251.56891833424316, 678.4109274833114, 686.6986346730379, 690.4507355478908, 644.1499024915091, 630.4531078254121, 634.9186193070752, 670.3358356023064, 616.4673216826523, 692.8191725727598, 645.9365401566274, 150.5751671986379, 656.4672019071261, 658.1981968251505, 611.6900860529116, 666.7800606230381, 683.7420374881032, 669.4534821479307, 687.5104045858915, 694.6824113125064, 635.5110009516269, 633.5331855447895, 644.9277422950354, 648.3678169473493, 641.020088962466, 644.4517455624522, 621.8131353776297, 623.2516522918978, 643.2096773176114, 640.5434096188003, 651.3155134458657, 674.944499944581, 680.2074670746397, 684.0997448594098, 685.895245384331, 684.4595154310591, 337.0768645702429, 640.9507010077513, 620.5623478329414, 608.5842363433629, 603.3253864106222, 598.9292120635702, 571.1227835664564, 635.346779294241, 640.2798886720159, 646.3242278647419, 643.9515599241067, 636.8385981294052, 633.1232939460514, 719.8420820481067, 748.3976675401851, 721.6931633169095, 670.8360426808919, 710.4815341431342, 152.83823951889727, 680.0864703093762, 664.5663587855147, 671.1720626274582, 660.1629257560476, 663.496726184281, 678.8336602118013, 667.0629434281094, 660.5656109947438, 153.26685339618464, 654.94133819946, 656.8498671899451, 656.6249371396531, 651.4888550576383, 670.7892646903716, 676.0546019420323, 717.6715623513746, 718.936853575067, 712.9046214543623, 664.6151491026283, 677.9255170204752, 648.8276777767982, 661.2627861651926, 704.4786229048409, 555.7734920088152, 334.04530299236495, 660.6579873947347, 662.6469986056904, 647.7335749411855, 673.6178493542375, 663.4345472665615, 662.4039993614008, 666.0480371112731, 583.3126144284735, 621.0560324099592, 689.998821709244, 716.686160536572, 722.0705948525128, 732.8922662665897, 573.9169609683149, 707.9385512580725, 708.3003644249211, 718.6349599095697, 739.891681572487, 723.1466998810371, 703.3735743095964, 701.0511152753845, 713.6827827358555, 718.9972483152424, 714.5749272142763, 372.5425146371752, 705.5956706712748, 734.318508703755, 658.5410965405096, 651.102372339179, 659.5331054714645, 650.8807498799205, 660.2444636480338, 680.1220645059476, 670.4687964550435, 683.2888816756748, 672.5518077927923, 622.601193610162, 661.1972227333653, 648.995994891232, 674.9755423323602, 637.6578418161076, 660.9872794967628, 677.1692253931144, 680.0141746632302, 722.921260923439, 717.1916748773791, 733.8131902824398, 715.2976703803408, 704.8277224042931, 706.3658627900455, 138.827175814939, 620.1504594612217, 734.5654181941727, 725.6326196033442, 701.7283896557005, 691.8546154092611, 702.9090495905235, 698.5451027538968, 655.4473190182209, 650.5848152261999, 659.4354205245297, 726.5288231943805, 680.7767085953445, 674.3250653006739, 605.0193077107219, 694.2815147857176, 667.994650388684, 659.1777331131308, 613.9506551052348, 633.5731191444917, 668.8298282367375, 661.4641231462888, 666.1189825988815, 663.588224312098, 663.847793967845, 669.9610469127603, 660.3623156173683, 595.4143457961084, 661.5288235719215, 647.5128256880598, 656.6956844717528, 643.1161135359142, 664.7784587936783, 655.0929615793348, 634.7317945961336, 632.7880294719747, 663.0451142573227, 650.4129184205791, 675.6044208508678, 643.6275639867127, 518.1930218848814, 727.9575059664076, 722.3700672982409, 712.9929940234363, 742.0194300342682, 711.7925603455254, 702.2892948763753, 713.0088480826755, 714.5042077686801, 722.0057426989864, 739.4593678433924, 752.5449495057088, 751.0977512013781, 731.3013722314416, 756.3214445595505, 161.9516960084621, 716.1874656374113, 712.6253209639203, 698.9226854400839, 691.5236059781855, 726.6154736169993, 752.1363777886568, 729.0337769436749, 661.5841311977844, 737.9251463417232, 658.6613981454586, 655.2199936881168, 630.3561767507794, 629.118209626933, 316.50029926584415, 724.182290836277, 693.7590436755993, 696.5471687174211, 706.7769023845538, 705.0822591504784, 731.2764736876856, 694.5311374594503, 689.5817018201999, 709.9302567881882, 751.2400221040266, 712.3083486740188, 720.2057939450359, 697.6198880474468, 722.0262191655091, 725.6850694926912, 666.5474115149054, 751.9521201712711, 672.2369050451053, 658.5210471837378, 628.7954109344523, 694.8488078570365, 703.9516571845376, 671.5494454244889, 727.0461962725938, 718.9143523927518, 701.8709463651878, 721.4670680431457, 724.4395529174832, 713.4151345007436, 715.3444478244031, 713.5891943384795, 704.7758413718224, 711.2976144400767, 743.7983863448449, 705.6564154609131, 150.3577465359393, 666.1205923688295, 664.8633510970782, 669.0432222922101, 675.29823447458, 666.8202317460633, 656.4160331796254, 157.78636271740376, 669.2536068219439, 533.7708102347594, 531.5405139282996, 738.2661945570627, 719.0970108206647, 654.4894490637504, 652.070013694763, 652.7344099452316, 653.1830147575819, 689.157354321442, 677.5160729270831, 690.0237125858263, 633.6854045202264, 710.2191903195636, 702.0996092895701, 705.5727233035117, 708.5205578350864, 690.8517142832585, 683.4263903223186, 154.00351458229477, 662.8423335866617, 722.0226790370489, 731.9035195243829, 706.9731457674842, 703.0278674613133, 723.9058823390003, 150.96401901110576, 267.13190690884267, 714.58491407576, 689.8813893405593, 723.6263602694486, 705.6715395964846, 695.6873451647508, 695.0658054384651, 670.831793669696, 671.6904215774991, 642.9839623616574]
Elapsed: 0.07150182126541997~0.035610244552735856
Time per graph: 0.001652263323818328~0.0008257143499053818
Speed: 652.33728326326~101.10373099610534
Total Time: 0.0681
best val loss: 0.33251461115750397 test_score: 0.8605

Testing...
Test loss: 0.4026 score: 0.8837 time: 0.06s
test Score 0.8837
Epoch Time List: [0.2903120949995355, 0.3980348620007135, 0.30279004399926635, 0.3042066350008099, 0.4868346109997219, 0.29824983599974075, 0.310318843999994, 0.43608431900065625, 0.36730222500045784, 0.29387197100004414, 0.29760491799970623, 0.29845263400056865, 0.5031798919999346, 0.2784005949997663, 0.2730238439999084, 0.42297254799996153, 0.29083148099925893, 0.30617148199962685, 0.3189013840010375, 0.3225448150005832, 0.5247720550005397, 0.39632135500050936, 0.29003623599965067, 0.3036314580003818, 0.2934266460006256, 0.3004526899994744, 0.3251651419996051, 0.5294976730001508, 0.2959876010008884, 0.47337431400046626, 0.3070974260017465, 0.30001463899952796, 0.2955653769995479, 0.3137485619990912, 0.3008091700003206, 0.45015781999973115, 0.29322713400051725, 0.3004105750005692, 0.29942723700060014, 0.29620807399987825, 0.29807307299870445, 0.30179881700041733, 0.2978595459990174, 0.46511933699912333, 0.2931461339994712, 0.2813477319996309, 0.27237540800069837, 0.28173125100056495, 0.2993500269994911, 0.2982135489992288, 0.2799033359997338, 0.2806151540007704, 0.4630859680000867, 0.2699940639995475, 0.2642221639998752, 0.2957152870003483, 0.29535603300064395, 0.2966215549995468, 0.301226211001449, 0.510928702999081, 0.3045476580000468, 0.3051510400009647, 0.3049188270006198, 0.307775944000241, 0.29399757699957263, 0.26335352800106193, 0.2729243060002773, 0.4717341060004401, 0.2754923000011331, 0.2800423040007445, 0.277669899999637, 0.27604659300050116, 0.2781622860002244, 0.28070408800067526, 0.2795754470007523, 0.2855379159991571, 0.47519074300089414, 0.291433101999246, 0.28083213200079626, 0.27418556599877775, 0.2707319250002911, 0.27410356100062927, 0.273878438999418, 0.2913270889994237, 0.4849043719996189, 0.2861967849994471, 0.2978630059997158, 0.3025898050009346, 0.295970991998729, 0.3019773300002271, 0.28894363600011275, 0.40762626200012164, 0.2868598839995684, 0.4137685590003457, 0.29408404500009055, 0.29099297399989155, 0.29434484600005817, 0.2935690860003888, 0.3019691360004799, 0.4139528639998389, 0.3066005810005663, 0.3101512819994241, 0.534819979000531, 0.2672785240001758, 0.2637353229993096, 0.3789664209998591, 0.29177950999928726, 0.29935186000057, 0.29535366799973417, 0.2922522980006761, 0.29214395899907686, 0.29855259399937495, 0.5081580810001469, 0.41097782500037283, 0.29650085600133025, 0.2988588100006382, 0.2971635590010919, 0.29952708600012556, 0.29568360700068297, 0.2759674620001533, 0.28395227799956047, 0.47846048199971847, 0.4067353930004174, 0.2804292630007694, 0.2844048049992125, 0.2832687780010019, 0.288318846999573, 0.2849268590007341, 0.27939025100113213, 0.633094624999103, 0.29216120400087675, 0.2765926420006508, 0.2801642720005475, 0.28328552299990406, 0.2848107589998108, 0.28273712700047327, 0.4266155360000994, 0.5167160040000454, 0.3136212669996894, 0.3160634799996842, 0.2868938490000801, 0.3142810979998103, 0.39580781899985595, 0.29710261799937143, 0.29866252899955725, 0.4889817849989413, 0.2872428859991487, 0.29603562700140174, 0.3151126539996767, 0.30645502200150077, 0.4331352910003261, 0.29013606300031825, 0.2861828470004184, 0.5272148270005346, 0.29946655099865893, 0.2984198860003744, 0.4361406279995208, 0.28603941399978794, 0.28573714300091524, 0.28585426499921596, 0.28642492099879746, 0.45586502400055906, 0.30937441200057947, 0.30312168299860787, 0.4261700300003213, 0.30184755599930213, 0.3017575769999894, 0.3050954649997948, 0.31234976300129347, 0.30837125400012155, 0.5397257930007981, 0.301038160000644, 0.292466949998925, 0.2876617180008907, 0.28527020499950595, 0.28302319400063425, 0.27891543799978535, 0.28602954000052705, 0.4104514549990199, 0.4367798280009083, 0.30667892600013147, 0.32031298599940783, 0.3196448930002589, 0.327583541000422, 0.3232178680009383, 0.5031732610004838, 0.304730226999709, 0.30203237799923954, 0.30153256300036446, 0.2941003889991407, 0.304556536999371, 0.2846242750001693, 0.28505923000011535, 0.2868564529990181, 0.43993335199957073, 0.29640825300066354, 0.517878847999782, 0.3195658259992342, 0.31612382200000866, 0.3157387060000474, 0.316448390000005, 0.31616967900026793, 0.3071819639999376, 0.31276561800041236, 0.3126895820005302, 0.547989948000577, 0.3253722489998836, 0.31944583899985446, 0.319635840999581, 0.3209734809997826, 0.3177901920007571, 0.31039618799968594, 0.3015068519998749, 0.2885220030002529, 0.5014436950004892, 0.3076245320007729, 0.3123453910002354, 0.31998738099991897, 0.3189283570000043, 0.3086951740006043, 0.3275596929997846, 0.5534739550002996, 0.30787553299978754, 0.3167698819997895, 0.31692333499995584, 0.3180022039996402, 0.31807742699947994, 0.31625420799900894, 0.32105539799977123, 0.36450670500016713, 0.4904218060000858, 0.2995610489997489, 0.29822752400013997, 0.2933611380012735, 0.29143452100015566, 0.3088485969992689, 0.30058867600018857, 0.29436144700048317, 0.5134180679988276, 0.2870354469996528, 0.28889229300057195, 0.2935670949991618, 0.29706782800076326, 0.29590392100180907, 0.2939380470006654, 0.29548568400059594, 0.6096080759998586, 0.3053221870013658, 0.2870597920000364, 0.3151511340001889, 0.32677994899950136, 0.3339596290015834, 0.3144833500000459, 0.6666294639999251, 0.3076292600007946, 0.30968512300023576, 0.30745302499963145, 0.3100817079994158, 0.31795758700081933, 0.4522262230002525, 0.5579935810001189, 0.3095998230000987, 0.3200970990010319, 0.31826175799960765, 0.3156364950000352, 0.31025551600032486, 0.6019474139984595, 0.3146709580005336, 0.297726896000313, 0.29578070399929857, 0.2971185019996483, 0.2932460929987428, 0.5415736589993685, 0.32626373400034936, 0.4247961840010248, 0.30171286200038594, 0.30345569900055125, 0.30907273999946483, 0.5305501559996628, 0.2938429490004637, 0.45303860700005316, 0.3142324769987681, 0.31834032500046305, 0.2979770000001736, 0.472524945999794, 0.30798808399958943, 0.30840356100088684, 0.43317405300058454, 0.32371611500002473, 0.32423850700070034, 0.33870019300047716, 0.5581207879995418, 0.31525934700039215, 0.45753488700029266, 0.3176982200002385, 0.31509070600077393, 0.313228787999833, 0.3150311360004707, 0.4601253640003051, 0.3199301190006736, 0.44122196299940697, 0.32148947799942107, 0.32383134700012306, 0.3239263569994364, 0.32257843499974115, 0.4253527370010488, 0.32228848800059495, 0.46622009799921216, 0.32170253500044055, 0.3199618239996198, 0.3163863540003149, 0.3157535029995415, 0.5529155359990909, 0.31566420099989045, 0.4174683869996443, 0.30486602900055004, 0.30209078900043096, 0.30393197600005806, 0.31150216799960617, 0.2867435510006544, 0.48908567700073036, 0.28663298100036627, 0.4111165960002836, 0.28616304300066986, 0.2759896889992888, 0.2816244999994524, 0.2779673590002858, 0.4878997550003987, 0.2982546450002701, 0.41588356099964585, 0.29082967399972404, 0.29463705600119283, 0.2899193639996156, 0.2867872160004481, 0.28505167399998754, 0.5262340499994025, 0.2884934229987266, 0.43716799700086995, 0.31564745400009997, 0.32371004299966444, 0.33074060200033273, 0.4943164079986673, 0.3915724879998379, 0.3049135920009576, 0.3130929279996053, 0.31520012600049085, 0.303331094000896, 0.28859108699907665, 0.2926703310004086, 0.5021289099995556, 0.4214686100012841, 0.28712703099881765, 0.28368691299874627, 0.2919108859996413, 0.29517825599941716, 0.47464004599987675, 0.2870320180009003, 0.41736889900039387, 0.2971634819996325, 0.2937004679997699, 0.3135168580010941, 0.31729845000063506, 0.5193202920008844, 0.2973012100001142, 0.4265470100008315, 0.3050738239990096, 0.3074887390002914, 0.30206094999994093, 0.2976582689998395, 0.49746333900020545, 0.2912095069996212, 0.29265210199992, 0.2941205090000949, 0.29683730699980515, 0.2963118249999752, 0.2823063579999143, 0.28951428000073065, 0.5237096299997575, 0.324092080999435, 0.3193252860000939, 0.3131172809999043, 0.310805497999354, 0.31523514299988165, 0.31707048999942344, 0.5236036229998717, 0.3117215470001611, 0.32822039799975755, 0.38720835100048134, 0.32962445500015747, 0.28766737500063755, 0.5332662849996268, 0.32436433599923475, 0.3273546009995698, 0.3372063499991782, 0.3361180890005926, 0.3175487139997131, 0.30636130699986097, 0.3058524700009002, 0.4937371859996347, 0.29700541900001554, 0.2986299600006532, 0.2967230969998127, 0.30093604499961657, 0.30739910199918086, 0.5288405960000091, 0.33537931600039883, 0.3129750710004373, 0.29970539699934307, 0.2989554750001844, 0.3019671760011988, 0.30106322599931445, 0.5243788650004717, 0.4381884229997013, 0.2912307400001737, 0.2984834729995782, 0.30007289499917533, 0.292853414000092, 0.299908492999748, 0.30131961299866816, 0.5265407010001582, 0.3147591670003749, 0.4443137240004944]
Total Epoch List: [119, 72, 231]
Total Time List: [0.06781762899936439, 0.06866784999965603, 0.06809460200020112]
T-times Epoch Time: 0.35024819589370493 ~ 0.010560538097643847
T-times Total Epoch: 125.55555555555554 ~ 10.740485309861837
T-times Total Time: 0.0689921104445198 ~ 0.0006448414618449538
T-times Inference Elapsed: 0.0756639602805918 ~ 0.003670235924477213
T-times Time Per Graph: 0.001746689631171282 ~ 8.402134969755218e-05
T-times Speed: 634.1455824467353 ~ 15.630583004226786
T-times cross validation test micro f1 score:0.6959918035223343 ~ 0.007225544980412222
T-times cross validation test precision:0.9358465608465609 ~ 0.006079621795916114
T-times cross validation test recall:0.6038961038961039 ~ 0.0214274782177742
T-times cross validation test f1_score:0.6959918035223343 ~ 0.019516030005428307
