Namespace(seed=15, model='Ethident', dataset='mining/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Volume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 124], edge_attr=[124, 2], x=[30, 14887], y=[1, 1], num_nodes=32)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99682dbd00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7457;  Loss pred: 0.7137; Loss self: 3.1914; time: 0.35s
Val loss: 0.7256 score: 0.5116 time: 0.07s
Test loss: 0.7277 score: 0.4773 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7457;  Loss pred: 0.7137; Loss self: 3.1914; time: 0.13s
Val loss: 0.7150 score: 0.4186 time: 0.05s
Test loss: 0.7155 score: 0.4545 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.7204;  Loss pred: 0.6884; Loss self: 3.1974; time: 0.13s
Val loss: 0.7053 score: 0.5116 time: 0.05s
Test loss: 0.7070 score: 0.4091 time: 0.07s
Epoch 4/1000, LR 0.000060
Train loss: 0.6920;  Loss pred: 0.6599; Loss self: 3.2146; time: 0.13s
Val loss: 0.6992 score: 0.5581 time: 0.05s
Test loss: 0.7059 score: 0.5455 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.6870;  Loss pred: 0.6545; Loss self: 3.2456; time: 0.13s
Val loss: 0.6933 score: 0.5581 time: 0.05s
Test loss: 0.6970 score: 0.5227 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.6708;  Loss pred: 0.6383; Loss self: 3.2465; time: 0.13s
Val loss: 0.6747 score: 0.5814 time: 0.05s
Test loss: 0.6740 score: 0.5682 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.6348;  Loss pred: 0.6025; Loss self: 3.2228; time: 0.17s
Val loss: 0.6546 score: 0.7209 time: 0.06s
Test loss: 0.6533 score: 0.6364 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.6124;  Loss pred: 0.5805; Loss self: 3.1874; time: 0.16s
Val loss: 0.6335 score: 0.6977 time: 0.05s
Test loss: 0.6360 score: 0.6136 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.5669;  Loss pred: 0.5358; Loss self: 3.1096; time: 0.15s
Val loss: 0.6105 score: 0.7209 time: 0.07s
Test loss: 0.6162 score: 0.7045 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.5234;  Loss pred: 0.4930; Loss self: 3.0346; time: 0.14s
Val loss: 0.6019 score: 0.7674 time: 0.06s
Test loss: 0.5991 score: 0.7273 time: 0.06s
Epoch 11/1000, LR 0.000270
Train loss: 0.4913;  Loss pred: 0.4614; Loss self: 2.9917; time: 0.15s
Val loss: 0.6054 score: 0.7442 time: 0.06s
Test loss: 0.5855 score: 0.7273 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4633;  Loss pred: 0.4335; Loss self: 2.9833; time: 0.18s
Val loss: 0.5953 score: 0.7209 time: 0.06s
Test loss: 0.5779 score: 0.7045 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.4353;  Loss pred: 0.4053; Loss self: 2.9987; time: 0.15s
Val loss: 0.5857 score: 0.7209 time: 0.06s
Test loss: 0.5734 score: 0.6818 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.4081;  Loss pred: 0.3783; Loss self: 2.9818; time: 0.16s
Val loss: 0.5770 score: 0.7442 time: 0.05s
Test loss: 0.5663 score: 0.7500 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.3750;  Loss pred: 0.3459; Loss self: 2.9084; time: 0.16s
Val loss: 0.5637 score: 0.7442 time: 0.05s
Test loss: 0.5555 score: 0.7500 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.3398;  Loss pred: 0.3117; Loss self: 2.8094; time: 0.19s
Val loss: 0.5534 score: 0.7442 time: 0.05s
Test loss: 0.5488 score: 0.7500 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.3139;  Loss pred: 0.2867; Loss self: 2.7218; time: 0.15s
Val loss: 0.5432 score: 0.7442 time: 0.05s
Test loss: 0.5359 score: 0.7500 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.2899;  Loss pred: 0.2634; Loss self: 2.6562; time: 0.15s
Val loss: 0.5399 score: 0.7907 time: 0.05s
Test loss: 0.5243 score: 0.7727 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.2703;  Loss pred: 0.2442; Loss self: 2.6121; time: 0.14s
Val loss: 0.5343 score: 0.7907 time: 0.05s
Test loss: 0.5171 score: 0.7727 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.2512;  Loss pred: 0.2254; Loss self: 2.5780; time: 0.18s
Val loss: 0.5254 score: 0.7674 time: 0.06s
Test loss: 0.5116 score: 0.7500 time: 0.06s
Epoch 21/1000, LR 0.000270
Train loss: 0.2320;  Loss pred: 0.2065; Loss self: 2.5570; time: 0.14s
Val loss: 0.5176 score: 0.7674 time: 0.05s
Test loss: 0.5094 score: 0.7273 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.2156;  Loss pred: 0.1902; Loss self: 2.5399; time: 0.14s
Val loss: 0.5144 score: 0.7674 time: 0.05s
Test loss: 0.5142 score: 0.7273 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.2007;  Loss pred: 0.1755; Loss self: 2.5216; time: 0.14s
Val loss: 0.5170 score: 0.7674 time: 0.06s
Test loss: 0.5202 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1886;  Loss pred: 0.1636; Loss self: 2.5054; time: 0.15s
Val loss: 0.5194 score: 0.7674 time: 0.07s
Test loss: 0.5249 score: 0.7273 time: 0.12s
     INFO: Early stopping counter 2 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1787;  Loss pred: 0.1538; Loss self: 2.4939; time: 0.19s
Val loss: 0.5225 score: 0.7674 time: 0.07s
Test loss: 0.5332 score: 0.7273 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1709;  Loss pred: 0.1460; Loss self: 2.4888; time: 0.16s
Val loss: 0.5215 score: 0.7674 time: 0.06s
Test loss: 0.5379 score: 0.7273 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1623;  Loss pred: 0.1374; Loss self: 2.4896; time: 0.14s
Val loss: 0.5170 score: 0.7907 time: 0.07s
Test loss: 0.5399 score: 0.7500 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1544;  Loss pred: 0.1295; Loss self: 2.4925; time: 0.14s
Val loss: 0.5125 score: 0.7907 time: 0.05s
Test loss: 0.5441 score: 0.7500 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.1473;  Loss pred: 0.1224; Loss self: 2.4957; time: 0.14s
Val loss: 0.5096 score: 0.7907 time: 0.08s
Test loss: 0.5475 score: 0.7500 time: 0.07s
Epoch 30/1000, LR 0.000270
Train loss: 0.1418;  Loss pred: 0.1169; Loss self: 2.4994; time: 0.14s
Val loss: 0.5092 score: 0.7907 time: 0.05s
Test loss: 0.5510 score: 0.7500 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.1373;  Loss pred: 0.1122; Loss self: 2.5049; time: 0.14s
Val loss: 0.5107 score: 0.7907 time: 0.06s
Test loss: 0.5551 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1329;  Loss pred: 0.1078; Loss self: 2.5105; time: 0.18s
Val loss: 0.5147 score: 0.7907 time: 0.05s
Test loss: 0.5616 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1288;  Loss pred: 0.1036; Loss self: 2.5164; time: 0.18s
Val loss: 0.5197 score: 0.7907 time: 0.05s
Test loss: 0.5685 score: 0.7500 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1252;  Loss pred: 0.1000; Loss self: 2.5238; time: 0.15s
Val loss: 0.5245 score: 0.7907 time: 0.06s
Test loss: 0.5749 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1219;  Loss pred: 0.0966; Loss self: 2.5317; time: 0.15s
Val loss: 0.5292 score: 0.7907 time: 0.05s
Test loss: 0.5811 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1186;  Loss pred: 0.0932; Loss self: 2.5399; time: 0.19s
Val loss: 0.5356 score: 0.7907 time: 0.05s
Test loss: 0.5882 score: 0.7727 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1156;  Loss pred: 0.0902; Loss self: 2.5472; time: 0.16s
Val loss: 0.5425 score: 0.7907 time: 0.05s
Test loss: 0.5943 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1128;  Loss pred: 0.0873; Loss self: 2.5523; time: 0.16s
Val loss: 0.5472 score: 0.7907 time: 0.05s
Test loss: 0.6000 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1101;  Loss pred: 0.0845; Loss self: 2.5550; time: 0.14s
Val loss: 0.5499 score: 0.7907 time: 0.06s
Test loss: 0.6050 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1076;  Loss pred: 0.0820; Loss self: 2.5558; time: 0.14s
Val loss: 0.5516 score: 0.7907 time: 0.06s
Test loss: 0.6082 score: 0.7727 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1055;  Loss pred: 0.0799; Loss self: 2.5563; time: 0.14s
Val loss: 0.5526 score: 0.7907 time: 0.08s
Test loss: 0.6111 score: 0.7727 time: 0.07s
     INFO: Early stopping counter 11 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.1035;  Loss pred: 0.0779; Loss self: 2.5566; time: 0.15s
Val loss: 0.5531 score: 0.7907 time: 0.06s
Test loss: 0.6136 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.1016;  Loss pred: 0.0761; Loss self: 2.5568; time: 0.15s
Val loss: 0.5534 score: 0.7907 time: 0.05s
Test loss: 0.6176 score: 0.7727 time: 0.12s
     INFO: Early stopping counter 13 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0997;  Loss pred: 0.0742; Loss self: 2.5571; time: 0.17s
Val loss: 0.5535 score: 0.7907 time: 0.06s
Test loss: 0.6223 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0978;  Loss pred: 0.0722; Loss self: 2.5588; time: 0.15s
Val loss: 0.5530 score: 0.7907 time: 0.05s
Test loss: 0.6268 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0961;  Loss pred: 0.0705; Loss self: 2.5619; time: 0.14s
Val loss: 0.5524 score: 0.7907 time: 0.14s
Test loss: 0.6311 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0945;  Loss pred: 0.0688; Loss self: 2.5645; time: 0.15s
Val loss: 0.5521 score: 0.7907 time: 0.07s
Test loss: 0.6350 score: 0.7727 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0930;  Loss pred: 0.0673; Loss self: 2.5674; time: 0.15s
Val loss: 0.5521 score: 0.7907 time: 0.05s
Test loss: 0.6386 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0916;  Loss pred: 0.0659; Loss self: 2.5705; time: 0.15s
Val loss: 0.5521 score: 0.7907 time: 0.05s
Test loss: 0.6416 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0902;  Loss pred: 0.0645; Loss self: 2.5734; time: 0.16s
Val loss: 0.5521 score: 0.7907 time: 0.06s
Test loss: 0.6440 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 029,   Train_Loss: 0.1418,   Val_Loss: 0.5092,   Val_Precision: 0.8421,   Val_Recall: 0.7273,   Val_accuracy: 0.7805,   Val_Score: 0.7907,   Val_Loss: 0.5092,   Test_Precision: 0.7895,   Test_Recall: 0.6818,   Test_accuracy: 0.7317,   Test_Score: 0.7500,   Test_loss: 0.5510


[0.05025202000979334, 0.04852998408023268, 0.07722773100249469, 0.06249312707222998, 0.048798928037285805, 0.05701630504336208, 0.05826850899029523, 0.05204870004672557, 0.061193574918434024, 0.059933124925009906, 0.07320707605686039, 0.06162248202599585, 0.05847699707373977, 0.06834113097283989, 0.05222530208993703, 0.06729812594130635, 0.057869029929861426, 0.05219875602051616, 0.053110869019292295, 0.06343615404330194, 0.05469589098356664, 0.05227933404967189, 0.05411505710799247, 0.12759433803148568, 0.06256816105451435, 0.06369866395834833, 0.06701812904793769, 0.0550609320634976, 0.07025990693364292, 0.05224888795055449, 0.05339397699572146, 0.052045236923731863, 0.06933916301932186, 0.056827411986887455, 0.05560868000611663, 0.08486140798777342, 0.05283599498216063, 0.051882929052226245, 0.05262732901610434, 0.1181516950018704, 0.07017820503097028, 0.05791969504207373, 0.12226334901060909, 0.05688850302249193, 0.05712041002698243, 0.058423976064659655, 0.07428967906162143, 0.05691064498387277, 0.05328257801011205, 0.05237728205975145]
[0.0011420913638589395, 0.0011029541836416518, 0.0017551757046021521, 0.0014202983425506814, 0.0011090665463019502, 0.0012958251146218654, 0.0013242842952339825, 0.0011829250010619448, 0.001390763066328046, 0.001362116475568407, 0.0016637971831104633, 0.0014005109551362693, 0.0013290226607668128, 0.0015532075221099976, 0.0011869386838622052, 0.001529502862302417, 0.0013152052256786687, 0.00118633536410264, 0.0012070652049839157, 0.0014417307737114077, 0.0012430884314446964, 0.0011881666829470885, 0.0012298876615452834, 0.0028998713188974016, 0.0014220036603298715, 0.0014476969081442803, 0.0015231392965440384, 0.0012513848196249455, 0.0015968160666737028, 0.0011874747261489656, 0.0012134994771754878, 0.0011828462937211787, 0.0015758900686209513, 0.0012915320906110785, 0.0012638336365026507, 0.0019286683633584869, 0.0012008180677763778, 0.0011791574784596874, 0.0011960756594569168, 0.0026852657954970546, 0.0015949592052493244, 0.0013163567055016756, 0.002778712477513843, 0.001292920523238453, 0.0012981911369768734, 0.0013278176378331739, 0.0016884017968550324, 0.001293423749633472, 0.001210967682048001, 0.0011903927740852603]
[875.586692662804, 906.6559743200526, 569.7435290255862, 704.0774251726034, 901.6591505120955, 771.7090745627429, 755.1248652566056, 845.3621312443918, 719.029735697716, 734.1516073966458, 601.034795677742, 714.0251179989523, 752.4326179833721, 643.8289705431779, 842.5035038424045, 653.8072105956461, 760.3376115571489, 842.9319653270334, 828.4556591235062, 693.6107754887743, 804.4479979898267, 841.6327560369173, 813.0823905848094, 344.84288784932124, 703.2330702777681, 690.7523214108695, 656.5387698084953, 799.1146962288639, 626.2462038493144, 842.1231862702841, 824.0629837991987, 845.4183821754618, 634.562029364835, 774.2742184027791, 791.2433813419102, 518.4924578005987, 832.7656177357125, 848.0631453114154, 836.0675113596528, 372.40261343100883, 626.9752835739016, 759.6725080827471, 359.8789036621434, 773.4427461134595, 770.3025937526598, 753.1154666930545, 592.2760813585303, 773.1418263220991, 825.7858692882618, 840.0588627299383]
Elapsed: 0.06300630751531572~0.01699089282723712
Time per graph: 0.0014319615344389935~0.00038615665516448003
Speed: 731.8017035318968~129.68154202712944
Total Time: 0.0528
best val loss: 0.5091570615768433 test_score: 0.7500

Testing...
Test loss: 0.5243 score: 0.7727 time: 0.05s
test Score 0.7727
Epoch Time List: [0.46829244191758335, 0.22363715700339526, 0.25134136399719864, 0.23286378907505423, 0.22325502801686525, 0.23022996704094112, 0.28728948708157986, 0.2583311719354242, 0.2777334910351783, 0.26241942506749183, 0.27644279203377664, 0.2943784698145464, 0.2594052649801597, 0.27051805309019983, 0.26301634206902236, 0.3106766688870266, 0.25222413113806397, 0.24463869910687208, 0.236561132944189, 0.2993236209731549, 0.23920244304463267, 0.23644461808726192, 0.25415298982989043, 0.3479688009247184, 0.3155434438958764, 0.28239104291424155, 0.2744255040306598, 0.24559854704421014, 0.2812638309551403, 0.2381300930865109, 0.24543312401510775, 0.2787253380520269, 0.29518771485891193, 0.2608633489580825, 0.2505665741628036, 0.32001556002069265, 0.25562329194508493, 0.2583820710424334, 0.2546141130151227, 0.31423056800849736, 0.29414757806807756, 0.25587219512090087, 0.3181265629827976, 0.27953519101720303, 0.2567751321475953, 0.3377778581343591, 0.28665792709216475, 0.2577853651018813, 0.25090013607405126, 0.2722125311847776]
Total Epoch List: [50]
Total Time List: [0.052826775005087256]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99682dbee0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8433;  Loss pred: 0.8103; Loss self: 3.2939; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8070 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8345 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8433;  Loss pred: 0.8103; Loss self: 3.2939; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7601 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7770 score: 0.4884 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7772;  Loss pred: 0.7442; Loss self: 3.3011; time: 0.15s
Val loss: 0.7072 score: 0.4773 time: 0.05s
Test loss: 0.7154 score: 0.5349 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6952;  Loss pred: 0.6621; Loss self: 3.3068; time: 0.14s
Val loss: 0.6856 score: 0.5455 time: 0.07s
Test loss: 0.7042 score: 0.5814 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6649;  Loss pred: 0.6319; Loss self: 3.3008; time: 0.15s
Val loss: 0.6793 score: 0.5455 time: 0.06s
Test loss: 0.6807 score: 0.5814 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6555;  Loss pred: 0.6225; Loss self: 3.3053; time: 0.15s
Val loss: 0.6772 score: 0.5909 time: 0.05s
Test loss: 0.6609 score: 0.5349 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.6461;  Loss pred: 0.6130; Loss self: 3.3083; time: 0.15s
Val loss: 0.6710 score: 0.5682 time: 0.05s
Test loss: 0.6414 score: 0.6047 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.6289;  Loss pred: 0.5957; Loss self: 3.3164; time: 0.15s
Val loss: 0.6563 score: 0.6364 time: 0.05s
Test loss: 0.6251 score: 0.6512 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.6027;  Loss pred: 0.5694; Loss self: 3.3296; time: 0.19s
Val loss: 0.6487 score: 0.5682 time: 0.05s
Test loss: 0.6192 score: 0.6977 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.5696;  Loss pred: 0.5362; Loss self: 3.3364; time: 0.16s
Val loss: 0.6435 score: 0.6136 time: 0.05s
Test loss: 0.6119 score: 0.7674 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.5432;  Loss pred: 0.5098; Loss self: 3.3358; time: 0.15s
Val loss: 0.6249 score: 0.6364 time: 0.05s
Test loss: 0.5905 score: 0.7442 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.5123;  Loss pred: 0.4791; Loss self: 3.3191; time: 0.15s
Val loss: 0.6102 score: 0.6591 time: 0.05s
Test loss: 0.5744 score: 0.6977 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.4815;  Loss pred: 0.4485; Loss self: 3.3027; time: 0.20s
Val loss: 0.5987 score: 0.6364 time: 0.04s
Test loss: 0.5654 score: 0.7674 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.4452;  Loss pred: 0.4122; Loss self: 3.2944; time: 0.24s
Val loss: 0.5939 score: 0.6364 time: 0.17s
Test loss: 0.5636 score: 0.7907 time: 0.06s
Epoch 15/1000, LR 0.000270
Train loss: 0.4135;  Loss pred: 0.3806; Loss self: 3.2887; time: 0.13s
Val loss: 0.5848 score: 0.6591 time: 0.04s
Test loss: 0.5538 score: 0.7907 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.3846;  Loss pred: 0.3519; Loss self: 3.2702; time: 0.15s
Val loss: 0.5658 score: 0.6591 time: 0.06s
Test loss: 0.5317 score: 0.8140 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.3547;  Loss pred: 0.3224; Loss self: 3.2338; time: 0.15s
Val loss: 0.5500 score: 0.7045 time: 0.06s
Test loss: 0.5151 score: 0.8140 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.3297;  Loss pred: 0.2977; Loss self: 3.1930; time: 0.14s
Val loss: 0.5411 score: 0.7045 time: 0.06s
Test loss: 0.5060 score: 0.8372 time: 0.11s
Epoch 19/1000, LR 0.000270
Train loss: 0.3044;  Loss pred: 0.2729; Loss self: 3.1568; time: 0.20s
Val loss: 0.5393 score: 0.7045 time: 0.06s
Test loss: 0.5052 score: 0.8140 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.2807;  Loss pred: 0.2494; Loss self: 3.1239; time: 0.15s
Val loss: 0.5416 score: 0.7045 time: 0.07s
Test loss: 0.5064 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2596;  Loss pred: 0.2287; Loss self: 3.0885; time: 0.20s
Val loss: 0.5399 score: 0.7273 time: 0.05s
Test loss: 0.5011 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.2402;  Loss pred: 0.2097; Loss self: 3.0477; time: 0.15s
Val loss: 0.5303 score: 0.7273 time: 0.04s
Test loss: 0.4879 score: 0.7907 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.2210;  Loss pred: 0.1910; Loss self: 3.0034; time: 0.14s
Val loss: 0.5153 score: 0.7500 time: 0.04s
Test loss: 0.4712 score: 0.8140 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.2042;  Loss pred: 0.1747; Loss self: 2.9592; time: 0.14s
Val loss: 0.5031 score: 0.7727 time: 0.06s
Test loss: 0.4578 score: 0.8140 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.1909;  Loss pred: 0.1617; Loss self: 2.9193; time: 0.15s
Val loss: 0.4982 score: 0.7955 time: 0.05s
Test loss: 0.4519 score: 0.8140 time: 0.05s
Epoch 26/1000, LR 0.000270
Train loss: 0.1785;  Loss pred: 0.1496; Loss self: 2.8856; time: 0.14s
Val loss: 0.5036 score: 0.7955 time: 0.05s
Test loss: 0.4550 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1659;  Loss pred: 0.1373; Loss self: 2.8591; time: 0.15s
Val loss: 0.5165 score: 0.7727 time: 0.05s
Test loss: 0.4659 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1550;  Loss pred: 0.1267; Loss self: 2.8376; time: 0.17s
Val loss: 0.5285 score: 0.7727 time: 0.05s
Test loss: 0.4771 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1459;  Loss pred: 0.1178; Loss self: 2.8179; time: 0.15s
Val loss: 0.5374 score: 0.7727 time: 0.05s
Test loss: 0.4830 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1376;  Loss pred: 0.1096; Loss self: 2.7998; time: 0.15s
Val loss: 0.5424 score: 0.7727 time: 0.08s
Test loss: 0.4825 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1301;  Loss pred: 0.1023; Loss self: 2.7828; time: 0.15s
Val loss: 0.5464 score: 0.7727 time: 0.05s
Test loss: 0.4813 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1241;  Loss pred: 0.0965; Loss self: 2.7681; time: 0.14s
Val loss: 0.5513 score: 0.7727 time: 0.10s
Test loss: 0.4829 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1192;  Loss pred: 0.0916; Loss self: 2.7562; time: 0.27s
Val loss: 0.5609 score: 0.7500 time: 0.18s
Test loss: 0.4891 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1150;  Loss pred: 0.0875; Loss self: 2.7484; time: 0.17s
Val loss: 0.5743 score: 0.7500 time: 0.07s
Test loss: 0.4997 score: 0.8140 time: 0.10s
     INFO: Early stopping counter 9 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1110;  Loss pred: 0.0836; Loss self: 2.7443; time: 0.17s
Val loss: 0.5904 score: 0.7500 time: 0.19s
Test loss: 0.5142 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1073;  Loss pred: 0.0799; Loss self: 2.7429; time: 0.20s
Val loss: 0.6089 score: 0.7500 time: 0.14s
Test loss: 0.5300 score: 0.7907 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1038;  Loss pred: 0.0763; Loss self: 2.7436; time: 0.16s
Val loss: 0.6246 score: 0.7500 time: 0.07s
Test loss: 0.5428 score: 0.7907 time: 0.07s
     INFO: Early stopping counter 12 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1005;  Loss pred: 0.0731; Loss self: 2.7451; time: 0.18s
Val loss: 0.6353 score: 0.7727 time: 0.08s
Test loss: 0.5508 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0974;  Loss pred: 0.0699; Loss self: 2.7472; time: 0.15s
Val loss: 0.6407 score: 0.7727 time: 0.10s
Test loss: 0.5540 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0943;  Loss pred: 0.0669; Loss self: 2.7488; time: 0.28s
Val loss: 0.6416 score: 0.7727 time: 0.10s
Test loss: 0.5533 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 15 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0915;  Loss pred: 0.0640; Loss self: 2.7501; time: 0.17s
Val loss: 0.6404 score: 0.7727 time: 0.06s
Test loss: 0.5512 score: 0.7907 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0889;  Loss pred: 0.0614; Loss self: 2.7511; time: 0.24s
Val loss: 0.6398 score: 0.7727 time: 0.07s
Test loss: 0.5497 score: 0.7907 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0865;  Loss pred: 0.0590; Loss self: 2.7522; time: 0.22s
Val loss: 0.6412 score: 0.7727 time: 0.10s
Test loss: 0.5501 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 18 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0843;  Loss pred: 0.0567; Loss self: 2.7537; time: 0.33s
Val loss: 0.6455 score: 0.7727 time: 0.06s
Test loss: 0.5529 score: 0.7907 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0821;  Loss pred: 0.0545; Loss self: 2.7559; time: 0.19s
Val loss: 0.6536 score: 0.7727 time: 0.19s
Test loss: 0.5589 score: 0.7907 time: 0.12s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 024,   Train_Loss: 0.1909,   Val_Loss: 0.4982,   Val_Precision: 0.8824,   Val_Recall: 0.6818,   Val_accuracy: 0.7692,   Val_Score: 0.7955,   Val_Loss: 0.4982,   Test_Precision: 1.0000,   Test_Recall: 0.6364,   Test_accuracy: 0.7778,   Test_Score: 0.8140,   Test_loss: 0.4519


[0.05025202000979334, 0.04852998408023268, 0.07722773100249469, 0.06249312707222998, 0.048798928037285805, 0.05701630504336208, 0.05826850899029523, 0.05204870004672557, 0.061193574918434024, 0.059933124925009906, 0.07320707605686039, 0.06162248202599585, 0.05847699707373977, 0.06834113097283989, 0.05222530208993703, 0.06729812594130635, 0.057869029929861426, 0.05219875602051616, 0.053110869019292295, 0.06343615404330194, 0.05469589098356664, 0.05227933404967189, 0.05411505710799247, 0.12759433803148568, 0.06256816105451435, 0.06369866395834833, 0.06701812904793769, 0.0550609320634976, 0.07025990693364292, 0.05224888795055449, 0.05339397699572146, 0.052045236923731863, 0.06933916301932186, 0.056827411986887455, 0.05560868000611663, 0.08486140798777342, 0.05283599498216063, 0.051882929052226245, 0.05262732901610434, 0.1181516950018704, 0.07017820503097028, 0.05791969504207373, 0.12226334901060909, 0.05688850302249193, 0.05712041002698243, 0.058423976064659655, 0.07428967906162143, 0.05691064498387277, 0.05328257801011205, 0.05237728205975145, 0.05406885698903352, 0.05180131597444415, 0.052326270029880106, 0.052668404998257756, 0.06616180296987295, 0.054010796011425555, 0.053352706017903984, 0.05346375599037856, 0.05350702290888876, 0.05331641505472362, 0.05728522303979844, 0.0852060440229252, 0.06096382800024003, 0.05972102703526616, 0.048491955967620015, 0.04821320902556181, 0.06190972891636193, 0.10970463999547064, 0.08565233997069299, 0.05094276904128492, 0.05126797198317945, 0.05175239301752299, 0.05199599906336516, 0.050801039091311395, 0.055533624021336436, 0.053112462046556175, 0.05493934906553477, 0.052514947950839996, 0.04992786701768637, 0.050386841990984976, 0.05020967498421669, 0.05884660710580647, 0.11613305995706469, 0.1025956820230931, 0.11314241401851177, 0.09721194102894515, 0.07908154197502881, 0.054263577912934124, 0.10685807606205344, 0.102815443999134, 0.10720715299248695, 0.08060866198502481, 0.12119106203317642, 0.07261551404371858, 0.11945987201761454]
[0.0011420913638589395, 0.0011029541836416518, 0.0017551757046021521, 0.0014202983425506814, 0.0011090665463019502, 0.0012958251146218654, 0.0013242842952339825, 0.0011829250010619448, 0.001390763066328046, 0.001362116475568407, 0.0016637971831104633, 0.0014005109551362693, 0.0013290226607668128, 0.0015532075221099976, 0.0011869386838622052, 0.001529502862302417, 0.0013152052256786687, 0.00118633536410264, 0.0012070652049839157, 0.0014417307737114077, 0.0012430884314446964, 0.0011881666829470885, 0.0012298876615452834, 0.0028998713188974016, 0.0014220036603298715, 0.0014476969081442803, 0.0015231392965440384, 0.0012513848196249455, 0.0015968160666737028, 0.0011874747261489656, 0.0012134994771754878, 0.0011828462937211787, 0.0015758900686209513, 0.0012915320906110785, 0.0012638336365026507, 0.0019286683633584869, 0.0012008180677763778, 0.0011791574784596874, 0.0011960756594569168, 0.0026852657954970546, 0.0015949592052493244, 0.0013163567055016756, 0.002778712477513843, 0.001292920523238453, 0.0012981911369768734, 0.0013278176378331739, 0.0016884017968550324, 0.001293423749633472, 0.001210967682048001, 0.0011903927740852603, 0.001257415278814733, 0.0012046817668475384, 0.0012168900006948862, 0.0012248466278664595, 0.0015386465806947198, 0.0012560650235215246, 0.0012407606050675345, 0.0012433431625669432, 0.001244349369974157, 0.001239916629179619, 0.0013322144892976382, 0.0019815359075098884, 0.001417763441866047, 0.0013888610938433991, 0.0011277199062237212, 0.0011212374191991118, 0.0014397611375898123, 0.0025512706975690845, 0.001991914883039372, 0.0011847155590996492, 0.0011922784182134755, 0.0012035440236633254, 0.0012092092805433758, 0.0011814195137514279, 0.001291479628403173, 0.0012351735359664227, 0.0012776592805938318, 0.0012212778593218604, 0.0011611131864578226, 0.0011717870230461623, 0.0011676668600980626, 0.001368525746646662, 0.002700768836210807, 0.002385946093560305, 0.0026312189306630647, 0.0022607428146266314, 0.0018391056273262514, 0.0012619436723938168, 0.0024850715363268243, 0.002391056837189163, 0.002493189604476441, 0.0018746200461633676, 0.002818396791469219, 0.0016887328847376413, 0.0027781365585491753]
[875.586692662804, 906.6559743200526, 569.7435290255862, 704.0774251726034, 901.6591505120955, 771.7090745627429, 755.1248652566056, 845.3621312443918, 719.029735697716, 734.1516073966458, 601.034795677742, 714.0251179989523, 752.4326179833721, 643.8289705431779, 842.5035038424045, 653.8072105956461, 760.3376115571489, 842.9319653270334, 828.4556591235062, 693.6107754887743, 804.4479979898267, 841.6327560369173, 813.0823905848094, 344.84288784932124, 703.2330702777681, 690.7523214108695, 656.5387698084953, 799.1146962288639, 626.2462038493144, 842.1231862702841, 824.0629837991987, 845.4183821754618, 634.562029364835, 774.2742184027791, 791.2433813419102, 518.4924578005987, 832.7656177357125, 848.0631453114154, 836.0675113596528, 372.40261343100883, 626.9752835739016, 759.6725080827471, 359.8789036621434, 773.4427461134595, 770.3025937526598, 753.1154666930545, 592.2760813585303, 773.1418263220991, 825.7858692882618, 840.0588627299383, 795.2822085497654, 830.0947416319265, 821.7669628552832, 816.4287489135548, 649.9218290586825, 796.1371276754317, 805.9572458343566, 804.2831859351289, 803.6328254184499, 806.5058379462513, 750.6298783217812, 504.6590355542219, 705.3362856386036, 720.0144092399458, 886.7450104242607, 891.871768527213, 694.5596556898453, 391.9615433018634, 502.02948354607685, 844.0844659455406, 838.7302703158983, 830.879452964436, 826.9867061809477, 846.4393793739226, 774.3056708036752, 809.6028378859181, 782.6812791084794, 818.8144838352106, 861.2424797712217, 853.3974010058697, 856.408650594073, 730.7133259643297, 370.2649358924792, 419.1209527738331, 380.0519935252978, 442.332490688532, 543.7425589599391, 792.4283958752862, 402.4029028468519, 418.2251063406601, 401.09263980747096, 533.4414309964404, 354.81164434575726, 592.1599614940633, 359.9535080169815]
Elapsed: 0.0659742764748731~0.020650700564319113
Time per graph: 0.0015167584304506718~0.00047920993519650734
Speed: 707.9180830102168~158.00326770070052
Total Time: 0.1200
best val loss: 0.4981519877910614 test_score: 0.8140

Testing...
Test loss: 0.4519 score: 0.8140 time: 0.07s
test Score 0.8140
Epoch Time List: [0.46829244191758335, 0.22363715700339526, 0.25134136399719864, 0.23286378907505423, 0.22325502801686525, 0.23022996704094112, 0.28728948708157986, 0.2583311719354242, 0.2777334910351783, 0.26241942506749183, 0.27644279203377664, 0.2943784698145464, 0.2594052649801597, 0.27051805309019983, 0.26301634206902236, 0.3106766688870266, 0.25222413113806397, 0.24463869910687208, 0.236561132944189, 0.2993236209731549, 0.23920244304463267, 0.23644461808726192, 0.25415298982989043, 0.3479688009247184, 0.3155434438958764, 0.28239104291424155, 0.2744255040306598, 0.24559854704421014, 0.2812638309551403, 0.2381300930865109, 0.24543312401510775, 0.2787253380520269, 0.29518771485891193, 0.2608633489580825, 0.2505665741628036, 0.32001556002069265, 0.25562329194508493, 0.2583820710424334, 0.2546141130151227, 0.31423056800849736, 0.29414757806807756, 0.25587219512090087, 0.3181265629827976, 0.27953519101720303, 0.2567751321475953, 0.3377778581343591, 0.28665792709216475, 0.2577853651018813, 0.25090013607405126, 0.2722125311847776, 0.2721132430015132, 0.3326338209444657, 0.24276719987392426, 0.25838627095799893, 0.27085979701951146, 0.24555044306907803, 0.24680185702163726, 0.25027933495584875, 0.2966559389606118, 0.26511760288849473, 0.2529120050603524, 0.2782466651406139, 0.29534315993078053, 0.4682412699330598, 0.2205795970512554, 0.2514534330694005, 0.2665820689871907, 0.31232418201398104, 0.3442503570113331, 0.2697259009582922, 0.29262538510374725, 0.2401811140589416, 0.23318086005747318, 0.24545618297997862, 0.25115139700938016, 0.2412274138769135, 0.24697151395957917, 0.26385060395114124, 0.24338444496970624, 0.2707024160772562, 0.2467939400812611, 0.295997632201761, 0.5647948301630095, 0.3325694919330999, 0.46343348105438054, 0.43511409300845116, 0.30871063307859004, 0.3117544399574399, 0.3439911649329588, 0.48079038108699024, 0.33071801089681685, 0.38269949913956225, 0.43193536915350705, 0.46353559801355004, 0.4934370219707489]
Total Epoch List: [50, 45]
Total Time List: [0.052826775005087256, 0.11995680397376418]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99682db940>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8367;  Loss pred: 0.8032; Loss self: 3.3558; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7982 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8604 score: 0.4884 time: 0.12s
Epoch 2/1000, LR 0.000000
Train loss: 0.8367;  Loss pred: 0.8032; Loss self: 3.3558; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7518 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8082 score: 0.4884 time: 0.12s
Epoch 3/1000, LR 0.000030
Train loss: 0.7870;  Loss pred: 0.7531; Loss self: 3.3892; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7139 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7474 score: 0.4884 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.7384;  Loss pred: 0.7041; Loss self: 3.4296; time: 0.15s
Val loss: 0.7011 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7149 score: 0.4884 time: 0.10s
Epoch 5/1000, LR 0.000090
Train loss: 0.7268;  Loss pred: 0.6921; Loss self: 3.4656; time: 0.23s
Val loss: 0.6912 score: 0.5227 time: 0.06s
Test loss: 0.6999 score: 0.4884 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.7193;  Loss pred: 0.6844; Loss self: 3.4863; time: 0.22s
Val loss: 0.6802 score: 0.5682 time: 0.07s
Test loss: 0.6887 score: 0.4884 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.7043;  Loss pred: 0.6693; Loss self: 3.4950; time: 0.19s
Val loss: 0.6731 score: 0.5909 time: 0.07s
Test loss: 0.6771 score: 0.5814 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.6931;  Loss pred: 0.6581; Loss self: 3.5020; time: 0.15s
Val loss: 0.6699 score: 0.5455 time: 0.05s
Test loss: 0.6691 score: 0.5116 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.6830;  Loss pred: 0.6480; Loss self: 3.5047; time: 0.16s
Val loss: 0.6686 score: 0.5000 time: 0.05s
Test loss: 0.6685 score: 0.5116 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6753;  Loss pred: 0.6403; Loss self: 3.5067; time: 0.16s
Val loss: 0.6680 score: 0.5227 time: 0.05s
Test loss: 0.6677 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.6664;  Loss pred: 0.6313; Loss self: 3.5044; time: 0.17s
Val loss: 0.6656 score: 0.5455 time: 0.04s
Test loss: 0.6636 score: 0.5116 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.6543;  Loss pred: 0.6193; Loss self: 3.4954; time: 0.16s
Val loss: 0.6613 score: 0.5455 time: 0.05s
Test loss: 0.6569 score: 0.5116 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6435;  Loss pred: 0.6087; Loss self: 3.4796; time: 0.21s
Val loss: 0.6564 score: 0.5682 time: 0.08s
Test loss: 0.6475 score: 0.5116 time: 0.06s
Epoch 14/1000, LR 0.000270
Train loss: 0.6326;  Loss pred: 0.5979; Loss self: 3.4605; time: 0.17s
Val loss: 0.6492 score: 0.5909 time: 0.08s
Test loss: 0.6361 score: 0.5814 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.6210;  Loss pred: 0.5866; Loss self: 3.4406; time: 0.17s
Val loss: 0.6393 score: 0.6818 time: 0.04s
Test loss: 0.6257 score: 0.6512 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.6068;  Loss pred: 0.5726; Loss self: 3.4170; time: 0.16s
Val loss: 0.6310 score: 0.7273 time: 0.12s
Test loss: 0.6135 score: 0.6744 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.5887;  Loss pred: 0.5548; Loss self: 3.3916; time: 0.16s
Val loss: 0.6228 score: 0.7500 time: 0.08s
Test loss: 0.6008 score: 0.6977 time: 0.06s
Epoch 18/1000, LR 0.000270
Train loss: 0.5672;  Loss pred: 0.5336; Loss self: 3.3622; time: 0.17s
Val loss: 0.6145 score: 0.7500 time: 0.05s
Test loss: 0.5862 score: 0.7442 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.5421;  Loss pred: 0.5089; Loss self: 3.3245; time: 0.19s
Val loss: 0.6013 score: 0.7727 time: 0.04s
Test loss: 0.5699 score: 0.7674 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.5148;  Loss pred: 0.4820; Loss self: 3.2772; time: 0.18s
Val loss: 0.5841 score: 0.7955 time: 0.07s
Test loss: 0.5510 score: 0.8140 time: 0.04s
Epoch 21/1000, LR 0.000270
Train loss: 0.4833;  Loss pred: 0.4511; Loss self: 3.2212; time: 0.17s
Val loss: 0.5646 score: 0.8182 time: 0.07s
Test loss: 0.5325 score: 0.7907 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 0.4496;  Loss pred: 0.4180; Loss self: 3.1562; time: 0.18s
Val loss: 0.5455 score: 0.8182 time: 0.06s
Test loss: 0.5114 score: 0.7907 time: 0.04s
Epoch 23/1000, LR 0.000270
Train loss: 0.4150;  Loss pred: 0.3841; Loss self: 3.0819; time: 0.17s
Val loss: 0.5245 score: 0.8182 time: 0.06s
Test loss: 0.4922 score: 0.7907 time: 0.11s
Epoch 24/1000, LR 0.000270
Train loss: 0.3787;  Loss pred: 0.3487; Loss self: 3.0008; time: 0.16s
Val loss: 0.4998 score: 0.8182 time: 0.06s
Test loss: 0.4769 score: 0.7907 time: 0.06s
Epoch 25/1000, LR 0.000270
Train loss: 0.3455;  Loss pred: 0.3164; Loss self: 2.9111; time: 0.17s
Val loss: 0.4793 score: 0.8409 time: 0.05s
Test loss: 0.4642 score: 0.7907 time: 0.09s
Epoch 26/1000, LR 0.000270
Train loss: 0.3139;  Loss pred: 0.2856; Loss self: 2.8289; time: 0.17s
Val loss: 0.4659 score: 0.8182 time: 0.05s
Test loss: 0.4567 score: 0.7907 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.2877;  Loss pred: 0.2601; Loss self: 2.7585; time: 0.17s
Val loss: 0.4495 score: 0.8182 time: 0.13s
Test loss: 0.4452 score: 0.7674 time: 0.04s
Epoch 28/1000, LR 0.000270
Train loss: 0.2668;  Loss pred: 0.2399; Loss self: 2.6921; time: 0.15s
Val loss: 0.4342 score: 0.8409 time: 0.09s
Test loss: 0.4324 score: 0.8140 time: 0.10s
Epoch 29/1000, LR 0.000270
Train loss: 0.2471;  Loss pred: 0.2207; Loss self: 2.6397; time: 0.22s
Val loss: 0.4207 score: 0.8409 time: 0.11s
Test loss: 0.4244 score: 0.8140 time: 0.10s
Epoch 30/1000, LR 0.000270
Train loss: 0.2259;  Loss pred: 0.1998; Loss self: 2.6060; time: 0.21s
Val loss: 0.4148 score: 0.8182 time: 0.06s
Test loss: 0.4215 score: 0.8140 time: 0.09s
Epoch 31/1000, LR 0.000270
Train loss: 0.2079;  Loss pred: 0.1820; Loss self: 2.5918; time: 0.18s
Val loss: 0.4096 score: 0.8182 time: 0.04s
Test loss: 0.4190 score: 0.8140 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.1929;  Loss pred: 0.1670; Loss self: 2.5887; time: 0.17s
Val loss: 0.4001 score: 0.8409 time: 0.05s
Test loss: 0.4046 score: 0.8140 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.1774;  Loss pred: 0.1517; Loss self: 2.5794; time: 0.17s
Val loss: 0.3920 score: 0.8409 time: 0.06s
Test loss: 0.3973 score: 0.8140 time: 0.04s
Epoch 34/1000, LR 0.000270
Train loss: 0.1648;  Loss pred: 0.1390; Loss self: 2.5718; time: 0.16s
Val loss: 0.3870 score: 0.8409 time: 0.05s
Test loss: 0.3986 score: 0.8140 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.1529;  Loss pred: 0.1272; Loss self: 2.5700; time: 0.16s
Val loss: 0.3851 score: 0.8182 time: 0.05s
Test loss: 0.4071 score: 0.8140 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.1429;  Loss pred: 0.1171; Loss self: 2.5712; time: 0.16s
Val loss: 0.3795 score: 0.8182 time: 0.05s
Test loss: 0.4129 score: 0.8140 time: 0.08s
Epoch 37/1000, LR 0.000270
Train loss: 0.1335;  Loss pred: 0.1078; Loss self: 2.5674; time: 0.16s
Val loss: 0.3708 score: 0.8182 time: 0.05s
Test loss: 0.4130 score: 0.8140 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.1246;  Loss pred: 0.0990; Loss self: 2.5629; time: 0.16s
Val loss: 0.3631 score: 0.8409 time: 0.05s
Test loss: 0.4136 score: 0.8140 time: 0.06s
Epoch 39/1000, LR 0.000269
Train loss: 0.1161;  Loss pred: 0.0905; Loss self: 2.5588; time: 0.17s
Val loss: 0.3584 score: 0.8409 time: 0.05s
Test loss: 0.4166 score: 0.8372 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.1088;  Loss pred: 0.0833; Loss self: 2.5559; time: 0.18s
Val loss: 0.3555 score: 0.8409 time: 0.05s
Test loss: 0.4234 score: 0.8372 time: 0.09s
Epoch 41/1000, LR 0.000269
Train loss: 0.1019;  Loss pred: 0.0763; Loss self: 2.5566; time: 0.19s
Val loss: 0.3518 score: 0.8409 time: 0.14s
Test loss: 0.4317 score: 0.8372 time: 0.06s
Epoch 42/1000, LR 0.000269
Train loss: 0.0955;  Loss pred: 0.0699; Loss self: 2.5585; time: 0.18s
Val loss: 0.3475 score: 0.8182 time: 0.05s
Test loss: 0.4383 score: 0.8140 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0895;  Loss pred: 0.0639; Loss self: 2.5611; time: 0.17s
Val loss: 0.3430 score: 0.8409 time: 0.06s
Test loss: 0.4427 score: 0.8140 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0839;  Loss pred: 0.0583; Loss self: 2.5639; time: 0.18s
Val loss: 0.3386 score: 0.8182 time: 0.04s
Test loss: 0.4502 score: 0.8140 time: 0.10s
Epoch 45/1000, LR 0.000269
Train loss: 0.0786;  Loss pred: 0.0529; Loss self: 2.5666; time: 0.25s
Val loss: 0.3360 score: 0.8182 time: 0.07s
Test loss: 0.4590 score: 0.8140 time: 0.06s
Epoch 46/1000, LR 0.000269
Train loss: 0.0736;  Loss pred: 0.0479; Loss self: 2.5690; time: 0.18s
Val loss: 0.3349 score: 0.8182 time: 0.05s
Test loss: 0.4683 score: 0.8140 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.0695;  Loss pred: 0.0438; Loss self: 2.5716; time: 0.27s
Val loss: 0.3350 score: 0.8182 time: 0.06s
Test loss: 0.4784 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0659;  Loss pred: 0.0401; Loss self: 2.5741; time: 0.15s
Val loss: 0.3354 score: 0.8409 time: 0.05s
Test loss: 0.4891 score: 0.8140 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0626;  Loss pred: 0.0369; Loss self: 2.5763; time: 0.18s
Val loss: 0.3353 score: 0.8409 time: 0.05s
Test loss: 0.5002 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0598;  Loss pred: 0.0340; Loss self: 2.5785; time: 0.16s
Val loss: 0.3343 score: 0.8409 time: 0.06s
Test loss: 0.5107 score: 0.7907 time: 0.08s
Epoch 51/1000, LR 0.000269
Train loss: 0.0572;  Loss pred: 0.0314; Loss self: 2.5812; time: 0.16s
Val loss: 0.3341 score: 0.8636 time: 0.05s
Test loss: 0.5215 score: 0.7674 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0550;  Loss pred: 0.0291; Loss self: 2.5841; time: 0.16s
Val loss: 0.3342 score: 0.8636 time: 0.05s
Test loss: 0.5331 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0531;  Loss pred: 0.0273; Loss self: 2.5866; time: 0.16s
Val loss: 0.3347 score: 0.8636 time: 0.05s
Test loss: 0.5439 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0516;  Loss pred: 0.0257; Loss self: 2.5891; time: 0.17s
Val loss: 0.3352 score: 0.8636 time: 0.05s
Test loss: 0.5529 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 3 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0502;  Loss pred: 0.0243; Loss self: 2.5912; time: 0.16s
Val loss: 0.3353 score: 0.8636 time: 0.05s
Test loss: 0.5596 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0489;  Loss pred: 0.0230; Loss self: 2.5931; time: 0.17s
Val loss: 0.3355 score: 0.8636 time: 0.05s
Test loss: 0.5650 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0477;  Loss pred: 0.0217; Loss self: 2.5951; time: 0.16s
Val loss: 0.3360 score: 0.8864 time: 0.05s
Test loss: 0.5697 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0466;  Loss pred: 0.0206; Loss self: 2.5978; time: 0.15s
Val loss: 0.3368 score: 0.8864 time: 0.05s
Test loss: 0.5739 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0455;  Loss pred: 0.0195; Loss self: 2.6007; time: 0.15s
Val loss: 0.3378 score: 0.8864 time: 0.05s
Test loss: 0.5783 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0446;  Loss pred: 0.0186; Loss self: 2.6034; time: 0.16s
Val loss: 0.3391 score: 0.8864 time: 0.21s
Test loss: 0.5831 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0437;  Loss pred: 0.0177; Loss self: 2.6063; time: 0.15s
Val loss: 0.3407 score: 0.8864 time: 0.05s
Test loss: 0.5883 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0429;  Loss pred: 0.0168; Loss self: 2.6093; time: 0.17s
Val loss: 0.3425 score: 0.8864 time: 0.05s
Test loss: 0.5935 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0421;  Loss pred: 0.0160; Loss self: 2.6122; time: 0.18s
Val loss: 0.3448 score: 0.8864 time: 0.05s
Test loss: 0.5989 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 12 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0414;  Loss pred: 0.0153; Loss self: 2.6151; time: 0.16s
Val loss: 0.3475 score: 0.8864 time: 0.12s
Test loss: 0.6036 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0408;  Loss pred: 0.0146; Loss self: 2.6178; time: 0.16s
Val loss: 0.3501 score: 0.8864 time: 0.05s
Test loss: 0.6076 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0402;  Loss pred: 0.0140; Loss self: 2.6203; time: 0.16s
Val loss: 0.3525 score: 0.8864 time: 0.05s
Test loss: 0.6113 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0397;  Loss pred: 0.0134; Loss self: 2.6225; time: 0.16s
Val loss: 0.3547 score: 0.8864 time: 0.06s
Test loss: 0.6146 score: 0.7674 time: 0.12s
     INFO: Early stopping counter 16 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0392;  Loss pred: 0.0129; Loss self: 2.6245; time: 0.17s
Val loss: 0.3565 score: 0.8864 time: 0.07s
Test loss: 0.6175 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0387;  Loss pred: 0.0124; Loss self: 2.6263; time: 0.17s
Val loss: 0.3582 score: 0.8864 time: 0.05s
Test loss: 0.6204 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0383;  Loss pred: 0.0120; Loss self: 2.6278; time: 0.18s
Val loss: 0.3598 score: 0.8864 time: 0.05s
Test loss: 0.6232 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0378;  Loss pred: 0.0116; Loss self: 2.6292; time: 0.16s
Val loss: 0.3614 score: 0.8864 time: 0.06s
Test loss: 0.6261 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 050,   Train_Loss: 0.0572,   Val_Loss: 0.3341,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3341,   Test_Precision: 0.7895,   Test_Recall: 0.7143,   Test_accuracy: 0.7500,   Test_Score: 0.7674,   Test_loss: 0.5215


[0.05025202000979334, 0.04852998408023268, 0.07722773100249469, 0.06249312707222998, 0.048798928037285805, 0.05701630504336208, 0.05826850899029523, 0.05204870004672557, 0.061193574918434024, 0.059933124925009906, 0.07320707605686039, 0.06162248202599585, 0.05847699707373977, 0.06834113097283989, 0.05222530208993703, 0.06729812594130635, 0.057869029929861426, 0.05219875602051616, 0.053110869019292295, 0.06343615404330194, 0.05469589098356664, 0.05227933404967189, 0.05411505710799247, 0.12759433803148568, 0.06256816105451435, 0.06369866395834833, 0.06701812904793769, 0.0550609320634976, 0.07025990693364292, 0.05224888795055449, 0.05339397699572146, 0.052045236923731863, 0.06933916301932186, 0.056827411986887455, 0.05560868000611663, 0.08486140798777342, 0.05283599498216063, 0.051882929052226245, 0.05262732901610434, 0.1181516950018704, 0.07017820503097028, 0.05791969504207373, 0.12226334901060909, 0.05688850302249193, 0.05712041002698243, 0.058423976064659655, 0.07428967906162143, 0.05691064498387277, 0.05328257801011205, 0.05237728205975145, 0.05406885698903352, 0.05180131597444415, 0.052326270029880106, 0.052668404998257756, 0.06616180296987295, 0.054010796011425555, 0.053352706017903984, 0.05346375599037856, 0.05350702290888876, 0.05331641505472362, 0.05728522303979844, 0.0852060440229252, 0.06096382800024003, 0.05972102703526616, 0.048491955967620015, 0.04821320902556181, 0.06190972891636193, 0.10970463999547064, 0.08565233997069299, 0.05094276904128492, 0.05126797198317945, 0.05175239301752299, 0.05199599906336516, 0.050801039091311395, 0.055533624021336436, 0.053112462046556175, 0.05493934906553477, 0.052514947950839996, 0.04992786701768637, 0.050386841990984976, 0.05020967498421669, 0.05884660710580647, 0.11613305995706469, 0.1025956820230931, 0.11314241401851177, 0.09721194102894515, 0.07908154197502881, 0.054263577912934124, 0.10685807606205344, 0.102815443999134, 0.10720715299248695, 0.08060866198502481, 0.12119106203317642, 0.07261551404371858, 0.11945987201761454, 0.12155393499415368, 0.12108008493669331, 0.04765172896441072, 0.10318452003411949, 0.0820658189477399, 0.06146219396032393, 0.04965269099920988, 0.05023919604718685, 0.08493696607183665, 0.051657231990247965, 0.05626684601884335, 0.08558502094820142, 0.06384752597659826, 0.07998765492811799, 0.04963226499967277, 0.046906532952561975, 0.06442205409985036, 0.04990818502847105, 0.049130736966617405, 0.04934529704041779, 0.046219801995903254, 0.04704578605014831, 0.1103562309872359, 0.061169728985987604, 0.09438206895720214, 0.05104741198010743, 0.049373326008208096, 0.10673880204558372, 0.10548533801920712, 0.0926456640008837, 0.050666772993281484, 0.05318024102598429, 0.04822540108580142, 0.06236314296256751, 0.06319336895830929, 0.08682039298582822, 0.049945548991672695, 0.06762827700003982, 0.05110096209682524, 0.09808625804726034, 0.06895167101174593, 0.04997041402384639, 0.048717636964283884, 0.10732867592014372, 0.06398309604264796, 0.09615292400121689, 0.051662274985574186, 0.0783936750376597, 0.05043649498838931, 0.08260366704780608, 0.05371520493645221, 0.08179883600678295, 0.05263223091606051, 0.04868601995985955, 0.052415069891139865, 0.05447161803022027, 0.048912719008512795, 0.050096085062250495, 0.0496736679924652, 0.05466074706055224, 0.05653881002217531, 0.054029410937801, 0.04821509402245283, 0.05368800100404769, 0.05092431791126728, 0.05535862501710653, 0.12725849403068423, 0.057439067983068526, 0.05084597400855273, 0.08669415605254471, 0.06747541506774724]
[0.0011420913638589395, 0.0011029541836416518, 0.0017551757046021521, 0.0014202983425506814, 0.0011090665463019502, 0.0012958251146218654, 0.0013242842952339825, 0.0011829250010619448, 0.001390763066328046, 0.001362116475568407, 0.0016637971831104633, 0.0014005109551362693, 0.0013290226607668128, 0.0015532075221099976, 0.0011869386838622052, 0.001529502862302417, 0.0013152052256786687, 0.00118633536410264, 0.0012070652049839157, 0.0014417307737114077, 0.0012430884314446964, 0.0011881666829470885, 0.0012298876615452834, 0.0028998713188974016, 0.0014220036603298715, 0.0014476969081442803, 0.0015231392965440384, 0.0012513848196249455, 0.0015968160666737028, 0.0011874747261489656, 0.0012134994771754878, 0.0011828462937211787, 0.0015758900686209513, 0.0012915320906110785, 0.0012638336365026507, 0.0019286683633584869, 0.0012008180677763778, 0.0011791574784596874, 0.0011960756594569168, 0.0026852657954970546, 0.0015949592052493244, 0.0013163567055016756, 0.002778712477513843, 0.001292920523238453, 0.0012981911369768734, 0.0013278176378331739, 0.0016884017968550324, 0.001293423749633472, 0.001210967682048001, 0.0011903927740852603, 0.001257415278814733, 0.0012046817668475384, 0.0012168900006948862, 0.0012248466278664595, 0.0015386465806947198, 0.0012560650235215246, 0.0012407606050675345, 0.0012433431625669432, 0.001244349369974157, 0.001239916629179619, 0.0013322144892976382, 0.0019815359075098884, 0.001417763441866047, 0.0013888610938433991, 0.0011277199062237212, 0.0011212374191991118, 0.0014397611375898123, 0.0025512706975690845, 0.001991914883039372, 0.0011847155590996492, 0.0011922784182134755, 0.0012035440236633254, 0.0012092092805433758, 0.0011814195137514279, 0.001291479628403173, 0.0012351735359664227, 0.0012776592805938318, 0.0012212778593218604, 0.0011611131864578226, 0.0011717870230461623, 0.0011676668600980626, 0.001368525746646662, 0.002700768836210807, 0.002385946093560305, 0.0026312189306630647, 0.0022607428146266314, 0.0018391056273262514, 0.0012619436723938168, 0.0024850715363268243, 0.002391056837189163, 0.002493189604476441, 0.0018746200461633676, 0.002818396791469219, 0.0016887328847376413, 0.0027781365585491753, 0.0028268356975384577, 0.0028158159287603097, 0.0011081797433583889, 0.0023996400007934766, 0.0019085074173893, 0.00142935334791451, 0.0011547137441676716, 0.001168353396446206, 0.001975278280740387, 0.0012013309765173945, 0.001308531302763799, 0.0019903493243767774, 0.0014848261855022853, 0.0018601780215841393, 0.0011542387209226225, 0.0010908496035479528, 0.0014981873046476827, 0.0011606554657783966, 0.001142575278293428, 0.0011475650474515764, 0.0010748791161837967, 0.0010940880476778677, 0.0025664239764473466, 0.0014225518368834325, 0.002194931836214003, 0.0011871491158164519, 0.0011482168839118162, 0.002482297721990319, 0.0024531473957955146, 0.0021545503256019463, 0.0011782970463553833, 0.0012367497913019602, 0.001121520955483754, 0.0014503056502922676, 0.0014696132315885882, 0.002019078906647168, 0.001161524395155179, 0.0015727506279079027, 0.0011883944673680288, 0.0022810757685409384, 0.0016035272328313007, 0.001162102651717358, 0.001132968301494974, 0.00249601571907311, 0.001487978977735999, 0.0022361145116562065, 0.0012014482554784695, 0.0018231087218060396, 0.0011729417439160306, 0.0019210155127396763, 0.0012491908124756328, 0.00190229851178565, 0.001224005370140942, 0.001132233022322315, 0.0012189551137474386, 0.0012667818146562854, 0.0011375050932212279, 0.0011650252340058254, 0.0011552015812201209, 0.0012711801641988893, 0.0013148560470273329, 0.0012564979287860698, 0.0011212812563361123, 0.00124855816288483, 0.0011842864630527276, 0.0012874098841187565, 0.002959499861178703, 0.0013357922786760123, 0.0011824645118268078, 0.002016143164012668, 0.0015691956992499357]
[875.586692662804, 906.6559743200526, 569.7435290255862, 704.0774251726034, 901.6591505120955, 771.7090745627429, 755.1248652566056, 845.3621312443918, 719.029735697716, 734.1516073966458, 601.034795677742, 714.0251179989523, 752.4326179833721, 643.8289705431779, 842.5035038424045, 653.8072105956461, 760.3376115571489, 842.9319653270334, 828.4556591235062, 693.6107754887743, 804.4479979898267, 841.6327560369173, 813.0823905848094, 344.84288784932124, 703.2330702777681, 690.7523214108695, 656.5387698084953, 799.1146962288639, 626.2462038493144, 842.1231862702841, 824.0629837991987, 845.4183821754618, 634.562029364835, 774.2742184027791, 791.2433813419102, 518.4924578005987, 832.7656177357125, 848.0631453114154, 836.0675113596528, 372.40261343100883, 626.9752835739016, 759.6725080827471, 359.8789036621434, 773.4427461134595, 770.3025937526598, 753.1154666930545, 592.2760813585303, 773.1418263220991, 825.7858692882618, 840.0588627299383, 795.2822085497654, 830.0947416319265, 821.7669628552832, 816.4287489135548, 649.9218290586825, 796.1371276754317, 805.9572458343566, 804.2831859351289, 803.6328254184499, 806.5058379462513, 750.6298783217812, 504.6590355542219, 705.3362856386036, 720.0144092399458, 886.7450104242607, 891.871768527213, 694.5596556898453, 391.9615433018634, 502.02948354607685, 844.0844659455406, 838.7302703158983, 830.879452964436, 826.9867061809477, 846.4393793739226, 774.3056708036752, 809.6028378859181, 782.6812791084794, 818.8144838352106, 861.2424797712217, 853.3974010058697, 856.408650594073, 730.7133259643297, 370.2649358924792, 419.1209527738331, 380.0519935252978, 442.332490688532, 543.7425589599391, 792.4283958752862, 402.4029028468519, 418.2251063406601, 401.09263980747096, 533.4414309964404, 354.81164434575726, 592.1599614940633, 359.9535080169815, 353.75243098520957, 355.1368503126054, 902.380688686345, 416.72917590527544, 523.9696691186706, 699.6170691166354, 866.0154995563937, 855.9054161538039, 506.25778137203696, 832.4100681220723, 764.2155735119686, 502.4243672969931, 673.4795020211212, 537.5829562529685, 866.3719054587475, 916.7166553001738, 667.4732838129091, 861.5821227614238, 875.2158557934308, 871.4102980225151, 930.3371746121144, 914.0032213334534, 389.6472325606471, 702.9620812910612, 455.59501370433503, 842.3541631602517, 870.9156031508074, 402.8525632284732, 407.63959055779316, 464.13397177001013, 848.6824295224384, 808.5709874648718, 891.6462907896915, 689.5098283582352, 680.4511408209379, 495.27534397383954, 860.9375783849987, 635.8287081596753, 841.4714368493557, 438.3896465831284, 623.6252054380951, 860.5091800816372, 882.6372270790632, 400.6385025376955, 672.0525054201555, 447.20428886235226, 832.3288126976023, 548.5136393891872, 852.5572605689348, 520.5580034977644, 800.5182154823976, 525.6798519288751, 816.9898796153581, 883.2104171885987, 820.3747527057792, 789.4019225965357, 879.1169428245495, 858.3505067624997, 865.6497846408786, 786.6705508500522, 760.5395299818797, 795.8628319953714, 891.8369002863654, 800.9238413767372, 844.3902984606499, 776.7533963625802, 337.894930531175, 748.619389379285, 845.6913421063982, 495.9965233866283, 637.2691439812082]
Elapsed: 0.06617757452494767~0.021145528887752615
Time per graph: 0.0015289828095291908~0.0004913491894002123
Speed: 706.4724978302671~167.73512237225594
Total Time: 0.0681
best val loss: 0.33414384722709656 test_score: 0.7674

Testing...
Test loss: 0.5697 score: 0.7674 time: 0.04s
test Score 0.7674
Epoch Time List: [0.46829244191758335, 0.22363715700339526, 0.25134136399719864, 0.23286378907505423, 0.22325502801686525, 0.23022996704094112, 0.28728948708157986, 0.2583311719354242, 0.2777334910351783, 0.26241942506749183, 0.27644279203377664, 0.2943784698145464, 0.2594052649801597, 0.27051805309019983, 0.26301634206902236, 0.3106766688870266, 0.25222413113806397, 0.24463869910687208, 0.236561132944189, 0.2993236209731549, 0.23920244304463267, 0.23644461808726192, 0.25415298982989043, 0.3479688009247184, 0.3155434438958764, 0.28239104291424155, 0.2744255040306598, 0.24559854704421014, 0.2812638309551403, 0.2381300930865109, 0.24543312401510775, 0.2787253380520269, 0.29518771485891193, 0.2608633489580825, 0.2505665741628036, 0.32001556002069265, 0.25562329194508493, 0.2583820710424334, 0.2546141130151227, 0.31423056800849736, 0.29414757806807756, 0.25587219512090087, 0.3181265629827976, 0.27953519101720303, 0.2567751321475953, 0.3377778581343591, 0.28665792709216475, 0.2577853651018813, 0.25090013607405126, 0.2722125311847776, 0.2721132430015132, 0.3326338209444657, 0.24276719987392426, 0.25838627095799893, 0.27085979701951146, 0.24555044306907803, 0.24680185702163726, 0.25027933495584875, 0.2966559389606118, 0.26511760288849473, 0.2529120050603524, 0.2782466651406139, 0.29534315993078053, 0.4682412699330598, 0.2205795970512554, 0.2514534330694005, 0.2665820689871907, 0.31232418201398104, 0.3442503570113331, 0.2697259009582922, 0.29262538510374725, 0.2401811140589416, 0.23318086005747318, 0.24545618297997862, 0.25115139700938016, 0.2412274138769135, 0.24697151395957917, 0.26385060395114124, 0.24338444496970624, 0.2707024160772562, 0.2467939400812611, 0.295997632201761, 0.5647948301630095, 0.3325694919330999, 0.46343348105438054, 0.43511409300845116, 0.30871063307859004, 0.3117544399574399, 0.3439911649329588, 0.48079038108699024, 0.33071801089681685, 0.38269949913956225, 0.43193536915350705, 0.46353559801355004, 0.4934370219707489, 0.3433546780142933, 0.42631604499183595, 0.31845569889992476, 0.37552694510668516, 0.36982036498375237, 0.3457576900254935, 0.30989269400015473, 0.24960164004005492, 0.28215567115694284, 0.261275643017143, 0.2642718069255352, 0.2884015969466418, 0.34655500005465, 0.324215542874299, 0.25532494904473424, 0.31999564205762, 0.29560709302313626, 0.26736804004758596, 0.28189974289853126, 0.29783860489260405, 0.2822959099430591, 0.27928303088992834, 0.33282711904030293, 0.2686417909571901, 0.30569987499620765, 0.26628074306063354, 0.3438913639402017, 0.3344003949314356, 0.42789984308183193, 0.34991806908510625, 0.26503119501285255, 0.2654365999624133, 0.26765293709468096, 0.2627221291186288, 0.2690093070268631, 0.2868602750822902, 0.255834223004058, 0.2701443510595709, 0.26836085692048073, 0.32081055908929557, 0.3961929929209873, 0.2785934299463406, 0.2646867538569495, 0.32142955902963877, 0.37698914704378694, 0.3235402428545058, 0.37175230705179274, 0.2731291789095849, 0.2738284869119525, 0.2988217300735414, 0.2557898849481717, 0.29522782494314015, 0.26415994693525136, 0.26750085095409304, 0.2631343131652102, 0.2734590549953282, 0.2471663529286161, 0.24580797483213246, 0.2472126700449735, 0.41430569801013917, 0.2576567310607061, 0.2707042390247807, 0.27055740589275956, 0.33090183895546943, 0.25713267805986106, 0.2581206620670855, 0.3373199920170009, 0.29950185399502516, 0.2652505909791216, 0.3034302219748497, 0.28184893203433603]
Total Epoch List: [50, 45, 71]
Total Time List: [0.052826775005087256, 0.11995680397376418, 0.06810841208789498]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99681e7dc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8667;  Loss pred: 0.8330; Loss self: 3.3759; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8035 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8368 score: 0.5000 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.8667;  Loss pred: 0.8330; Loss self: 3.3759; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7728 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8083 score: 0.5000 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.8378;  Loss pred: 0.8042; Loss self: 3.3585; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7265 score: 0.5116 time: 0.06s
Test loss: 0.7597 score: 0.4773 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.8010;  Loss pred: 0.7676; Loss self: 3.3321; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7081 score: 0.5000 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.7598;  Loss pred: 0.7269; Loss self: 3.2824; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6830 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.7231;  Loss pred: 0.6907; Loss self: 3.2427; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7068 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6753 score: 0.5000 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.7044;  Loss pred: 0.6721; Loss self: 3.2344; time: 0.14s
Val loss: 0.6979 score: 0.5349 time: 0.05s
Test loss: 0.6599 score: 0.5455 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6782;  Loss pred: 0.6457; Loss self: 3.2523; time: 0.14s
Val loss: 0.6786 score: 0.5814 time: 0.07s
Test loss: 0.6419 score: 0.6818 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.6507;  Loss pred: 0.6179; Loss self: 3.2887; time: 0.14s
Val loss: 0.6636 score: 0.5814 time: 0.05s
Test loss: 0.6290 score: 0.6818 time: 0.06s
Epoch 10/1000, LR 0.000240
Train loss: 0.6224;  Loss pred: 0.5892; Loss self: 3.3178; time: 0.15s
Val loss: 0.6549 score: 0.5814 time: 0.06s
Test loss: 0.6195 score: 0.6364 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.5954;  Loss pred: 0.5619; Loss self: 3.3423; time: 0.15s
Val loss: 0.6514 score: 0.5814 time: 0.05s
Test loss: 0.6133 score: 0.6591 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.5710;  Loss pred: 0.5374; Loss self: 3.3599; time: 0.17s
Val loss: 0.6456 score: 0.5814 time: 0.05s
Test loss: 0.6024 score: 0.6818 time: 0.06s
Epoch 13/1000, LR 0.000270
Train loss: 0.5480;  Loss pred: 0.5143; Loss self: 3.3680; time: 0.15s
Val loss: 0.6317 score: 0.6047 time: 0.05s
Test loss: 0.5898 score: 0.6364 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.5197;  Loss pred: 0.4861; Loss self: 3.3615; time: 0.16s
Val loss: 0.6158 score: 0.6279 time: 0.05s
Test loss: 0.5736 score: 0.7045 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.4922;  Loss pred: 0.4588; Loss self: 3.3405; time: 0.18s
Val loss: 0.6065 score: 0.6512 time: 0.05s
Test loss: 0.5616 score: 0.7273 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.4623;  Loss pred: 0.4292; Loss self: 3.3099; time: 0.13s
Val loss: 0.5965 score: 0.7209 time: 0.05s
Test loss: 0.5522 score: 0.7045 time: 0.06s
Epoch 17/1000, LR 0.000270
Train loss: 0.4312;  Loss pred: 0.3984; Loss self: 3.2718; time: 0.13s
Val loss: 0.5833 score: 0.7209 time: 0.05s
Test loss: 0.5429 score: 0.7273 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.4005;  Loss pred: 0.3683; Loss self: 3.2225; time: 0.15s
Val loss: 0.5575 score: 0.7442 time: 0.07s
Test loss: 0.5288 score: 0.7500 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.3697;  Loss pred: 0.3381; Loss self: 3.1626; time: 0.14s
Val loss: 0.5382 score: 0.7442 time: 0.05s
Test loss: 0.5177 score: 0.7727 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.3393;  Loss pred: 0.3084; Loss self: 3.0976; time: 0.15s
Val loss: 0.5422 score: 0.7442 time: 0.05s
Test loss: 0.5154 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3041;  Loss pred: 0.2737; Loss self: 3.0358; time: 0.16s
Val loss: 0.5255 score: 0.7442 time: 0.05s
Test loss: 0.5010 score: 0.7727 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.2711;  Loss pred: 0.2415; Loss self: 2.9608; time: 0.14s
Val loss: 0.5029 score: 0.7442 time: 0.05s
Test loss: 0.4871 score: 0.8182 time: 0.05s
Epoch 23/1000, LR 0.000270
Train loss: 0.2438;  Loss pred: 0.2151; Loss self: 2.8774; time: 0.14s
Val loss: 0.4972 score: 0.7442 time: 0.06s
Test loss: 0.4843 score: 0.8182 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.2202;  Loss pred: 0.1921; Loss self: 2.8046; time: 0.14s
Val loss: 0.5026 score: 0.7674 time: 0.06s
Test loss: 0.4895 score: 0.7727 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1995;  Loss pred: 0.1720; Loss self: 2.7507; time: 0.15s
Val loss: 0.5086 score: 0.7674 time: 0.06s
Test loss: 0.4941 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1859;  Loss pred: 0.1587; Loss self: 2.7148; time: 0.15s
Val loss: 0.5031 score: 0.7674 time: 0.07s
Test loss: 0.4920 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1727;  Loss pred: 0.1457; Loss self: 2.6981; time: 0.31s
Val loss: 0.4973 score: 0.7442 time: 0.04s
Test loss: 0.4909 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1628;  Loss pred: 0.1358; Loss self: 2.6927; time: 0.16s
Val loss: 0.4953 score: 0.7442 time: 0.05s
Test loss: 0.4938 score: 0.8182 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.1549;  Loss pred: 0.1279; Loss self: 2.6940; time: 0.19s
Val loss: 0.5040 score: 0.7674 time: 0.05s
Test loss: 0.4991 score: 0.8182 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1477;  Loss pred: 0.1207; Loss self: 2.6958; time: 0.16s
Val loss: 0.5128 score: 0.7674 time: 0.10s
Test loss: 0.5055 score: 0.7955 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1412;  Loss pred: 0.1142; Loss self: 2.6965; time: 0.18s
Val loss: 0.5180 score: 0.7674 time: 0.07s
Test loss: 0.5095 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1345;  Loss pred: 0.1075; Loss self: 2.6970; time: 0.14s
Val loss: 0.5109 score: 0.7674 time: 0.05s
Test loss: 0.5093 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1284;  Loss pred: 0.1015; Loss self: 2.6950; time: 0.14s
Val loss: 0.5024 score: 0.7442 time: 0.05s
Test loss: 0.5092 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1238;  Loss pred: 0.0968; Loss self: 2.6912; time: 0.15s
Val loss: 0.4989 score: 0.7442 time: 0.05s
Test loss: 0.5108 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 6 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1199;  Loss pred: 0.0930; Loss self: 2.6893; time: 0.14s
Val loss: 0.4990 score: 0.7674 time: 0.05s
Test loss: 0.5146 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1161;  Loss pred: 0.0892; Loss self: 2.6911; time: 0.14s
Val loss: 0.5045 score: 0.7674 time: 0.05s
Test loss: 0.5166 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1124;  Loss pred: 0.0855; Loss self: 2.6939; time: 0.15s
Val loss: 0.5108 score: 0.7907 time: 0.06s
Test loss: 0.5162 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1092;  Loss pred: 0.0823; Loss self: 2.6961; time: 0.16s
Val loss: 0.5126 score: 0.7907 time: 0.05s
Test loss: 0.5130 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1065;  Loss pred: 0.0795; Loss self: 2.6977; time: 0.15s
Val loss: 0.5087 score: 0.7907 time: 0.05s
Test loss: 0.5098 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1041;  Loss pred: 0.0771; Loss self: 2.6980; time: 0.14s
Val loss: 0.5008 score: 0.7907 time: 0.05s
Test loss: 0.5070 score: 0.7955 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1020;  Loss pred: 0.0750; Loss self: 2.6969; time: 0.15s
Val loss: 0.4919 score: 0.8140 time: 0.08s
Test loss: 0.5070 score: 0.8182 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0999;  Loss pred: 0.0730; Loss self: 2.6940; time: 0.52s
Val loss: 0.4828 score: 0.8140 time: 0.06s
Test loss: 0.5097 score: 0.8409 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0979;  Loss pred: 0.0710; Loss self: 2.6899; time: 0.15s
Val loss: 0.4755 score: 0.8140 time: 0.05s
Test loss: 0.5131 score: 0.8409 time: 0.12s
Epoch 44/1000, LR 0.000269
Train loss: 0.0960;  Loss pred: 0.0692; Loss self: 2.6849; time: 0.29s
Val loss: 0.4707 score: 0.8140 time: 0.05s
Test loss: 0.5169 score: 0.8409 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0943;  Loss pred: 0.0675; Loss self: 2.6793; time: 0.14s
Val loss: 0.4665 score: 0.8140 time: 0.07s
Test loss: 0.5204 score: 0.8409 time: 0.15s
Epoch 46/1000, LR 0.000269
Train loss: 0.0927;  Loss pred: 0.0659; Loss self: 2.6738; time: 0.18s
Val loss: 0.4634 score: 0.8140 time: 0.05s
Test loss: 0.5235 score: 0.8409 time: 0.06s
Epoch 47/1000, LR 0.000269
Train loss: 0.0912;  Loss pred: 0.0645; Loss self: 2.6674; time: 0.14s
Val loss: 0.4632 score: 0.8372 time: 0.05s
Test loss: 0.5258 score: 0.8409 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0897;  Loss pred: 0.0631; Loss self: 2.6612; time: 0.16s
Val loss: 0.4654 score: 0.8372 time: 0.05s
Test loss: 0.5290 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0882;  Loss pred: 0.0616; Loss self: 2.6551; time: 0.15s
Val loss: 0.4695 score: 0.8372 time: 0.06s
Test loss: 0.5337 score: 0.8182 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0867;  Loss pred: 0.0602; Loss self: 2.6492; time: 0.16s
Val loss: 0.4739 score: 0.8140 time: 0.06s
Test loss: 0.5384 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0853;  Loss pred: 0.0589; Loss self: 2.6443; time: 0.16s
Val loss: 0.4774 score: 0.8140 time: 0.06s
Test loss: 0.5429 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0840;  Loss pred: 0.0576; Loss self: 2.6408; time: 0.15s
Val loss: 0.4807 score: 0.8140 time: 0.05s
Test loss: 0.5470 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0825;  Loss pred: 0.0562; Loss self: 2.6381; time: 0.15s
Val loss: 0.4828 score: 0.8140 time: 0.05s
Test loss: 0.5505 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0811;  Loss pred: 0.0548; Loss self: 2.6356; time: 0.14s
Val loss: 0.4824 score: 0.8140 time: 0.06s
Test loss: 0.5528 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0797;  Loss pred: 0.0534; Loss self: 2.6335; time: 0.16s
Val loss: 0.4794 score: 0.8140 time: 0.05s
Test loss: 0.5537 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0784;  Loss pred: 0.0521; Loss self: 2.6316; time: 0.14s
Val loss: 0.4730 score: 0.8140 time: 0.08s
Test loss: 0.5543 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0772;  Loss pred: 0.0509; Loss self: 2.6302; time: 0.23s
Val loss: 0.4645 score: 0.8140 time: 0.06s
Test loss: 0.5558 score: 0.8182 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0760;  Loss pred: 0.0497; Loss self: 2.6296; time: 0.19s
Val loss: 0.4577 score: 0.8372 time: 0.07s
Test loss: 0.5579 score: 0.8409 time: 0.09s
Epoch 59/1000, LR 0.000268
Train loss: 0.0749;  Loss pred: 0.0486; Loss self: 2.6297; time: 0.25s
Val loss: 0.4538 score: 0.8372 time: 0.08s
Test loss: 0.5605 score: 0.8409 time: 0.10s
Epoch 60/1000, LR 0.000268
Train loss: 0.0738;  Loss pred: 0.0475; Loss self: 2.6300; time: 0.19s
Val loss: 0.4531 score: 0.8372 time: 0.07s
Test loss: 0.5634 score: 0.8409 time: 0.10s
Epoch 61/1000, LR 0.000268
Train loss: 0.0726;  Loss pred: 0.0463; Loss self: 2.6303; time: 0.18s
Val loss: 0.4544 score: 0.8372 time: 0.05s
Test loss: 0.5664 score: 0.8409 time: 0.07s
     INFO: Early stopping counter 1 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0715;  Loss pred: 0.0452; Loss self: 2.6308; time: 0.15s
Val loss: 0.4573 score: 0.8372 time: 0.06s
Test loss: 0.5703 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0705;  Loss pred: 0.0442; Loss self: 2.6318; time: 0.14s
Val loss: 0.4607 score: 0.8372 time: 0.06s
Test loss: 0.5753 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0693;  Loss pred: 0.0430; Loss self: 2.6333; time: 0.17s
Val loss: 0.4641 score: 0.8372 time: 0.08s
Test loss: 0.5811 score: 0.8409 time: 0.12s
     INFO: Early stopping counter 4 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0681;  Loss pred: 0.0417; Loss self: 2.6353; time: 0.16s
Val loss: 0.4678 score: 0.8140 time: 0.06s
Test loss: 0.5873 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0668;  Loss pred: 0.0404; Loss self: 2.6376; time: 0.14s
Val loss: 0.4726 score: 0.8140 time: 0.06s
Test loss: 0.5940 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0655;  Loss pred: 0.0391; Loss self: 2.6403; time: 0.15s
Val loss: 0.4792 score: 0.8140 time: 0.05s
Test loss: 0.6012 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0643;  Loss pred: 0.0379; Loss self: 2.6430; time: 0.14s
Val loss: 0.4883 score: 0.8140 time: 0.05s
Test loss: 0.6080 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0631;  Loss pred: 0.0366; Loss self: 2.6450; time: 0.14s
Val loss: 0.4993 score: 0.8140 time: 0.06s
Test loss: 0.6146 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0618;  Loss pred: 0.0353; Loss self: 2.6460; time: 0.14s
Val loss: 0.5113 score: 0.8140 time: 0.05s
Test loss: 0.6206 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0604;  Loss pred: 0.0340; Loss self: 2.6457; time: 0.14s
Val loss: 0.5219 score: 0.7907 time: 0.05s
Test loss: 0.6260 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0592;  Loss pred: 0.0328; Loss self: 2.6455; time: 0.14s
Val loss: 0.5284 score: 0.7907 time: 0.05s
Test loss: 0.6304 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0579;  Loss pred: 0.0314; Loss self: 2.6459; time: 0.14s
Val loss: 0.5300 score: 0.7907 time: 0.05s
Test loss: 0.6338 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0564;  Loss pred: 0.0299; Loss self: 2.6471; time: 0.19s
Val loss: 0.5288 score: 0.7907 time: 0.05s
Test loss: 0.6368 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0551;  Loss pred: 0.0286; Loss self: 2.6488; time: 0.14s
Val loss: 0.5285 score: 0.7907 time: 0.05s
Test loss: 0.6396 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0538;  Loss pred: 0.0273; Loss self: 2.6497; time: 0.14s
Val loss: 0.5296 score: 0.7907 time: 0.05s
Test loss: 0.6419 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0524;  Loss pred: 0.0259; Loss self: 2.6491; time: 0.19s
Val loss: 0.5312 score: 0.7907 time: 0.09s
Test loss: 0.6432 score: 0.8182 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0509;  Loss pred: 0.0244; Loss self: 2.6479; time: 0.14s
Val loss: 0.5313 score: 0.8140 time: 0.05s
Test loss: 0.6438 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0494;  Loss pred: 0.0230; Loss self: 2.6462; time: 0.15s
Val loss: 0.5290 score: 0.8140 time: 0.05s
Test loss: 0.6444 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0479;  Loss pred: 0.0215; Loss self: 2.6454; time: 0.14s
Val loss: 0.5258 score: 0.8140 time: 0.05s
Test loss: 0.6454 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 059,   Train_Loss: 0.0738,   Val_Loss: 0.4531,   Val_Precision: 0.8571,   Val_Recall: 0.8182,   Val_accuracy: 0.8372,   Val_Score: 0.8372,   Val_Loss: 0.4531,   Test_Precision: 0.8947,   Test_Recall: 0.7727,   Test_accuracy: 0.8293,   Test_Score: 0.8409,   Test_loss: 0.5634


[0.05408939393237233, 0.053419443080201745, 0.052653372986242175, 0.05426811205688864, 0.054068861063569784, 0.05229471600614488, 0.05465789302252233, 0.0610549480188638, 0.06308428209740669, 0.053126511978916824, 0.05468843400012702, 0.06161342596169561, 0.05828596593346447, 0.05153659207280725, 0.050625399919226766, 0.06712685595266521, 0.05206565500702709, 0.05312926496844739, 0.05328796501271427, 0.05847860500216484, 0.05575089098419994, 0.05433332803659141, 0.05210984905716032, 0.05113120994064957, 0.05098316597286612, 0.06774164596572518, 0.06642202497459948, 0.06085629598237574, 0.04787648399360478, 0.061413150979205966, 0.051070797024294734, 0.06581383699085563, 0.05134249303955585, 0.06030470004770905, 0.057762717944569886, 0.05717306805308908, 0.05089607392437756, 0.050193737959489226, 0.05695200606714934, 0.05865215591620654, 0.04781213600654155, 0.05332139297388494, 0.12912246398627758, 0.08578926895279437, 0.1594151600729674, 0.06592567800544202, 0.05434604501351714, 0.05505227204412222, 0.1146682930411771, 0.06652754894457757, 0.056191927986219525, 0.0542575700674206, 0.05156796099618077, 0.05067572102416307, 0.05175472004339099, 0.07797310501337051, 0.10160834703128785, 0.09676699398551136, 0.10302402102388442, 0.10748129396233708, 0.07161506195552647, 0.055123389000073075, 0.05101589998230338, 0.125156500027515, 0.06292042299173772, 0.05784528295043856, 0.053323247004300356, 0.05385795293841511, 0.05509430100210011, 0.054140739957802, 0.05415035807527602, 0.05629795102868229, 0.05163829098455608, 0.05663580494001508, 0.05508233094587922, 0.05522335902787745, 0.0647876060102135, 0.05649742798414081, 0.05207063700072467, 0.052364762988872826]
[0.0012293044075539167, 0.001214078251822767, 0.0011966675678691404, 0.0012333661831111056, 0.0012288377514447677, 0.001188516272866929, 0.001242224841420962, 0.0013876124549741771, 0.0014337336840319701, 0.0012074207267935642, 0.0012429189545483414, 0.0014003051354930822, 0.0013246810439423743, 0.0011712861834728922, 0.0011505772708915174, 0.0015256103625605729, 0.0011833103410687975, 0.0012074832947374407, 0.0012110901139253242, 0.0013290592045946555, 0.0012670657041863624, 0.0012348483644679866, 0.0011843147512990981, 0.001162072953196581, 0.001158708317565139, 0.0015395828628573906, 0.0015095914766954427, 0.001383097635963085, 0.001088101908945563, 0.0013957534313455901, 0.0011606999323703349, 0.0014957690225194463, 0.0011668748418080875, 0.00137056136472066, 0.0013127890441947702, 0.001299387910297479, 0.0011567289528267627, 0.0011407667718065734, 0.001294363774253394, 0.0013330035435501486, 0.0010866394546941262, 0.0012118498403155668, 0.0029346014542335815, 0.0019497561125635084, 0.0036230718198401683, 0.001498310863760046, 0.001235137386670844, 0.0012511880010027778, 0.0026060975691176613, 0.0015119897487403994, 0.0012770892724140801, 0.0012331265924413774, 0.001171999113549563, 0.0011517209323673424, 0.0011762436373497951, 0.001772116023031148, 0.0023092806143474513, 0.0021992498633070763, 0.0023414550232701004, 0.0024427566809622062, 0.0016276150444437835, 0.0012528042954562063, 0.0011594522723250768, 0.00284446590971625, 0.0014300096134485846, 0.0013146655216008764, 0.0012118919773704627, 0.0012240443849639798, 0.0012521432045931842, 0.0012304713626773182, 0.001230689956256273, 0.0012794988870155066, 0.0011735975223762746, 0.0012871773850003426, 0.0012518711578608913, 0.0012550763415426693, 0.0014724455911412158, 0.0012840324541850184, 0.001183423568198288, 0.0011901082497471098]
[813.4681644799525, 823.6701369937574, 835.6539667742992, 810.7892154765824, 813.777082307474, 841.3851983598073, 805.0072472034251, 720.6623120276112, 697.4796024794389, 828.21172256634, 804.5576876437494, 714.1300668356653, 754.8987015198064, 853.7623119867901, 869.1289366642507, 655.4753589387054, 845.0868426424579, 828.1688072690425, 825.702388700747, 752.4119290870764, 789.2250549407328, 809.8160298660094, 844.3701295648647, 860.5311716869774, 863.0299660758093, 649.5265854960537, 662.430873145257, 723.0147561518138, 919.0315647631395, 716.4589228599925, 861.5491154185218, 668.5524201561669, 856.9899394269974, 729.6280383650054, 761.7370090206484, 769.5931230967526, 864.5067606860229, 876.6033730245769, 772.5803362944185, 750.1855526480671, 920.2684438524147, 825.1847437959798, 340.7617748424943, 512.8846595511968, 276.0088813376366, 667.418240224513, 809.6265328793689, 799.2404012814537, 383.7154878044597, 661.380145489131, 783.0306162620107, 810.9467479897364, 853.2429661754268, 868.2658896756502, 850.164003652431, 564.2971379997635, 433.035289772515, 454.70049432958507, 427.0848639250775, 409.3735605324793, 614.3958938040765, 798.2092683006422, 862.4762086969536, 351.559846994178, 699.2959981495638, 760.6497497418916, 825.1560524146539, 816.9638391253494, 798.6306968178576, 812.696687084332, 812.5523369362451, 781.5559748805633, 852.0808717925903, 776.8936990760864, 798.8042489202555, 796.7642819008573, 679.1422420063427, 778.7965146369332, 845.0059867596328, 840.2596992437399]
Elapsed: 0.06275573222374078~0.020141281775028094
Time per graph: 0.001426266641448654~0.00045775640397791126
Speed: 744.9413672662611~142.5502683266552
Total Time: 0.0528
best val loss: 0.4530751407146454 test_score: 0.8409

Testing...
Test loss: 0.5258 score: 0.8409 time: 0.11s
test Score 0.8409
Epoch Time List: [0.24228799506090581, 0.23929360217880458, 0.24765392299741507, 0.3317982309963554, 0.2394183329306543, 0.25851948792114854, 0.24442521098535508, 0.26535777817480266, 0.2508795871399343, 0.25552063703071326, 0.25416517083067447, 0.2799532990902662, 0.26041977596469223, 0.2563794059678912, 0.27585022908169776, 0.2465682690963149, 0.23009828000795096, 0.266987832961604, 0.24583989090751857, 0.2549490408273414, 0.258024926064536, 0.24338302086107433, 0.24464340484701097, 0.24005280109122396, 0.25081054703332484, 0.275554375955835, 0.41209831496234983, 0.25980446895118803, 0.28732512204442173, 0.3138433749554679, 0.29295934294350445, 0.25505446398165077, 0.2441829270683229, 0.2594833648763597, 0.24240373901557177, 0.24724392511416227, 0.2594070080667734, 0.2612203120952472, 0.2564186000963673, 0.2453440009849146, 0.2704116489039734, 0.6251569329760969, 0.32568966690450907, 0.4225558158941567, 0.37024785100948066, 0.2883321789558977, 0.2443746409844607, 0.26521265995688736, 0.32048019592184573, 0.275596117018722, 0.26920206495560706, 0.2505432809703052, 0.2454797540558502, 0.24449741595890373, 0.2515813910868019, 0.29115095897577703, 0.38789549784269184, 0.34878779598511755, 0.42672368802595884, 0.368529018945992, 0.29179269494488835, 0.25160381209570915, 0.2434811609564349, 0.36464877100661397, 0.2730072069680318, 0.25108764914330095, 0.24412274593487382, 0.24753268097992986, 0.2494685729034245, 0.23909339401870966, 0.24163507611956447, 0.24244843795895576, 0.23737648909445852, 0.29344117490109056, 0.24109184788540006, 0.24757845792919397, 0.34487982804421335, 0.24203483201563358, 0.24554159410763532, 0.23595262283924967]
Total Epoch List: [80]
Total Time List: [0.052797268028371036]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99681e49a0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7209;  Loss pred: 0.6884; Loss self: 3.2510; time: 0.15s
Val loss: 0.6720 score: 0.6136 time: 0.07s
Test loss: 0.6733 score: 0.5814 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7209;  Loss pred: 0.6884; Loss self: 3.2510; time: 0.15s
Val loss: 0.6644 score: 0.5909 time: 0.05s
Test loss: 0.6652 score: 0.6047 time: 0.08s
Epoch 3/1000, LR 0.000030
Train loss: 0.6927;  Loss pred: 0.6601; Loss self: 3.2614; time: 0.19s
Val loss: 0.6633 score: 0.5455 time: 0.06s
Test loss: 0.6638 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6667;  Loss pred: 0.6339; Loss self: 3.2755; time: 0.15s
Val loss: 0.6687 score: 0.5455 time: 0.06s
Test loss: 0.6632 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6509;  Loss pred: 0.6180; Loss self: 3.2969; time: 0.16s
Val loss: 0.6396 score: 0.6818 time: 0.05s
Test loss: 0.6473 score: 0.6977 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.6253;  Loss pred: 0.5921; Loss self: 3.3175; time: 0.15s
Val loss: 0.6190 score: 0.7727 time: 0.05s
Test loss: 0.6309 score: 0.7674 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.6010;  Loss pred: 0.5677; Loss self: 3.3258; time: 0.15s
Val loss: 0.6060 score: 0.7955 time: 0.07s
Test loss: 0.6128 score: 0.7674 time: 0.08s
Epoch 8/1000, LR 0.000180
Train loss: 0.5768;  Loss pred: 0.5438; Loss self: 3.3053; time: 0.18s
Val loss: 0.5975 score: 0.7273 time: 0.05s
Test loss: 0.6030 score: 0.7442 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.5493;  Loss pred: 0.5167; Loss self: 3.2618; time: 0.14s
Val loss: 0.5890 score: 0.6591 time: 0.05s
Test loss: 0.5916 score: 0.6977 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.5138;  Loss pred: 0.4817; Loss self: 3.2089; time: 0.14s
Val loss: 0.5820 score: 0.6364 time: 0.04s
Test loss: 0.5892 score: 0.6977 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.4775;  Loss pred: 0.4462; Loss self: 3.1387; time: 0.15s
Val loss: 0.5573 score: 0.7045 time: 0.06s
Test loss: 0.5621 score: 0.6977 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.4312;  Loss pred: 0.4007; Loss self: 3.0484; time: 0.15s
Val loss: 0.5283 score: 0.7273 time: 0.06s
Test loss: 0.5381 score: 0.7209 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.3907;  Loss pred: 0.3614; Loss self: 2.9291; time: 0.23s
Val loss: 0.4905 score: 0.7273 time: 0.05s
Test loss: 0.5404 score: 0.7209 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.3435;  Loss pred: 0.3154; Loss self: 2.8131; time: 0.21s
Val loss: 0.4617 score: 0.7955 time: 0.04s
Test loss: 0.5397 score: 0.6977 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.3068;  Loss pred: 0.2798; Loss self: 2.7022; time: 0.17s
Val loss: 0.4486 score: 0.7727 time: 0.05s
Test loss: 0.5230 score: 0.7209 time: 0.09s
Epoch 16/1000, LR 0.000270
Train loss: 0.2822;  Loss pred: 0.2560; Loss self: 2.6180; time: 0.19s
Val loss: 0.4447 score: 0.7955 time: 0.08s
Test loss: 0.5148 score: 0.7209 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.2587;  Loss pred: 0.2332; Loss self: 2.5490; time: 0.19s
Val loss: 0.4440 score: 0.7727 time: 0.18s
Test loss: 0.5224 score: 0.6977 time: 0.05s
Epoch 18/1000, LR 0.000270
Train loss: 0.2346;  Loss pred: 0.2097; Loss self: 2.4948; time: 0.17s
Val loss: 0.4391 score: 0.7727 time: 0.05s
Test loss: 0.5118 score: 0.6744 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.2121;  Loss pred: 0.1876; Loss self: 2.4483; time: 0.18s
Val loss: 0.4247 score: 0.7955 time: 0.18s
Test loss: 0.4971 score: 0.6977 time: 0.10s
Epoch 20/1000, LR 0.000270
Train loss: 0.1914;  Loss pred: 0.1673; Loss self: 2.4118; time: 0.16s
Val loss: 0.4243 score: 0.7955 time: 0.05s
Test loss: 0.4933 score: 0.6977 time: 0.09s
Epoch 21/1000, LR 0.000270
Train loss: 0.1757;  Loss pred: 0.1518; Loss self: 2.3917; time: 0.20s
Val loss: 0.4182 score: 0.8182 time: 0.13s
Test loss: 0.4982 score: 0.7209 time: 0.14s
Epoch 22/1000, LR 0.000270
Train loss: 0.1642;  Loss pred: 0.1402; Loss self: 2.3939; time: 0.16s
Val loss: 0.4071 score: 0.8182 time: 0.07s
Test loss: 0.4980 score: 0.7442 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.1540;  Loss pred: 0.1299; Loss self: 2.4091; time: 0.28s
Val loss: 0.4038 score: 0.8182 time: 0.18s
Test loss: 0.5011 score: 0.7442 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.1461;  Loss pred: 0.1218; Loss self: 2.4294; time: 0.31s
Val loss: 0.4108 score: 0.8182 time: 0.11s
Test loss: 0.5185 score: 0.7442 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1378;  Loss pred: 0.1133; Loss self: 2.4479; time: 0.17s
Val loss: 0.4272 score: 0.8182 time: 0.27s
Test loss: 0.5411 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1299;  Loss pred: 0.1053; Loss self: 2.4644; time: 0.16s
Val loss: 0.4442 score: 0.8182 time: 0.07s
Test loss: 0.5633 score: 0.7442 time: 0.10s
     INFO: Early stopping counter 3 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1223;  Loss pred: 0.0975; Loss self: 2.4807; time: 0.20s
Val loss: 0.4592 score: 0.7955 time: 0.09s
Test loss: 0.5787 score: 0.7442 time: 0.16s
     INFO: Early stopping counter 4 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1159;  Loss pred: 0.0909; Loss self: 2.4960; time: 0.24s
Val loss: 0.4705 score: 0.7955 time: 0.04s
Test loss: 0.5856 score: 0.7442 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1105;  Loss pred: 0.0854; Loss self: 2.5088; time: 0.31s
Val loss: 0.4823 score: 0.7955 time: 0.14s
Test loss: 0.5935 score: 0.7442 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1064;  Loss pred: 0.0812; Loss self: 2.5180; time: 0.17s
Val loss: 0.4949 score: 0.7500 time: 0.10s
Test loss: 0.6028 score: 0.7209 time: 0.07s
     INFO: Early stopping counter 7 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1028;  Loss pred: 0.0776; Loss self: 2.5261; time: 0.18s
Val loss: 0.5059 score: 0.7727 time: 0.05s
Test loss: 0.6082 score: 0.7209 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0997;  Loss pred: 0.0743; Loss self: 2.5336; time: 0.21s
Val loss: 0.5114 score: 0.7727 time: 0.15s
Test loss: 0.6091 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0965;  Loss pred: 0.0711; Loss self: 2.5393; time: 0.27s
Val loss: 0.5120 score: 0.7727 time: 0.07s
Test loss: 0.6053 score: 0.7442 time: 0.11s
     INFO: Early stopping counter 10 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0934;  Loss pred: 0.0679; Loss self: 2.5441; time: 0.16s
Val loss: 0.5114 score: 0.7727 time: 0.14s
Test loss: 0.6014 score: 0.7442 time: 0.19s
     INFO: Early stopping counter 11 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0902;  Loss pred: 0.0647; Loss self: 2.5482; time: 0.26s
Val loss: 0.5126 score: 0.7955 time: 0.11s
Test loss: 0.5989 score: 0.7674 time: 0.14s
     INFO: Early stopping counter 12 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0871;  Loss pred: 0.0616; Loss self: 2.5523; time: 0.27s
Val loss: 0.5162 score: 0.7955 time: 0.11s
Test loss: 0.5980 score: 0.7674 time: 0.15s
     INFO: Early stopping counter 13 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0844;  Loss pred: 0.0588; Loss self: 2.5562; time: 0.27s
Val loss: 0.5231 score: 0.7955 time: 0.15s
Test loss: 0.5997 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0820;  Loss pred: 0.0564; Loss self: 2.5607; time: 0.15s
Val loss: 0.5355 score: 0.7955 time: 0.06s
Test loss: 0.6065 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0797;  Loss pred: 0.0541; Loss self: 2.5663; time: 0.17s
Val loss: 0.5515 score: 0.7727 time: 0.11s
Test loss: 0.6166 score: 0.7674 time: 0.09s
     INFO: Early stopping counter 16 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0775;  Loss pred: 0.0518; Loss self: 2.5728; time: 0.21s
Val loss: 0.5670 score: 0.7727 time: 0.06s
Test loss: 0.6290 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0754;  Loss pred: 0.0496; Loss self: 2.5787; time: 0.14s
Val loss: 0.5802 score: 0.7727 time: 0.05s
Test loss: 0.6386 score: 0.8140 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0734;  Loss pred: 0.0475; Loss self: 2.5833; time: 0.17s
Val loss: 0.5902 score: 0.7727 time: 0.09s
Test loss: 0.6445 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 19 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0714;  Loss pred: 0.0455; Loss self: 2.5867; time: 0.17s
Val loss: 0.5949 score: 0.7727 time: 0.07s
Test loss: 0.6434 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 022,   Train_Loss: 0.1540,   Val_Loss: 0.4038,   Val_Precision: 0.8889,   Val_Recall: 0.7273,   Val_accuracy: 0.8000,   Val_Score: 0.8182,   Val_Loss: 0.4038,   Test_Precision: 0.8235,   Test_Recall: 0.6364,   Test_accuracy: 0.7179,   Test_Score: 0.7442,   Test_loss: 0.5011


[0.05408939393237233, 0.053419443080201745, 0.052653372986242175, 0.05426811205688864, 0.054068861063569784, 0.05229471600614488, 0.05465789302252233, 0.0610549480188638, 0.06308428209740669, 0.053126511978916824, 0.05468843400012702, 0.06161342596169561, 0.05828596593346447, 0.05153659207280725, 0.050625399919226766, 0.06712685595266521, 0.05206565500702709, 0.05312926496844739, 0.05328796501271427, 0.05847860500216484, 0.05575089098419994, 0.05433332803659141, 0.05210984905716032, 0.05113120994064957, 0.05098316597286612, 0.06774164596572518, 0.06642202497459948, 0.06085629598237574, 0.04787648399360478, 0.061413150979205966, 0.051070797024294734, 0.06581383699085563, 0.05134249303955585, 0.06030470004770905, 0.057762717944569886, 0.05717306805308908, 0.05089607392437756, 0.050193737959489226, 0.05695200606714934, 0.05865215591620654, 0.04781213600654155, 0.05332139297388494, 0.12912246398627758, 0.08578926895279437, 0.1594151600729674, 0.06592567800544202, 0.05434604501351714, 0.05505227204412222, 0.1146682930411771, 0.06652754894457757, 0.056191927986219525, 0.0542575700674206, 0.05156796099618077, 0.05067572102416307, 0.05175472004339099, 0.07797310501337051, 0.10160834703128785, 0.09676699398551136, 0.10302402102388442, 0.10748129396233708, 0.07161506195552647, 0.055123389000073075, 0.05101589998230338, 0.125156500027515, 0.06292042299173772, 0.05784528295043856, 0.053323247004300356, 0.05385795293841511, 0.05509430100210011, 0.054140739957802, 0.05415035807527602, 0.05629795102868229, 0.05163829098455608, 0.05663580494001508, 0.05508233094587922, 0.05522335902787745, 0.0647876060102135, 0.05649742798414081, 0.05207063700072467, 0.052364762988872826, 0.049931479967199266, 0.08429481799248606, 0.05932475603185594, 0.05733953102026135, 0.05935039499308914, 0.05952363403048366, 0.08767884597182274, 0.05013970902655274, 0.05041495501063764, 0.05602945201098919, 0.051147087942808867, 0.05531397694721818, 0.058785585104487836, 0.058358135051093996, 0.09672784991562366, 0.07525110000278801, 0.05558368004858494, 0.05712615908123553, 0.1008974559372291, 0.09750714397523552, 0.14590369595680386, 0.06673213699832559, 0.04989767505321652, 0.10625364596489817, 0.05131660006009042, 0.10659358196426183, 0.16235041001345962, 0.06364170601591468, 0.0744382239645347, 0.07822857704013586, 0.07497192302253097, 0.052607018034905195, 0.11123968404717743, 0.19238451507408172, 0.14190583303570747, 0.16015204600989819, 0.06535686494316906, 0.08108340296894312, 0.09775926801376045, 0.05722445005085319, 0.09767813410144299, 0.11592804093379527, 0.05258288700133562]
[0.0012293044075539167, 0.001214078251822767, 0.0011966675678691404, 0.0012333661831111056, 0.0012288377514447677, 0.001188516272866929, 0.001242224841420962, 0.0013876124549741771, 0.0014337336840319701, 0.0012074207267935642, 0.0012429189545483414, 0.0014003051354930822, 0.0013246810439423743, 0.0011712861834728922, 0.0011505772708915174, 0.0015256103625605729, 0.0011833103410687975, 0.0012074832947374407, 0.0012110901139253242, 0.0013290592045946555, 0.0012670657041863624, 0.0012348483644679866, 0.0011843147512990981, 0.001162072953196581, 0.001158708317565139, 0.0015395828628573906, 0.0015095914766954427, 0.001383097635963085, 0.001088101908945563, 0.0013957534313455901, 0.0011606999323703349, 0.0014957690225194463, 0.0011668748418080875, 0.00137056136472066, 0.0013127890441947702, 0.001299387910297479, 0.0011567289528267627, 0.0011407667718065734, 0.001294363774253394, 0.0013330035435501486, 0.0010866394546941262, 0.0012118498403155668, 0.0029346014542335815, 0.0019497561125635084, 0.0036230718198401683, 0.001498310863760046, 0.001235137386670844, 0.0012511880010027778, 0.0026060975691176613, 0.0015119897487403994, 0.0012770892724140801, 0.0012331265924413774, 0.001171999113549563, 0.0011517209323673424, 0.0011762436373497951, 0.001772116023031148, 0.0023092806143474513, 0.0021992498633070763, 0.0023414550232701004, 0.0024427566809622062, 0.0016276150444437835, 0.0012528042954562063, 0.0011594522723250768, 0.00284446590971625, 0.0014300096134485846, 0.0013146655216008764, 0.0012118919773704627, 0.0012240443849639798, 0.0012521432045931842, 0.0012304713626773182, 0.001230689956256273, 0.0012794988870155066, 0.0011735975223762746, 0.0012871773850003426, 0.0012518711578608913, 0.0012550763415426693, 0.0014724455911412158, 0.0012840324541850184, 0.001183423568198288, 0.0011901082497471098, 0.0011611972085395178, 0.00196034460447642, 0.0013796454891129289, 0.001333477465587473, 0.001380241744025329, 0.0013842705588484573, 0.002039042929577273, 0.001166039744803552, 0.0011724408142008754, 0.0013030105118834695, 0.0011894671614606713, 0.0012863715569120507, 0.0013671066303369264, 0.0013571659314207906, 0.00224948488175869, 0.0017500255814601863, 0.0012926437220601148, 0.0013285153274705938, 0.0023464524636564905, 0.0022676079994240817, 0.003393109208297764, 0.001551910162751758, 0.0011604110477492213, 0.002471015022439492, 0.0011934093037230331, 0.0024789205107967867, 0.0037755909305455726, 0.0014800396747887135, 0.0017311214875473187, 0.0018192692334915315, 0.0017435330935472318, 0.0012234190240675627, 0.002586969396445987, 0.004474058490094923, 0.003300135651993197, 0.003724466186276702, 0.001519927091701606, 0.0018856605341614678, 0.0022734713491572196, 0.00133080116397333, 0.002271584513987046, 0.0026960009519487274, 0.0012228578372403633]
[813.4681644799525, 823.6701369937574, 835.6539667742992, 810.7892154765824, 813.777082307474, 841.3851983598073, 805.0072472034251, 720.6623120276112, 697.4796024794389, 828.21172256634, 804.5576876437494, 714.1300668356653, 754.8987015198064, 853.7623119867901, 869.1289366642507, 655.4753589387054, 845.0868426424579, 828.1688072690425, 825.702388700747, 752.4119290870764, 789.2250549407328, 809.8160298660094, 844.3701295648647, 860.5311716869774, 863.0299660758093, 649.5265854960537, 662.430873145257, 723.0147561518138, 919.0315647631395, 716.4589228599925, 861.5491154185218, 668.5524201561669, 856.9899394269974, 729.6280383650054, 761.7370090206484, 769.5931230967526, 864.5067606860229, 876.6033730245769, 772.5803362944185, 750.1855526480671, 920.2684438524147, 825.1847437959798, 340.7617748424943, 512.8846595511968, 276.0088813376366, 667.418240224513, 809.6265328793689, 799.2404012814537, 383.7154878044597, 661.380145489131, 783.0306162620107, 810.9467479897364, 853.2429661754268, 868.2658896756502, 850.164003652431, 564.2971379997635, 433.035289772515, 454.70049432958507, 427.0848639250775, 409.3735605324793, 614.3958938040765, 798.2092683006422, 862.4762086969536, 351.559846994178, 699.2959981495638, 760.6497497418916, 825.1560524146539, 816.9638391253494, 798.6306968178576, 812.696687084332, 812.5523369362451, 781.5559748805633, 852.0808717925903, 776.8936990760864, 798.8042489202555, 796.7642819008573, 679.1422420063427, 778.7965146369332, 845.0059867596328, 840.2596992437399, 861.1801618587581, 510.11439402874055, 724.8238825779588, 749.9189343701753, 724.5107636605787, 722.4021298494399, 490.4261629289563, 857.6037004368787, 852.9215188415209, 767.4535169747209, 840.7125748406498, 777.3803724333825, 731.4718382673251, 736.8295776133427, 444.54621949634134, 571.4202184208187, 773.6083678233302, 752.7199568739147, 426.1752647832013, 440.9933287649259, 294.7149468559765, 644.3671959895265, 861.7635982866926, 404.69199536179144, 837.9354818839928, 403.40139816688804, 264.85920175030725, 675.6575631276623, 577.6602088261387, 549.671253485008, 573.5480466077602, 817.3814370445621, 386.5526980619923, 223.5107123015693, 303.01784697729795, 268.49485268107276, 657.9262949254156, 530.3181468156934, 439.85599394982563, 751.4270554245176, 440.2213493896457, 370.9197503350949, 817.7565449935792]
Elapsed: 0.06949117600187145~0.02780090333952119
Time per graph: 0.0015945005325986566~0.0006469933488789778
Speed: 694.7006165803891~176.03362799726082
Total Time: 0.0532
best val loss: 0.40376603603363037 test_score: 0.7442

Testing...
Test loss: 0.4982 score: 0.7209 time: 0.11s
test Score 0.7209
Epoch Time List: [0.24228799506090581, 0.23929360217880458, 0.24765392299741507, 0.3317982309963554, 0.2394183329306543, 0.25851948792114854, 0.24442521098535508, 0.26535777817480266, 0.2508795871399343, 0.25552063703071326, 0.25416517083067447, 0.2799532990902662, 0.26041977596469223, 0.2563794059678912, 0.27585022908169776, 0.2465682690963149, 0.23009828000795096, 0.266987832961604, 0.24583989090751857, 0.2549490408273414, 0.258024926064536, 0.24338302086107433, 0.24464340484701097, 0.24005280109122396, 0.25081054703332484, 0.275554375955835, 0.41209831496234983, 0.25980446895118803, 0.28732512204442173, 0.3138433749554679, 0.29295934294350445, 0.25505446398165077, 0.2441829270683229, 0.2594833648763597, 0.24240373901557177, 0.24724392511416227, 0.2594070080667734, 0.2612203120952472, 0.2564186000963673, 0.2453440009849146, 0.2704116489039734, 0.6251569329760969, 0.32568966690450907, 0.4225558158941567, 0.37024785100948066, 0.2883321789558977, 0.2443746409844607, 0.26521265995688736, 0.32048019592184573, 0.275596117018722, 0.26920206495560706, 0.2505432809703052, 0.2454797540558502, 0.24449741595890373, 0.2515813910868019, 0.29115095897577703, 0.38789549784269184, 0.34878779598511755, 0.42672368802595884, 0.368529018945992, 0.29179269494488835, 0.25160381209570915, 0.2434811609564349, 0.36464877100661397, 0.2730072069680318, 0.25108764914330095, 0.24412274593487382, 0.24753268097992986, 0.2494685729034245, 0.23909339401870966, 0.24163507611956447, 0.24244843795895576, 0.23737648909445852, 0.29344117490109056, 0.24109184788540006, 0.24757845792919397, 0.34487982804421335, 0.24203483201563358, 0.24554159410763532, 0.23595262283924967, 0.271982982987538, 0.27497225208207965, 0.3057591039687395, 0.2651183750713244, 0.26245505688712, 0.2627903170650825, 0.30082117894198745, 0.2768241410376504, 0.23056786297820508, 0.24079106107819825, 0.26035572891123593, 0.2596934858011082, 0.3307456821203232, 0.30458932486362755, 0.30588872800581157, 0.33603786991443485, 0.41583874204661697, 0.2755309919593856, 0.45354878809303045, 0.3097088240319863, 0.4785517439013347, 0.29792036197613925, 0.5116928139468655, 0.5216693219263107, 0.49053596588782966, 0.3369740741327405, 0.4492953539593145, 0.34218819707166404, 0.5242264880798757, 0.34129656897857785, 0.291917972965166, 0.40581877587828785, 0.4517860698979348, 0.4818869010778144, 0.507732308935374, 0.5352984110359102, 0.47266816697083414, 0.2847174860071391, 0.3733511620666832, 0.3252995330840349, 0.289676615037024, 0.37128819699864835, 0.2798939769854769]
Total Epoch List: [80, 43]
Total Time List: [0.052797268028371036, 0.05315539694856852]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99681e7490>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7361;  Loss pred: 0.7028; Loss self: 3.3222; time: 0.18s
Val loss: 0.6785 score: 0.6591 time: 0.05s
Test loss: 0.6958 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7361;  Loss pred: 0.7028; Loss self: 3.3222; time: 0.17s
Val loss: 0.6731 score: 0.5227 time: 0.05s
Test loss: 0.6898 score: 0.4651 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7088;  Loss pred: 0.6756; Loss self: 3.3151; time: 0.22s
Val loss: 0.6717 score: 0.5455 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.4884 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6891;  Loss pred: 0.6563; Loss self: 3.2761; time: 0.17s
Val loss: 0.6551 score: 0.5909 time: 0.05s
Test loss: 0.6575 score: 0.5814 time: 0.06s
Epoch 5/1000, LR 0.000090
Train loss: 0.6668;  Loss pred: 0.6348; Loss self: 3.1924; time: 0.16s
Val loss: 0.6158 score: 0.6591 time: 0.05s
Test loss: 0.6178 score: 0.6977 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6336;  Loss pred: 0.6026; Loss self: 3.1006; time: 0.15s
Val loss: 0.6044 score: 0.7955 time: 0.05s
Test loss: 0.6050 score: 0.7442 time: 0.08s
Epoch 7/1000, LR 0.000150
Train loss: 0.6225;  Loss pred: 0.5923; Loss self: 3.0197; time: 0.17s
Val loss: 0.5885 score: 0.7727 time: 0.05s
Test loss: 0.5934 score: 0.7209 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.5719;  Loss pred: 0.5418; Loss self: 3.0109; time: 0.16s
Val loss: 0.5817 score: 0.7500 time: 0.06s
Test loss: 0.5942 score: 0.6512 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.5403;  Loss pred: 0.5102; Loss self: 3.0042; time: 0.16s
Val loss: 0.5612 score: 0.7727 time: 0.05s
Test loss: 0.5529 score: 0.7674 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.4911;  Loss pred: 0.4615; Loss self: 2.9588; time: 0.16s
Val loss: 0.5486 score: 0.7727 time: 0.05s
Test loss: 0.5352 score: 0.7442 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.4550;  Loss pred: 0.4257; Loss self: 2.9295; time: 0.17s
Val loss: 0.5392 score: 0.7955 time: 0.05s
Test loss: 0.5314 score: 0.7674 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.4248;  Loss pred: 0.3959; Loss self: 2.8950; time: 0.16s
Val loss: 0.5118 score: 0.7727 time: 0.05s
Test loss: 0.4957 score: 0.7674 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.3783;  Loss pred: 0.3503; Loss self: 2.8084; time: 0.16s
Val loss: 0.5044 score: 0.7727 time: 0.06s
Test loss: 0.4721 score: 0.8372 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.3463;  Loss pred: 0.3191; Loss self: 2.7245; time: 0.16s
Val loss: 0.4958 score: 0.7955 time: 0.06s
Test loss: 0.4752 score: 0.7907 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.3164;  Loss pred: 0.2892; Loss self: 2.7158; time: 0.16s
Val loss: 0.4889 score: 0.8182 time: 0.06s
Test loss: 0.4590 score: 0.7674 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.2898;  Loss pred: 0.2629; Loss self: 2.6932; time: 0.22s
Val loss: 0.4842 score: 0.7727 time: 0.06s
Test loss: 0.4339 score: 0.8140 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.2704;  Loss pred: 0.2439; Loss self: 2.6554; time: 0.18s
Val loss: 0.4768 score: 0.8182 time: 0.05s
Test loss: 0.4288 score: 0.8140 time: 0.04s
Epoch 18/1000, LR 0.000270
Train loss: 0.2469;  Loss pred: 0.2207; Loss self: 2.6197; time: 0.19s
Val loss: 0.4724 score: 0.8182 time: 0.05s
Test loss: 0.4399 score: 0.8140 time: 0.05s
Epoch 19/1000, LR 0.000270
Train loss: 0.2251;  Loss pred: 0.1992; Loss self: 2.5869; time: 0.20s
Val loss: 0.4717 score: 0.8182 time: 0.05s
Test loss: 0.4403 score: 0.8140 time: 0.05s
Epoch 20/1000, LR 0.000270
Train loss: 0.2067;  Loss pred: 0.1812; Loss self: 2.5470; time: 0.17s
Val loss: 0.4653 score: 0.7955 time: 0.13s
Test loss: 0.4213 score: 0.8605 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.1886;  Loss pred: 0.1636; Loss self: 2.4986; time: 0.15s
Val loss: 0.4662 score: 0.8409 time: 0.06s
Test loss: 0.4027 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.1746;  Loss pred: 0.1499; Loss self: 2.4655; time: 0.16s
Val loss: 0.4660 score: 0.8182 time: 0.05s
Test loss: 0.4003 score: 0.8605 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1595;  Loss pred: 0.1351; Loss self: 2.4440; time: 0.15s
Val loss: 0.4700 score: 0.8409 time: 0.05s
Test loss: 0.4118 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1490;  Loss pred: 0.1247; Loss self: 2.4329; time: 0.21s
Val loss: 0.4785 score: 0.7727 time: 0.05s
Test loss: 0.4234 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1403;  Loss pred: 0.1161; Loss self: 2.4265; time: 0.15s
Val loss: 0.4773 score: 0.7500 time: 0.05s
Test loss: 0.4157 score: 0.8140 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1295;  Loss pred: 0.1053; Loss self: 2.4239; time: 0.16s
Val loss: 0.4704 score: 0.7955 time: 0.05s
Test loss: 0.4025 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1215;  Loss pred: 0.0973; Loss self: 2.4238; time: 0.16s
Val loss: 0.4680 score: 0.7955 time: 0.05s
Test loss: 0.4008 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1145;  Loss pred: 0.0901; Loss self: 2.4318; time: 0.15s
Val loss: 0.4702 score: 0.7727 time: 0.05s
Test loss: 0.4145 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1065;  Loss pred: 0.0820; Loss self: 2.4434; time: 0.16s
Val loss: 0.4781 score: 0.7955 time: 0.09s
Test loss: 0.4376 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1001;  Loss pred: 0.0755; Loss self: 2.4561; time: 0.17s
Val loss: 0.4833 score: 0.7955 time: 0.05s
Test loss: 0.4568 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.0946;  Loss pred: 0.0700; Loss self: 2.4642; time: 0.17s
Val loss: 0.4820 score: 0.7955 time: 0.06s
Test loss: 0.4540 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0885;  Loss pred: 0.0639; Loss self: 2.4661; time: 0.17s
Val loss: 0.4747 score: 0.7727 time: 0.05s
Test loss: 0.4369 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0827;  Loss pred: 0.0580; Loss self: 2.4640; time: 0.16s
Val loss: 0.4692 score: 0.7955 time: 0.05s
Test loss: 0.4209 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0778;  Loss pred: 0.0532; Loss self: 2.4623; time: 0.16s
Val loss: 0.4672 score: 0.7955 time: 0.04s
Test loss: 0.4089 score: 0.8140 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0734;  Loss pred: 0.0488; Loss self: 2.4608; time: 0.16s
Val loss: 0.4657 score: 0.7955 time: 0.05s
Test loss: 0.4015 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0697;  Loss pred: 0.0451; Loss self: 2.4594; time: 0.15s
Val loss: 0.4653 score: 0.7955 time: 0.05s
Test loss: 0.3984 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0659;  Loss pred: 0.0413; Loss self: 2.4573; time: 0.22s
Val loss: 0.4641 score: 0.8182 time: 0.06s
Test loss: 0.4003 score: 0.8372 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.0622;  Loss pred: 0.0376; Loss self: 2.4569; time: 0.16s
Val loss: 0.4605 score: 0.8182 time: 0.05s
Test loss: 0.4039 score: 0.8372 time: 0.05s
Epoch 39/1000, LR 0.000269
Train loss: 0.0586;  Loss pred: 0.0340; Loss self: 2.4582; time: 0.16s
Val loss: 0.4559 score: 0.8182 time: 0.05s
Test loss: 0.4093 score: 0.8372 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.0555;  Loss pred: 0.0309; Loss self: 2.4600; time: 0.17s
Val loss: 0.4520 score: 0.8182 time: 0.09s
Test loss: 0.4156 score: 0.8372 time: 0.11s
Epoch 41/1000, LR 0.000269
Train loss: 0.0529;  Loss pred: 0.0283; Loss self: 2.4630; time: 0.18s
Val loss: 0.4501 score: 0.8182 time: 0.05s
Test loss: 0.4228 score: 0.8372 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.0508;  Loss pred: 0.0261; Loss self: 2.4671; time: 0.15s
Val loss: 0.4493 score: 0.8182 time: 0.06s
Test loss: 0.4318 score: 0.8372 time: 0.04s
Epoch 43/1000, LR 0.000269
Train loss: 0.0489;  Loss pred: 0.0242; Loss self: 2.4711; time: 0.16s
Val loss: 0.4495 score: 0.8182 time: 0.05s
Test loss: 0.4417 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0474;  Loss pred: 0.0226; Loss self: 2.4744; time: 0.20s
Val loss: 0.4504 score: 0.8182 time: 0.07s
Test loss: 0.4521 score: 0.8372 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0459;  Loss pred: 0.0211; Loss self: 2.4770; time: 0.25s
Val loss: 0.4519 score: 0.7955 time: 0.06s
Test loss: 0.4618 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0446;  Loss pred: 0.0198; Loss self: 2.4789; time: 0.20s
Val loss: 0.4544 score: 0.8182 time: 0.05s
Test loss: 0.4717 score: 0.8372 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0433;  Loss pred: 0.0185; Loss self: 2.4800; time: 0.22s
Val loss: 0.4581 score: 0.8182 time: 0.11s
Test loss: 0.4814 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0421;  Loss pred: 0.0173; Loss self: 2.4805; time: 0.16s
Val loss: 0.4622 score: 0.8182 time: 0.05s
Test loss: 0.4907 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0411;  Loss pred: 0.0163; Loss self: 2.4805; time: 0.16s
Val loss: 0.4663 score: 0.8182 time: 0.05s
Test loss: 0.4999 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0401;  Loss pred: 0.0153; Loss self: 2.4804; time: 0.16s
Val loss: 0.4707 score: 0.8182 time: 0.05s
Test loss: 0.5079 score: 0.8372 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0393;  Loss pred: 0.0145; Loss self: 2.4801; time: 0.17s
Val loss: 0.4745 score: 0.8182 time: 0.05s
Test loss: 0.5151 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0386;  Loss pred: 0.0138; Loss self: 2.4799; time: 0.16s
Val loss: 0.4782 score: 0.8182 time: 0.06s
Test loss: 0.5218 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0380;  Loss pred: 0.0132; Loss self: 2.4797; time: 0.18s
Val loss: 0.4809 score: 0.8182 time: 0.05s
Test loss: 0.5275 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0374;  Loss pred: 0.0126; Loss self: 2.4799; time: 0.17s
Val loss: 0.4827 score: 0.8182 time: 0.05s
Test loss: 0.5324 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0369;  Loss pred: 0.0121; Loss self: 2.4802; time: 0.21s
Val loss: 0.4843 score: 0.8182 time: 0.06s
Test loss: 0.5359 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 56/1000, LR 0.000269
Train loss: 0.0364;  Loss pred: 0.0116; Loss self: 2.4807; time: 0.16s
Val loss: 0.4858 score: 0.8182 time: 0.05s
Test loss: 0.5389 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0359;  Loss pred: 0.0111; Loss self: 2.4811; time: 0.15s
Val loss: 0.4874 score: 0.8182 time: 0.06s
Test loss: 0.5414 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0355;  Loss pred: 0.0107; Loss self: 2.4816; time: 0.19s
Val loss: 0.4888 score: 0.8182 time: 0.05s
Test loss: 0.5435 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0351;  Loss pred: 0.0103; Loss self: 2.4822; time: 0.15s
Val loss: 0.4900 score: 0.8182 time: 0.05s
Test loss: 0.5458 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0347;  Loss pred: 0.0099; Loss self: 2.4829; time: 0.18s
Val loss: 0.4916 score: 0.8182 time: 0.06s
Test loss: 0.5485 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 18 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0344;  Loss pred: 0.0096; Loss self: 2.4837; time: 0.15s
Val loss: 0.4932 score: 0.8182 time: 0.05s
Test loss: 0.5518 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0341;  Loss pred: 0.0092; Loss self: 2.4843; time: 0.15s
Val loss: 0.4950 score: 0.8182 time: 0.05s
Test loss: 0.5556 score: 0.8140 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 041,   Train_Loss: 0.0508,   Val_Loss: 0.4493,   Val_Precision: 0.8889,   Val_Recall: 0.7273,   Val_accuracy: 0.8000,   Val_Score: 0.8182,   Val_Loss: 0.4493,   Test_Precision: 0.8500,   Test_Recall: 0.8095,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.4318


[0.05408939393237233, 0.053419443080201745, 0.052653372986242175, 0.05426811205688864, 0.054068861063569784, 0.05229471600614488, 0.05465789302252233, 0.0610549480188638, 0.06308428209740669, 0.053126511978916824, 0.05468843400012702, 0.06161342596169561, 0.05828596593346447, 0.05153659207280725, 0.050625399919226766, 0.06712685595266521, 0.05206565500702709, 0.05312926496844739, 0.05328796501271427, 0.05847860500216484, 0.05575089098419994, 0.05433332803659141, 0.05210984905716032, 0.05113120994064957, 0.05098316597286612, 0.06774164596572518, 0.06642202497459948, 0.06085629598237574, 0.04787648399360478, 0.061413150979205966, 0.051070797024294734, 0.06581383699085563, 0.05134249303955585, 0.06030470004770905, 0.057762717944569886, 0.05717306805308908, 0.05089607392437756, 0.050193737959489226, 0.05695200606714934, 0.05865215591620654, 0.04781213600654155, 0.05332139297388494, 0.12912246398627758, 0.08578926895279437, 0.1594151600729674, 0.06592567800544202, 0.05434604501351714, 0.05505227204412222, 0.1146682930411771, 0.06652754894457757, 0.056191927986219525, 0.0542575700674206, 0.05156796099618077, 0.05067572102416307, 0.05175472004339099, 0.07797310501337051, 0.10160834703128785, 0.09676699398551136, 0.10302402102388442, 0.10748129396233708, 0.07161506195552647, 0.055123389000073075, 0.05101589998230338, 0.125156500027515, 0.06292042299173772, 0.05784528295043856, 0.053323247004300356, 0.05385795293841511, 0.05509430100210011, 0.054140739957802, 0.05415035807527602, 0.05629795102868229, 0.05163829098455608, 0.05663580494001508, 0.05508233094587922, 0.05522335902787745, 0.0647876060102135, 0.05649742798414081, 0.05207063700072467, 0.052364762988872826, 0.049931479967199266, 0.08429481799248606, 0.05932475603185594, 0.05733953102026135, 0.05935039499308914, 0.05952363403048366, 0.08767884597182274, 0.05013970902655274, 0.05041495501063764, 0.05602945201098919, 0.051147087942808867, 0.05531397694721818, 0.058785585104487836, 0.058358135051093996, 0.09672784991562366, 0.07525110000278801, 0.05558368004858494, 0.05712615908123553, 0.1008974559372291, 0.09750714397523552, 0.14590369595680386, 0.06673213699832559, 0.04989767505321652, 0.10625364596489817, 0.05131660006009042, 0.10659358196426183, 0.16235041001345962, 0.06364170601591468, 0.0744382239645347, 0.07822857704013586, 0.07497192302253097, 0.052607018034905195, 0.11123968404717743, 0.19238451507408172, 0.14190583303570747, 0.16015204600989819, 0.06535686494316906, 0.08108340296894312, 0.09775926801376045, 0.05722445005085319, 0.09767813410144299, 0.11592804093379527, 0.05258288700133562, 0.053579150000587106, 0.055668679997324944, 0.050257700961083174, 0.06034808000549674, 0.06281622196547687, 0.08036179596092552, 0.05202210508286953, 0.05525067599955946, 0.055579383042640984, 0.07430809095967561, 0.053274765028618276, 0.05289716203697026, 0.05426034901756793, 0.08720599999651313, 0.04724603798240423, 0.10290208295919001, 0.04871489305514842, 0.0499457330442965, 0.05832310894038528, 0.054194323951378465, 0.04985774599481374, 0.07615085504949093, 0.04976269695907831, 0.049994216999039054, 0.08176276599988341, 0.05133682303130627, 0.050656727980822325, 0.04950424307025969, 0.05669930193107575, 0.05119879497215152, 0.05017215304542333, 0.05158936802763492, 0.049511817982420325, 0.06287752802018076, 0.050631246995180845, 0.05177088198252022, 0.09892774501349777, 0.05090106499847025, 0.05392769805621356, 0.11322306096553802, 0.04950764495879412, 0.04753164597786963, 0.10424959089141339, 0.11009176494553685, 0.05370124406181276, 0.06974876404274255, 0.0528884349623695, 0.04985430801752955, 0.05084032705053687, 0.08325192402116954, 0.04862963396590203, 0.05525801202747971, 0.05615893902722746, 0.053793976083397865, 0.05169630004093051, 0.05103487998712808, 0.05329541303217411, 0.04922474594786763, 0.05945286795031279, 0.04725138295907527, 0.0481154250446707, 0.06770309305284172]
[0.0012293044075539167, 0.001214078251822767, 0.0011966675678691404, 0.0012333661831111056, 0.0012288377514447677, 0.001188516272866929, 0.001242224841420962, 0.0013876124549741771, 0.0014337336840319701, 0.0012074207267935642, 0.0012429189545483414, 0.0014003051354930822, 0.0013246810439423743, 0.0011712861834728922, 0.0011505772708915174, 0.0015256103625605729, 0.0011833103410687975, 0.0012074832947374407, 0.0012110901139253242, 0.0013290592045946555, 0.0012670657041863624, 0.0012348483644679866, 0.0011843147512990981, 0.001162072953196581, 0.001158708317565139, 0.0015395828628573906, 0.0015095914766954427, 0.001383097635963085, 0.001088101908945563, 0.0013957534313455901, 0.0011606999323703349, 0.0014957690225194463, 0.0011668748418080875, 0.00137056136472066, 0.0013127890441947702, 0.001299387910297479, 0.0011567289528267627, 0.0011407667718065734, 0.001294363774253394, 0.0013330035435501486, 0.0010866394546941262, 0.0012118498403155668, 0.0029346014542335815, 0.0019497561125635084, 0.0036230718198401683, 0.001498310863760046, 0.001235137386670844, 0.0012511880010027778, 0.0026060975691176613, 0.0015119897487403994, 0.0012770892724140801, 0.0012331265924413774, 0.001171999113549563, 0.0011517209323673424, 0.0011762436373497951, 0.001772116023031148, 0.0023092806143474513, 0.0021992498633070763, 0.0023414550232701004, 0.0024427566809622062, 0.0016276150444437835, 0.0012528042954562063, 0.0011594522723250768, 0.00284446590971625, 0.0014300096134485846, 0.0013146655216008764, 0.0012118919773704627, 0.0012240443849639798, 0.0012521432045931842, 0.0012304713626773182, 0.001230689956256273, 0.0012794988870155066, 0.0011735975223762746, 0.0012871773850003426, 0.0012518711578608913, 0.0012550763415426693, 0.0014724455911412158, 0.0012840324541850184, 0.001183423568198288, 0.0011901082497471098, 0.0011611972085395178, 0.00196034460447642, 0.0013796454891129289, 0.001333477465587473, 0.001380241744025329, 0.0013842705588484573, 0.002039042929577273, 0.001166039744803552, 0.0011724408142008754, 0.0013030105118834695, 0.0011894671614606713, 0.0012863715569120507, 0.0013671066303369264, 0.0013571659314207906, 0.00224948488175869, 0.0017500255814601863, 0.0012926437220601148, 0.0013285153274705938, 0.0023464524636564905, 0.0022676079994240817, 0.003393109208297764, 0.001551910162751758, 0.0011604110477492213, 0.002471015022439492, 0.0011934093037230331, 0.0024789205107967867, 0.0037755909305455726, 0.0014800396747887135, 0.0017311214875473187, 0.0018192692334915315, 0.0017435330935472318, 0.0012234190240675627, 0.002586969396445987, 0.004474058490094923, 0.003300135651993197, 0.003724466186276702, 0.001519927091701606, 0.0018856605341614678, 0.0022734713491572196, 0.00133080116397333, 0.002271584513987046, 0.0026960009519487274, 0.0012228578372403633, 0.0012460267441997, 0.0012946204650540684, 0.001168783743281004, 0.0014034437210580638, 0.0014608423712901598, 0.0018688789758354771, 0.0012098163972760355, 0.00128489944185022, 0.0012925437916893252, 0.0017280951385971071, 0.0012389480239213553, 0.0012301665589993084, 0.0012618685818039054, 0.002028046511546817, 0.001098745069358238, 0.002393071696725349, 0.0011329044896546145, 0.001161528675448756, 0.0013563513707066344, 0.0012603331151483364, 0.0011594824649956684, 0.0017709501174300215, 0.0011572720223041468, 0.001162656209279978, 0.0019014596744158932, 0.0011938796053792154, 0.0011780634414144727, 0.0011512614667502253, 0.0013185884170017618, 0.0011906696505151516, 0.00116679425687031, 0.001199752744828719, 0.0011514376274981471, 0.001462268093492576, 0.0011774708603530428, 0.0012039739995934936, 0.002300645232872041, 0.001183745697638843, 0.0012541325129351991, 0.002633094441059024, 0.0011513405804370726, 0.00110538711576441, 0.002424409090497986, 0.002560273603384578, 0.0012488661409723897, 0.0016220642800637802, 0.001229963603776035, 0.0011594025120355708, 0.0011823331872217876, 0.0019360912563062686, 0.0011309217201372565, 0.0012850700471506908, 0.001306021837842499, 0.0012510226996139038, 0.0012022395358355933, 0.0011868576741192577, 0.0012394282100505608, 0.0011447615336713402, 0.001382624836053786, 0.0010988693711412854, 0.0011189633731318767, 0.0015744905361125981]
[813.4681644799525, 823.6701369937574, 835.6539667742992, 810.7892154765824, 813.777082307474, 841.3851983598073, 805.0072472034251, 720.6623120276112, 697.4796024794389, 828.21172256634, 804.5576876437494, 714.1300668356653, 754.8987015198064, 853.7623119867901, 869.1289366642507, 655.4753589387054, 845.0868426424579, 828.1688072690425, 825.702388700747, 752.4119290870764, 789.2250549407328, 809.8160298660094, 844.3701295648647, 860.5311716869774, 863.0299660758093, 649.5265854960537, 662.430873145257, 723.0147561518138, 919.0315647631395, 716.4589228599925, 861.5491154185218, 668.5524201561669, 856.9899394269974, 729.6280383650054, 761.7370090206484, 769.5931230967526, 864.5067606860229, 876.6033730245769, 772.5803362944185, 750.1855526480671, 920.2684438524147, 825.1847437959798, 340.7617748424943, 512.8846595511968, 276.0088813376366, 667.418240224513, 809.6265328793689, 799.2404012814537, 383.7154878044597, 661.380145489131, 783.0306162620107, 810.9467479897364, 853.2429661754268, 868.2658896756502, 850.164003652431, 564.2971379997635, 433.035289772515, 454.70049432958507, 427.0848639250775, 409.3735605324793, 614.3958938040765, 798.2092683006422, 862.4762086969536, 351.559846994178, 699.2959981495638, 760.6497497418916, 825.1560524146539, 816.9638391253494, 798.6306968178576, 812.696687084332, 812.5523369362451, 781.5559748805633, 852.0808717925903, 776.8936990760864, 798.8042489202555, 796.7642819008573, 679.1422420063427, 778.7965146369332, 845.0059867596328, 840.2596992437399, 861.1801618587581, 510.11439402874055, 724.8238825779588, 749.9189343701753, 724.5107636605787, 722.4021298494399, 490.4261629289563, 857.6037004368787, 852.9215188415209, 767.4535169747209, 840.7125748406498, 777.3803724333825, 731.4718382673251, 736.8295776133427, 444.54621949634134, 571.4202184208187, 773.6083678233302, 752.7199568739147, 426.1752647832013, 440.9933287649259, 294.7149468559765, 644.3671959895265, 861.7635982866926, 404.69199536179144, 837.9354818839928, 403.40139816688804, 264.85920175030725, 675.6575631276623, 577.6602088261387, 549.671253485008, 573.5480466077602, 817.3814370445621, 386.5526980619923, 223.5107123015693, 303.01784697729795, 268.49485268107276, 657.9262949254156, 530.3181468156934, 439.85599394982563, 751.4270554245176, 440.2213493896457, 370.9197503350949, 817.7565449935792, 802.55099230818, 772.427152971227, 855.5902712958728, 712.5330250122853, 684.5365520968826, 535.0801271403637, 826.5717031539265, 778.2710206177904, 773.6681777667454, 578.6718437341445, 807.1363614067776, 812.8980524502798, 792.4755512736905, 493.0853381845209, 910.1292264128989, 417.8729794716924, 882.6869423960596, 860.9344057852472, 737.2720827340029, 793.4410260118443, 862.453750004521, 564.6686432089833, 864.1010762612098, 860.0994791222855, 525.9117579273347, 837.6053963015534, 848.8507196177176, 868.6124124546569, 758.3867620146583, 839.863516775911, 857.0491276519462, 833.505073699801, 868.4795217026284, 683.8691238974758, 849.2779173322119, 830.582720505291, 434.66067071611764, 844.7760376191009, 797.3639066732894, 379.7812886642237, 868.5527262666096, 904.6604449595638, 412.4716426445155, 390.58325589813546, 800.7263286210818, 616.4983794357888, 813.032188050087, 862.513225233826, 845.7852750879564, 516.5045793904515, 884.23449845727, 778.167697719856, 765.6839809447321, 799.3460073175526, 831.780997207823, 842.561009467356, 806.8236561754604, 873.5443763496502, 723.2619969811527, 910.0262745165063, 893.6843010339926, 635.1260786038068]
Elapsed: 0.06632598942347072~0.024997687245724372
Time per graph: 0.0015281215225677173~0.0005799537393162608
Speed: 714.9268999682506~168.71563506094236
Total Time: 0.0684
best val loss: 0.44931161403656006 test_score: 0.8372

Testing...
Test loss: 0.4027 score: 0.8372 time: 0.04s
test Score 0.8372
Epoch Time List: [0.24228799506090581, 0.23929360217880458, 0.24765392299741507, 0.3317982309963554, 0.2394183329306543, 0.25851948792114854, 0.24442521098535508, 0.26535777817480266, 0.2508795871399343, 0.25552063703071326, 0.25416517083067447, 0.2799532990902662, 0.26041977596469223, 0.2563794059678912, 0.27585022908169776, 0.2465682690963149, 0.23009828000795096, 0.266987832961604, 0.24583989090751857, 0.2549490408273414, 0.258024926064536, 0.24338302086107433, 0.24464340484701097, 0.24005280109122396, 0.25081054703332484, 0.275554375955835, 0.41209831496234983, 0.25980446895118803, 0.28732512204442173, 0.3138433749554679, 0.29295934294350445, 0.25505446398165077, 0.2441829270683229, 0.2594833648763597, 0.24240373901557177, 0.24724392511416227, 0.2594070080667734, 0.2612203120952472, 0.2564186000963673, 0.2453440009849146, 0.2704116489039734, 0.6251569329760969, 0.32568966690450907, 0.4225558158941567, 0.37024785100948066, 0.2883321789558977, 0.2443746409844607, 0.26521265995688736, 0.32048019592184573, 0.275596117018722, 0.26920206495560706, 0.2505432809703052, 0.2454797540558502, 0.24449741595890373, 0.2515813910868019, 0.29115095897577703, 0.38789549784269184, 0.34878779598511755, 0.42672368802595884, 0.368529018945992, 0.29179269494488835, 0.25160381209570915, 0.2434811609564349, 0.36464877100661397, 0.2730072069680318, 0.25108764914330095, 0.24412274593487382, 0.24753268097992986, 0.2494685729034245, 0.23909339401870966, 0.24163507611956447, 0.24244843795895576, 0.23737648909445852, 0.29344117490109056, 0.24109184788540006, 0.24757845792919397, 0.34487982804421335, 0.24203483201563358, 0.24554159410763532, 0.23595262283924967, 0.271982982987538, 0.27497225208207965, 0.3057591039687395, 0.2651183750713244, 0.26245505688712, 0.2627903170650825, 0.30082117894198745, 0.2768241410376504, 0.23056786297820508, 0.24079106107819825, 0.26035572891123593, 0.2596934858011082, 0.3307456821203232, 0.30458932486362755, 0.30588872800581157, 0.33603786991443485, 0.41583874204661697, 0.2755309919593856, 0.45354878809303045, 0.3097088240319863, 0.4785517439013347, 0.29792036197613925, 0.5116928139468655, 0.5216693219263107, 0.49053596588782966, 0.3369740741327405, 0.4492953539593145, 0.34218819707166404, 0.5242264880798757, 0.34129656897857785, 0.291917972965166, 0.40581877587828785, 0.4517860698979348, 0.4818869010778144, 0.507732308935374, 0.5352984110359102, 0.47266816697083414, 0.2847174860071391, 0.3733511620666832, 0.3252995330840349, 0.289676615037024, 0.37128819699864835, 0.2798939769854769, 0.2782946809893474, 0.272355119115673, 0.3562963210279122, 0.2767027140362188, 0.25920354505069554, 0.27665079291909933, 0.26810806803405285, 0.26601969299372286, 0.2629926069639623, 0.28414881706703454, 0.2640165709890425, 0.261647691950202, 0.27398354408796877, 0.30870601802598685, 0.26959968300070614, 0.37632760184351355, 0.27774310193490237, 0.28385451005306095, 0.30045173806138337, 0.3458920589182526, 0.259107663994655, 0.2773599278880283, 0.24415484303608537, 0.3024670189479366, 0.2786512679886073, 0.24983786093071103, 0.2502852660836652, 0.24974693404510617, 0.300955833052285, 0.26230016606859863, 0.27067219314631075, 0.2663529679412022, 0.25725163298193365, 0.2665561781032011, 0.25623279612045735, 0.2515128590166569, 0.37878659600391984, 0.2616465591127053, 0.2611150919692591, 0.36066972999833524, 0.2817180189304054, 0.2542367089772597, 0.3059450020082295, 0.3710207719122991, 0.35785089107230306, 0.31772144604474306, 0.37561150500550866, 0.25949770398437977, 0.2584883871022612, 0.2908442310290411, 0.26732412492856383, 0.27082511596381664, 0.2871353580849245, 0.26821893511805683, 0.31478244985919446, 0.255745240021497, 0.26378014497458935, 0.2817605740856379, 0.2555978619493544, 0.2837023949250579, 0.2410806859843433, 0.26010907092131674]
Total Epoch List: [80, 43, 62]
Total Time List: [0.052797268028371036, 0.05315539694856852, 0.06836757296696305]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99681e66b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7293;  Loss pred: 0.7002; Loss self: 2.9144; time: 0.13s
Val loss: 0.7206 score: 0.5116 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.7293;  Loss pred: 0.7002; Loss self: 2.9144; time: 0.15s
Val loss: 0.7000 score: 0.5814 time: 0.04s
Test loss: 0.6694 score: 0.5455 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.6828;  Loss pred: 0.6539; Loss self: 2.8908; time: 0.13s
Val loss: 0.6985 score: 0.5116 time: 0.05s
Test loss: 0.6582 score: 0.5682 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.6494;  Loss pred: 0.6206; Loss self: 2.8787; time: 0.13s
Val loss: 0.7026 score: 0.5116 time: 0.05s
Test loss: 0.6584 score: 0.5909 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6460;  Loss pred: 0.6168; Loss self: 2.9216; time: 0.15s
Val loss: 0.6797 score: 0.5116 time: 0.05s
Test loss: 0.6381 score: 0.6364 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.6268;  Loss pred: 0.5968; Loss self: 3.0071; time: 0.16s
Val loss: 0.6553 score: 0.6977 time: 0.05s
Test loss: 0.6331 score: 0.7500 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6162;  Loss pred: 0.5851; Loss self: 3.1107; time: 0.13s
Val loss: 0.6483 score: 0.6977 time: 0.05s
Test loss: 0.6284 score: 0.6591 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.5973;  Loss pred: 0.5661; Loss self: 3.1208; time: 0.14s
Val loss: 0.6445 score: 0.6047 time: 0.05s
Test loss: 0.6169 score: 0.7045 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.5640;  Loss pred: 0.5331; Loss self: 3.0903; time: 0.14s
Val loss: 0.6550 score: 0.5814 time: 0.05s
Test loss: 0.6144 score: 0.6591 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5463;  Loss pred: 0.5159; Loss self: 3.0493; time: 0.13s
Val loss: 0.6125 score: 0.7209 time: 0.05s
Test loss: 0.5867 score: 0.7273 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.5058;  Loss pred: 0.4760; Loss self: 2.9776; time: 0.15s
Val loss: 0.6068 score: 0.7209 time: 0.05s
Test loss: 0.5742 score: 0.7273 time: 0.05s
Epoch 12/1000, LR 0.000270
Train loss: 0.4736;  Loss pred: 0.4443; Loss self: 2.9313; time: 0.14s
Val loss: 0.6206 score: 0.6744 time: 0.06s
Test loss: 0.5733 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4435;  Loss pred: 0.4145; Loss self: 2.9047; time: 0.14s
Val loss: 0.6088 score: 0.6977 time: 0.05s
Test loss: 0.5661 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4126;  Loss pred: 0.3839; Loss self: 2.8721; time: 0.14s
Val loss: 0.5847 score: 0.7907 time: 0.09s
Test loss: 0.5648 score: 0.7500 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.3885;  Loss pred: 0.3601; Loss self: 2.8386; time: 0.14s
Val loss: 0.5690 score: 0.7442 time: 0.05s
Test loss: 0.5517 score: 0.7500 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.3577;  Loss pred: 0.3296; Loss self: 2.8119; time: 0.15s
Val loss: 0.5655 score: 0.7442 time: 0.06s
Test loss: 0.5383 score: 0.7500 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.3352;  Loss pred: 0.3072; Loss self: 2.7976; time: 0.14s
Val loss: 0.5415 score: 0.7442 time: 0.05s
Test loss: 0.5241 score: 0.7273 time: 0.08s
Epoch 18/1000, LR 0.000270
Train loss: 0.3083;  Loss pred: 0.2807; Loss self: 2.7589; time: 0.16s
Val loss: 0.5204 score: 0.7674 time: 0.09s
Test loss: 0.5185 score: 0.7500 time: 0.15s
Epoch 19/1000, LR 0.000270
Train loss: 0.2840;  Loss pred: 0.2571; Loss self: 2.6942; time: 0.13s
Val loss: 0.5147 score: 0.7674 time: 0.15s
Test loss: 0.5204 score: 0.7500 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.2603;  Loss pred: 0.2339; Loss self: 2.6429; time: 0.15s
Val loss: 0.5155 score: 0.7674 time: 0.06s
Test loss: 0.5273 score: 0.7273 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.2419;  Loss pred: 0.2158; Loss self: 2.6051; time: 0.14s
Val loss: 0.5131 score: 0.7674 time: 0.06s
Test loss: 0.5319 score: 0.7273 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 0.2259;  Loss pred: 0.2002; Loss self: 2.5727; time: 0.12s
Val loss: 0.5070 score: 0.7674 time: 0.05s
Test loss: 0.5277 score: 0.7273 time: 0.06s
Epoch 23/1000, LR 0.000270
Train loss: 0.2078;  Loss pred: 0.1823; Loss self: 2.5421; time: 0.18s
Val loss: 0.5009 score: 0.7907 time: 0.07s
Test loss: 0.5204 score: 0.7045 time: 0.09s
Epoch 24/1000, LR 0.000270
Train loss: 0.1943;  Loss pred: 0.1691; Loss self: 2.5192; time: 0.17s
Val loss: 0.4895 score: 0.7907 time: 0.06s
Test loss: 0.5174 score: 0.7500 time: 0.11s
Epoch 25/1000, LR 0.000270
Train loss: 0.1824;  Loss pred: 0.1574; Loss self: 2.5007; time: 0.17s
Val loss: 0.4771 score: 0.8140 time: 0.08s
Test loss: 0.5164 score: 0.7273 time: 0.10s
Epoch 26/1000, LR 0.000270
Train loss: 0.1723;  Loss pred: 0.1475; Loss self: 2.4853; time: 0.25s
Val loss: 0.4687 score: 0.8140 time: 0.09s
Test loss: 0.5117 score: 0.7500 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.1655;  Loss pred: 0.1407; Loss self: 2.4736; time: 0.13s
Val loss: 0.4582 score: 0.8140 time: 0.08s
Test loss: 0.5042 score: 0.7727 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.1573;  Loss pred: 0.1326; Loss self: 2.4687; time: 0.13s
Val loss: 0.4471 score: 0.8140 time: 0.05s
Test loss: 0.4991 score: 0.8182 time: 0.05s
Epoch 29/1000, LR 0.000270
Train loss: 0.1500;  Loss pred: 0.1252; Loss self: 2.4736; time: 0.13s
Val loss: 0.4382 score: 0.8372 time: 0.05s
Test loss: 0.4958 score: 0.8182 time: 0.04s
Epoch 30/1000, LR 0.000270
Train loss: 0.1436;  Loss pred: 0.1188; Loss self: 2.4819; time: 0.13s
Val loss: 0.4318 score: 0.8140 time: 0.05s
Test loss: 0.4942 score: 0.8182 time: 0.05s
Epoch 31/1000, LR 0.000270
Train loss: 0.1374;  Loss pred: 0.1125; Loss self: 2.4879; time: 0.13s
Val loss: 0.4257 score: 0.8372 time: 0.04s
Test loss: 0.4929 score: 0.8182 time: 0.04s
Epoch 32/1000, LR 0.000270
Train loss: 0.1319;  Loss pred: 0.1070; Loss self: 2.4896; time: 0.13s
Val loss: 0.4189 score: 0.8372 time: 0.06s
Test loss: 0.4904 score: 0.8182 time: 0.04s
Epoch 33/1000, LR 0.000270
Train loss: 0.1279;  Loss pred: 0.1030; Loss self: 2.4908; time: 0.13s
Val loss: 0.4141 score: 0.8605 time: 0.05s
Test loss: 0.4892 score: 0.8182 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.1242;  Loss pred: 0.0992; Loss self: 2.4972; time: 0.15s
Val loss: 0.4111 score: 0.8605 time: 0.12s
Test loss: 0.4908 score: 0.7955 time: 0.06s
Epoch 35/1000, LR 0.000270
Train loss: 0.1205;  Loss pred: 0.0954; Loss self: 2.5072; time: 0.35s
Val loss: 0.4105 score: 0.8605 time: 0.05s
Test loss: 0.4939 score: 0.7955 time: 0.05s
Epoch 36/1000, LR 0.000270
Train loss: 0.1166;  Loss pred: 0.0914; Loss self: 2.5175; time: 0.14s
Val loss: 0.4114 score: 0.8605 time: 0.05s
Test loss: 0.4972 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1132;  Loss pred: 0.0879; Loss self: 2.5278; time: 0.15s
Val loss: 0.4098 score: 0.8605 time: 0.06s
Test loss: 0.5003 score: 0.8182 time: 0.05s
Epoch 38/1000, LR 0.000270
Train loss: 0.1100;  Loss pred: 0.0847; Loss self: 2.5372; time: 0.15s
Val loss: 0.4067 score: 0.8605 time: 0.04s
Test loss: 0.5031 score: 0.8182 time: 0.13s
Epoch 39/1000, LR 0.000269
Train loss: 0.1070;  Loss pred: 0.0816; Loss self: 2.5454; time: 0.16s
Val loss: 0.4027 score: 0.8605 time: 0.05s
Test loss: 0.5056 score: 0.8182 time: 0.05s
Epoch 40/1000, LR 0.000269
Train loss: 0.1042;  Loss pred: 0.0787; Loss self: 2.5517; time: 0.13s
Val loss: 0.3984 score: 0.8605 time: 0.05s
Test loss: 0.5064 score: 0.8182 time: 0.04s
Epoch 41/1000, LR 0.000269
Train loss: 0.1013;  Loss pred: 0.0757; Loss self: 2.5539; time: 0.13s
Val loss: 0.3942 score: 0.8605 time: 0.04s
Test loss: 0.5064 score: 0.8182 time: 0.04s
Epoch 42/1000, LR 0.000269
Train loss: 0.0984;  Loss pred: 0.0729; Loss self: 2.5533; time: 0.12s
Val loss: 0.3897 score: 0.8605 time: 0.04s
Test loss: 0.5063 score: 0.8409 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.0958;  Loss pred: 0.0703; Loss self: 2.5504; time: 0.13s
Val loss: 0.3845 score: 0.8605 time: 0.04s
Test loss: 0.5075 score: 0.8409 time: 0.04s
Epoch 44/1000, LR 0.000269
Train loss: 0.0932;  Loss pred: 0.0677; Loss self: 2.5483; time: 0.20s
Val loss: 0.3803 score: 0.8605 time: 0.06s
Test loss: 0.5113 score: 0.8409 time: 0.08s
Epoch 45/1000, LR 0.000269
Train loss: 0.0905;  Loss pred: 0.0650; Loss self: 2.5469; time: 0.16s
Val loss: 0.3779 score: 0.8605 time: 0.05s
Test loss: 0.5184 score: 0.8409 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0880;  Loss pred: 0.0625; Loss self: 2.5473; time: 0.16s
Val loss: 0.3780 score: 0.8605 time: 0.09s
Test loss: 0.5283 score: 0.8182 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0858;  Loss pred: 0.0603; Loss self: 2.5487; time: 0.14s
Val loss: 0.3780 score: 0.8837 time: 0.04s
Test loss: 0.5380 score: 0.7955 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0839;  Loss pred: 0.0584; Loss self: 2.5493; time: 0.13s
Val loss: 0.3765 score: 0.8605 time: 0.05s
Test loss: 0.5439 score: 0.7955 time: 0.04s
Epoch 49/1000, LR 0.000269
Train loss: 0.0820;  Loss pred: 0.0566; Loss self: 2.5472; time: 0.14s
Val loss: 0.3728 score: 0.8837 time: 0.07s
Test loss: 0.5457 score: 0.7955 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 0.0802;  Loss pred: 0.0548; Loss self: 2.5431; time: 0.13s
Val loss: 0.3668 score: 0.8837 time: 0.07s
Test loss: 0.5444 score: 0.8182 time: 0.05s
Epoch 51/1000, LR 0.000269
Train loss: 0.0783;  Loss pred: 0.0530; Loss self: 2.5379; time: 0.15s
Val loss: 0.3620 score: 0.8837 time: 0.05s
Test loss: 0.5416 score: 0.8409 time: 0.04s
Epoch 52/1000, LR 0.000269
Train loss: 0.0765;  Loss pred: 0.0512; Loss self: 2.5328; time: 0.21s
Val loss: 0.3582 score: 0.8837 time: 0.07s
Test loss: 0.5402 score: 0.8409 time: 0.10s
Epoch 53/1000, LR 0.000269
Train loss: 0.0748;  Loss pred: 0.0495; Loss self: 2.5287; time: 0.19s
Val loss: 0.3557 score: 0.8837 time: 0.05s
Test loss: 0.5414 score: 0.8409 time: 0.04s
Epoch 54/1000, LR 0.000269
Train loss: 0.0730;  Loss pred: 0.0478; Loss self: 2.5273; time: 0.15s
Val loss: 0.3538 score: 0.8837 time: 0.05s
Test loss: 0.5440 score: 0.8409 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0713;  Loss pred: 0.0460; Loss self: 2.5274; time: 0.15s
Val loss: 0.3525 score: 0.8837 time: 0.06s
Test loss: 0.5479 score: 0.8409 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0695;  Loss pred: 0.0443; Loss self: 2.5278; time: 0.16s
Val loss: 0.3518 score: 0.8837 time: 0.19s
Test loss: 0.5522 score: 0.8409 time: 0.05s
Epoch 57/1000, LR 0.000269
Train loss: 0.0680;  Loss pred: 0.0427; Loss self: 2.5276; time: 0.36s
Val loss: 0.3521 score: 0.8837 time: 0.10s
Test loss: 0.5572 score: 0.8409 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0665;  Loss pred: 0.0412; Loss self: 2.5283; time: 0.13s
Val loss: 0.3529 score: 0.8837 time: 0.05s
Test loss: 0.5624 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0650;  Loss pred: 0.0397; Loss self: 2.5299; time: 0.13s
Val loss: 0.3544 score: 0.8837 time: 0.04s
Test loss: 0.5680 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0635;  Loss pred: 0.0382; Loss self: 2.5330; time: 0.13s
Val loss: 0.3554 score: 0.8837 time: 0.04s
Test loss: 0.5723 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0620;  Loss pred: 0.0366; Loss self: 2.5353; time: 0.13s
Val loss: 0.3558 score: 0.8837 time: 0.05s
Test loss: 0.5755 score: 0.8409 time: 0.07s
     INFO: Early stopping counter 5 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0603;  Loss pred: 0.0350; Loss self: 2.5363; time: 0.13s
Val loss: 0.3556 score: 0.8837 time: 0.04s
Test loss: 0.5777 score: 0.8409 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0586;  Loss pred: 0.0333; Loss self: 2.5355; time: 0.20s
Val loss: 0.3554 score: 0.8837 time: 0.05s
Test loss: 0.5803 score: 0.8409 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0570;  Loss pred: 0.0317; Loss self: 2.5343; time: 0.13s
Val loss: 0.3558 score: 0.8837 time: 0.10s
Test loss: 0.5840 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0554;  Loss pred: 0.0300; Loss self: 2.5346; time: 0.14s
Val loss: 0.3569 score: 0.8837 time: 0.06s
Test loss: 0.5882 score: 0.8409 time: 0.07s
     INFO: Early stopping counter 9 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0537;  Loss pred: 0.0284; Loss self: 2.5361; time: 0.20s
Val loss: 0.3576 score: 0.8837 time: 0.05s
Test loss: 0.5923 score: 0.8409 time: 0.04s
     INFO: Early stopping counter 10 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0522;  Loss pred: 0.0268; Loss self: 2.5376; time: 0.13s
Val loss: 0.3582 score: 0.8837 time: 0.05s
Test loss: 0.5968 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0507;  Loss pred: 0.0253; Loss self: 2.5393; time: 0.13s
Val loss: 0.3589 score: 0.8837 time: 0.05s
Test loss: 0.6012 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0493;  Loss pred: 0.0239; Loss self: 2.5409; time: 0.13s
Val loss: 0.3597 score: 0.8837 time: 0.04s
Test loss: 0.6053 score: 0.8182 time: 0.04s
     INFO: Early stopping counter 13 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0478;  Loss pred: 0.0224; Loss self: 2.5421; time: 0.18s
Val loss: 0.3606 score: 0.8837 time: 0.05s
Test loss: 0.6090 score: 0.8182 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0465;  Loss pred: 0.0211; Loss self: 2.5430; time: 0.13s
Val loss: 0.3612 score: 0.8837 time: 0.06s
Test loss: 0.6120 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0453;  Loss pred: 0.0198; Loss self: 2.5420; time: 0.13s
Val loss: 0.3621 score: 0.8837 time: 0.05s
Test loss: 0.6149 score: 0.8409 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0441;  Loss pred: 0.0187; Loss self: 2.5406; time: 0.14s
Val loss: 0.3632 score: 0.8837 time: 0.04s
Test loss: 0.6175 score: 0.8409 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0430;  Loss pred: 0.0176; Loss self: 2.5390; time: 0.13s
Val loss: 0.3646 score: 0.8837 time: 0.05s
Test loss: 0.6203 score: 0.8409 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0420;  Loss pred: 0.0166; Loss self: 2.5377; time: 0.13s
Val loss: 0.3655 score: 0.8837 time: 0.05s
Test loss: 0.6226 score: 0.8409 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0410;  Loss pred: 0.0157; Loss self: 2.5364; time: 0.12s
Val loss: 0.3657 score: 0.8837 time: 0.04s
Test loss: 0.6247 score: 0.8409 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 055,   Train_Loss: 0.0695,   Val_Loss: 0.3518,   Val_Precision: 0.9474,   Val_Recall: 0.8182,   Val_accuracy: 0.8780,   Val_Score: 0.8837,   Val_Loss: 0.3518,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8409,   Test_loss: 0.5522


[0.04782959702424705, 0.050653373007662594, 0.051212672027759254, 0.04921023000497371, 0.05084794096183032, 0.0625604810193181, 0.052449023933149874, 0.05455954105127603, 0.056235497002489865, 0.053706959006376565, 0.05345787794794887, 0.05624976207036525, 0.05084289296064526, 0.05360714392736554, 0.049973808927461505, 0.05574840400367975, 0.0834277420071885, 0.1558884660480544, 0.07165373605675995, 0.14746161398943514, 0.04857453296426684, 0.07051960797980428, 0.09585125301964581, 0.11159280897118151, 0.10050046490505338, 0.06722288602031767, 0.050431355950422585, 0.056090947007760406, 0.047943184967152774, 0.05003216001205146, 0.04830343194771558, 0.04901265096850693, 0.05866874102503061, 0.06754677195567638, 0.049702938995324075, 0.05242571095004678, 0.05417458596639335, 0.13587625592481345, 0.051274894969537854, 0.047381043899804354, 0.04829341603908688, 0.05071757407858968, 0.04735324694775045, 0.08831956295762211, 0.04976393701508641, 0.09417908894829452, 0.04793416301254183, 0.04864169005304575, 0.05052545398939401, 0.051859880913980305, 0.04888797691091895, 0.10423387598711997, 0.048510866006836295, 0.04852380196098238, 0.083586864056997, 0.04935214307624847, 0.04920945188496262, 0.04998316103592515, 0.049601098988205194, 0.05042179999873042, 0.07228852494154125, 0.048453439958393574, 0.04830464906990528, 0.050713525037281215, 0.07829408103134483, 0.04830439400393516, 0.05069357296451926, 0.05001788097433746, 0.04819686699192971, 0.04928215395193547, 0.04970421805046499, 0.049556590034626424, 0.04776971903629601, 0.06625933200120926, 0.04819887294434011, 0.04689954896457493]
[0.0010870362960056148, 0.0011512130229014226, 0.0011639243642672557, 0.0011184143182948571, 0.00115563502185978, 0.0014218291140754115, 0.0011920232712079517, 0.0012399895693471824, 0.001278079477329315, 0.0012206127046903764, 0.0012149517715442926, 0.0012784036834173921, 0.0011555202945601195, 0.0012183441801673987, 0.0011357683847150342, 0.0012670091819018126, 0.0018960850456179205, 0.003542919682910327, 0.001628494001289999, 0.0033514003179417077, 0.0011039666582787918, 0.0016027183631773698, 0.002178437568628314, 0.002536200203890489, 0.0022841014751148496, 0.001527792864098129, 0.0011461671806914223, 0.0012747942501763728, 0.001089617840162563, 0.0011370945457284424, 0.0010978052715389904, 0.0011139238856478848, 0.001333380477841605, 0.001535153908083554, 0.001129612249893729, 0.0011914934306828814, 0.0012312405901453035, 0.003088096725563942, 0.0011653385220349512, 0.0010768419068137353, 0.0010975776372519745, 0.0011526721381497655, 0.0010762101579034193, 0.0020072627944914116, 0.0011309985685246911, 0.0021404338397339666, 0.001089412795739587, 0.0011054929557510397, 0.0011483057724862274, 0.001178633657135916, 0.001111090384339067, 0.0023689517269799994, 0.0011025196819735522, 0.0011028136809314178, 0.001899701455840841, 0.0011216396153692833, 0.0011183966337491504, 0.0011359809326346624, 0.0011272977042773907, 0.0011459499999711459, 0.0016429210213986648, 0.0011012145445089448, 0.0010978329334069383, 0.001152580114483664, 0.0017794109325305644, 0.0010978271364530717, 0.0011521266582845287, 0.0011367700221440332, 0.0010953833407256752, 0.0011200489534530789, 0.0011296413193287497, 0.0011262861371506006, 0.0010856754326430912, 0.0015058939091183922, 0.0010954289305531843, 0.0010658988401039758]
[919.9324840160027, 868.6489642722094, 859.1623568508649, 894.1230308322658, 865.3251079139898, 703.3194004121101, 838.909796607106, 806.4583966835062, 782.4239554253761, 819.260684537658, 823.0779389118688, 782.225530926842, 865.4110228160704, 820.7861261852964, 880.4612044654697, 789.260262896418, 527.402503548625, 282.25308206212316, 614.0642822189444, 298.38273710439904, 905.8244581037552, 623.9399404007027, 459.0445989368725, 394.2906393848627, 437.80892000418584, 654.5389911807906, 872.4730709849435, 784.4403125144673, 917.752961763922, 879.4343476156442, 910.9083604582447, 897.7274056910774, 749.9734821517254, 651.4004848206873, 885.2595216580534, 839.2828481033836, 812.1889482883164, 323.8240537356815, 858.1197489754035, 928.641422359664, 911.09728010104, 867.5493810453073, 929.1865465645805, 498.19087104305845, 884.1744170414206, 467.1950057210316, 917.9256971377081, 904.5738326940574, 870.8481869205215, 848.4400508551602, 900.0167889985393, 422.1276392469279, 907.0132863387636, 906.7714857829991, 526.3985016831934, 891.5519622323296, 894.1371690718929, 880.2964656111928, 887.0771192078406, 872.6384222917048, 608.6719854303598, 908.0882603543176, 910.8854084898599, 867.6186474447226, 561.9837338966239, 910.8902183187621, 867.9601264404043, 879.6854073561205, 912.922410648965, 892.8181191697279, 885.2367409809473, 887.8738421924531, 921.08559329328, 664.0574040075892, 912.884416422165, 938.1753336953182]
Elapsed: 0.06096762388417693~0.022829192649488957
Time per graph: 0.0013856278155494758~0.0005188452874883854
Speed: 783.4975150467161~173.15329691294977
Total Time: 0.0475
best val loss: 0.35180461406707764 test_score: 0.8409

Testing...
Test loss: 0.5380 score: 0.7955 time: 0.04s
test Score 0.7955
Epoch Time List: [0.22410397906787694, 0.24005058605689555, 0.22047194314654917, 0.22293940710369498, 0.2426389609463513, 0.26818261994048953, 0.22790477704256773, 0.2379904551198706, 0.24569437897298485, 0.2326085219392553, 0.24761512107215822, 0.24713366094511002, 0.24122760398313403, 0.28028166806325316, 0.24307807092554867, 0.25480596988927573, 0.2626666750293225, 0.40247902495320886, 0.350597141077742, 0.3482417459599674, 0.24426231195684522, 0.24230172904208302, 0.34211287822108716, 0.3393531389301643, 0.3453452681424096, 0.40126782294828445, 0.2471999649424106, 0.22506614995654672, 0.22632351610809565, 0.22091501613613218, 0.2179763870080933, 0.22860757098533213, 0.2308259770506993, 0.33429908589459956, 0.4431022119242698, 0.23228058207314461, 0.2514903690898791, 0.3234496890800074, 0.251804664847441, 0.21424264775123447, 0.21609029395040125, 0.21549889107700437, 0.2140648189233616, 0.345238043111749, 0.25825428508687764, 0.3322947120759636, 0.2247628989862278, 0.2230896418914199, 0.25469822518061846, 0.24396134493872523, 0.24199334089644253, 0.3737288099946454, 0.28311040205881, 0.24715489288792014, 0.28734521102160215, 0.3884050069609657, 0.5079300219658762, 0.22047395398840308, 0.2231623261468485, 0.2218177969334647, 0.24567646405193955, 0.2195744999917224, 0.29268545284867287, 0.27583142986986786, 0.27610218594782054, 0.2886070170206949, 0.2250947969732806, 0.22096336202230304, 0.22205777000635862, 0.2704197319690138, 0.24121928284876049, 0.21853977802675217, 0.23007072205655277, 0.24661271797958761, 0.21963485202286392, 0.2100751670077443]
Total Epoch List: [76]
Total Time List: [0.04752644000109285]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99681e4880>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7446;  Loss pred: 0.7113; Loss self: 3.3347; time: 0.14s
Val loss: 0.7004 score: 0.5000 time: 0.04s
Test loss: 0.7203 score: 0.3721 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7446;  Loss pred: 0.7113; Loss self: 3.3347; time: 0.13s
Val loss: 0.6866 score: 0.5909 time: 0.05s
Test loss: 0.7040 score: 0.4884 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.7211;  Loss pred: 0.6877; Loss self: 3.3387; time: 0.13s
Val loss: 0.6693 score: 0.5682 time: 0.05s
Test loss: 0.6854 score: 0.5116 time: 0.04s
Epoch 4/1000, LR 0.000060
Train loss: 0.6995;  Loss pred: 0.6662; Loss self: 3.3218; time: 0.13s
Val loss: 0.6567 score: 0.5682 time: 0.04s
Test loss: 0.6699 score: 0.5581 time: 0.04s
Epoch 5/1000, LR 0.000090
Train loss: 0.6629;  Loss pred: 0.6302; Loss self: 3.2788; time: 0.14s
Val loss: 0.6524 score: 0.6136 time: 0.04s
Test loss: 0.6740 score: 0.4884 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.6367;  Loss pred: 0.6041; Loss self: 3.2599; time: 0.14s
Val loss: 0.6397 score: 0.6591 time: 0.04s
Test loss: 0.6657 score: 0.5349 time: 0.05s
Epoch 7/1000, LR 0.000150
Train loss: 0.5991;  Loss pred: 0.5666; Loss self: 3.2532; time: 0.18s
Val loss: 0.6151 score: 0.7727 time: 0.06s
Test loss: 0.6379 score: 0.6977 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.5571;  Loss pred: 0.5250; Loss self: 3.2135; time: 0.15s
Val loss: 0.5887 score: 0.7500 time: 0.05s
Test loss: 0.6081 score: 0.6512 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.5003;  Loss pred: 0.4688; Loss self: 3.1518; time: 0.15s
Val loss: 0.5638 score: 0.7727 time: 0.05s
Test loss: 0.5903 score: 0.7209 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.4447;  Loss pred: 0.4140; Loss self: 3.0703; time: 0.15s
Val loss: 0.5254 score: 0.7727 time: 0.05s
Test loss: 0.5412 score: 0.7442 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.4024;  Loss pred: 0.3726; Loss self: 2.9864; time: 0.15s
Val loss: 0.5690 score: 0.7273 time: 0.06s
Test loss: 0.6489 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.3802;  Loss pred: 0.3511; Loss self: 2.9056; time: 0.14s
Val loss: 0.4895 score: 0.7955 time: 0.05s
Test loss: 0.5176 score: 0.7442 time: 0.09s
Epoch 13/1000, LR 0.000270
Train loss: 0.3510;  Loss pred: 0.3225; Loss self: 2.8419; time: 0.16s
Val loss: 0.4817 score: 0.7955 time: 0.04s
Test loss: 0.5179 score: 0.8140 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.3042;  Loss pred: 0.2765; Loss self: 2.7703; time: 0.14s
Val loss: 0.5407 score: 0.7500 time: 0.05s
Test loss: 0.5961 score: 0.7442 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.2743;  Loss pred: 0.2472; Loss self: 2.7112; time: 0.15s
Val loss: 0.4846 score: 0.7500 time: 0.05s
Test loss: 0.5168 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.2303;  Loss pred: 0.2041; Loss self: 2.6213; time: 0.15s
Val loss: 0.4498 score: 0.7955 time: 0.04s
Test loss: 0.4559 score: 0.7674 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.2200;  Loss pred: 0.1945; Loss self: 2.5467; time: 0.15s
Val loss: 0.4770 score: 0.7727 time: 0.05s
Test loss: 0.4611 score: 0.7907 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.1955;  Loss pred: 0.1705; Loss self: 2.4993; time: 0.16s
Val loss: 0.5400 score: 0.7500 time: 0.09s
Test loss: 0.5211 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.1926;  Loss pred: 0.1678; Loss self: 2.4862; time: 0.31s
Val loss: 0.5188 score: 0.7500 time: 0.07s
Test loss: 0.4880 score: 0.7907 time: 0.11s
     INFO: Early stopping counter 3 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.1773;  Loss pred: 0.1525; Loss self: 2.4744; time: 0.26s
Val loss: 0.4682 score: 0.7727 time: 0.05s
Test loss: 0.4308 score: 0.8140 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.1640;  Loss pred: 0.1392; Loss self: 2.4723; time: 0.25s
Val loss: 0.4437 score: 0.7955 time: 0.40s
Test loss: 0.4058 score: 0.8372 time: 0.04s
Epoch 22/1000, LR 0.000270
Train loss: 0.1584;  Loss pred: 0.1336; Loss self: 2.4820; time: 0.26s
Val loss: 0.4578 score: 0.7727 time: 0.11s
Test loss: 0.4069 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.1471;  Loss pred: 0.1221; Loss self: 2.4969; time: 0.46s
Val loss: 0.5002 score: 0.7500 time: 0.18s
Test loss: 0.4312 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.1360;  Loss pred: 0.1109; Loss self: 2.5143; time: 0.36s
Val loss: 0.5425 score: 0.7500 time: 0.07s
Test loss: 0.4682 score: 0.8140 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.1302;  Loss pred: 0.1049; Loss self: 2.5312; time: 0.28s
Val loss: 0.5644 score: 0.7500 time: 0.27s
Test loss: 0.4819 score: 0.8140 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.1242;  Loss pred: 0.0988; Loss self: 2.5443; time: 0.40s
Val loss: 0.5537 score: 0.7273 time: 0.12s
Test loss: 0.4628 score: 0.8140 time: 0.15s
     INFO: Early stopping counter 5 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.1175;  Loss pred: 0.0919; Loss self: 2.5537; time: 0.26s
Val loss: 0.5297 score: 0.7727 time: 0.12s
Test loss: 0.4348 score: 0.8140 time: 0.33s
     INFO: Early stopping counter 6 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1116;  Loss pred: 0.0860; Loss self: 2.5596; time: 0.43s
Val loss: 0.5086 score: 0.7727 time: 0.14s
Test loss: 0.4161 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1078;  Loss pred: 0.0822; Loss self: 2.5631; time: 0.17s
Val loss: 0.5002 score: 0.7727 time: 0.04s
Test loss: 0.4082 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1047;  Loss pred: 0.0791; Loss self: 2.5657; time: 0.22s
Val loss: 0.5078 score: 0.7727 time: 0.06s
Test loss: 0.4087 score: 0.8372 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1016;  Loss pred: 0.0759; Loss self: 2.5689; time: 0.21s
Val loss: 0.5278 score: 0.7727 time: 0.08s
Test loss: 0.4156 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.0982;  Loss pred: 0.0725; Loss self: 2.5737; time: 0.22s
Val loss: 0.5549 score: 0.7727 time: 0.12s
Test loss: 0.4284 score: 0.8372 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.0958;  Loss pred: 0.0700; Loss self: 2.5799; time: 0.28s
Val loss: 0.5807 score: 0.7727 time: 0.11s
Test loss: 0.4441 score: 0.8140 time: 0.28s
     INFO: Early stopping counter 12 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.0940;  Loss pred: 0.0681; Loss self: 2.5867; time: 0.32s
Val loss: 0.5992 score: 0.7727 time: 0.12s
Test loss: 0.4564 score: 0.8140 time: 0.13s
     INFO: Early stopping counter 13 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.0924;  Loss pred: 0.0664; Loss self: 2.5927; time: 0.21s
Val loss: 0.6065 score: 0.7727 time: 0.10s
Test loss: 0.4600 score: 0.8140 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0906;  Loss pred: 0.0646; Loss self: 2.5973; time: 0.16s
Val loss: 0.6035 score: 0.7727 time: 0.22s
Test loss: 0.4547 score: 0.8140 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0885;  Loss pred: 0.0625; Loss self: 2.6007; time: 0.21s
Val loss: 0.5953 score: 0.7727 time: 0.06s
Test loss: 0.4453 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0864;  Loss pred: 0.0604; Loss self: 2.6029; time: 0.21s
Val loss: 0.5851 score: 0.7727 time: 0.05s
Test loss: 0.4356 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 17 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0845;  Loss pred: 0.0584; Loss self: 2.6044; time: 0.19s
Val loss: 0.5783 score: 0.7727 time: 0.07s
Test loss: 0.4287 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0826;  Loss pred: 0.0565; Loss self: 2.6059; time: 0.21s
Val loss: 0.5762 score: 0.7727 time: 0.05s
Test loss: 0.4251 score: 0.8605 time: 0.06s
     INFO: Early stopping counter 19 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0807;  Loss pred: 0.0547; Loss self: 2.6081; time: 0.15s
Val loss: 0.5792 score: 0.7727 time: 0.05s
Test loss: 0.4243 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 020,   Train_Loss: 0.1640,   Val_Loss: 0.4437,   Val_Precision: 0.9333,   Val_Recall: 0.6364,   Val_accuracy: 0.7568,   Val_Score: 0.7955,   Val_Loss: 0.4437,   Test_Precision: 0.8947,   Test_Recall: 0.7727,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.4058


[0.04782959702424705, 0.050653373007662594, 0.051212672027759254, 0.04921023000497371, 0.05084794096183032, 0.0625604810193181, 0.052449023933149874, 0.05455954105127603, 0.056235497002489865, 0.053706959006376565, 0.05345787794794887, 0.05624976207036525, 0.05084289296064526, 0.05360714392736554, 0.049973808927461505, 0.05574840400367975, 0.0834277420071885, 0.1558884660480544, 0.07165373605675995, 0.14746161398943514, 0.04857453296426684, 0.07051960797980428, 0.09585125301964581, 0.11159280897118151, 0.10050046490505338, 0.06722288602031767, 0.050431355950422585, 0.056090947007760406, 0.047943184967152774, 0.05003216001205146, 0.04830343194771558, 0.04901265096850693, 0.05866874102503061, 0.06754677195567638, 0.049702938995324075, 0.05242571095004678, 0.05417458596639335, 0.13587625592481345, 0.051274894969537854, 0.047381043899804354, 0.04829341603908688, 0.05071757407858968, 0.04735324694775045, 0.08831956295762211, 0.04976393701508641, 0.09417908894829452, 0.04793416301254183, 0.04864169005304575, 0.05052545398939401, 0.051859880913980305, 0.04888797691091895, 0.10423387598711997, 0.048510866006836295, 0.04852380196098238, 0.083586864056997, 0.04935214307624847, 0.04920945188496262, 0.04998316103592515, 0.049601098988205194, 0.05042179999873042, 0.07228852494154125, 0.048453439958393574, 0.04830464906990528, 0.050713525037281215, 0.07829408103134483, 0.04830439400393516, 0.05069357296451926, 0.05001788097433746, 0.04819686699192971, 0.04928215395193547, 0.04970421805046499, 0.049556590034626424, 0.04776971903629601, 0.06625933200120926, 0.04819887294434011, 0.04689954896457493, 0.050580163951963186, 0.10604070697445422, 0.04683195205871016, 0.04879824002273381, 0.048886500066146255, 0.05058409401681274, 0.0543882769998163, 0.0544982529245317, 0.053063066909089684, 0.05059633497148752, 0.04997894400730729, 0.09031052107457072, 0.05080473702400923, 0.056812655995599926, 0.056568437023088336, 0.055580619955435395, 0.051179231028072536, 0.04959268600214273, 0.11100283998530358, 0.11107384401839226, 0.0479171919869259, 0.05525230697821826, 0.05544752103742212, 0.08319383603520691, 0.25909502897411585, 0.1599519489100203, 0.3356217349646613, 0.09109882998745888, 0.09565883700270206, 0.09440782002639025, 0.10457286890596151, 0.060060447081923485, 0.29119928600266576, 0.1304727609967813, 0.135388775030151, 0.053949700901284814, 0.1072016020771116, 0.10723388905171305, 0.05262035201303661, 0.06000856193713844, 0.056410338962450624]
[0.0010870362960056148, 0.0011512130229014226, 0.0011639243642672557, 0.0011184143182948571, 0.00115563502185978, 0.0014218291140754115, 0.0011920232712079517, 0.0012399895693471824, 0.001278079477329315, 0.0012206127046903764, 0.0012149517715442926, 0.0012784036834173921, 0.0011555202945601195, 0.0012183441801673987, 0.0011357683847150342, 0.0012670091819018126, 0.0018960850456179205, 0.003542919682910327, 0.001628494001289999, 0.0033514003179417077, 0.0011039666582787918, 0.0016027183631773698, 0.002178437568628314, 0.002536200203890489, 0.0022841014751148496, 0.001527792864098129, 0.0011461671806914223, 0.0012747942501763728, 0.001089617840162563, 0.0011370945457284424, 0.0010978052715389904, 0.0011139238856478848, 0.001333380477841605, 0.001535153908083554, 0.001129612249893729, 0.0011914934306828814, 0.0012312405901453035, 0.003088096725563942, 0.0011653385220349512, 0.0010768419068137353, 0.0010975776372519745, 0.0011526721381497655, 0.0010762101579034193, 0.0020072627944914116, 0.0011309985685246911, 0.0021404338397339666, 0.001089412795739587, 0.0011054929557510397, 0.0011483057724862274, 0.001178633657135916, 0.001111090384339067, 0.0023689517269799994, 0.0011025196819735522, 0.0011028136809314178, 0.001899701455840841, 0.0011216396153692833, 0.0011183966337491504, 0.0011359809326346624, 0.0011272977042773907, 0.0011459499999711459, 0.0016429210213986648, 0.0011012145445089448, 0.0010978329334069383, 0.001152580114483664, 0.0017794109325305644, 0.0010978271364530717, 0.0011521266582845287, 0.0011367700221440332, 0.0010953833407256752, 0.0011200489534530789, 0.0011296413193287497, 0.0011262861371506006, 0.0010856754326430912, 0.0015058939091183922, 0.0010954289305531843, 0.0010658988401039758, 0.001176282882603795, 0.002466062952894284, 0.0010891151641560502, 0.0011348427912263677, 0.0011368953503754942, 0.0011763742794607614, 0.0012648436511585185, 0.0012674012308030627, 0.001234024811839295, 0.0011766589528252912, 0.001162301023425751, 0.002100244676152807, 0.001181505512186261, 0.0013212245580372076, 0.001315545047048566, 0.0012925725571031486, 0.0011902146750714542, 0.0011533182791195983, 0.0025814613950070602, 0.002583112651590518, 0.0011143533020215326, 0.0012849373715864711, 0.0012894772334284214, 0.0019347403729117888, 0.006025465790095718, 0.003719812765349309, 0.0078051566270851466, 0.0021185774415688114, 0.0022246241163419085, 0.0021955306982881454, 0.00243192718385957, 0.0013967545833005462, 0.006772076418666646, 0.0030342502557390997, 0.003148576163491884, 0.0012546442070066235, 0.0024930605134212, 0.0024938113732956525, 0.0012237291165822467, 0.0013955479520264753, 0.001311868347963968]
[919.9324840160027, 868.6489642722094, 859.1623568508649, 894.1230308322658, 865.3251079139898, 703.3194004121101, 838.909796607106, 806.4583966835062, 782.4239554253761, 819.260684537658, 823.0779389118688, 782.225530926842, 865.4110228160704, 820.7861261852964, 880.4612044654697, 789.260262896418, 527.402503548625, 282.25308206212316, 614.0642822189444, 298.38273710439904, 905.8244581037552, 623.9399404007027, 459.0445989368725, 394.2906393848627, 437.80892000418584, 654.5389911807906, 872.4730709849435, 784.4403125144673, 917.752961763922, 879.4343476156442, 910.9083604582447, 897.7274056910774, 749.9734821517254, 651.4004848206873, 885.2595216580534, 839.2828481033836, 812.1889482883164, 323.8240537356815, 858.1197489754035, 928.641422359664, 911.09728010104, 867.5493810453073, 929.1865465645805, 498.19087104305845, 884.1744170414206, 467.1950057210316, 917.9256971377081, 904.5738326940574, 870.8481869205215, 848.4400508551602, 900.0167889985393, 422.1276392469279, 907.0132863387636, 906.7714857829991, 526.3985016831934, 891.5519622323296, 894.1371690718929, 880.2964656111928, 887.0771192078406, 872.6384222917048, 608.6719854303598, 908.0882603543176, 910.8854084898599, 867.6186474447226, 561.9837338966239, 910.8902183187621, 867.9601264404043, 879.6854073561205, 912.922410648965, 892.8181191697279, 885.2367409809473, 887.8738421924531, 921.08559329328, 664.0574040075892, 912.884416422165, 938.1753336953182, 850.1356389599252, 405.50465219322734, 918.1765463479658, 881.179320810903, 879.5884332447306, 850.0695887863089, 790.6115503556996, 789.0161187285344, 810.3564777676677, 849.8639283701424, 860.3623156526293, 476.13500053326317, 846.3777694524651, 756.8736093473661, 760.1412070559702, 773.6509602533662, 840.1845658136967, 867.0633407140343, 387.37747615910604, 387.1298448343951, 897.3814661704813, 778.2480470354224, 775.5080695308063, 516.8652156128824, 165.9622732642076, 268.83073506149844, 128.1204270174207, 472.01484372433316, 449.5141415819783, 455.470743715722, 411.1965220985598, 715.945386509482, 147.66519722718812, 329.5707063412323, 317.60387809420627, 797.0387097915487, 401.1134084457945, 400.99263749786644, 817.1743128846196, 716.5644136755744, 762.2716117451948]
Elapsed: 0.07108953127410644~0.04483110482995242
Time per graph: 0.001632313147503219~0.0010434768634011843
Speed: 728.6381387688702~212.74959985531595
Total Time: 0.0569
best val loss: 0.4437254071235657 test_score: 0.8372

Testing...
Test loss: 0.5176 score: 0.7442 time: 0.08s
test Score 0.7442
Epoch Time List: [0.22410397906787694, 0.24005058605689555, 0.22047194314654917, 0.22293940710369498, 0.2426389609463513, 0.26818261994048953, 0.22790477704256773, 0.2379904551198706, 0.24569437897298485, 0.2326085219392553, 0.24761512107215822, 0.24713366094511002, 0.24122760398313403, 0.28028166806325316, 0.24307807092554867, 0.25480596988927573, 0.2626666750293225, 0.40247902495320886, 0.350597141077742, 0.3482417459599674, 0.24426231195684522, 0.24230172904208302, 0.34211287822108716, 0.3393531389301643, 0.3453452681424096, 0.40126782294828445, 0.2471999649424106, 0.22506614995654672, 0.22632351610809565, 0.22091501613613218, 0.2179763870080933, 0.22860757098533213, 0.2308259770506993, 0.33429908589459956, 0.4431022119242698, 0.23228058207314461, 0.2514903690898791, 0.3234496890800074, 0.251804664847441, 0.21424264775123447, 0.21609029395040125, 0.21549889107700437, 0.2140648189233616, 0.345238043111749, 0.25825428508687764, 0.3322947120759636, 0.2247628989862278, 0.2230896418914199, 0.25469822518061846, 0.24396134493872523, 0.24199334089644253, 0.3737288099946454, 0.28311040205881, 0.24715489288792014, 0.28734521102160215, 0.3884050069609657, 0.5079300219658762, 0.22047395398840308, 0.2231623261468485, 0.2218177969334647, 0.24567646405193955, 0.2195744999917224, 0.29268545284867287, 0.27583142986986786, 0.27610218594782054, 0.2886070170206949, 0.2250947969732806, 0.22096336202230304, 0.22205777000635862, 0.2704197319690138, 0.24121928284876049, 0.21853977802675217, 0.23007072205655277, 0.24661271797958761, 0.21963485202286392, 0.2100751670077443, 0.2266124370507896, 0.28210726380348206, 0.22050196304917336, 0.2169948008377105, 0.22426082100719213, 0.22718248597811908, 0.28917216695845127, 0.2510133710457012, 0.24989420198835433, 0.2465580920688808, 0.2516431080875918, 0.27441631292458624, 0.25127004797104746, 0.2481281600194052, 0.24589193402789533, 0.2475979921873659, 0.2453465589787811, 0.29759749316144735, 0.47799290798138827, 0.41265354200731963, 0.6990582781145349, 0.4203493749955669, 0.6869461430469528, 0.5086229620501399, 0.7986205652123317, 0.6725837129633874, 0.715971818077378, 0.6568870760966092, 0.302181379054673, 0.36761406098958105, 0.39357718999963254, 0.39576980809215456, 0.6762584389653057, 0.5631099360762164, 0.44731447915546596, 0.4289510319940746, 0.3645679240580648, 0.3636852230411023, 0.31300375901628286, 0.32298517099116, 0.24861783301457763]
Total Epoch List: [76, 41]
Total Time List: [0.04752644000109285, 0.056944590993225574]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
emb dim:  64
Model: Ethident
Ethident(
  (encoder): HGATE_encoder(
    (lin1): Linear(in_features=14887, out_features=64, bias=True)
    (account_convs): ModuleList(
      (0): GATEConv()
      (1): GATConv(64, 64, heads=1)
    )
    (subgraph_conv): GATConv(64, 64, heads=1)
    (lin2): Linear(in_features=64, out_features=64, bias=True)
    (lin3): Linear(in_features=64, out_features=2, bias=True)
  )
  (proj_head_g1): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (proj_head_g2): Project_Head(
    (block): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Linear(in_features=64, out_features=64, bias=True)
      (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (5): ReLU()
    )
    (linear_shortcut): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=True)
    (3): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1003972
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a99681e6350>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.9337;  Loss pred: 0.9023; Loss self: 3.1454; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9298 score: 0.5000 time: 0.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9464 score: 0.4884 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.9337;  Loss pred: 0.9023; Loss self: 3.1454; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8501 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8855 score: 0.4884 time: 0.04s
Epoch 3/1000, LR 0.000030
Train loss: 0.8621;  Loss pred: 0.8308; Loss self: 3.1265; time: 0.16s
Val loss: 0.7513 score: 0.5682 time: 0.05s
Test loss: 0.8171 score: 0.3953 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.8197;  Loss pred: 0.7888; Loss self: 3.0889; time: 0.15s
Val loss: 0.7093 score: 0.5682 time: 0.09s
Test loss: 0.7810 score: 0.3721 time: 0.08s
Epoch 5/1000, LR 0.000090
Train loss: 0.7980;  Loss pred: 0.7667; Loss self: 3.1386; time: 0.15s
Val loss: 0.6729 score: 0.7500 time: 0.05s
Test loss: 0.7365 score: 0.4651 time: 0.05s
Epoch 6/1000, LR 0.000120
Train loss: 0.7286;  Loss pred: 0.6964; Loss self: 3.2171; time: 0.15s
Val loss: 0.6669 score: 0.6136 time: 0.05s
Test loss: 0.7106 score: 0.5349 time: 0.12s
Epoch 7/1000, LR 0.000150
Train loss: 0.6962;  Loss pred: 0.6634; Loss self: 3.2740; time: 0.16s
Val loss: 0.6600 score: 0.6136 time: 0.05s
Test loss: 0.6947 score: 0.4651 time: 0.04s
Epoch 8/1000, LR 0.000180
Train loss: 0.6852;  Loss pred: 0.6522; Loss self: 3.3045; time: 0.20s
Val loss: 0.6461 score: 0.6364 time: 0.05s
Test loss: 0.6801 score: 0.5116 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.6586;  Loss pred: 0.6254; Loss self: 3.3197; time: 0.15s
Val loss: 0.6217 score: 0.6818 time: 0.05s
Test loss: 0.6510 score: 0.6279 time: 0.04s
Epoch 10/1000, LR 0.000240
Train loss: 0.6200;  Loss pred: 0.5867; Loss self: 3.3290; time: 0.15s
Val loss: 0.6037 score: 0.7273 time: 0.05s
Test loss: 0.6248 score: 0.6977 time: 0.04s
Epoch 11/1000, LR 0.000270
Train loss: 0.5928;  Loss pred: 0.5595; Loss self: 3.3298; time: 0.15s
Val loss: 0.5928 score: 0.6818 time: 0.05s
Test loss: 0.6093 score: 0.7209 time: 0.04s
Epoch 12/1000, LR 0.000270
Train loss: 0.5657;  Loss pred: 0.5325; Loss self: 3.3151; time: 0.15s
Val loss: 0.5740 score: 0.7500 time: 0.05s
Test loss: 0.5943 score: 0.7442 time: 0.04s
Epoch 13/1000, LR 0.000270
Train loss: 0.5299;  Loss pred: 0.4971; Loss self: 3.2833; time: 0.16s
Val loss: 0.5670 score: 0.7727 time: 0.05s
Test loss: 0.5879 score: 0.6977 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.5026;  Loss pred: 0.4702; Loss self: 3.2353; time: 0.15s
Val loss: 0.5672 score: 0.7727 time: 0.04s
Test loss: 0.5832 score: 0.6744 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4836;  Loss pred: 0.4517; Loss self: 3.1830; time: 0.17s
Val loss: 0.5524 score: 0.7727 time: 0.05s
Test loss: 0.5553 score: 0.7907 time: 0.04s
Epoch 16/1000, LR 0.000270
Train loss: 0.4501;  Loss pred: 0.4188; Loss self: 3.1266; time: 0.16s
Val loss: 0.5476 score: 0.7273 time: 0.05s
Test loss: 0.5289 score: 0.7907 time: 0.04s
Epoch 17/1000, LR 0.000270
Train loss: 0.4232;  Loss pred: 0.3925; Loss self: 3.0687; time: 0.16s
Val loss: 0.5505 score: 0.7273 time: 0.05s
Test loss: 0.5165 score: 0.7674 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4019;  Loss pred: 0.3718; Loss self: 3.0111; time: 0.16s
Val loss: 0.5433 score: 0.7045 time: 0.04s
Test loss: 0.5012 score: 0.7907 time: 0.04s
Epoch 19/1000, LR 0.000270
Train loss: 0.3713;  Loss pred: 0.3417; Loss self: 2.9583; time: 0.15s
Val loss: 0.5364 score: 0.7500 time: 0.12s
Test loss: 0.4890 score: 0.8140 time: 0.07s
Epoch 20/1000, LR 0.000270
Train loss: 0.3492;  Loss pred: 0.3200; Loss self: 2.9179; time: 0.25s
Val loss: 0.5330 score: 0.7500 time: 0.09s
Test loss: 0.4702 score: 0.8372 time: 0.15s
Epoch 21/1000, LR 0.000270
Train loss: 0.3268;  Loss pred: 0.2980; Loss self: 2.8795; time: 0.15s
Val loss: 0.5425 score: 0.7273 time: 0.05s
Test loss: 0.4552 score: 0.7907 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3047;  Loss pred: 0.2762; Loss self: 2.8484; time: 0.15s
Val loss: 0.5546 score: 0.7273 time: 0.05s
Test loss: 0.4483 score: 0.8140 time: 0.04s
     INFO: Early stopping counter 2 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.2887;  Loss pred: 0.2605; Loss self: 2.8241; time: 0.15s
Val loss: 0.5429 score: 0.7273 time: 0.15s
Test loss: 0.4329 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2645;  Loss pred: 0.2365; Loss self: 2.7957; time: 0.29s
Val loss: 0.5307 score: 0.7500 time: 0.05s
Test loss: 0.4204 score: 0.8372 time: 0.04s
Epoch 25/1000, LR 0.000270
Train loss: 0.2460;  Loss pred: 0.2183; Loss self: 2.7705; time: 0.16s
Val loss: 0.5316 score: 0.7500 time: 0.11s
Test loss: 0.4109 score: 0.8372 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2279;  Loss pred: 0.2004; Loss self: 2.7535; time: 0.23s
Val loss: 0.5414 score: 0.7500 time: 0.05s
Test loss: 0.4004 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2082;  Loss pred: 0.1808; Loss self: 2.7410; time: 0.19s
Val loss: 0.5580 score: 0.7273 time: 0.07s
Test loss: 0.3945 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.1931;  Loss pred: 0.1658; Loss self: 2.7296; time: 0.16s
Val loss: 0.5589 score: 0.7500 time: 0.05s
Test loss: 0.3835 score: 0.8372 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1771;  Loss pred: 0.1500; Loss self: 2.7105; time: 0.16s
Val loss: 0.5477 score: 0.7500 time: 0.05s
Test loss: 0.3668 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 5 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1611;  Loss pred: 0.1343; Loss self: 2.6862; time: 0.20s
Val loss: 0.5427 score: 0.7500 time: 0.11s
Test loss: 0.3544 score: 0.8837 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1494;  Loss pred: 0.1227; Loss self: 2.6640; time: 0.15s
Val loss: 0.5455 score: 0.7500 time: 0.04s
Test loss: 0.3448 score: 0.8837 time: 0.04s
     INFO: Early stopping counter 7 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1383;  Loss pred: 0.1119; Loss self: 2.6428; time: 0.15s
Val loss: 0.5539 score: 0.7273 time: 0.05s
Test loss: 0.3391 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 8 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1280;  Loss pred: 0.1017; Loss self: 2.6247; time: 0.15s
Val loss: 0.5640 score: 0.7273 time: 0.06s
Test loss: 0.3373 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 9 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1187;  Loss pred: 0.0926; Loss self: 2.6102; time: 0.15s
Val loss: 0.5731 score: 0.7273 time: 0.05s
Test loss: 0.3345 score: 0.8372 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1104;  Loss pred: 0.0845; Loss self: 2.5987; time: 0.17s
Val loss: 0.5735 score: 0.7273 time: 0.05s
Test loss: 0.3290 score: 0.8372 time: 0.04s
     INFO: Early stopping counter 11 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1027;  Loss pred: 0.0768; Loss self: 2.5881; time: 0.15s
Val loss: 0.5726 score: 0.7500 time: 0.05s
Test loss: 0.3246 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0957;  Loss pred: 0.0699; Loss self: 2.5789; time: 0.20s
Val loss: 0.5723 score: 0.7500 time: 0.08s
Test loss: 0.3247 score: 0.8605 time: 0.10s
     INFO: Early stopping counter 13 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0888;  Loss pred: 0.0631; Loss self: 2.5671; time: 0.19s
Val loss: 0.5753 score: 0.7500 time: 0.06s
Test loss: 0.3270 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 14 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0810;  Loss pred: 0.0554; Loss self: 2.5559; time: 0.14s
Val loss: 0.5781 score: 0.7727 time: 0.04s
Test loss: 0.3276 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 15 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0737;  Loss pred: 0.0482; Loss self: 2.5454; time: 0.14s
Val loss: 0.5776 score: 0.7727 time: 0.05s
Test loss: 0.3286 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 16 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.0676;  Loss pred: 0.0423; Loss self: 2.5370; time: 0.15s
Val loss: 0.5779 score: 0.7727 time: 0.06s
Test loss: 0.3301 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 17 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0631;  Loss pred: 0.0377; Loss self: 2.5329; time: 0.15s
Val loss: 0.5795 score: 0.7727 time: 0.05s
Test loss: 0.3339 score: 0.8605 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0593;  Loss pred: 0.0339; Loss self: 2.5330; time: 0.15s
Val loss: 0.5840 score: 0.7727 time: 0.05s
Test loss: 0.3394 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 19 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0562;  Loss pred: 0.0308; Loss self: 2.5360; time: 0.14s
Val loss: 0.5905 score: 0.7500 time: 0.05s
Test loss: 0.3461 score: 0.8605 time: 0.04s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 023,   Train_Loss: 0.2645,   Val_Loss: 0.5307,   Val_Precision: 0.8235,   Val_Recall: 0.6364,   Val_accuracy: 0.7179,   Val_Score: 0.7500,   Val_Loss: 0.5307,   Test_Precision: 0.8889,   Test_Recall: 0.7619,   Test_accuracy: 0.8205,   Test_Score: 0.8372,   Test_loss: 0.4204


[0.04782959702424705, 0.050653373007662594, 0.051212672027759254, 0.04921023000497371, 0.05084794096183032, 0.0625604810193181, 0.052449023933149874, 0.05455954105127603, 0.056235497002489865, 0.053706959006376565, 0.05345787794794887, 0.05624976207036525, 0.05084289296064526, 0.05360714392736554, 0.049973808927461505, 0.05574840400367975, 0.0834277420071885, 0.1558884660480544, 0.07165373605675995, 0.14746161398943514, 0.04857453296426684, 0.07051960797980428, 0.09585125301964581, 0.11159280897118151, 0.10050046490505338, 0.06722288602031767, 0.050431355950422585, 0.056090947007760406, 0.047943184967152774, 0.05003216001205146, 0.04830343194771558, 0.04901265096850693, 0.05866874102503061, 0.06754677195567638, 0.049702938995324075, 0.05242571095004678, 0.05417458596639335, 0.13587625592481345, 0.051274894969537854, 0.047381043899804354, 0.04829341603908688, 0.05071757407858968, 0.04735324694775045, 0.08831956295762211, 0.04976393701508641, 0.09417908894829452, 0.04793416301254183, 0.04864169005304575, 0.05052545398939401, 0.051859880913980305, 0.04888797691091895, 0.10423387598711997, 0.048510866006836295, 0.04852380196098238, 0.083586864056997, 0.04935214307624847, 0.04920945188496262, 0.04998316103592515, 0.049601098988205194, 0.05042179999873042, 0.07228852494154125, 0.048453439958393574, 0.04830464906990528, 0.050713525037281215, 0.07829408103134483, 0.04830439400393516, 0.05069357296451926, 0.05001788097433746, 0.04819686699192971, 0.04928215395193547, 0.04970421805046499, 0.049556590034626424, 0.04776971903629601, 0.06625933200120926, 0.04819887294434011, 0.04689954896457493, 0.050580163951963186, 0.10604070697445422, 0.04683195205871016, 0.04879824002273381, 0.048886500066146255, 0.05058409401681274, 0.0543882769998163, 0.0544982529245317, 0.053063066909089684, 0.05059633497148752, 0.04997894400730729, 0.09031052107457072, 0.05080473702400923, 0.056812655995599926, 0.056568437023088336, 0.055580619955435395, 0.051179231028072536, 0.04959268600214273, 0.11100283998530358, 0.11107384401839226, 0.0479171919869259, 0.05525230697821826, 0.05544752103742212, 0.08319383603520691, 0.25909502897411585, 0.1599519489100203, 0.3356217349646613, 0.09109882998745888, 0.09565883700270206, 0.09440782002639025, 0.10457286890596151, 0.060060447081923485, 0.29119928600266576, 0.1304727609967813, 0.135388775030151, 0.053949700901284814, 0.1072016020771116, 0.10723388905171305, 0.05262035201303661, 0.06000856193713844, 0.056410338962450624, 0.04835999198257923, 0.047457832959480584, 0.04946385999210179, 0.0854573150863871, 0.05603407393209636, 0.12710130598861724, 0.04633706505410373, 0.061397743062116206, 0.04670407099183649, 0.047204673988744617, 0.047972511034458876, 0.046198073076084256, 0.05027717596385628, 0.04841596202459186, 0.04666359501425177, 0.04663160303607583, 0.046161259058862925, 0.04600217205006629, 0.07014459499623626, 0.15595569205470383, 0.04677918704692274, 0.04754864692222327, 0.058116558007895947, 0.04754623502958566, 0.11137004592455924, 0.05233913299161941, 0.05201855394989252, 0.050337883993051946, 0.04922289599198848, 0.0538201630115509, 0.04658809397369623, 0.04853696492500603, 0.046725644962862134, 0.10460943100042641, 0.04664494702592492, 0.10240732505917549, 0.10255448007956147, 0.04626862402074039, 0.0461740909377113, 0.04724646906834096, 0.04814492305740714, 0.05060125794261694, 0.04847687506116927, 0.04911859892308712]
[0.0010870362960056148, 0.0011512130229014226, 0.0011639243642672557, 0.0011184143182948571, 0.00115563502185978, 0.0014218291140754115, 0.0011920232712079517, 0.0012399895693471824, 0.001278079477329315, 0.0012206127046903764, 0.0012149517715442926, 0.0012784036834173921, 0.0011555202945601195, 0.0012183441801673987, 0.0011357683847150342, 0.0012670091819018126, 0.0018960850456179205, 0.003542919682910327, 0.001628494001289999, 0.0033514003179417077, 0.0011039666582787918, 0.0016027183631773698, 0.002178437568628314, 0.002536200203890489, 0.0022841014751148496, 0.001527792864098129, 0.0011461671806914223, 0.0012747942501763728, 0.001089617840162563, 0.0011370945457284424, 0.0010978052715389904, 0.0011139238856478848, 0.001333380477841605, 0.001535153908083554, 0.001129612249893729, 0.0011914934306828814, 0.0012312405901453035, 0.003088096725563942, 0.0011653385220349512, 0.0010768419068137353, 0.0010975776372519745, 0.0011526721381497655, 0.0010762101579034193, 0.0020072627944914116, 0.0011309985685246911, 0.0021404338397339666, 0.001089412795739587, 0.0011054929557510397, 0.0011483057724862274, 0.001178633657135916, 0.001111090384339067, 0.0023689517269799994, 0.0011025196819735522, 0.0011028136809314178, 0.001899701455840841, 0.0011216396153692833, 0.0011183966337491504, 0.0011359809326346624, 0.0011272977042773907, 0.0011459499999711459, 0.0016429210213986648, 0.0011012145445089448, 0.0010978329334069383, 0.001152580114483664, 0.0017794109325305644, 0.0010978271364530717, 0.0011521266582845287, 0.0011367700221440332, 0.0010953833407256752, 0.0011200489534530789, 0.0011296413193287497, 0.0011262861371506006, 0.0010856754326430912, 0.0015058939091183922, 0.0010954289305531843, 0.0010658988401039758, 0.001176282882603795, 0.002466062952894284, 0.0010891151641560502, 0.0011348427912263677, 0.0011368953503754942, 0.0011763742794607614, 0.0012648436511585185, 0.0012674012308030627, 0.001234024811839295, 0.0011766589528252912, 0.001162301023425751, 0.002100244676152807, 0.001181505512186261, 0.0013212245580372076, 0.001315545047048566, 0.0012925725571031486, 0.0011902146750714542, 0.0011533182791195983, 0.0025814613950070602, 0.002583112651590518, 0.0011143533020215326, 0.0012849373715864711, 0.0012894772334284214, 0.0019347403729117888, 0.006025465790095718, 0.003719812765349309, 0.0078051566270851466, 0.0021185774415688114, 0.0022246241163419085, 0.0021955306982881454, 0.00243192718385957, 0.0013967545833005462, 0.006772076418666646, 0.0030342502557390997, 0.003148576163491884, 0.0012546442070066235, 0.0024930605134212, 0.0024938113732956525, 0.0012237291165822467, 0.0013955479520264753, 0.001311868347963968, 0.001124650976339052, 0.0011036705339414088, 0.001150322325397716, 0.0019873794206136533, 0.0013031179984208457, 0.00295584432531668, 0.001077606164048924, 0.0014278544898166559, 0.0010861411858566624, 0.0010977831160173166, 0.0011156397914990437, 0.0010743737924670757, 0.0011692366503222391, 0.0011259526052230665, 0.0010851998840523667, 0.001084455884559903, 0.001073517652531696, 0.0010698179546527046, 0.001631269651075262, 0.003626876559411717, 0.0010878880708586685, 0.0011057824865633318, 0.0013515478606487428, 0.0011057263960368758, 0.0025900010680130056, 0.0012171891393399862, 0.0012097338127881982, 0.0011706484649546963, 0.0011447185114415925, 0.0012516316979430443, 0.0010834440458999124, 0.001128766626162931, 0.0010866429061130729, 0.0024327774651261955, 0.0010847662099052308, 0.002381565699050593, 0.002384987908827011, 0.0010760145121102417, 0.0010738160683188675, 0.0010987550946125804, 0.001119649373428073, 0.0011767734405259754, 0.0011273691874690527, 0.0011422929982113283]
[919.9324840160027, 868.6489642722094, 859.1623568508649, 894.1230308322658, 865.3251079139898, 703.3194004121101, 838.909796607106, 806.4583966835062, 782.4239554253761, 819.260684537658, 823.0779389118688, 782.225530926842, 865.4110228160704, 820.7861261852964, 880.4612044654697, 789.260262896418, 527.402503548625, 282.25308206212316, 614.0642822189444, 298.38273710439904, 905.8244581037552, 623.9399404007027, 459.0445989368725, 394.2906393848627, 437.80892000418584, 654.5389911807906, 872.4730709849435, 784.4403125144673, 917.752961763922, 879.4343476156442, 910.9083604582447, 897.7274056910774, 749.9734821517254, 651.4004848206873, 885.2595216580534, 839.2828481033836, 812.1889482883164, 323.8240537356815, 858.1197489754035, 928.641422359664, 911.09728010104, 867.5493810453073, 929.1865465645805, 498.19087104305845, 884.1744170414206, 467.1950057210316, 917.9256971377081, 904.5738326940574, 870.8481869205215, 848.4400508551602, 900.0167889985393, 422.1276392469279, 907.0132863387636, 906.7714857829991, 526.3985016831934, 891.5519622323296, 894.1371690718929, 880.2964656111928, 887.0771192078406, 872.6384222917048, 608.6719854303598, 908.0882603543176, 910.8854084898599, 867.6186474447226, 561.9837338966239, 910.8902183187621, 867.9601264404043, 879.6854073561205, 912.922410648965, 892.8181191697279, 885.2367409809473, 887.8738421924531, 921.08559329328, 664.0574040075892, 912.884416422165, 938.1753336953182, 850.1356389599252, 405.50465219322734, 918.1765463479658, 881.179320810903, 879.5884332447306, 850.0695887863089, 790.6115503556996, 789.0161187285344, 810.3564777676677, 849.8639283701424, 860.3623156526293, 476.13500053326317, 846.3777694524651, 756.8736093473661, 760.1412070559702, 773.6509602533662, 840.1845658136967, 867.0633407140343, 387.37747615910604, 387.1298448343951, 897.3814661704813, 778.2480470354224, 775.5080695308063, 516.8652156128824, 165.9622732642076, 268.83073506149844, 128.1204270174207, 472.01484372433316, 449.5141415819783, 455.470743715722, 411.1965220985598, 715.945386509482, 147.66519722718812, 329.5707063412323, 317.60387809420627, 797.0387097915487, 401.1134084457945, 400.99263749786644, 817.1743128846196, 716.5644136755744, 762.2716117451948, 889.1647462532652, 906.0674986300644, 869.3215613756404, 503.1751811595316, 767.3902142490762, 338.31281012841, 927.9828135379888, 700.3514763807657, 920.690618329954, 910.926744462907, 896.3466592172526, 930.774751777692, 855.2588560445845, 888.136849953721, 921.4892248843482, 922.1214198176654, 931.5170529722376, 934.7384717660964, 613.0194351012682, 275.71933690574815, 919.212211979401, 904.3369850321161, 739.8924071545642, 904.3828596153458, 386.10022688800626, 821.5650038926934, 826.6281304439999, 854.2274046707092, 873.5772069769865, 798.957074707695, 922.982597748642, 885.922720269775, 920.2655208756711, 411.05280459679324, 921.8576232083813, 419.8918385491731, 419.28933740038195, 929.3554954373574, 931.258182386457, 910.1209222175199, 893.1367477465396, 849.7812455327306, 887.0208722352995, 875.4321365585367]
Elapsed: 0.06795411651754486~0.040712194582450734
Time per graph: 0.0015651170078496259~0.0009463845384070248
Speed: 747.5740094101293~208.87403707713656
Total Time: 0.0498
best val loss: 0.5306521058082581 test_score: 0.8372

Testing...
Test loss: 0.5879 score: 0.6977 time: 0.05s
test Score 0.6977
Epoch Time List: [0.22410397906787694, 0.24005058605689555, 0.22047194314654917, 0.22293940710369498, 0.2426389609463513, 0.26818261994048953, 0.22790477704256773, 0.2379904551198706, 0.24569437897298485, 0.2326085219392553, 0.24761512107215822, 0.24713366094511002, 0.24122760398313403, 0.28028166806325316, 0.24307807092554867, 0.25480596988927573, 0.2626666750293225, 0.40247902495320886, 0.350597141077742, 0.3482417459599674, 0.24426231195684522, 0.24230172904208302, 0.34211287822108716, 0.3393531389301643, 0.3453452681424096, 0.40126782294828445, 0.2471999649424106, 0.22506614995654672, 0.22632351610809565, 0.22091501613613218, 0.2179763870080933, 0.22860757098533213, 0.2308259770506993, 0.33429908589459956, 0.4431022119242698, 0.23228058207314461, 0.2514903690898791, 0.3234496890800074, 0.251804664847441, 0.21424264775123447, 0.21609029395040125, 0.21549889107700437, 0.2140648189233616, 0.345238043111749, 0.25825428508687764, 0.3322947120759636, 0.2247628989862278, 0.2230896418914199, 0.25469822518061846, 0.24396134493872523, 0.24199334089644253, 0.3737288099946454, 0.28311040205881, 0.24715489288792014, 0.28734521102160215, 0.3884050069609657, 0.5079300219658762, 0.22047395398840308, 0.2231623261468485, 0.2218177969334647, 0.24567646405193955, 0.2195744999917224, 0.29268545284867287, 0.27583142986986786, 0.27610218594782054, 0.2886070170206949, 0.2250947969732806, 0.22096336202230304, 0.22205777000635862, 0.2704197319690138, 0.24121928284876049, 0.21853977802675217, 0.23007072205655277, 0.24661271797958761, 0.21963485202286392, 0.2100751670077443, 0.2266124370507896, 0.28210726380348206, 0.22050196304917336, 0.2169948008377105, 0.22426082100719213, 0.22718248597811908, 0.28917216695845127, 0.2510133710457012, 0.24989420198835433, 0.2465580920688808, 0.2516431080875918, 0.27441631292458624, 0.25127004797104746, 0.2481281600194052, 0.24589193402789533, 0.2475979921873659, 0.2453465589787811, 0.29759749316144735, 0.47799290798138827, 0.41265354200731963, 0.6990582781145349, 0.4203493749955669, 0.6869461430469528, 0.5086229620501399, 0.7986205652123317, 0.6725837129633874, 0.715971818077378, 0.6568870760966092, 0.302181379054673, 0.36761406098958105, 0.39357718999963254, 0.39576980809215456, 0.6762584389653057, 0.5631099360762164, 0.44731447915546596, 0.4289510319940746, 0.3645679240580648, 0.3636852230411023, 0.31300375901628286, 0.32298517099116, 0.24861783301457763, 0.25413407711312175, 0.2543291209731251, 0.25304441596381366, 0.3235672051087022, 0.26031947392039, 0.32183521694969386, 0.25195504690054804, 0.30743987997993827, 0.2423977330327034, 0.2451049069641158, 0.23739627399481833, 0.24394395586568862, 0.2508642988977954, 0.235448727151379, 0.2566605501342565, 0.2602468020049855, 0.24528080713935196, 0.24338462192099541, 0.3308799680089578, 0.4992762439651415, 0.2405686560086906, 0.2367042900295928, 0.34964471915736794, 0.3831402088981122, 0.3698151661083102, 0.3361441600136459, 0.3055791408987716, 0.2589436920825392, 0.25395523593761027, 0.36389808903913945, 0.2405118850292638, 0.24207308096811175, 0.24990169203374535, 0.2990425219759345, 0.26051951793488115, 0.2926489688688889, 0.37867823196575046, 0.2858371860347688, 0.22656984708737582, 0.23379307298455387, 0.2567049029748887, 0.2446664390154183, 0.23890708608087152, 0.23712072288617492]
Total Epoch List: [76, 41, 44]
Total Time List: [0.04752644000109285, 0.056944590993225574, 0.04984876897651702]
T-times Epoch Time: 0.29756990901761055 ~ 0.0019735987796003387
T-times Total Epoch: 56.888888888888886 ~ 3.446236093171293
T-times Total Time: 0.06328133655349828 ~ 0.01233611585434135
T-times Inference Elapsed: 0.06681922682198775 ~ 0.0008047723120391205
T-times Time Per Graph: 0.0015407404466488445 ~ 1.724041774028065e-05
T-times Speed: 722.9911357362157 ~ 17.72206698682712
T-times cross validation test micro f1 score:0.7895849257637876 ~ 0.025030573182941017
T-times cross validation test precision:0.8746684248748231 ~ 0.023802484954513974
T-times cross validation test recall:0.7236652236652237 ~ 0.033178575009101555
T-times cross validation test f1_score:0.7895849257637876 ~ 0.028745257141246393
