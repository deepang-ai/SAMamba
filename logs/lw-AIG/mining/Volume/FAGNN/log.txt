Namespace(seed=15, model='FAGNN', dataset='mining/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Volume/seed15/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=2, training_times=10, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 124], edge_attr=[124, 2], x=[30, 14887], y=[1, 1], num_nodes=32)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288336f20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.46s
Epoch 2/1000, LR 0.000000
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.96s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.27s
Epoch 3/1000, LR 0.000030
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.26s
Epoch 4/1000, LR 0.000060
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.47s
Epoch 5/1000, LR 0.000090
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.26s
Epoch 6/1000, LR 0.000120
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.46s
Epoch 7/1000, LR 0.000150
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.49s
Epoch 8/1000, LR 0.000180
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.48s
Epoch 9/1000, LR 0.000210
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.48s
Epoch 11/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.30s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.26s
Epoch 13/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.30s
Epoch 14/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.59s
Val loss: 0.6926 score: 0.5349 time: 0.42s
Test loss: 0.6922 score: 0.5227 time: 0.29s
Epoch 15/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.32s
Val loss: 0.6925 score: 0.6512 time: 0.74s
Test loss: 0.6921 score: 0.7500 time: 0.49s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.89s
Val loss: 0.6925 score: 0.6279 time: 0.47s
Test loss: 0.6920 score: 0.6364 time: 0.40s
Epoch 17/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4884 time: 0.44s
Test loss: 0.6918 score: 0.6136 time: 0.46s
Epoch 18/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.35s
Test loss: 0.6916 score: 0.5455 time: 0.45s
Epoch 19/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.33s
Test loss: 0.6914 score: 0.5455 time: 0.33s
Epoch 20/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.26s
Test loss: 0.6912 score: 0.5455 time: 0.27s
Epoch 21/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.46s
Test loss: 0.6909 score: 0.5455 time: 0.29s
Epoch 22/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4884 time: 0.47s
Test loss: 0.6907 score: 0.5455 time: 0.29s
Epoch 23/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4884 time: 0.66s
Test loss: 0.6903 score: 0.5455 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.4884 time: 0.29s
Test loss: 0.6900 score: 0.5455 time: 2.16s
Epoch 25/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 10.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.4884 time: 0.26s
Test loss: 0.6896 score: 0.5682 time: 0.27s
Epoch 26/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 10.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4884 time: 2.15s
Test loss: 0.6892 score: 0.5909 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4884 time: 0.36s
Test loss: 0.6888 score: 0.6364 time: 0.54s
Epoch 28/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.83s
Val loss: 0.6903 score: 0.5116 time: 0.48s
Test loss: 0.6883 score: 0.6591 time: 0.29s
Epoch 29/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.53s
Val loss: 0.6898 score: 0.5814 time: 0.24s
Test loss: 0.6877 score: 0.6591 time: 0.51s
Epoch 30/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.77s
Val loss: 0.6894 score: 0.6047 time: 0.34s
Test loss: 0.6871 score: 0.6818 time: 0.48s
Epoch 31/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.75s
Val loss: 0.6888 score: 0.6744 time: 0.72s
Test loss: 0.6864 score: 0.7045 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.50s
Val loss: 0.6882 score: 0.6977 time: 0.30s
Test loss: 0.6857 score: 0.7045 time: 0.55s
Epoch 33/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.92s
Val loss: 0.6875 score: 0.6977 time: 0.37s
Test loss: 0.6849 score: 0.7273 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.59s
Val loss: 0.6868 score: 0.7209 time: 0.30s
Test loss: 0.6840 score: 0.7045 time: 0.66s
Epoch 35/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.57s
Val loss: 0.6860 score: 0.7209 time: 0.30s
Test loss: 0.6830 score: 0.7273 time: 0.43s
Epoch 36/1000, LR 0.000270
Train loss: 0.6826;  Loss pred: 0.6826; Loss self: 0.0000; time: 0.55s
Val loss: 0.6851 score: 0.7674 time: 0.31s
Test loss: 0.6819 score: 0.7273 time: 0.48s
Epoch 37/1000, LR 0.000270
Train loss: 0.6816;  Loss pred: 0.6816; Loss self: 0.0000; time: 0.81s
Val loss: 0.6841 score: 0.7442 time: 0.63s
Test loss: 0.6808 score: 0.7500 time: 0.28s
Epoch 38/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.88s
Val loss: 0.6830 score: 0.7442 time: 0.45s
Test loss: 0.6795 score: 0.7500 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.76s
Val loss: 0.6817 score: 0.7674 time: 0.56s
Test loss: 0.6781 score: 0.8182 time: 0.46s
Epoch 40/1000, LR 0.000269
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.44s
Val loss: 0.6804 score: 0.7907 time: 0.42s
Test loss: 0.6765 score: 0.8182 time: 0.63s
Epoch 41/1000, LR 0.000269
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.86s
Val loss: 0.6789 score: 0.7907 time: 0.53s
Test loss: 0.6749 score: 0.8182 time: 0.49s
Epoch 42/1000, LR 0.000269
Train loss: 0.6734;  Loss pred: 0.6734; Loss self: 0.0000; time: 1.09s
Val loss: 0.6772 score: 0.7674 time: 0.52s
Test loss: 0.6730 score: 0.7955 time: 0.28s
Epoch 43/1000, LR 0.000269
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.73s
Val loss: 0.6754 score: 0.7674 time: 0.52s
Test loss: 0.6710 score: 0.7955 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.54s
Val loss: 0.6734 score: 0.7674 time: 0.30s
Test loss: 0.6689 score: 0.7955 time: 0.49s
Epoch 45/1000, LR 0.000269
Train loss: 0.6648;  Loss pred: 0.6648; Loss self: 0.0000; time: 0.49s
Val loss: 0.6712 score: 0.7674 time: 0.33s
Test loss: 0.6666 score: 0.8409 time: 0.67s
Epoch 46/1000, LR 0.000269
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.66s
Val loss: 0.6688 score: 0.7674 time: 0.52s
Test loss: 0.6641 score: 0.8409 time: 0.26s
Epoch 47/1000, LR 0.000269
Train loss: 0.6596;  Loss pred: 0.6596; Loss self: 0.0000; time: 0.64s
Val loss: 0.6662 score: 0.7907 time: 0.47s
Test loss: 0.6614 score: 0.8409 time: 0.59s
Epoch 48/1000, LR 0.000269
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 0.98s
Val loss: 0.6633 score: 0.8140 time: 0.67s
Test loss: 0.6585 score: 0.8182 time: 0.56s
Epoch 49/1000, LR 0.000269
Train loss: 0.6524;  Loss pred: 0.6524; Loss self: 0.0000; time: 0.62s
Val loss: 0.6601 score: 0.8140 time: 0.72s
Test loss: 0.6554 score: 0.8182 time: 0.28s
Epoch 50/1000, LR 0.000269
Train loss: 0.6487;  Loss pred: 0.6487; Loss self: 0.0000; time: 0.57s
Val loss: 0.6567 score: 0.8140 time: 0.30s
Test loss: 0.6521 score: 0.8182 time: 0.64s
Epoch 51/1000, LR 0.000269
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.56s
Val loss: 0.6531 score: 0.8140 time: 0.24s
Test loss: 0.6485 score: 0.8182 time: 0.28s
Epoch 52/1000, LR 0.000269
Train loss: 0.6407;  Loss pred: 0.6407; Loss self: 0.0000; time: 0.82s
Val loss: 0.6491 score: 0.8140 time: 0.47s
Test loss: 0.6446 score: 0.8409 time: 0.46s
Epoch 53/1000, LR 0.000269
Train loss: 0.6359;  Loss pred: 0.6359; Loss self: 0.0000; time: 0.50s
Val loss: 0.6450 score: 0.8140 time: 0.39s
Test loss: 0.6405 score: 0.8409 time: 0.37s
Epoch 54/1000, LR 0.000269
Train loss: 0.6278;  Loss pred: 0.6278; Loss self: 0.0000; time: 0.91s
Val loss: 0.6406 score: 0.8140 time: 0.26s
Test loss: 0.6360 score: 0.8409 time: 0.73s
Epoch 55/1000, LR 0.000269
Train loss: 0.6200;  Loss pred: 0.6200; Loss self: 0.0000; time: 0.84s
Val loss: 0.6358 score: 0.8140 time: 0.55s
Test loss: 0.6313 score: 0.8409 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 0.57s
Val loss: 0.6308 score: 0.8140 time: 0.53s
Test loss: 0.6261 score: 0.8409 time: 0.50s
Epoch 57/1000, LR 0.000269
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 1.84s
Val loss: 0.6255 score: 0.8372 time: 0.53s
Test loss: 0.6206 score: 0.8409 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6017;  Loss pred: 0.6017; Loss self: 0.0000; time: 0.81s
Val loss: 0.6198 score: 0.8372 time: 0.28s
Test loss: 0.6148 score: 0.8409 time: 0.47s
Epoch 59/1000, LR 0.000268
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.54s
Val loss: 0.6139 score: 0.8372 time: 0.27s
Test loss: 0.6087 score: 0.8409 time: 0.22s
Epoch 60/1000, LR 0.000268
Train loss: 0.5829;  Loss pred: 0.5829; Loss self: 0.0000; time: 1.41s
Val loss: 0.6076 score: 0.8372 time: 0.76s
Test loss: 0.6023 score: 0.8409 time: 0.30s
Epoch 61/1000, LR 0.000268
Train loss: 0.5748;  Loss pred: 0.5748; Loss self: 0.0000; time: 0.58s
Val loss: 0.6011 score: 0.8372 time: 0.47s
Test loss: 0.5956 score: 0.8409 time: 0.47s
Epoch 62/1000, LR 0.000268
Train loss: 0.5710;  Loss pred: 0.5710; Loss self: 0.0000; time: 1.12s
Val loss: 0.5942 score: 0.8372 time: 0.27s
Test loss: 0.5885 score: 0.8409 time: 0.28s
Epoch 63/1000, LR 0.000268
Train loss: 0.5577;  Loss pred: 0.5577; Loss self: 0.0000; time: 0.83s
Val loss: 0.5868 score: 0.8372 time: 0.45s
Test loss: 0.5810 score: 0.8636 time: 0.39s
Epoch 64/1000, LR 0.000268
Train loss: 0.5459;  Loss pred: 0.5459; Loss self: 0.0000; time: 0.91s
Val loss: 0.5790 score: 0.8372 time: 0.31s
Test loss: 0.5732 score: 0.8636 time: 0.44s
Epoch 65/1000, LR 0.000268
Train loss: 0.5400;  Loss pred: 0.5400; Loss self: 0.0000; time: 1.28s
Val loss: 0.5707 score: 0.8372 time: 0.35s
Test loss: 0.5650 score: 0.8409 time: 0.49s
Epoch 66/1000, LR 0.000268
Train loss: 0.5284;  Loss pred: 0.5284; Loss self: 0.0000; time: 0.78s
Val loss: 0.5620 score: 0.8372 time: 0.45s
Test loss: 0.5565 score: 0.8409 time: 0.56s
Epoch 67/1000, LR 0.000268
Train loss: 0.5095;  Loss pred: 0.5095; Loss self: 0.0000; time: 1.03s
Val loss: 0.5530 score: 0.8372 time: 0.48s
Test loss: 0.5479 score: 0.8409 time: 0.27s
Epoch 68/1000, LR 0.000268
Train loss: 0.5013;  Loss pred: 0.5013; Loss self: 0.0000; time: 0.51s
Val loss: 0.5437 score: 0.8372 time: 0.27s
Test loss: 0.5391 score: 0.8409 time: 0.34s
Epoch 69/1000, LR 0.000268
Train loss: 0.4886;  Loss pred: 0.4886; Loss self: 0.0000; time: 0.37s
Val loss: 0.5342 score: 0.8372 time: 3.83s
Test loss: 0.5301 score: 0.8409 time: 3.86s
Epoch 70/1000, LR 0.000268
Train loss: 0.4688;  Loss pred: 0.4688; Loss self: 0.0000; time: 4.41s
Val loss: 0.5245 score: 0.8372 time: 0.32s
Test loss: 0.5210 score: 0.8409 time: 1.01s
Epoch 71/1000, LR 0.000268
Train loss: 0.4537;  Loss pred: 0.4537; Loss self: 0.0000; time: 10.35s
Val loss: 0.5146 score: 0.8140 time: 0.87s
Test loss: 0.5119 score: 0.8409 time: 0.39s
Epoch 72/1000, LR 0.000267
Train loss: 0.4452;  Loss pred: 0.4452; Loss self: 0.0000; time: 0.88s
Val loss: 0.5046 score: 0.8140 time: 0.27s
Test loss: 0.5027 score: 0.8409 time: 0.32s
Epoch 73/1000, LR 0.000267
Train loss: 0.4355;  Loss pred: 0.4355; Loss self: 0.0000; time: 0.99s
Val loss: 0.4945 score: 0.8140 time: 0.45s
Test loss: 0.4935 score: 0.8409 time: 0.29s
Epoch 74/1000, LR 0.000267
Train loss: 0.4064;  Loss pred: 0.4064; Loss self: 0.0000; time: 0.84s
Val loss: 0.4843 score: 0.8372 time: 0.27s
Test loss: 0.4843 score: 0.8409 time: 0.29s
Epoch 75/1000, LR 0.000267
Train loss: 0.4105;  Loss pred: 0.4105; Loss self: 0.0000; time: 1.03s
Val loss: 0.4744 score: 0.8372 time: 0.29s
Test loss: 0.4752 score: 0.8409 time: 0.47s
Epoch 76/1000, LR 0.000267
Train loss: 0.3956;  Loss pred: 0.3956; Loss self: 0.0000; time: 0.40s
Val loss: 0.4647 score: 0.8372 time: 0.93s
Test loss: 0.4665 score: 0.8409 time: 0.29s
Epoch 77/1000, LR 0.000267
Train loss: 0.3700;  Loss pred: 0.3700; Loss self: 0.0000; time: 0.86s
Val loss: 0.4554 score: 0.8372 time: 0.40s
Test loss: 0.4581 score: 0.8409 time: 0.30s
Epoch 78/1000, LR 0.000267
Train loss: 0.3417;  Loss pred: 0.3417; Loss self: 0.0000; time: 0.66s
Val loss: 0.4464 score: 0.8605 time: 0.50s
Test loss: 0.4501 score: 0.8409 time: 0.42s
Epoch 79/1000, LR 0.000267
Train loss: 0.3334;  Loss pred: 0.3334; Loss self: 0.0000; time: 0.54s
Val loss: 0.4381 score: 0.8372 time: 0.28s
Test loss: 0.4428 score: 0.8409 time: 0.32s
Epoch 80/1000, LR 0.000267
Train loss: 0.3412;  Loss pred: 0.3412; Loss self: 0.0000; time: 1.00s
Val loss: 0.4305 score: 0.8372 time: 0.74s
Test loss: 0.4361 score: 0.8409 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.2935;  Loss pred: 0.2935; Loss self: 0.0000; time: 1.70s
Val loss: 0.4236 score: 0.8372 time: 0.25s
Test loss: 0.4302 score: 0.8636 time: 0.26s
Epoch 82/1000, LR 0.000267
Train loss: 0.3096;  Loss pred: 0.3096; Loss self: 0.0000; time: 0.89s
Val loss: 0.4174 score: 0.8372 time: 0.25s
Test loss: 0.4248 score: 0.8636 time: 0.27s
Epoch 83/1000, LR 0.000266
Train loss: 0.2903;  Loss pred: 0.2903; Loss self: 0.0000; time: 0.58s
Val loss: 0.4118 score: 0.8372 time: 0.50s
Test loss: 0.4200 score: 0.8636 time: 0.49s
Epoch 84/1000, LR 0.000266
Train loss: 0.2545;  Loss pred: 0.2545; Loss self: 0.0000; time: 0.56s
Val loss: 0.4070 score: 0.8372 time: 0.43s
Test loss: 0.4158 score: 0.8636 time: 0.40s
Epoch 85/1000, LR 0.000266
Train loss: 0.2626;  Loss pred: 0.2626; Loss self: 0.0000; time: 0.82s
Val loss: 0.4031 score: 0.8372 time: 0.28s
Test loss: 0.4125 score: 0.8636 time: 0.45s
Epoch 86/1000, LR 0.000266
Train loss: 0.2429;  Loss pred: 0.2429; Loss self: 0.0000; time: 0.52s
Val loss: 0.4000 score: 0.8372 time: 0.36s
Test loss: 0.4099 score: 0.8636 time: 0.72s
Epoch 87/1000, LR 0.000266
Train loss: 0.2208;  Loss pred: 0.2208; Loss self: 0.0000; time: 0.81s
Val loss: 0.3977 score: 0.8372 time: 0.49s
Test loss: 0.4082 score: 0.8636 time: 0.44s
Epoch 88/1000, LR 0.000266
Train loss: 0.2379;  Loss pred: 0.2379; Loss self: 0.0000; time: 0.80s
Val loss: 0.3962 score: 0.8372 time: 0.72s
Test loss: 0.4074 score: 0.8636 time: 0.31s
Epoch 89/1000, LR 0.000266
Train loss: 0.2088;  Loss pred: 0.2088; Loss self: 0.0000; time: 0.97s
Val loss: 0.3957 score: 0.8372 time: 0.28s
Test loss: 0.4079 score: 0.8409 time: 0.23s
Epoch 90/1000, LR 0.000266
Train loss: 0.2103;  Loss pred: 0.2103; Loss self: 0.0000; time: 0.51s
Val loss: 0.3957 score: 0.8605 time: 0.28s
Test loss: 0.4086 score: 0.8409 time: 0.43s
Epoch 91/1000, LR 0.000266
Train loss: 0.1721;  Loss pred: 0.1721; Loss self: 0.0000; time: 0.83s
Val loss: 0.3962 score: 0.8837 time: 0.38s
Test loss: 0.4095 score: 0.8409 time: 0.41s
     INFO: Early stopping counter 1 of 2
Epoch 92/1000, LR 0.000266
Train loss: 0.1640;  Loss pred: 0.1640; Loss self: 0.0000; time: 0.78s
Val loss: 0.3971 score: 0.8837 time: 0.62s
Test loss: 0.4106 score: 0.8636 time: 0.33s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.2103,   Val_Loss: 0.3957,   Val_Precision: 0.9000,   Val_Recall: 0.8182,   Val_accuracy: 0.8571,   Val_Score: 0.8605,   Val_Loss: 0.3957,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8409,   Test_loss: 0.4086


[0.46545960498042405, 0.27674458210822195, 0.2698508290341124, 0.47736146696843207, 0.26430180307943374, 0.46257937292102724, 0.4939271570183337, 0.48757957201451063, 0.24213109002448618, 0.4850555839948356, 0.3081564479507506, 0.26851814799010754, 0.3008955700788647, 0.29447822202928364, 0.49427214497700334, 0.4067340219626203, 0.460240580025129, 0.45246625994332135, 0.3322573760524392, 0.27757831010967493, 0.2992864459520206, 0.28991120390128344, 0.2558385779848322, 2.1643191420007497, 0.2776658959919587, 0.2442546000238508, 0.5480268599931151, 0.2989716499578208, 0.5131954519310966, 0.482181471074, 0.25728534697555006, 0.5546903269132599, 0.2514519600663334, 0.6610975209623575, 0.43740429100580513, 0.48671340697910637, 0.2840684549883008, 0.26357609999831766, 0.47012843703851104, 0.6353680450702086, 0.49161284998990595, 0.28205167094711214, 0.24412600102368742, 0.4950331290019676, 0.6764270369894803, 0.26060777402017266, 0.5908879509661347, 0.5603805090067908, 0.2857805419480428, 0.6448580980068073, 0.2859691829653457, 0.462867752998136, 0.37767010799143463, 0.7379967749584466, 0.2616277019260451, 0.5009041889570653, 0.22151952900458127, 0.4725363020552322, 0.22406340297311544, 0.30281000200193375, 0.4735675279516727, 0.28206373495049775, 0.3951599590945989, 0.44620508200023323, 0.4943289649672806, 0.5667253969004378, 0.27813655603677034, 0.3455525320023298, 3.8671414819546044, 1.0187446019845083, 0.3978623040020466, 0.3271546079777181, 0.29590333194937557, 0.2980129710631445, 0.4733989629894495, 0.29934965702705085, 0.3011383470147848, 0.42546709498856217, 0.32903218804858625, 0.2537226870190352, 0.2648888509720564, 0.2792313339887187, 0.4913474329514429, 0.4077108280034736, 0.4585880381055176, 0.7265519239008427, 0.44098644400946796, 0.3204885319573805, 0.23230906995013356, 0.4360774150118232, 0.4120911529753357, 0.3295028660213575]
[0.010578627385918728, 0.006289649593368681, 0.006132973387138918, 0.010849124249282548, 0.0060068591608962215, 0.010513167566386983, 0.011225617204962129, 0.011081353909420695, 0.005502979318738322, 0.011023990545337174, 0.007003555635244332, 0.006102685181593353, 0.006838535683610561, 0.006692686864301901, 0.01123345784038644, 0.009243955044605007, 0.010460013182389295, 0.010283324089620939, 0.0075513040011918, 0.006308597957038067, 0.00680196468072774, 0.006588890997756442, 0.005814513136018914, 0.04918907140910795, 0.0063105885452717884, 0.005551240909632973, 0.012455155908934434, 0.006794810226314108, 0.011663532998434013, 0.010958669797136363, 0.005847394249444319, 0.012606598338937725, 0.005714817274234851, 0.015024943658235397, 0.009941006613768299, 0.011061668340434235, 0.006456101249734109, 0.005990365909052674, 0.010684737205420706, 0.01444018284250474, 0.011173019317952409, 0.006410265248798003, 0.005548318205083805, 0.0112507529318629, 0.015373341749760915, 0.005922903955003924, 0.013429271612866696, 0.012735920659245246, 0.006495012317000973, 0.014655865863791074, 0.006499299612848766, 0.010519721659048546, 0.008583411545259878, 0.016772653976328333, 0.005946084134682844, 0.011384186112660576, 0.00503453475010412, 0.010739461410346186, 0.0050923500675708055, 0.006882045500043949, 0.010762898362538015, 0.00641053943069313, 0.008980908161240884, 0.010141024590914392, 0.011234749203801832, 0.012880122656828131, 0.0063212853644720535, 0.007853466636416588, 0.08788957913533192, 0.023153286408738826, 0.009042325090955605, 0.007435331999493594, 0.0067250757261221724, 0.006773022069616921, 0.010759067340669308, 0.006803401296069337, 0.0068440533412451095, 0.009669706704285503, 0.007478004273831506, 0.005766424704978073, 0.006020201158455827, 0.006346166681561788, 0.011166987112532794, 0.009266155181897127, 0.010422455411489036, 0.01651254372501915, 0.010022419182033364, 0.0072838302717586475, 0.005279751589775763, 0.0099108503411778, 0.00936570802216672, 0.0074887015004853974]
[94.53022244937993, 158.99136909857785, 163.0530473354146, 92.17333832877156, 166.47635198605195, 95.11881111809063, 89.08196152973788, 90.2416805901182, 181.7197452650561, 90.71125341476012, 142.78461571257486, 163.8622950789196, 146.230135553234, 149.41682171534075, 89.01978484352412, 108.1788038966745, 95.60217397083414, 97.24481999058163, 132.42745886567047, 158.5138261797725, 147.01634703181233, 151.770609096509, 171.98344497759288, 20.32971900776395, 158.46382517669457, 180.1399031817764, 80.28803551809993, 147.17114484335744, 85.73731476854084, 91.25195105899736, 171.01634631443406, 79.32353939693009, 174.98372249074032, 66.55599000878016, 100.59343473476817, 90.40227651236415, 154.89224244139362, 166.9347106975209, 93.59144551469814, 69.25120068815858, 89.50132202789945, 155.99978490554838, 180.23479602949257, 88.8829401957563, 65.04766603627685, 168.8360992508003, 74.46420243983238, 78.51807707942034, 153.96429616960958, 68.23206552883441, 153.86273284325188, 95.05954933131234, 116.50379277832043, 59.6208567476158, 168.1779095871032, 87.84115000438024, 198.62808573906833, 93.1145391552522, 196.37298825314815, 145.30563623760028, 92.91177583546263, 155.9931127187336, 111.34731388477209, 98.6093654576014, 89.009552581877, 77.63901219293673, 158.19567419315817, 127.33230384693965, 11.377913170572874, 43.190412900631095, 110.59102497876668, 134.49298566198632, 148.6972103697964, 147.64458017727375, 92.94485928348055, 146.98530286281212, 146.11224520615357, 103.41575298833122, 133.72551865200123, 173.41768100027633, 166.10740632734914, 157.57543887169072, 89.54966903093248, 107.9196258177993, 95.94668055837063, 60.560021317905104, 99.77630967507773, 137.29040390703045, 189.40285030388546, 100.89951574035781, 106.77249361534696, 133.53449859567547]
Elapsed: 0.4559151708431866~0.42564227960053397
Time per graph: 0.010361708428254242~0.00967368817273941
Speed: 120.58926872227745~40.73087436479056
Total Time: 0.3300
best val loss: 0.3956974446773529 test_score: 0.8409

Testing...
Test loss: 0.4095 score: 0.8409 time: 0.27s
test Score 0.8409
Epoch Time List: [1.4978335000341758, 1.7319690879667178, 1.1010903398273513, 1.7651524341199547, 1.4266435150057077, 1.3117675218963996, 1.582302390015684, 1.583562702871859, 1.0731592919910327, 1.5531772560207173, 1.3472630050964653, 0.9930485709337518, 1.2606335770105943, 1.2950805480359122, 1.5502120670862496, 1.764099970809184, 1.600817508995533, 1.2955862201051787, 1.4572128258878365, 1.1912567089311779, 1.3007659671129659, 1.817173390998505, 1.8182459148811176, 2.8317635589046404, 10.590977359097451, 12.872784472070634, 1.2521445750026032, 1.6069865729659796, 1.280645874910988, 1.579662550939247, 1.7252446849597618, 1.3412245570216328, 1.5364911400247365, 1.5416439450345933, 1.305025601061061, 1.343354952055961, 1.7193118560826406, 1.592377250897698, 1.7840605018427595, 1.4981972131645307, 1.8753794390941039, 1.8787904001073912, 1.4968482240801677, 1.3307059411890805, 1.4944920140551403, 1.4293092489242554, 1.696197395096533, 2.2073390140431, 1.6297502090455964, 1.5105931790312752, 1.083161060931161, 1.7547707168851048, 1.260581401991658, 1.9052891748724505, 1.6450478410115466, 1.597282923059538, 2.5831246699672192, 1.5585185080999509, 1.030358660966158, 2.466092560091056, 1.5141923119081184, 1.6697334020864218, 1.677684896858409, 1.661356445052661, 2.1242184080183506, 1.797703380114399, 1.7845555449603125, 1.1238430730300024, 8.062608361011371, 5.74445052491501, 11.619161146110855, 1.4700658469228074, 1.726659759064205, 1.3991077480604872, 1.7913692030124366, 1.6194597850553691, 1.5538142679724842, 1.579953387961723, 1.1424518751446158, 1.9934481577947736, 2.2141959820874035, 1.4194038428831846, 1.5674257137579843, 1.3878520488506183, 1.5542415090603754, 1.6045608730055392, 1.735110031091608, 1.837497841916047, 1.4814229330513626, 1.2194120400818065, 1.615424201823771, 1.7266734630102292]
Total Epoch List: [92]
Total Time List: [0.3300163180101663]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335a50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.28s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.28s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.30s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.50s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.41s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.33s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.35s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 1.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.54s
Epoch 9/1000, LR 0.000210
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.45s
Epoch 10/1000, LR 0.000240
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.48s
Epoch 11/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.50s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.42s
Epoch 13/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.64s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.49s
Epoch 14/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.44s
Epoch 16/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.53s
Epoch 17/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.87s
Epoch 18/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.26s
Epoch 19/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 4.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 4.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 3.36s
Epoch 20/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 3.35s
Epoch 21/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 9.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.37s
Val loss: 0.6902 score: 0.5227 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.26s
Epoch 24/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.32s
Val loss: 0.6898 score: 0.5227 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.50s
Val loss: 0.6893 score: 0.5455 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.45s
Val loss: 0.6887 score: 0.5909 time: 0.23s
Test loss: 0.6909 score: 0.5349 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.33s
Val loss: 0.6881 score: 0.5682 time: 0.23s
Test loss: 0.6905 score: 0.6047 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.36s
Val loss: 0.6875 score: 0.6364 time: 0.24s
Test loss: 0.6900 score: 0.6047 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6827;  Loss pred: 0.6827; Loss self: 0.0000; time: 0.34s
Val loss: 0.6868 score: 0.6364 time: 0.24s
Test loss: 0.6895 score: 0.5814 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.37s
Val loss: 0.6860 score: 0.6591 time: 0.23s
Test loss: 0.6890 score: 0.5116 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6811;  Loss pred: 0.6811; Loss self: 0.0000; time: 0.32s
Val loss: 0.6852 score: 0.7045 time: 0.26s
Test loss: 0.6884 score: 0.5349 time: 0.41s
Epoch 32/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 1.05s
Val loss: 0.6842 score: 0.6818 time: 0.70s
Test loss: 0.6878 score: 0.5814 time: 0.56s
Epoch 33/1000, LR 0.000270
Train loss: 0.6779;  Loss pred: 0.6779; Loss self: 0.0000; time: 0.99s
Val loss: 0.6832 score: 0.7045 time: 0.54s
Test loss: 0.6871 score: 0.6279 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 0.56s
Val loss: 0.6821 score: 0.7727 time: 0.25s
Test loss: 0.6864 score: 0.6047 time: 0.27s
Epoch 35/1000, LR 0.000270
Train loss: 0.6739;  Loss pred: 0.6739; Loss self: 0.0000; time: 0.58s
Val loss: 0.6808 score: 0.7727 time: 0.34s
Test loss: 0.6856 score: 0.6279 time: 0.38s
Epoch 36/1000, LR 0.000270
Train loss: 0.6718;  Loss pred: 0.6718; Loss self: 0.0000; time: 1.07s
Val loss: 0.6795 score: 0.7727 time: 0.67s
Test loss: 0.6847 score: 0.6512 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6696;  Loss pred: 0.6696; Loss self: 0.0000; time: 0.83s
Val loss: 0.6780 score: 0.7727 time: 0.41s
Test loss: 0.6837 score: 0.6744 time: 0.29s
Epoch 38/1000, LR 0.000270
Train loss: 0.6669;  Loss pred: 0.6669; Loss self: 0.0000; time: 0.81s
Val loss: 0.6764 score: 0.7955 time: 0.22s
Test loss: 0.6827 score: 0.6744 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6639;  Loss pred: 0.6639; Loss self: 0.0000; time: 0.84s
Val loss: 0.6746 score: 0.7727 time: 0.44s
Test loss: 0.6816 score: 0.6977 time: 0.38s
Epoch 40/1000, LR 0.000269
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.94s
Val loss: 0.6727 score: 0.7727 time: 0.62s
Test loss: 0.6804 score: 0.6977 time: 0.43s
Epoch 41/1000, LR 0.000269
Train loss: 0.6568;  Loss pred: 0.6568; Loss self: 0.0000; time: 0.81s
Val loss: 0.6707 score: 0.7727 time: 0.42s
Test loss: 0.6791 score: 0.7209 time: 0.50s
Epoch 42/1000, LR 0.000269
Train loss: 0.6538;  Loss pred: 0.6538; Loss self: 0.0000; time: 1.14s
Val loss: 0.6684 score: 0.7955 time: 0.48s
Test loss: 0.6777 score: 0.7209 time: 0.30s
Epoch 43/1000, LR 0.000269
Train loss: 0.6495;  Loss pred: 0.6495; Loss self: 0.0000; time: 0.57s
Val loss: 0.6659 score: 0.7955 time: 0.29s
Test loss: 0.6762 score: 0.6977 time: 0.42s
Epoch 44/1000, LR 0.000269
Train loss: 0.6461;  Loss pred: 0.6461; Loss self: 0.0000; time: 0.78s
Val loss: 0.6633 score: 0.7955 time: 0.31s
Test loss: 0.6746 score: 0.7209 time: 0.28s
Epoch 45/1000, LR 0.000269
Train loss: 0.6411;  Loss pred: 0.6411; Loss self: 0.0000; time: 0.78s
Val loss: 0.6604 score: 0.8182 time: 0.72s
Test loss: 0.6728 score: 0.6977 time: 0.50s
Epoch 46/1000, LR 0.000269
Train loss: 0.6346;  Loss pred: 0.6346; Loss self: 0.0000; time: 0.83s
Val loss: 0.6572 score: 0.8182 time: 0.46s
Test loss: 0.6708 score: 0.6744 time: 0.45s
Epoch 47/1000, LR 0.000269
Train loss: 0.6321;  Loss pred: 0.6321; Loss self: 0.0000; time: 0.58s
Val loss: 0.6538 score: 0.8636 time: 0.26s
Test loss: 0.6687 score: 0.6744 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6254;  Loss pred: 0.6254; Loss self: 0.0000; time: 0.64s
Val loss: 0.6501 score: 0.8636 time: 0.53s
Test loss: 0.6665 score: 0.6744 time: 0.46s
Epoch 49/1000, LR 0.000269
Train loss: 0.6166;  Loss pred: 0.6166; Loss self: 0.0000; time: 1.01s
Val loss: 0.6461 score: 0.8409 time: 0.46s
Test loss: 0.6641 score: 0.6512 time: 0.29s
Epoch 50/1000, LR 0.000269
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 1.31s
Val loss: 0.6418 score: 0.8409 time: 0.22s
Test loss: 0.6615 score: 0.6512 time: 0.60s
Epoch 51/1000, LR 0.000269
Train loss: 0.6032;  Loss pred: 0.6032; Loss self: 0.0000; time: 1.28s
Val loss: 0.6372 score: 0.8409 time: 0.69s
Test loss: 0.6587 score: 0.6744 time: 0.39s
Epoch 52/1000, LR 0.000269
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.49s
Val loss: 0.6322 score: 0.8409 time: 0.76s
Test loss: 0.6557 score: 0.6744 time: 0.29s
Epoch 53/1000, LR 0.000269
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 1.07s
Val loss: 0.6269 score: 0.8409 time: 0.27s
Test loss: 0.6525 score: 0.6977 time: 0.44s
Epoch 54/1000, LR 0.000269
Train loss: 0.5777;  Loss pred: 0.5777; Loss self: 0.0000; time: 1.46s
Val loss: 0.6212 score: 0.8636 time: 0.95s
Test loss: 0.6490 score: 0.7209 time: 0.31s
Epoch 55/1000, LR 0.000269
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 1.22s
Val loss: 0.6152 score: 0.8636 time: 0.63s
Test loss: 0.6452 score: 0.7209 time: 0.50s
Epoch 56/1000, LR 0.000269
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 1.14s
Val loss: 0.6088 score: 0.8636 time: 0.81s
Test loss: 0.6412 score: 0.7209 time: 0.26s
Epoch 57/1000, LR 0.000269
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 1.57s
Val loss: 0.6020 score: 0.8636 time: 0.25s
Test loss: 0.6370 score: 0.7442 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.5360;  Loss pred: 0.5360; Loss self: 0.0000; time: 0.38s
Val loss: 0.5949 score: 0.8636 time: 0.36s
Test loss: 0.6325 score: 0.7442 time: 0.26s
Epoch 59/1000, LR 0.000268
Train loss: 0.5252;  Loss pred: 0.5252; Loss self: 0.0000; time: 0.86s
Val loss: 0.5875 score: 0.8636 time: 0.43s
Test loss: 0.6278 score: 0.7442 time: 0.28s
Epoch 60/1000, LR 0.000268
Train loss: 0.5134;  Loss pred: 0.5134; Loss self: 0.0000; time: 0.79s
Val loss: 0.5797 score: 0.8636 time: 0.69s
Test loss: 0.6230 score: 0.7442 time: 0.29s
Epoch 61/1000, LR 0.000268
Train loss: 0.5032;  Loss pred: 0.5032; Loss self: 0.0000; time: 0.61s
Val loss: 0.5717 score: 0.8636 time: 0.42s
Test loss: 0.6180 score: 0.7674 time: 0.60s
Epoch 62/1000, LR 0.000268
Train loss: 0.4890;  Loss pred: 0.4890; Loss self: 0.0000; time: 0.77s
Val loss: 0.5634 score: 0.8636 time: 0.25s
Test loss: 0.6129 score: 0.7674 time: 0.28s
Epoch 63/1000, LR 0.000268
Train loss: 0.4721;  Loss pred: 0.4721; Loss self: 0.0000; time: 0.59s
Val loss: 0.5549 score: 0.8636 time: 0.45s
Test loss: 0.6075 score: 0.7674 time: 0.48s
Epoch 64/1000, LR 0.000268
Train loss: 0.4618;  Loss pred: 0.4618; Loss self: 0.0000; time: 0.60s
Val loss: 0.5464 score: 0.8636 time: 0.44s
Test loss: 0.6022 score: 0.7674 time: 0.27s
Epoch 65/1000, LR 0.000268
Train loss: 0.4492;  Loss pred: 0.4492; Loss self: 0.0000; time: 0.56s
Val loss: 0.5377 score: 0.8636 time: 0.22s
Test loss: 0.5967 score: 0.7674 time: 0.55s
Epoch 66/1000, LR 0.000268
Train loss: 0.4380;  Loss pred: 0.4380; Loss self: 0.0000; time: 0.85s
Val loss: 0.5290 score: 0.8636 time: 0.64s
Test loss: 0.5912 score: 0.7907 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.4227;  Loss pred: 0.4227; Loss self: 0.0000; time: 0.83s
Val loss: 0.5203 score: 0.8636 time: 0.24s
Test loss: 0.5856 score: 0.7907 time: 0.45s
Epoch 68/1000, LR 0.000268
Train loss: 0.4102;  Loss pred: 0.4102; Loss self: 0.0000; time: 0.89s
Val loss: 0.5116 score: 0.8636 time: 0.39s
Test loss: 0.5800 score: 0.7907 time: 0.27s
Epoch 69/1000, LR 0.000268
Train loss: 0.3918;  Loss pred: 0.3918; Loss self: 0.0000; time: 0.98s
Val loss: 0.5028 score: 0.8636 time: 0.58s
Test loss: 0.5743 score: 0.7907 time: 0.50s
Epoch 70/1000, LR 0.000268
Train loss: 0.3851;  Loss pred: 0.3851; Loss self: 0.0000; time: 0.61s
Val loss: 0.4939 score: 0.8636 time: 0.33s
Test loss: 0.5686 score: 0.7907 time: 0.53s
Epoch 71/1000, LR 0.000268
Train loss: 0.3668;  Loss pred: 0.3668; Loss self: 0.0000; time: 0.50s
Val loss: 0.4848 score: 0.8636 time: 0.24s
Test loss: 0.5630 score: 0.7674 time: 4.43s
Epoch 72/1000, LR 0.000267
Train loss: 0.3539;  Loss pred: 0.3539; Loss self: 0.0000; time: 8.13s
Val loss: 0.4760 score: 0.8636 time: 0.25s
Test loss: 0.5576 score: 0.7674 time: 0.27s
Epoch 73/1000, LR 0.000267
Train loss: 0.3479;  Loss pred: 0.3479; Loss self: 0.0000; time: 9.95s
Val loss: 0.4677 score: 0.8636 time: 2.34s
Test loss: 0.5525 score: 0.7674 time: 0.26s
Epoch 74/1000, LR 0.000267
Train loss: 0.3314;  Loss pred: 0.3314; Loss self: 0.0000; time: 0.32s
Val loss: 0.4602 score: 0.8636 time: 0.26s
Test loss: 0.5479 score: 0.7674 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.3167;  Loss pred: 0.3167; Loss self: 0.0000; time: 0.37s
Val loss: 0.4532 score: 0.8636 time: 0.37s
Test loss: 0.5439 score: 0.7674 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3100;  Loss pred: 0.3100; Loss self: 0.0000; time: 0.48s
Val loss: 0.4469 score: 0.8636 time: 0.23s
Test loss: 0.5410 score: 0.7907 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.3031;  Loss pred: 0.3031; Loss self: 0.0000; time: 0.36s
Val loss: 0.4412 score: 0.8636 time: 0.24s
Test loss: 0.5387 score: 0.7907 time: 0.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.2818;  Loss pred: 0.2818; Loss self: 0.0000; time: 0.47s
Val loss: 0.4359 score: 0.8636 time: 0.22s
Test loss: 0.5369 score: 0.7674 time: 0.28s
Epoch 79/1000, LR 0.000267
Train loss: 0.2720;  Loss pred: 0.2720; Loss self: 0.0000; time: 0.41s
Val loss: 0.4309 score: 0.8409 time: 0.25s
Test loss: 0.5360 score: 0.7674 time: 0.26s
Epoch 80/1000, LR 0.000267
Train loss: 0.2666;  Loss pred: 0.2666; Loss self: 0.0000; time: 0.38s
Val loss: 0.4260 score: 0.8182 time: 0.24s
Test loss: 0.5357 score: 0.7674 time: 0.27s
Epoch 81/1000, LR 0.000267
Train loss: 0.2574;  Loss pred: 0.2574; Loss self: 0.0000; time: 0.77s
Val loss: 0.4214 score: 0.8182 time: 0.30s
Test loss: 0.5356 score: 0.7674 time: 0.26s
Epoch 82/1000, LR 0.000267
Train loss: 0.2502;  Loss pred: 0.2502; Loss self: 0.0000; time: 0.54s
Val loss: 0.4170 score: 0.8182 time: 0.27s
Test loss: 0.5359 score: 0.7442 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.2392;  Loss pred: 0.2392; Loss self: 0.0000; time: 0.77s
Val loss: 0.4130 score: 0.8182 time: 0.29s
Test loss: 0.5365 score: 0.7442 time: 0.51s
Epoch 84/1000, LR 0.000266
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 0.83s
Val loss: 0.4102 score: 0.8182 time: 0.46s
Test loss: 0.5385 score: 0.7442 time: 0.32s
Epoch 85/1000, LR 0.000266
Train loss: 0.2231;  Loss pred: 0.2231; Loss self: 0.0000; time: 0.78s
Val loss: 0.4085 score: 0.8182 time: 0.42s
Test loss: 0.5419 score: 0.7209 time: 0.33s
Epoch 86/1000, LR 0.000266
Train loss: 0.2063;  Loss pred: 0.2063; Loss self: 0.0000; time: 1.05s
Val loss: 0.4077 score: 0.8182 time: 0.47s
Test loss: 0.5461 score: 0.7209 time: 0.27s
Epoch 87/1000, LR 0.000266
Train loss: 0.2030;  Loss pred: 0.2030; Loss self: 0.0000; time: 0.56s
Val loss: 0.4057 score: 0.8182 time: 0.41s
Test loss: 0.5487 score: 0.7209 time: 0.61s
Epoch 88/1000, LR 0.000266
Train loss: 0.1779;  Loss pred: 0.1779; Loss self: 0.0000; time: 0.88s
Val loss: 0.4030 score: 0.8182 time: 0.41s
Test loss: 0.5506 score: 0.7209 time: 0.29s
Epoch 89/1000, LR 0.000266
Train loss: 0.1947;  Loss pred: 0.1947; Loss self: 0.0000; time: 0.55s
Val loss: 0.4007 score: 0.8182 time: 0.30s
Test loss: 0.5527 score: 0.7442 time: 0.72s
Epoch 90/1000, LR 0.000266
Train loss: 0.1787;  Loss pred: 0.1787; Loss self: 0.0000; time: 0.81s
Val loss: 0.3974 score: 0.8182 time: 0.70s
Test loss: 0.5534 score: 0.7674 time: 0.28s
Epoch 91/1000, LR 0.000266
Train loss: 0.1780;  Loss pred: 0.1780; Loss self: 0.0000; time: 1.06s
Val loss: 0.3960 score: 0.8182 time: 0.46s
Test loss: 0.5565 score: 0.7907 time: 0.33s
Epoch 92/1000, LR 0.000266
Train loss: 0.1605;  Loss pred: 0.1605; Loss self: 0.0000; time: 0.61s
Val loss: 0.3943 score: 0.8182 time: 0.53s
Test loss: 0.5589 score: 0.8140 time: 0.27s
Epoch 93/1000, LR 0.000265
Train loss: 0.1687;  Loss pred: 0.1687; Loss self: 0.0000; time: 0.63s
Val loss: 0.3938 score: 0.8182 time: 0.32s
Test loss: 0.5629 score: 0.8372 time: 0.44s
Epoch 94/1000, LR 0.000265
Train loss: 0.1439;  Loss pred: 0.1439; Loss self: 0.0000; time: 0.62s
Val loss: 0.3947 score: 0.8182 time: 0.47s
Test loss: 0.5685 score: 0.8372 time: 0.34s
     INFO: Early stopping counter 1 of 2
Epoch 95/1000, LR 0.000265
Train loss: 0.1364;  Loss pred: 0.1364; Loss self: 0.0000; time: 0.56s
Val loss: 0.3966 score: 0.8182 time: 0.31s
Test loss: 0.5754 score: 0.8372 time: 0.64s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.1687,   Val_Loss: 0.3938,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.3938,   Test_Precision: 0.8947,   Test_Recall: 0.7727,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.5629


[0.46545960498042405, 0.27674458210822195, 0.2698508290341124, 0.47736146696843207, 0.26430180307943374, 0.46257937292102724, 0.4939271570183337, 0.48757957201451063, 0.24213109002448618, 0.4850555839948356, 0.3081564479507506, 0.26851814799010754, 0.3008955700788647, 0.29447822202928364, 0.49427214497700334, 0.4067340219626203, 0.460240580025129, 0.45246625994332135, 0.3322573760524392, 0.27757831010967493, 0.2992864459520206, 0.28991120390128344, 0.2558385779848322, 2.1643191420007497, 0.2776658959919587, 0.2442546000238508, 0.5480268599931151, 0.2989716499578208, 0.5131954519310966, 0.482181471074, 0.25728534697555006, 0.5546903269132599, 0.2514519600663334, 0.6610975209623575, 0.43740429100580513, 0.48671340697910637, 0.2840684549883008, 0.26357609999831766, 0.47012843703851104, 0.6353680450702086, 0.49161284998990595, 0.28205167094711214, 0.24412600102368742, 0.4950331290019676, 0.6764270369894803, 0.26060777402017266, 0.5908879509661347, 0.5603805090067908, 0.2857805419480428, 0.6448580980068073, 0.2859691829653457, 0.462867752998136, 0.37767010799143463, 0.7379967749584466, 0.2616277019260451, 0.5009041889570653, 0.22151952900458127, 0.4725363020552322, 0.22406340297311544, 0.30281000200193375, 0.4735675279516727, 0.28206373495049775, 0.3951599590945989, 0.44620508200023323, 0.4943289649672806, 0.5667253969004378, 0.27813655603677034, 0.3455525320023298, 3.8671414819546044, 1.0187446019845083, 0.3978623040020466, 0.3271546079777181, 0.29590333194937557, 0.2980129710631445, 0.4733989629894495, 0.29934965702705085, 0.3011383470147848, 0.42546709498856217, 0.32903218804858625, 0.2537226870190352, 0.2648888509720564, 0.2792313339887187, 0.4913474329514429, 0.4077108280034736, 0.4585880381055176, 0.7265519239008427, 0.44098644400946796, 0.3204885319573805, 0.23230906995013356, 0.4360774150118232, 0.4120911529753357, 0.3295028660213575, 0.2824336210032925, 0.28405224601738155, 0.30188961408566684, 0.5040679380763322, 0.4136977429734543, 0.3346277209930122, 0.35748599900398403, 0.5453769629821181, 0.45408454700373113, 0.4817488400731236, 0.5032968129962683, 0.4225394179811701, 0.49374865100253373, 0.24151886894833297, 0.447691431036219, 0.5368635070044547, 0.8752837859792635, 0.26605870097409934, 3.3631414560368285, 3.3516027429141104, 0.2571418529842049, 0.2575427759438753, 0.2642065240070224, 0.2539486719761044, 0.242865253938362, 0.2433014790294692, 0.22909767599776387, 0.24273751908913255, 0.22875831404235214, 0.24677043210249394, 0.41345533402636647, 0.5679363970411941, 0.264947135001421, 0.2781232870183885, 0.3820105819031596, 0.2690833610249683, 0.2915277179563418, 0.2669719470432028, 0.38509102701209486, 0.43671359098516405, 0.5061273950850591, 0.30611318605951965, 0.42600725404918194, 0.285159430000931, 0.5091516959946603, 0.45491884707007557, 0.25221159402281046, 0.46174214605707675, 0.2975091189146042, 0.6068789220880717, 0.39614159893244505, 0.292188900988549, 0.44842766504734755, 0.31785793497692794, 0.5036267210962251, 0.26500659296289086, 0.22232300997711718, 0.26270973100326955, 0.28521721391007304, 0.2981059519806877, 0.6098421700298786, 0.2795462040230632, 0.485394066083245, 0.2763110090745613, 0.5560853990027681, 0.2550034620799124, 0.45459087402559817, 0.269984787912108, 0.5085196349537, 0.5372439329512417, 4.431808320921846, 0.27439472102560103, 0.26167259900830686, 0.22986306005623192, 0.2590017099864781, 0.24502437794581056, 0.2282421119743958, 0.28757710999343544, 0.26194397697690874, 0.2765543849673122, 0.26156489399727434, 0.251346156001091, 0.5162586529040709, 0.32817080500535667, 0.3301945829298347, 0.27532678295392543, 0.6129567279713228, 0.2980951339704916, 0.7240242339903489, 0.2864055280806497, 0.33475594595074654, 0.2768953850027174, 0.4493200359866023, 0.3423772680107504, 0.6465292890788987]
[0.010578627385918728, 0.006289649593368681, 0.006132973387138918, 0.010849124249282548, 0.0060068591608962215, 0.010513167566386983, 0.011225617204962129, 0.011081353909420695, 0.005502979318738322, 0.011023990545337174, 0.007003555635244332, 0.006102685181593353, 0.006838535683610561, 0.006692686864301901, 0.01123345784038644, 0.009243955044605007, 0.010460013182389295, 0.010283324089620939, 0.0075513040011918, 0.006308597957038067, 0.00680196468072774, 0.006588890997756442, 0.005814513136018914, 0.04918907140910795, 0.0063105885452717884, 0.005551240909632973, 0.012455155908934434, 0.006794810226314108, 0.011663532998434013, 0.010958669797136363, 0.005847394249444319, 0.012606598338937725, 0.005714817274234851, 0.015024943658235397, 0.009941006613768299, 0.011061668340434235, 0.006456101249734109, 0.005990365909052674, 0.010684737205420706, 0.01444018284250474, 0.011173019317952409, 0.006410265248798003, 0.005548318205083805, 0.0112507529318629, 0.015373341749760915, 0.005922903955003924, 0.013429271612866696, 0.012735920659245246, 0.006495012317000973, 0.014655865863791074, 0.006499299612848766, 0.010519721659048546, 0.008583411545259878, 0.016772653976328333, 0.005946084134682844, 0.011384186112660576, 0.00503453475010412, 0.010739461410346186, 0.0050923500675708055, 0.006882045500043949, 0.010762898362538015, 0.00641053943069313, 0.008980908161240884, 0.010141024590914392, 0.011234749203801832, 0.012880122656828131, 0.0063212853644720535, 0.007853466636416588, 0.08788957913533192, 0.023153286408738826, 0.009042325090955605, 0.007435331999493594, 0.0067250757261221724, 0.006773022069616921, 0.010759067340669308, 0.006803401296069337, 0.0068440533412451095, 0.009669706704285503, 0.007478004273831506, 0.005766424704978073, 0.006020201158455827, 0.006346166681561788, 0.011166987112532794, 0.009266155181897127, 0.010422455411489036, 0.01651254372501915, 0.010022419182033364, 0.0072838302717586475, 0.005279751589775763, 0.0099108503411778, 0.00936570802216672, 0.0074887015004853974, 0.006568223744262617, 0.0066058661864507335, 0.007020688699666671, 0.011722510187821679, 0.009620877743568705, 0.007782040023093306, 0.008313627883813582, 0.012683185185630655, 0.010560105744272817, 0.011203461397049386, 0.011704577046424844, 0.009826498092585352, 0.011482526767500785, 0.005616717882519371, 0.010411428628749279, 0.012485197837312901, 0.020355436883238685, 0.0061874116505604496, 0.07821259200085648, 0.07794424983521187, 0.005980043092655928, 0.005989366882415705, 0.006144337767605172, 0.00590578306921173, 0.005648029161357256, 0.005658173930917888, 0.005327852930180555, 0.005645058583468199, 0.005319960791682608, 0.005738847258197534, 0.009615240326194568, 0.013207823187004515, 0.006161561279102813, 0.006467983419032291, 0.008883967021003712, 0.006257752581976007, 0.006779714371077716, 0.006208649931237275, 0.008955605279351042, 0.010156130022910792, 0.01177040453686184, 0.007118911303709759, 0.009907145443004231, 0.006631614651184442, 0.01184073711615489, 0.010579508071397107, 0.00586538590750722, 0.01073818944318783, 0.006918816718944284, 0.01411346330437376, 0.00921259532401035, 0.00679509072066393, 0.010428550349938314, 0.00739204499946344, 0.011712249327819188, 0.006162944022392811, 0.005170302557607376, 0.006109528627983013, 0.006632958463024955, 0.006932696557690411, 0.01418237604720648, 0.00650107451216426, 0.011288234094959186, 0.006425837420338635, 0.012932218581459723, 0.005930313071625871, 0.010571880791292981, 0.006278715997956, 0.01182603802217907, 0.012494044952354458, 0.10306530978888014, 0.006381272581990722, 0.00608540927926295, 0.0053456525594472535, 0.006023295581080886, 0.0056982413475769895, 0.005307956092427809, 0.006687839767289197, 0.006091720394811831, 0.006431497324821215, 0.006082904511564519, 0.005845259441885837, 0.012006015183815603, 0.007631879186171086, 0.007678943789065923, 0.006402948440788964, 0.014254807627240067, 0.006932444976057944, 0.016837772883496485, 0.006660593676294179, 0.007785021998854571, 0.00643942755820273, 0.010449303162479124, 0.007962262046761637, 0.01503556486229997]
[94.53022244937993, 158.99136909857785, 163.0530473354146, 92.17333832877156, 166.47635198605195, 95.11881111809063, 89.08196152973788, 90.2416805901182, 181.7197452650561, 90.71125341476012, 142.78461571257486, 163.8622950789196, 146.230135553234, 149.41682171534075, 89.01978484352412, 108.1788038966745, 95.60217397083414, 97.24481999058163, 132.42745886567047, 158.5138261797725, 147.01634703181233, 151.770609096509, 171.98344497759288, 20.32971900776395, 158.46382517669457, 180.1399031817764, 80.28803551809993, 147.17114484335744, 85.73731476854084, 91.25195105899736, 171.01634631443406, 79.32353939693009, 174.98372249074032, 66.55599000878016, 100.59343473476817, 90.40227651236415, 154.89224244139362, 166.9347106975209, 93.59144551469814, 69.25120068815858, 89.50132202789945, 155.99978490554838, 180.23479602949257, 88.8829401957563, 65.04766603627685, 168.8360992508003, 74.46420243983238, 78.51807707942034, 153.96429616960958, 68.23206552883441, 153.86273284325188, 95.05954933131234, 116.50379277832043, 59.6208567476158, 168.1779095871032, 87.84115000438024, 198.62808573906833, 93.1145391552522, 196.37298825314815, 145.30563623760028, 92.91177583546263, 155.9931127187336, 111.34731388477209, 98.6093654576014, 89.009552581877, 77.63901219293673, 158.19567419315817, 127.33230384693965, 11.377913170572874, 43.190412900631095, 110.59102497876668, 134.49298566198632, 148.6972103697964, 147.64458017727375, 92.94485928348055, 146.98530286281212, 146.11224520615357, 103.41575298833122, 133.72551865200123, 173.41768100027633, 166.10740632734914, 157.57543887169072, 89.54966903093248, 107.9196258177993, 95.94668055837063, 60.560021317905104, 99.77630967507773, 137.29040390703045, 189.40285030388546, 100.89951574035781, 106.77249361534696, 133.53449859567547, 152.24816311617064, 151.38060199449637, 142.43616869773447, 85.30596126407153, 103.94062024834199, 128.5010096366103, 120.28443105409782, 78.84454775074518, 94.69602144299942, 89.25812876575505, 85.43666260076006, 101.7656534991399, 87.08884553444449, 178.0399195608969, 96.04829804419738, 80.0948461554553, 49.12692396317132, 161.61846931736326, 12.785664998662227, 12.829682781143951, 167.22287523782177, 166.96255541398187, 162.7514693727135, 169.32555569357788, 177.05291021544477, 176.73546487069135, 187.6928686104162, 177.14608010066428, 187.97131015766715, 174.2510220273891, 104.00156065529885, 75.71270343654534, 162.2965275686766, 154.60769380723232, 112.56232690145892, 159.80177977637948, 147.49883922337543, 161.0656118601162, 111.66191103863208, 98.462701614113, 84.95884715502008, 140.47091715820181, 100.93724834797229, 150.7928389387635, 84.45420164219775, 94.522353331684, 170.49176571998115, 93.12556882057869, 144.53338491564875, 70.8543309628403, 108.54704508660524, 147.16506977000194, 95.89060477671424, 135.28056174882406, 85.38069605680084, 162.26011405694095, 193.4122788479061, 163.6787485403989, 150.7622888903108, 144.24401698220936, 70.51004688293901, 153.82072580907735, 88.587815559792, 155.6217399517246, 77.32625254522453, 168.62516159300125, 94.59054824223914, 159.2682326013064, 84.55917342093406, 80.03813047043292, 9.702585691038125, 156.70855415614247, 164.32748466198768, 187.06790029455283, 166.02206990156526, 175.4927422344476, 188.39643406745088, 149.52511345907038, 164.15723887322133, 155.48478829971347, 164.39515006340295, 171.07880496017353, 83.29158215192199, 131.0293278504714, 130.2262430184609, 156.17804972934957, 70.15177097788828, 144.2492516642575, 59.39027726048902, 150.13676687096458, 128.45178859444874, 155.2933068912579, 95.70016147973905, 125.5924502518369, 66.5089744986828]
Elapsed: 0.4638924462450569~0.5254658825003568
Time per graph: 0.010669644356725065~0.012133588617240827
Speed: 123.76468789939459~41.958503101729306
Total Time: 0.6471
best val loss: 0.3938318192958832 test_score: 0.8372

Testing...
Test loss: 0.6687 score: 0.6744 time: 0.26s
test Score 0.6744
Epoch Time List: [1.4978335000341758, 1.7319690879667178, 1.1010903398273513, 1.7651524341199547, 1.4266435150057077, 1.3117675218963996, 1.582302390015684, 1.583562702871859, 1.0731592919910327, 1.5531772560207173, 1.3472630050964653, 0.9930485709337518, 1.2606335770105943, 1.2950805480359122, 1.5502120670862496, 1.764099970809184, 1.600817508995533, 1.2955862201051787, 1.4572128258878365, 1.1912567089311779, 1.3007659671129659, 1.817173390998505, 1.8182459148811176, 2.8317635589046404, 10.590977359097451, 12.872784472070634, 1.2521445750026032, 1.6069865729659796, 1.280645874910988, 1.579662550939247, 1.7252446849597618, 1.3412245570216328, 1.5364911400247365, 1.5416439450345933, 1.305025601061061, 1.343354952055961, 1.7193118560826406, 1.592377250897698, 1.7840605018427595, 1.4981972131645307, 1.8753794390941039, 1.8787904001073912, 1.4968482240801677, 1.3307059411890805, 1.4944920140551403, 1.4293092489242554, 1.696197395096533, 2.2073390140431, 1.6297502090455964, 1.5105931790312752, 1.083161060931161, 1.7547707168851048, 1.260581401991658, 1.9052891748724505, 1.6450478410115466, 1.597282923059538, 2.5831246699672192, 1.5585185080999509, 1.030358660966158, 2.466092560091056, 1.5141923119081184, 1.6697334020864218, 1.677684896858409, 1.661356445052661, 2.1242184080183506, 1.797703380114399, 1.7845555449603125, 1.1238430730300024, 8.062608361011371, 5.74445052491501, 11.619161146110855, 1.4700658469228074, 1.726659759064205, 1.3991077480604872, 1.7913692030124366, 1.6194597850553691, 1.5538142679724842, 1.579953387961723, 1.1424518751446158, 1.9934481577947736, 2.2141959820874035, 1.4194038428831846, 1.5674257137579843, 1.3878520488506183, 1.5542415090603754, 1.6045608730055392, 1.735110031091608, 1.837497841916047, 1.4814229330513626, 1.2194120400818065, 1.615424201823771, 1.7266734630102292, 1.6406757960794494, 1.3407422309974208, 1.3175701870350167, 1.292597500840202, 1.9030127589358017, 1.7144522219896317, 1.3945667441003025, 2.0833642311627045, 1.815530380117707, 1.6847440800629556, 1.630772195989266, 1.7324693500995636, 2.0395870229694992, 1.092702120076865, 1.3357080980204046, 1.6143955900333822, 2.1507918699644506, 1.6142310908762738, 12.981022331980057, 3.9847688989248127, 9.642814612016082, 0.8919608938740566, 0.8459050939418375, 0.8050568262115121, 0.9759593880735338, 0.9134176869411021, 0.7839593950193375, 0.8339009361807257, 0.8054846519371495, 0.8409378739306703, 0.9852423691190779, 2.319019579095766, 1.794862091075629, 1.0870719600934535, 1.2907635719748214, 2.000856130034663, 1.5296406210400164, 1.2853072748985142, 1.6619175270898268, 1.9906312329694629, 1.738057491951622, 1.9156134651275352, 1.2849804310826585, 1.3739327939692885, 2.0121307101799175, 1.7381864589406177, 1.0859195899683982, 1.6259700549999252, 1.762152866111137, 2.1331540369428694, 2.359792808885686, 1.5415823588846251, 1.7836879610549659, 2.723958290182054, 2.3451370960101485, 2.2060474670724943, 2.0330566819757223, 0.9992624419974163, 1.575862335972488, 1.7743365609785542, 1.6333259268430993, 1.2943810910219327, 1.517671469016932, 1.3071411800337955, 1.3343670739559457, 1.7395210879622027, 1.5213862298987806, 1.5513818640029058, 2.064566060900688, 1.468760950025171, 5.168223493965343, 8.654573645093478, 12.551701462129131, 0.8074624739820138, 0.9938796308124438, 0.9549744100077078, 0.8286172408843413, 0.9822467520134524, 0.917885096045211, 0.8920921849785373, 1.3264737679855898, 1.0512735630618408, 1.5720926449866965, 1.6114631820237264, 1.5286282090237364, 1.7919768679421395, 1.5745241040131077, 1.5854066361207515, 1.5687776850536466, 1.7975619680946693, 1.846962547977455, 1.4180175139335915, 1.3926004369277507, 1.4292404100997373, 1.5138135849265382]
Total Epoch List: [92, 95]
Total Time List: [0.3300163180101663, 0.6471302880672738]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288337640>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4884 time: 0.55s
Epoch 3/1000, LR 0.000030
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.5000 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.48s
Epoch 4/1000, LR 0.000060
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.37s
Epoch 5/1000, LR 0.000090
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4884 time: 0.47s
Epoch 6/1000, LR 0.000120
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.86s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6967 score: 0.4884 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.4884 time: 0.28s
Epoch 8/1000, LR 0.000180
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 1.03s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4884 time: 0.76s
Epoch 9/1000, LR 0.000210
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.4884 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.4884 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.4884 time: 0.50s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.29s
Epoch 13/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4884 time: 0.48s
Epoch 14/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.97s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.53s
Epoch 16/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.28s
Epoch 17/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.85s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.99s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.28s
Epoch 19/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.57s
Epoch 20/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.79s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.36s
Epoch 21/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.98s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.22s
Epoch 22/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.44s
Epoch 23/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.32s
Epoch 24/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 10.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 1.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 3.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 3.70s
Epoch 27/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 4.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.42s
Epoch 28/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.26s
Epoch 29/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.53s
Epoch 30/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.53s
Epoch 31/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.31s
Epoch 32/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6891 score: 0.4884 time: 0.50s
Epoch 33/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 1.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6873 score: 0.5000 time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4884 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6868 score: 0.4884 time: 0.77s
Epoch 36/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.5000 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6858 score: 0.4884 time: 0.59s
Epoch 37/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 1.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.5000 time: 0.76s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.4884 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.66s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.5000 time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6835 score: 0.4884 time: 0.47s
Epoch 39/1000, LR 0.000269
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 1.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6807 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4884 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6758;  Loss pred: 0.6758; Loss self: 0.0000; time: 1.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6791 score: 0.5000 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6805 score: 0.4884 time: 0.26s
Epoch 41/1000, LR 0.000269
Train loss: 0.6733;  Loss pred: 0.6733; Loss self: 0.0000; time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6788 score: 0.4884 time: 0.56s
Epoch 42/1000, LR 0.000269
Train loss: 0.6714;  Loss pred: 0.6714; Loss self: 0.0000; time: 0.88s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6768 score: 0.4884 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6685;  Loss pred: 0.6685; Loss self: 0.0000; time: 1.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6732 score: 0.5000 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6747 score: 0.4884 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 1.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6708 score: 0.5000 time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6724 score: 0.4884 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6682 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6697 score: 0.4884 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6572;  Loss pred: 0.6572; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6653 score: 0.5000 time: 0.70s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6669 score: 0.4884 time: 0.46s
Epoch 47/1000, LR 0.000269
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6621 score: 0.5000 time: 0.41s
Test loss: 0.6638 score: 0.5116 time: 0.47s
Epoch 48/1000, LR 0.000269
Train loss: 0.6476;  Loss pred: 0.6476; Loss self: 0.0000; time: 0.65s
Val loss: 0.6587 score: 0.5227 time: 0.70s
Test loss: 0.6605 score: 0.5116 time: 0.26s
Epoch 49/1000, LR 0.000269
Train loss: 0.6417;  Loss pred: 0.6417; Loss self: 0.0000; time: 1.07s
Val loss: 0.6550 score: 0.5682 time: 0.25s
Test loss: 0.6571 score: 0.5116 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6405;  Loss pred: 0.6405; Loss self: 0.0000; time: 1.12s
Val loss: 0.6510 score: 0.5682 time: 0.51s
Test loss: 0.6534 score: 0.5116 time: 0.42s
Epoch 51/1000, LR 0.000269
Train loss: 0.6345;  Loss pred: 0.6345; Loss self: 0.0000; time: 0.91s
Val loss: 0.6466 score: 0.5909 time: 0.66s
Test loss: 0.6494 score: 0.5349 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6274;  Loss pred: 0.6274; Loss self: 0.0000; time: 0.99s
Val loss: 0.6419 score: 0.5909 time: 0.25s
Test loss: 0.6450 score: 0.5814 time: 0.31s
Epoch 53/1000, LR 0.000269
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.89s
Val loss: 0.6369 score: 0.6136 time: 0.57s
Test loss: 0.6404 score: 0.6047 time: 0.31s
Epoch 54/1000, LR 0.000269
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.87s
Val loss: 0.6315 score: 0.6136 time: 0.45s
Test loss: 0.6353 score: 0.6279 time: 0.26s
Epoch 55/1000, LR 0.000269
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 1.08s
Val loss: 0.6257 score: 0.6364 time: 0.49s
Test loss: 0.6299 score: 0.6279 time: 0.46s
Epoch 56/1000, LR 0.000269
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.61s
Val loss: 0.6194 score: 0.6591 time: 0.45s
Test loss: 0.6239 score: 0.6279 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.5898;  Loss pred: 0.5898; Loss self: 0.0000; time: 0.88s
Val loss: 0.6128 score: 0.6591 time: 0.43s
Test loss: 0.6177 score: 0.6279 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5831;  Loss pred: 0.5831; Loss self: 0.0000; time: 0.39s
Val loss: 0.6059 score: 0.6818 time: 0.42s
Test loss: 0.6113 score: 0.6512 time: 0.50s
Epoch 59/1000, LR 0.000268
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.49s
Val loss: 0.5984 score: 0.7045 time: 0.31s
Test loss: 0.6042 score: 0.6512 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 1.10s
Val loss: 0.5905 score: 0.7273 time: 0.67s
Test loss: 0.5968 score: 0.6512 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.5509;  Loss pred: 0.5509; Loss self: 0.0000; time: 1.13s
Val loss: 0.5823 score: 0.7727 time: 0.44s
Test loss: 0.5892 score: 0.6744 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.5350;  Loss pred: 0.5350; Loss self: 0.0000; time: 0.89s
Val loss: 0.5740 score: 0.7727 time: 0.40s
Test loss: 0.5819 score: 0.6977 time: 0.54s
Epoch 63/1000, LR 0.000268
Train loss: 0.5291;  Loss pred: 0.5291; Loss self: 0.0000; time: 0.86s
Val loss: 0.5656 score: 0.7727 time: 0.46s
Test loss: 0.5747 score: 0.6977 time: 0.27s
Epoch 64/1000, LR 0.000268
Train loss: 0.5200;  Loss pred: 0.5200; Loss self: 0.0000; time: 1.36s
Val loss: 0.5566 score: 0.7727 time: 0.48s
Test loss: 0.5669 score: 0.6977 time: 0.45s
Epoch 65/1000, LR 0.000268
Train loss: 0.4963;  Loss pred: 0.4963; Loss self: 0.0000; time: 1.65s
Val loss: 0.5476 score: 0.7727 time: 0.27s
Test loss: 0.5595 score: 0.6977 time: 0.21s
Epoch 66/1000, LR 0.000268
Train loss: 0.4912;  Loss pred: 0.4912; Loss self: 0.0000; time: 0.64s
Val loss: 0.5383 score: 0.7955 time: 4.16s
Test loss: 0.5516 score: 0.7209 time: 3.63s
Epoch 67/1000, LR 0.000268
Train loss: 0.4752;  Loss pred: 0.4752; Loss self: 0.0000; time: 4.56s
Val loss: 0.5287 score: 0.7955 time: 0.22s
Test loss: 0.5434 score: 0.7674 time: 0.28s
Epoch 68/1000, LR 0.000268
Train loss: 0.4704;  Loss pred: 0.4704; Loss self: 0.0000; time: 9.93s
Val loss: 0.5191 score: 0.8182 time: 2.14s
Test loss: 0.5354 score: 0.7907 time: 0.23s
Epoch 69/1000, LR 0.000268
Train loss: 0.4314;  Loss pred: 0.4314; Loss self: 0.0000; time: 0.38s
Val loss: 0.5097 score: 0.8409 time: 0.27s
Test loss: 0.5284 score: 0.7907 time: 0.21s
Epoch 70/1000, LR 0.000268
Train loss: 0.4298;  Loss pred: 0.4298; Loss self: 0.0000; time: 0.40s
Val loss: 0.5006 score: 0.8409 time: 0.22s
Test loss: 0.5221 score: 0.7907 time: 0.22s
Epoch 71/1000, LR 0.000268
Train loss: 0.4230;  Loss pred: 0.4230; Loss self: 0.0000; time: 0.34s
Val loss: 0.4909 score: 0.8409 time: 0.24s
Test loss: 0.5146 score: 0.7907 time: 0.21s
Epoch 72/1000, LR 0.000267
Train loss: 0.4082;  Loss pred: 0.4082; Loss self: 0.0000; time: 0.39s
Val loss: 0.4809 score: 0.8409 time: 0.23s
Test loss: 0.5066 score: 0.7907 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.3874;  Loss pred: 0.3874; Loss self: 0.0000; time: 0.39s
Val loss: 0.4704 score: 0.8864 time: 0.31s
Test loss: 0.4973 score: 0.7907 time: 0.73s
Epoch 74/1000, LR 0.000267
Train loss: 0.3693;  Loss pred: 0.3693; Loss self: 0.0000; time: 0.69s
Val loss: 0.4600 score: 0.8864 time: 0.53s
Test loss: 0.4876 score: 0.8140 time: 0.39s
Epoch 75/1000, LR 0.000267
Train loss: 0.3602;  Loss pred: 0.3602; Loss self: 0.0000; time: 0.88s
Val loss: 0.4496 score: 0.9091 time: 0.57s
Test loss: 0.4771 score: 0.8140 time: 0.50s
Epoch 76/1000, LR 0.000267
Train loss: 0.3597;  Loss pred: 0.3597; Loss self: 0.0000; time: 1.18s
Val loss: 0.4399 score: 0.9318 time: 0.41s
Test loss: 0.4676 score: 0.8140 time: 0.27s
Epoch 77/1000, LR 0.000267
Train loss: 0.3289;  Loss pred: 0.3289; Loss self: 0.0000; time: 0.83s
Val loss: 0.4303 score: 0.9318 time: 0.45s
Test loss: 0.4580 score: 0.8140 time: 0.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.3311;  Loss pred: 0.3311; Loss self: 0.0000; time: 0.64s
Val loss: 0.4207 score: 0.9091 time: 0.58s
Test loss: 0.4507 score: 0.8140 time: 0.27s
Epoch 79/1000, LR 0.000267
Train loss: 0.3193;  Loss pred: 0.3193; Loss self: 0.0000; time: 1.14s
Val loss: 0.4113 score: 0.9091 time: 0.49s
Test loss: 0.4451 score: 0.8140 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.2842;  Loss pred: 0.2842; Loss self: 0.0000; time: 0.89s
Val loss: 0.4019 score: 0.9091 time: 0.35s
Test loss: 0.4395 score: 0.8140 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.2725;  Loss pred: 0.2725; Loss self: 0.0000; time: 0.86s
Val loss: 0.3926 score: 0.9091 time: 0.30s
Test loss: 0.4360 score: 0.8140 time: 0.20s
Epoch 82/1000, LR 0.000267
Train loss: 0.2592;  Loss pred: 0.2592; Loss self: 0.0000; time: 0.84s
Val loss: 0.3835 score: 0.9091 time: 0.55s
Test loss: 0.4339 score: 0.8140 time: 0.40s
Epoch 83/1000, LR 0.000266
Train loss: 0.2621;  Loss pred: 0.2621; Loss self: 0.0000; time: 0.63s
Val loss: 0.3746 score: 0.9318 time: 0.66s
Test loss: 0.4266 score: 0.8140 time: 0.29s
Epoch 84/1000, LR 0.000266
Train loss: 0.2586;  Loss pred: 0.2586; Loss self: 0.0000; time: 0.88s
Val loss: 0.3660 score: 0.9318 time: 0.45s
Test loss: 0.4213 score: 0.8140 time: 0.45s
Epoch 85/1000, LR 0.000266
Train loss: 0.2187;  Loss pred: 0.2187; Loss self: 0.0000; time: 0.89s
Val loss: 0.3577 score: 0.9091 time: 0.37s
Test loss: 0.4154 score: 0.8140 time: 0.26s
Epoch 86/1000, LR 0.000266
Train loss: 0.2048;  Loss pred: 0.2048; Loss self: 0.0000; time: 0.86s
Val loss: 0.3502 score: 0.9091 time: 0.30s
Test loss: 0.4072 score: 0.8140 time: 0.43s
Epoch 87/1000, LR 0.000266
Train loss: 0.1985;  Loss pred: 0.1985; Loss self: 0.0000; time: 0.59s
Val loss: 0.3435 score: 0.9091 time: 0.62s
Test loss: 0.3987 score: 0.8372 time: 0.34s
Epoch 88/1000, LR 0.000266
Train loss: 0.1869;  Loss pred: 0.1869; Loss self: 0.0000; time: 1.31s
Val loss: 0.3373 score: 0.9091 time: 0.69s
Test loss: 0.3911 score: 0.8372 time: 0.19s
Epoch 89/1000, LR 0.000266
Train loss: 0.1987;  Loss pred: 0.1987; Loss self: 0.0000; time: 0.67s
Val loss: 0.3333 score: 0.9091 time: 0.41s
Test loss: 0.3806 score: 0.8605 time: 0.55s
Epoch 90/1000, LR 0.000266
Train loss: 0.1669;  Loss pred: 0.1669; Loss self: 0.0000; time: 1.00s
Val loss: 0.3301 score: 0.8864 time: 0.27s
Test loss: 0.3721 score: 0.8605 time: 0.29s
Epoch 91/1000, LR 0.000266
Train loss: 0.1587;  Loss pred: 0.1587; Loss self: 0.0000; time: 0.84s
Val loss: 0.3272 score: 0.9091 time: 0.45s
Test loss: 0.3652 score: 0.8837 time: 0.51s
Epoch 92/1000, LR 0.000266
Train loss: 0.1933;  Loss pred: 0.1933; Loss self: 0.0000; time: 0.62s
Val loss: 0.3220 score: 0.9091 time: 0.78s
Test loss: 0.3634 score: 0.8837 time: 0.68s
Epoch 93/1000, LR 0.000265
Train loss: 0.1448;  Loss pred: 0.1448; Loss self: 0.0000; time: 0.85s
Val loss: 0.3153 score: 0.9091 time: 0.52s
Test loss: 0.3648 score: 0.8605 time: 0.44s
Epoch 94/1000, LR 0.000265
Train loss: 0.1304;  Loss pred: 0.1304; Loss self: 0.0000; time: 0.59s
Val loss: 0.3063 score: 0.8864 time: 0.43s
Test loss: 0.3715 score: 0.8605 time: 0.54s
Epoch 95/1000, LR 0.000265
Train loss: 0.1275;  Loss pred: 0.1275; Loss self: 0.0000; time: 0.84s
Val loss: 0.2989 score: 0.9091 time: 0.40s
Test loss: 0.3783 score: 0.8605 time: 0.48s
Epoch 96/1000, LR 0.000265
Train loss: 0.1114;  Loss pred: 0.1114; Loss self: 0.0000; time: 0.67s
Val loss: 0.2935 score: 0.9091 time: 0.40s
Test loss: 0.3826 score: 0.8605 time: 0.54s
Epoch 97/1000, LR 0.000265
Train loss: 0.1222;  Loss pred: 0.1222; Loss self: 0.0000; time: 0.93s
Val loss: 0.2874 score: 0.9091 time: 0.40s
Test loss: 0.3910 score: 0.8605 time: 0.47s
Epoch 98/1000, LR 0.000265
Train loss: 0.1028;  Loss pred: 0.1028; Loss self: 0.0000; time: 0.97s
Val loss: 0.2827 score: 0.9091 time: 0.79s
Test loss: 0.3982 score: 0.8605 time: 0.34s
Epoch 99/1000, LR 0.000265
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.60s
Val loss: 0.2814 score: 0.9091 time: 0.27s
Test loss: 0.3968 score: 0.8605 time: 0.47s
Epoch 100/1000, LR 0.000265
Train loss: 0.1034;  Loss pred: 0.1034; Loss self: 0.0000; time: 0.66s
Val loss: 0.2838 score: 0.8864 time: 0.38s
Test loss: 0.3893 score: 0.8605 time: 0.54s
     INFO: Early stopping counter 1 of 2
Epoch 101/1000, LR 0.000265
Train loss: 0.1144;  Loss pred: 0.1144; Loss self: 0.0000; time: 0.44s
Val loss: 0.2859 score: 0.8864 time: 0.74s
Test loss: 0.3862 score: 0.8605 time: 0.40s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 098,   Train_Loss: 0.1302,   Val_Loss: 0.2814,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2814,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.3968


[0.46545960498042405, 0.27674458210822195, 0.2698508290341124, 0.47736146696843207, 0.26430180307943374, 0.46257937292102724, 0.4939271570183337, 0.48757957201451063, 0.24213109002448618, 0.4850555839948356, 0.3081564479507506, 0.26851814799010754, 0.3008955700788647, 0.29447822202928364, 0.49427214497700334, 0.4067340219626203, 0.460240580025129, 0.45246625994332135, 0.3322573760524392, 0.27757831010967493, 0.2992864459520206, 0.28991120390128344, 0.2558385779848322, 2.1643191420007497, 0.2776658959919587, 0.2442546000238508, 0.5480268599931151, 0.2989716499578208, 0.5131954519310966, 0.482181471074, 0.25728534697555006, 0.5546903269132599, 0.2514519600663334, 0.6610975209623575, 0.43740429100580513, 0.48671340697910637, 0.2840684549883008, 0.26357609999831766, 0.47012843703851104, 0.6353680450702086, 0.49161284998990595, 0.28205167094711214, 0.24412600102368742, 0.4950331290019676, 0.6764270369894803, 0.26060777402017266, 0.5908879509661347, 0.5603805090067908, 0.2857805419480428, 0.6448580980068073, 0.2859691829653457, 0.462867752998136, 0.37767010799143463, 0.7379967749584466, 0.2616277019260451, 0.5009041889570653, 0.22151952900458127, 0.4725363020552322, 0.22406340297311544, 0.30281000200193375, 0.4735675279516727, 0.28206373495049775, 0.3951599590945989, 0.44620508200023323, 0.4943289649672806, 0.5667253969004378, 0.27813655603677034, 0.3455525320023298, 3.8671414819546044, 1.0187446019845083, 0.3978623040020466, 0.3271546079777181, 0.29590333194937557, 0.2980129710631445, 0.4733989629894495, 0.29934965702705085, 0.3011383470147848, 0.42546709498856217, 0.32903218804858625, 0.2537226870190352, 0.2648888509720564, 0.2792313339887187, 0.4913474329514429, 0.4077108280034736, 0.4585880381055176, 0.7265519239008427, 0.44098644400946796, 0.3204885319573805, 0.23230906995013356, 0.4360774150118232, 0.4120911529753357, 0.3295028660213575, 0.2824336210032925, 0.28405224601738155, 0.30188961408566684, 0.5040679380763322, 0.4136977429734543, 0.3346277209930122, 0.35748599900398403, 0.5453769629821181, 0.45408454700373113, 0.4817488400731236, 0.5032968129962683, 0.4225394179811701, 0.49374865100253373, 0.24151886894833297, 0.447691431036219, 0.5368635070044547, 0.8752837859792635, 0.26605870097409934, 3.3631414560368285, 3.3516027429141104, 0.2571418529842049, 0.2575427759438753, 0.2642065240070224, 0.2539486719761044, 0.242865253938362, 0.2433014790294692, 0.22909767599776387, 0.24273751908913255, 0.22875831404235214, 0.24677043210249394, 0.41345533402636647, 0.5679363970411941, 0.264947135001421, 0.2781232870183885, 0.3820105819031596, 0.2690833610249683, 0.2915277179563418, 0.2669719470432028, 0.38509102701209486, 0.43671359098516405, 0.5061273950850591, 0.30611318605951965, 0.42600725404918194, 0.285159430000931, 0.5091516959946603, 0.45491884707007557, 0.25221159402281046, 0.46174214605707675, 0.2975091189146042, 0.6068789220880717, 0.39614159893244505, 0.292188900988549, 0.44842766504734755, 0.31785793497692794, 0.5036267210962251, 0.26500659296289086, 0.22232300997711718, 0.26270973100326955, 0.28521721391007304, 0.2981059519806877, 0.6098421700298786, 0.2795462040230632, 0.485394066083245, 0.2763110090745613, 0.5560853990027681, 0.2550034620799124, 0.45459087402559817, 0.269984787912108, 0.5085196349537, 0.5372439329512417, 4.431808320921846, 0.27439472102560103, 0.26167259900830686, 0.22986306005623192, 0.2590017099864781, 0.24502437794581056, 0.2282421119743958, 0.28757710999343544, 0.26194397697690874, 0.2765543849673122, 0.26156489399727434, 0.251346156001091, 0.5162586529040709, 0.32817080500535667, 0.3301945829298347, 0.27532678295392543, 0.6129567279713228, 0.2980951339704916, 0.7240242339903489, 0.2864055280806497, 0.33475594595074654, 0.2768953850027174, 0.4493200359866023, 0.3423772680107504, 0.6465292890788987, 0.21742080803960562, 0.5518820349825546, 0.48384289199020714, 0.3762317639775574, 0.47191058401949704, 0.24427751102484763, 0.2797749900491908, 0.7619992019608617, 0.2457353820791468, 0.2329735190141946, 0.5046052880352363, 0.2987817790126428, 0.47987971594557166, 0.25400900293607265, 0.5312616640003398, 0.28726243402343243, 0.25864216906484216, 0.28813572507351637, 0.5732966349460185, 0.3621011630166322, 0.22150554903782904, 0.44080622598994523, 0.32138515694532543, 0.23345946590416133, 0.24071562301833183, 3.701011618017219, 0.42960307805333287, 0.26430829404853284, 0.5332523670513183, 0.5376905760494992, 0.31197686307132244, 0.5097816130146384, 0.26600284792948514, 0.24634093302302063, 0.7771417520707473, 0.5999603018863127, 0.239887431031093, 0.4711918589891866, 0.2560540910344571, 0.26262093195691705, 0.5644252499332651, 0.23911945801228285, 0.2154165479587391, 0.2542402899125591, 0.2492561979452148, 0.46390332200098783, 0.4796090329764411, 0.26192846707999706, 0.23290114104747772, 0.4283798149554059, 0.2299737570574507, 0.30976757500320673, 0.31288924894761294, 0.2678373589878902, 0.4641227179672569, 0.22439989598933607, 0.2520392790902406, 0.5010941360378638, 0.24036753794644028, 0.2454100149916485, 0.24024598894175142, 0.5451507180696353, 0.27545735298190266, 0.45988492702599615, 0.2154419249854982, 3.6350415450287983, 0.2877479400485754, 0.23247737402562052, 0.21374215104151517, 0.2275027099531144, 0.209955302067101, 0.24080868193414062, 0.7308311549713835, 0.3912791459588334, 0.5075668609933928, 0.27637740201316774, 0.22322294593323022, 0.27343960001599044, 0.24785514501854777, 0.25984253303613514, 0.2011774629354477, 0.40210675599519163, 0.2956651400309056, 0.45094655302818865, 0.2602726500481367, 0.43293406011071056, 0.34739127289503813, 0.19615121197421104, 0.5507728080265224, 0.29412514402065426, 0.5186462349956855, 0.6824499359354377, 0.44070181204006076, 0.5420301840640604, 0.48967090994119644, 0.5505812639603391, 0.4704737310530618, 0.34820435498841107, 0.472410521004349, 0.543828715919517, 0.3995030289515853]
[0.010578627385918728, 0.006289649593368681, 0.006132973387138918, 0.010849124249282548, 0.0060068591608962215, 0.010513167566386983, 0.011225617204962129, 0.011081353909420695, 0.005502979318738322, 0.011023990545337174, 0.007003555635244332, 0.006102685181593353, 0.006838535683610561, 0.006692686864301901, 0.01123345784038644, 0.009243955044605007, 0.010460013182389295, 0.010283324089620939, 0.0075513040011918, 0.006308597957038067, 0.00680196468072774, 0.006588890997756442, 0.005814513136018914, 0.04918907140910795, 0.0063105885452717884, 0.005551240909632973, 0.012455155908934434, 0.006794810226314108, 0.011663532998434013, 0.010958669797136363, 0.005847394249444319, 0.012606598338937725, 0.005714817274234851, 0.015024943658235397, 0.009941006613768299, 0.011061668340434235, 0.006456101249734109, 0.005990365909052674, 0.010684737205420706, 0.01444018284250474, 0.011173019317952409, 0.006410265248798003, 0.005548318205083805, 0.0112507529318629, 0.015373341749760915, 0.005922903955003924, 0.013429271612866696, 0.012735920659245246, 0.006495012317000973, 0.014655865863791074, 0.006499299612848766, 0.010519721659048546, 0.008583411545259878, 0.016772653976328333, 0.005946084134682844, 0.011384186112660576, 0.00503453475010412, 0.010739461410346186, 0.0050923500675708055, 0.006882045500043949, 0.010762898362538015, 0.00641053943069313, 0.008980908161240884, 0.010141024590914392, 0.011234749203801832, 0.012880122656828131, 0.0063212853644720535, 0.007853466636416588, 0.08788957913533192, 0.023153286408738826, 0.009042325090955605, 0.007435331999493594, 0.0067250757261221724, 0.006773022069616921, 0.010759067340669308, 0.006803401296069337, 0.0068440533412451095, 0.009669706704285503, 0.007478004273831506, 0.005766424704978073, 0.006020201158455827, 0.006346166681561788, 0.011166987112532794, 0.009266155181897127, 0.010422455411489036, 0.01651254372501915, 0.010022419182033364, 0.0072838302717586475, 0.005279751589775763, 0.0099108503411778, 0.00936570802216672, 0.0074887015004853974, 0.006568223744262617, 0.0066058661864507335, 0.007020688699666671, 0.011722510187821679, 0.009620877743568705, 0.007782040023093306, 0.008313627883813582, 0.012683185185630655, 0.010560105744272817, 0.011203461397049386, 0.011704577046424844, 0.009826498092585352, 0.011482526767500785, 0.005616717882519371, 0.010411428628749279, 0.012485197837312901, 0.020355436883238685, 0.0061874116505604496, 0.07821259200085648, 0.07794424983521187, 0.005980043092655928, 0.005989366882415705, 0.006144337767605172, 0.00590578306921173, 0.005648029161357256, 0.005658173930917888, 0.005327852930180555, 0.005645058583468199, 0.005319960791682608, 0.005738847258197534, 0.009615240326194568, 0.013207823187004515, 0.006161561279102813, 0.006467983419032291, 0.008883967021003712, 0.006257752581976007, 0.006779714371077716, 0.006208649931237275, 0.008955605279351042, 0.010156130022910792, 0.01177040453686184, 0.007118911303709759, 0.009907145443004231, 0.006631614651184442, 0.01184073711615489, 0.010579508071397107, 0.00586538590750722, 0.01073818944318783, 0.006918816718944284, 0.01411346330437376, 0.00921259532401035, 0.00679509072066393, 0.010428550349938314, 0.00739204499946344, 0.011712249327819188, 0.006162944022392811, 0.005170302557607376, 0.006109528627983013, 0.006632958463024955, 0.006932696557690411, 0.01418237604720648, 0.00650107451216426, 0.011288234094959186, 0.006425837420338635, 0.012932218581459723, 0.005930313071625871, 0.010571880791292981, 0.006278715997956, 0.01182603802217907, 0.012494044952354458, 0.10306530978888014, 0.006381272581990722, 0.00608540927926295, 0.0053456525594472535, 0.006023295581080886, 0.0056982413475769895, 0.005307956092427809, 0.006687839767289197, 0.006091720394811831, 0.006431497324821215, 0.006082904511564519, 0.005845259441885837, 0.012006015183815603, 0.007631879186171086, 0.007678943789065923, 0.006402948440788964, 0.014254807627240067, 0.006932444976057944, 0.016837772883496485, 0.006660593676294179, 0.007785021998854571, 0.00643942755820273, 0.010449303162479124, 0.007962262046761637, 0.01503556486229997, 0.005056297861386177, 0.01283446592982685, 0.011252160278842026, 0.008749575906454823, 0.010974664744639467, 0.005680872349415061, 0.0065063951174230425, 0.01772091167350841, 0.005714776327422019, 0.005417988814283596, 0.011735006698493868, 0.006948413465410298, 0.011159993394083061, 0.0059071861147923874, 0.012354922418612553, 0.006680521721475173, 0.006014934164298655, 0.006700830815663171, 0.013332479882465546, 0.008420957279456564, 0.0051512918380890475, 0.010251307581161518, 0.007474073417333149, 0.005429289904747938, 0.0055980377446123684, 0.08607003762830742, 0.009990769257054253, 0.006146704512756578, 0.01240121783840275, 0.012504432001151145, 0.007255275885379592, 0.011855386349177637, 0.006186112742546166, 0.005728858907512108, 0.018073064001645288, 0.013952565160146806, 0.005578777465839372, 0.01095795020905085, 0.005954746303126909, 0.006107463533881792, 0.01312616860309919, 0.005560917628192624, 0.005009687161831142, 0.0059125648816874205, 0.005796655766167786, 0.010788449348860182, 0.011153698441312584, 0.006091359699534816, 0.0054163056057552955, 0.009962321278032696, 0.005348226908312807, 0.0072038970930978306, 0.007276494161572394, 0.006228775790416051, 0.010793551580633881, 0.005218602232310141, 0.005861378583493967, 0.011653352000880553, 0.005589942742940472, 0.005707209650968569, 0.005587116021901196, 0.012677923676038032, 0.006405984953067503, 0.010694998302930143, 0.005010277325244144, 0.08453584988439065, 0.006691812559269195, 0.005406450558735361, 0.0049707476986398874, 0.0052907606965840554, 0.004882681443420953, 0.005600201905445131, 0.016996073371427525, 0.00909951502229845, 0.011803880488218437, 0.0064273814421666915, 0.005191231300772796, 0.0063590604654881495, 0.005764073139966228, 0.006042849605491515, 0.004678545649661575, 0.009351319906864922, 0.006875933489090828, 0.010487129140190433, 0.006052852326700854, 0.010068233956063037, 0.008078866811512514, 0.004561656092423512, 0.012808669954105172, 0.006840119628387309, 0.012061540348736872, 0.015870928742684597, 0.010248879349768855, 0.012605353117768848, 0.011387695580027824, 0.01280421544093812, 0.01094124955937353, 0.008097775697404908, 0.010986291186147652, 0.012647179439988767, 0.009290768115153147]
[94.53022244937993, 158.99136909857785, 163.0530473354146, 92.17333832877156, 166.47635198605195, 95.11881111809063, 89.08196152973788, 90.2416805901182, 181.7197452650561, 90.71125341476012, 142.78461571257486, 163.8622950789196, 146.230135553234, 149.41682171534075, 89.01978484352412, 108.1788038966745, 95.60217397083414, 97.24481999058163, 132.42745886567047, 158.5138261797725, 147.01634703181233, 151.770609096509, 171.98344497759288, 20.32971900776395, 158.46382517669457, 180.1399031817764, 80.28803551809993, 147.17114484335744, 85.73731476854084, 91.25195105899736, 171.01634631443406, 79.32353939693009, 174.98372249074032, 66.55599000878016, 100.59343473476817, 90.40227651236415, 154.89224244139362, 166.9347106975209, 93.59144551469814, 69.25120068815858, 89.50132202789945, 155.99978490554838, 180.23479602949257, 88.8829401957563, 65.04766603627685, 168.8360992508003, 74.46420243983238, 78.51807707942034, 153.96429616960958, 68.23206552883441, 153.86273284325188, 95.05954933131234, 116.50379277832043, 59.6208567476158, 168.1779095871032, 87.84115000438024, 198.62808573906833, 93.1145391552522, 196.37298825314815, 145.30563623760028, 92.91177583546263, 155.9931127187336, 111.34731388477209, 98.6093654576014, 89.009552581877, 77.63901219293673, 158.19567419315817, 127.33230384693965, 11.377913170572874, 43.190412900631095, 110.59102497876668, 134.49298566198632, 148.6972103697964, 147.64458017727375, 92.94485928348055, 146.98530286281212, 146.11224520615357, 103.41575298833122, 133.72551865200123, 173.41768100027633, 166.10740632734914, 157.57543887169072, 89.54966903093248, 107.9196258177993, 95.94668055837063, 60.560021317905104, 99.77630967507773, 137.29040390703045, 189.40285030388546, 100.89951574035781, 106.77249361534696, 133.53449859567547, 152.24816311617064, 151.38060199449637, 142.43616869773447, 85.30596126407153, 103.94062024834199, 128.5010096366103, 120.28443105409782, 78.84454775074518, 94.69602144299942, 89.25812876575505, 85.43666260076006, 101.7656534991399, 87.08884553444449, 178.0399195608969, 96.04829804419738, 80.0948461554553, 49.12692396317132, 161.61846931736326, 12.785664998662227, 12.829682781143951, 167.22287523782177, 166.96255541398187, 162.7514693727135, 169.32555569357788, 177.05291021544477, 176.73546487069135, 187.6928686104162, 177.14608010066428, 187.97131015766715, 174.2510220273891, 104.00156065529885, 75.71270343654534, 162.2965275686766, 154.60769380723232, 112.56232690145892, 159.80177977637948, 147.49883922337543, 161.0656118601162, 111.66191103863208, 98.462701614113, 84.95884715502008, 140.47091715820181, 100.93724834797229, 150.7928389387635, 84.45420164219775, 94.522353331684, 170.49176571998115, 93.12556882057869, 144.53338491564875, 70.8543309628403, 108.54704508660524, 147.16506977000194, 95.89060477671424, 135.28056174882406, 85.38069605680084, 162.26011405694095, 193.4122788479061, 163.6787485403989, 150.7622888903108, 144.24401698220936, 70.51004688293901, 153.82072580907735, 88.587815559792, 155.6217399517246, 77.32625254522453, 168.62516159300125, 94.59054824223914, 159.2682326013064, 84.55917342093406, 80.03813047043292, 9.702585691038125, 156.70855415614247, 164.32748466198768, 187.06790029455283, 166.02206990156526, 175.4927422344476, 188.39643406745088, 149.52511345907038, 164.15723887322133, 155.48478829971347, 164.39515006340295, 171.07880496017353, 83.29158215192199, 131.0293278504714, 130.2262430184609, 156.17804972934957, 70.15177097788828, 144.2492516642575, 59.39027726048902, 150.13676687096458, 128.45178859444874, 155.2933068912579, 95.70016147973905, 125.5924502518369, 66.5089744986828, 197.77315882372707, 77.91520157266808, 88.87182329604278, 114.29125373519763, 91.11895654839445, 176.0293029824841, 153.69493889514433, 56.43050529363756, 174.9849762625982, 184.57033306596574, 85.21511965803523, 143.9177453929695, 89.60578780720353, 169.28533832646067, 80.93939938412815, 149.68890779673762, 166.2528587487209, 149.23522582640098, 75.0048009684356, 118.75134462913864, 194.12606224441086, 97.54853145150638, 133.795849219369, 184.1861491178608, 178.63402242373488, 11.618445019374684, 100.09239271479751, 162.68880306913204, 80.63724168309568, 79.97164524609683, 137.83073390980783, 84.3498449183283, 161.65240460658111, 174.55483127516464, 55.33096103178546, 71.67140869955115, 179.2507419633276, 91.2579434038711, 167.93326685889002, 163.73409263148204, 76.18369306667996, 179.8264363655057, 199.61326280391523, 169.13133640143738, 172.5132628776243, 92.6917268333518, 89.6563597502389, 164.1669593204893, 184.62769141708196, 100.3782122751892, 186.97785586578033, 138.8137541495583, 137.4288191257076, 160.54519116559902, 92.64791042405638, 191.62219220477465, 170.60832801622243, 85.81221951627633, 178.89270892137458, 175.21697311930535, 178.98321711595992, 78.87726930317893, 156.10401949526127, 93.50165111536546, 199.58975024426843, 11.829300839437678, 149.436343463453, 184.9642365422672, 201.17697791694866, 189.00873756124395, 204.8054970588804, 178.56499049216248, 58.83711950085613, 109.89596671355453, 84.71790281154654, 155.58435561946308, 192.63252628545646, 157.2559351223649, 173.48843009404615, 165.48483998199086, 213.74163573082495, 106.93677576636935, 145.43479828398185, 95.35498100882938, 165.21136581983265, 99.32228475857033, 123.77973586282967, 219.21863019461446, 78.0721186183348, 146.1962735052003, 82.90815029315253, 63.008284909661064, 97.57164328630263, 79.33137538133485, 87.81407906212722, 78.09927946094709, 91.39723891438754, 123.49070131943388, 91.02252826330289, 79.0690133515563, 107.63372711552346]
Elapsed: 0.4525611266977244~0.5104863313279482
Time per graph: 0.010447700848961987~0.011812412483459005
Speed: 126.54565128296538~44.325946168183044
Total Time: 0.4024
best val loss: 0.2814330756664276 test_score: 0.8605

Testing...
Test loss: 0.4676 score: 0.8140 time: 0.27s
test Score 0.8140
Epoch Time List: [1.4978335000341758, 1.7319690879667178, 1.1010903398273513, 1.7651524341199547, 1.4266435150057077, 1.3117675218963996, 1.582302390015684, 1.583562702871859, 1.0731592919910327, 1.5531772560207173, 1.3472630050964653, 0.9930485709337518, 1.2606335770105943, 1.2950805480359122, 1.5502120670862496, 1.764099970809184, 1.600817508995533, 1.2955862201051787, 1.4572128258878365, 1.1912567089311779, 1.3007659671129659, 1.817173390998505, 1.8182459148811176, 2.8317635589046404, 10.590977359097451, 12.872784472070634, 1.2521445750026032, 1.6069865729659796, 1.280645874910988, 1.579662550939247, 1.7252446849597618, 1.3412245570216328, 1.5364911400247365, 1.5416439450345933, 1.305025601061061, 1.343354952055961, 1.7193118560826406, 1.592377250897698, 1.7840605018427595, 1.4981972131645307, 1.8753794390941039, 1.8787904001073912, 1.4968482240801677, 1.3307059411890805, 1.4944920140551403, 1.4293092489242554, 1.696197395096533, 2.2073390140431, 1.6297502090455964, 1.5105931790312752, 1.083161060931161, 1.7547707168851048, 1.260581401991658, 1.9052891748724505, 1.6450478410115466, 1.597282923059538, 2.5831246699672192, 1.5585185080999509, 1.030358660966158, 2.466092560091056, 1.5141923119081184, 1.6697334020864218, 1.677684896858409, 1.661356445052661, 2.1242184080183506, 1.797703380114399, 1.7845555449603125, 1.1238430730300024, 8.062608361011371, 5.74445052491501, 11.619161146110855, 1.4700658469228074, 1.726659759064205, 1.3991077480604872, 1.7913692030124366, 1.6194597850553691, 1.5538142679724842, 1.579953387961723, 1.1424518751446158, 1.9934481577947736, 2.2141959820874035, 1.4194038428831846, 1.5674257137579843, 1.3878520488506183, 1.5542415090603754, 1.6045608730055392, 1.735110031091608, 1.837497841916047, 1.4814229330513626, 1.2194120400818065, 1.615424201823771, 1.7266734630102292, 1.6406757960794494, 1.3407422309974208, 1.3175701870350167, 1.292597500840202, 1.9030127589358017, 1.7144522219896317, 1.3945667441003025, 2.0833642311627045, 1.815530380117707, 1.6847440800629556, 1.630772195989266, 1.7324693500995636, 2.0395870229694992, 1.092702120076865, 1.3357080980204046, 1.6143955900333822, 2.1507918699644506, 1.6142310908762738, 12.981022331980057, 3.9847688989248127, 9.642814612016082, 0.8919608938740566, 0.8459050939418375, 0.8050568262115121, 0.9759593880735338, 0.9134176869411021, 0.7839593950193375, 0.8339009361807257, 0.8054846519371495, 0.8409378739306703, 0.9852423691190779, 2.319019579095766, 1.794862091075629, 1.0870719600934535, 1.2907635719748214, 2.000856130034663, 1.5296406210400164, 1.2853072748985142, 1.6619175270898268, 1.9906312329694629, 1.738057491951622, 1.9156134651275352, 1.2849804310826585, 1.3739327939692885, 2.0121307101799175, 1.7381864589406177, 1.0859195899683982, 1.6259700549999252, 1.762152866111137, 2.1331540369428694, 2.359792808885686, 1.5415823588846251, 1.7836879610549659, 2.723958290182054, 2.3451370960101485, 2.2060474670724943, 2.0330566819757223, 0.9992624419974163, 1.575862335972488, 1.7743365609785542, 1.6333259268430993, 1.2943810910219327, 1.517671469016932, 1.3071411800337955, 1.3343670739559457, 1.7395210879622027, 1.5213862298987806, 1.5513818640029058, 2.064566060900688, 1.468760950025171, 5.168223493965343, 8.654573645093478, 12.551701462129131, 0.8074624739820138, 0.9938796308124438, 0.9549744100077078, 0.8286172408843413, 0.9822467520134524, 0.917885096045211, 0.8920921849785373, 1.3264737679855898, 1.0512735630618408, 1.5720926449866965, 1.6114631820237264, 1.5286282090237364, 1.7919768679421395, 1.5745241040131077, 1.5854066361207515, 1.5687776850536466, 1.7975619680946693, 1.846962547977455, 1.4180175139335915, 1.3926004369277507, 1.4292404100997373, 1.5138135849265382, 1.2720975449774414, 1.5740516339428723, 1.785540160140954, 1.3394593100529164, 1.7418474989244714, 1.3636221858905628, 1.5261752438964322, 2.292712934082374, 1.040630951989442, 0.9691621731035411, 1.8047798979096115, 1.77404691290576, 1.5172759030247107, 1.8503659140551463, 1.5738496610429138, 1.6043229659553617, 2.121789108030498, 1.6506074029020965, 1.9963041209848598, 1.4999628070509061, 1.619700010982342, 1.748242137953639, 1.7610742220422253, 0.9559753568610176, 12.409153332118876, 8.10080075613223, 5.038984167971648, 1.5671406749170274, 1.6152787470491603, 1.555970027926378, 1.338618895970285, 1.8049403330078349, 2.046540705137886, 1.803496939013712, 1.580458358861506, 2.193691020947881, 2.0872905030846596, 1.8803174280328676, 1.8458726020762697, 2.091082834871486, 1.6585258609848097, 1.5238396840868518, 1.844512881943956, 2.0881940430263057, 1.5933573740767315, 2.234719622065313, 1.5956764080328867, 1.6093327950220555, 1.5404924839967862, 2.0539311539614573, 1.7882296449970454, 1.5412691631354392, 1.760991262854077, 1.5797613251488656, 2.0345278920140117, 1.270759995910339, 1.5592946260003373, 1.3070089999819174, 1.029382755048573, 2.0096592541085556, 1.8056105739669874, 1.8347938051447272, 1.586993267876096, 2.294568400015123, 2.129155697999522, 8.430786111974157, 5.061088089947589, 12.300681221066043, 0.8578912620432675, 0.839145109988749, 0.7909896818455309, 0.8625647448934615, 1.4239062040578574, 1.6048763019498438, 1.9558382249670103, 1.853399718995206, 1.4955317421117797, 1.493358721025288, 1.868967924034223, 1.502200118964538, 1.3526166719384491, 1.7817808028776199, 1.5801147400634363, 1.773889133008197, 1.5233844389440492, 1.589766504126601, 1.5527385961031541, 2.1944682949688286, 1.6222226419486105, 1.5509314920054749, 1.8069269011029974, 2.0776824000058696, 1.8111041560769081, 1.5597967191133648, 1.732737780897878, 1.617124469135888, 1.793122959905304, 2.0952843440463766, 1.3414555999916047, 1.5773884401423857, 1.5686433269875124]
Total Epoch List: [92, 95, 101]
Total Time List: [0.3300163180101663, 0.6471302880672738, 0.4023720920085907]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334670>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.34s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.69s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.29s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.33s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.94s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.32s
     INFO: Early stopping counter 1 of 2
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.30s
     INFO: Early stopping counter 1 of 2
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.26s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 005,   Train_Loss: 0.6929,   Val_Loss: 0.6933,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6933,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6930


[0.3409481570124626, 0.29934434790629894, 0.2467072430299595, 0.3319905719254166, 0.32373185793403536, 0.2523573429789394, 0.30996870808303356, 0.26098833594005555]
[0.007748821750283241, 0.006803280634234066, 0.005606982796135443, 0.007545240271032195, 0.007357542225773531, 0.005735394158612259, 0.00704474336552349, 0.005931553089546717]
[129.0518781082359, 146.9879097692966, 178.34904018061906, 132.53388415465253, 135.91495221012687, 174.3559330614447, 141.949812521764, 168.58990974257958]
Elapsed: 0.2957545706012752~0.03509062488103164
Time per graph: 0.006721694786392618~0.0007975142018416281
Speed: 150.9666649685899~18.544354127847622
Total Time: 0.2612
best val loss: 0.6932737827301025 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.23s
test Score 0.5000
Epoch Time List: [1.1201642368687317, 1.2768734149867669, 1.2420692391460761, 1.1716866868082434, 1.6731043020263314, 1.274855533032678, 1.4292868719203398, 1.5099459220655262]
Total Epoch List: [8]
Total Time List: [0.2612073279451579]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288336c50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 10.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 11.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6970 score: 0.4884 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.4884 time: 0.79s
Epoch 4/1000, LR 0.000060
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.72s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6968 score: 0.4884 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.71s
Epoch 6/1000, LR 0.000120
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.4884 time: 0.26s
Epoch 7/1000, LR 0.000150
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 1.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6962 score: 0.4884 time: 0.87s
Epoch 8/1000, LR 0.000180
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6959 score: 0.4884 time: 0.57s
Epoch 9/1000, LR 0.000210
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4884 time: 0.27s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.81s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4884 time: 0.27s
Epoch 11/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.4884 time: 0.32s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.32s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 1.01s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.33s
Epoch 14/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.27s
Epoch 16/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.53s
Epoch 17/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.29s
Epoch 18/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.48s
Epoch 19/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.29s
Epoch 20/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.59s
Val loss: 0.6910 score: 0.5227 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.86s
Val loss: 0.6906 score: 0.5455 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.35s
Epoch 22/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 1.02s
Val loss: 0.6902 score: 0.5909 time: 0.75s
Test loss: 0.6917 score: 0.5814 time: 0.93s
Epoch 23/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.62s
Val loss: 0.6898 score: 0.6591 time: 0.25s
Test loss: 0.6914 score: 0.4884 time: 0.27s
Epoch 24/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.87s
Val loss: 0.6893 score: 0.6591 time: 0.24s
Test loss: 0.6910 score: 0.5814 time: 0.73s
Epoch 25/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.57s
Val loss: 0.6888 score: 0.7045 time: 0.22s
Test loss: 0.6907 score: 0.6279 time: 0.54s
Epoch 26/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.60s
Val loss: 0.6882 score: 0.6136 time: 0.69s
Test loss: 0.6902 score: 0.6512 time: 0.23s
Epoch 27/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.84s
Val loss: 0.6876 score: 0.6591 time: 0.26s
Test loss: 0.6898 score: 0.6047 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6837;  Loss pred: 0.6837; Loss self: 0.0000; time: 0.76s
Val loss: 0.6869 score: 0.5909 time: 0.37s
Test loss: 0.6894 score: 0.6047 time: 0.37s
Epoch 29/1000, LR 0.000270
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.54s
Val loss: 0.6861 score: 0.5682 time: 0.24s
Test loss: 0.6888 score: 0.5581 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.61s
Val loss: 0.6853 score: 0.5455 time: 0.54s
Test loss: 0.6883 score: 0.5581 time: 0.41s
Epoch 31/1000, LR 0.000270
Train loss: 0.6800;  Loss pred: 0.6800; Loss self: 0.0000; time: 0.60s
Val loss: 0.6844 score: 0.5455 time: 0.48s
Test loss: 0.6877 score: 0.5581 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.80s
Val loss: 0.6835 score: 0.5682 time: 0.64s
Test loss: 0.6870 score: 0.5581 time: 0.31s
Epoch 33/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.84s
Val loss: 0.6824 score: 0.5682 time: 0.42s
Test loss: 0.6864 score: 0.5581 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.78s
Val loss: 0.6812 score: 0.5682 time: 0.29s
Test loss: 0.6856 score: 0.5581 time: 0.54s
Epoch 35/1000, LR 0.000270
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 0.82s
Val loss: 0.6799 score: 0.5682 time: 0.49s
Test loss: 0.6848 score: 0.5581 time: 0.43s
Epoch 36/1000, LR 0.000270
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.59s
Val loss: 0.6785 score: 0.5909 time: 0.28s
Test loss: 0.6839 score: 0.5581 time: 0.71s
Epoch 37/1000, LR 0.000270
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.62s
Val loss: 0.6770 score: 0.5909 time: 0.46s
Test loss: 0.6829 score: 0.5581 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.95s
Val loss: 0.6753 score: 0.5909 time: 0.38s
Test loss: 0.6818 score: 0.5581 time: 0.32s
Epoch 39/1000, LR 0.000269
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.55s
Val loss: 0.6734 score: 0.5909 time: 0.24s
Test loss: 0.6807 score: 0.5581 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 1.06s
Val loss: 0.6715 score: 0.5909 time: 0.45s
Test loss: 0.6795 score: 0.5581 time: 0.48s
Epoch 41/1000, LR 0.000269
Train loss: 0.6546;  Loss pred: 0.6546; Loss self: 0.0000; time: 0.59s
Val loss: 0.6693 score: 0.5909 time: 0.29s
Test loss: 0.6781 score: 0.5581 time: 0.26s
Epoch 42/1000, LR 0.000269
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 0.39s
Val loss: 0.6670 score: 0.5909 time: 0.64s
Test loss: 0.6767 score: 0.5581 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6468;  Loss pred: 0.6468; Loss self: 0.0000; time: 1.13s
Val loss: 0.6644 score: 0.5909 time: 0.90s
Test loss: 0.6751 score: 0.5581 time: 0.31s
Epoch 44/1000, LR 0.000269
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.58s
Val loss: 0.6616 score: 0.5909 time: 0.40s
Test loss: 0.6733 score: 0.5581 time: 0.35s
Epoch 45/1000, LR 0.000269
Train loss: 0.6375;  Loss pred: 0.6375; Loss self: 0.0000; time: 0.69s
Val loss: 0.6585 score: 0.5909 time: 0.58s
Test loss: 0.6714 score: 0.5581 time: 0.26s
Epoch 46/1000, LR 0.000269
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 0.86s
Val loss: 0.6551 score: 0.5909 time: 0.22s
Test loss: 0.6693 score: 0.5581 time: 0.27s
Epoch 47/1000, LR 0.000269
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.41s
Val loss: 0.6513 score: 0.6136 time: 4.38s
Test loss: 0.6670 score: 0.5581 time: 4.01s
Epoch 48/1000, LR 0.000269
Train loss: 0.6181;  Loss pred: 0.6181; Loss self: 0.0000; time: 3.68s
Val loss: 0.6472 score: 0.6136 time: 0.24s
Test loss: 0.6644 score: 0.5581 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6123;  Loss pred: 0.6123; Loss self: 0.0000; time: 10.87s
Val loss: 0.6427 score: 0.6136 time: 1.08s
Test loss: 0.6617 score: 0.5814 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6057;  Loss pred: 0.6057; Loss self: 0.0000; time: 0.36s
Val loss: 0.6378 score: 0.6136 time: 0.26s
Test loss: 0.6586 score: 0.5814 time: 0.37s
Epoch 51/1000, LR 0.000269
Train loss: 0.5950;  Loss pred: 0.5950; Loss self: 0.0000; time: 0.64s
Val loss: 0.6325 score: 0.6364 time: 0.65s
Test loss: 0.6553 score: 0.5814 time: 0.37s
Epoch 52/1000, LR 0.000269
Train loss: 0.5848;  Loss pred: 0.5848; Loss self: 0.0000; time: 1.25s
Val loss: 0.6268 score: 0.5909 time: 0.43s
Test loss: 0.6518 score: 0.5814 time: 0.26s
Epoch 53/1000, LR 0.000269
Train loss: 0.5753;  Loss pred: 0.5753; Loss self: 0.0000; time: 1.09s
Val loss: 0.6206 score: 0.6136 time: 0.22s
Test loss: 0.6480 score: 0.5814 time: 0.31s
Epoch 54/1000, LR 0.000269
Train loss: 0.5681;  Loss pred: 0.5681; Loss self: 0.0000; time: 0.63s
Val loss: 0.6141 score: 0.6364 time: 0.28s
Test loss: 0.6439 score: 0.6047 time: 0.29s
Epoch 55/1000, LR 0.000269
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.60s
Val loss: 0.6071 score: 0.6591 time: 0.45s
Test loss: 0.6396 score: 0.6279 time: 0.47s
Epoch 56/1000, LR 0.000269
Train loss: 0.5496;  Loss pred: 0.5496; Loss self: 0.0000; time: 0.85s
Val loss: 0.5997 score: 0.7045 time: 0.44s
Test loss: 0.6351 score: 0.6512 time: 0.47s
Epoch 57/1000, LR 0.000269
Train loss: 0.5309;  Loss pred: 0.5309; Loss self: 0.0000; time: 0.72s
Val loss: 0.5920 score: 0.7500 time: 0.54s
Test loss: 0.6304 score: 0.6279 time: 0.30s
Epoch 58/1000, LR 0.000269
Train loss: 0.5217;  Loss pred: 0.5217; Loss self: 0.0000; time: 0.96s
Val loss: 0.5839 score: 0.7955 time: 0.38s
Test loss: 0.6256 score: 0.6744 time: 0.30s
Epoch 59/1000, LR 0.000268
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 1.11s
Val loss: 0.5756 score: 0.8182 time: 0.69s
Test loss: 0.6206 score: 0.6744 time: 0.38s
Epoch 60/1000, LR 0.000268
Train loss: 0.4945;  Loss pred: 0.4945; Loss self: 0.0000; time: 0.87s
Val loss: 0.5671 score: 0.8409 time: 0.38s
Test loss: 0.6154 score: 0.6744 time: 0.27s
Epoch 61/1000, LR 0.000268
Train loss: 0.4819;  Loss pred: 0.4819; Loss self: 0.0000; time: 0.80s
Val loss: 0.5583 score: 0.8182 time: 0.30s
Test loss: 0.6102 score: 0.6977 time: 0.49s
Epoch 62/1000, LR 0.000268
Train loss: 0.4710;  Loss pred: 0.4710; Loss self: 0.0000; time: 0.79s
Val loss: 0.5495 score: 0.7955 time: 0.45s
Test loss: 0.6048 score: 0.6977 time: 0.28s
Epoch 63/1000, LR 0.000268
Train loss: 0.4578;  Loss pred: 0.4578; Loss self: 0.0000; time: 0.88s
Val loss: 0.5405 score: 0.7955 time: 0.42s
Test loss: 0.5994 score: 0.7209 time: 0.29s
Epoch 64/1000, LR 0.000268
Train loss: 0.4386;  Loss pred: 0.4386; Loss self: 0.0000; time: 1.56s
Val loss: 0.5316 score: 0.7955 time: 0.43s
Test loss: 0.5938 score: 0.7209 time: 0.28s
Epoch 65/1000, LR 0.000268
Train loss: 0.4393;  Loss pred: 0.4393; Loss self: 0.0000; time: 1.36s
Val loss: 0.5228 score: 0.7955 time: 0.22s
Test loss: 0.5882 score: 0.7442 time: 0.30s
Epoch 66/1000, LR 0.000268
Train loss: 0.4198;  Loss pred: 0.4198; Loss self: 0.0000; time: 0.86s
Val loss: 0.5142 score: 0.7955 time: 0.45s
Test loss: 0.5828 score: 0.7442 time: 0.41s
Epoch 67/1000, LR 0.000268
Train loss: 0.4050;  Loss pred: 0.4050; Loss self: 0.0000; time: 0.95s
Val loss: 0.5057 score: 0.7955 time: 0.26s
Test loss: 0.5775 score: 0.7907 time: 0.46s
Epoch 68/1000, LR 0.000268
Train loss: 0.3919;  Loss pred: 0.3919; Loss self: 0.0000; time: 0.90s
Val loss: 0.4974 score: 0.7955 time: 0.25s
Test loss: 0.5722 score: 0.7674 time: 0.29s
Epoch 69/1000, LR 0.000268
Train loss: 0.3842;  Loss pred: 0.3842; Loss self: 0.0000; time: 0.83s
Val loss: 0.4896 score: 0.8182 time: 0.54s
Test loss: 0.5670 score: 0.7674 time: 0.28s
Epoch 70/1000, LR 0.000268
Train loss: 0.3699;  Loss pred: 0.3699; Loss self: 0.0000; time: 1.42s
Val loss: 0.4820 score: 0.8182 time: 0.71s
Test loss: 0.5621 score: 0.7674 time: 0.26s
Epoch 71/1000, LR 0.000268
Train loss: 0.3603;  Loss pred: 0.3603; Loss self: 0.0000; time: 0.87s
Val loss: 0.4750 score: 0.8409 time: 0.45s
Test loss: 0.5575 score: 0.7674 time: 0.50s
Epoch 72/1000, LR 0.000267
Train loss: 0.3515;  Loss pred: 0.3515; Loss self: 0.0000; time: 0.88s
Val loss: 0.4683 score: 0.8409 time: 0.28s
Test loss: 0.5531 score: 0.8140 time: 0.28s
Epoch 73/1000, LR 0.000267
Train loss: 0.3506;  Loss pred: 0.3506; Loss self: 0.0000; time: 0.75s
Val loss: 0.4622 score: 0.8409 time: 0.24s
Test loss: 0.5490 score: 0.8140 time: 0.56s
Epoch 74/1000, LR 0.000267
Train loss: 0.3331;  Loss pred: 0.3331; Loss self: 0.0000; time: 0.92s
Val loss: 0.4562 score: 0.8636 time: 0.48s
Test loss: 0.5451 score: 0.8140 time: 0.42s
Epoch 75/1000, LR 0.000267
Train loss: 0.3270;  Loss pred: 0.3270; Loss self: 0.0000; time: 0.63s
Val loss: 0.4503 score: 0.8636 time: 0.84s
Test loss: 0.5415 score: 0.8140 time: 0.67s
Epoch 76/1000, LR 0.000267
Train loss: 0.3095;  Loss pred: 0.3095; Loss self: 0.0000; time: 0.87s
Val loss: 0.4445 score: 0.8636 time: 0.67s
Test loss: 0.5382 score: 0.8140 time: 0.27s
Epoch 77/1000, LR 0.000267
Train loss: 0.3161;  Loss pred: 0.3161; Loss self: 0.0000; time: 0.56s
Val loss: 0.4390 score: 0.8636 time: 0.41s
Test loss: 0.5354 score: 0.8372 time: 0.61s
Epoch 78/1000, LR 0.000267
Train loss: 0.2991;  Loss pred: 0.2991; Loss self: 0.0000; time: 0.82s
Val loss: 0.4338 score: 0.8636 time: 0.44s
Test loss: 0.5328 score: 0.8372 time: 0.31s
Epoch 79/1000, LR 0.000267
Train loss: 0.3084;  Loss pred: 0.3084; Loss self: 0.0000; time: 0.97s
Val loss: 0.4290 score: 0.8636 time: 0.50s
Test loss: 0.5304 score: 0.8372 time: 0.31s
Epoch 80/1000, LR 0.000267
Train loss: 0.2892;  Loss pred: 0.2892; Loss self: 0.0000; time: 1.10s
Val loss: 0.4243 score: 0.8636 time: 0.27s
Test loss: 0.5283 score: 0.8372 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.2811;  Loss pred: 0.2811; Loss self: 0.0000; time: 0.57s
Val loss: 0.4198 score: 0.8636 time: 0.46s
Test loss: 0.5264 score: 0.8372 time: 0.42s
Epoch 82/1000, LR 0.000267
Train loss: 0.2675;  Loss pred: 0.2675; Loss self: 0.0000; time: 0.99s
Val loss: 0.4154 score: 0.8636 time: 0.66s
Test loss: 0.5248 score: 0.8372 time: 0.33s
Epoch 83/1000, LR 0.000266
Train loss: 0.2792;  Loss pred: 0.2792; Loss self: 0.0000; time: 0.87s
Val loss: 0.4112 score: 0.8636 time: 0.23s
Test loss: 0.5235 score: 0.8372 time: 0.28s
Epoch 84/1000, LR 0.000266
Train loss: 0.2641;  Loss pred: 0.2641; Loss self: 0.0000; time: 1.06s
Val loss: 0.4073 score: 0.8636 time: 0.28s
Test loss: 0.5225 score: 0.8372 time: 0.46s
Epoch 85/1000, LR 0.000266
Train loss: 0.2547;  Loss pred: 0.2547; Loss self: 0.0000; time: 0.37s
Val loss: 0.4035 score: 0.8636 time: 0.42s
Test loss: 0.5217 score: 0.8372 time: 0.48s
Epoch 86/1000, LR 0.000266
Train loss: 0.2562;  Loss pred: 0.2562; Loss self: 0.0000; time: 0.91s
Val loss: 0.4000 score: 0.8636 time: 0.26s
Test loss: 0.5211 score: 0.8372 time: 0.73s
Epoch 87/1000, LR 0.000266
Train loss: 0.2494;  Loss pred: 0.2494; Loss self: 0.0000; time: 0.63s
Val loss: 0.3967 score: 0.8636 time: 0.35s
Test loss: 0.5207 score: 0.8372 time: 0.61s
Epoch 88/1000, LR 0.000266
Train loss: 0.2350;  Loss pred: 0.2350; Loss self: 0.0000; time: 0.91s
Val loss: 0.3936 score: 0.8636 time: 0.40s
Test loss: 0.5203 score: 0.8372 time: 0.32s
Epoch 89/1000, LR 0.000266
Train loss: 0.2534;  Loss pred: 0.2534; Loss self: 0.0000; time: 0.62s
Val loss: 0.3912 score: 0.8636 time: 0.48s
Test loss: 0.5204 score: 0.8372 time: 0.41s
Epoch 90/1000, LR 0.000266
Train loss: 0.2413;  Loss pred: 0.2413; Loss self: 0.0000; time: 0.83s
Val loss: 0.3902 score: 0.8409 time: 0.26s
Test loss: 0.5219 score: 0.8372 time: 0.32s
Epoch 91/1000, LR 0.000266
Train loss: 0.2253;  Loss pred: 0.2253; Loss self: 0.0000; time: 2.18s
Val loss: 0.3900 score: 0.8182 time: 3.94s
Test loss: 0.5242 score: 0.7907 time: 3.60s
Epoch 92/1000, LR 0.000266
Train loss: 0.2272;  Loss pred: 0.2272; Loss self: 0.0000; time: 3.46s
Val loss: 0.3899 score: 0.8182 time: 0.24s
Test loss: 0.5265 score: 0.7907 time: 0.26s
Epoch 93/1000, LR 0.000265
Train loss: 0.2022;  Loss pred: 0.2022; Loss self: 0.0000; time: 6.93s
Val loss: 0.3902 score: 0.8182 time: 3.71s
Test loss: 0.5295 score: 0.7674 time: 1.55s
     INFO: Early stopping counter 1 of 2
Epoch 94/1000, LR 0.000265
Train loss: 0.2058;  Loss pred: 0.2058; Loss self: 0.0000; time: 0.35s
Val loss: 0.3897 score: 0.8182 time: 0.23s
Test loss: 0.5317 score: 0.7907 time: 0.25s
Epoch 95/1000, LR 0.000265
Train loss: 0.2033;  Loss pred: 0.2033; Loss self: 0.0000; time: 0.41s
Val loss: 0.3884 score: 0.8182 time: 0.23s
Test loss: 0.5334 score: 0.7907 time: 0.24s
Epoch 96/1000, LR 0.000265
Train loss: 0.1857;  Loss pred: 0.1857; Loss self: 0.0000; time: 0.35s
Val loss: 0.3865 score: 0.8182 time: 0.24s
Test loss: 0.5346 score: 0.8140 time: 0.30s
Epoch 97/1000, LR 0.000265
Train loss: 0.1780;  Loss pred: 0.1780; Loss self: 0.0000; time: 0.36s
Val loss: 0.3837 score: 0.8182 time: 0.21s
Test loss: 0.5354 score: 0.8140 time: 0.24s
Epoch 98/1000, LR 0.000265
Train loss: 0.1877;  Loss pred: 0.1877; Loss self: 0.0000; time: 0.33s
Val loss: 0.3814 score: 0.8182 time: 0.27s
Test loss: 0.5369 score: 0.8140 time: 0.25s
Epoch 99/1000, LR 0.000265
Train loss: 0.1703;  Loss pred: 0.1703; Loss self: 0.0000; time: 0.52s
Val loss: 0.3799 score: 0.8182 time: 0.23s
Test loss: 0.5393 score: 0.8372 time: 2.03s
Epoch 100/1000, LR 0.000265
Train loss: 0.1797;  Loss pred: 0.1797; Loss self: 0.0000; time: 1.59s
Val loss: 0.3785 score: 0.8409 time: 0.20s
Test loss: 0.5422 score: 0.8372 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.1542;  Loss pred: 0.1542; Loss self: 0.0000; time: 0.34s
Val loss: 0.3779 score: 0.8409 time: 3.34s
Test loss: 0.5461 score: 0.8372 time: 0.22s
Epoch 102/1000, LR 0.000264
Train loss: 0.1771;  Loss pred: 0.1771; Loss self: 0.0000; time: 0.35s
Val loss: 0.3780 score: 0.8409 time: 0.24s
Test loss: 0.5509 score: 0.8140 time: 2.56s
     INFO: Early stopping counter 1 of 2
Epoch 103/1000, LR 0.000264
Train loss: 0.1380;  Loss pred: 0.1380; Loss self: 0.0000; time: 0.68s
Val loss: 0.3793 score: 0.8409 time: 0.28s
Test loss: 0.5563 score: 0.8140 time: 0.21s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 100,   Train_Loss: 0.1542,   Val_Loss: 0.3779,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3779,   Test_Precision: 0.8947,   Test_Recall: 0.7727,   Test_accuracy: 0.8293,   Test_Score: 0.8372,   Test_loss: 0.5461


[0.3409481570124626, 0.29934434790629894, 0.2467072430299595, 0.3319905719254166, 0.32373185793403536, 0.2523573429789394, 0.30996870808303356, 0.26098833594005555, 0.24222101899795234, 0.25220429711043835, 0.7935017260024324, 0.24727815901860595, 0.7153167240321636, 0.2654184600105509, 0.8792746590916067, 0.5794876009458676, 0.27160830795764923, 0.27053978596813977, 0.32051243202295154, 0.3229585289955139, 0.3390637870179489, 0.2520378549816087, 0.27430555899627507, 0.5382425209973007, 0.29658364003989846, 0.48240046901628375, 0.2907604619394988, 0.2451030060183257, 0.351971402997151, 0.9360642649699003, 0.273413299000822, 0.729303541011177, 0.5452021459350362, 0.2352315440075472, 0.25875813397578895, 0.3749309490667656, 0.26477791590150446, 0.4115926680387929, 0.2675259249517694, 0.3108817560132593, 0.2528272570343688, 0.5465070969657972, 0.43243973597418517, 0.7136125230463222, 0.25983172899577767, 0.31999093701597303, 0.25766399898566306, 0.48250408202875406, 0.26550659199710935, 0.2622740480583161, 0.3182081599952653, 0.3548201989615336, 0.2639129540184513, 0.27418025105725974, 4.014726023073308, 0.25360282603651285, 0.24368257902096957, 0.3767808419652283, 0.3755599759751931, 0.2670667259953916, 0.3202478770399466, 0.29795782500877976, 0.4764360610861331, 0.4731219890527427, 0.29990839899983257, 0.305838021915406, 0.388830405077897, 0.2713434100151062, 0.49604218709282577, 0.2887933619786054, 0.2981726099969819, 0.279582150047645, 0.3067644799593836, 0.42062224890105426, 0.4666510360548273, 0.2919992810348049, 0.2865177099592984, 0.2689092429354787, 0.5007110210135579, 0.29025176202412695, 0.559851857018657, 0.4274923630291596, 0.6704327730694786, 0.27626061206683517, 0.6125694679794833, 0.30971427098847926, 0.3172248291084543, 0.26669388997834176, 0.424208975979127, 0.33194965892471373, 0.28810484299901873, 0.46736328199040145, 0.4795524259097874, 0.7344876030692831, 0.6132679430302233, 0.3230304450262338, 0.418981260037981, 0.3267729850485921, 3.6051274130586535, 0.2648379410384223, 1.5587521239649504, 0.25389063195325434, 0.24467868905048817, 0.30088847200386226, 0.24824396497569978, 0.25622088194359094, 2.0337077780859545, 0.23460649303160608, 0.22484350705053657, 2.5636552340583876, 0.21921966702211648]
[0.007748821750283241, 0.006803280634234066, 0.005606982796135443, 0.007545240271032195, 0.007357542225773531, 0.005735394158612259, 0.00704474336552349, 0.005931553089546717, 0.005633046953440752, 0.005865216211870659, 0.018453528511684473, 0.005750654860897813, 0.016635272651910782, 0.0061725223258267655, 0.02044824788585132, 0.01347645583595041, 0.006316472278084866, 0.006291622929491623, 0.00745377748890585, 0.007510663465011952, 0.007885204349254627, 0.0058613454646885745, 0.006379199046425001, 0.012517267930169785, 0.006897293954416243, 0.011218615558518226, 0.006761871207895321, 0.005700069907402923, 0.008185381465050023, 0.021768936394648844, 0.006358448813972605, 0.01696054746537621, 0.012679119672907819, 0.00547050102343133, 0.006017631022692766, 0.008719324396901526, 0.006157625951197778, 0.009571922512530067, 0.006221533138413242, 0.007229808279378123, 0.005879703651962065, 0.012709467371297611, 0.010056738045911283, 0.0165956400708447, 0.006042598348739016, 0.007441649698045885, 0.005992186022922397, 0.011221025163459396, 0.00617457190690952, 0.006099396466472467, 0.007400189767331752, 0.008251632533989153, 0.006137510558568635, 0.006376284908308366, 0.09336572146682112, 0.00589774014038402, 0.005667036721417897, 0.008762345161982054, 0.008733952929655652, 0.006210854092916084, 0.007447625047440619, 0.006929251744390227, 0.011079908397351934, 0.011002836954714946, 0.006974613930228664, 0.007112512137567581, 0.009042567559951094, 0.006310311860816423, 0.011535864816112228, 0.00671612469717687, 0.006934246744115858, 0.006501910466224302, 0.007134057673474037, 0.009781912765140796, 0.010852349675693658, 0.006790680954297788, 0.006663202557192985, 0.0062537033240808995, 0.01164444234915251, 0.006750040977305278, 0.013019810628340862, 0.009941682861143247, 0.015591459838825083, 0.006424665396903144, 0.014245801580918217, 0.007202657464848355, 0.007377321607173356, 0.006202183487868413, 0.009865325022770394, 0.007719759509877064, 0.006700112627884156, 0.010868913534660499, 0.011152381997902033, 0.01708110704812286, 0.014262045186749377, 0.007512335930842646, 0.009743750233441418, 0.007599371745316095, 0.08384017239671288, 0.006159021884614472, 0.03625004939453373, 0.005904433301238473, 0.005690202070941585, 0.006997406325671215, 0.005773115464551158, 0.005958625161478859, 0.04729552972292917, 0.005455964954223397, 0.00522891876861713, 0.05961988916414855, 0.005098131791212011]
[129.0518781082359, 146.9879097692966, 178.34904018061906, 132.53388415465253, 135.91495221012687, 174.3559330614447, 141.949812521764, 168.58990974257958, 177.52381761866633, 170.496698480798, 54.190178283075575, 173.89323897693913, 60.113231741058165, 162.0083245087424, 48.90394549118931, 74.2034858551129, 158.31621765673242, 158.9415022493731, 134.16016261397567, 133.14402977292883, 126.81979511343015, 170.60929201741433, 156.7594917045918, 79.8896377051854, 144.98439628772292, 89.13755844327031, 147.8880577956552, 175.43644485855472, 122.16901610142475, 45.937016943364135, 157.27106237019848, 58.96036092239541, 78.86982896271226, 182.798613091705, 166.17835095388094, 114.6877847961887, 162.40025099372602, 104.47222056916529, 160.73208608755283, 138.3162542293605, 170.07659895687067, 78.68150338529112, 99.43582058464423, 60.25679008047449, 165.49172099262273, 134.3788058530344, 166.8840046311344, 89.11841702810192, 161.95454763122473, 163.95064749387265, 135.13166978697154, 121.18814015056023, 162.93250992519938, 156.83113511709453, 10.7105689785235, 169.55647013889745, 176.45906479141345, 114.12469852691795, 114.49569376594135, 161.0084515011503, 134.27099157518015, 144.3157265587267, 90.25345374145847, 90.88565104761268, 143.37711162275255, 140.59729961205966, 110.58805957159014, 158.47077324489328, 86.68617532716686, 148.895389095493, 144.2117704923844, 153.80094899718083, 140.1726823317136, 102.22949478384623, 92.14594349459001, 147.2606365591517, 150.0779829843971, 159.90525104530266, 85.8778780482159, 148.14724878888302, 76.8060326333204, 100.58659222660063, 64.13767603145469, 155.65012934090328, 70.19612019161261, 138.83764497761717, 135.5505498130443, 161.2335400840718, 101.36513472104323, 129.5377140596864, 149.25122241053913, 92.00551617335466, 89.66694291749675, 58.54421479724264, 70.11617106143254, 133.11438801537082, 102.62988849692708, 131.58982525316705, 11.927456390096916, 162.36344321133967, 27.586169307421507, 169.36426393202663, 175.7406832890427, 142.9100946062435, 173.21669835643016, 167.82394812561293, 21.143647314202585, 183.2856347850827, 191.24412603266848, 16.772926183186094, 196.15028425192273]
Elapsed: 0.47410397304259744~0.5529308855932112
Time per graph: 0.011014407594686188~0.012862470915955751
Speed: 126.87506378538299~43.79323629951919
Total Time: 0.2198
best val loss: 0.37794044613838196 test_score: 0.8372

Testing...
Test loss: 0.5451 score: 0.8140 time: 0.23s
test Score 0.8140
Epoch Time List: [1.1201642368687317, 1.2768734149867669, 1.2420692391460761, 1.1716866868082434, 1.6731043020263314, 1.274855533032678, 1.4292868719203398, 1.5099459220655262, 11.29237914585974, 12.202261009952053, 1.747786173131317, 1.8038731949636713, 1.5844638041453436, 1.535190035123378, 2.476223963079974, 2.0649980129674077, 2.1856550839729607, 1.346751676988788, 1.1354167609242722, 1.3301823551300913, 1.867101546144113, 1.0095936508150771, 1.346835868083872, 1.5226773619651794, 1.0609677860047668, 1.486631517065689, 1.3483571679098532, 1.273470439016819, 1.4885630809003487, 2.698825749917887, 1.1386720378650352, 1.8431445920141414, 1.3308644469361752, 1.5152896819636226, 1.3589983279816806, 1.5074298980180174, 1.0370611810358241, 1.5504737138981, 1.34448014292866, 1.742409373051487, 1.5056943920208141, 1.616746870917268, 1.7389542550081387, 1.5734845991246402, 1.3360558690037578, 1.6380572350462899, 1.04029714607168, 1.993154140887782, 1.1436669969698414, 1.2877086349762976, 2.350908833905123, 1.3358707638690248, 1.5259466459974647, 1.3484895470319316, 8.803685955004767, 4.165301772067323, 12.181328654056415, 0.9904258649330586, 1.6608955960255116, 1.9455684138229117, 1.6199012050637975, 1.2014522490790114, 1.523798643029295, 1.7612110020127147, 1.5558274149661884, 1.6432914319448173, 2.1813937550177798, 1.5177018629619852, 1.590792660950683, 1.5209006789373234, 1.5867193420417607, 2.263552648946643, 1.8852667949395254, 1.7200393669772893, 1.672591065056622, 1.4360096160089597, 1.6475510919699445, 2.3985746450489387, 1.8108959577511996, 1.4437551849987358, 1.5493466518819332, 1.8256629997631535, 2.13320630392991, 1.8112117131240666, 1.5774514769436792, 1.5701200369512662, 1.7830693878931925, 1.6348868110217154, 1.4506439489778131, 1.9751216470031068, 1.387019605957903, 1.8040546359261498, 1.263785142917186, 1.8931813619565219, 1.5825860550394282, 1.6302071120589972, 1.516791998874396, 1.4134986731223762, 9.724611237994395, 3.964912381954491, 12.196711343131028, 0.8203868119744584, 0.8720460610929877, 0.8856613878160715, 0.8217530249385163, 0.8432725128950551, 2.7800434199161828, 2.023002907051705, 3.905478597036563, 3.151906862971373, 1.1753298899857327]
Total Epoch List: [8, 103]
Total Time List: [0.2612073279451579, 0.21976986795198172]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334e20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.20s
Epoch 3/1000, LR 0.000030
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.23s
Epoch 4/1000, LR 0.000060
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.21s
Epoch 5/1000, LR 0.000090
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.21s
Epoch 7/1000, LR 0.000150
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.38s
Val loss: 0.6930 score: 0.5682 time: 0.22s
Test loss: 0.6930 score: 0.5349 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.29s
Epoch 11/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.20s
Epoch 12/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.20s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.22s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.22s
Epoch 22/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.20s
Epoch 25/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.22s
Epoch 26/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.26s
Epoch 27/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.20s
Epoch 28/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4884 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4884 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6877 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4884 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6870 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6880 score: 0.4884 time: 0.21s
Epoch 35/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4884 time: 0.21s
Epoch 36/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6854 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.4884 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6854 score: 0.4884 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6843 score: 0.4884 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6824 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6831 score: 0.4884 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6812 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6818 score: 0.4884 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6798 score: 0.5000 time: 0.22s
Test loss: 0.6804 score: 0.5116 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6784 score: 0.5000 time: 0.24s
Test loss: 0.6788 score: 0.5116 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6768 score: 0.5000 time: 0.23s
Test loss: 0.6771 score: 0.5116 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6698;  Loss pred: 0.6698; Loss self: 0.0000; time: 0.36s
Val loss: 0.6750 score: 0.5455 time: 0.23s
Test loss: 0.6752 score: 0.5349 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.40s
Val loss: 0.6731 score: 0.5682 time: 0.25s
Test loss: 0.6732 score: 0.5349 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.36s
Val loss: 0.6710 score: 0.6364 time: 0.23s
Test loss: 0.6709 score: 0.5581 time: 0.21s
Epoch 47/1000, LR 0.000269
Train loss: 0.6622;  Loss pred: 0.6622; Loss self: 0.0000; time: 0.42s
Val loss: 0.6687 score: 0.6364 time: 0.26s
Test loss: 0.6685 score: 0.5814 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6576;  Loss pred: 0.6576; Loss self: 0.0000; time: 0.36s
Val loss: 0.6662 score: 0.6591 time: 0.24s
Test loss: 0.6659 score: 0.6279 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6552;  Loss pred: 0.6552; Loss self: 0.0000; time: 0.41s
Val loss: 0.6635 score: 0.6591 time: 0.38s
Test loss: 0.6631 score: 0.6744 time: 0.32s
Epoch 50/1000, LR 0.000269
Train loss: 0.6520;  Loss pred: 0.6520; Loss self: 0.0000; time: 0.40s
Val loss: 0.6605 score: 0.7045 time: 0.21s
Test loss: 0.6602 score: 0.7209 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.35s
Val loss: 0.6573 score: 0.7727 time: 0.24s
Test loss: 0.6569 score: 0.7442 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.41s
Val loss: 0.6538 score: 0.7955 time: 0.23s
Test loss: 0.6533 score: 0.7674 time: 3.04s
Epoch 53/1000, LR 0.000269
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.39s
Val loss: 0.6500 score: 0.7955 time: 0.27s
Test loss: 0.6494 score: 0.7674 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.6323;  Loss pred: 0.6323; Loss self: 0.0000; time: 0.35s
Val loss: 0.6459 score: 0.8409 time: 0.24s
Test loss: 0.6453 score: 0.7674 time: 0.21s
Epoch 55/1000, LR 0.000269
Train loss: 0.6251;  Loss pred: 0.6251; Loss self: 0.0000; time: 0.40s
Val loss: 0.6415 score: 0.8636 time: 0.22s
Test loss: 0.6410 score: 0.7674 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.6191;  Loss pred: 0.6191; Loss self: 0.0000; time: 0.35s
Val loss: 0.6367 score: 0.8636 time: 0.28s
Test loss: 0.6363 score: 0.7674 time: 0.74s
Epoch 57/1000, LR 0.000269
Train loss: 0.6094;  Loss pred: 0.6094; Loss self: 0.0000; time: 0.49s
Val loss: 0.6315 score: 0.8636 time: 0.21s
Test loss: 0.6314 score: 0.7674 time: 0.41s
Epoch 58/1000, LR 0.000269
Train loss: 0.6018;  Loss pred: 0.6018; Loss self: 0.0000; time: 0.77s
Val loss: 0.6260 score: 0.8636 time: 0.46s
Test loss: 0.6262 score: 0.7674 time: 0.27s
Epoch 59/1000, LR 0.000268
Train loss: 0.5956;  Loss pred: 0.5956; Loss self: 0.0000; time: 1.11s
Val loss: 0.6201 score: 0.8636 time: 0.69s
Test loss: 0.6207 score: 0.7674 time: 0.42s
Epoch 60/1000, LR 0.000268
Train loss: 0.5852;  Loss pred: 0.5852; Loss self: 0.0000; time: 0.41s
Val loss: 0.6137 score: 0.8636 time: 0.67s
Test loss: 0.6149 score: 0.7674 time: 0.46s
Epoch 61/1000, LR 0.000268
Train loss: 0.5819;  Loss pred: 0.5819; Loss self: 0.0000; time: 1.11s
Val loss: 0.6069 score: 0.8636 time: 0.46s
Test loss: 0.6085 score: 0.7674 time: 0.35s
Epoch 62/1000, LR 0.000268
Train loss: 0.5757;  Loss pred: 0.5757; Loss self: 0.0000; time: 1.07s
Val loss: 0.5997 score: 0.8636 time: 0.42s
Test loss: 0.6018 score: 0.7674 time: 0.42s
Epoch 63/1000, LR 0.000268
Train loss: 0.5619;  Loss pred: 0.5619; Loss self: 0.0000; time: 1.14s
Val loss: 0.5921 score: 0.8864 time: 0.33s
Test loss: 0.5949 score: 0.7674 time: 0.36s
Epoch 64/1000, LR 0.000268
Train loss: 0.5503;  Loss pred: 0.5503; Loss self: 0.0000; time: 1.19s
Val loss: 0.5841 score: 0.8864 time: 0.40s
Test loss: 0.5878 score: 0.7674 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5337;  Loss pred: 0.5337; Loss self: 0.0000; time: 0.58s
Val loss: 0.5756 score: 0.8864 time: 0.42s
Test loss: 0.5804 score: 0.7674 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 1.28s
Val loss: 0.5668 score: 0.8864 time: 0.28s
Test loss: 0.5727 score: 0.7674 time: 0.26s
Epoch 67/1000, LR 0.000268
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.46s
Val loss: 0.5576 score: 0.8864 time: 0.50s
Test loss: 0.5647 score: 0.7674 time: 0.27s
Epoch 68/1000, LR 0.000268
Train loss: 0.5013;  Loss pred: 0.5013; Loss self: 0.0000; time: 1.38s
Val loss: 0.5480 score: 0.8864 time: 0.62s
Test loss: 0.5562 score: 0.7674 time: 0.30s
Epoch 69/1000, LR 0.000268
Train loss: 0.4836;  Loss pred: 0.4836; Loss self: 0.0000; time: 0.93s
Val loss: 0.5380 score: 0.8864 time: 0.36s
Test loss: 0.5468 score: 0.7674 time: 0.26s
Epoch 70/1000, LR 0.000268
Train loss: 0.4686;  Loss pred: 0.4686; Loss self: 0.0000; time: 0.88s
Val loss: 0.5277 score: 0.8864 time: 0.49s
Test loss: 0.5373 score: 0.7674 time: 0.26s
Epoch 71/1000, LR 0.000268
Train loss: 0.4563;  Loss pred: 0.4563; Loss self: 0.0000; time: 0.88s
Val loss: 0.5171 score: 0.8864 time: 0.42s
Test loss: 0.5277 score: 0.7674 time: 0.45s
Epoch 72/1000, LR 0.000267
Train loss: 0.4416;  Loss pred: 0.4416; Loss self: 0.0000; time: 1.13s
Val loss: 0.5063 score: 0.8864 time: 0.58s
Test loss: 0.5172 score: 0.7907 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.4217;  Loss pred: 0.4217; Loss self: 0.0000; time: 1.31s
Val loss: 0.4952 score: 0.8864 time: 0.28s
Test loss: 0.5067 score: 0.7907 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.4226;  Loss pred: 0.4226; Loss self: 0.0000; time: 0.87s
Val loss: 0.4841 score: 0.8864 time: 0.26s
Test loss: 0.4959 score: 0.7907 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.3979;  Loss pred: 0.3979; Loss self: 0.0000; time: 0.62s
Val loss: 0.4729 score: 0.8864 time: 0.45s
Test loss: 0.4850 score: 0.8140 time: 0.31s
Epoch 76/1000, LR 0.000267
Train loss: 0.3814;  Loss pred: 0.3814; Loss self: 0.0000; time: 0.48s
Val loss: 0.4619 score: 0.8864 time: 0.29s
Test loss: 0.4741 score: 0.8140 time: 0.43s
Epoch 77/1000, LR 0.000267
Train loss: 0.3694;  Loss pred: 0.3694; Loss self: 0.0000; time: 0.96s
Val loss: 0.4511 score: 0.8864 time: 0.67s
Test loss: 0.4632 score: 0.8372 time: 0.33s
Epoch 78/1000, LR 0.000267
Train loss: 0.3498;  Loss pred: 0.3498; Loss self: 0.0000; time: 1.13s
Val loss: 0.4402 score: 0.8864 time: 0.45s
Test loss: 0.4530 score: 0.8372 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.3506;  Loss pred: 0.3506; Loss self: 0.0000; time: 0.84s
Val loss: 0.4290 score: 0.8864 time: 0.91s
Test loss: 0.4444 score: 0.8372 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.3305;  Loss pred: 0.3305; Loss self: 0.0000; time: 0.57s
Val loss: 0.4178 score: 0.8864 time: 0.28s
Test loss: 0.4366 score: 0.8140 time: 0.68s
Epoch 81/1000, LR 0.000267
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 0.89s
Val loss: 0.4067 score: 0.9091 time: 0.42s
Test loss: 0.4295 score: 0.8140 time: 0.43s
Epoch 82/1000, LR 0.000267
Train loss: 0.2967;  Loss pred: 0.2967; Loss self: 0.0000; time: 0.94s
Val loss: 0.3960 score: 0.9091 time: 0.62s
Test loss: 0.4236 score: 0.8140 time: 0.27s
Epoch 83/1000, LR 0.000266
Train loss: 0.2827;  Loss pred: 0.2827; Loss self: 0.0000; time: 1.02s
Val loss: 0.3858 score: 0.9091 time: 0.54s
Test loss: 0.4179 score: 0.8140 time: 0.39s
Epoch 84/1000, LR 0.000266
Train loss: 0.2510;  Loss pred: 0.2510; Loss self: 0.0000; time: 1.28s
Val loss: 0.3759 score: 0.9091 time: 0.97s
Test loss: 0.4130 score: 0.8140 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.2614;  Loss pred: 0.2614; Loss self: 0.0000; time: 1.26s
Val loss: 0.3664 score: 0.9091 time: 0.36s
Test loss: 0.4070 score: 0.8140 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.2247;  Loss pred: 0.2247; Loss self: 0.0000; time: 0.89s
Val loss: 0.3572 score: 0.9091 time: 0.29s
Test loss: 0.4015 score: 0.8140 time: 0.45s
Epoch 87/1000, LR 0.000266
Train loss: 0.2313;  Loss pred: 0.2313; Loss self: 0.0000; time: 1.17s
Val loss: 0.3488 score: 0.9091 time: 0.40s
Test loss: 0.3954 score: 0.8140 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.1983;  Loss pred: 0.1983; Loss self: 0.0000; time: 0.95s
Val loss: 0.3408 score: 0.9091 time: 0.54s
Test loss: 0.3915 score: 0.8140 time: 0.37s
Epoch 89/1000, LR 0.000266
Train loss: 0.1805;  Loss pred: 0.1805; Loss self: 0.0000; time: 1.67s
Val loss: 0.3334 score: 0.9091 time: 0.44s
Test loss: 0.3876 score: 0.8140 time: 1.02s
Epoch 90/1000, LR 0.000266
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 1.05s
Val loss: 0.3273 score: 0.9091 time: 0.24s
Test loss: 0.3828 score: 0.8140 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.1888;  Loss pred: 0.1888; Loss self: 0.0000; time: 0.88s
Val loss: 0.3234 score: 0.9091 time: 0.47s
Test loss: 0.3778 score: 0.8372 time: 0.27s
Epoch 92/1000, LR 0.000266
Train loss: 0.1813;  Loss pred: 0.1813; Loss self: 0.0000; time: 0.88s
Val loss: 0.3228 score: 0.9318 time: 0.51s
Test loss: 0.3726 score: 0.8605 time: 0.25s
Epoch 93/1000, LR 0.000265
Train loss: 0.1407;  Loss pred: 0.1407; Loss self: 0.0000; time: 0.82s
Val loss: 0.3231 score: 0.9318 time: 0.21s
Test loss: 0.3694 score: 0.8605 time: 0.18s
     INFO: Early stopping counter 1 of 2
Epoch 94/1000, LR 0.000265
Train loss: 0.1345;  Loss pred: 0.1345; Loss self: 0.0000; time: 8.33s
Val loss: 0.3224 score: 0.9091 time: 1.92s
Test loss: 0.3686 score: 0.8605 time: 0.26s
Epoch 95/1000, LR 0.000265
Train loss: 0.1425;  Loss pred: 0.1425; Loss self: 0.0000; time: 0.44s
Val loss: 0.3205 score: 0.8864 time: 3.86s
Test loss: 0.3695 score: 0.8605 time: 3.66s
Epoch 96/1000, LR 0.000265
Train loss: 0.1156;  Loss pred: 0.1156; Loss self: 0.0000; time: 2.49s
Val loss: 0.3134 score: 0.9091 time: 0.32s
Test loss: 0.3737 score: 0.8605 time: 0.22s
Epoch 97/1000, LR 0.000265
Train loss: 0.1215;  Loss pred: 0.1215; Loss self: 0.0000; time: 0.38s
Val loss: 0.3043 score: 0.9318 time: 0.23s
Test loss: 0.3802 score: 0.8605 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.1103;  Loss pred: 0.1103; Loss self: 0.0000; time: 0.35s
Val loss: 0.2938 score: 0.9318 time: 0.24s
Test loss: 0.3908 score: 0.8372 time: 0.20s
Epoch 99/1000, LR 0.000265
Train loss: 0.0923;  Loss pred: 0.0923; Loss self: 0.0000; time: 0.40s
Val loss: 0.2868 score: 0.9091 time: 0.23s
Test loss: 0.4025 score: 0.8140 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.0943;  Loss pred: 0.0943; Loss self: 0.0000; time: 0.35s
Val loss: 0.2823 score: 0.9091 time: 0.24s
Test loss: 0.4139 score: 0.8140 time: 0.21s
Epoch 101/1000, LR 0.000265
Train loss: 0.0702;  Loss pred: 0.0702; Loss self: 0.0000; time: 0.39s
Val loss: 0.2796 score: 0.9091 time: 0.24s
Test loss: 0.4257 score: 0.8140 time: 0.23s
Epoch 102/1000, LR 0.000264
Train loss: 0.0752;  Loss pred: 0.0752; Loss self: 0.0000; time: 0.34s
Val loss: 0.2782 score: 0.9091 time: 0.24s
Test loss: 0.4361 score: 0.8140 time: 0.21s
Epoch 103/1000, LR 0.000264
Train loss: 0.0667;  Loss pred: 0.0667; Loss self: 0.0000; time: 0.38s
Val loss: 0.2776 score: 0.8864 time: 0.24s
Test loss: 0.4466 score: 0.8140 time: 0.31s
Epoch 104/1000, LR 0.000264
Train loss: 0.0600;  Loss pred: 0.0600; Loss self: 0.0000; time: 0.51s
Val loss: 0.2774 score: 0.8864 time: 0.25s
Test loss: 0.4571 score: 0.8140 time: 0.24s
Epoch 105/1000, LR 0.000264
Train loss: 0.0597;  Loss pred: 0.0597; Loss self: 0.0000; time: 0.70s
Val loss: 0.2776 score: 0.8864 time: 0.44s
Test loss: 0.4672 score: 0.8140 time: 0.30s
     INFO: Early stopping counter 1 of 2
Epoch 106/1000, LR 0.000264
Train loss: 0.0858;  Loss pred: 0.0858; Loss self: 0.0000; time: 0.63s
Val loss: 0.2777 score: 0.8864 time: 0.68s
Test loss: 0.4744 score: 0.8140 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 103,   Train_Loss: 0.0600,   Val_Loss: 0.2774,   Val_Precision: 0.9048,   Val_Recall: 0.8636,   Val_accuracy: 0.8837,   Val_Score: 0.8864,   Val_Loss: 0.2774,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4571


[0.3409481570124626, 0.29934434790629894, 0.2467072430299595, 0.3319905719254166, 0.32373185793403536, 0.2523573429789394, 0.30996870808303356, 0.26098833594005555, 0.24222101899795234, 0.25220429711043835, 0.7935017260024324, 0.24727815901860595, 0.7153167240321636, 0.2654184600105509, 0.8792746590916067, 0.5794876009458676, 0.27160830795764923, 0.27053978596813977, 0.32051243202295154, 0.3229585289955139, 0.3390637870179489, 0.2520378549816087, 0.27430555899627507, 0.5382425209973007, 0.29658364003989846, 0.48240046901628375, 0.2907604619394988, 0.2451030060183257, 0.351971402997151, 0.9360642649699003, 0.273413299000822, 0.729303541011177, 0.5452021459350362, 0.2352315440075472, 0.25875813397578895, 0.3749309490667656, 0.26477791590150446, 0.4115926680387929, 0.2675259249517694, 0.3108817560132593, 0.2528272570343688, 0.5465070969657972, 0.43243973597418517, 0.7136125230463222, 0.25983172899577767, 0.31999093701597303, 0.25766399898566306, 0.48250408202875406, 0.26550659199710935, 0.2622740480583161, 0.3182081599952653, 0.3548201989615336, 0.2639129540184513, 0.27418025105725974, 4.014726023073308, 0.25360282603651285, 0.24368257902096957, 0.3767808419652283, 0.3755599759751931, 0.2670667259953916, 0.3202478770399466, 0.29795782500877976, 0.4764360610861331, 0.4731219890527427, 0.29990839899983257, 0.305838021915406, 0.388830405077897, 0.2713434100151062, 0.49604218709282577, 0.2887933619786054, 0.2981726099969819, 0.279582150047645, 0.3067644799593836, 0.42062224890105426, 0.4666510360548273, 0.2919992810348049, 0.2865177099592984, 0.2689092429354787, 0.5007110210135579, 0.29025176202412695, 0.559851857018657, 0.4274923630291596, 0.6704327730694786, 0.27626061206683517, 0.6125694679794833, 0.30971427098847926, 0.3172248291084543, 0.26669388997834176, 0.424208975979127, 0.33194965892471373, 0.28810484299901873, 0.46736328199040145, 0.4795524259097874, 0.7344876030692831, 0.6132679430302233, 0.3230304450262338, 0.418981260037981, 0.3267729850485921, 3.6051274130586535, 0.2648379410384223, 1.5587521239649504, 0.25389063195325434, 0.24467868905048817, 0.30088847200386226, 0.24824396497569978, 0.25622088194359094, 2.0337077780859545, 0.23460649303160608, 0.22484350705053657, 2.5636552340583876, 0.21921966702211648, 0.23392671602778137, 0.20307245198637247, 0.234868539031595, 0.21601491002365947, 0.23744275700300932, 0.2120166840031743, 0.2366760850418359, 0.21034666895866394, 0.24330471002031118, 0.2937869719462469, 0.2091659780126065, 0.2296749249799177, 0.20251253410242498, 0.22552584705408663, 0.2155995579669252, 0.23291015403810889, 0.2226550349732861, 0.23247500194702297, 0.2109603879507631, 0.2245419790269807, 0.2218080930178985, 0.22331447596661747, 0.23649892304092646, 0.20913394493982196, 0.21984034997876734, 0.26675225293729454, 0.2095345779089257, 0.2302831090055406, 0.23980368103366345, 0.22566798806656152, 0.22975555900484324, 0.22590809396933764, 0.23102920805104077, 0.21769394201692194, 0.21896541398018599, 0.2503508160589263, 0.20376604003831744, 0.23811050096992403, 0.21121093002147973, 0.2307600099593401, 0.23042734700720757, 0.22633430699352175, 0.24400846695061773, 0.2264385799644515, 0.22801815089769661, 0.2152958509977907, 0.23748559807427227, 0.22101391502656043, 0.32095422595739365, 0.2260310749989003, 0.22613376297522336, 3.0477379850344732, 0.24563424696680158, 0.21411388006526977, 0.23470883397385478, 0.7465374229941517, 0.41742725297808647, 0.27775394602213055, 0.424209991004318, 0.46473431901540607, 0.35191531700547785, 0.4299675510264933, 0.36237565497867763, 0.24260415299795568, 0.2480316678993404, 0.2602350360248238, 0.2736007779603824, 0.3093095369404182, 0.2619800860993564, 0.265820820000954, 0.459674604004249, 0.25071845098864287, 0.23453239898663014, 0.22103780996985734, 0.3124376580817625, 0.4364492080640048, 0.3310595139628276, 0.24150411097798496, 0.24207769404165447, 0.6867691610241309, 0.43043586995918304, 0.2784293449949473, 0.3991892000194639, 0.22684423893224448, 0.21988790994510055, 0.45566997898276895, 0.232853683992289, 0.3745608199387789, 1.0292913309531286, 0.23487402696628124, 0.2704227219801396, 0.2596485180547461, 0.18862595094833523, 0.26247619604691863, 3.6686940629733726, 0.22281623003073037, 0.2349938719999045, 0.2084268240723759, 0.22786074900068343, 0.2126170180272311, 0.2301738760434091, 0.2101221299963072, 0.31020000798162073, 0.24681593803688884, 0.30936788592953235, 0.2538548879092559]
[0.007748821750283241, 0.006803280634234066, 0.005606982796135443, 0.007545240271032195, 0.007357542225773531, 0.005735394158612259, 0.00704474336552349, 0.005931553089546717, 0.005633046953440752, 0.005865216211870659, 0.018453528511684473, 0.005750654860897813, 0.016635272651910782, 0.0061725223258267655, 0.02044824788585132, 0.01347645583595041, 0.006316472278084866, 0.006291622929491623, 0.00745377748890585, 0.007510663465011952, 0.007885204349254627, 0.0058613454646885745, 0.006379199046425001, 0.012517267930169785, 0.006897293954416243, 0.011218615558518226, 0.006761871207895321, 0.005700069907402923, 0.008185381465050023, 0.021768936394648844, 0.006358448813972605, 0.01696054746537621, 0.012679119672907819, 0.00547050102343133, 0.006017631022692766, 0.008719324396901526, 0.006157625951197778, 0.009571922512530067, 0.006221533138413242, 0.007229808279378123, 0.005879703651962065, 0.012709467371297611, 0.010056738045911283, 0.0165956400708447, 0.006042598348739016, 0.007441649698045885, 0.005992186022922397, 0.011221025163459396, 0.00617457190690952, 0.006099396466472467, 0.007400189767331752, 0.008251632533989153, 0.006137510558568635, 0.006376284908308366, 0.09336572146682112, 0.00589774014038402, 0.005667036721417897, 0.008762345161982054, 0.008733952929655652, 0.006210854092916084, 0.007447625047440619, 0.006929251744390227, 0.011079908397351934, 0.011002836954714946, 0.006974613930228664, 0.007112512137567581, 0.009042567559951094, 0.006310311860816423, 0.011535864816112228, 0.00671612469717687, 0.006934246744115858, 0.006501910466224302, 0.007134057673474037, 0.009781912765140796, 0.010852349675693658, 0.006790680954297788, 0.006663202557192985, 0.0062537033240808995, 0.01164444234915251, 0.006750040977305278, 0.013019810628340862, 0.009941682861143247, 0.015591459838825083, 0.006424665396903144, 0.014245801580918217, 0.007202657464848355, 0.007377321607173356, 0.006202183487868413, 0.009865325022770394, 0.007719759509877064, 0.006700112627884156, 0.010868913534660499, 0.011152381997902033, 0.01708110704812286, 0.014262045186749377, 0.007512335930842646, 0.009743750233441418, 0.007599371745316095, 0.08384017239671288, 0.006159021884614472, 0.03625004939453373, 0.005904433301238473, 0.005690202070941585, 0.006997406325671215, 0.005773115464551158, 0.005958625161478859, 0.04729552972292917, 0.005455964954223397, 0.00522891876861713, 0.05961988916414855, 0.005098131791212011, 0.00544015618669259, 0.004722615162473779, 0.005462059047246395, 0.005023602558689755, 0.005521924581465333, 0.004930620558213356, 0.005504095000972928, 0.004891782999038696, 0.005658249070239795, 0.006832255161540626, 0.004864325070060616, 0.005341277325114365, 0.004709593816335464, 0.0052447871407927124, 0.005013943208533145, 0.0054165152101885786, 0.005178024069146189, 0.0054063953941168134, 0.004906055533738677, 0.005221906488999551, 0.00515832774460229, 0.005193359906200406, 0.0054999749544401505, 0.004863580114879581, 0.005112566278575984, 0.006203540765983594, 0.004872897160672691, 0.005355421139663735, 0.005576829791480545, 0.005248092745733989, 0.005343152534996354, 0.005253676603938085, 0.005372772280256762, 0.005062649814347022, 0.005092218929771767, 0.005822112001370379, 0.004738745117170173, 0.005537453510928466, 0.004911882093522784, 0.005366511859519537, 0.005358775511795525, 0.005263588534733064, 0.0056746155104794825, 0.005266013487545383, 0.00530274769529527, 0.005006880255762574, 0.005522920885448193, 0.005139858488989777, 0.007464051766451015, 0.005256536627881402, 0.0052589247203540315, 0.07087762755894124, 0.005712424348065153, 0.004979392559657437, 0.0054583449761361575, 0.017361335418468644, 0.009707610534374104, 0.00645939409353792, 0.009865348628007396, 0.010807774860823398, 0.008184077139662276, 0.009999245372709147, 0.00842734081345762, 0.005641957046464086, 0.005768178323240474, 0.006051977581972646, 0.006362808789776335, 0.0071932450451260045, 0.006092560141845498, 0.006181879534905906, 0.010690107069866256, 0.005830661650898672, 0.0054542418368983756, 0.00514041418534552, 0.0072659920484130815, 0.010149981582883832, 0.007699058464251805, 0.005616374673906627, 0.005629713814922197, 0.015971375837770484, 0.010010136510678676, 0.006475101046394123, 0.00928346976789451, 0.005275447417028942, 0.0051136723243046635, 0.010596976255413232, 0.005415201953309047, 0.0087107167427623, 0.023937007696584387, 0.005462186673634448, 0.0062889005111660375, 0.0060383376291801415, 0.004386650022054308, 0.006104097582486479, 0.0853184665807761, 0.005181772791412334, 0.00546497376743964, 0.004847135443543625, 0.005299087186062405, 0.0049445818145867695, 0.005352880838218816, 0.004886561162704819, 0.00721395367399118, 0.005739905535741601, 0.007194601998361218, 0.0059036020444013]
[129.0518781082359, 146.9879097692966, 178.34904018061906, 132.53388415465253, 135.91495221012687, 174.3559330614447, 141.949812521764, 168.58990974257958, 177.52381761866633, 170.496698480798, 54.190178283075575, 173.89323897693913, 60.113231741058165, 162.0083245087424, 48.90394549118931, 74.2034858551129, 158.31621765673242, 158.9415022493731, 134.16016261397567, 133.14402977292883, 126.81979511343015, 170.60929201741433, 156.7594917045918, 79.8896377051854, 144.98439628772292, 89.13755844327031, 147.8880577956552, 175.43644485855472, 122.16901610142475, 45.937016943364135, 157.27106237019848, 58.96036092239541, 78.86982896271226, 182.798613091705, 166.17835095388094, 114.6877847961887, 162.40025099372602, 104.47222056916529, 160.73208608755283, 138.3162542293605, 170.07659895687067, 78.68150338529112, 99.43582058464423, 60.25679008047449, 165.49172099262273, 134.3788058530344, 166.8840046311344, 89.11841702810192, 161.95454763122473, 163.95064749387265, 135.13166978697154, 121.18814015056023, 162.93250992519938, 156.83113511709453, 10.7105689785235, 169.55647013889745, 176.45906479141345, 114.12469852691795, 114.49569376594135, 161.0084515011503, 134.27099157518015, 144.3157265587267, 90.25345374145847, 90.88565104761268, 143.37711162275255, 140.59729961205966, 110.58805957159014, 158.47077324489328, 86.68617532716686, 148.895389095493, 144.2117704923844, 153.80094899718083, 140.1726823317136, 102.22949478384623, 92.14594349459001, 147.2606365591517, 150.0779829843971, 159.90525104530266, 85.8778780482159, 148.14724878888302, 76.8060326333204, 100.58659222660063, 64.13767603145469, 155.65012934090328, 70.19612019161261, 138.83764497761717, 135.5505498130443, 161.2335400840718, 101.36513472104323, 129.5377140596864, 149.25122241053913, 92.00551617335466, 89.66694291749675, 58.54421479724264, 70.11617106143254, 133.11438801537082, 102.62988849692708, 131.58982525316705, 11.927456390096916, 162.36344321133967, 27.586169307421507, 169.36426393202663, 175.7406832890427, 142.9100946062435, 173.21669835643016, 167.82394812561293, 21.143647314202585, 183.2856347850827, 191.24412603266848, 16.772926183186094, 196.15028425192273, 183.8182518447071, 211.74708622164857, 183.08114052778927, 199.06033335981456, 181.09627997393503, 202.81422757916638, 181.68291060078644, 204.42443996320236, 176.73311789323907, 146.36455699562995, 205.57836608307483, 187.22113440881643, 212.33253630736675, 190.6655071322603, 199.44382263806202, 184.6205468266717, 193.12386088713015, 184.96612384069988, 203.8297351350906, 191.50093976339795, 193.86127627241345, 192.55357188052568, 181.81900977434384, 205.6098545473964, 195.59648628722178, 161.19826365668226, 205.21672570285733, 186.7266782426731, 179.3133442099402, 190.5454130575091, 187.15542808299824, 190.34289229954, 186.1236523413962, 197.52501884805548, 196.37804536515085, 171.75897677073633, 211.02633192417153, 180.58842354639106, 203.58794876584741, 186.34077892255507, 186.60979505464252, 189.9844551680394, 176.2233931361993, 189.89696900038214, 188.58147840735944, 199.7251679524528, 181.06361121970855, 194.55788561924916, 133.97549096520763, 190.23932881887683, 190.15294060582784, 14.10882438423053, 175.0570229150971, 200.82770900649703, 183.20571608646802, 57.59925581163646, 103.01196122970283, 154.81328210031586, 101.36489218039729, 92.52598364394633, 122.18848661063159, 100.00754684241386, 118.66139297500584, 177.24346211155893, 173.36495925774628, 165.23524524921478, 157.1632958084148, 139.01931516674517, 164.13461282584728, 161.7631004217265, 93.54443257344391, 171.50712215411633, 183.34353882787545, 194.53685324634668, 137.62745587072362, 98.52234625591095, 129.88601199006223, 178.05079932539863, 177.62892269041922, 62.61201352704467, 99.89873753801598, 154.43774434329228, 107.71834508023609, 189.55738176292658, 195.55418035823715, 94.36654153954267, 184.66531970962484, 114.80111562930756, 41.776316099137645, 183.07686275661771, 159.01030684528797, 165.60849382908313, 227.9643908158625, 163.8243796870387, 11.72079199352745, 192.98414659501924, 182.9834949909565, 206.30741840152166, 188.71174692694015, 202.2415721891685, 186.81529259163415, 204.64289030744027, 138.62024143644683, 174.2188950276512, 138.99309513268133, 169.3881112715504]
Elapsed: 0.40548138458266736~0.5046300712639767
Time per graph: 0.009424036748059979~0.011736826030820326
Speed: 146.5435899011707~47.57100833695659
Total Time: 0.2546
best val loss: 0.2774156332015991 test_score: 0.8140

Testing...
Test loss: 0.3726 score: 0.8605 time: 0.53s
test Score 0.8605
Epoch Time List: [1.1201642368687317, 1.2768734149867669, 1.2420692391460761, 1.1716866868082434, 1.6731043020263314, 1.274855533032678, 1.4292868719203398, 1.5099459220655262, 11.29237914585974, 12.202261009952053, 1.747786173131317, 1.8038731949636713, 1.5844638041453436, 1.535190035123378, 2.476223963079974, 2.0649980129674077, 2.1856550839729607, 1.346751676988788, 1.1354167609242722, 1.3301823551300913, 1.867101546144113, 1.0095936508150771, 1.346835868083872, 1.5226773619651794, 1.0609677860047668, 1.486631517065689, 1.3483571679098532, 1.273470439016819, 1.4885630809003487, 2.698825749917887, 1.1386720378650352, 1.8431445920141414, 1.3308644469361752, 1.5152896819636226, 1.3589983279816806, 1.5074298980180174, 1.0370611810358241, 1.5504737138981, 1.34448014292866, 1.742409373051487, 1.5056943920208141, 1.616746870917268, 1.7389542550081387, 1.5734845991246402, 1.3360558690037578, 1.6380572350462899, 1.04029714607168, 1.993154140887782, 1.1436669969698414, 1.2877086349762976, 2.350908833905123, 1.3358707638690248, 1.5259466459974647, 1.3484895470319316, 8.803685955004767, 4.165301772067323, 12.181328654056415, 0.9904258649330586, 1.6608955960255116, 1.9455684138229117, 1.6199012050637975, 1.2014522490790114, 1.523798643029295, 1.7612110020127147, 1.5558274149661884, 1.6432914319448173, 2.1813937550177798, 1.5177018629619852, 1.590792660950683, 1.5209006789373234, 1.5867193420417607, 2.263552648946643, 1.8852667949395254, 1.7200393669772893, 1.672591065056622, 1.4360096160089597, 1.6475510919699445, 2.3985746450489387, 1.8108959577511996, 1.4437551849987358, 1.5493466518819332, 1.8256629997631535, 2.13320630392991, 1.8112117131240666, 1.5774514769436792, 1.5701200369512662, 1.7830693878931925, 1.6348868110217154, 1.4506439489778131, 1.9751216470031068, 1.387019605957903, 1.8040546359261498, 1.263785142917186, 1.8931813619565219, 1.5825860550394282, 1.6302071120589972, 1.516791998874396, 1.4134986731223762, 9.724611237994395, 3.964912381954491, 12.196711343131028, 0.8203868119744584, 0.8720460610929877, 0.8856613878160715, 0.8217530249385163, 0.8432725128950551, 2.7800434199161828, 2.023002907051705, 3.905478597036563, 3.151906862971373, 1.1753298899857327, 0.8512496679322794, 0.9018192200455815, 0.8294001870090142, 0.7961693529505283, 0.8488011389272287, 0.8032361599616706, 0.8373874699464068, 0.7952073342166841, 0.94192965188995, 0.9475681439507753, 0.8007538779638708, 0.9587611911119893, 0.8380850090179592, 0.8171881139278412, 0.7748381041456014, 0.811607590992935, 0.7810151189332828, 0.826294141006656, 0.8599655509460717, 0.8445725350175053, 0.9093818081310019, 0.8629943650448695, 0.8377956380136311, 0.8271666779182851, 0.9238872120622545, 0.9698643260635436, 0.8777800201205537, 0.9543700711801648, 0.968179346062243, 0.8384128080215305, 0.8315917570143938, 0.8151251699309796, 0.8332363020163029, 0.8836820139549673, 0.9508308439981192, 0.9568664758699015, 0.8182401451049373, 0.9892407350707799, 0.8257935320725664, 0.8887854419881478, 0.9531579260947183, 0.8253693680744618, 0.8640104940859601, 0.820911010960117, 0.8809481380740181, 0.7987864430760965, 0.9135855890344828, 0.8161360800731927, 1.1087129230145365, 0.8326545020099729, 0.8142959380056709, 3.6786294230259955, 0.9003904380369931, 0.7992410160368308, 0.8493916860315949, 1.37422250804957, 1.1108407670399174, 1.493734730174765, 2.212737369001843, 1.5432644420070574, 1.9178784000687301, 1.912816843832843, 1.833397202892229, 1.8237825579708442, 1.2358397679636255, 1.8080116690834984, 1.235299754072912, 2.306937405024655, 1.5425318661145866, 1.6231266659451649, 1.7473011801484972, 1.9496876639313996, 1.8282841939944774, 1.341895051067695, 1.3752264740178362, 1.199535335879773, 1.9631638900609687, 1.813858991023153, 1.9883835219079629, 1.5301358188735321, 1.7377092051319778, 1.8277235981076956, 1.9579406001139432, 2.471971432911232, 1.837874420802109, 1.6339673680486158, 1.7939311490627006, 1.863598445081152, 3.1257855580188334, 1.521823395974934, 1.6119377230061218, 1.6485266870586202, 1.2116103589069098, 10.508535919943824, 7.96745019312948, 3.029080262989737, 0.8338277089642361, 0.7933178420644253, 0.848624374018982, 0.7936439699260518, 0.8537624310702085, 0.7825704519636929, 0.9254784460645169, 1.0013763179304078, 1.4457476650131866, 1.566528478055261]
Total Epoch List: [8, 103, 106]
Total Time List: [0.2612073279451579, 0.21976986795198172, 0.25456526898778975]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334af0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.47s
Epoch 2/1000, LR 0.000000
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.56s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 1.00s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 1.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.51s
Epoch 4/1000, LR 0.000060
Train loss: 0.6951;  Loss pred: 0.6951; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.89s
Epoch 5/1000, LR 0.000090
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.68s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.53s
Epoch 6/1000, LR 0.000120
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.46s
Epoch 7/1000, LR 0.000150
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.60s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.43s
Epoch 8/1000, LR 0.000180
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.49s
Epoch 9/1000, LR 0.000210
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.73s
Epoch 10/1000, LR 0.000240
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.63s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.44s
Epoch 11/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.44s
Epoch 12/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.73s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.28s
Epoch 13/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.29s
Epoch 14/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.78s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5116 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.35s
Epoch 16/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5116 time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.45s
Epoch 17/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5116 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.58s
Epoch 18/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 1.02s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5116 time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.29s
Epoch 19/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.61s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5116 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5000 time: 0.49s
Epoch 20/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.56s
Val loss: 0.6915 score: 0.5814 time: 0.40s
Test loss: 0.6907 score: 0.5227 time: 0.35s
Epoch 21/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.55s
Val loss: 0.6913 score: 0.5581 time: 0.26s
Test loss: 0.6904 score: 0.5682 time: 0.45s
Epoch 22/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.52s
Val loss: 0.6911 score: 0.5814 time: 0.29s
Test loss: 0.6900 score: 0.6818 time: 0.69s
Epoch 23/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.61s
Val loss: 0.6908 score: 0.6279 time: 0.26s
Test loss: 0.6896 score: 0.7045 time: 0.46s
Epoch 24/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.89s
Val loss: 0.6905 score: 0.7209 time: 0.43s
Test loss: 0.6891 score: 0.7727 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 1.05s
Val loss: 0.6902 score: 0.7209 time: 0.47s
Test loss: 0.6886 score: 0.7727 time: 0.27s
Epoch 26/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.58s
Val loss: 0.6898 score: 0.7209 time: 0.72s
Test loss: 0.6880 score: 0.7727 time: 0.45s
Epoch 27/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 1.03s
Val loss: 0.6894 score: 0.6977 time: 0.56s
Test loss: 0.6874 score: 0.7727 time: 0.58s
Epoch 28/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.68s
Val loss: 0.6889 score: 0.6744 time: 0.33s
Test loss: 0.6867 score: 0.7955 time: 0.38s
Epoch 29/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.55s
Val loss: 0.6883 score: 0.6977 time: 0.36s
Test loss: 0.6860 score: 0.7955 time: 0.35s
Epoch 30/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.81s
Val loss: 0.6877 score: 0.6977 time: 0.36s
Test loss: 0.6851 score: 0.7955 time: 0.45s
Epoch 31/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.50s
Val loss: 0.6870 score: 0.6977 time: 0.45s
Test loss: 0.6842 score: 0.8182 time: 0.56s
Epoch 32/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.80s
Val loss: 0.6862 score: 0.6977 time: 0.45s
Test loss: 0.6832 score: 0.8182 time: 0.47s
Epoch 33/1000, LR 0.000270
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.47s
Val loss: 0.6853 score: 0.6977 time: 0.30s
Test loss: 0.6821 score: 0.8182 time: 0.69s
Epoch 34/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.83s
Val loss: 0.6843 score: 0.6977 time: 0.30s
Test loss: 0.6809 score: 0.8182 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.38s
Val loss: 0.6832 score: 0.6977 time: 0.24s
Test loss: 0.6796 score: 0.8182 time: 4.04s
Epoch 36/1000, LR 0.000270
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 6.35s
Val loss: 0.6819 score: 0.7209 time: 0.31s
Test loss: 0.6781 score: 0.8182 time: 4.37s
Epoch 37/1000, LR 0.000270
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 5.97s
Val loss: 0.6805 score: 0.7209 time: 0.29s
Test loss: 0.6765 score: 0.7955 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6751;  Loss pred: 0.6751; Loss self: 0.0000; time: 0.34s
Val loss: 0.6790 score: 0.7442 time: 0.25s
Test loss: 0.6748 score: 0.7955 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6730;  Loss pred: 0.6730; Loss self: 0.0000; time: 0.31s
Val loss: 0.6773 score: 0.7442 time: 0.25s
Test loss: 0.6728 score: 0.7955 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6710;  Loss pred: 0.6710; Loss self: 0.0000; time: 0.33s
Val loss: 0.6755 score: 0.6977 time: 0.26s
Test loss: 0.6707 score: 0.7955 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6678;  Loss pred: 0.6678; Loss self: 0.0000; time: 0.36s
Val loss: 0.6734 score: 0.7209 time: 0.27s
Test loss: 0.6684 score: 0.7955 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6663;  Loss pred: 0.6663; Loss self: 0.0000; time: 0.32s
Val loss: 0.6711 score: 0.7209 time: 0.26s
Test loss: 0.6659 score: 0.7955 time: 0.32s
Epoch 43/1000, LR 0.000269
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.32s
Val loss: 0.6686 score: 0.7442 time: 0.29s
Test loss: 0.6632 score: 0.7955 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6592;  Loss pred: 0.6592; Loss self: 0.0000; time: 0.35s
Val loss: 0.6658 score: 0.7442 time: 0.26s
Test loss: 0.6603 score: 0.7727 time: 0.27s
Epoch 45/1000, LR 0.000269
Train loss: 0.6542;  Loss pred: 0.6542; Loss self: 0.0000; time: 0.28s
Val loss: 0.6627 score: 0.7442 time: 0.27s
Test loss: 0.6572 score: 0.7727 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6528;  Loss pred: 0.6528; Loss self: 0.0000; time: 0.34s
Val loss: 0.6594 score: 0.7209 time: 0.23s
Test loss: 0.6538 score: 0.8409 time: 0.26s
Epoch 47/1000, LR 0.000269
Train loss: 0.6458;  Loss pred: 0.6458; Loss self: 0.0000; time: 0.28s
Val loss: 0.6558 score: 0.7209 time: 0.26s
Test loss: 0.6501 score: 0.8409 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6445;  Loss pred: 0.6445; Loss self: 0.0000; time: 0.33s
Val loss: 0.6518 score: 0.7442 time: 0.25s
Test loss: 0.6462 score: 0.8409 time: 0.26s
Epoch 49/1000, LR 0.000269
Train loss: 0.6354;  Loss pred: 0.6354; Loss self: 0.0000; time: 0.37s
Val loss: 0.6477 score: 0.7674 time: 0.27s
Test loss: 0.6420 score: 0.8409 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6305;  Loss pred: 0.6305; Loss self: 0.0000; time: 0.35s
Val loss: 0.6431 score: 0.7674 time: 0.24s
Test loss: 0.6374 score: 0.8409 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 0.27s
Val loss: 0.6383 score: 0.7674 time: 0.26s
Test loss: 0.6326 score: 0.8409 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 0.34s
Val loss: 0.6331 score: 0.7674 time: 0.24s
Test loss: 0.6275 score: 0.8409 time: 0.28s
Epoch 53/1000, LR 0.000269
Train loss: 0.6144;  Loss pred: 0.6144; Loss self: 0.0000; time: 0.31s
Val loss: 0.6277 score: 0.8140 time: 0.26s
Test loss: 0.6220 score: 0.8409 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 0.36s
Val loss: 0.6218 score: 0.8372 time: 0.27s
Test loss: 0.6161 score: 0.8409 time: 0.24s
Epoch 55/1000, LR 0.000269
Train loss: 0.5925;  Loss pred: 0.5925; Loss self: 0.0000; time: 0.33s
Val loss: 0.6157 score: 0.8372 time: 0.28s
Test loss: 0.6100 score: 0.8409 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.5891;  Loss pred: 0.5891; Loss self: 0.0000; time: 0.38s
Val loss: 0.6093 score: 0.8372 time: 0.27s
Test loss: 0.6035 score: 0.8409 time: 0.34s
Epoch 57/1000, LR 0.000269
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.35s
Val loss: 0.6026 score: 0.8140 time: 0.24s
Test loss: 0.5967 score: 0.8409 time: 0.26s
Epoch 58/1000, LR 0.000269
Train loss: 0.5664;  Loss pred: 0.5664; Loss self: 0.0000; time: 0.34s
Val loss: 0.5957 score: 0.8140 time: 0.26s
Test loss: 0.5897 score: 0.8409 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.5639;  Loss pred: 0.5639; Loss self: 0.0000; time: 0.47s
Val loss: 0.5885 score: 0.8140 time: 0.26s
Test loss: 0.5824 score: 0.8409 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5504;  Loss pred: 0.5504; Loss self: 0.0000; time: 0.34s
Val loss: 0.5810 score: 0.8140 time: 0.27s
Test loss: 0.5748 score: 0.8409 time: 0.26s
Epoch 61/1000, LR 0.000268
Train loss: 0.5416;  Loss pred: 0.5416; Loss self: 0.0000; time: 0.41s
Val loss: 0.5730 score: 0.7907 time: 0.26s
Test loss: 0.5669 score: 0.8636 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.5361;  Loss pred: 0.5361; Loss self: 0.0000; time: 0.33s
Val loss: 0.5648 score: 0.7907 time: 0.26s
Test loss: 0.5587 score: 0.8636 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5214;  Loss pred: 0.5214; Loss self: 0.0000; time: 0.34s
Val loss: 0.5562 score: 0.7907 time: 0.25s
Test loss: 0.5503 score: 0.8636 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5119;  Loss pred: 0.5119; Loss self: 0.0000; time: 0.33s
Val loss: 0.5474 score: 0.7907 time: 0.25s
Test loss: 0.5417 score: 0.8636 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.4936;  Loss pred: 0.4936; Loss self: 0.0000; time: 0.34s
Val loss: 0.5382 score: 0.7907 time: 0.25s
Test loss: 0.5329 score: 0.8636 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.4834;  Loss pred: 0.4834; Loss self: 0.0000; time: 0.34s
Val loss: 0.5287 score: 0.7907 time: 0.27s
Test loss: 0.5238 score: 0.8636 time: 0.37s
Epoch 67/1000, LR 0.000268
Train loss: 0.4847;  Loss pred: 0.4847; Loss self: 0.0000; time: 0.34s
Val loss: 0.5189 score: 0.7907 time: 0.24s
Test loss: 0.5144 score: 0.8636 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.4659;  Loss pred: 0.4659; Loss self: 0.0000; time: 0.32s
Val loss: 0.5091 score: 0.7907 time: 0.33s
Test loss: 0.5052 score: 0.8636 time: 0.27s
Epoch 69/1000, LR 0.000268
Train loss: 0.4468;  Loss pred: 0.4468; Loss self: 0.0000; time: 0.66s
Val loss: 0.4993 score: 0.7907 time: 0.30s
Test loss: 0.4962 score: 0.8636 time: 0.70s
Epoch 70/1000, LR 0.000268
Train loss: 0.4404;  Loss pred: 0.4404; Loss self: 0.0000; time: 0.55s
Val loss: 0.4897 score: 0.7907 time: 0.45s
Test loss: 0.4873 score: 0.8636 time: 0.52s
Epoch 71/1000, LR 0.000268
Train loss: 0.4160;  Loss pred: 0.4160; Loss self: 0.0000; time: 0.90s
Val loss: 0.4801 score: 0.8140 time: 0.24s
Test loss: 0.4786 score: 0.8409 time: 0.36s
Epoch 72/1000, LR 0.000267
Train loss: 0.4077;  Loss pred: 0.4077; Loss self: 0.0000; time: 0.52s
Val loss: 0.4706 score: 0.8140 time: 0.53s
Test loss: 0.4700 score: 0.8409 time: 0.50s
Epoch 73/1000, LR 0.000267
Train loss: 0.3904;  Loss pred: 0.3904; Loss self: 0.0000; time: 0.80s
Val loss: 0.4610 score: 0.8140 time: 0.35s
Test loss: 0.4614 score: 0.8636 time: 0.59s
Epoch 74/1000, LR 0.000267
Train loss: 0.3715;  Loss pred: 0.3715; Loss self: 0.0000; time: 1.11s
Val loss: 0.4516 score: 0.8140 time: 0.48s
Test loss: 0.4530 score: 0.8636 time: 0.45s
Epoch 75/1000, LR 0.000267
Train loss: 0.3466;  Loss pred: 0.3466; Loss self: 0.0000; time: 1.01s
Val loss: 0.4426 score: 0.8372 time: 0.42s
Test loss: 0.4450 score: 0.8636 time: 0.52s
Epoch 76/1000, LR 0.000267
Train loss: 0.3350;  Loss pred: 0.3350; Loss self: 0.0000; time: 0.85s
Val loss: 0.4341 score: 0.8140 time: 0.52s
Test loss: 0.4376 score: 0.8409 time: 0.42s
Epoch 77/1000, LR 0.000267
Train loss: 0.3526;  Loss pred: 0.3526; Loss self: 0.0000; time: 0.98s
Val loss: 0.4265 score: 0.8372 time: 0.45s
Test loss: 0.4309 score: 0.8409 time: 0.28s
Epoch 78/1000, LR 0.000267
Train loss: 0.3324;  Loss pred: 0.3324; Loss self: 0.0000; time: 1.00s
Val loss: 0.4194 score: 0.8372 time: 0.27s
Test loss: 0.4247 score: 0.8409 time: 0.47s
Epoch 79/1000, LR 0.000267
Train loss: 0.2909;  Loss pred: 0.2909; Loss self: 0.0000; time: 0.38s
Val loss: 0.4131 score: 0.8605 time: 0.64s
Test loss: 0.4190 score: 0.8636 time: 0.70s
Epoch 80/1000, LR 0.000267
Train loss: 0.2945;  Loss pred: 0.2945; Loss self: 0.0000; time: 0.93s
Val loss: 0.4072 score: 0.8605 time: 0.50s
Test loss: 0.4139 score: 0.8636 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.2819;  Loss pred: 0.2819; Loss self: 0.0000; time: 1.08s
Val loss: 0.4018 score: 0.8605 time: 0.60s
Test loss: 0.4093 score: 0.8636 time: 0.83s
Epoch 82/1000, LR 0.000267
Train loss: 0.2588;  Loss pred: 0.2588; Loss self: 0.0000; time: 0.95s
Val loss: 0.3972 score: 0.8605 time: 0.31s
Test loss: 0.4055 score: 0.8636 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.2567;  Loss pred: 0.2567; Loss self: 0.0000; time: 1.12s
Val loss: 0.3934 score: 0.8605 time: 0.50s
Test loss: 0.4025 score: 0.8636 time: 0.49s
Epoch 84/1000, LR 0.000266
Train loss: 0.2229;  Loss pred: 0.2229; Loss self: 0.0000; time: 0.84s
Val loss: 0.3905 score: 0.8605 time: 0.49s
Test loss: 0.4002 score: 0.8636 time: 0.41s
Epoch 85/1000, LR 0.000266
Train loss: 0.2388;  Loss pred: 0.2388; Loss self: 0.0000; time: 0.83s
Val loss: 0.3883 score: 0.8605 time: 0.53s
Test loss: 0.3987 score: 0.8636 time: 0.30s
Epoch 86/1000, LR 0.000266
Train loss: 0.2052;  Loss pred: 0.2052; Loss self: 0.0000; time: 0.76s
Val loss: 0.3870 score: 0.8605 time: 0.29s
Test loss: 0.3975 score: 0.8636 time: 0.24s
Epoch 87/1000, LR 0.000266
Train loss: 0.2041;  Loss pred: 0.2041; Loss self: 0.0000; time: 0.60s
Val loss: 0.3864 score: 0.8605 time: 0.47s
Test loss: 0.3973 score: 0.8636 time: 0.48s
Epoch 88/1000, LR 0.000266
Train loss: 0.1863;  Loss pred: 0.1863; Loss self: 0.0000; time: 0.84s
Val loss: 0.3865 score: 0.8605 time: 0.49s
Test loss: 0.3978 score: 0.8636 time: 0.47s
     INFO: Early stopping counter 1 of 2
Epoch 89/1000, LR 0.000266
Train loss: 0.1936;  Loss pred: 0.1936; Loss self: 0.0000; time: 0.40s
Val loss: 0.3872 score: 0.8605 time: 0.47s
Test loss: 0.3991 score: 0.8636 time: 0.46s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.2041,   Val_Loss: 0.3864,   Val_Precision: 0.9000,   Val_Recall: 0.8182,   Val_accuracy: 0.8571,   Val_Score: 0.8605,   Val_Loss: 0.3864,   Test_Precision: 0.9444,   Test_Recall: 0.7727,   Test_accuracy: 0.8500,   Test_Score: 0.8636,   Test_loss: 0.3973


[0.4750008130213246, 0.2631252839928493, 0.5170289130182937, 0.9000069790054113, 0.5307258020620793, 0.46501301997341216, 0.43520022893790156, 0.4975579329766333, 0.7342285390477628, 0.4409626049455255, 0.4422386040678248, 0.2875031860312447, 0.2955530929611996, 0.25926076201722026, 0.3586061700480059, 0.4599633930483833, 0.5854537429986522, 0.2947639930061996, 0.4970294889062643, 0.35809657094068825, 0.4542290640529245, 0.6925757750868797, 0.46469670499209315, 0.25840423500631005, 0.27010509092360735, 0.4555436830269173, 0.5855305090080947, 0.38917365903034806, 0.35105917998589575, 0.4591894129989669, 0.561283832998015, 0.47328772104810923, 0.6918625149410218, 0.23674789804499596, 4.049542690976523, 4.381657504942268, 0.23985040606930852, 0.24084969598334283, 0.2489144039573148, 0.2323405840434134, 0.24973722407594323, 0.32212266500573605, 0.23332794406451285, 0.2700109879951924, 0.24460555706173182, 0.2617763429880142, 0.24219672102481127, 0.2604445000179112, 0.23085203801747411, 0.25202304404228926, 0.25423878396395594, 0.28004125703591853, 0.23174647602718323, 0.24742666503880173, 0.25231824594084173, 0.34323251398745924, 0.26408101397100836, 0.2390072689158842, 0.25084779295139015, 0.2626024349592626, 0.2386522040469572, 0.25029452599119395, 0.24000028800219297, 0.26140701200347394, 0.23955019598361105, 0.3781608260469511, 0.2564388749888167, 0.2763956020353362, 0.7027518569957465, 0.5231539070373401, 0.3700473939534277, 0.5013921608915552, 0.5895882210461423, 0.4571878139395267, 0.5219456870108843, 0.42018577898852527, 0.283888011937961, 0.47732575493864715, 0.7026161570101976, 0.26074565399903804, 0.8304567459272221, 0.23892625991720706, 0.4987050120253116, 0.41825154493562877, 0.3084625320043415, 0.2475069990614429, 0.485217617941089, 0.47498324990738183, 0.46161280502565205]
[0.010795473023211922, 0.0059801200907465745, 0.011750657114052128, 0.0204547040683048, 0.012061950046865439, 0.010568477726668458, 0.009890914294043218, 0.01130813484037803, 0.016687012251085518, 0.010021877385125581, 0.010050877365177836, 0.006534163318891925, 0.006717115749118172, 0.005892290045845915, 0.00815014022836377, 0.010453713478372347, 0.013305766886333004, 0.006699181659231809, 0.011296124747869644, 0.008138558430470188, 0.010323387819384649, 0.015740358524701813, 0.010561288749820298, 0.005872823522870683, 0.0061387520664456215, 0.010353265523339029, 0.013307511568365788, 0.008844855887053365, 0.007978617726952176, 0.010436123022703792, 0.012756450749954885, 0.010756539114729756, 0.015724148066841404, 0.005380634046477181, 0.09203506115855734, 0.09958312511232427, 0.005451145592484285, 0.005473856726894155, 0.005657145544484427, 0.005280467819168486, 0.005675846001725982, 0.007320969659221274, 0.0053029078196480195, 0.0061366133635271, 0.00555921720594845, 0.005949462340636687, 0.005504470932382074, 0.0059191931822252545, 0.005246637227669867, 0.005727796455506574, 0.0057781541809989985, 0.006364574023543603, 0.005266965364254164, 0.005623333296336403, 0.005734505589564585, 0.007800738954260437, 0.006001841226613827, 0.0054319833844519135, 0.0057010862034406855, 0.0059682371581650595, 0.005423913728339936, 0.0056885119543453174, 0.00545455200004984, 0.0059410684546244074, 0.0054443226359911605, 0.008594564228339797, 0.0058281562497458335, 0.0062817182280758225, 0.015971633113539694, 0.011889861523575912, 0.008410168044396083, 0.011395276383898983, 0.013399732296503234, 0.010390632134989242, 0.011862401977520098, 0.009549676795193756, 0.006452000271317295, 0.01084831261224198, 0.015968549022959036, 0.005926037590887228, 0.018874016952891412, 0.005430142270845615, 0.011334204818757082, 0.0095057169303552, 0.007010512091007761, 0.005625159069578248, 0.01102767313502475, 0.010795073861531406, 0.010491200114219364]
[92.63142039722082, 167.22072213020678, 85.10162370444296, 48.88850978536184, 82.90533422163125, 94.62100653120588, 101.10288799107762, 88.43191331865744, 59.92684519872324, 99.78170372391453, 99.49380175153559, 153.04178227513017, 148.8734208772985, 169.7133019962253, 122.69727538182015, 95.65978655038677, 75.15538251516703, 149.27196348257698, 88.52593454127636, 122.87188309124508, 96.86742545138709, 63.530954420807525, 94.68541422248445, 170.27584706158376, 162.89955827764953, 96.58788309309102, 75.14552926462747, 113.06006709094575, 125.33499338136595, 95.82102451499463, 78.39171095483096, 92.96670512085278, 63.59645023368668, 185.8517028592052, 10.865424409043497, 10.041862000936957, 183.44767774662643, 182.6865498848005, 176.76759279686107, 189.37716017696889, 176.1851889032767, 136.59392765552988, 188.57578408111476, 162.95633124672477, 179.88144066937772, 168.08241530830233, 181.670502448679, 168.9419434734619, 190.59827401943693, 174.58720954349252, 173.0656484190783, 157.11970609515043, 189.8626497122611, 177.83046945687875, 174.38294973846718, 128.1929834934217, 166.61553717311332, 184.09481937340274, 175.40517092979334, 167.55366341833692, 184.36871419525028, 175.7928976902517, 183.33311333192214, 168.3198918910992, 183.67757880277537, 116.35261235265314, 171.58084943992537, 159.19211331870164, 62.61100495429401, 84.10526884750858, 118.90368833549364, 87.7556600042589, 74.62835658746391, 96.24053541772656, 84.29995897079317, 104.71558581996081, 154.99069404035092, 92.18023445153409, 62.62309734981143, 168.7468202256684, 52.982891903506776, 184.15723753114003, 88.22850971821954, 105.19985050329424, 142.64293207377523, 177.77275053574192, 90.680961228704, 92.63484556261652, 95.31797974615306]
Elapsed: 0.4712657736943972~0.5884355622419546
Time per graph: 0.010710585765781754~0.013373535505498964
Speed: 128.09383463390728~46.725567284043116
Total Time: 0.4621
best val loss: 0.3864053189754486 test_score: 0.8636

Testing...
Test loss: 0.4190 score: 0.8636 time: 0.31s
test Score 0.8636
Epoch Time List: [1.7608320949366316, 1.384632449131459, 2.5586590870516375, 1.9334400609368458, 1.668121075956151, 1.7001923019997776, 1.4822224030504003, 1.6038846380542964, 1.74325524806045, 1.553041424951516, 1.722251403843984, 1.4221997618442401, 1.5858034319244325, 1.4872932969592512, 1.179690942983143, 1.5142534569604322, 1.8968538669869304, 1.8146002978319302, 1.6651129788951948, 1.3135791310342029, 1.2570316239725798, 1.49937800609041, 1.3318269540322945, 1.578525546938181, 1.7837930389214307, 1.7506346091395244, 2.175501978956163, 1.398768470971845, 1.2535952089820057, 1.625775224994868, 1.5064678319031373, 1.7206453839316964, 1.4639345599571243, 1.363475403864868, 4.66681337216869, 11.03550478105899, 6.498498068889603, 0.8284382539568469, 0.806134590995498, 0.8161564629990608, 0.8666771681746468, 0.89684079203289, 0.8391823279671371, 0.8783365390263498, 0.7845741219352931, 0.8307436329778284, 0.7778650788823143, 0.8395597640192136, 0.8656018481124192, 0.8312685569981113, 0.7800973849371076, 0.853192865033634, 0.7989277560263872, 0.8794622650602832, 0.8569967179792002, 0.9892935970565304, 0.8522592339431867, 0.8321516149444506, 0.977019413956441, 0.8756983180064708, 0.9018809418193996, 0.8402820390183479, 0.8291244428837672, 0.8405553179327399, 0.8278652881272137, 0.9822619151091203, 0.8301656959811226, 0.9239286199444905, 1.6627276620129123, 1.518990293261595, 1.5028761230641976, 1.5417230220045894, 1.7296291328966618, 2.043984869029373, 1.9453048270661384, 1.7824599930318072, 1.7072918640915304, 1.7410500210244209, 1.7237746769096702, 1.6811653848271817, 2.511383901932277, 1.4987384889973328, 2.1083124038996175, 1.746965229860507, 1.6737301180837676, 1.2986861079698429, 1.5426038600271568, 1.7903364651137963, 1.3223777728853747]
Total Epoch List: [89]
Total Time List: [0.4620678350329399]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335cf0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.83s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.26s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.50s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.82s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.4884 time: 0.34s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.29s
Epoch 6/1000, LR 0.000120
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 1.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.4884 time: 0.57s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.50s
Epoch 9/1000, LR 0.000210
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.80s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.27s
Epoch 10/1000, LR 0.000240
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.77s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.35s
Epoch 11/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.37s
Epoch 12/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.74s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.28s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 1.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.47s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 1.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 10.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.48s
Epoch 16/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 10.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.26s
Epoch 17/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.54s
Epoch 18/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.87s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.28s
Epoch 19/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.51s
Epoch 20/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.61s
Val loss: 0.6911 score: 0.5227 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.38s
Epoch 22/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.82s
Val loss: 0.6908 score: 0.5227 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.41s
Epoch 23/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.60s
Val loss: 0.6905 score: 0.5227 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.60s
Epoch 24/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.81s
Val loss: 0.6901 score: 0.5909 time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.53s
Epoch 25/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.36s
Val loss: 0.6897 score: 0.5909 time: 0.60s
Test loss: 0.6914 score: 0.5581 time: 0.34s
Epoch 26/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.83s
Val loss: 0.6892 score: 0.5909 time: 0.48s
Test loss: 0.6911 score: 0.6047 time: 0.45s
Epoch 27/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.55s
Val loss: 0.6887 score: 0.6364 time: 0.25s
Test loss: 0.6908 score: 0.6047 time: 0.50s
Epoch 28/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.50s
Val loss: 0.6881 score: 0.6591 time: 0.27s
Test loss: 0.6904 score: 0.5814 time: 0.44s
Epoch 29/1000, LR 0.000270
Train loss: 0.6841;  Loss pred: 0.6841; Loss self: 0.0000; time: 0.58s
Val loss: 0.6875 score: 0.6591 time: 0.24s
Test loss: 0.6900 score: 0.5814 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6831;  Loss pred: 0.6831; Loss self: 0.0000; time: 0.86s
Val loss: 0.6868 score: 0.6818 time: 0.37s
Test loss: 0.6896 score: 0.5814 time: 0.47s
Epoch 31/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.83s
Val loss: 0.6860 score: 0.7045 time: 0.24s
Test loss: 0.6891 score: 0.5814 time: 0.24s
Epoch 32/1000, LR 0.000270
Train loss: 0.6804;  Loss pred: 0.6804; Loss self: 0.0000; time: 0.60s
Val loss: 0.6851 score: 0.7045 time: 0.45s
Test loss: 0.6885 score: 0.6047 time: 0.30s
Epoch 33/1000, LR 0.000270
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.82s
Val loss: 0.6842 score: 0.7273 time: 0.25s
Test loss: 0.6879 score: 0.5814 time: 0.48s
Epoch 34/1000, LR 0.000270
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.74s
Val loss: 0.6831 score: 0.7045 time: 0.43s
Test loss: 0.6873 score: 0.5814 time: 0.29s
Epoch 35/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.95s
Val loss: 0.6820 score: 0.7727 time: 0.34s
Test loss: 0.6866 score: 0.6047 time: 0.28s
Epoch 36/1000, LR 0.000270
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.59s
Val loss: 0.6807 score: 0.7727 time: 0.43s
Test loss: 0.6858 score: 0.6279 time: 0.49s
Epoch 37/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.82s
Val loss: 0.6793 score: 0.7955 time: 0.57s
Test loss: 0.6849 score: 0.6512 time: 0.27s
Epoch 38/1000, LR 0.000270
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.75s
Val loss: 0.6778 score: 0.7955 time: 0.42s
Test loss: 0.6839 score: 0.6512 time: 0.35s
Epoch 39/1000, LR 0.000269
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.80s
Val loss: 0.6761 score: 0.8182 time: 0.73s
Test loss: 0.6828 score: 0.6744 time: 0.27s
Epoch 40/1000, LR 0.000269
Train loss: 0.6628;  Loss pred: 0.6628; Loss self: 0.0000; time: 0.52s
Val loss: 0.6741 score: 0.8182 time: 0.28s
Test loss: 0.6817 score: 0.6744 time: 0.43s
Epoch 41/1000, LR 0.000269
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 0.55s
Val loss: 0.6720 score: 0.8182 time: 0.27s
Test loss: 0.6803 score: 0.6744 time: 0.45s
Epoch 42/1000, LR 0.000269
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 1.03s
Val loss: 0.6698 score: 0.8409 time: 0.47s
Test loss: 0.6789 score: 0.6744 time: 0.61s
Epoch 43/1000, LR 0.000269
Train loss: 0.6518;  Loss pred: 0.6518; Loss self: 0.0000; time: 0.87s
Val loss: 0.6672 score: 0.8636 time: 0.48s
Test loss: 0.6774 score: 0.6744 time: 0.64s
Epoch 44/1000, LR 0.000269
Train loss: 0.6473;  Loss pred: 0.6473; Loss self: 0.0000; time: 0.44s
Val loss: 0.6644 score: 0.9091 time: 0.48s
Test loss: 0.6757 score: 0.6512 time: 0.29s
Epoch 45/1000, LR 0.000269
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.82s
Val loss: 0.6614 score: 0.9091 time: 0.42s
Test loss: 0.6738 score: 0.6512 time: 0.26s
Epoch 46/1000, LR 0.000269
Train loss: 0.6379;  Loss pred: 0.6379; Loss self: 0.0000; time: 0.63s
Val loss: 0.6581 score: 0.8636 time: 0.24s
Test loss: 0.6718 score: 0.6512 time: 0.27s
Epoch 47/1000, LR 0.000269
Train loss: 0.6290;  Loss pred: 0.6290; Loss self: 0.0000; time: 0.62s
Val loss: 0.6545 score: 0.8636 time: 0.27s
Test loss: 0.6696 score: 0.6512 time: 0.28s
Epoch 48/1000, LR 0.000269
Train loss: 0.6254;  Loss pred: 0.6254; Loss self: 0.0000; time: 0.86s
Val loss: 0.6506 score: 0.8636 time: 0.24s
Test loss: 0.6672 score: 0.6512 time: 0.32s
Epoch 49/1000, LR 0.000269
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 0.65s
Val loss: 0.6464 score: 0.8636 time: 0.23s
Test loss: 0.6646 score: 0.6977 time: 0.30s
Epoch 50/1000, LR 0.000269
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.58s
Val loss: 0.6418 score: 0.8636 time: 0.22s
Test loss: 0.6619 score: 0.6977 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6040;  Loss pred: 0.6040; Loss self: 0.0000; time: 0.65s
Val loss: 0.6370 score: 0.8864 time: 0.25s
Test loss: 0.6590 score: 0.6977 time: 0.28s
Epoch 52/1000, LR 0.000269
Train loss: 0.5961;  Loss pred: 0.5961; Loss self: 0.0000; time: 1.64s
Val loss: 0.6319 score: 0.8864 time: 0.73s
Test loss: 0.6560 score: 0.6977 time: 0.26s
Epoch 53/1000, LR 0.000269
Train loss: 0.5887;  Loss pred: 0.5887; Loss self: 0.0000; time: 0.90s
Val loss: 0.6264 score: 0.8864 time: 0.25s
Test loss: 0.6527 score: 0.6977 time: 0.63s
Epoch 54/1000, LR 0.000269
Train loss: 0.5768;  Loss pred: 0.5768; Loss self: 0.0000; time: 0.80s
Val loss: 0.6206 score: 0.8636 time: 0.57s
Test loss: 0.6492 score: 0.7209 time: 1.00s
Epoch 55/1000, LR 0.000269
Train loss: 0.5665;  Loss pred: 0.5665; Loss self: 0.0000; time: 1.05s
Val loss: 0.6144 score: 0.8636 time: 0.40s
Test loss: 0.6455 score: 0.7209 time: 0.86s
Epoch 56/1000, LR 0.000269
Train loss: 0.5602;  Loss pred: 0.5602; Loss self: 0.0000; time: 0.61s
Val loss: 0.6080 score: 0.8636 time: 0.36s
Test loss: 0.6416 score: 0.7209 time: 0.29s
Epoch 57/1000, LR 0.000269
Train loss: 0.5440;  Loss pred: 0.5440; Loss self: 0.0000; time: 0.98s
Val loss: 0.6012 score: 0.8636 time: 0.52s
Test loss: 0.6374 score: 0.7209 time: 0.27s
Epoch 58/1000, LR 0.000269
Train loss: 0.5336;  Loss pred: 0.5336; Loss self: 0.0000; time: 1.10s
Val loss: 0.5940 score: 0.8636 time: 0.42s
Test loss: 0.6329 score: 0.7209 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.5272;  Loss pred: 0.5272; Loss self: 0.0000; time: 0.39s
Val loss: 0.5864 score: 0.8636 time: 3.77s
Test loss: 0.6282 score: 0.7442 time: 3.67s
Epoch 60/1000, LR 0.000268
Train loss: 0.5091;  Loss pred: 0.5091; Loss self: 0.0000; time: 2.78s
Val loss: 0.5785 score: 0.8636 time: 0.26s
Test loss: 0.6233 score: 0.7442 time: 1.09s
Epoch 61/1000, LR 0.000268
Train loss: 0.4979;  Loss pred: 0.4979; Loss self: 0.0000; time: 9.40s
Val loss: 0.5703 score: 0.8636 time: 0.24s
Test loss: 0.6183 score: 0.7442 time: 0.32s
Epoch 62/1000, LR 0.000268
Train loss: 0.4851;  Loss pred: 0.4851; Loss self: 0.0000; time: 0.88s
Val loss: 0.5617 score: 0.8636 time: 0.24s
Test loss: 0.6129 score: 0.7209 time: 0.26s
Epoch 63/1000, LR 0.000268
Train loss: 0.4701;  Loss pred: 0.4701; Loss self: 0.0000; time: 0.55s
Val loss: 0.5528 score: 0.8636 time: 0.44s
Test loss: 0.6074 score: 0.7442 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.4578;  Loss pred: 0.4578; Loss self: 0.0000; time: 0.39s
Val loss: 0.5438 score: 0.8636 time: 0.40s
Test loss: 0.6019 score: 0.7442 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.4453;  Loss pred: 0.4453; Loss self: 0.0000; time: 0.85s
Val loss: 0.5349 score: 0.8636 time: 0.24s
Test loss: 0.5965 score: 0.7442 time: 0.45s
Epoch 66/1000, LR 0.000268
Train loss: 0.4315;  Loss pred: 0.4315; Loss self: 0.0000; time: 0.81s
Val loss: 0.5259 score: 0.8636 time: 0.43s
Test loss: 0.5911 score: 0.7442 time: 0.50s
Epoch 67/1000, LR 0.000268
Train loss: 0.4179;  Loss pred: 0.4179; Loss self: 0.0000; time: 0.86s
Val loss: 0.5167 score: 0.8636 time: 0.58s
Test loss: 0.5855 score: 0.7442 time: 0.47s
Epoch 68/1000, LR 0.000268
Train loss: 0.3978;  Loss pred: 0.3978; Loss self: 0.0000; time: 0.87s
Val loss: 0.5073 score: 0.8636 time: 0.65s
Test loss: 0.5797 score: 0.7442 time: 0.28s
Epoch 69/1000, LR 0.000268
Train loss: 0.3934;  Loss pred: 0.3934; Loss self: 0.0000; time: 0.78s
Val loss: 0.4982 score: 0.8636 time: 0.48s
Test loss: 0.5744 score: 0.7442 time: 0.29s
Epoch 70/1000, LR 0.000268
Train loss: 0.3746;  Loss pred: 0.3746; Loss self: 0.0000; time: 1.57s
Val loss: 0.4891 score: 0.8636 time: 0.71s
Test loss: 0.5692 score: 0.7442 time: 0.49s
Epoch 71/1000, LR 0.000268
Train loss: 0.3513;  Loss pred: 0.3513; Loss self: 0.0000; time: 0.96s
Val loss: 0.4799 score: 0.8636 time: 0.43s
Test loss: 0.5639 score: 0.7907 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.3390;  Loss pred: 0.3390; Loss self: 0.0000; time: 1.11s
Val loss: 0.4709 score: 0.8409 time: 0.44s
Test loss: 0.5589 score: 0.7907 time: 0.47s
Epoch 73/1000, LR 0.000267
Train loss: 0.3224;  Loss pred: 0.3224; Loss self: 0.0000; time: 0.57s
Val loss: 0.4622 score: 0.8409 time: 0.71s
Test loss: 0.5542 score: 0.7907 time: 0.73s
Epoch 74/1000, LR 0.000267
Train loss: 0.3203;  Loss pred: 0.3203; Loss self: 0.0000; time: 1.09s
Val loss: 0.4540 score: 0.8409 time: 0.59s
Test loss: 0.5505 score: 0.7907 time: 0.47s
Epoch 75/1000, LR 0.000267
Train loss: 0.3021;  Loss pred: 0.3021; Loss self: 0.0000; time: 0.86s
Val loss: 0.4467 score: 0.8182 time: 0.33s
Test loss: 0.5477 score: 0.7907 time: 0.31s
Epoch 76/1000, LR 0.000267
Train loss: 0.2850;  Loss pred: 0.2850; Loss self: 0.0000; time: 0.62s
Val loss: 0.4400 score: 0.8182 time: 0.32s
Test loss: 0.5457 score: 0.8140 time: 0.31s
Epoch 77/1000, LR 0.000267
Train loss: 0.2786;  Loss pred: 0.2786; Loss self: 0.0000; time: 0.43s
Val loss: 0.4344 score: 0.8182 time: 0.54s
Test loss: 0.5448 score: 0.7907 time: 0.39s
Epoch 78/1000, LR 0.000267
Train loss: 0.2505;  Loss pred: 0.2505; Loss self: 0.0000; time: 0.50s
Val loss: 0.4298 score: 0.8182 time: 0.29s
Test loss: 0.5453 score: 0.7674 time: 0.77s
Epoch 79/1000, LR 0.000267
Train loss: 0.2530;  Loss pred: 0.2530; Loss self: 0.0000; time: 0.71s
Val loss: 0.4255 score: 0.8182 time: 0.53s
Test loss: 0.5466 score: 0.7674 time: 0.27s
Epoch 80/1000, LR 0.000267
Train loss: 0.2397;  Loss pred: 0.2397; Loss self: 0.0000; time: 1.27s
Val loss: 0.4226 score: 0.8182 time: 0.28s
Test loss: 0.5494 score: 0.7674 time: 0.52s
Epoch 81/1000, LR 0.000267
Train loss: 0.2234;  Loss pred: 0.2234; Loss self: 0.0000; time: 0.61s
Val loss: 0.4205 score: 0.8182 time: 0.27s
Test loss: 0.5529 score: 0.7674 time: 0.53s
Epoch 82/1000, LR 0.000267
Train loss: 0.2125;  Loss pred: 0.2125; Loss self: 0.0000; time: 0.86s
Val loss: 0.4182 score: 0.8182 time: 0.21s
Test loss: 0.5560 score: 0.7674 time: 0.40s
Epoch 83/1000, LR 0.000266
Train loss: 0.2025;  Loss pred: 0.2025; Loss self: 0.0000; time: 1.02s
Val loss: 0.4165 score: 0.8182 time: 0.68s
Test loss: 0.5596 score: 0.7674 time: 0.57s
Epoch 84/1000, LR 0.000266
Train loss: 0.2037;  Loss pred: 0.2037; Loss self: 0.0000; time: 0.38s
Val loss: 0.4167 score: 0.8182 time: 0.66s
Test loss: 0.5656 score: 0.7674 time: 0.51s
     INFO: Early stopping counter 1 of 2
Epoch 85/1000, LR 0.000266
Train loss: 0.1847;  Loss pred: 0.1847; Loss self: 0.0000; time: 1.10s
Val loss: 0.4171 score: 0.8182 time: 0.45s
Test loss: 0.5714 score: 0.7674 time: 0.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 082,   Train_Loss: 0.2025,   Val_Loss: 0.4165,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4165,   Test_Precision: 0.8750,   Test_Recall: 0.6364,   Test_accuracy: 0.7368,   Test_Score: 0.7674,   Test_loss: 0.5596


[0.4750008130213246, 0.2631252839928493, 0.5170289130182937, 0.9000069790054113, 0.5307258020620793, 0.46501301997341216, 0.43520022893790156, 0.4975579329766333, 0.7342285390477628, 0.4409626049455255, 0.4422386040678248, 0.2875031860312447, 0.2955530929611996, 0.25926076201722026, 0.3586061700480059, 0.4599633930483833, 0.5854537429986522, 0.2947639930061996, 0.4970294889062643, 0.35809657094068825, 0.4542290640529245, 0.6925757750868797, 0.46469670499209315, 0.25840423500631005, 0.27010509092360735, 0.4555436830269173, 0.5855305090080947, 0.38917365903034806, 0.35105917998589575, 0.4591894129989669, 0.561283832998015, 0.47328772104810923, 0.6918625149410218, 0.23674789804499596, 4.049542690976523, 4.381657504942268, 0.23985040606930852, 0.24084969598334283, 0.2489144039573148, 0.2323405840434134, 0.24973722407594323, 0.32212266500573605, 0.23332794406451285, 0.2700109879951924, 0.24460555706173182, 0.2617763429880142, 0.24219672102481127, 0.2604445000179112, 0.23085203801747411, 0.25202304404228926, 0.25423878396395594, 0.28004125703591853, 0.23174647602718323, 0.24742666503880173, 0.25231824594084173, 0.34323251398745924, 0.26408101397100836, 0.2390072689158842, 0.25084779295139015, 0.2626024349592626, 0.2386522040469572, 0.25029452599119395, 0.24000028800219297, 0.26140701200347394, 0.23955019598361105, 0.3781608260469511, 0.2564388749888167, 0.2763956020353362, 0.7027518569957465, 0.5231539070373401, 0.3700473939534277, 0.5013921608915552, 0.5895882210461423, 0.4571878139395267, 0.5219456870108843, 0.42018577898852527, 0.283888011937961, 0.47732575493864715, 0.7026161570101976, 0.26074565399903804, 0.8304567459272221, 0.23892625991720706, 0.4987050120253116, 0.41825154493562877, 0.3084625320043415, 0.2475069990614429, 0.485217617941089, 0.47498324990738183, 0.46161280502565205, 0.2638095919974148, 0.2522637950023636, 0.5006253729807213, 0.3443668039981276, 0.29157105600461364, 0.5731961550191045, 0.25761703005991876, 0.5100079810945317, 0.2777699630241841, 0.3548992599826306, 0.37485217407811433, 0.279676869045943, 0.47779211692977697, 0.25613230909220874, 0.48258002707734704, 0.2692728000693023, 0.5478678409708664, 0.28623532108031213, 0.5200734839309007, 0.2581489080330357, 0.38372141506988555, 0.41298466303851455, 0.6024502790533006, 0.5403662510216236, 0.3395681600086391, 0.45070455002132803, 0.5022960200440139, 0.44063376099802554, 0.2589306739391759, 0.47522214194759727, 0.24705794895999134, 0.2995660360902548, 0.4889666100498289, 0.294887377996929, 0.2878318449947983, 0.4990983339957893, 0.2753788339905441, 0.35171555809210986, 0.2763076809933409, 0.4357403010362759, 0.45265890494920313, 0.6195968230022117, 0.6500141179421917, 0.29524628398939967, 0.2682188630569726, 0.2786319099832326, 0.2855490179499611, 0.32306473900098354, 0.30153034802060574, 0.26984322094358504, 0.28395971504505724, 0.2648513849126175, 0.6304603309836239, 1.0070553009863943, 0.8700764809036627, 0.2970611499622464, 0.27682790299877524, 0.2452537550125271, 3.6791571530047804, 1.092883724020794, 0.3270615900401026, 0.2598422080045566, 0.24557819601614028, 0.25369203404989094, 0.4504037769511342, 0.5085467799799517, 0.4795391799416393, 0.28135611000470817, 0.2924496670020744, 0.49669201392680407, 0.2460119309835136, 0.4726318040629849, 0.7347450830275193, 0.47690594801679254, 0.31619797204621136, 0.3120616839732975, 0.39758932206314057, 0.7766735190525651, 0.27102728688623756, 0.5206884699873626, 0.5375556800281629, 0.401732912985608, 0.5743515769718215, 0.5181871610693634, 0.30763734003994614]
[0.010795473023211922, 0.0059801200907465745, 0.011750657114052128, 0.0204547040683048, 0.012061950046865439, 0.010568477726668458, 0.009890914294043218, 0.01130813484037803, 0.016687012251085518, 0.010021877385125581, 0.010050877365177836, 0.006534163318891925, 0.006717115749118172, 0.005892290045845915, 0.00815014022836377, 0.010453713478372347, 0.013305766886333004, 0.006699181659231809, 0.011296124747869644, 0.008138558430470188, 0.010323387819384649, 0.015740358524701813, 0.010561288749820298, 0.005872823522870683, 0.0061387520664456215, 0.010353265523339029, 0.013307511568365788, 0.008844855887053365, 0.007978617726952176, 0.010436123022703792, 0.012756450749954885, 0.010756539114729756, 0.015724148066841404, 0.005380634046477181, 0.09203506115855734, 0.09958312511232427, 0.005451145592484285, 0.005473856726894155, 0.005657145544484427, 0.005280467819168486, 0.005675846001725982, 0.007320969659221274, 0.0053029078196480195, 0.0061366133635271, 0.00555921720594845, 0.005949462340636687, 0.005504470932382074, 0.0059191931822252545, 0.005246637227669867, 0.005727796455506574, 0.0057781541809989985, 0.006364574023543603, 0.005266965364254164, 0.005623333296336403, 0.005734505589564585, 0.007800738954260437, 0.006001841226613827, 0.0054319833844519135, 0.0057010862034406855, 0.0059682371581650595, 0.005423913728339936, 0.0056885119543453174, 0.00545455200004984, 0.0059410684546244074, 0.0054443226359911605, 0.008594564228339797, 0.0058281562497458335, 0.0062817182280758225, 0.015971633113539694, 0.011889861523575912, 0.008410168044396083, 0.011395276383898983, 0.013399732296503234, 0.010390632134989242, 0.011862401977520098, 0.009549676795193756, 0.006452000271317295, 0.01084831261224198, 0.015968549022959036, 0.005926037590887228, 0.018874016952891412, 0.005430142270845615, 0.011334204818757082, 0.0095057169303552, 0.007010512091007761, 0.005625159069578248, 0.01102767313502475, 0.010795073861531406, 0.010491200114219364, 0.006135106790637554, 0.005866599883775898, 0.011642450534435378, 0.00800853032553785, 0.006780722232665433, 0.013330143139979173, 0.0059910937223236925, 0.011860650723128644, 0.00645976658195777, 0.008253471162386759, 0.008717492420421264, 0.006504113233626582, 0.011111444579762255, 0.0059565653277257845, 0.011222791327380164, 0.006262158141146565, 0.012741112580717823, 0.006656635373960747, 0.01209473218443955, 0.006003462977512458, 0.008923753838834548, 0.00960429448926778, 0.014010471605890713, 0.012566657000502874, 0.007896933953689282, 0.010481501163286699, 0.011681302791721252, 0.010247296767395943, 0.006021643579980835, 0.011051677719711564, 0.005745533696743984, 0.006966652002098949, 0.011371316512786719, 0.006857845999928581, 0.006693763837088333, 0.011606937999902077, 0.006404158930012653, 0.008179431583537438, 0.006425760023100952, 0.01013349537293665, 0.010526951277888445, 0.0144092284419119, 0.015116607394004456, 0.0068661926509162715, 0.006237647978069131, 0.006479811860075177, 0.006640674836045608, 0.007513133465139152, 0.007012333674897808, 0.006275423742874071, 0.006603714303373424, 0.00615933453285157, 0.014661868162409858, 0.02341989072061382, 0.02023433676520146, 0.006908398836331312, 0.0064378582092738425, 0.005703575697965747, 0.08556179425592512, 0.025415900558623118, 0.007606083489304712, 0.006042842046617595, 0.005711120837584658, 0.005899814745346301, 0.01047450644072405, 0.011826669301859341, 0.011152073952131147, 0.0065431653489467015, 0.00680115504655987, 0.01155097706806521, 0.005721207697291014, 0.010991437303790347, 0.017087094954128357, 0.011090836000390524, 0.007353441210377009, 0.00725724846449529, 0.009246263303793967, 0.018062174861687562, 0.006302960160145059, 0.01210903418575262, 0.012501294884375881, 0.009342625883386232, 0.013357013417949338, 0.012050864210915427, 0.007154356745115027]
[92.63142039722082, 167.22072213020678, 85.10162370444296, 48.88850978536184, 82.90533422163125, 94.62100653120588, 101.10288799107762, 88.43191331865744, 59.92684519872324, 99.78170372391453, 99.49380175153559, 153.04178227513017, 148.8734208772985, 169.7133019962253, 122.69727538182015, 95.65978655038677, 75.15538251516703, 149.27196348257698, 88.52593454127636, 122.87188309124508, 96.86742545138709, 63.530954420807525, 94.68541422248445, 170.27584706158376, 162.89955827764953, 96.58788309309102, 75.14552926462747, 113.06006709094575, 125.33499338136595, 95.82102451499463, 78.39171095483096, 92.96670512085278, 63.59645023368668, 185.8517028592052, 10.865424409043497, 10.041862000936957, 183.44767774662643, 182.6865498848005, 176.76759279686107, 189.37716017696889, 176.1851889032767, 136.59392765552988, 188.57578408111476, 162.95633124672477, 179.88144066937772, 168.08241530830233, 181.670502448679, 168.9419434734619, 190.59827401943693, 174.58720954349252, 173.0656484190783, 157.11970609515043, 189.8626497122611, 177.83046945687875, 174.38294973846718, 128.1929834934217, 166.61553717311332, 184.09481937340274, 175.40517092979334, 167.55366341833692, 184.36871419525028, 175.7928976902517, 183.33311333192214, 168.3198918910992, 183.67757880277537, 116.35261235265314, 171.58084943992537, 159.19211331870164, 62.61100495429401, 84.10526884750858, 118.90368833549364, 87.7556600042589, 74.62835658746391, 96.24053541772656, 84.29995897079317, 104.71558581996081, 154.99069404035092, 92.18023445153409, 62.62309734981143, 168.7468202256684, 52.982891903506776, 184.15723753114003, 88.22850971821954, 105.19985050329424, 142.64293207377523, 177.77275053574192, 90.680961228704, 92.63484556261652, 95.31797974615306, 162.99634776138606, 170.45648583696723, 85.89257021468606, 124.8668556340692, 147.4769155389676, 75.01794913220732, 166.91443104517856, 84.3124060680725, 154.804355128406, 121.1611430300094, 114.71188637427872, 153.74886077166545, 89.99729898498907, 167.8819831531672, 89.10439219878455, 159.6893558834504, 78.48608146775051, 150.2260442132333, 82.68062365916192, 166.57052833435665, 112.06046447047683, 104.12008931186352, 71.37518479960013, 79.57565802583642, 126.63142503969165, 95.40618127322028, 85.60688973054596, 97.58671215434326, 166.06761704138967, 90.48399938557915, 174.0482351651168, 143.54097200473268, 87.94056509424645, 145.8183808750465, 149.39278174997312, 86.15536673052243, 156.14852956155858, 122.25788427800752, 155.62361439035172, 98.68263251697758, 94.99426506328294, 69.3999685015275, 66.15240932939885, 145.64112177460578, 160.31683793569107, 154.3254683305571, 150.58710517973208, 133.10025765414494, 142.60587792331106, 159.3517889744943, 151.42993080260342, 162.35520163199072, 68.20413257867119, 42.69874748475302, 49.420942806476205, 144.75134161927562, 155.3311625533292, 175.32861014830797, 11.687459440236672, 39.34544824384433, 131.47370803990637, 165.48504698376775, 175.09697806060063, 169.49684747114938, 95.46989212895791, 84.55465985193177, 89.66941972339605, 152.83122872035887, 147.03384839106346, 86.57276298856856, 174.78827074806233, 90.9799121226079, 58.52369888998559, 90.1645286220794, 135.9907520017734, 137.79327039611638, 108.15179788246736, 55.364318397843775, 158.65561174306796, 82.58296943092216, 79.99171359838891, 107.03628856404023, 74.86703566952971, 82.98160053070886, 139.7749700813839]
Elapsed: 0.4599766754792821~0.5018858134329138
Time per graph: 0.010569727265469193~0.011483650561167822
Speed: 123.26569074403751~43.66978191948709
Total Time: 0.3082
best val loss: 0.41647225618362427 test_score: 0.7674

Testing...
Test loss: 0.6757 score: 0.6512 time: 0.27s
test Score 0.6512
Epoch Time List: [1.7608320949366316, 1.384632449131459, 2.5586590870516375, 1.9334400609368458, 1.668121075956151, 1.7001923019997776, 1.4822224030504003, 1.6038846380542964, 1.74325524806045, 1.553041424951516, 1.722251403843984, 1.4221997618442401, 1.5858034319244325, 1.4872932969592512, 1.179690942983143, 1.5142534569604322, 1.8968538669869304, 1.8146002978319302, 1.6651129788951948, 1.3135791310342029, 1.2570316239725798, 1.49937800609041, 1.3318269540322945, 1.578525546938181, 1.7837930389214307, 1.7506346091395244, 2.175501978956163, 1.398768470971845, 1.2535952089820057, 1.625775224994868, 1.5064678319031373, 1.7206453839316964, 1.4639345599571243, 1.363475403864868, 4.66681337216869, 11.03550478105899, 6.498498068889603, 0.8284382539568469, 0.806134590995498, 0.8161564629990608, 0.8666771681746468, 0.89684079203289, 0.8391823279671371, 0.8783365390263498, 0.7845741219352931, 0.8307436329778284, 0.7778650788823143, 0.8395597640192136, 0.8656018481124192, 0.8312685569981113, 0.7800973849371076, 0.853192865033634, 0.7989277560263872, 0.8794622650602832, 0.8569967179792002, 0.9892935970565304, 0.8522592339431867, 0.8321516149444506, 0.977019413956441, 0.8756983180064708, 0.9018809418193996, 0.8402820390183479, 0.8291244428837672, 0.8405553179327399, 0.8278652881272137, 0.9822619151091203, 0.8301656959811226, 0.9239286199444905, 1.6627276620129123, 1.518990293261595, 1.5028761230641976, 1.5417230220045894, 1.7296291328966618, 2.043984869029373, 1.9453048270661384, 1.7824599930318072, 1.7072918640915304, 1.7410500210244209, 1.7237746769096702, 1.6811653848271817, 2.511383901932277, 1.4987384889973328, 2.1083124038996175, 1.746965229860507, 1.6737301180837676, 1.2986861079698429, 1.5426038600271568, 1.7903364651137963, 1.3223777728853747, 1.5199186372337863, 1.7571831850800663, 1.6403144089272246, 1.3944305701879784, 1.5300017190165818, 2.0935234752250835, 1.8079064378980547, 1.3709902859991416, 1.5054494622163475, 1.6910966909490526, 1.3145077250665054, 1.4687776959035546, 1.9406383509049192, 1.7375218420056626, 11.05798359401524, 10.691695747082122, 1.646164235076867, 1.5539003351004794, 1.8125635508913547, 1.0234172919299453, 1.4163324479013681, 1.720123720006086, 1.6381416958756745, 1.8010562030831352, 1.299538736930117, 1.7590198400430381, 1.303307669935748, 1.2066239179112017, 1.0704307279083878, 1.7045821601059288, 1.3083907309919596, 1.3484043231001124, 1.5603779039811343, 1.4574184188386425, 1.5668061199830845, 1.5131331039592624, 1.6667594929458573, 1.5082179259043187, 1.7938192919827998, 1.2379229189828038, 1.2740639069816098, 2.109576179878786, 1.9984533799579367, 1.2078591320896521, 1.5106919730314985, 1.139168035122566, 1.1733515600208193, 1.421758740907535, 1.175800762954168, 1.0664344870019704, 1.1741143540712073, 2.6362115959636867, 1.7803850980708376, 2.3756437009433284, 2.3139347921824083, 1.2687527399975806, 1.7697845300426707, 1.7575347470119596, 7.83408132311888, 4.134532475844026, 9.963118942920119, 1.3780607777880505, 1.2345508278813213, 1.041175740887411, 1.5438650000141934, 1.7470886398805305, 1.917313130106777, 1.8021632758900523, 1.5493450060021132, 2.773123527993448, 1.6258702740306035, 2.020623918971978, 2.007310903049074, 2.1435361900366843, 1.4910798149649054, 1.2455647990573198, 1.3618635119637474, 1.5635108830174431, 1.5069386478280649, 2.0643318539950997, 1.409142081043683, 1.4701070218579844, 2.2621027069399133, 1.5536694780457765, 1.8569374119397253]
Total Epoch List: [89, 85]
Total Time List: [0.4620678350329399, 0.3082298790104687]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335ab0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.42s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.93s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.26s
Epoch 4/1000, LR 0.000060
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.84s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.42s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 1.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.50s
Epoch 6/1000, LR 0.000120
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.67s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.42s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.59s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.71s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.43s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.90s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.51s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.65s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.91s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.43s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.75s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 10.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.28s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 10.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 2.89s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 3.23s
Epoch 18/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 3.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 1.04s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.21s
Epoch 23/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.22s
Epoch 24/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.21s
Epoch 27/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6883;  Loss pred: 0.6883; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.4884 time: 0.29s
Epoch 29/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.4884 time: 0.21s
Epoch 30/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6879 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4884 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4884 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.4884 time: 0.19s
Epoch 34/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.4884 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.5000 time: 0.24s
Test loss: 0.6854 score: 0.5116 time: 0.20s
Epoch 36/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5000 time: 0.22s
Test loss: 0.6845 score: 0.5116 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6813;  Loss pred: 0.6813; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5000 time: 0.27s
Test loss: 0.6835 score: 0.5349 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.39s
Val loss: 0.6810 score: 0.5455 time: 0.22s
Test loss: 0.6824 score: 0.5349 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.35s
Val loss: 0.6798 score: 0.5455 time: 0.23s
Test loss: 0.6812 score: 0.5581 time: 0.20s
Epoch 40/1000, LR 0.000269
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.39s
Val loss: 0.6785 score: 0.5455 time: 0.22s
Test loss: 0.6799 score: 0.5581 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.34s
Val loss: 0.6770 score: 0.5455 time: 0.24s
Test loss: 0.6784 score: 0.5581 time: 0.21s
Epoch 42/1000, LR 0.000269
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.38s
Val loss: 0.6754 score: 0.5227 time: 0.21s
Test loss: 0.6768 score: 0.5581 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6712;  Loss pred: 0.6712; Loss self: 0.0000; time: 0.34s
Val loss: 0.6736 score: 0.5227 time: 0.24s
Test loss: 0.6750 score: 0.5349 time: 0.28s
Epoch 44/1000, LR 0.000269
Train loss: 0.6674;  Loss pred: 0.6674; Loss self: 0.0000; time: 0.47s
Val loss: 0.6716 score: 0.5227 time: 0.24s
Test loss: 0.6731 score: 0.5116 time: 0.20s
Epoch 45/1000, LR 0.000269
Train loss: 0.6647;  Loss pred: 0.6647; Loss self: 0.0000; time: 0.42s
Val loss: 0.6694 score: 0.5227 time: 0.23s
Test loss: 0.6709 score: 0.5349 time: 0.23s
Epoch 46/1000, LR 0.000269
Train loss: 0.6611;  Loss pred: 0.6611; Loss self: 0.0000; time: 0.37s
Val loss: 0.6671 score: 0.5227 time: 0.24s
Test loss: 0.6686 score: 0.5349 time: 0.22s
Epoch 47/1000, LR 0.000269
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.40s
Val loss: 0.6646 score: 0.5909 time: 0.21s
Test loss: 0.6660 score: 0.5349 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6542;  Loss pred: 0.6542; Loss self: 0.0000; time: 0.34s
Val loss: 0.6619 score: 0.5909 time: 0.24s
Test loss: 0.6632 score: 0.5581 time: 0.21s
Epoch 49/1000, LR 0.000269
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.39s
Val loss: 0.6589 score: 0.5909 time: 0.21s
Test loss: 0.6602 score: 0.5581 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.34s
Val loss: 0.6557 score: 0.5909 time: 0.24s
Test loss: 0.6569 score: 0.5581 time: 0.20s
Epoch 51/1000, LR 0.000269
Train loss: 0.6423;  Loss pred: 0.6423; Loss self: 0.0000; time: 0.48s
Val loss: 0.6522 score: 0.6364 time: 0.23s
Test loss: 0.6533 score: 0.6047 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6387;  Loss pred: 0.6387; Loss self: 0.0000; time: 0.47s
Val loss: 0.6485 score: 0.6364 time: 0.23s
Test loss: 0.6496 score: 0.6279 time: 0.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6318;  Loss pred: 0.6318; Loss self: 0.0000; time: 0.38s
Val loss: 0.6445 score: 0.6818 time: 0.24s
Test loss: 0.6456 score: 0.6279 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.40s
Val loss: 0.6402 score: 0.7045 time: 0.22s
Test loss: 0.6413 score: 0.6512 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.34s
Val loss: 0.6355 score: 0.6818 time: 0.24s
Test loss: 0.6367 score: 0.6977 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6113;  Loss pred: 0.6113; Loss self: 0.0000; time: 0.38s
Val loss: 0.6305 score: 0.6818 time: 0.21s
Test loss: 0.6318 score: 0.6977 time: 0.23s
Epoch 57/1000, LR 0.000269
Train loss: 0.6068;  Loss pred: 0.6068; Loss self: 0.0000; time: 0.34s
Val loss: 0.6251 score: 0.6818 time: 0.40s
Test loss: 0.6265 score: 0.7209 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 0.38s
Val loss: 0.6194 score: 0.6818 time: 0.24s
Test loss: 0.6208 score: 0.7209 time: 0.20s
Epoch 59/1000, LR 0.000268
Train loss: 0.5873;  Loss pred: 0.5873; Loss self: 0.0000; time: 0.50s
Val loss: 0.6132 score: 0.7273 time: 0.22s
Test loss: 0.6148 score: 0.7209 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.34s
Val loss: 0.6068 score: 0.7727 time: 0.29s
Test loss: 0.6084 score: 0.7674 time: 0.21s
Epoch 61/1000, LR 0.000268
Train loss: 0.5702;  Loss pred: 0.5702; Loss self: 0.0000; time: 0.45s
Val loss: 0.5999 score: 0.7727 time: 0.27s
Test loss: 0.6016 score: 0.7674 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.5624;  Loss pred: 0.5624; Loss self: 0.0000; time: 0.42s
Val loss: 0.5927 score: 0.7727 time: 0.22s
Test loss: 0.5946 score: 0.7674 time: 0.29s
Epoch 63/1000, LR 0.000268
Train loss: 0.5515;  Loss pred: 0.5515; Loss self: 0.0000; time: 0.37s
Val loss: 0.5852 score: 0.7727 time: 0.23s
Test loss: 0.5873 score: 0.7674 time: 0.20s
Epoch 64/1000, LR 0.000268
Train loss: 0.5441;  Loss pred: 0.5441; Loss self: 0.0000; time: 0.45s
Val loss: 0.5774 score: 0.7727 time: 0.22s
Test loss: 0.5796 score: 0.7674 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.5311;  Loss pred: 0.5311; Loss self: 0.0000; time: 0.38s
Val loss: 0.5692 score: 0.7727 time: 0.24s
Test loss: 0.5717 score: 0.7674 time: 0.21s
Epoch 66/1000, LR 0.000268
Train loss: 0.5151;  Loss pred: 0.5151; Loss self: 0.0000; time: 0.55s
Val loss: 0.5606 score: 0.7955 time: 0.23s
Test loss: 0.5633 score: 0.7674 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.5028;  Loss pred: 0.5028; Loss self: 0.0000; time: 0.41s
Val loss: 0.5519 score: 0.7955 time: 0.22s
Test loss: 0.5550 score: 0.7674 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.4988;  Loss pred: 0.4988; Loss self: 0.0000; time: 0.39s
Val loss: 0.5430 score: 0.7955 time: 0.28s
Test loss: 0.5467 score: 0.7674 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.4825;  Loss pred: 0.4825; Loss self: 0.0000; time: 0.40s
Val loss: 0.5338 score: 0.8182 time: 0.21s
Test loss: 0.5383 score: 0.7674 time: 0.23s
Epoch 70/1000, LR 0.000268
Train loss: 0.4646;  Loss pred: 0.4646; Loss self: 0.0000; time: 0.34s
Val loss: 0.5246 score: 0.8182 time: 0.34s
Test loss: 0.5300 score: 0.7674 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.4479;  Loss pred: 0.4479; Loss self: 0.0000; time: 0.51s
Val loss: 0.5153 score: 0.8182 time: 0.24s
Test loss: 0.5219 score: 0.7674 time: 0.21s
Epoch 72/1000, LR 0.000267
Train loss: 0.4510;  Loss pred: 0.4510; Loss self: 0.0000; time: 0.40s
Val loss: 0.5061 score: 0.8182 time: 0.21s
Test loss: 0.5140 score: 0.7674 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.4207;  Loss pred: 0.4207; Loss self: 0.0000; time: 0.36s
Val loss: 0.4967 score: 0.8182 time: 0.24s
Test loss: 0.5060 score: 0.7674 time: 0.31s
Epoch 74/1000, LR 0.000267
Train loss: 0.4248;  Loss pred: 0.4248; Loss self: 0.0000; time: 0.37s
Val loss: 0.4871 score: 0.8182 time: 0.21s
Test loss: 0.4974 score: 0.7674 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.3928;  Loss pred: 0.3928; Loss self: 0.0000; time: 0.35s
Val loss: 0.4775 score: 0.8182 time: 0.24s
Test loss: 0.4887 score: 0.7674 time: 0.20s
Epoch 76/1000, LR 0.000267
Train loss: 0.3935;  Loss pred: 0.3935; Loss self: 0.0000; time: 0.38s
Val loss: 0.4679 score: 0.8182 time: 0.21s
Test loss: 0.4800 score: 0.7674 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.3685;  Loss pred: 0.3685; Loss self: 0.0000; time: 0.33s
Val loss: 0.4583 score: 0.8409 time: 0.25s
Test loss: 0.4716 score: 0.7674 time: 0.21s
Epoch 78/1000, LR 0.000267
Train loss: 0.3524;  Loss pred: 0.3524; Loss self: 0.0000; time: 0.37s
Val loss: 0.4487 score: 0.8636 time: 0.21s
Test loss: 0.4635 score: 0.7674 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.3402;  Loss pred: 0.3402; Loss self: 0.0000; time: 0.34s
Val loss: 0.4392 score: 0.8636 time: 0.23s
Test loss: 0.4559 score: 0.7674 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.38s
Val loss: 0.4297 score: 0.8636 time: 0.21s
Test loss: 0.4481 score: 0.7674 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.3238;  Loss pred: 0.3238; Loss self: 0.0000; time: 0.34s
Val loss: 0.4205 score: 0.8636 time: 0.23s
Test loss: 0.4409 score: 0.7907 time: 0.29s
Epoch 82/1000, LR 0.000267
Train loss: 0.2981;  Loss pred: 0.2981; Loss self: 0.0000; time: 0.39s
Val loss: 0.4116 score: 0.8636 time: 0.22s
Test loss: 0.4347 score: 0.7907 time: 0.22s
Epoch 83/1000, LR 0.000266
Train loss: 0.3062;  Loss pred: 0.3062; Loss self: 0.0000; time: 0.33s
Val loss: 0.4031 score: 0.8636 time: 0.24s
Test loss: 0.4294 score: 0.7907 time: 0.20s
Epoch 84/1000, LR 0.000266
Train loss: 0.2870;  Loss pred: 0.2870; Loss self: 0.0000; time: 0.38s
Val loss: 0.3949 score: 0.8636 time: 0.21s
Test loss: 0.4257 score: 0.7907 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.2541;  Loss pred: 0.2541; Loss self: 0.0000; time: 0.33s
Val loss: 0.3870 score: 0.8636 time: 0.23s
Test loss: 0.4230 score: 0.7907 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.2544;  Loss pred: 0.2544; Loss self: 0.0000; time: 0.38s
Val loss: 0.3797 score: 0.8636 time: 0.21s
Test loss: 0.4220 score: 0.7907 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.2566;  Loss pred: 0.2566; Loss self: 0.0000; time: 0.39s
Val loss: 0.3714 score: 0.8636 time: 0.24s
Test loss: 0.4168 score: 0.7907 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.2904;  Loss pred: 0.2904; Loss self: 0.0000; time: 0.39s
Val loss: 0.3632 score: 0.8864 time: 0.21s
Test loss: 0.4083 score: 0.8140 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.2351;  Loss pred: 0.2351; Loss self: 0.0000; time: 0.42s
Val loss: 0.3564 score: 0.8864 time: 0.23s
Test loss: 0.3993 score: 0.8140 time: 0.20s
Epoch 90/1000, LR 0.000266
Train loss: 0.2000;  Loss pred: 0.2000; Loss self: 0.0000; time: 0.38s
Val loss: 0.3510 score: 0.9091 time: 0.21s
Test loss: 0.3912 score: 0.8372 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.2073;  Loss pred: 0.2073; Loss self: 0.0000; time: 0.33s
Val loss: 0.3452 score: 0.9091 time: 0.23s
Test loss: 0.3874 score: 0.8372 time: 0.20s
Epoch 92/1000, LR 0.000266
Train loss: 0.1743;  Loss pred: 0.1743; Loss self: 0.0000; time: 0.38s
Val loss: 0.3391 score: 0.9091 time: 0.21s
Test loss: 0.3853 score: 0.8372 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 0.33s
Val loss: 0.3335 score: 0.9091 time: 0.23s
Test loss: 0.3830 score: 0.8605 time: 0.22s
Epoch 94/1000, LR 0.000265
Train loss: 0.1604;  Loss pred: 0.1604; Loss self: 0.0000; time: 0.39s
Val loss: 0.3274 score: 0.9091 time: 0.21s
Test loss: 0.3833 score: 0.8140 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.1672;  Loss pred: 0.1672; Loss self: 0.0000; time: 0.35s
Val loss: 0.3213 score: 0.9091 time: 0.24s
Test loss: 0.3850 score: 0.8140 time: 0.21s
Epoch 96/1000, LR 0.000265
Train loss: 0.1432;  Loss pred: 0.1432; Loss self: 0.0000; time: 0.38s
Val loss: 0.3159 score: 0.9091 time: 0.21s
Test loss: 0.3859 score: 0.8140 time: 0.30s
Epoch 97/1000, LR 0.000265
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.34s
Val loss: 0.3101 score: 0.8864 time: 0.24s
Test loss: 0.3903 score: 0.8140 time: 0.22s
Epoch 98/1000, LR 0.000265
Train loss: 0.1239;  Loss pred: 0.1239; Loss self: 0.0000; time: 0.38s
Val loss: 0.3049 score: 0.8864 time: 0.23s
Test loss: 0.3959 score: 0.8140 time: 0.21s
Epoch 99/1000, LR 0.000265
Train loss: 0.1132;  Loss pred: 0.1132; Loss self: 0.0000; time: 0.37s
Val loss: 0.3004 score: 0.8864 time: 0.22s
Test loss: 0.4014 score: 0.8140 time: 0.21s
Epoch 100/1000, LR 0.000265
Train loss: 0.1701;  Loss pred: 0.1701; Loss self: 0.0000; time: 0.39s
Val loss: 0.2968 score: 0.8864 time: 0.31s
Test loss: 0.4045 score: 0.8140 time: 0.22s
Epoch 101/1000, LR 0.000265
Train loss: 0.1135;  Loss pred: 0.1135; Loss self: 0.0000; time: 0.44s
Val loss: 0.2935 score: 0.8864 time: 0.23s
Test loss: 0.4091 score: 0.8140 time: 0.27s
Epoch 102/1000, LR 0.000264
Train loss: 0.1238;  Loss pred: 0.1238; Loss self: 0.0000; time: 0.38s
Val loss: 0.2906 score: 0.8864 time: 0.23s
Test loss: 0.4116 score: 0.8140 time: 0.20s
Epoch 103/1000, LR 0.000264
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.41s
Val loss: 0.2881 score: 0.8864 time: 0.23s
Test loss: 0.4180 score: 0.8140 time: 0.21s
Epoch 104/1000, LR 0.000264
Train loss: 0.1109;  Loss pred: 0.1109; Loss self: 0.0000; time: 0.45s
Val loss: 0.2859 score: 0.8864 time: 0.24s
Test loss: 0.4176 score: 0.8140 time: 0.20s
Epoch 105/1000, LR 0.000264
Train loss: 0.1093;  Loss pred: 0.1093; Loss self: 0.0000; time: 0.38s
Val loss: 0.2847 score: 0.8864 time: 0.24s
Test loss: 0.4142 score: 0.8140 time: 0.22s
Epoch 106/1000, LR 0.000264
Train loss: 0.1265;  Loss pred: 0.1265; Loss self: 0.0000; time: 0.33s
Val loss: 0.2839 score: 0.8864 time: 0.24s
Test loss: 0.4140 score: 0.8140 time: 0.21s
Epoch 107/1000, LR 0.000264
Train loss: 0.0865;  Loss pred: 0.0865; Loss self: 0.0000; time: 0.38s
Val loss: 0.2828 score: 0.8864 time: 0.22s
Test loss: 0.4163 score: 0.8372 time: 0.23s
Epoch 108/1000, LR 0.000264
Train loss: 0.0710;  Loss pred: 0.0710; Loss self: 0.0000; time: 0.36s
Val loss: 0.2822 score: 0.9091 time: 0.24s
Test loss: 0.4180 score: 0.8372 time: 0.21s
Epoch 109/1000, LR 0.000264
Train loss: 0.0648;  Loss pred: 0.0648; Loss self: 0.0000; time: 0.39s
Val loss: 0.2811 score: 0.9091 time: 0.21s
Test loss: 0.4222 score: 0.8372 time: 0.23s
Epoch 110/1000, LR 0.000263
Train loss: 0.0633;  Loss pred: 0.0633; Loss self: 0.0000; time: 0.34s
Val loss: 0.2793 score: 0.8864 time: 0.31s
Test loss: 0.4296 score: 0.8372 time: 0.21s
Epoch 111/1000, LR 0.000263
Train loss: 0.1295;  Loss pred: 0.1295; Loss self: 0.0000; time: 0.38s
Val loss: 0.2789 score: 0.8864 time: 0.30s
Test loss: 0.4325 score: 0.8372 time: 0.22s
Epoch 112/1000, LR 0.000263
Train loss: 0.0616;  Loss pred: 0.0616; Loss self: 0.0000; time: 0.49s
Val loss: 0.2786 score: 0.8864 time: 0.23s
Test loss: 0.4353 score: 0.8372 time: 0.22s
Epoch 113/1000, LR 0.000263
Train loss: 0.0698;  Loss pred: 0.0698; Loss self: 0.0000; time: 0.38s
Val loss: 0.2769 score: 0.8864 time: 0.24s
Test loss: 0.4436 score: 0.8372 time: 0.21s
Epoch 114/1000, LR 0.000263
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.41s
Val loss: 0.2752 score: 0.8864 time: 0.22s
Test loss: 0.4533 score: 0.8140 time: 0.23s
Epoch 115/1000, LR 0.000263
Train loss: 0.0433;  Loss pred: 0.0433; Loss self: 0.0000; time: 0.36s
Val loss: 0.2743 score: 0.9091 time: 0.23s
Test loss: 0.4621 score: 0.8140 time: 0.22s
Epoch 116/1000, LR 0.000263
Train loss: 0.0476;  Loss pred: 0.0476; Loss self: 0.0000; time: 0.38s
Val loss: 0.2738 score: 0.9091 time: 0.22s
Test loss: 0.4708 score: 0.8140 time: 0.23s
Epoch 117/1000, LR 0.000262
Train loss: 0.0683;  Loss pred: 0.0683; Loss self: 0.0000; time: 0.34s
Val loss: 0.2736 score: 0.9091 time: 0.25s
Test loss: 0.4841 score: 0.8140 time: 0.21s
Epoch 118/1000, LR 0.000262
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.38s
Val loss: 0.2744 score: 0.8864 time: 0.22s
Test loss: 0.4991 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 119/1000, LR 0.000262
Train loss: 0.0440;  Loss pred: 0.0440; Loss self: 0.0000; time: 0.45s
Val loss: 0.2758 score: 0.8864 time: 0.28s
Test loss: 0.5123 score: 0.8140 time: 0.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 116,   Train_Loss: 0.0683,   Val_Loss: 0.2736,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2736,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4841


[0.4750008130213246, 0.2631252839928493, 0.5170289130182937, 0.9000069790054113, 0.5307258020620793, 0.46501301997341216, 0.43520022893790156, 0.4975579329766333, 0.7342285390477628, 0.4409626049455255, 0.4422386040678248, 0.2875031860312447, 0.2955530929611996, 0.25926076201722026, 0.3586061700480059, 0.4599633930483833, 0.5854537429986522, 0.2947639930061996, 0.4970294889062643, 0.35809657094068825, 0.4542290640529245, 0.6925757750868797, 0.46469670499209315, 0.25840423500631005, 0.27010509092360735, 0.4555436830269173, 0.5855305090080947, 0.38917365903034806, 0.35105917998589575, 0.4591894129989669, 0.561283832998015, 0.47328772104810923, 0.6918625149410218, 0.23674789804499596, 4.049542690976523, 4.381657504942268, 0.23985040606930852, 0.24084969598334283, 0.2489144039573148, 0.2323405840434134, 0.24973722407594323, 0.32212266500573605, 0.23332794406451285, 0.2700109879951924, 0.24460555706173182, 0.2617763429880142, 0.24219672102481127, 0.2604445000179112, 0.23085203801747411, 0.25202304404228926, 0.25423878396395594, 0.28004125703591853, 0.23174647602718323, 0.24742666503880173, 0.25231824594084173, 0.34323251398745924, 0.26408101397100836, 0.2390072689158842, 0.25084779295139015, 0.2626024349592626, 0.2386522040469572, 0.25029452599119395, 0.24000028800219297, 0.26140701200347394, 0.23955019598361105, 0.3781608260469511, 0.2564388749888167, 0.2763956020353362, 0.7027518569957465, 0.5231539070373401, 0.3700473939534277, 0.5013921608915552, 0.5895882210461423, 0.4571878139395267, 0.5219456870108843, 0.42018577898852527, 0.283888011937961, 0.47732575493864715, 0.7026161570101976, 0.26074565399903804, 0.8304567459272221, 0.23892625991720706, 0.4987050120253116, 0.41825154493562877, 0.3084625320043415, 0.2475069990614429, 0.485217617941089, 0.47498324990738183, 0.46161280502565205, 0.2638095919974148, 0.2522637950023636, 0.5006253729807213, 0.3443668039981276, 0.29157105600461364, 0.5731961550191045, 0.25761703005991876, 0.5100079810945317, 0.2777699630241841, 0.3548992599826306, 0.37485217407811433, 0.279676869045943, 0.47779211692977697, 0.25613230909220874, 0.48258002707734704, 0.2692728000693023, 0.5478678409708664, 0.28623532108031213, 0.5200734839309007, 0.2581489080330357, 0.38372141506988555, 0.41298466303851455, 0.6024502790533006, 0.5403662510216236, 0.3395681600086391, 0.45070455002132803, 0.5022960200440139, 0.44063376099802554, 0.2589306739391759, 0.47522214194759727, 0.24705794895999134, 0.2995660360902548, 0.4889666100498289, 0.294887377996929, 0.2878318449947983, 0.4990983339957893, 0.2753788339905441, 0.35171555809210986, 0.2763076809933409, 0.4357403010362759, 0.45265890494920313, 0.6195968230022117, 0.6500141179421917, 0.29524628398939967, 0.2682188630569726, 0.2786319099832326, 0.2855490179499611, 0.32306473900098354, 0.30153034802060574, 0.26984322094358504, 0.28395971504505724, 0.2648513849126175, 0.6304603309836239, 1.0070553009863943, 0.8700764809036627, 0.2970611499622464, 0.27682790299877524, 0.2452537550125271, 3.6791571530047804, 1.092883724020794, 0.3270615900401026, 0.2598422080045566, 0.24557819601614028, 0.25369203404989094, 0.4504037769511342, 0.5085467799799517, 0.4795391799416393, 0.28135611000470817, 0.2924496670020744, 0.49669201392680407, 0.2460119309835136, 0.4726318040629849, 0.7347450830275193, 0.47690594801679254, 0.31619797204621136, 0.3120616839732975, 0.39758932206314057, 0.7766735190525651, 0.27102728688623756, 0.5206884699873626, 0.5375556800281629, 0.401732912985608, 0.5743515769718215, 0.5181871610693634, 0.30763734003994614, 0.23805945401545614, 0.4209328000433743, 0.2617273699725047, 0.4228704069973901, 0.5050645179580897, 0.4296639069216326, 0.5990165319526568, 0.4381515010027215, 0.5104506859788671, 0.22754635906312615, 0.43403723707888275, 0.2379760229960084, 0.2848600000143051, 0.2318454950582236, 0.24771629401948303, 0.21371213695965707, 3.2349009671015665, 0.23035617091227323, 0.20554917003028095, 0.2250107959844172, 0.25455993798095733, 0.21245721296872944, 0.22572394693270326, 0.21096470998600125, 0.23594831698574126, 0.20987007208168507, 0.23120033298619092, 0.29346621804870665, 0.21434040600433946, 0.22613393003121018, 0.25262527097947896, 0.23653881205245852, 0.19692192692309618, 0.2247953520854935, 0.20261505199596286, 0.22722030791919678, 0.20737339998595417, 0.23273583897389472, 0.20845858205575496, 0.23158229293767363, 0.21567971701733768, 0.2312181870220229, 0.2849204719532281, 0.20237988699227571, 0.23042922094464302, 0.2244020669022575, 0.2360707769403234, 0.21009752294048667, 0.23621036298573017, 0.2086917640408501, 0.2194515890441835, 0.22327674902044237, 0.22049903799779713, 0.2315121500287205, 0.22503310500178486, 0.23808939394075423, 0.2396909479284659, 0.2055915220407769, 0.23727547097951174, 0.21610933891497552, 0.21656644495669752, 0.28988105594180524, 0.20264633500482887, 0.22538571804761887, 0.2161253779195249, 0.2139941060449928, 0.23646177095361054, 0.22040662204381078, 0.23123777995351702, 0.23154825798701495, 0.21693469502497464, 0.23172568599693477, 0.31198852602392435, 0.24740516603924334, 0.20803194807376713, 0.23424957995302975, 0.21842738101258874, 0.22523968294262886, 0.21705241408199072, 0.22075892507564276, 0.29601974599063396, 0.2204434679588303, 0.20196255098562688, 0.22812316101044416, 0.2172371350461617, 0.22075626289006323, 0.2217540240380913, 0.23366462800186127, 0.20627905090805143, 0.23261296609416604, 0.20464356895536184, 0.2317135330522433, 0.2195393299916759, 0.23022104101255536, 0.21046708803623915, 0.30764658097177744, 0.22356377507094294, 0.21564401302020997, 0.20939018996432424, 0.22711466194596142, 0.27564542298205197, 0.20907250698655844, 0.2190584319178015, 0.20181632798630744, 0.2243114310549572, 0.2160155869787559, 0.2319640989881009, 0.20936414203606546, 0.23374717601109296, 0.20969753293320537, 0.22463015199173242, 0.22400799300521612, 0.21850198600441217, 0.22987347503658384, 0.22268290701322258, 0.23390511609613895, 0.21916691097430885, 0.23232060100417584, 0.22323317197151482]
[0.010795473023211922, 0.0059801200907465745, 0.011750657114052128, 0.0204547040683048, 0.012061950046865439, 0.010568477726668458, 0.009890914294043218, 0.01130813484037803, 0.016687012251085518, 0.010021877385125581, 0.010050877365177836, 0.006534163318891925, 0.006717115749118172, 0.005892290045845915, 0.00815014022836377, 0.010453713478372347, 0.013305766886333004, 0.006699181659231809, 0.011296124747869644, 0.008138558430470188, 0.010323387819384649, 0.015740358524701813, 0.010561288749820298, 0.005872823522870683, 0.0061387520664456215, 0.010353265523339029, 0.013307511568365788, 0.008844855887053365, 0.007978617726952176, 0.010436123022703792, 0.012756450749954885, 0.010756539114729756, 0.015724148066841404, 0.005380634046477181, 0.09203506115855734, 0.09958312511232427, 0.005451145592484285, 0.005473856726894155, 0.005657145544484427, 0.005280467819168486, 0.005675846001725982, 0.007320969659221274, 0.0053029078196480195, 0.0061366133635271, 0.00555921720594845, 0.005949462340636687, 0.005504470932382074, 0.0059191931822252545, 0.005246637227669867, 0.005727796455506574, 0.0057781541809989985, 0.006364574023543603, 0.005266965364254164, 0.005623333296336403, 0.005734505589564585, 0.007800738954260437, 0.006001841226613827, 0.0054319833844519135, 0.0057010862034406855, 0.0059682371581650595, 0.005423913728339936, 0.0056885119543453174, 0.00545455200004984, 0.0059410684546244074, 0.0054443226359911605, 0.008594564228339797, 0.0058281562497458335, 0.0062817182280758225, 0.015971633113539694, 0.011889861523575912, 0.008410168044396083, 0.011395276383898983, 0.013399732296503234, 0.010390632134989242, 0.011862401977520098, 0.009549676795193756, 0.006452000271317295, 0.01084831261224198, 0.015968549022959036, 0.005926037590887228, 0.018874016952891412, 0.005430142270845615, 0.011334204818757082, 0.0095057169303552, 0.007010512091007761, 0.005625159069578248, 0.01102767313502475, 0.010795073861531406, 0.010491200114219364, 0.006135106790637554, 0.005866599883775898, 0.011642450534435378, 0.00800853032553785, 0.006780722232665433, 0.013330143139979173, 0.0059910937223236925, 0.011860650723128644, 0.00645976658195777, 0.008253471162386759, 0.008717492420421264, 0.006504113233626582, 0.011111444579762255, 0.0059565653277257845, 0.011222791327380164, 0.006262158141146565, 0.012741112580717823, 0.006656635373960747, 0.01209473218443955, 0.006003462977512458, 0.008923753838834548, 0.00960429448926778, 0.014010471605890713, 0.012566657000502874, 0.007896933953689282, 0.010481501163286699, 0.011681302791721252, 0.010247296767395943, 0.006021643579980835, 0.011051677719711564, 0.005745533696743984, 0.006966652002098949, 0.011371316512786719, 0.006857845999928581, 0.006693763837088333, 0.011606937999902077, 0.006404158930012653, 0.008179431583537438, 0.006425760023100952, 0.01013349537293665, 0.010526951277888445, 0.0144092284419119, 0.015116607394004456, 0.0068661926509162715, 0.006237647978069131, 0.006479811860075177, 0.006640674836045608, 0.007513133465139152, 0.007012333674897808, 0.006275423742874071, 0.006603714303373424, 0.00615933453285157, 0.014661868162409858, 0.02341989072061382, 0.02023433676520146, 0.006908398836331312, 0.0064378582092738425, 0.005703575697965747, 0.08556179425592512, 0.025415900558623118, 0.007606083489304712, 0.006042842046617595, 0.005711120837584658, 0.005899814745346301, 0.01047450644072405, 0.011826669301859341, 0.011152073952131147, 0.0065431653489467015, 0.00680115504655987, 0.01155097706806521, 0.005721207697291014, 0.010991437303790347, 0.017087094954128357, 0.011090836000390524, 0.007353441210377009, 0.00725724846449529, 0.009246263303793967, 0.018062174861687562, 0.006302960160145059, 0.01210903418575262, 0.012501294884375881, 0.009342625883386232, 0.013357013417949338, 0.012050864210915427, 0.007154356745115027, 0.005536266372452469, 0.009789134884729635, 0.006086683022616388, 0.009834195511567211, 0.011745686464141621, 0.009992183881898433, 0.01393061702215481, 0.010189569790760964, 0.011870946185555049, 0.005291775792165724, 0.010093889234392622, 0.005534326116186242, 0.006624651163123375, 0.005391755699028456, 0.005760844046964721, 0.004970049696736211, 0.07523025504887364, 0.005357120253773796, 0.004780213256518162, 0.0052328092089399345, 0.005919998557696682, 0.004940865417877429, 0.005249394114714029, 0.004906156046186076, 0.005487170162459099, 0.004880699350736862, 0.005376751929911417, 0.006824795768574574, 0.00498466060475208, 0.005258928605376981, 0.005875006301848348, 0.005500902605871129, 0.004579579695885958, 0.0052277988857091515, 0.004711977953394485, 0.0052841932074231815, 0.004822637208975678, 0.005412461371485924, 0.004847874001296627, 0.005385634719480782, 0.0050158073724962254, 0.005377167140047045, 0.006626057487284375, 0.004706508999820365, 0.005358819091735884, 0.005218652718657151, 0.005490018068379615, 0.004885988905592714, 0.005493264255482097, 0.004853296838159305, 0.005103525326608918, 0.005192482535359125, 0.005127884604599934, 0.005384003489040012, 0.005233328023297322, 0.005536962649784982, 0.005574208091359672, 0.0047811981869948115, 0.005518034208825854, 0.005025798579418036, 0.005036428952481338, 0.006741419905623378, 0.004712705465228578, 0.005241528326688811, 0.0050261715795238355, 0.004976607117325414, 0.005499110952409548, 0.005125735396367692, 0.005377622789616675, 0.005384843209000348, 0.00504499290755755, 0.005388969441789181, 0.00725554711683545, 0.0057536085125405425, 0.004837952280785282, 0.00544766465007046, 0.005079706535176482, 0.005238132161456485, 0.005047730560046296, 0.005133928490131227, 0.006884180139317069, 0.005126592278112333, 0.004696803511293648, 0.005305189790940562, 0.005052026396422366, 0.00513386657883868, 0.005157070326467239, 0.005434061116322355, 0.004797187230419801, 0.0054096038626550245, 0.004759152766403763, 0.0053886868151684485, 0.005105565813759905, 0.005353977697966404, 0.004894583442703236, 0.007154571650506452, 0.0051991575597893705, 0.005014977046981627, 0.004869539301495912, 0.005281736324324684, 0.006410358674001209, 0.004862151325268801, 0.005094382137623291, 0.004693402976425755, 0.005216544908254819, 0.0050236183018315325, 0.005394513929955835, 0.004868933535722452, 0.005435980837467278, 0.0048766868124001245, 0.0052239570230635446, 0.00520948820942363, 0.0050814415349863295, 0.005345894768292648, 0.005178672256121455, 0.0054396538627009055, 0.005096904906379276, 0.005402804674515717, 0.005191469115616624]
[92.63142039722082, 167.22072213020678, 85.10162370444296, 48.88850978536184, 82.90533422163125, 94.62100653120588, 101.10288799107762, 88.43191331865744, 59.92684519872324, 99.78170372391453, 99.49380175153559, 153.04178227513017, 148.8734208772985, 169.7133019962253, 122.69727538182015, 95.65978655038677, 75.15538251516703, 149.27196348257698, 88.52593454127636, 122.87188309124508, 96.86742545138709, 63.530954420807525, 94.68541422248445, 170.27584706158376, 162.89955827764953, 96.58788309309102, 75.14552926462747, 113.06006709094575, 125.33499338136595, 95.82102451499463, 78.39171095483096, 92.96670512085278, 63.59645023368668, 185.8517028592052, 10.865424409043497, 10.041862000936957, 183.44767774662643, 182.6865498848005, 176.76759279686107, 189.37716017696889, 176.1851889032767, 136.59392765552988, 188.57578408111476, 162.95633124672477, 179.88144066937772, 168.08241530830233, 181.670502448679, 168.9419434734619, 190.59827401943693, 174.58720954349252, 173.0656484190783, 157.11970609515043, 189.8626497122611, 177.83046945687875, 174.38294973846718, 128.1929834934217, 166.61553717311332, 184.09481937340274, 175.40517092979334, 167.55366341833692, 184.36871419525028, 175.7928976902517, 183.33311333192214, 168.3198918910992, 183.67757880277537, 116.35261235265314, 171.58084943992537, 159.19211331870164, 62.61100495429401, 84.10526884750858, 118.90368833549364, 87.7556600042589, 74.62835658746391, 96.24053541772656, 84.29995897079317, 104.71558581996081, 154.99069404035092, 92.18023445153409, 62.62309734981143, 168.7468202256684, 52.982891903506776, 184.15723753114003, 88.22850971821954, 105.19985050329424, 142.64293207377523, 177.77275053574192, 90.680961228704, 92.63484556261652, 95.31797974615306, 162.99634776138606, 170.45648583696723, 85.89257021468606, 124.8668556340692, 147.4769155389676, 75.01794913220732, 166.91443104517856, 84.3124060680725, 154.804355128406, 121.1611430300094, 114.71188637427872, 153.74886077166545, 89.99729898498907, 167.8819831531672, 89.10439219878455, 159.6893558834504, 78.48608146775051, 150.2260442132333, 82.68062365916192, 166.57052833435665, 112.06046447047683, 104.12008931186352, 71.37518479960013, 79.57565802583642, 126.63142503969165, 95.40618127322028, 85.60688973054596, 97.58671215434326, 166.06761704138967, 90.48399938557915, 174.0482351651168, 143.54097200473268, 87.94056509424645, 145.8183808750465, 149.39278174997312, 86.15536673052243, 156.14852956155858, 122.25788427800752, 155.62361439035172, 98.68263251697758, 94.99426506328294, 69.3999685015275, 66.15240932939885, 145.64112177460578, 160.31683793569107, 154.3254683305571, 150.58710517973208, 133.10025765414494, 142.60587792331106, 159.3517889744943, 151.42993080260342, 162.35520163199072, 68.20413257867119, 42.69874748475302, 49.420942806476205, 144.75134161927562, 155.3311625533292, 175.32861014830797, 11.687459440236672, 39.34544824384433, 131.47370803990637, 165.48504698376775, 175.09697806060063, 169.49684747114938, 95.46989212895791, 84.55465985193177, 89.66941972339605, 152.83122872035887, 147.03384839106346, 86.57276298856856, 174.78827074806233, 90.9799121226079, 58.52369888998559, 90.1645286220794, 135.9907520017734, 137.79327039611638, 108.15179788246736, 55.364318397843775, 158.65561174306796, 82.58296943092216, 79.99171359838891, 107.03628856404023, 74.86703566952971, 82.98160053070886, 139.7749700813839, 180.627147020207, 102.15407303866351, 164.29309630290976, 101.68599951300303, 85.13763780881582, 100.0782223205052, 71.78432932364962, 98.13957022078743, 84.23928340411756, 188.9724809355042, 99.06984084912764, 180.6904723368759, 150.95134451253466, 185.46834386064464, 173.5856745726142, 201.20523154058026, 13.292524388629893, 186.66745427182732, 209.19568779414365, 191.10194162851593, 168.91896007303657, 202.3936932954541, 190.49817524597825, 203.82555927412363, 182.24330035207905, 204.88867027816931, 185.98589130305575, 146.5245311229086, 200.61546397896362, 190.15280013072473, 170.21258337806174, 181.788348503516, 218.3606501920569, 191.28509375783875, 212.2251016220492, 189.24364813065694, 207.35542747002498, 184.7588243803877, 206.27598814089166, 185.67913571687757, 199.369697784532, 185.97152998135206, 150.91930637774172, 212.4717067444612, 186.60827747332476, 191.6203384112743, 182.14876299945425, 204.66685850542086, 182.04112409156957, 206.04550542580597, 195.94298764153652, 192.58610754881192, 195.01218867190516, 185.73539226630476, 191.08299643138704, 180.60443301686948, 179.39768010276737, 209.15259332275093, 181.22395805385617, 198.97335402482355, 198.55338165890774, 148.33670265307856, 212.1923399156237, 190.7840495506245, 198.95858789897042, 200.94011370088455, 181.84757657268756, 195.09395680249924, 185.95577248944258, 185.70642843018672, 198.21633416014723, 185.5642365023306, 137.82558143405123, 173.80396977312654, 206.69902098283677, 183.56489693011227, 196.86176614241307, 190.90774519937764, 198.1088309101087, 194.78261178009498, 145.2605800200927, 195.06134791905296, 212.91075890133806, 188.49467020155544, 197.94037511525244, 194.78496073932013, 193.9085443275373, 184.02443008899695, 208.4554869275949, 184.85641932183952, 210.12143318014276, 185.57396900208244, 195.86467719305872, 186.77702007982384, 204.30747819628726, 139.77077159178535, 192.33885268914838, 199.40270725702956, 205.35823577659636, 189.33167780348418, 155.9975113492084, 205.670274967165, 196.29465811265885, 213.06502020449705, 191.69776501254108, 199.0597095395197, 185.37351334788139, 205.38378531216077, 183.95944170876035, 205.0572526940349, 191.42577084479126, 191.95743608576828, 196.79454995494507, 187.05942472552192, 193.09968859642524, 183.83522651264403, 196.19749992753486, 185.08905286116703, 192.62370202528376]
Elapsed: 0.38273525521844154~0.436273163184839
Time per graph: 0.008825159746475816~0.010004218866178536
Speed: 147.06575690761437~48.94794318526623
Total Time: 0.2241
best val loss: 0.2735859155654907 test_score: 0.8140

Testing...
Test loss: 0.3912 score: 0.8372 time: 0.23s
test Score 0.8372
Epoch Time List: [1.7608320949366316, 1.384632449131459, 2.5586590870516375, 1.9334400609368458, 1.668121075956151, 1.7001923019997776, 1.4822224030504003, 1.6038846380542964, 1.74325524806045, 1.553041424951516, 1.722251403843984, 1.4221997618442401, 1.5858034319244325, 1.4872932969592512, 1.179690942983143, 1.5142534569604322, 1.8968538669869304, 1.8146002978319302, 1.6651129788951948, 1.3135791310342029, 1.2570316239725798, 1.49937800609041, 1.3318269540322945, 1.578525546938181, 1.7837930389214307, 1.7506346091395244, 2.175501978956163, 1.398768470971845, 1.2535952089820057, 1.625775224994868, 1.5064678319031373, 1.7206453839316964, 1.4639345599571243, 1.363475403864868, 4.66681337216869, 11.03550478105899, 6.498498068889603, 0.8284382539568469, 0.806134590995498, 0.8161564629990608, 0.8666771681746468, 0.89684079203289, 0.8391823279671371, 0.8783365390263498, 0.7845741219352931, 0.8307436329778284, 0.7778650788823143, 0.8395597640192136, 0.8656018481124192, 0.8312685569981113, 0.7800973849371076, 0.853192865033634, 0.7989277560263872, 0.8794622650602832, 0.8569967179792002, 0.9892935970565304, 0.8522592339431867, 0.8321516149444506, 0.977019413956441, 0.8756983180064708, 0.9018809418193996, 0.8402820390183479, 0.8291244428837672, 0.8405553179327399, 0.8278652881272137, 0.9822619151091203, 0.8301656959811226, 0.9239286199444905, 1.6627276620129123, 1.518990293261595, 1.5028761230641976, 1.5417230220045894, 1.7296291328966618, 2.043984869029373, 1.9453048270661384, 1.7824599930318072, 1.7072918640915304, 1.7410500210244209, 1.7237746769096702, 1.6811653848271817, 2.511383901932277, 1.4987384889973328, 2.1083124038996175, 1.746965229860507, 1.6737301180837676, 1.2986861079698429, 1.5426038600271568, 1.7903364651137963, 1.3223777728853747, 1.5199186372337863, 1.7571831850800663, 1.6403144089272246, 1.3944305701879784, 1.5300017190165818, 2.0935234752250835, 1.8079064378980547, 1.3709902859991416, 1.5054494622163475, 1.6910966909490526, 1.3145077250665054, 1.4687776959035546, 1.9406383509049192, 1.7375218420056626, 11.05798359401524, 10.691695747082122, 1.646164235076867, 1.5539003351004794, 1.8125635508913547, 1.0234172919299453, 1.4163324479013681, 1.720123720006086, 1.6381416958756745, 1.8010562030831352, 1.299538736930117, 1.7590198400430381, 1.303307669935748, 1.2066239179112017, 1.0704307279083878, 1.7045821601059288, 1.3083907309919596, 1.3484043231001124, 1.5603779039811343, 1.4574184188386425, 1.5668061199830845, 1.5131331039592624, 1.6667594929458573, 1.5082179259043187, 1.7938192919827998, 1.2379229189828038, 1.2740639069816098, 2.109576179878786, 1.9984533799579367, 1.2078591320896521, 1.5106919730314985, 1.139168035122566, 1.1733515600208193, 1.421758740907535, 1.175800762954168, 1.0664344870019704, 1.1741143540712073, 2.6362115959636867, 1.7803850980708376, 2.3756437009433284, 2.3139347921824083, 1.2687527399975806, 1.7697845300426707, 1.7575347470119596, 7.83408132311888, 4.134532475844026, 9.963118942920119, 1.3780607777880505, 1.2345508278813213, 1.041175740887411, 1.5438650000141934, 1.7470886398805305, 1.917313130106777, 1.8021632758900523, 1.5493450060021132, 2.773123527993448, 1.6258702740306035, 2.020623918971978, 2.007310903049074, 2.1435361900366843, 1.4910798149649054, 1.2455647990573198, 1.3618635119637474, 1.5635108830174431, 1.5069386478280649, 2.0643318539950997, 1.409142081043683, 1.4701070218579844, 2.2621027069399133, 1.5536694780457765, 1.8569374119397253, 1.1975133470259607, 1.504163017962128, 1.5850742699112743, 1.7485654900083318, 2.0645997871179134, 1.5293225391069427, 1.8939901079284027, 1.6115135239670053, 1.8129783460171893, 1.302595255896449, 1.7630829379195347, 1.2111273891059682, 10.79718027706258, 10.714117566938512, 0.8390147810569033, 1.1994719898793846, 6.632823967956938, 4.545898721087724, 0.7750482530100271, 0.9268603408709168, 0.9665630409726873, 0.8050516130169854, 0.9554375859443098, 0.839133984874934, 0.8422265499830246, 0.779322168091312, 0.8108619450358674, 0.8547868139576167, 0.8806660989066586, 0.8284964378690347, 0.8765358600066975, 0.8568623369792476, 0.7871532239951193, 0.8581077560083941, 0.8037089889403433, 0.9145757749211043, 0.8326124170562252, 0.8325679370900616, 0.7793034148635343, 0.8340121920919046, 0.7927106318529695, 0.8197472029132769, 0.8606697870418429, 0.9030855899909511, 0.8715932009508833, 0.8349209189182147, 0.8487871231045574, 0.7868193159811199, 0.8362406378146261, 0.7851237119175494, 0.9255611019907519, 0.9112997639458627, 0.8340163399698213, 0.843714045942761, 0.8001457868376747, 0.8322982109384611, 0.972499395837076, 0.8221410220721737, 0.9555214560823515, 0.8443871029885486, 0.9284254049416631, 0.9285361999645829, 0.7968041450949386, 0.8958464269526303, 0.8259857798693702, 0.9953385819680989, 0.8607508820714429, 0.8852179860696197, 0.8425680849468336, 0.9036186658777297, 0.9664261911530048, 0.8406063721049577, 0.9038621070794761, 0.8244152340339497, 0.7907360680401325, 0.8155175009742379, 0.7855927599593997, 0.8032810549484566, 0.7833770189899951, 0.8087630670052022, 0.8700352458981797, 0.8225533480290323, 0.7646674049319699, 0.8151972891064361, 0.7764401900349185, 0.8107033020351082, 0.8440323569811881, 0.8328804339980707, 0.8551611079601571, 0.8213289460400119, 0.7596551140304655, 0.819087999057956, 0.7763963240431622, 0.8233432580018416, 0.7950520860031247, 0.891986025031656, 0.7977473620558158, 0.8213235319126397, 0.7971603679470718, 0.9299284749431536, 0.9416955979540944, 0.8197454720502719, 0.8475792401004583, 0.8904929929412901, 0.8414826838998124, 0.781987032154575, 0.8321100550238043, 0.8036043520551175, 0.8345259790075943, 0.8581235709134489, 0.9068872169591486, 0.9363505239598453, 0.8272534030256793, 0.8485578800318763, 0.8157244030153379, 0.8272027149796486, 0.8003644479904324, 0.827247284934856, 0.943199609988369]
Total Epoch List: [89, 85, 119]
Total Time List: [0.4620678350329399, 0.3082298790104687, 0.22405704902485013]
========================training times:3========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335cc0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.29s
Epoch 2/1000, LR 0.000000
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5000 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.31s
Epoch 10/1000, LR 0.000240
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.22s
Epoch 12/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.23s
Epoch 14/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.26s
Epoch 15/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5000 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.31s
Epoch 17/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.4884 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5000 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4884 time: 0.35s
Test loss: 0.6909 score: 0.5455 time: 0.26s
Epoch 20/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.4884 time: 0.25s
Test loss: 0.6906 score: 0.6136 time: 0.33s
Epoch 21/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.32s
Val loss: 0.6916 score: 0.5814 time: 0.27s
Test loss: 0.6903 score: 0.6364 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.36s
Val loss: 0.6913 score: 0.6512 time: 0.28s
Test loss: 0.6899 score: 0.6591 time: 0.26s
Epoch 23/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.42s
Val loss: 0.6910 score: 0.6512 time: 0.26s
Test loss: 0.6895 score: 0.7045 time: 0.26s
Epoch 24/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.45s
Val loss: 0.6907 score: 0.6279 time: 0.26s
Test loss: 0.6891 score: 0.7045 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.38s
Val loss: 0.6904 score: 0.6279 time: 0.26s
Test loss: 0.6886 score: 0.7273 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.29s
Val loss: 0.6900 score: 0.6744 time: 0.26s
Test loss: 0.6881 score: 0.7045 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.33s
Val loss: 0.6896 score: 0.6744 time: 0.26s
Test loss: 0.6876 score: 0.7045 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.30s
Val loss: 0.6892 score: 0.6744 time: 0.26s
Test loss: 0.6870 score: 0.7045 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.33s
Val loss: 0.6887 score: 0.6744 time: 0.26s
Test loss: 0.6863 score: 0.7045 time: 0.33s
Epoch 30/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.42s
Val loss: 0.6881 score: 0.7209 time: 0.25s
Test loss: 0.6856 score: 0.7727 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.32s
Val loss: 0.6875 score: 0.7209 time: 0.28s
Test loss: 0.6848 score: 0.7727 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.35s
Val loss: 0.6868 score: 0.7442 time: 0.26s
Test loss: 0.6839 score: 0.7955 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.34s
Val loss: 0.6860 score: 0.7209 time: 0.27s
Test loss: 0.6829 score: 0.7955 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.33s
Val loss: 0.6852 score: 0.7209 time: 0.28s
Test loss: 0.6818 score: 0.7955 time: 0.33s
Epoch 35/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.32s
Val loss: 0.6842 score: 0.6977 time: 0.26s
Test loss: 0.6806 score: 0.7955 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 0.36s
Val loss: 0.6830 score: 0.6744 time: 0.25s
Test loss: 0.6792 score: 0.7500 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.31s
Val loss: 0.6818 score: 0.6512 time: 0.26s
Test loss: 0.6776 score: 0.7500 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.34s
Val loss: 0.6803 score: 0.6512 time: 0.26s
Test loss: 0.6759 score: 0.7273 time: 0.25s
Epoch 39/1000, LR 0.000269
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.31s
Val loss: 0.6787 score: 0.6744 time: 0.26s
Test loss: 0.6741 score: 0.7273 time: 0.33s
Epoch 40/1000, LR 0.000269
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.33s
Val loss: 0.6770 score: 0.6744 time: 0.28s
Test loss: 0.6721 score: 0.7273 time: 0.27s
Epoch 41/1000, LR 0.000269
Train loss: 0.6698;  Loss pred: 0.6698; Loss self: 0.0000; time: 0.35s
Val loss: 0.6751 score: 0.6744 time: 0.25s
Test loss: 0.6699 score: 0.7273 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6678;  Loss pred: 0.6678; Loss self: 0.0000; time: 0.33s
Val loss: 0.6731 score: 0.6744 time: 0.26s
Test loss: 0.6676 score: 0.7500 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6645;  Loss pred: 0.6645; Loss self: 0.0000; time: 0.41s
Val loss: 0.6709 score: 0.6744 time: 0.29s
Test loss: 0.6651 score: 0.7500 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6619;  Loss pred: 0.6619; Loss self: 0.0000; time: 0.29s
Val loss: 0.6684 score: 0.6977 time: 0.26s
Test loss: 0.6624 score: 0.7727 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.35s
Val loss: 0.6657 score: 0.6977 time: 0.24s
Test loss: 0.6596 score: 0.7727 time: 0.26s
Epoch 46/1000, LR 0.000269
Train loss: 0.6554;  Loss pred: 0.6554; Loss self: 0.0000; time: 0.30s
Val loss: 0.6628 score: 0.7209 time: 0.26s
Test loss: 0.6564 score: 0.7727 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6523;  Loss pred: 0.6523; Loss self: 0.0000; time: 0.34s
Val loss: 0.6595 score: 0.7442 time: 0.24s
Test loss: 0.6531 score: 0.8182 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.37s
Val loss: 0.6560 score: 0.7442 time: 0.25s
Test loss: 0.6495 score: 0.7955 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6408;  Loss pred: 0.6408; Loss self: 0.0000; time: 0.33s
Val loss: 0.6521 score: 0.7674 time: 0.24s
Test loss: 0.6456 score: 0.7955 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 0.6377;  Loss pred: 0.6377; Loss self: 0.0000; time: 0.34s
Val loss: 0.6479 score: 0.7442 time: 0.25s
Test loss: 0.6415 score: 0.8636 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.6303;  Loss pred: 0.6303; Loss self: 0.0000; time: 0.33s
Val loss: 0.6434 score: 0.7907 time: 0.25s
Test loss: 0.6369 score: 0.8636 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6233;  Loss pred: 0.6233; Loss self: 0.0000; time: 0.33s
Val loss: 0.6385 score: 0.7907 time: 0.25s
Test loss: 0.6321 score: 0.8636 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.6168;  Loss pred: 0.6168; Loss self: 0.0000; time: 0.32s
Val loss: 0.6334 score: 0.7907 time: 0.25s
Test loss: 0.6269 score: 0.8636 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.6135;  Loss pred: 0.6135; Loss self: 0.0000; time: 0.33s
Val loss: 0.6279 score: 0.8140 time: 0.25s
Test loss: 0.6214 score: 0.8636 time: 0.32s
Epoch 55/1000, LR 0.000269
Train loss: 0.6071;  Loss pred: 0.6071; Loss self: 0.0000; time: 0.33s
Val loss: 0.6220 score: 0.8140 time: 0.26s
Test loss: 0.6156 score: 0.8636 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.5953;  Loss pred: 0.5953; Loss self: 0.0000; time: 0.35s
Val loss: 0.6158 score: 0.8140 time: 0.26s
Test loss: 0.6094 score: 0.8636 time: 0.26s
Epoch 57/1000, LR 0.000269
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.29s
Val loss: 0.6092 score: 0.8372 time: 0.26s
Test loss: 0.6028 score: 0.8636 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5821;  Loss pred: 0.5821; Loss self: 0.0000; time: 0.34s
Val loss: 0.6023 score: 0.8372 time: 0.24s
Test loss: 0.5960 score: 0.8636 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5633;  Loss pred: 0.5633; Loss self: 0.0000; time: 0.28s
Val loss: 0.5953 score: 0.8372 time: 0.25s
Test loss: 0.5888 score: 0.8636 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5616;  Loss pred: 0.5616; Loss self: 0.0000; time: 0.32s
Val loss: 0.5880 score: 0.8140 time: 0.26s
Test loss: 0.5815 score: 0.8636 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5546;  Loss pred: 0.5546; Loss self: 0.0000; time: 0.40s
Val loss: 0.5804 score: 0.8140 time: 0.26s
Test loss: 0.5738 score: 0.8636 time: 0.38s
Epoch 62/1000, LR 0.000268
Train loss: 0.5348;  Loss pred: 0.5348; Loss self: 0.0000; time: 0.32s
Val loss: 0.5726 score: 0.8140 time: 0.25s
Test loss: 0.5659 score: 0.8636 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.5313;  Loss pred: 0.5313; Loss self: 0.0000; time: 0.39s
Val loss: 0.5644 score: 0.8140 time: 0.25s
Test loss: 0.5578 score: 0.8636 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5233;  Loss pred: 0.5233; Loss self: 0.0000; time: 0.32s
Val loss: 0.5557 score: 0.8140 time: 0.25s
Test loss: 0.5494 score: 0.8636 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.37s
Val loss: 0.5467 score: 0.8140 time: 0.26s
Test loss: 0.5406 score: 0.8636 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.4988;  Loss pred: 0.4988; Loss self: 0.0000; time: 0.29s
Val loss: 0.5373 score: 0.8140 time: 0.26s
Test loss: 0.5317 score: 0.8636 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.4718;  Loss pred: 0.4718; Loss self: 0.0000; time: 0.33s
Val loss: 0.5275 score: 0.8140 time: 0.24s
Test loss: 0.5224 score: 0.8636 time: 0.28s
Epoch 68/1000, LR 0.000268
Train loss: 0.4698;  Loss pred: 0.4698; Loss self: 0.0000; time: 0.29s
Val loss: 0.5178 score: 0.8140 time: 0.26s
Test loss: 0.5133 score: 0.8636 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4636;  Loss pred: 0.4636; Loss self: 0.0000; time: 0.32s
Val loss: 0.5079 score: 0.8140 time: 0.25s
Test loss: 0.5041 score: 0.8636 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.4387;  Loss pred: 0.4387; Loss self: 0.0000; time: 0.29s
Val loss: 0.4981 score: 0.8140 time: 0.26s
Test loss: 0.4952 score: 0.8636 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.4222;  Loss pred: 0.4222; Loss self: 0.0000; time: 0.42s
Val loss: 0.4885 score: 0.8140 time: 0.34s
Test loss: 0.4865 score: 0.8636 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.4179;  Loss pred: 0.4179; Loss self: 0.0000; time: 0.42s
Val loss: 0.4791 score: 0.8140 time: 0.28s
Test loss: 0.4780 score: 0.8636 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.4054;  Loss pred: 0.4054; Loss self: 0.0000; time: 0.29s
Val loss: 0.4699 score: 0.8140 time: 0.26s
Test loss: 0.4698 score: 0.8636 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.3761;  Loss pred: 0.3761; Loss self: 0.0000; time: 0.34s
Val loss: 0.4608 score: 0.8140 time: 0.25s
Test loss: 0.4616 score: 0.8636 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.3656;  Loss pred: 0.3656; Loss self: 0.0000; time: 0.40s
Val loss: 0.4521 score: 0.8372 time: 0.26s
Test loss: 0.4538 score: 0.8636 time: 0.26s
Epoch 76/1000, LR 0.000267
Train loss: 0.3572;  Loss pred: 0.3572; Loss self: 0.0000; time: 0.32s
Val loss: 0.4431 score: 0.8372 time: 0.27s
Test loss: 0.4460 score: 0.8636 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.3388;  Loss pred: 0.3388; Loss self: 0.0000; time: 0.54s
Val loss: 0.4344 score: 0.8372 time: 0.26s
Test loss: 0.4384 score: 0.8636 time: 0.31s
Epoch 78/1000, LR 0.000267
Train loss: 0.3377;  Loss pred: 0.3377; Loss self: 0.0000; time: 0.54s
Val loss: 0.4264 score: 0.8372 time: 0.26s
Test loss: 0.4316 score: 0.8636 time: 0.38s
Epoch 79/1000, LR 0.000267
Train loss: 0.3077;  Loss pred: 0.3077; Loss self: 0.0000; time: 0.30s
Val loss: 0.4193 score: 0.8372 time: 0.27s
Test loss: 0.4256 score: 0.8636 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.2941;  Loss pred: 0.2941; Loss self: 0.0000; time: 0.33s
Val loss: 0.4128 score: 0.8372 time: 0.24s
Test loss: 0.4205 score: 0.8636 time: 0.26s
Epoch 81/1000, LR 0.000267
Train loss: 0.2931;  Loss pred: 0.2931; Loss self: 0.0000; time: 0.29s
Val loss: 0.4070 score: 0.8372 time: 0.27s
Test loss: 0.4159 score: 0.8636 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.2764;  Loss pred: 0.2764; Loss self: 0.0000; time: 0.34s
Val loss: 0.4020 score: 0.8372 time: 0.24s
Test loss: 0.4122 score: 0.8409 time: 0.26s
Epoch 83/1000, LR 0.000266
Train loss: 0.2598;  Loss pred: 0.2598; Loss self: 0.0000; time: 0.31s
Val loss: 0.3974 score: 0.8372 time: 0.26s
Test loss: 0.4085 score: 0.8409 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2642;  Loss pred: 0.2642; Loss self: 0.0000; time: 0.33s
Val loss: 0.3934 score: 0.8372 time: 0.24s
Test loss: 0.4050 score: 0.8636 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2307;  Loss pred: 0.2307; Loss self: 0.0000; time: 0.30s
Val loss: 0.3901 score: 0.8372 time: 0.26s
Test loss: 0.4024 score: 0.8636 time: 0.35s
Epoch 86/1000, LR 0.000266
Train loss: 0.2178;  Loss pred: 0.2178; Loss self: 0.0000; time: 0.37s
Val loss: 0.3875 score: 0.8372 time: 0.27s
Test loss: 0.4005 score: 0.8636 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.2230;  Loss pred: 0.2230; Loss self: 0.0000; time: 0.35s
Val loss: 0.3855 score: 0.8605 time: 0.26s
Test loss: 0.3993 score: 0.8636 time: 0.26s
Epoch 88/1000, LR 0.000266
Train loss: 0.2100;  Loss pred: 0.2100; Loss self: 0.0000; time: 0.27s
Val loss: 0.3842 score: 0.8605 time: 0.26s
Test loss: 0.3981 score: 0.8636 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.2016;  Loss pred: 0.2016; Loss self: 0.0000; time: 0.34s
Val loss: 0.3835 score: 0.8837 time: 0.29s
Test loss: 0.3979 score: 0.8636 time: 0.27s
Epoch 90/1000, LR 0.000266
Train loss: 0.1928;  Loss pred: 0.1928; Loss self: 0.0000; time: 0.27s
Val loss: 0.3838 score: 0.8837 time: 0.26s
Test loss: 0.3982 score: 0.8636 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 91/1000, LR 0.000266
Train loss: 0.1962;  Loss pred: 0.1962; Loss self: 0.0000; time: 0.33s
Val loss: 0.3846 score: 0.8837 time: 0.26s
Test loss: 0.3994 score: 0.8409 time: 0.35s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 088,   Train_Loss: 0.2016,   Val_Loss: 0.3835,   Val_Precision: 0.9048,   Val_Recall: 0.8636,   Val_accuracy: 0.8837,   Val_Score: 0.8837,   Val_Loss: 0.3835,   Test_Precision: 0.9444,   Test_Recall: 0.7727,   Test_accuracy: 0.8500,   Test_Score: 0.8636,   Test_loss: 0.3979


[0.29504061897750944, 0.2602143249241635, 0.24874609697144479, 0.2589874389814213, 0.2487967440392822, 0.2585598030127585, 0.23240336799062788, 0.25865164201240987, 0.31789859698619694, 0.24847819795832038, 0.22744410601444542, 0.250394704984501, 0.2351388270035386, 0.2624381099594757, 0.23377270996570587, 0.31641894194763154, 0.2522380700102076, 0.23372588900383562, 0.2611715809907764, 0.33705676603130996, 0.2356680299853906, 0.2595043950714171, 0.2642884140368551, 0.24788584001362324, 0.2569998319959268, 0.2573337920475751, 0.25340779102407396, 0.24834224593359977, 0.3394831409677863, 0.24500947899650782, 0.25162036600522697, 0.2315665720961988, 0.26495987293310463, 0.3329469310119748, 0.24564528395421803, 0.24575149803422391, 0.23504657298326492, 0.25257519795559347, 0.3368729429785162, 0.27291108400095254, 0.25675732793752104, 0.2607671859441325, 0.25857046991586685, 0.2468771890271455, 0.26368959702085704, 0.23690679401624948, 0.2643598180729896, 0.24004649801645428, 0.2577205839334056, 0.2332571200095117, 0.24856221594382077, 0.23504908592440188, 0.25187534105498344, 0.3289831329602748, 0.2339599970728159, 0.2609609740320593, 0.25221223500557244, 0.25008642103057355, 0.24965568294283003, 0.2536674509756267, 0.3835186630021781, 0.24533449299633503, 0.24475579999852926, 0.24159664602484554, 0.2582373139448464, 0.24696325999684632, 0.2893312709638849, 0.2493147529894486, 0.24862971401307732, 0.25333679595496505, 0.24122315598651767, 0.2507089090067893, 0.24644195404835045, 0.25853196694515646, 0.2678471519611776, 0.2515125690260902, 0.3185991980135441, 0.3815767130581662, 0.23832819005474448, 0.26072070107329637, 0.23346387199126184, 0.264013655949384, 0.23430509807076305, 0.24285605200566351, 0.3515391770051792, 0.2507219979306683, 0.26848886092193425, 0.23092354799155146, 0.26951280096545815, 0.2320034260628745, 0.3585730119375512]
[0.006705468613125215, 0.005913961930094625, 0.0056533203857146545, 0.005886078158668665, 0.005654471455438232, 0.005876359159380875, 0.005281894727059725, 0.005878446409372951, 0.007224968113322658, 0.005647231771780009, 0.005169184227601032, 0.00569078874964775, 0.005344064250080423, 0.005964502499078993, 0.005313016135584225, 0.007191339589718898, 0.0057326834093229, 0.005311952022814446, 0.005935717749790373, 0.007660381046166135, 0.005356091590577059, 0.005897827160714025, 0.006006554864473979, 0.00563376909121871, 0.0058409052726347, 0.005848495273808526, 0.005759267977819863, 0.005644141953036358, 0.007715525931086053, 0.005568397249920632, 0.005718644681936977, 0.005262876638549973, 0.006021815293934196, 0.00756697570481761, 0.005582847362595864, 0.005585261318959634, 0.0053419675678014755, 0.00574034540808167, 0.007656203249511732, 0.006202524636385285, 0.005835393816761842, 0.005926526953275738, 0.005876601588996974, 0.005610845205162398, 0.0059929453868376595, 0.005384245318551125, 0.006008177683477036, 0.005455602227646688, 0.005857285998486491, 0.005301298182034357, 0.005649141271450472, 0.005342024680100043, 0.005724439569431442, 0.007476889385460791, 0.005317272660745816, 0.005930931228001348, 0.005732096250126647, 0.0056837822961493985, 0.005673992794155228, 0.005765169340355153, 0.008716333250049502, 0.005575783931734887, 0.005562631818148392, 0.005490832864201035, 0.005869029862382872, 0.005612801363564689, 0.006575710703724657, 0.005666244386123832, 0.00565067531847903, 0.005757654453521933, 0.005482344454239038, 0.005697929750154303, 0.005600953501098874, 0.0058757265214808285, 0.006087435271844945, 0.005716194750592959, 0.007240890863944183, 0.008672198024049232, 0.005416549773971466, 0.005925470478938554, 0.005305997090710496, 0.006000310362486, 0.005325115865244615, 0.005519455727401443, 0.00798952675011771, 0.005698227225697006, 0.006102019566407596, 0.005248262454353442, 0.00612529093103314, 0.005272805137792602, 0.008149386634944345]
[149.13200817055656, 169.09138270086896, 176.8871975709876, 169.89240935023255, 176.85118898924534, 170.1733969755107, 189.32599979262946, 170.11297379619543, 138.4089153495398, 177.07791010050926, 193.45412273380904, 175.7225657097003, 187.12349874628677, 167.65857674708238, 188.2170079067601, 139.05615046043027, 174.43837878326377, 188.25471233645806, 168.47162249844448, 130.54180907886817, 186.70330465582296, 169.55396839383374, 166.484785798685, 177.50106257615144, 171.20633760063063, 170.98415116762206, 173.6331776627182, 177.17484930761418, 129.60879257381202, 179.58488863456236, 174.86660836939558, 190.01015389095647, 166.06288157116091, 132.15319290153627, 179.12006813939269, 179.04265223999758, 187.19694331868754, 174.20554494719573, 130.61304244551948, 161.22467198820854, 171.36803982750163, 168.73288654281328, 170.16637674950522, 178.22626777868066, 166.8628588200229, 185.72705009457025, 166.439817975104, 183.2978208954499, 170.7275349467992, 188.63304150460993, 177.01805494824177, 187.19494197119144, 174.68958976176617, 133.74545863210884, 188.06633847882028, 168.60758649143668, 174.45624713260978, 175.93918061877065, 176.24273351740922, 173.45544267020557, 114.7271417134402, 179.34697833401395, 179.77102074910746, 182.12173357520487, 170.38591103606893, 178.16415284023168, 152.07481670894882, 176.48373982049168, 176.96999803364494, 173.68183660071927, 182.40371584583374, 175.50233924398935, 178.5410287380186, 170.1917194995616, 164.2727939342714, 174.941555288379, 138.1045535404314, 115.31102002362707, 184.61936873641807, 168.7629705614756, 188.46599101057848, 166.65804593242544, 187.78934117221579, 181.17728438974171, 125.16385904650322, 175.493177156985, 163.88016936312837, 190.53925155181565, 163.2575515611194, 189.65237172004393, 122.70862149453403]
Elapsed: 0.2627722160496677~0.033991062781256264
Time per graph: 0.005972095819310631~0.0007725241541194605
Speed: 169.75367290726888~18.006267531097883
Total Time: 0.3590
best val loss: 0.3835447132587433 test_score: 0.8636

Testing...
Test loss: 0.3979 score: 0.8636 time: 0.23s
test Score 0.8636
Epoch Time List: [0.934929934097454, 0.8734084410825744, 0.7895889698993415, 0.831338676973246, 0.7911594390170649, 0.8215853300644085, 0.825209322036244, 0.837743850890547, 0.9295234250603244, 0.8328781710006297, 0.7930705251637846, 0.802812666981481, 0.7761545989196748, 0.845190201071091, 0.8117428759578615, 0.8847868650918826, 0.8940225508995354, 0.8316801900509745, 0.9685200039530173, 1.1244132581632584, 0.8194382269866765, 0.8910377660067752, 0.9436331499600783, 0.952214110060595, 0.8943131359992549, 0.8082897100830451, 0.8313664890592918, 0.8067292099585757, 0.9258880651323125, 0.9114917579572648, 0.8459604900563136, 0.8354824390262365, 0.8632762040942907, 0.9406201289966702, 0.8238228260306641, 0.8479417680064216, 0.8005321229575202, 0.8464690271066502, 0.9025670021073893, 0.8822696179850027, 0.8527349609648809, 0.8415270490804687, 0.9569018579786643, 0.7929080701433122, 0.8433375079184771, 0.7994014719733968, 0.8385097770951688, 0.8620888689765707, 0.820860258070752, 0.8168894230620936, 0.8205616631312296, 0.8104007210349664, 0.820833187084645, 0.911158507806249, 0.8206888299901038, 0.8745032419683412, 0.7944751180475578, 0.8232337039662525, 0.7812779799569398, 0.8223919089650735, 1.0431068709585816, 0.8149045129539445, 0.8839968420797959, 0.8134818070102483, 0.8917078251251951, 0.7952356220921502, 0.8587284249952063, 0.7964818381005898, 0.8121213659178466, 0.7932692429749295, 0.9975144992349669, 0.9453789420658723, 0.7985852970741689, 0.8439159189583734, 0.917123589082621, 0.8350905089173466, 1.1162859270116314, 1.1827819219324738, 0.8072354408213869, 0.8269225110998377, 0.7871438489528373, 0.8420832951087505, 0.7935800299746916, 0.8035385539988056, 0.9085831359261647, 0.8807221408933401, 0.8730751849943772, 0.7609814869938418, 0.8914572860812768, 0.7631307900883257, 0.9444005859550089]
Total Epoch List: [91]
Total Time List: [0.3590319160139188]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334a90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 3/1000, LR 0.000030
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.36s
Epoch 8/1000, LR 0.000180
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.46s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.22s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.24s
Epoch 19/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.33s
Val loss: 0.6917 score: 0.5455 time: 0.24s
Test loss: 0.6922 score: 0.5581 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.36s
Val loss: 0.6915 score: 0.5909 time: 0.23s
Test loss: 0.6921 score: 0.5581 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.31s
Val loss: 0.6913 score: 0.5682 time: 0.25s
Test loss: 0.6920 score: 0.5581 time: 0.32s
Epoch 22/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.36s
Val loss: 0.6910 score: 0.5455 time: 0.34s
Test loss: 0.6919 score: 0.5581 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.42s
Val loss: 0.6908 score: 0.5909 time: 0.21s
Test loss: 0.6917 score: 0.6047 time: 0.26s
Epoch 24/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.36s
Val loss: 0.6905 score: 0.6591 time: 0.23s
Test loss: 0.6915 score: 0.5814 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.45s
Val loss: 0.6902 score: 0.6364 time: 0.25s
Test loss: 0.6914 score: 0.6279 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.32s
Val loss: 0.6898 score: 0.6364 time: 0.22s
Test loss: 0.6912 score: 0.6279 time: 0.22s
Epoch 27/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.36s
Val loss: 0.6894 score: 0.6364 time: 0.21s
Test loss: 0.6909 score: 0.6279 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.30s
Val loss: 0.6890 score: 0.6364 time: 0.24s
Test loss: 0.6907 score: 0.6744 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.36s
Val loss: 0.6885 score: 0.6591 time: 0.21s
Test loss: 0.6904 score: 0.6744 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.39s
Val loss: 0.6880 score: 0.6591 time: 0.24s
Test loss: 0.6900 score: 0.6744 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.36s
Val loss: 0.6874 score: 0.6591 time: 0.21s
Test loss: 0.6897 score: 0.6744 time: 0.24s
Epoch 32/1000, LR 0.000270
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.31s
Val loss: 0.6868 score: 0.6364 time: 0.24s
Test loss: 0.6892 score: 0.6512 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.37s
Val loss: 0.6861 score: 0.6364 time: 0.21s
Test loss: 0.6888 score: 0.6279 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.31s
Val loss: 0.6853 score: 0.6364 time: 0.24s
Test loss: 0.6883 score: 0.6279 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.37s
Val loss: 0.6845 score: 0.6591 time: 0.25s
Test loss: 0.6878 score: 0.6512 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.31s
Val loss: 0.6836 score: 0.6818 time: 0.24s
Test loss: 0.6872 score: 0.6512 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.35s
Val loss: 0.6827 score: 0.7045 time: 0.22s
Test loss: 0.6866 score: 0.6744 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.40s
Val loss: 0.6816 score: 0.7273 time: 0.25s
Test loss: 0.6860 score: 0.7209 time: 0.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6725;  Loss pred: 0.6725; Loss self: 0.0000; time: 0.36s
Val loss: 0.6804 score: 0.7727 time: 0.25s
Test loss: 0.6853 score: 0.6977 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.36s
Val loss: 0.6791 score: 0.7955 time: 0.23s
Test loss: 0.6845 score: 0.6977 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6688;  Loss pred: 0.6688; Loss self: 0.0000; time: 0.38s
Val loss: 0.6777 score: 0.7727 time: 0.22s
Test loss: 0.6836 score: 0.6977 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6654;  Loss pred: 0.6654; Loss self: 0.0000; time: 0.37s
Val loss: 0.6761 score: 0.7727 time: 0.24s
Test loss: 0.6827 score: 0.6977 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6632;  Loss pred: 0.6632; Loss self: 0.0000; time: 0.39s
Val loss: 0.6743 score: 0.7727 time: 0.20s
Test loss: 0.6816 score: 0.6744 time: 0.22s
Epoch 44/1000, LR 0.000269
Train loss: 0.6607;  Loss pred: 0.6607; Loss self: 0.0000; time: 0.34s
Val loss: 0.6724 score: 0.7955 time: 0.21s
Test loss: 0.6805 score: 0.6512 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6560;  Loss pred: 0.6560; Loss self: 0.0000; time: 0.34s
Val loss: 0.6702 score: 0.8409 time: 0.34s
Test loss: 0.6792 score: 0.6279 time: 0.23s
Epoch 46/1000, LR 0.000269
Train loss: 0.6517;  Loss pred: 0.6517; Loss self: 0.0000; time: 0.34s
Val loss: 0.6679 score: 0.8409 time: 0.22s
Test loss: 0.6778 score: 0.6512 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6486;  Loss pred: 0.6486; Loss self: 0.0000; time: 0.33s
Val loss: 0.6653 score: 0.8636 time: 0.22s
Test loss: 0.6762 score: 0.6744 time: 0.22s
Epoch 48/1000, LR 0.000269
Train loss: 0.6424;  Loss pred: 0.6424; Loss self: 0.0000; time: 0.39s
Val loss: 0.6625 score: 0.8636 time: 0.20s
Test loss: 0.6745 score: 0.6744 time: 0.20s
Epoch 49/1000, LR 0.000269
Train loss: 0.6406;  Loss pred: 0.6406; Loss self: 0.0000; time: 0.32s
Val loss: 0.6594 score: 0.8409 time: 0.19s
Test loss: 0.6726 score: 0.6744 time: 0.20s
Epoch 50/1000, LR 0.000269
Train loss: 0.6346;  Loss pred: 0.6346; Loss self: 0.0000; time: 0.30s
Val loss: 0.6561 score: 0.8409 time: 0.19s
Test loss: 0.6706 score: 0.6744 time: 0.20s
Epoch 51/1000, LR 0.000269
Train loss: 0.6270;  Loss pred: 0.6270; Loss self: 0.0000; time: 0.33s
Val loss: 0.6526 score: 0.8409 time: 0.24s
Test loss: 0.6684 score: 0.6977 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.36s
Val loss: 0.6487 score: 0.8409 time: 0.34s
Test loss: 0.6660 score: 0.7209 time: 0.31s
Epoch 53/1000, LR 0.000269
Train loss: 0.6145;  Loss pred: 0.6145; Loss self: 0.0000; time: 0.39s
Val loss: 0.6446 score: 0.8636 time: 0.23s
Test loss: 0.6635 score: 0.7209 time: 0.29s
Epoch 54/1000, LR 0.000269
Train loss: 0.6084;  Loss pred: 0.6084; Loss self: 0.0000; time: 0.33s
Val loss: 0.6402 score: 0.8636 time: 0.24s
Test loss: 0.6609 score: 0.7209 time: 0.24s
Epoch 55/1000, LR 0.000269
Train loss: 0.6019;  Loss pred: 0.6019; Loss self: 0.0000; time: 0.38s
Val loss: 0.6356 score: 0.8636 time: 0.22s
Test loss: 0.6580 score: 0.7209 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.5932;  Loss pred: 0.5932; Loss self: 0.0000; time: 0.31s
Val loss: 0.6306 score: 0.8636 time: 0.30s
Test loss: 0.6549 score: 0.7442 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.5840;  Loss pred: 0.5840; Loss self: 0.0000; time: 0.36s
Val loss: 0.6253 score: 0.8636 time: 0.22s
Test loss: 0.6517 score: 0.7442 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5766;  Loss pred: 0.5766; Loss self: 0.0000; time: 0.34s
Val loss: 0.6198 score: 0.8636 time: 0.23s
Test loss: 0.6482 score: 0.7209 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.5651;  Loss pred: 0.5651; Loss self: 0.0000; time: 0.35s
Val loss: 0.6138 score: 0.8636 time: 0.22s
Test loss: 0.6446 score: 0.7209 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5565;  Loss pred: 0.5565; Loss self: 0.0000; time: 0.35s
Val loss: 0.6076 score: 0.8636 time: 0.25s
Test loss: 0.6407 score: 0.7209 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5476;  Loss pred: 0.5476; Loss self: 0.0000; time: 0.48s
Val loss: 0.6009 score: 0.8636 time: 0.24s
Test loss: 0.6366 score: 0.7209 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.40s
Val loss: 0.5938 score: 0.8636 time: 0.21s
Test loss: 0.6322 score: 0.7442 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5248;  Loss pred: 0.5248; Loss self: 0.0000; time: 0.31s
Val loss: 0.5863 score: 0.8636 time: 0.23s
Test loss: 0.6276 score: 0.7442 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5070;  Loss pred: 0.5070; Loss self: 0.0000; time: 0.38s
Val loss: 0.5785 score: 0.8409 time: 0.22s
Test loss: 0.6227 score: 0.7674 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.5011;  Loss pred: 0.5011; Loss self: 0.0000; time: 0.36s
Val loss: 0.5703 score: 0.8409 time: 0.33s
Test loss: 0.6178 score: 0.7674 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.4821;  Loss pred: 0.4821; Loss self: 0.0000; time: 0.39s
Val loss: 0.5620 score: 0.8409 time: 0.26s
Test loss: 0.6128 score: 0.7674 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.4747;  Loss pred: 0.4747; Loss self: 0.0000; time: 0.45s
Val loss: 0.5534 score: 0.8409 time: 0.23s
Test loss: 0.6077 score: 0.7674 time: 0.26s
Epoch 68/1000, LR 0.000268
Train loss: 0.4597;  Loss pred: 0.4597; Loss self: 0.0000; time: 0.39s
Val loss: 0.5445 score: 0.8409 time: 0.22s
Test loss: 0.6025 score: 0.7674 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4482;  Loss pred: 0.4482; Loss self: 0.0000; time: 0.48s
Val loss: 0.5355 score: 0.8182 time: 0.24s
Test loss: 0.5973 score: 0.7209 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.4256;  Loss pred: 0.4256; Loss self: 0.0000; time: 0.39s
Val loss: 0.5263 score: 0.8182 time: 0.24s
Test loss: 0.5921 score: 0.7209 time: 0.35s
Epoch 71/1000, LR 0.000268
Train loss: 0.4101;  Loss pred: 0.4101; Loss self: 0.0000; time: 0.52s
Val loss: 0.5167 score: 0.8182 time: 0.22s
Test loss: 0.5866 score: 0.7209 time: 0.27s
Epoch 72/1000, LR 0.000267
Train loss: 0.3883;  Loss pred: 0.3883; Loss self: 0.0000; time: 0.35s
Val loss: 0.5071 score: 0.8182 time: 0.25s
Test loss: 0.5813 score: 0.7442 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.3823;  Loss pred: 0.3823; Loss self: 0.0000; time: 0.37s
Val loss: 0.4976 score: 0.8182 time: 0.21s
Test loss: 0.5764 score: 0.7442 time: 0.28s
Epoch 74/1000, LR 0.000267
Train loss: 0.3561;  Loss pred: 0.3561; Loss self: 0.0000; time: 0.34s
Val loss: 0.4881 score: 0.8182 time: 0.25s
Test loss: 0.5715 score: 0.7442 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.3407;  Loss pred: 0.3407; Loss self: 0.0000; time: 0.38s
Val loss: 0.4788 score: 0.8182 time: 0.27s
Test loss: 0.5670 score: 0.7442 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3343;  Loss pred: 0.3343; Loss self: 0.0000; time: 0.49s
Val loss: 0.4702 score: 0.8182 time: 0.23s
Test loss: 0.5635 score: 0.7442 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.3052;  Loss pred: 0.3052; Loss self: 0.0000; time: 0.33s
Val loss: 0.4623 score: 0.8182 time: 0.24s
Test loss: 0.5607 score: 0.7209 time: 0.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.2983;  Loss pred: 0.2983; Loss self: 0.0000; time: 0.37s
Val loss: 0.4546 score: 0.8182 time: 0.22s
Test loss: 0.5584 score: 0.7209 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.2776;  Loss pred: 0.2776; Loss self: 0.0000; time: 0.34s
Val loss: 0.4475 score: 0.8182 time: 0.24s
Test loss: 0.5568 score: 0.7209 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.2706;  Loss pred: 0.2706; Loss self: 0.0000; time: 0.37s
Val loss: 0.4413 score: 0.8182 time: 0.22s
Test loss: 0.5562 score: 0.7209 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.2604;  Loss pred: 0.2604; Loss self: 0.0000; time: 0.32s
Val loss: 0.4360 score: 0.8182 time: 0.25s
Test loss: 0.5568 score: 0.7209 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.2193;  Loss pred: 0.2193; Loss self: 0.0000; time: 0.36s
Val loss: 0.4319 score: 0.8182 time: 0.21s
Test loss: 0.5588 score: 0.7209 time: 0.36s
Epoch 83/1000, LR 0.000266
Train loss: 0.2201;  Loss pred: 0.2201; Loss self: 0.0000; time: 0.37s
Val loss: 0.4285 score: 0.8182 time: 0.24s
Test loss: 0.5616 score: 0.7442 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.2107;  Loss pred: 0.2107; Loss self: 0.0000; time: 0.37s
Val loss: 0.4256 score: 0.8182 time: 0.23s
Test loss: 0.5646 score: 0.7442 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.1954;  Loss pred: 0.1954; Loss self: 0.0000; time: 0.38s
Val loss: 0.4237 score: 0.8182 time: 0.22s
Test loss: 0.5687 score: 0.7442 time: 0.24s
Epoch 86/1000, LR 0.000266
Train loss: 0.2055;  Loss pred: 0.2055; Loss self: 0.0000; time: 0.38s
Val loss: 0.4240 score: 0.8182 time: 0.23s
Test loss: 0.5754 score: 0.7674 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 87/1000, LR 0.000266
Train loss: 0.1968;  Loss pred: 0.1968; Loss self: 0.0000; time: 0.37s
Val loss: 0.4252 score: 0.8182 time: 0.22s
Test loss: 0.5828 score: 0.7674 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 084,   Train_Loss: 0.1954,   Val_Loss: 0.4237,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4237,   Test_Precision: 0.8667,   Test_Recall: 0.5909,   Test_accuracy: 0.7027,   Test_Score: 0.7442,   Test_loss: 0.5687


[0.29504061897750944, 0.2602143249241635, 0.24874609697144479, 0.2589874389814213, 0.2487967440392822, 0.2585598030127585, 0.23240336799062788, 0.25865164201240987, 0.31789859698619694, 0.24847819795832038, 0.22744410601444542, 0.250394704984501, 0.2351388270035386, 0.2624381099594757, 0.23377270996570587, 0.31641894194763154, 0.2522380700102076, 0.23372588900383562, 0.2611715809907764, 0.33705676603130996, 0.2356680299853906, 0.2595043950714171, 0.2642884140368551, 0.24788584001362324, 0.2569998319959268, 0.2573337920475751, 0.25340779102407396, 0.24834224593359977, 0.3394831409677863, 0.24500947899650782, 0.25162036600522697, 0.2315665720961988, 0.26495987293310463, 0.3329469310119748, 0.24564528395421803, 0.24575149803422391, 0.23504657298326492, 0.25257519795559347, 0.3368729429785162, 0.27291108400095254, 0.25675732793752104, 0.2607671859441325, 0.25857046991586685, 0.2468771890271455, 0.26368959702085704, 0.23690679401624948, 0.2643598180729896, 0.24004649801645428, 0.2577205839334056, 0.2332571200095117, 0.24856221594382077, 0.23504908592440188, 0.25187534105498344, 0.3289831329602748, 0.2339599970728159, 0.2609609740320593, 0.25221223500557244, 0.25008642103057355, 0.24965568294283003, 0.2536674509756267, 0.3835186630021781, 0.24533449299633503, 0.24475579999852926, 0.24159664602484554, 0.2582373139448464, 0.24696325999684632, 0.2893312709638849, 0.2493147529894486, 0.24862971401307732, 0.25333679595496505, 0.24122315598651767, 0.2507089090067893, 0.24644195404835045, 0.25853196694515646, 0.2678471519611776, 0.2515125690260902, 0.3185991980135441, 0.3815767130581662, 0.23832819005474448, 0.26072070107329637, 0.23346387199126184, 0.264013655949384, 0.23430509807076305, 0.24285605200566351, 0.3515391770051792, 0.2507219979306683, 0.26848886092193425, 0.23092354799155146, 0.26951280096545815, 0.2320034260628745, 0.3585730119375512, 0.24059864599257708, 0.24228043504990637, 0.24189007095992565, 0.24142927094362676, 0.23139324295334518, 0.23171573400031775, 0.3611600650474429, 0.23307989409659058, 0.25186249206308275, 0.2322830290067941, 0.2557235589483753, 0.23051331704482436, 0.2513884430518374, 0.4674680879106745, 0.22710607200860977, 0.24512519198469818, 0.24174267600756139, 0.2454549630638212, 0.2425770009867847, 0.24862066004425287, 0.32161483296658844, 0.24614614201709628, 0.26116467907559127, 0.24083472799975425, 0.2554755399469286, 0.22576584701891989, 0.2546053120167926, 0.24088071507867426, 0.2486091060563922, 0.22720168507657945, 0.24557475897017866, 0.24149091891013086, 0.2529303659684956, 0.23994251701515168, 0.25468490505591035, 0.24302110902499408, 0.2546192560112104, 0.22815500001888722, 0.24708477000240237, 0.22719051002059132, 0.2452556990319863, 0.2213391059776768, 0.2252955560106784, 0.22645261697471142, 0.2310735520441085, 0.23191398999188095, 0.2261459679575637, 0.20828472299035639, 0.20652670797426254, 0.2081641189288348, 0.23859637405257672, 0.3098489079857245, 0.28987446997780353, 0.24532116495538503, 0.2469316819915548, 0.22715531999710947, 0.2564259640639648, 0.22846885200124234, 0.2521724870894104, 0.2387927140807733, 0.24144184007309377, 0.25521896802820265, 0.23097602603957057, 0.26851565204560757, 0.24505854689050466, 0.24880324001424015, 0.26261783798690885, 0.2518609329126775, 0.2290902870008722, 0.3570128530263901, 0.27413848298601806, 0.258822206989862, 0.28487950400449336, 0.2373178619891405, 0.2554803730454296, 0.24921532708685845, 0.2254539109999314, 0.2536574490368366, 0.24840166000649333, 0.2505395490443334, 0.23202913999557495, 0.36631637904793024, 0.24063928902614862, 0.236363735049963, 0.24141485593281686, 0.23774402809794992, 0.2450829460285604]
[0.006705468613125215, 0.005913961930094625, 0.0056533203857146545, 0.005886078158668665, 0.005654471455438232, 0.005876359159380875, 0.005281894727059725, 0.005878446409372951, 0.007224968113322658, 0.005647231771780009, 0.005169184227601032, 0.00569078874964775, 0.005344064250080423, 0.005964502499078993, 0.005313016135584225, 0.007191339589718898, 0.0057326834093229, 0.005311952022814446, 0.005935717749790373, 0.007660381046166135, 0.005356091590577059, 0.005897827160714025, 0.006006554864473979, 0.00563376909121871, 0.0058409052726347, 0.005848495273808526, 0.005759267977819863, 0.005644141953036358, 0.007715525931086053, 0.005568397249920632, 0.005718644681936977, 0.005262876638549973, 0.006021815293934196, 0.00756697570481761, 0.005582847362595864, 0.005585261318959634, 0.0053419675678014755, 0.00574034540808167, 0.007656203249511732, 0.006202524636385285, 0.005835393816761842, 0.005926526953275738, 0.005876601588996974, 0.005610845205162398, 0.0059929453868376595, 0.005384245318551125, 0.006008177683477036, 0.005455602227646688, 0.005857285998486491, 0.005301298182034357, 0.005649141271450472, 0.005342024680100043, 0.005724439569431442, 0.007476889385460791, 0.005317272660745816, 0.005930931228001348, 0.005732096250126647, 0.0056837822961493985, 0.005673992794155228, 0.005765169340355153, 0.008716333250049502, 0.005575783931734887, 0.005562631818148392, 0.005490832864201035, 0.005869029862382872, 0.005612801363564689, 0.006575710703724657, 0.005666244386123832, 0.00565067531847903, 0.005757654453521933, 0.005482344454239038, 0.005697929750154303, 0.005600953501098874, 0.0058757265214808285, 0.006087435271844945, 0.005716194750592959, 0.007240890863944183, 0.008672198024049232, 0.005416549773971466, 0.005925470478938554, 0.005305997090710496, 0.006000310362486, 0.005325115865244615, 0.005519455727401443, 0.00798952675011771, 0.005698227225697006, 0.006102019566407596, 0.005248262454353442, 0.00612529093103314, 0.005272805137792602, 0.008149386634944345, 0.005595317348664583, 0.005634428722090846, 0.005625350487440131, 0.00561463420799132, 0.00538123820821733, 0.00538873800000739, 0.00839907128017309, 0.005420462653409083, 0.0058572672572809945, 0.005401930907134747, 0.0059470595104273325, 0.005360774814995916, 0.0058462428616706365, 0.010871350881643593, 0.005281536558339762, 0.00570058586010926, 0.005621922697850265, 0.005708254954972586, 0.00564132560434383, 0.005781875814982625, 0.007479414720153219, 0.005724328884118518, 0.006073597187804448, 0.0056008076279012615, 0.005941291626672758, 0.0052503685353237186, 0.005921053767832386, 0.005601877094852889, 0.005781607117590516, 0.005283760118059987, 0.005711040906283224, 0.00561606788163095, 0.005882101534151061, 0.005580058535236085, 0.005922904768742101, 0.005651653698255676, 0.005921378046772335, 0.005305930232997377, 0.005746157441916334, 0.005283500233037007, 0.005703620907720612, 0.005147421069248298, 0.005239431535132056, 0.005266339929644452, 0.0053738035359095, 0.005393348604462347, 0.005259208557152644, 0.00484383076721759, 0.0048029466970758725, 0.004841026021600809, 0.005548752884943645, 0.007205788557807547, 0.00674126674366985, 0.005705143371055466, 0.0057425972556175535, 0.005282681860397894, 0.00596339451311546, 0.005313229116307962, 0.005864476443939778, 0.005553318932111007, 0.005614926513327762, 0.005935324837865178, 0.005371535489292339, 0.006244550047572269, 0.005699035974197783, 0.005786121860796282, 0.006107391581090904, 0.005857230997969245, 0.00532768109304354, 0.008302624488985815, 0.006375313557814373, 0.006019121092787489, 0.006625104744290543, 0.005519020046259082, 0.005941404024312316, 0.005795705281089731, 0.0052431142093007305, 0.005899010442717131, 0.005776782790848682, 0.005826501140565893, 0.0053960265115249985, 0.008518985559254191, 0.005596262535491828, 0.0054968310476735584, 0.005614298975181787, 0.0055289308859988355, 0.005699603396013032]
[149.13200817055656, 169.09138270086896, 176.8871975709876, 169.89240935023255, 176.85118898924534, 170.1733969755107, 189.32599979262946, 170.11297379619543, 138.4089153495398, 177.07791010050926, 193.45412273380904, 175.7225657097003, 187.12349874628677, 167.65857674708238, 188.2170079067601, 139.05615046043027, 174.43837878326377, 188.25471233645806, 168.47162249844448, 130.54180907886817, 186.70330465582296, 169.55396839383374, 166.484785798685, 177.50106257615144, 171.20633760063063, 170.98415116762206, 173.6331776627182, 177.17484930761418, 129.60879257381202, 179.58488863456236, 174.86660836939558, 190.01015389095647, 166.06288157116091, 132.15319290153627, 179.12006813939269, 179.04265223999758, 187.19694331868754, 174.20554494719573, 130.61304244551948, 161.22467198820854, 171.36803982750163, 168.73288654281328, 170.16637674950522, 178.22626777868066, 166.8628588200229, 185.72705009457025, 166.439817975104, 183.2978208954499, 170.7275349467992, 188.63304150460993, 177.01805494824177, 187.19494197119144, 174.68958976176617, 133.74545863210884, 188.06633847882028, 168.60758649143668, 174.45624713260978, 175.93918061877065, 176.24273351740922, 173.45544267020557, 114.7271417134402, 179.34697833401395, 179.77102074910746, 182.12173357520487, 170.38591103606893, 178.16415284023168, 152.07481670894882, 176.48373982049168, 176.96999803364494, 173.68183660071927, 182.40371584583374, 175.50233924398935, 178.5410287380186, 170.1917194995616, 164.2727939342714, 174.941555288379, 138.1045535404314, 115.31102002362707, 184.61936873641807, 168.7629705614756, 188.46599101057848, 166.65804593242544, 187.78934117221579, 181.17728438974171, 125.16385904650322, 175.493177156985, 163.88016936312837, 190.53925155181565, 163.2575515611194, 189.65237172004393, 122.70862149453403, 178.72087277385023, 177.48028226522956, 177.76670133402823, 178.10599283150057, 185.8308369387861, 185.57220633080115, 119.06078263207591, 184.4860972099995, 170.72808121516562, 185.118991188729, 168.15032677016947, 186.54019885384085, 171.0500271133515, 91.98488862028287, 189.3388389825607, 175.4205663312005, 177.87508895886887, 175.18488713067697, 177.26330124075773, 172.95425083477085, 133.7003010817823, 174.69296755020196, 164.64707307359168, 178.54567884430637, 168.31356931052042, 190.4628205186255, 168.88885647901924, 178.51159228731007, 172.96228879985705, 189.25915969992317, 175.09942870481123, 178.06052581216167, 170.0072659735762, 179.20958959218, 168.83607605468535, 176.93936206824554, 168.8796074327811, 188.4683657883472, 174.02934223579186, 189.26846898711884, 175.32722040596468, 194.27204158101537, 190.86040027332768, 189.88519794762155, 186.0879344244119, 185.41356647568085, 190.1426781487829, 206.44817047859485, 208.20551696083143, 206.56778037093144, 180.22067674223996, 138.77731659451618, 148.3400728711729, 175.28043292889194, 174.13723363966972, 189.29778972619778, 167.68972735254596, 188.20946323031455, 170.5182055993043, 180.07249578584273, 178.09672087895885, 168.48277513310978, 186.16650713625702, 160.13964054764458, 175.46827297238875, 172.82733133836567, 163.7360216260081, 170.72913811094512, 187.6989223896528, 120.44384294708145, 156.85503009875902, 166.1372124907516, 150.94100977977612, 181.19158684299813, 168.3103852065917, 172.5415547375756, 190.72634317713425, 169.51995757773096, 173.1067336622306, 171.6295896756447, 185.32155056395095, 117.38486854384885, 178.6906875183108, 181.923000966754, 178.11662763606574, 180.86679334920711, 175.45080429622817]
Elapsed: 0.256880955429214~0.03569999416334186
Time per graph: 0.005902972216728877~0.0008125848473197725
Speed: 171.87222823120663~18.294725979083122
Total Time: 0.2459
best val loss: 0.42367810010910034 test_score: 0.7442

Testing...
Test loss: 0.6762 score: 0.6744 time: 0.25s
test Score 0.6744
Epoch Time List: [0.934929934097454, 0.8734084410825744, 0.7895889698993415, 0.831338676973246, 0.7911594390170649, 0.8215853300644085, 0.825209322036244, 0.837743850890547, 0.9295234250603244, 0.8328781710006297, 0.7930705251637846, 0.802812666981481, 0.7761545989196748, 0.845190201071091, 0.8117428759578615, 0.8847868650918826, 0.8940225508995354, 0.8316801900509745, 0.9685200039530173, 1.1244132581632584, 0.8194382269866765, 0.8910377660067752, 0.9436331499600783, 0.952214110060595, 0.8943131359992549, 0.8082897100830451, 0.8313664890592918, 0.8067292099585757, 0.9258880651323125, 0.9114917579572648, 0.8459604900563136, 0.8354824390262365, 0.8632762040942907, 0.9406201289966702, 0.8238228260306641, 0.8479417680064216, 0.8005321229575202, 0.8464690271066502, 0.9025670021073893, 0.8822696179850027, 0.8527349609648809, 0.8415270490804687, 0.9569018579786643, 0.7929080701433122, 0.8433375079184771, 0.7994014719733968, 0.8385097770951688, 0.8620888689765707, 0.820860258070752, 0.8168894230620936, 0.8205616631312296, 0.8104007210349664, 0.820833187084645, 0.911158507806249, 0.8206888299901038, 0.8745032419683412, 0.7944751180475578, 0.8232337039662525, 0.7812779799569398, 0.8223919089650735, 1.0431068709585816, 0.8149045129539445, 0.8839968420797959, 0.8134818070102483, 0.8917078251251951, 0.7952356220921502, 0.8587284249952063, 0.7964818381005898, 0.8121213659178466, 0.7932692429749295, 0.9975144992349669, 0.9453789420658723, 0.7985852970741689, 0.8439159189583734, 0.917123589082621, 0.8350905089173466, 1.1162859270116314, 1.1827819219324738, 0.8072354408213869, 0.8269225110998377, 0.7871438489528373, 0.8420832951087505, 0.7935800299746916, 0.8035385539988056, 0.9085831359261647, 0.8807221408933401, 0.8730751849943772, 0.7609814869938418, 0.8914572860812768, 0.7631307900883257, 0.9444005859550089, 0.8444003550102934, 0.8197855530306697, 0.8539528860710561, 0.8081450670724735, 0.8044020489323884, 0.9311172891175374, 1.0442269189516082, 0.7877255289349705, 0.8956932190340012, 0.8190791549859568, 0.8330680719809607, 0.8476198159623891, 0.8231751511339098, 1.1681761952349916, 0.7716173249064013, 0.8375209009973332, 0.8043192928889766, 0.8178725340403616, 0.8084529599873349, 0.8377044410444796, 0.879176834016107, 0.9399016150273383, 0.8903015339747071, 0.8298758059972897, 0.9426339030032977, 0.7604701339732856, 0.8156968308612704, 0.7746655847877264, 0.8174864249303937, 0.8495750930160284, 0.8128429950447753, 0.7782882230821997, 0.825303470948711, 0.7789522190578282, 0.8666004821425304, 0.7841260340064764, 0.8205498568713665, 0.879722467972897, 0.8555041089421138, 0.8123883150983602, 0.8404221548698843, 0.8266603139927611, 0.8077951190061867, 0.7743873200379312, 0.9101964210858569, 0.7854709011735395, 0.7689537099795416, 0.7995198589051142, 0.7062910820823163, 0.6964870380470529, 0.7983439159579575, 1.011531709926203, 0.9061950880568475, 0.8135238119866699, 0.8402997620869428, 0.832073159981519, 0.8267606641165912, 0.7940550860948861, 0.8237322570057586, 0.8324226050172001, 0.949220440001227, 0.864581958972849, 0.774267187109217, 0.8605050509795547, 0.9282360540237278, 0.8906164659420028, 0.9373113508336246, 0.8617976409150288, 0.9508540499955416, 0.97735298902262, 1.0128026180900633, 0.8532112939283252, 0.862465925863944, 0.8192725138505921, 0.9002804489573464, 0.9573603939497843, 0.7904387150192633, 0.8477474910905585, 0.8305368379224092, 0.8292796679306775, 0.7911261979024857, 0.9373582539847121, 0.8459689399460331, 0.8385870000347495, 0.8278010259382427, 0.8365060550859198, 0.8285633770283312]
Total Epoch List: [91, 87]
Total Time List: [0.3590319160139188, 0.24590896198060364]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335d80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.21s
Epoch 4/1000, LR 0.000060
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.21s
Epoch 6/1000, LR 0.000120
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.21s
Epoch 8/1000, LR 0.000180
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.49s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.36s
Val loss: 0.6928 score: 0.6591 time: 0.23s
Test loss: 0.6928 score: 0.7442 time: 0.34s
Epoch 11/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.21s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.4884 time: 0.21s
Epoch 19/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.23s
Epoch 20/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.21s
Epoch 29/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.4884 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6884 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.4884 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.4884 time: 0.26s
Epoch 34/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6871 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6851;  Loss pred: 0.6851; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4884 time: 0.22s
Epoch 36/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.4884 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6847 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6860 score: 0.4884 time: 0.22s
Epoch 38/1000, LR 0.000270
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6837 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6850 score: 0.4884 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6806;  Loss pred: 0.6806; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6826 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.4884 time: 0.22s
Epoch 40/1000, LR 0.000269
Train loss: 0.6783;  Loss pred: 0.6783; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6815 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6828 score: 0.4884 time: 0.32s
Epoch 41/1000, LR 0.000269
Train loss: 0.6772;  Loss pred: 0.6772; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6802 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6815 score: 0.4884 time: 0.21s
Epoch 42/1000, LR 0.000269
Train loss: 0.6747;  Loss pred: 0.6747; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6787 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6800 score: 0.4884 time: 0.24s
Epoch 43/1000, LR 0.000269
Train loss: 0.6726;  Loss pred: 0.6726; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6772 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6784 score: 0.4884 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6766 score: 0.4884 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.58s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6735 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6746 score: 0.4884 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6668;  Loss pred: 0.6668; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6714 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6724 score: 0.4884 time: 0.31s
Epoch 47/1000, LR 0.000269
Train loss: 0.6616;  Loss pred: 0.6616; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6692 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6700 score: 0.4884 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6598;  Loss pred: 0.6598; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6667 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6673 score: 0.4884 time: 0.30s
Epoch 49/1000, LR 0.000269
Train loss: 0.6551;  Loss pred: 0.6551; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6640 score: 0.5000 time: 0.24s
Test loss: 0.6645 score: 0.5116 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6508;  Loss pred: 0.6508; Loss self: 0.0000; time: 0.43s
Val loss: 0.6610 score: 0.5227 time: 0.21s
Test loss: 0.6615 score: 0.5116 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.6477;  Loss pred: 0.6477; Loss self: 0.0000; time: 0.37s
Val loss: 0.6578 score: 0.5227 time: 0.25s
Test loss: 0.6581 score: 0.5116 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 0.39s
Val loss: 0.6543 score: 0.5682 time: 0.22s
Test loss: 0.6544 score: 0.5116 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.45s
Val loss: 0.6505 score: 0.5682 time: 0.23s
Test loss: 0.6506 score: 0.5349 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6334;  Loss pred: 0.6334; Loss self: 0.0000; time: 0.41s
Val loss: 0.6463 score: 0.5909 time: 0.24s
Test loss: 0.6465 score: 0.5814 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6272;  Loss pred: 0.6272; Loss self: 0.0000; time: 0.44s
Val loss: 0.6419 score: 0.6136 time: 0.23s
Test loss: 0.6420 score: 0.6279 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6190;  Loss pred: 0.6190; Loss self: 0.0000; time: 0.53s
Val loss: 0.6370 score: 0.6364 time: 0.23s
Test loss: 0.6372 score: 0.6279 time: 0.21s
Epoch 57/1000, LR 0.000269
Train loss: 0.6118;  Loss pred: 0.6118; Loss self: 0.0000; time: 0.40s
Val loss: 0.6318 score: 0.6591 time: 0.32s
Test loss: 0.6319 score: 0.6279 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6027;  Loss pred: 0.6027; Loss self: 0.0000; time: 0.47s
Val loss: 0.6263 score: 0.6818 time: 0.22s
Test loss: 0.6263 score: 0.6279 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.5966;  Loss pred: 0.5966; Loss self: 0.0000; time: 0.46s
Val loss: 0.6203 score: 0.7045 time: 0.24s
Test loss: 0.6205 score: 0.6512 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.5886;  Loss pred: 0.5886; Loss self: 0.0000; time: 0.41s
Val loss: 0.6139 score: 0.7045 time: 0.23s
Test loss: 0.6142 score: 0.6744 time: 0.22s
Epoch 61/1000, LR 0.000268
Train loss: 0.5773;  Loss pred: 0.5773; Loss self: 0.0000; time: 0.45s
Val loss: 0.6070 score: 0.7273 time: 0.23s
Test loss: 0.6076 score: 0.6744 time: 0.22s
Epoch 62/1000, LR 0.000268
Train loss: 0.5692;  Loss pred: 0.5692; Loss self: 0.0000; time: 0.39s
Val loss: 0.5998 score: 0.7727 time: 0.24s
Test loss: 0.6008 score: 0.6977 time: 0.22s
Epoch 63/1000, LR 0.000268
Train loss: 0.5566;  Loss pred: 0.5566; Loss self: 0.0000; time: 0.40s
Val loss: 0.5923 score: 0.7727 time: 0.22s
Test loss: 0.5942 score: 0.6977 time: 0.33s
Epoch 64/1000, LR 0.000268
Train loss: 0.5473;  Loss pred: 0.5473; Loss self: 0.0000; time: 0.42s
Val loss: 0.5845 score: 0.7727 time: 0.22s
Test loss: 0.5873 score: 0.6977 time: 0.21s
Epoch 65/1000, LR 0.000268
Train loss: 0.5378;  Loss pred: 0.5378; Loss self: 0.0000; time: 0.39s
Val loss: 0.5764 score: 0.7727 time: 0.26s
Test loss: 0.5800 score: 0.7209 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.5198;  Loss pred: 0.5198; Loss self: 0.0000; time: 0.51s
Val loss: 0.5681 score: 0.7727 time: 0.22s
Test loss: 0.5727 score: 0.7209 time: 0.20s
Epoch 67/1000, LR 0.000268
Train loss: 0.5159;  Loss pred: 0.5159; Loss self: 0.0000; time: 0.43s
Val loss: 0.5593 score: 0.7727 time: 0.25s
Test loss: 0.5649 score: 0.7442 time: 0.21s
Epoch 68/1000, LR 0.000268
Train loss: 0.5030;  Loss pred: 0.5030; Loss self: 0.0000; time: 0.40s
Val loss: 0.5503 score: 0.7727 time: 0.21s
Test loss: 0.5568 score: 0.7442 time: 0.23s
Epoch 69/1000, LR 0.000268
Train loss: 0.4890;  Loss pred: 0.4890; Loss self: 0.0000; time: 0.35s
Val loss: 0.5409 score: 0.7727 time: 0.23s
Test loss: 0.5481 score: 0.7907 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.4669;  Loss pred: 0.4669; Loss self: 0.0000; time: 0.39s
Val loss: 0.5312 score: 0.8182 time: 0.23s
Test loss: 0.5393 score: 0.7907 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.4644;  Loss pred: 0.4644; Loss self: 0.0000; time: 0.47s
Val loss: 0.5215 score: 0.8409 time: 0.22s
Test loss: 0.5307 score: 0.7674 time: 0.25s
Epoch 72/1000, LR 0.000267
Train loss: 0.4466;  Loss pred: 0.4466; Loss self: 0.0000; time: 0.39s
Val loss: 0.5120 score: 0.8409 time: 0.23s
Test loss: 0.5230 score: 0.7674 time: 0.21s
Epoch 73/1000, LR 0.000267
Train loss: 0.4370;  Loss pred: 0.4370; Loss self: 0.0000; time: 0.44s
Val loss: 0.5023 score: 0.8409 time: 0.23s
Test loss: 0.5149 score: 0.7907 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.4158;  Loss pred: 0.4158; Loss self: 0.0000; time: 0.40s
Val loss: 0.4922 score: 0.8409 time: 0.24s
Test loss: 0.5063 score: 0.7907 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.4060;  Loss pred: 0.4060; Loss self: 0.0000; time: 0.41s
Val loss: 0.4820 score: 0.8636 time: 0.21s
Test loss: 0.4978 score: 0.8140 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.3921;  Loss pred: 0.3921; Loss self: 0.0000; time: 0.36s
Val loss: 0.4717 score: 0.8636 time: 0.24s
Test loss: 0.4890 score: 0.8140 time: 0.21s
Epoch 77/1000, LR 0.000267
Train loss: 0.3805;  Loss pred: 0.3805; Loss self: 0.0000; time: 0.41s
Val loss: 0.4617 score: 0.9091 time: 0.36s
Test loss: 0.4799 score: 0.8140 time: 0.20s
Epoch 78/1000, LR 0.000267
Train loss: 0.3724;  Loss pred: 0.3724; Loss self: 0.0000; time: 0.40s
Val loss: 0.4518 score: 0.8636 time: 0.23s
Test loss: 0.4712 score: 0.8140 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.3505;  Loss pred: 0.3505; Loss self: 0.0000; time: 0.37s
Val loss: 0.4423 score: 0.8864 time: 0.25s
Test loss: 0.4630 score: 0.8140 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.3418;  Loss pred: 0.3418; Loss self: 0.0000; time: 0.40s
Val loss: 0.4330 score: 0.8864 time: 0.22s
Test loss: 0.4546 score: 0.8140 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.3148;  Loss pred: 0.3148; Loss self: 0.0000; time: 0.35s
Val loss: 0.4238 score: 0.9091 time: 0.24s
Test loss: 0.4477 score: 0.8140 time: 0.22s
Epoch 82/1000, LR 0.000267
Train loss: 0.3107;  Loss pred: 0.3107; Loss self: 0.0000; time: 0.39s
Val loss: 0.4148 score: 0.9091 time: 0.28s
Test loss: 0.4414 score: 0.8140 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.3132;  Loss pred: 0.3132; Loss self: 0.0000; time: 0.49s
Val loss: 0.4062 score: 0.9091 time: 0.22s
Test loss: 0.4337 score: 0.8140 time: 0.22s
Epoch 84/1000, LR 0.000266
Train loss: 0.2976;  Loss pred: 0.2976; Loss self: 0.0000; time: 0.38s
Val loss: 0.3978 score: 0.9091 time: 0.23s
Test loss: 0.4277 score: 0.8140 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.2728;  Loss pred: 0.2728; Loss self: 0.0000; time: 0.46s
Val loss: 0.3892 score: 0.9091 time: 0.21s
Test loss: 0.4240 score: 0.8140 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.2689;  Loss pred: 0.2689; Loss self: 0.0000; time: 0.51s
Val loss: 0.3812 score: 0.9091 time: 0.24s
Test loss: 0.4180 score: 0.8140 time: 0.20s
Epoch 87/1000, LR 0.000266
Train loss: 0.2498;  Loss pred: 0.2498; Loss self: 0.0000; time: 0.40s
Val loss: 0.3733 score: 0.9091 time: 0.22s
Test loss: 0.4127 score: 0.8140 time: 0.23s
Epoch 88/1000, LR 0.000266
Train loss: 0.2386;  Loss pred: 0.2386; Loss self: 0.0000; time: 0.36s
Val loss: 0.3653 score: 0.9091 time: 0.25s
Test loss: 0.4089 score: 0.8140 time: 0.21s
Epoch 89/1000, LR 0.000266
Train loss: 0.2268;  Loss pred: 0.2268; Loss self: 0.0000; time: 0.40s
Val loss: 0.3571 score: 0.9091 time: 0.26s
Test loss: 0.4079 score: 0.8140 time: 0.23s
Epoch 90/1000, LR 0.000266
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 0.36s
Val loss: 0.3493 score: 0.9091 time: 0.23s
Test loss: 0.4062 score: 0.8140 time: 0.21s
Epoch 91/1000, LR 0.000266
Train loss: 0.2056;  Loss pred: 0.2056; Loss self: 0.0000; time: 0.39s
Val loss: 0.3418 score: 0.9091 time: 0.25s
Test loss: 0.4043 score: 0.8140 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.1941;  Loss pred: 0.1941; Loss self: 0.0000; time: 0.41s
Val loss: 0.3343 score: 0.9091 time: 0.23s
Test loss: 0.4038 score: 0.8140 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 0.1835;  Loss pred: 0.1835; Loss self: 0.0000; time: 0.45s
Val loss: 0.3271 score: 0.9091 time: 0.24s
Test loss: 0.4059 score: 0.8140 time: 0.32s
Epoch 94/1000, LR 0.000265
Train loss: 0.1767;  Loss pred: 0.1767; Loss self: 0.0000; time: 0.39s
Val loss: 0.3203 score: 0.9091 time: 0.22s
Test loss: 0.4107 score: 0.8140 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.1655;  Loss pred: 0.1655; Loss self: 0.0000; time: 0.33s
Val loss: 0.3141 score: 0.8864 time: 0.24s
Test loss: 0.4157 score: 0.8140 time: 0.20s
Epoch 96/1000, LR 0.000265
Train loss: 0.1460;  Loss pred: 0.1460; Loss self: 0.0000; time: 0.37s
Val loss: 0.3085 score: 0.8864 time: 0.21s
Test loss: 0.4201 score: 0.8140 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.1441;  Loss pred: 0.1441; Loss self: 0.0000; time: 0.34s
Val loss: 0.3030 score: 0.8864 time: 0.24s
Test loss: 0.4214 score: 0.8140 time: 0.22s
Epoch 98/1000, LR 0.000265
Train loss: 0.1337;  Loss pred: 0.1337; Loss self: 0.0000; time: 0.39s
Val loss: 0.2975 score: 0.8864 time: 0.21s
Test loss: 0.4184 score: 0.8372 time: 0.23s
Epoch 99/1000, LR 0.000265
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.34s
Val loss: 0.2924 score: 0.9091 time: 0.24s
Test loss: 0.4120 score: 0.8372 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.1317;  Loss pred: 0.1317; Loss self: 0.0000; time: 0.40s
Val loss: 0.2888 score: 0.9091 time: 0.21s
Test loss: 0.3991 score: 0.8372 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.1243;  Loss pred: 0.1243; Loss self: 0.0000; time: 0.34s
Val loss: 0.2868 score: 0.9091 time: 0.25s
Test loss: 0.3895 score: 0.8605 time: 0.34s
Epoch 102/1000, LR 0.000264
Train loss: 0.1040;  Loss pred: 0.1040; Loss self: 0.0000; time: 0.46s
Val loss: 0.2874 score: 0.9091 time: 0.24s
Test loss: 0.3783 score: 0.8605 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 103/1000, LR 0.000264
Train loss: 0.1234;  Loss pred: 0.1234; Loss self: 0.0000; time: 0.38s
Val loss: 0.2880 score: 0.9318 time: 0.21s
Test loss: 0.3726 score: 0.8605 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 100,   Train_Loss: 0.1243,   Val_Loss: 0.2868,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2868,   Test_Precision: 0.8571,   Test_Recall: 0.8571,   Test_accuracy: 0.8571,   Test_Score: 0.8605,   Test_loss: 0.3895


[0.29504061897750944, 0.2602143249241635, 0.24874609697144479, 0.2589874389814213, 0.2487967440392822, 0.2585598030127585, 0.23240336799062788, 0.25865164201240987, 0.31789859698619694, 0.24847819795832038, 0.22744410601444542, 0.250394704984501, 0.2351388270035386, 0.2624381099594757, 0.23377270996570587, 0.31641894194763154, 0.2522380700102076, 0.23372588900383562, 0.2611715809907764, 0.33705676603130996, 0.2356680299853906, 0.2595043950714171, 0.2642884140368551, 0.24788584001362324, 0.2569998319959268, 0.2573337920475751, 0.25340779102407396, 0.24834224593359977, 0.3394831409677863, 0.24500947899650782, 0.25162036600522697, 0.2315665720961988, 0.26495987293310463, 0.3329469310119748, 0.24564528395421803, 0.24575149803422391, 0.23504657298326492, 0.25257519795559347, 0.3368729429785162, 0.27291108400095254, 0.25675732793752104, 0.2607671859441325, 0.25857046991586685, 0.2468771890271455, 0.26368959702085704, 0.23690679401624948, 0.2643598180729896, 0.24004649801645428, 0.2577205839334056, 0.2332571200095117, 0.24856221594382077, 0.23504908592440188, 0.25187534105498344, 0.3289831329602748, 0.2339599970728159, 0.2609609740320593, 0.25221223500557244, 0.25008642103057355, 0.24965568294283003, 0.2536674509756267, 0.3835186630021781, 0.24533449299633503, 0.24475579999852926, 0.24159664602484554, 0.2582373139448464, 0.24696325999684632, 0.2893312709638849, 0.2493147529894486, 0.24862971401307732, 0.25333679595496505, 0.24122315598651767, 0.2507089090067893, 0.24644195404835045, 0.25853196694515646, 0.2678471519611776, 0.2515125690260902, 0.3185991980135441, 0.3815767130581662, 0.23832819005474448, 0.26072070107329637, 0.23346387199126184, 0.264013655949384, 0.23430509807076305, 0.24285605200566351, 0.3515391770051792, 0.2507219979306683, 0.26848886092193425, 0.23092354799155146, 0.26951280096545815, 0.2320034260628745, 0.3585730119375512, 0.24059864599257708, 0.24228043504990637, 0.24189007095992565, 0.24142927094362676, 0.23139324295334518, 0.23171573400031775, 0.3611600650474429, 0.23307989409659058, 0.25186249206308275, 0.2322830290067941, 0.2557235589483753, 0.23051331704482436, 0.2513884430518374, 0.4674680879106745, 0.22710607200860977, 0.24512519198469818, 0.24174267600756139, 0.2454549630638212, 0.2425770009867847, 0.24862066004425287, 0.32161483296658844, 0.24614614201709628, 0.26116467907559127, 0.24083472799975425, 0.2554755399469286, 0.22576584701891989, 0.2546053120167926, 0.24088071507867426, 0.2486091060563922, 0.22720168507657945, 0.24557475897017866, 0.24149091891013086, 0.2529303659684956, 0.23994251701515168, 0.25468490505591035, 0.24302110902499408, 0.2546192560112104, 0.22815500001888722, 0.24708477000240237, 0.22719051002059132, 0.2452556990319863, 0.2213391059776768, 0.2252955560106784, 0.22645261697471142, 0.2310735520441085, 0.23191398999188095, 0.2261459679575637, 0.20828472299035639, 0.20652670797426254, 0.2081641189288348, 0.23859637405257672, 0.3098489079857245, 0.28987446997780353, 0.24532116495538503, 0.2469316819915548, 0.22715531999710947, 0.2564259640639648, 0.22846885200124234, 0.2521724870894104, 0.2387927140807733, 0.24144184007309377, 0.25521896802820265, 0.23097602603957057, 0.26851565204560757, 0.24505854689050466, 0.24880324001424015, 0.26261783798690885, 0.2518609329126775, 0.2290902870008722, 0.3570128530263901, 0.27413848298601806, 0.258822206989862, 0.28487950400449336, 0.2373178619891405, 0.2554803730454296, 0.24921532708685845, 0.2254539109999314, 0.2536574490368366, 0.24840166000649333, 0.2505395490443334, 0.23202913999557495, 0.36631637904793024, 0.24063928902614862, 0.236363735049963, 0.24141485593281686, 0.23774402809794992, 0.2450829460285604, 0.23343996005132794, 0.21146657690405846, 0.21342027396894991, 0.2408499310258776, 0.21301097807008773, 0.23225328791886568, 0.21942803997080773, 0.22717988293152303, 0.22887853800784796, 0.34632765292190015, 0.2576874940423295, 0.24011771008372307, 0.21224787400569767, 0.23357777204364538, 0.2100977380760014, 0.23425755207426846, 0.2134373129811138, 0.21625426993705332, 0.22970324999187142, 0.22179712902288884, 0.23142949200700969, 0.22646188596263528, 0.22983087098691612, 0.20953761192504317, 0.24225726397708058, 0.2055456480011344, 0.23733494197949767, 0.2117473150137812, 0.2299140029354021, 0.22098916105460376, 0.22163794294465333, 0.22442969400435686, 0.2681317450478673, 0.23226470698136836, 0.2197808000491932, 0.22365354897920042, 0.22863385803066194, 0.21669122006278485, 0.22689053404610604, 0.32488772191572934, 0.2137968559982255, 0.24883113405667245, 0.21061394200660288, 0.2345699240686372, 0.2253027129918337, 0.31389065599069, 0.23523939901497215, 0.30684520304203033, 0.2320533050224185, 0.23587550700176507, 0.2317031710408628, 0.24343927600421011, 0.22057231096550822, 0.23160908196587116, 0.22011594590730965, 0.21795983100309968, 0.22346986597403884, 0.24506019009277225, 0.21424982999451458, 0.2289751839125529, 0.22460490697994828, 0.2241441309452057, 0.3392280759289861, 0.2158622050192207, 0.22141596802975982, 0.20883283100556582, 0.21759701205883175, 0.23356303991749883, 0.2236470429925248, 0.23636376101057976, 0.25927165895700455, 0.21492632199078798, 0.22558775101788342, 0.22204563196282834, 0.24038947897497565, 0.21245629095938057, 0.20531307195778936, 0.22409213695209473, 0.22330381092615426, 0.23121986002661288, 0.22589480597525835, 0.2359550290275365, 0.22553542000241578, 0.22196933708619326, 0.22384346509352326, 0.20554414601065218, 0.23913333902601153, 0.21475533896591514, 0.24074552697129548, 0.2101571650709957, 0.2417751600733027, 0.2291194290155545, 0.32603182701859623, 0.2338612659368664, 0.20521263009868562, 0.2296436660690233, 0.2200032490072772, 0.23065063101239502, 0.22026603005360812, 0.23169910500291735, 0.34149573592003435, 0.2454241239465773, 0.23202819796279073]
[0.006705468613125215, 0.005913961930094625, 0.0056533203857146545, 0.005886078158668665, 0.005654471455438232, 0.005876359159380875, 0.005281894727059725, 0.005878446409372951, 0.007224968113322658, 0.005647231771780009, 0.005169184227601032, 0.00569078874964775, 0.005344064250080423, 0.005964502499078993, 0.005313016135584225, 0.007191339589718898, 0.0057326834093229, 0.005311952022814446, 0.005935717749790373, 0.007660381046166135, 0.005356091590577059, 0.005897827160714025, 0.006006554864473979, 0.00563376909121871, 0.0058409052726347, 0.005848495273808526, 0.005759267977819863, 0.005644141953036358, 0.007715525931086053, 0.005568397249920632, 0.005718644681936977, 0.005262876638549973, 0.006021815293934196, 0.00756697570481761, 0.005582847362595864, 0.005585261318959634, 0.0053419675678014755, 0.00574034540808167, 0.007656203249511732, 0.006202524636385285, 0.005835393816761842, 0.005926526953275738, 0.005876601588996974, 0.005610845205162398, 0.0059929453868376595, 0.005384245318551125, 0.006008177683477036, 0.005455602227646688, 0.005857285998486491, 0.005301298182034357, 0.005649141271450472, 0.005342024680100043, 0.005724439569431442, 0.007476889385460791, 0.005317272660745816, 0.005930931228001348, 0.005732096250126647, 0.0056837822961493985, 0.005673992794155228, 0.005765169340355153, 0.008716333250049502, 0.005575783931734887, 0.005562631818148392, 0.005490832864201035, 0.005869029862382872, 0.005612801363564689, 0.006575710703724657, 0.005666244386123832, 0.00565067531847903, 0.005757654453521933, 0.005482344454239038, 0.005697929750154303, 0.005600953501098874, 0.0058757265214808285, 0.006087435271844945, 0.005716194750592959, 0.007240890863944183, 0.008672198024049232, 0.005416549773971466, 0.005925470478938554, 0.005305997090710496, 0.006000310362486, 0.005325115865244615, 0.005519455727401443, 0.00798952675011771, 0.005698227225697006, 0.006102019566407596, 0.005248262454353442, 0.00612529093103314, 0.005272805137792602, 0.008149386634944345, 0.005595317348664583, 0.005634428722090846, 0.005625350487440131, 0.00561463420799132, 0.00538123820821733, 0.00538873800000739, 0.00839907128017309, 0.005420462653409083, 0.0058572672572809945, 0.005401930907134747, 0.0059470595104273325, 0.005360774814995916, 0.0058462428616706365, 0.010871350881643593, 0.005281536558339762, 0.00570058586010926, 0.005621922697850265, 0.005708254954972586, 0.00564132560434383, 0.005781875814982625, 0.007479414720153219, 0.005724328884118518, 0.006073597187804448, 0.0056008076279012615, 0.005941291626672758, 0.0052503685353237186, 0.005921053767832386, 0.005601877094852889, 0.005781607117590516, 0.005283760118059987, 0.005711040906283224, 0.00561606788163095, 0.005882101534151061, 0.005580058535236085, 0.005922904768742101, 0.005651653698255676, 0.005921378046772335, 0.005305930232997377, 0.005746157441916334, 0.005283500233037007, 0.005703620907720612, 0.005147421069248298, 0.005239431535132056, 0.005266339929644452, 0.0053738035359095, 0.005393348604462347, 0.005259208557152644, 0.00484383076721759, 0.0048029466970758725, 0.004841026021600809, 0.005548752884943645, 0.007205788557807547, 0.00674126674366985, 0.005705143371055466, 0.0057425972556175535, 0.005282681860397894, 0.00596339451311546, 0.005313229116307962, 0.005864476443939778, 0.005553318932111007, 0.005614926513327762, 0.005935324837865178, 0.005371535489292339, 0.006244550047572269, 0.005699035974197783, 0.005786121860796282, 0.006107391581090904, 0.005857230997969245, 0.00532768109304354, 0.008302624488985815, 0.006375313557814373, 0.006019121092787489, 0.006625104744290543, 0.005519020046259082, 0.005941404024312316, 0.005795705281089731, 0.0052431142093007305, 0.005899010442717131, 0.005776782790848682, 0.005826501140565893, 0.0053960265115249985, 0.008518985559254191, 0.005596262535491828, 0.0054968310476735584, 0.005614298975181787, 0.0055289308859988355, 0.005699603396013032, 0.00542883628026344, 0.004917827369861825, 0.004963262185324417, 0.005601161186648316, 0.004953743676048552, 0.005401239253927109, 0.005102977673739715, 0.005283253091430768, 0.005322756697856929, 0.008054131463300003, 0.005992732419589058, 0.005584132792644723, 0.004935997069899945, 0.005432041210317334, 0.004885993908744219, 0.005447850048238802, 0.004963658441421251, 0.005029169068303566, 0.005341936046322591, 0.005158072767974159, 0.005382081209465341, 0.005266555487503146, 0.00534490397643991, 0.00487296771918705, 0.005633889859932107, 0.004780131348863591, 0.0055194172553371555, 0.004924356163111191, 0.005346837277567491, 0.005139282815223343, 0.005154370766154728, 0.005219295209403648, 0.006235621977857379, 0.0054015048135201945, 0.005111181396492865, 0.005201245325097684, 0.005317066465829348, 0.005039330699134531, 0.005276524047583862, 0.0075555284166448685, 0.004972019906935477, 0.005786770559457499, 0.004897998651316346, 0.005455114513224121, 0.005239597976554272, 0.007299782697457907, 0.005470683698022608, 0.0071359349544658215, 0.005396588488893453, 0.005485476907017792, 0.0053884458381596, 0.005661378511725817, 0.005129588627104842, 0.005386257720136538, 0.005118975486216503, 0.005068833279141853, 0.005196973627303229, 0.0056990741882040055, 0.004982554185918944, 0.005325004277036114, 0.005223369929766239, 0.005212654208028039, 0.007889025021604327, 0.005020051279516761, 0.005149208558831624, 0.004856577465245717, 0.005060395629275157, 0.005431698602732531, 0.005201094023081972, 0.005496831651408832, 0.006029573464116385, 0.004998286557925302, 0.005246226767857754, 0.005163851906112287, 0.005590452999418038, 0.004940843975799548, 0.00477472260366952, 0.005211445045397552, 0.005193111882003588, 0.005377206047130532, 0.005253367580819962, 0.005487326256454338, 0.005245009767498041, 0.005162077606655657, 0.005205661978919146, 0.0047800964188523766, 0.005561240442465384, 0.004994310208509655, 0.005598733185378965, 0.004887375931883621, 0.005622678141239597, 0.00532835881431522, 0.007582135512060378, 0.005438634091555033, 0.004772386746481061, 0.005340550373698217, 0.0051163546280762135, 0.005363968163078954, 0.005122465815200189, 0.005388351279137613, 0.007941761300465915, 0.005707537766199472, 0.005396004603785831]
[149.13200817055656, 169.09138270086896, 176.8871975709876, 169.89240935023255, 176.85118898924534, 170.1733969755107, 189.32599979262946, 170.11297379619543, 138.4089153495398, 177.07791010050926, 193.45412273380904, 175.7225657097003, 187.12349874628677, 167.65857674708238, 188.2170079067601, 139.05615046043027, 174.43837878326377, 188.25471233645806, 168.47162249844448, 130.54180907886817, 186.70330465582296, 169.55396839383374, 166.484785798685, 177.50106257615144, 171.20633760063063, 170.98415116762206, 173.6331776627182, 177.17484930761418, 129.60879257381202, 179.58488863456236, 174.86660836939558, 190.01015389095647, 166.06288157116091, 132.15319290153627, 179.12006813939269, 179.04265223999758, 187.19694331868754, 174.20554494719573, 130.61304244551948, 161.22467198820854, 171.36803982750163, 168.73288654281328, 170.16637674950522, 178.22626777868066, 166.8628588200229, 185.72705009457025, 166.439817975104, 183.2978208954499, 170.7275349467992, 188.63304150460993, 177.01805494824177, 187.19494197119144, 174.68958976176617, 133.74545863210884, 188.06633847882028, 168.60758649143668, 174.45624713260978, 175.93918061877065, 176.24273351740922, 173.45544267020557, 114.7271417134402, 179.34697833401395, 179.77102074910746, 182.12173357520487, 170.38591103606893, 178.16415284023168, 152.07481670894882, 176.48373982049168, 176.96999803364494, 173.68183660071927, 182.40371584583374, 175.50233924398935, 178.5410287380186, 170.1917194995616, 164.2727939342714, 174.941555288379, 138.1045535404314, 115.31102002362707, 184.61936873641807, 168.7629705614756, 188.46599101057848, 166.65804593242544, 187.78934117221579, 181.17728438974171, 125.16385904650322, 175.493177156985, 163.88016936312837, 190.53925155181565, 163.2575515611194, 189.65237172004393, 122.70862149453403, 178.72087277385023, 177.48028226522956, 177.76670133402823, 178.10599283150057, 185.8308369387861, 185.57220633080115, 119.06078263207591, 184.4860972099995, 170.72808121516562, 185.118991188729, 168.15032677016947, 186.54019885384085, 171.0500271133515, 91.98488862028287, 189.3388389825607, 175.4205663312005, 177.87508895886887, 175.18488713067697, 177.26330124075773, 172.95425083477085, 133.7003010817823, 174.69296755020196, 164.64707307359168, 178.54567884430637, 168.31356931052042, 190.4628205186255, 168.88885647901924, 178.51159228731007, 172.96228879985705, 189.25915969992317, 175.09942870481123, 178.06052581216167, 170.0072659735762, 179.20958959218, 168.83607605468535, 176.93936206824554, 168.8796074327811, 188.4683657883472, 174.02934223579186, 189.26846898711884, 175.32722040596468, 194.27204158101537, 190.86040027332768, 189.88519794762155, 186.0879344244119, 185.41356647568085, 190.1426781487829, 206.44817047859485, 208.20551696083143, 206.56778037093144, 180.22067674223996, 138.77731659451618, 148.3400728711729, 175.28043292889194, 174.13723363966972, 189.29778972619778, 167.68972735254596, 188.20946323031455, 170.5182055993043, 180.07249578584273, 178.09672087895885, 168.48277513310978, 186.16650713625702, 160.13964054764458, 175.46827297238875, 172.82733133836567, 163.7360216260081, 170.72913811094512, 187.6989223896528, 120.44384294708145, 156.85503009875902, 166.1372124907516, 150.94100977977612, 181.19158684299813, 168.3103852065917, 172.5415547375756, 190.72634317713425, 169.51995757773096, 173.1067336622306, 171.6295896756447, 185.32155056395095, 117.38486854384885, 178.6906875183108, 181.923000966754, 178.11662763606574, 180.86679334920711, 175.45080429622817, 184.20153940458744, 203.3418265407915, 201.48038984457486, 178.534408612224, 201.8675299723358, 185.1426965159754, 195.96401629308923, 189.27732264463373, 187.8725736990053, 124.15988049818496, 166.8687887233539, 179.07883589680648, 202.5933131318229, 184.09285962349705, 204.6666489310087, 183.55864995279802, 201.46430537103367, 198.84000446565202, 187.19804792279453, 193.87085932732035, 185.80173005218188, 189.87742602786022, 187.09410017615917, 205.21375425134735, 177.49725764288385, 209.19927236680138, 181.1785472520712, 203.07223256739488, 187.02645098168082, 194.57967890730723, 194.0101023710449, 191.5967501125998, 160.36892607521565, 185.13359416008623, 195.64948344157168, 192.26164841229038, 188.07363165884775, 198.43905068023076, 189.5187041662216, 132.35341657864655, 201.12550205301847, 172.80795734430302, 204.1650215096013, 183.31420863408655, 190.85433738899795, 136.99037922707512, 182.7925091632427, 140.13580650341808, 185.30225197975872, 182.29955516186016, 185.58226806665752, 176.63542508751277, 194.94740664309438, 185.6576591687207, 195.35158992119185, 197.2840582693812, 192.41967955086812, 175.46709640485273, 200.70027594001323, 187.7932763946244, 191.4472866073173, 191.84084730959023, 126.75837600482576, 199.20115240262282, 194.20460223636854, 205.9063213046897, 197.6130076104817, 184.10447138891854, 192.26724138461887, 181.92298098554667, 165.84921071967517, 200.06856117810938, 190.61318624782595, 193.6538882566194, 178.87638087720248, 202.39457163554246, 209.43625064867004, 191.88535833897785, 192.56276828262435, 185.97018437365543, 190.35408899445733, 182.238116208923, 190.65741425244607, 193.7204506012585, 192.09852734380394, 209.20080106670395, 179.81599794967406, 200.22785094448682, 178.61183358612087, 204.60877451155977, 177.8511902834147, 187.67504870606504, 131.88896431742342, 183.8696965388375, 209.53875976991054, 187.2466187988639, 195.45165898244375, 186.42914528895957, 195.21848189452865, 185.58552481011344, 125.91665276332765, 175.20690023675107, 185.32230296808885]
Elapsed: 0.24826005763114095~0.03508741565703529
Time per graph: 0.005728512412049436~0.0007941063513990866
Speed: 177.19649321506424~19.40616408714925
Total Time: 0.2336
best val loss: 0.28681647777557373 test_score: 0.8605

Testing...
Test loss: 0.3726 score: 0.8605 time: 0.25s
test Score 0.8605
Epoch Time List: [0.934929934097454, 0.8734084410825744, 0.7895889698993415, 0.831338676973246, 0.7911594390170649, 0.8215853300644085, 0.825209322036244, 0.837743850890547, 0.9295234250603244, 0.8328781710006297, 0.7930705251637846, 0.802812666981481, 0.7761545989196748, 0.845190201071091, 0.8117428759578615, 0.8847868650918826, 0.8940225508995354, 0.8316801900509745, 0.9685200039530173, 1.1244132581632584, 0.8194382269866765, 0.8910377660067752, 0.9436331499600783, 0.952214110060595, 0.8943131359992549, 0.8082897100830451, 0.8313664890592918, 0.8067292099585757, 0.9258880651323125, 0.9114917579572648, 0.8459604900563136, 0.8354824390262365, 0.8632762040942907, 0.9406201289966702, 0.8238228260306641, 0.8479417680064216, 0.8005321229575202, 0.8464690271066502, 0.9025670021073893, 0.8822696179850027, 0.8527349609648809, 0.8415270490804687, 0.9569018579786643, 0.7929080701433122, 0.8433375079184771, 0.7994014719733968, 0.8385097770951688, 0.8620888689765707, 0.820860258070752, 0.8168894230620936, 0.8205616631312296, 0.8104007210349664, 0.820833187084645, 0.911158507806249, 0.8206888299901038, 0.8745032419683412, 0.7944751180475578, 0.8232337039662525, 0.7812779799569398, 0.8223919089650735, 1.0431068709585816, 0.8149045129539445, 0.8839968420797959, 0.8134818070102483, 0.8917078251251951, 0.7952356220921502, 0.8587284249952063, 0.7964818381005898, 0.8121213659178466, 0.7932692429749295, 0.9975144992349669, 0.9453789420658723, 0.7985852970741689, 0.8439159189583734, 0.917123589082621, 0.8350905089173466, 1.1162859270116314, 1.1827819219324738, 0.8072354408213869, 0.8269225110998377, 0.7871438489528373, 0.8420832951087505, 0.7935800299746916, 0.8035385539988056, 0.9085831359261647, 0.8807221408933401, 0.8730751849943772, 0.7609814869938418, 0.8914572860812768, 0.7631307900883257, 0.9444005859550089, 0.8444003550102934, 0.8197855530306697, 0.8539528860710561, 0.8081450670724735, 0.8044020489323884, 0.9311172891175374, 1.0442269189516082, 0.7877255289349705, 0.8956932190340012, 0.8190791549859568, 0.8330680719809607, 0.8476198159623891, 0.8231751511339098, 1.1681761952349916, 0.7716173249064013, 0.8375209009973332, 0.8043192928889766, 0.8178725340403616, 0.8084529599873349, 0.8377044410444796, 0.879176834016107, 0.9399016150273383, 0.8903015339747071, 0.8298758059972897, 0.9426339030032977, 0.7604701339732856, 0.8156968308612704, 0.7746655847877264, 0.8174864249303937, 0.8495750930160284, 0.8128429950447753, 0.7782882230821997, 0.825303470948711, 0.7789522190578282, 0.8666004821425304, 0.7841260340064764, 0.8205498568713665, 0.879722467972897, 0.8555041089421138, 0.8123883150983602, 0.8404221548698843, 0.8266603139927611, 0.8077951190061867, 0.7743873200379312, 0.9101964210858569, 0.7854709011735395, 0.7689537099795416, 0.7995198589051142, 0.7062910820823163, 0.6964870380470529, 0.7983439159579575, 1.011531709926203, 0.9061950880568475, 0.8135238119866699, 0.8402997620869428, 0.832073159981519, 0.8267606641165912, 0.7940550860948861, 0.8237322570057586, 0.8324226050172001, 0.949220440001227, 0.864581958972849, 0.774267187109217, 0.8605050509795547, 0.9282360540237278, 0.8906164659420028, 0.9373113508336246, 0.8617976409150288, 0.9508540499955416, 0.97735298902262, 1.0128026180900633, 0.8532112939283252, 0.862465925863944, 0.8192725138505921, 0.9002804489573464, 0.9573603939497843, 0.7904387150192633, 0.8477474910905585, 0.8305368379224092, 0.8292796679306775, 0.7911261979024857, 0.9373582539847121, 0.8459689399460331, 0.8385870000347495, 0.8278010259382427, 0.8365060550859198, 0.8285633770283312, 0.8605136590776965, 0.8133984800660983, 0.9708248419919983, 0.8810617669951171, 0.8017099999124184, 0.8375217579305172, 0.8620449240552261, 0.8769860670436174, 0.9342164489207789, 0.9381246260600165, 0.930622830055654, 0.8586815890157595, 0.7990640519419685, 0.8512386109214276, 0.8074271949008107, 0.843064337852411, 0.8003634969936684, 0.924225911963731, 0.9381171120330691, 0.8103172309929505, 0.8350319190649316, 0.8043227479793131, 0.8256775849731639, 0.8714220999972895, 0.8346230359748006, 0.7903105908771977, 0.8518660339759663, 0.810443529044278, 0.8650719280121848, 0.8445720568997785, 0.8232655130559579, 0.8519179599825293, 0.9911439919378608, 0.8392001581378281, 0.8342523259343579, 0.8304236480034888, 0.8460852839052677, 0.8237536720698699, 0.8913588330615312, 1.0061323420377448, 0.8186653759330511, 0.8959273209329695, 0.9171199579723179, 0.8991599500877783, 1.0313216251088306, 0.9218948389170691, 0.8483089139917865, 0.9032036869321018, 0.8861330129439011, 0.8791608609026298, 0.8399919900111854, 0.8541407160228118, 0.9003100210102275, 0.867963636177592, 0.8822070519672707, 0.9736659480258822, 0.9383224289631471, 0.9311796028632671, 0.9030449941055849, 0.86477590596769, 0.9075703469570726, 0.8422438509296626, 0.9632137230364606, 0.854614051990211, 0.8645881400443614, 0.9340338080655783, 0.8928261351538822, 0.8475518189370632, 0.8029436080250889, 0.8461518220137805, 0.9413403840735555, 0.8316145949065685, 0.8930989808868617, 0.8527534210588783, 0.8624446920584887, 0.8056528181768954, 0.9710116269998252, 0.8534972299821675, 0.8448127749143168, 0.8416080550523475, 0.8101910391123965, 0.8998351100599393, 0.9293609919259325, 0.8337336760014296, 0.8874547139275819, 0.9523326768539846, 0.8513505229493603, 0.8164308540290222, 0.8932012160075828, 0.8018662899266928, 0.8813037170330063, 0.8630037519615144, 1.0184206010308117, 0.8392164100660011, 0.7760034759994596, 0.8099232770036906, 0.793486712849699, 0.8310101969400421, 0.7947484150063246, 0.8351425869623199, 0.9214981429977342, 0.9383804888930172, 0.8198375072097406]
Total Epoch List: [91, 87, 103]
Total Time List: [0.3590319160139188, 0.24590896198060364, 0.23360064206644893]
========================training times:4========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334550>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.27s
Epoch 7/1000, LR 0.000150
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.24s
Epoch 10/1000, LR 0.000240
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.27s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.31s
Epoch 14/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.38s
Val loss: 0.6925 score: 0.5814 time: 0.25s
Test loss: 0.6923 score: 0.5227 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.47s
Val loss: 0.6925 score: 0.6744 time: 0.26s
Test loss: 0.6921 score: 0.7045 time: 0.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.26s
Test loss: 0.6920 score: 0.6364 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.4884 time: 0.26s
Test loss: 0.6918 score: 0.5455 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.24s
Test loss: 0.6916 score: 0.5227 time: 0.26s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.4884 time: 0.27s
Test loss: 0.6913 score: 0.5227 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5000 time: 0.27s
Epoch 21/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5000 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4884 time: 0.38s
Test loss: 0.6905 score: 0.5227 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4884 time: 0.25s
Test loss: 0.6901 score: 0.5227 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.4884 time: 0.26s
Test loss: 0.6897 score: 0.5227 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4884 time: 0.25s
Test loss: 0.6892 score: 0.5455 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.4884 time: 0.27s
Test loss: 0.6886 score: 0.5455 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.4884 time: 0.24s
Test loss: 0.6880 score: 0.5455 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.4884 time: 0.25s
Test loss: 0.6873 score: 0.5909 time: 0.36s
Epoch 29/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.33s
Val loss: 0.6891 score: 0.5116 time: 0.26s
Test loss: 0.6866 score: 0.6591 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.35s
Val loss: 0.6884 score: 0.5581 time: 0.26s
Test loss: 0.6857 score: 0.6591 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.30s
Val loss: 0.6877 score: 0.6279 time: 0.26s
Test loss: 0.6847 score: 0.6818 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.34s
Val loss: 0.6868 score: 0.6512 time: 0.24s
Test loss: 0.6836 score: 0.7273 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.30s
Val loss: 0.6858 score: 0.6977 time: 0.26s
Test loss: 0.6823 score: 0.7273 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.33s
Val loss: 0.6847 score: 0.7209 time: 0.25s
Test loss: 0.6809 score: 0.7273 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6798;  Loss pred: 0.6798; Loss self: 0.0000; time: 0.30s
Val loss: 0.6835 score: 0.7442 time: 0.27s
Test loss: 0.6794 score: 0.7273 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6788;  Loss pred: 0.6788; Loss self: 0.0000; time: 0.33s
Val loss: 0.6822 score: 0.7442 time: 0.26s
Test loss: 0.6778 score: 0.7273 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.31s
Val loss: 0.6807 score: 0.7907 time: 0.26s
Test loss: 0.6759 score: 0.7955 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.42s
Val loss: 0.6790 score: 0.7674 time: 0.38s
Test loss: 0.6739 score: 0.7727 time: 0.24s
Epoch 39/1000, LR 0.000269
Train loss: 0.6731;  Loss pred: 0.6731; Loss self: 0.0000; time: 0.43s
Val loss: 0.6771 score: 0.7442 time: 0.27s
Test loss: 0.6717 score: 0.7727 time: 0.26s
Epoch 40/1000, LR 0.000269
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.29s
Val loss: 0.6751 score: 0.7442 time: 0.26s
Test loss: 0.6694 score: 0.8182 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.33s
Val loss: 0.6728 score: 0.7442 time: 0.24s
Test loss: 0.6669 score: 0.7955 time: 0.26s
Epoch 42/1000, LR 0.000269
Train loss: 0.6639;  Loss pred: 0.6639; Loss self: 0.0000; time: 0.28s
Val loss: 0.6703 score: 0.7674 time: 0.26s
Test loss: 0.6642 score: 0.7955 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6591;  Loss pred: 0.6591; Loss self: 0.0000; time: 0.34s
Val loss: 0.6676 score: 0.7442 time: 0.26s
Test loss: 0.6613 score: 0.7955 time: 0.25s
Epoch 44/1000, LR 0.000269
Train loss: 0.6563;  Loss pred: 0.6563; Loss self: 0.0000; time: 0.28s
Val loss: 0.6647 score: 0.7674 time: 0.26s
Test loss: 0.6582 score: 0.7955 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 0.41s
Val loss: 0.6615 score: 0.7442 time: 0.24s
Test loss: 0.6548 score: 0.7500 time: 0.26s
Epoch 46/1000, LR 0.000269
Train loss: 0.6454;  Loss pred: 0.6454; Loss self: 0.0000; time: 0.43s
Val loss: 0.6580 score: 0.7442 time: 0.25s
Test loss: 0.6512 score: 0.7500 time: 0.26s
Epoch 47/1000, LR 0.000269
Train loss: 0.6413;  Loss pred: 0.6413; Loss self: 0.0000; time: 0.34s
Val loss: 0.6542 score: 0.7442 time: 0.26s
Test loss: 0.6473 score: 0.8182 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 0.35s
Val loss: 0.6500 score: 0.7674 time: 0.25s
Test loss: 0.6432 score: 0.8409 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6305;  Loss pred: 0.6305; Loss self: 0.0000; time: 0.32s
Val loss: 0.6456 score: 0.7907 time: 0.26s
Test loss: 0.6386 score: 0.8409 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 0.6289;  Loss pred: 0.6289; Loss self: 0.0000; time: 0.36s
Val loss: 0.6408 score: 0.7907 time: 0.26s
Test loss: 0.6338 score: 0.8409 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6235;  Loss pred: 0.6235; Loss self: 0.0000; time: 0.29s
Val loss: 0.6357 score: 0.7907 time: 0.26s
Test loss: 0.6286 score: 0.8409 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6112;  Loss pred: 0.6112; Loss self: 0.0000; time: 0.34s
Val loss: 0.6302 score: 0.8140 time: 0.24s
Test loss: 0.6230 score: 0.8409 time: 0.37s
Epoch 53/1000, LR 0.000269
Train loss: 0.6080;  Loss pred: 0.6080; Loss self: 0.0000; time: 0.35s
Val loss: 0.6244 score: 0.8140 time: 0.25s
Test loss: 0.6170 score: 0.8409 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.32s
Val loss: 0.6181 score: 0.8140 time: 0.26s
Test loss: 0.6107 score: 0.8409 time: 0.24s
Epoch 55/1000, LR 0.000269
Train loss: 0.5926;  Loss pred: 0.5926; Loss self: 0.0000; time: 0.36s
Val loss: 0.6115 score: 0.8140 time: 0.25s
Test loss: 0.6039 score: 0.8409 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.5869;  Loss pred: 0.5869; Loss self: 0.0000; time: 0.34s
Val loss: 0.6046 score: 0.8140 time: 0.26s
Test loss: 0.5968 score: 0.8409 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.5697;  Loss pred: 0.5697; Loss self: 0.0000; time: 0.35s
Val loss: 0.5972 score: 0.8140 time: 0.25s
Test loss: 0.5894 score: 0.8409 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5684;  Loss pred: 0.5684; Loss self: 0.0000; time: 0.30s
Val loss: 0.5897 score: 0.7907 time: 0.27s
Test loss: 0.5818 score: 0.8636 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.5545;  Loss pred: 0.5545; Loss self: 0.0000; time: 0.44s
Val loss: 0.5819 score: 0.7907 time: 0.24s
Test loss: 0.5738 score: 0.8636 time: 0.26s
Epoch 60/1000, LR 0.000268
Train loss: 0.5458;  Loss pred: 0.5458; Loss self: 0.0000; time: 0.34s
Val loss: 0.5736 score: 0.7907 time: 0.25s
Test loss: 0.5655 score: 0.8636 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5283;  Loss pred: 0.5283; Loss self: 0.0000; time: 0.33s
Val loss: 0.5648 score: 0.7907 time: 0.25s
Test loss: 0.5568 score: 0.8636 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.5086;  Loss pred: 0.5086; Loss self: 0.0000; time: 0.33s
Val loss: 0.5555 score: 0.7907 time: 0.33s
Test loss: 0.5478 score: 0.8636 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5066;  Loss pred: 0.5066; Loss self: 0.0000; time: 0.37s
Val loss: 0.5460 score: 0.8140 time: 0.26s
Test loss: 0.5385 score: 0.8636 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.4906;  Loss pred: 0.4906; Loss self: 0.0000; time: 0.34s
Val loss: 0.5362 score: 0.8140 time: 0.25s
Test loss: 0.5291 score: 0.8636 time: 0.25s
Epoch 65/1000, LR 0.000268
Train loss: 0.4759;  Loss pred: 0.4759; Loss self: 0.0000; time: 0.29s
Val loss: 0.5263 score: 0.8140 time: 0.25s
Test loss: 0.5197 score: 0.8636 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.4710;  Loss pred: 0.4710; Loss self: 0.0000; time: 0.34s
Val loss: 0.5163 score: 0.8140 time: 0.25s
Test loss: 0.5102 score: 0.8636 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.4456;  Loss pred: 0.4456; Loss self: 0.0000; time: 0.34s
Val loss: 0.5064 score: 0.8140 time: 0.27s
Test loss: 0.5007 score: 0.8636 time: 0.28s
Epoch 68/1000, LR 0.000268
Train loss: 0.4314;  Loss pred: 0.4314; Loss self: 0.0000; time: 0.41s
Val loss: 0.4964 score: 0.8140 time: 0.28s
Test loss: 0.4914 score: 0.8636 time: 0.37s
Epoch 69/1000, LR 0.000268
Train loss: 0.4264;  Loss pred: 0.4264; Loss self: 0.0000; time: 0.48s
Val loss: 0.4865 score: 0.8140 time: 0.27s
Test loss: 0.4822 score: 0.8636 time: 0.28s
Epoch 70/1000, LR 0.000268
Train loss: 0.4184;  Loss pred: 0.4184; Loss self: 0.0000; time: 0.34s
Val loss: 0.4769 score: 0.8140 time: 0.26s
Test loss: 0.4734 score: 0.8636 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.4108;  Loss pred: 0.4108; Loss self: 0.0000; time: 0.33s
Val loss: 0.4676 score: 0.8140 time: 0.25s
Test loss: 0.4648 score: 0.8636 time: 0.26s
Epoch 72/1000, LR 0.000267
Train loss: 0.3848;  Loss pred: 0.3848; Loss self: 0.0000; time: 0.28s
Val loss: 0.4586 score: 0.8140 time: 0.26s
Test loss: 0.4565 score: 0.8636 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.3620;  Loss pred: 0.3620; Loss self: 0.0000; time: 0.33s
Val loss: 0.4500 score: 0.8140 time: 0.23s
Test loss: 0.4487 score: 0.8636 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.3469;  Loss pred: 0.3469; Loss self: 0.0000; time: 0.28s
Val loss: 0.4418 score: 0.8140 time: 0.26s
Test loss: 0.4412 score: 0.8636 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.3354;  Loss pred: 0.3354; Loss self: 0.0000; time: 0.34s
Val loss: 0.4339 score: 0.8372 time: 0.25s
Test loss: 0.4340 score: 0.8636 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3281;  Loss pred: 0.3281; Loss self: 0.0000; time: 0.32s
Val loss: 0.4264 score: 0.8372 time: 0.26s
Test loss: 0.4272 score: 0.8636 time: 0.33s
Epoch 77/1000, LR 0.000267
Train loss: 0.3243;  Loss pred: 0.3243; Loss self: 0.0000; time: 0.36s
Val loss: 0.4193 score: 0.8372 time: 0.25s
Test loss: 0.4207 score: 0.8636 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.2974;  Loss pred: 0.2974; Loss self: 0.0000; time: 0.28s
Val loss: 0.4127 score: 0.8372 time: 0.27s
Test loss: 0.4145 score: 0.8636 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.2823;  Loss pred: 0.2823; Loss self: 0.0000; time: 0.36s
Val loss: 0.4069 score: 0.8372 time: 0.24s
Test loss: 0.4093 score: 0.8636 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 0.28s
Val loss: 0.4018 score: 0.8372 time: 0.26s
Test loss: 0.4045 score: 0.8636 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.2505;  Loss pred: 0.2505; Loss self: 0.0000; time: 0.36s
Val loss: 0.3976 score: 0.8372 time: 0.24s
Test loss: 0.4008 score: 0.8636 time: 0.25s
Epoch 82/1000, LR 0.000267
Train loss: 0.2263;  Loss pred: 0.2263; Loss self: 0.0000; time: 0.29s
Val loss: 0.3941 score: 0.8372 time: 0.26s
Test loss: 0.3977 score: 0.8636 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.2653;  Loss pred: 0.2653; Loss self: 0.0000; time: 0.35s
Val loss: 0.3914 score: 0.8372 time: 0.25s
Test loss: 0.3952 score: 0.8636 time: 0.33s
Epoch 84/1000, LR 0.000266
Train loss: 0.2214;  Loss pred: 0.2214; Loss self: 0.0000; time: 0.54s
Val loss: 0.3893 score: 0.8372 time: 0.24s
Test loss: 0.3936 score: 0.8636 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2074;  Loss pred: 0.2074; Loss self: 0.0000; time: 0.32s
Val loss: 0.3881 score: 0.8372 time: 0.25s
Test loss: 0.3928 score: 0.8636 time: 0.24s
Epoch 86/1000, LR 0.000266
Train loss: 0.1923;  Loss pred: 0.1923; Loss self: 0.0000; time: 0.33s
Val loss: 0.3877 score: 0.8372 time: 0.26s
Test loss: 0.3933 score: 0.8409 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.1960;  Loss pred: 0.1960; Loss self: 0.0000; time: 0.29s
Val loss: 0.3883 score: 0.8372 time: 0.25s
Test loss: 0.3949 score: 0.8182 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 88/1000, LR 0.000266
Train loss: 0.2021;  Loss pred: 0.2021; Loss self: 0.0000; time: 0.33s
Val loss: 0.3889 score: 0.8372 time: 0.25s
Test loss: 0.3957 score: 0.8182 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 085,   Train_Loss: 0.1923,   Val_Loss: 0.3877,   Val_Precision: 0.8947,   Val_Recall: 0.7727,   Val_accuracy: 0.8293,   Val_Score: 0.8372,   Val_Loss: 0.3877,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8409,   Test_loss: 0.3933


[0.2584658929845318, 0.2540021979948506, 0.24756741407327354, 0.24729462107643485, 0.24313744297251105, 0.269764123018831, 0.2370913838967681, 0.25280163099523634, 0.25585390406195074, 0.2417142370250076, 0.27445204893592745, 0.2516138489590958, 0.3156533610308543, 0.24448092002421618, 0.25520695897284895, 0.25933848309796304, 0.256443899939768, 0.2623331630602479, 0.25407878309488297, 0.2713687060168013, 0.23849671997595578, 0.24735160707496107, 0.2571315059904009, 0.2464434850262478, 0.25910851801745594, 0.25245045905467123, 0.26334086002316326, 0.3622643850976601, 0.24066599400248379, 0.25489355402532965, 0.25414506101515144, 0.25491191702894866, 0.24644650402478874, 0.2549056709976867, 0.26510797708760947, 0.25105250894557685, 0.25622216099873185, 0.24265428690705448, 0.26515696407295763, 0.24730178399477154, 0.2618436840130016, 0.25160567997954786, 0.256584299961105, 0.24354007607325912, 0.262669840012677, 0.26354348799213767, 0.2418337669223547, 0.2552400149870664, 0.2503000079886988, 0.266865401994437, 0.25382309197448194, 0.37353180500213057, 0.24581950006540865, 0.24016977206338197, 0.24192113406024873, 0.24626518494915217, 0.2569065879797563, 0.24769598001148552, 0.26377025805413723, 0.23447924107313156, 0.2588770049624145, 0.25528103101532906, 0.2495994990458712, 0.2523250199155882, 0.24226498790085316, 0.25841553905047476, 0.2872433140873909, 0.37441899499390274, 0.2823460700456053, 0.2381214339984581, 0.26469589106272906, 0.24824698909651488, 0.25775827397592366, 0.24459869193378836, 0.25149479100946337, 0.33031413704156876, 0.2577367479680106, 0.2349484460428357, 0.25831051904242486, 0.25207631499506533, 0.25854220404289663, 0.25225736503489316, 0.3334519200725481, 0.24881555000320077, 0.24562752293422818, 0.25017125194426626, 0.22510442591737956, 0.24844532401766628]
[0.005874224840557541, 0.005772777227155695, 0.005626532138028944, 0.005620332297191701, 0.005525850976647978, 0.006131002795882523, 0.005388440543108366, 0.005745491613528098, 0.005814861455953426, 0.005493505386931991, 0.006237546566725624, 0.005718496567252177, 0.007173940023428507, 0.005556384546004913, 0.00580015815847384, 0.0058940564340446144, 0.005828270453176546, 0.005962117342278361, 0.005774517797610976, 0.0061674705912909385, 0.00542037999945354, 0.005621627433521842, 0.005843897863418202, 0.005600988296051087, 0.00588882995494218, 0.00573751043306071, 0.005985019545980983, 0.008233281479492276, 0.005469681681874631, 0.005793035318757492, 0.005776024113980715, 0.005793452659748833, 0.0056010569096542895, 0.005793310704492879, 0.00602518129744567, 0.005705738839672201, 0.0058232309317893605, 0.005514870156978511, 0.0060262946380217645, 0.005620495090790262, 0.00595099281847731, 0.0057183109086260875, 0.005831461362752386, 0.005535001728937707, 0.005969769091197205, 0.005989624727094038, 0.005496221975508061, 0.005800909431524236, 0.0056886365451977, 0.00606512277260084, 0.00576870663578368, 0.008489359204593877, 0.005586806819668378, 0.005458403910531409, 0.005498207592278381, 0.005596936021571641, 0.005838786090449007, 0.005629454091170125, 0.0059947785921394825, 0.00532907366075299, 0.00588356829460033, 0.005801841613984751, 0.005672715887406163, 0.005734659543536095, 0.005506022452292117, 0.005873080432965336, 0.006528257138349793, 0.00850952261349779, 0.0064169561374001205, 0.005411850772692229, 0.006015815705971115, 0.005641977024920793, 0.005858142590361901, 0.0055590611803133716, 0.005715790704760531, 0.007507139478217472, 0.005857653362909332, 0.005339737410064448, 0.005870693614600565, 0.005729007158978758, 0.005875959182793105, 0.005733121932611208, 0.0075784527289215475, 0.005654898863709109, 0.005582443703050641, 0.005685710271460597, 0.005116009679940445, 0.005646484636765143]
[170.23522713936276, 173.22684743417858, 177.72936783585396, 177.92542275474847, 180.9676019541532, 163.10545489745707, 185.58245043252197, 174.0495099923984, 171.97314288136144, 182.0331335942276, 160.31944440054838, 174.8711375865204, 139.39341515739196, 179.97314471674002, 172.4090917312372, 169.66244066207233, 171.5774873581881, 167.72564889134844, 173.17463293882622, 162.14102446018893, 184.48891039019693, 177.88443147921652, 171.11866486576167, 178.5399195897336, 169.8130201842139, 174.291622066218, 167.08383194362543, 121.45825482717096, 182.82599576384646, 172.62107772104574, 173.12947111483248, 172.60864267480758, 178.537732454806, 172.6128721569294, 165.97010955071883, 175.262140118816, 171.725973383769, 181.32793185250264, 165.93944705104352, 177.92026927282598, 168.03918783015297, 174.876815195812, 171.48360210141405, 180.66841691699395, 167.5106666143188, 166.9553679175433, 181.94316103973617, 172.3867631109079, 175.7890475256662, 164.87712409013295, 173.3490820623347, 117.7945208701814, 178.9931229552265, 183.20373801407524, 181.87745428244443, 178.66918545179232, 171.26847678762954, 177.63711788120156, 166.81183210189403, 187.64987381666282, 169.96488354146484, 172.35906571279045, 176.28240508573182, 174.3782682142246, 181.61931024885436, 170.26839857105398, 153.18024073003642, 117.51540543693976, 155.83712566954802, 184.77967002451746, 166.22849649590006, 177.24283448567198, 170.7025707508807, 179.88648938445908, 174.9539218024772, 133.20652998410046, 170.71682772012446, 187.27512669727525, 170.3376237371636, 174.5503142604308, 170.18498067998073, 174.42503608928826, 131.9530563519534, 176.83782223190624, 179.13302008823297, 175.8795211601788, 195.4648373557497, 177.10134080394798]
Elapsed: 0.25946151160116476~0.026647404395602148
Time per graph: 0.00589685253639011~0.0006056228271727761
Speed: 170.99188210411828~13.86602599994336
Total Time: 0.2489
best val loss: 0.38769975304603577 test_score: 0.8409

Testing...
Test loss: 0.4340 score: 0.8636 time: 0.24s
test Score 0.8636
Epoch Time List: [0.8516632040264085, 0.8506711019435897, 0.8377671010093763, 0.832902148948051, 0.835638041025959, 0.9168454060563818, 0.9344245268730447, 0.872180426842533, 0.8452723560621962, 0.8951164608588442, 0.8851969609968364, 0.7856000908650458, 0.915079406928271, 0.8724290120881051, 0.9828251118306071, 0.8532070700312033, 0.8062625620514154, 0.8352010269882157, 0.8049368839710951, 0.8732562850927934, 0.7981671020388603, 0.9747800970217213, 0.9000448259757832, 0.7985607050359249, 0.8274504239670932, 0.806539198034443, 0.8451857570325956, 1.0039323681266978, 0.8263145699165761, 0.8561057879123837, 0.8089968389831483, 0.8345479749841616, 0.8038675599964336, 0.8328209288883954, 0.8253129030345008, 0.8420890839770436, 0.8227660550037399, 1.034503179951571, 0.9621517879422754, 0.7996889180503786, 0.8254970939597115, 0.7901595642324537, 0.860626777051948, 0.7796866209246218, 0.9012399460189044, 0.9442814760841429, 0.8343248169403523, 0.8562544139567763, 0.8313235610257834, 0.8811052669771016, 0.8066619629971683, 0.947619580081664, 0.8427049340680242, 0.8136909530730918, 0.8429130939766765, 0.8456644529942423, 0.8547585691558197, 0.806820135912858, 0.9338290852028877, 0.8189379859250039, 0.8386800430016592, 0.9111267930129543, 0.8734518779674545, 0.8363219000166282, 0.7829537549987435, 0.8434253531740978, 0.8878382419934496, 1.0552120089996606, 1.0284449410391971, 0.8280587800545618, 0.844239863101393, 0.7802747581154108, 0.8194473129697144, 0.7816790760261938, 0.8323860689997673, 0.8997237069997936, 0.8645716210594401, 0.7820782029302791, 0.8568525530863553, 0.7897054359782487, 0.8470446760766208, 0.7960526871029288, 0.9278415930457413, 1.018859110889025, 0.809772192966193, 0.8398127588443458, 0.7608444510260597, 0.8263930020621046]
Total Epoch List: [88]
Total Time List: [0.24889481300488114]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a52883368f0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5116 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.30s
Epoch 22/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5116 time: 0.31s
Epoch 23/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.28s
Epoch 27/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5116 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6874;  Loss pred: 0.6874; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6891 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5116 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.33s
Val loss: 0.6886 score: 0.5227 time: 0.24s
Test loss: 0.6901 score: 0.5349 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.35s
Val loss: 0.6880 score: 0.5455 time: 0.23s
Test loss: 0.6898 score: 0.5814 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.39s
Val loss: 0.6873 score: 0.5682 time: 0.23s
Test loss: 0.6893 score: 0.5814 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.36s
Val loss: 0.6866 score: 0.5682 time: 0.22s
Test loss: 0.6889 score: 0.5581 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6819;  Loss pred: 0.6819; Loss self: 0.0000; time: 0.33s
Val loss: 0.6857 score: 0.5909 time: 0.24s
Test loss: 0.6883 score: 0.5581 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.34s
Val loss: 0.6848 score: 0.5909 time: 0.22s
Test loss: 0.6878 score: 0.5581 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.32s
Val loss: 0.6837 score: 0.5909 time: 0.23s
Test loss: 0.6871 score: 0.5581 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.35s
Val loss: 0.6825 score: 0.5682 time: 0.22s
Test loss: 0.6863 score: 0.5581 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6737;  Loss pred: 0.6737; Loss self: 0.0000; time: 0.31s
Val loss: 0.6811 score: 0.5682 time: 0.23s
Test loss: 0.6855 score: 0.5581 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.36s
Val loss: 0.6797 score: 0.5682 time: 0.21s
Test loss: 0.6846 score: 0.5581 time: 0.25s
Epoch 39/1000, LR 0.000269
Train loss: 0.6693;  Loss pred: 0.6693; Loss self: 0.0000; time: 0.41s
Val loss: 0.6781 score: 0.5909 time: 0.24s
Test loss: 0.6836 score: 0.5581 time: 0.23s
Epoch 40/1000, LR 0.000269
Train loss: 0.6660;  Loss pred: 0.6660; Loss self: 0.0000; time: 0.36s
Val loss: 0.6763 score: 0.5909 time: 0.21s
Test loss: 0.6825 score: 0.5581 time: 0.26s
Epoch 41/1000, LR 0.000269
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.34s
Val loss: 0.6744 score: 0.5909 time: 0.24s
Test loss: 0.6813 score: 0.5814 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.36s
Val loss: 0.6723 score: 0.5909 time: 0.22s
Test loss: 0.6800 score: 0.5814 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6568;  Loss pred: 0.6568; Loss self: 0.0000; time: 0.34s
Val loss: 0.6701 score: 0.5909 time: 0.24s
Test loss: 0.6786 score: 0.5814 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6526;  Loss pred: 0.6526; Loss self: 0.0000; time: 0.35s
Val loss: 0.6676 score: 0.6136 time: 0.22s
Test loss: 0.6770 score: 0.5814 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6491;  Loss pred: 0.6491; Loss self: 0.0000; time: 0.32s
Val loss: 0.6649 score: 0.6364 time: 0.23s
Test loss: 0.6754 score: 0.5814 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6436;  Loss pred: 0.6436; Loss self: 0.0000; time: 0.36s
Val loss: 0.6619 score: 0.6591 time: 0.30s
Test loss: 0.6736 score: 0.5814 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 0.49s
Val loss: 0.6587 score: 0.6591 time: 0.23s
Test loss: 0.6716 score: 0.6047 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6345;  Loss pred: 0.6345; Loss self: 0.0000; time: 0.35s
Val loss: 0.6553 score: 0.7273 time: 0.23s
Test loss: 0.6695 score: 0.6512 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6279;  Loss pred: 0.6279; Loss self: 0.0000; time: 0.39s
Val loss: 0.6515 score: 0.7500 time: 0.23s
Test loss: 0.6672 score: 0.6512 time: 0.26s
Epoch 50/1000, LR 0.000269
Train loss: 0.6171;  Loss pred: 0.6171; Loss self: 0.0000; time: 0.33s
Val loss: 0.6475 score: 0.7955 time: 0.24s
Test loss: 0.6648 score: 0.7209 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6133;  Loss pred: 0.6133; Loss self: 0.0000; time: 0.38s
Val loss: 0.6431 score: 0.8409 time: 0.23s
Test loss: 0.6621 score: 0.7442 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6036;  Loss pred: 0.6036; Loss self: 0.0000; time: 0.46s
Val loss: 0.6383 score: 0.8636 time: 0.22s
Test loss: 0.6593 score: 0.7209 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.5957;  Loss pred: 0.5957; Loss self: 0.0000; time: 0.38s
Val loss: 0.6333 score: 0.8636 time: 0.33s
Test loss: 0.6561 score: 0.7209 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.5871;  Loss pred: 0.5871; Loss self: 0.0000; time: 0.37s
Val loss: 0.6279 score: 0.8636 time: 0.22s
Test loss: 0.6528 score: 0.7209 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.5783;  Loss pred: 0.5783; Loss self: 0.0000; time: 0.44s
Val loss: 0.6221 score: 0.8864 time: 0.24s
Test loss: 0.6492 score: 0.7209 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.5705;  Loss pred: 0.5705; Loss self: 0.0000; time: 0.37s
Val loss: 0.6160 score: 0.8864 time: 0.22s
Test loss: 0.6453 score: 0.7442 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.5613;  Loss pred: 0.5613; Loss self: 0.0000; time: 0.37s
Val loss: 0.6095 score: 0.8636 time: 0.25s
Test loss: 0.6413 score: 0.7209 time: 0.30s
Epoch 58/1000, LR 0.000269
Train loss: 0.5485;  Loss pred: 0.5485; Loss self: 0.0000; time: 0.44s
Val loss: 0.6026 score: 0.8636 time: 0.24s
Test loss: 0.6371 score: 0.7209 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.5376;  Loss pred: 0.5376; Loss self: 0.0000; time: 0.37s
Val loss: 0.5954 score: 0.8636 time: 0.29s
Test loss: 0.6326 score: 0.7209 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5294;  Loss pred: 0.5294; Loss self: 0.0000; time: 0.35s
Val loss: 0.5879 score: 0.8409 time: 0.23s
Test loss: 0.6281 score: 0.7209 time: 0.22s
Epoch 61/1000, LR 0.000268
Train loss: 0.5156;  Loss pred: 0.5156; Loss self: 0.0000; time: 0.37s
Val loss: 0.5802 score: 0.8409 time: 0.21s
Test loss: 0.6234 score: 0.7209 time: 0.25s
Epoch 62/1000, LR 0.000268
Train loss: 0.5041;  Loss pred: 0.5041; Loss self: 0.0000; time: 0.42s
Val loss: 0.5722 score: 0.8409 time: 0.29s
Test loss: 0.6186 score: 0.7209 time: 0.31s
Epoch 63/1000, LR 0.000268
Train loss: 0.4907;  Loss pred: 0.4907; Loss self: 0.0000; time: 0.41s
Val loss: 0.5640 score: 0.8409 time: 0.24s
Test loss: 0.6137 score: 0.7209 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.4760;  Loss pred: 0.4760; Loss self: 0.0000; time: 0.37s
Val loss: 0.5557 score: 0.8409 time: 0.22s
Test loss: 0.6087 score: 0.6977 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.4749;  Loss pred: 0.4749; Loss self: 0.0000; time: 0.41s
Val loss: 0.5475 score: 0.8409 time: 0.25s
Test loss: 0.6037 score: 0.6977 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.4503;  Loss pred: 0.4503; Loss self: 0.0000; time: 0.37s
Val loss: 0.5391 score: 0.8409 time: 0.21s
Test loss: 0.5986 score: 0.7442 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.4385;  Loss pred: 0.4385; Loss self: 0.0000; time: 0.35s
Val loss: 0.5307 score: 0.8636 time: 0.24s
Test loss: 0.5935 score: 0.7442 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.4280;  Loss pred: 0.4280; Loss self: 0.0000; time: 0.36s
Val loss: 0.5222 score: 0.8636 time: 0.23s
Test loss: 0.5882 score: 0.7442 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4113;  Loss pred: 0.4113; Loss self: 0.0000; time: 0.35s
Val loss: 0.5136 score: 0.8636 time: 0.24s
Test loss: 0.5829 score: 0.7907 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.4009;  Loss pred: 0.4009; Loss self: 0.0000; time: 0.44s
Val loss: 0.5050 score: 0.8636 time: 0.23s
Test loss: 0.5776 score: 0.7907 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.3863;  Loss pred: 0.3863; Loss self: 0.0000; time: 0.41s
Val loss: 0.4962 score: 0.8636 time: 0.26s
Test loss: 0.5723 score: 0.7907 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.3758;  Loss pred: 0.3758; Loss self: 0.0000; time: 0.35s
Val loss: 0.4877 score: 0.8636 time: 0.23s
Test loss: 0.5672 score: 0.7907 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.3554;  Loss pred: 0.3554; Loss self: 0.0000; time: 0.40s
Val loss: 0.4793 score: 0.8636 time: 0.28s
Test loss: 0.5622 score: 0.7907 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.3512;  Loss pred: 0.3512; Loss self: 0.0000; time: 0.36s
Val loss: 0.4710 score: 0.8636 time: 0.24s
Test loss: 0.5575 score: 0.7907 time: 0.30s
Epoch 75/1000, LR 0.000267
Train loss: 0.3477;  Loss pred: 0.3477; Loss self: 0.0000; time: 0.34s
Val loss: 0.4634 score: 0.8636 time: 0.22s
Test loss: 0.5533 score: 0.7907 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.43s
Val loss: 0.4564 score: 0.8636 time: 0.24s
Test loss: 0.5499 score: 0.7907 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.3106;  Loss pred: 0.3106; Loss self: 0.0000; time: 0.46s
Val loss: 0.4498 score: 0.8636 time: 0.23s
Test loss: 0.5470 score: 0.7907 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.3018;  Loss pred: 0.3018; Loss self: 0.0000; time: 0.47s
Val loss: 0.4433 score: 0.8636 time: 0.25s
Test loss: 0.5445 score: 0.7907 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.2951;  Loss pred: 0.2951; Loss self: 0.0000; time: 0.39s
Val loss: 0.4374 score: 0.8409 time: 0.24s
Test loss: 0.5428 score: 0.7674 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.2759;  Loss pred: 0.2759; Loss self: 0.0000; time: 0.36s
Val loss: 0.4317 score: 0.8409 time: 0.22s
Test loss: 0.5416 score: 0.7674 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.2714;  Loss pred: 0.2714; Loss self: 0.0000; time: 0.35s
Val loss: 0.4263 score: 0.8409 time: 0.23s
Test loss: 0.5406 score: 0.7674 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.35s
Val loss: 0.4210 score: 0.8182 time: 0.21s
Test loss: 0.5399 score: 0.7674 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2570;  Loss pred: 0.2570; Loss self: 0.0000; time: 0.37s
Val loss: 0.4160 score: 0.8182 time: 0.23s
Test loss: 0.5395 score: 0.7674 time: 0.32s
Epoch 84/1000, LR 0.000266
Train loss: 0.2389;  Loss pred: 0.2389; Loss self: 0.0000; time: 0.35s
Val loss: 0.4115 score: 0.8182 time: 0.36s
Test loss: 0.5396 score: 0.7674 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2192;  Loss pred: 0.2192; Loss self: 0.0000; time: 0.46s
Val loss: 0.4075 score: 0.8182 time: 0.22s
Test loss: 0.5403 score: 0.7674 time: 0.24s
Epoch 86/1000, LR 0.000266
Train loss: 0.2167;  Loss pred: 0.2167; Loss self: 0.0000; time: 0.34s
Val loss: 0.4036 score: 0.8182 time: 0.23s
Test loss: 0.5412 score: 0.7674 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.2273;  Loss pred: 0.2273; Loss self: 0.0000; time: 0.42s
Val loss: 0.4018 score: 0.8182 time: 0.25s
Test loss: 0.5443 score: 0.7674 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.2211;  Loss pred: 0.2211; Loss self: 0.0000; time: 0.33s
Val loss: 0.4026 score: 0.8182 time: 0.23s
Test loss: 0.5505 score: 0.7674 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 89/1000, LR 0.000266
Train loss: 0.2002;  Loss pred: 0.2002; Loss self: 0.0000; time: 0.36s
Val loss: 0.4051 score: 0.8182 time: 0.23s
Test loss: 0.5586 score: 0.7442 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.2273,   Val_Loss: 0.4018,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4018,   Test_Precision: 0.8750,   Test_Recall: 0.6364,   Test_accuracy: 0.7368,   Test_Score: 0.7674,   Test_loss: 0.5443


[0.2584658929845318, 0.2540021979948506, 0.24756741407327354, 0.24729462107643485, 0.24313744297251105, 0.269764123018831, 0.2370913838967681, 0.25280163099523634, 0.25585390406195074, 0.2417142370250076, 0.27445204893592745, 0.2516138489590958, 0.3156533610308543, 0.24448092002421618, 0.25520695897284895, 0.25933848309796304, 0.256443899939768, 0.2623331630602479, 0.25407878309488297, 0.2713687060168013, 0.23849671997595578, 0.24735160707496107, 0.2571315059904009, 0.2464434850262478, 0.25910851801745594, 0.25245045905467123, 0.26334086002316326, 0.3622643850976601, 0.24066599400248379, 0.25489355402532965, 0.25414506101515144, 0.25491191702894866, 0.24644650402478874, 0.2549056709976867, 0.26510797708760947, 0.25105250894557685, 0.25622216099873185, 0.24265428690705448, 0.26515696407295763, 0.24730178399477154, 0.2618436840130016, 0.25160567997954786, 0.256584299961105, 0.24354007607325912, 0.262669840012677, 0.26354348799213767, 0.2418337669223547, 0.2552400149870664, 0.2503000079886988, 0.266865401994437, 0.25382309197448194, 0.37353180500213057, 0.24581950006540865, 0.24016977206338197, 0.24192113406024873, 0.24626518494915217, 0.2569065879797563, 0.24769598001148552, 0.26377025805413723, 0.23447924107313156, 0.2588770049624145, 0.25528103101532906, 0.2495994990458712, 0.2523250199155882, 0.24226498790085316, 0.25841553905047476, 0.2872433140873909, 0.37441899499390274, 0.2823460700456053, 0.2381214339984581, 0.26469589106272906, 0.24824698909651488, 0.25775827397592366, 0.24459869193378836, 0.25149479100946337, 0.33031413704156876, 0.2577367479680106, 0.2349484460428357, 0.25831051904242486, 0.25207631499506533, 0.25854220404289663, 0.25225736503489316, 0.3334519200725481, 0.24881555000320077, 0.24562752293422818, 0.25017125194426626, 0.22510442591737956, 0.24844532401766628, 0.25539325608406216, 0.2530335020273924, 0.2509494569385424, 0.24949132406618446, 0.23923518892843276, 0.2504835099680349, 0.24867932498455048, 0.24846718600019813, 0.25673655001446605, 0.24249405798036605, 0.24574408796615899, 0.23275289894081652, 0.2574262310517952, 0.23335679795127362, 0.24683992296922952, 0.23467552103102207, 0.253699182998389, 0.23376991297118366, 0.254316347069107, 0.23739258502610028, 0.30053537199273705, 0.31317043909803033, 0.24196656607091427, 0.24354710394982249, 0.2392263519577682, 0.2845153498928994, 0.24285895901266485, 0.25038840097840875, 0.24346271099057049, 0.2555452010128647, 0.23015008901711553, 0.2386079569114372, 0.23124861402902752, 0.24810339999385178, 0.2429979039588943, 0.2529294940177351, 0.24524287600070238, 0.25320164707954973, 0.2302084401017055, 0.2606633650138974, 0.23567828896921128, 0.2571479449979961, 0.23489037505351007, 0.25618906202726066, 0.24592430307529867, 0.24413490598089993, 0.24376512505114079, 0.24987947300542146, 0.25988187396433204, 0.24933716701343656, 0.25874975300394, 0.23801289894618094, 0.2507786339847371, 0.25894974707625806, 0.23709594004321843, 0.24598104099277407, 0.30038189701735973, 0.2241986959706992, 0.2570331250317395, 0.22754332004114985, 0.25829290994443, 0.3112543999450281, 0.24934538104571402, 0.2607049710350111, 0.23713537701405585, 0.2542082149302587, 0.22985502996016294, 0.2562664339784533, 0.2278737019514665, 0.2454497340368107, 0.2418196490034461, 0.24298081803135574, 0.2587201870046556, 0.3055437229340896, 0.24997553892899305, 0.23850846593268216, 0.23792988702189177, 0.2466952980030328, 0.2428831789875403, 0.25053087901324034, 0.2440003149677068, 0.246515192091465, 0.3222992739174515, 0.24252659804187715, 0.2417452149093151, 0.2345643319422379, 0.24433313007466495, 0.22949085501022637, 0.2474419119535014]
[0.005874224840557541, 0.005772777227155695, 0.005626532138028944, 0.005620332297191701, 0.005525850976647978, 0.006131002795882523, 0.005388440543108366, 0.005745491613528098, 0.005814861455953426, 0.005493505386931991, 0.006237546566725624, 0.005718496567252177, 0.007173940023428507, 0.005556384546004913, 0.00580015815847384, 0.0058940564340446144, 0.005828270453176546, 0.005962117342278361, 0.005774517797610976, 0.0061674705912909385, 0.00542037999945354, 0.005621627433521842, 0.005843897863418202, 0.005600988296051087, 0.00588882995494218, 0.00573751043306071, 0.005985019545980983, 0.008233281479492276, 0.005469681681874631, 0.005793035318757492, 0.005776024113980715, 0.005793452659748833, 0.0056010569096542895, 0.005793310704492879, 0.00602518129744567, 0.005705738839672201, 0.0058232309317893605, 0.005514870156978511, 0.0060262946380217645, 0.005620495090790262, 0.00595099281847731, 0.0057183109086260875, 0.005831461362752386, 0.005535001728937707, 0.005969769091197205, 0.005989624727094038, 0.005496221975508061, 0.005800909431524236, 0.0056886365451977, 0.00606512277260084, 0.00576870663578368, 0.008489359204593877, 0.005586806819668378, 0.005458403910531409, 0.005498207592278381, 0.005596936021571641, 0.005838786090449007, 0.005629454091170125, 0.0059947785921394825, 0.00532907366075299, 0.00588356829460033, 0.005801841613984751, 0.005672715887406163, 0.005734659543536095, 0.005506022452292117, 0.005873080432965336, 0.006528257138349793, 0.00850952261349779, 0.0064169561374001205, 0.005411850772692229, 0.006015815705971115, 0.005641977024920793, 0.005858142590361901, 0.0055590611803133716, 0.005715790704760531, 0.007507139478217472, 0.005857653362909332, 0.005339737410064448, 0.005870693614600565, 0.005729007158978758, 0.005875959182793105, 0.005733121932611208, 0.0075784527289215475, 0.005654898863709109, 0.005582443703050641, 0.005685710271460597, 0.005116009679940445, 0.005646484636765143, 0.005939378048466562, 0.00588450004714866, 0.005836033882291684, 0.005802123815492662, 0.005563609044847273, 0.00582519790623337, 0.005783240115919778, 0.005778306651167398, 0.005970617442196885, 0.005639396697217815, 0.005714978789910674, 0.00541285811490271, 0.0059866565360882605, 0.0054269022779365956, 0.005740463324865803, 0.005457570256535397, 0.005899980999962535, 0.005436509603981015, 0.00591433365276993, 0.0055207577913046575, 0.006989194697505513, 0.0072830334673960545, 0.005627129443509634, 0.005663886138367965, 0.005563403533901586, 0.006616636044020916, 0.005647882767736392, 0.00582298606926532, 0.005661923511408616, 0.00594291165146197, 0.005352327651560827, 0.005549022253754354, 0.0053778747448611055, 0.005769846511484925, 0.005651114045555682, 0.0058820812562263985, 0.005703322697690753, 0.005888410397198831, 0.005353684653528035, 0.006061938721253429, 0.005480890441144449, 0.005980184767395258, 0.0054625668617095365, 0.005957885163424667, 0.005719169838960434, 0.005677555953044184, 0.0056689563965381575, 0.005811150535009801, 0.006043764510798419, 0.005798538767754338, 0.006017436116370697, 0.005535183696422813, 0.005832061255459002, 0.006022087141308327, 0.005513859070772522, 0.00572048932541335, 0.006985625512031622, 0.005213923162109283, 0.005977514535621848, 0.005291705117236043, 0.00600681185917279, 0.007238474417326235, 0.005798729791760791, 0.0060629063031397935, 0.005514776209629205, 0.005911818951866482, 0.005345465813027045, 0.00595968451112682, 0.005299388417475965, 0.005708133349693272, 0.005623712767522002, 0.0056507166984036215, 0.0060167485349919905, 0.007105667975211386, 0.005813384626255652, 0.005546708510062376, 0.005533253186555623, 0.005737099953558903, 0.005648446022966053, 0.005826299511935822, 0.005674425929481554, 0.005732911443987558, 0.00749533195156864, 0.005640153442834353, 0.005621981742077096, 0.005454984463772974, 0.005682165815689882, 0.005336996628144799, 0.005754463068686079]
[170.23522713936276, 173.22684743417858, 177.72936783585396, 177.92542275474847, 180.9676019541532, 163.10545489745707, 185.58245043252197, 174.0495099923984, 171.97314288136144, 182.0331335942276, 160.31944440054838, 174.8711375865204, 139.39341515739196, 179.97314471674002, 172.4090917312372, 169.66244066207233, 171.5774873581881, 167.72564889134844, 173.17463293882622, 162.14102446018893, 184.48891039019693, 177.88443147921652, 171.11866486576167, 178.5399195897336, 169.8130201842139, 174.291622066218, 167.08383194362543, 121.45825482717096, 182.82599576384646, 172.62107772104574, 173.12947111483248, 172.60864267480758, 178.537732454806, 172.6128721569294, 165.97010955071883, 175.262140118816, 171.725973383769, 181.32793185250264, 165.93944705104352, 177.92026927282598, 168.03918783015297, 174.876815195812, 171.48360210141405, 180.66841691699395, 167.5106666143188, 166.9553679175433, 181.94316103973617, 172.3867631109079, 175.7890475256662, 164.87712409013295, 173.3490820623347, 117.7945208701814, 178.9931229552265, 183.20373801407524, 181.87745428244443, 178.66918545179232, 171.26847678762954, 177.63711788120156, 166.81183210189403, 187.64987381666282, 169.96488354146484, 172.35906571279045, 176.28240508573182, 174.3782682142246, 181.61931024885436, 170.26839857105398, 153.18024073003642, 117.51540543693976, 155.83712566954802, 184.77967002451746, 166.22849649590006, 177.24283448567198, 170.7025707508807, 179.88648938445908, 174.9539218024772, 133.20652998410046, 170.71682772012446, 187.27512669727525, 170.3376237371636, 174.5503142604308, 170.18498067998073, 174.42503608928826, 131.9530563519534, 176.83782223190624, 179.13302008823297, 175.8795211601788, 195.4648373557497, 177.10134080394798, 168.3677974090539, 169.93797127838428, 171.34924508137394, 172.35068257761566, 179.73944465529047, 171.667987267854, 172.91344989243248, 173.06108179599102, 167.4868654173982, 177.3239326279969, 174.97877713306966, 184.745282209928, 167.03814457567157, 184.26718388970468, 174.2019665326191, 183.23172272542126, 169.49207124672944, 183.94154942129154, 169.0807551129041, 181.13455395109472, 143.07800015313728, 137.30542424069566, 177.71050231542233, 176.5572215913485, 179.74608419222562, 151.13420072479943, 177.0574994425369, 171.73319463671137, 176.61842269416536, 168.2676873976408, 186.83460077568003, 180.21192820472484, 185.94706039882433, 173.31483567361664, 176.95625887898157, 170.0078520577157, 175.3363877525105, 169.82511960710295, 186.78724368664638, 164.96372629006547, 182.45210531725078, 167.21891361152075, 183.06412082744654, 167.84479266887843, 174.85055141879974, 176.13212591305617, 176.39931056987254, 172.08296256918655, 165.45978888047273, 172.4572413934693, 166.1837335139224, 180.6624774975876, 171.46596309563225, 166.05538520698744, 181.36118227988996, 174.81022043996947, 143.15110340193016, 191.79415747190484, 167.29361242715385, 188.9750048132536, 166.4776629341129, 138.15065749301664, 172.4515602401175, 164.93739965635467, 181.3310208769535, 169.1526767213126, 187.07443560165942, 167.7941169759884, 188.70102004643923, 175.1886192451575, 177.8184699928467, 176.96870209092395, 166.20272464176222, 140.73272259393053, 172.0168308636566, 180.28710147394324, 180.72550925913563, 174.30409232798337, 177.03984351343598, 171.63552919848848, 176.22928071093273, 174.43144024992029, 133.41637254514362, 177.30014088011566, 177.87322084588277, 183.31857893291664, 175.98923235199325, 187.37130069119257, 173.77815932848287]
Elapsed: 0.2546924535582358~0.02345500843570012
Time per graph: 0.005854899652687611~0.0005277594641755645
Speed: 171.92882777559757~12.687997901875455
Total Time: 0.2480
best val loss: 0.40175142884254456 test_score: 0.7674

Testing...
Test loss: 0.6492 score: 0.7209 time: 0.24s
test Score 0.7209
Epoch Time List: [0.8516632040264085, 0.8506711019435897, 0.8377671010093763, 0.832902148948051, 0.835638041025959, 0.9168454060563818, 0.9344245268730447, 0.872180426842533, 0.8452723560621962, 0.8951164608588442, 0.8851969609968364, 0.7856000908650458, 0.915079406928271, 0.8724290120881051, 0.9828251118306071, 0.8532070700312033, 0.8062625620514154, 0.8352010269882157, 0.8049368839710951, 0.8732562850927934, 0.7981671020388603, 0.9747800970217213, 0.9000448259757832, 0.7985607050359249, 0.8274504239670932, 0.806539198034443, 0.8451857570325956, 1.0039323681266978, 0.8263145699165761, 0.8561057879123837, 0.8089968389831483, 0.8345479749841616, 0.8038675599964336, 0.8328209288883954, 0.8253129030345008, 0.8420890839770436, 0.8227660550037399, 1.034503179951571, 0.9621517879422754, 0.7996889180503786, 0.8254970939597115, 0.7901595642324537, 0.860626777051948, 0.7796866209246218, 0.9012399460189044, 0.9442814760841429, 0.8343248169403523, 0.8562544139567763, 0.8313235610257834, 0.8811052669771016, 0.8066619629971683, 0.947619580081664, 0.8427049340680242, 0.8136909530730918, 0.8429130939766765, 0.8456644529942423, 0.8547585691558197, 0.806820135912858, 0.9338290852028877, 0.8189379859250039, 0.8386800430016592, 0.9111267930129543, 0.8734518779674545, 0.8363219000166282, 0.7829537549987435, 0.8434253531740978, 0.8878382419934496, 1.0552120089996606, 1.0284449410391971, 0.8280587800545618, 0.844239863101393, 0.7802747581154108, 0.8194473129697144, 0.7816790760261938, 0.8323860689997673, 0.8997237069997936, 0.8645716210594401, 0.7820782029302791, 0.8568525530863553, 0.7897054359782487, 0.8470446760766208, 0.7960526871029288, 0.9278415930457413, 1.018859110889025, 0.809772192966193, 0.8398127588443458, 0.7608444510260597, 0.8263930020621046, 0.9290778820868582, 0.9119667118648067, 0.8806683929869905, 0.7966394668910652, 0.8041017460636795, 0.8199696779483929, 0.9507708420278504, 0.8809091438306496, 0.9767137828748673, 0.7839486058801413, 0.8695876859128475, 0.8107808571076021, 0.8292628631461412, 0.7786838208558038, 0.8113718639360741, 0.8544086291221902, 0.8482224079780281, 0.8005864800652489, 0.8331844841595739, 0.7845006319694221, 0.8626600360730663, 0.8815129029098898, 0.8305924899177626, 0.8666362660005689, 0.8126015680609271, 0.9516527859959751, 0.7986768431728706, 0.8224886289099231, 0.8092636869987473, 0.8256095900433138, 0.8527917879400775, 0.8143719929503277, 0.7942004549549893, 0.8020031959749758, 0.7869341020705178, 0.8165621990337968, 0.7808824080275372, 0.8263644619146362, 0.8717333269305527, 0.828926409012638, 0.8018829819047824, 0.8336612670682371, 0.8084847859572619, 0.8141050328267738, 0.7932890770025551, 0.9064006999833509, 0.9582039510132745, 0.8309596730396152, 0.8717949509155005, 0.8128158840117976, 0.8562610780354589, 0.9217996690422297, 0.9587448291713372, 0.845193778979592, 0.9137873268919066, 0.8333780210232362, 0.9089862530818209, 0.8970323459943756, 0.912231559981592, 0.80429837689735, 0.8403967241756618, 1.0197406940860674, 0.8945658119628206, 0.8474319479428232, 0.8882024029735476, 0.8324568981770426, 0.8166835471056402, 0.8403466920135543, 0.8077145038405433, 0.9092842360259965, 0.9080249170074239, 0.817767666070722, 0.9294916230719537, 0.8981587179005146, 0.8038122471189126, 0.8979491780046374, 0.9160978909349069, 0.9609790738904849, 0.8682698600459844, 0.8235772469779477, 0.8242586110718548, 0.8013805040391162, 0.9143012970453128, 0.9427479020087048, 0.9152075401507318, 0.7933677749242634, 0.9068941930308938, 0.7916356059722602, 0.833957894006744]
Total Epoch List: [88, 89]
Total Time List: [0.24889481300488114, 0.24803507199976593]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288336440>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.29s
Epoch 3/1000, LR 0.000030
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.20s
Epoch 4/1000, LR 0.000060
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.21s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.21s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.21s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.22s
Epoch 14/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.22s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.22s
Epoch 17/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.4884 time: 0.20s
Epoch 22/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.23s
Epoch 23/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.4884 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6912 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.30s
Epoch 26/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6895;  Loss pred: 0.6895; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.4884 time: 0.22s
Epoch 29/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.4884 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.4884 time: 0.20s
Epoch 33/1000, LR 0.000270
Train loss: 0.6867;  Loss pred: 0.6867; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6875 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6886 score: 0.4884 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6868 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6879 score: 0.4884 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6848;  Loss pred: 0.6848; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.4884 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4884 time: 0.21s
Epoch 37/1000, LR 0.000270
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6852 score: 0.4884 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6832 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6841 score: 0.4884 time: 0.21s
Epoch 39/1000, LR 0.000269
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.5000 time: 0.21s
Test loss: 0.6829 score: 0.5116 time: 0.33s
Epoch 40/1000, LR 0.000269
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6808 score: 0.5000 time: 0.33s
Test loss: 0.6816 score: 0.5116 time: 0.23s
Epoch 41/1000, LR 0.000269
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.39s
Val loss: 0.6793 score: 0.5455 time: 0.24s
Test loss: 0.6801 score: 0.5349 time: 0.22s
Epoch 42/1000, LR 0.000269
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.39s
Val loss: 0.6778 score: 0.5455 time: 0.24s
Test loss: 0.6784 score: 0.5349 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.34s
Val loss: 0.6760 score: 0.5682 time: 0.24s
Test loss: 0.6765 score: 0.5581 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6703;  Loss pred: 0.6703; Loss self: 0.0000; time: 0.37s
Val loss: 0.6741 score: 0.5909 time: 0.23s
Test loss: 0.6745 score: 0.5581 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.35s
Val loss: 0.6721 score: 0.6136 time: 0.24s
Test loss: 0.6723 score: 0.5581 time: 0.21s
Epoch 46/1000, LR 0.000269
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.37s
Val loss: 0.6698 score: 0.6136 time: 0.21s
Test loss: 0.6699 score: 0.5814 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.35s
Val loss: 0.6674 score: 0.6591 time: 0.23s
Test loss: 0.6672 score: 0.6279 time: 0.30s
Epoch 48/1000, LR 0.000269
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.39s
Val loss: 0.6647 score: 0.6591 time: 0.23s
Test loss: 0.6643 score: 0.6279 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6557;  Loss pred: 0.6557; Loss self: 0.0000; time: 0.44s
Val loss: 0.6619 score: 0.6818 time: 0.27s
Test loss: 0.6612 score: 0.6744 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6501;  Loss pred: 0.6501; Loss self: 0.0000; time: 0.38s
Val loss: 0.6589 score: 0.7045 time: 0.24s
Test loss: 0.6579 score: 0.7209 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.40s
Val loss: 0.6555 score: 0.7500 time: 0.22s
Test loss: 0.6543 score: 0.7442 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6392;  Loss pred: 0.6392; Loss self: 0.0000; time: 0.37s
Val loss: 0.6520 score: 0.7955 time: 0.24s
Test loss: 0.6507 score: 0.7674 time: 0.21s
Epoch 53/1000, LR 0.000269
Train loss: 0.6344;  Loss pred: 0.6344; Loss self: 0.0000; time: 0.40s
Val loss: 0.6482 score: 0.8409 time: 0.22s
Test loss: 0.6468 score: 0.7907 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6285;  Loss pred: 0.6285; Loss self: 0.0000; time: 0.35s
Val loss: 0.6441 score: 0.8864 time: 0.23s
Test loss: 0.6427 score: 0.8140 time: 0.21s
Epoch 55/1000, LR 0.000269
Train loss: 0.6213;  Loss pred: 0.6213; Loss self: 0.0000; time: 0.50s
Val loss: 0.6396 score: 0.8864 time: 0.23s
Test loss: 0.6382 score: 0.8140 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.45s
Val loss: 0.6348 score: 0.8864 time: 0.22s
Test loss: 0.6333 score: 0.8140 time: 0.22s
Epoch 57/1000, LR 0.000269
Train loss: 0.6100;  Loss pred: 0.6100; Loss self: 0.0000; time: 0.37s
Val loss: 0.6297 score: 0.9091 time: 0.22s
Test loss: 0.6279 score: 0.8140 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.6038;  Loss pred: 0.6038; Loss self: 0.0000; time: 0.35s
Val loss: 0.6242 score: 0.9091 time: 0.23s
Test loss: 0.6220 score: 0.8140 time: 0.20s
Epoch 59/1000, LR 0.000268
Train loss: 0.5978;  Loss pred: 0.5978; Loss self: 0.0000; time: 0.38s
Val loss: 0.6184 score: 0.9091 time: 0.22s
Test loss: 0.6159 score: 0.8372 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.38s
Val loss: 0.6122 score: 0.9091 time: 0.24s
Test loss: 0.6096 score: 0.8372 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.5798;  Loss pred: 0.5798; Loss self: 0.0000; time: 0.37s
Val loss: 0.6056 score: 0.9091 time: 0.29s
Test loss: 0.6031 score: 0.8372 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.5675;  Loss pred: 0.5675; Loss self: 0.0000; time: 0.36s
Val loss: 0.5985 score: 0.9091 time: 0.23s
Test loss: 0.5966 score: 0.8372 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.5587;  Loss pred: 0.5587; Loss self: 0.0000; time: 0.39s
Val loss: 0.5910 score: 0.9091 time: 0.23s
Test loss: 0.5899 score: 0.8372 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5407;  Loss pred: 0.5407; Loss self: 0.0000; time: 0.35s
Val loss: 0.5829 score: 0.9091 time: 0.24s
Test loss: 0.5828 score: 0.8372 time: 0.21s
Epoch 65/1000, LR 0.000268
Train loss: 0.5358;  Loss pred: 0.5358; Loss self: 0.0000; time: 0.39s
Val loss: 0.5744 score: 0.9091 time: 0.23s
Test loss: 0.5757 score: 0.8140 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5302;  Loss pred: 0.5302; Loss self: 0.0000; time: 0.37s
Val loss: 0.5655 score: 0.9091 time: 0.23s
Test loss: 0.5682 score: 0.8140 time: 0.21s
Epoch 67/1000, LR 0.000268
Train loss: 0.5035;  Loss pred: 0.5035; Loss self: 0.0000; time: 0.39s
Val loss: 0.5561 score: 0.9091 time: 0.23s
Test loss: 0.5603 score: 0.8140 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.4978;  Loss pred: 0.4978; Loss self: 0.0000; time: 0.37s
Val loss: 0.5462 score: 0.9091 time: 0.23s
Test loss: 0.5515 score: 0.8140 time: 0.20s
Epoch 69/1000, LR 0.000268
Train loss: 0.4804;  Loss pred: 0.4804; Loss self: 0.0000; time: 0.39s
Val loss: 0.5360 score: 0.9091 time: 0.23s
Test loss: 0.5420 score: 0.8140 time: 0.23s
Epoch 70/1000, LR 0.000268
Train loss: 0.4619;  Loss pred: 0.4619; Loss self: 0.0000; time: 0.44s
Val loss: 0.5253 score: 0.9091 time: 0.23s
Test loss: 0.5325 score: 0.8140 time: 0.20s
Epoch 71/1000, LR 0.000268
Train loss: 0.4584;  Loss pred: 0.4584; Loss self: 0.0000; time: 0.39s
Val loss: 0.5144 score: 0.9091 time: 0.23s
Test loss: 0.5225 score: 0.8372 time: 0.23s
Epoch 72/1000, LR 0.000267
Train loss: 0.4381;  Loss pred: 0.4381; Loss self: 0.0000; time: 0.37s
Val loss: 0.5033 score: 0.9091 time: 0.23s
Test loss: 0.5121 score: 0.8372 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.4295;  Loss pred: 0.4295; Loss self: 0.0000; time: 0.38s
Val loss: 0.4921 score: 0.9091 time: 0.21s
Test loss: 0.5013 score: 0.8605 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.4187;  Loss pred: 0.4187; Loss self: 0.0000; time: 0.34s
Val loss: 0.4811 score: 0.9091 time: 0.23s
Test loss: 0.4900 score: 0.8605 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.3881;  Loss pred: 0.3881; Loss self: 0.0000; time: 0.39s
Val loss: 0.4696 score: 0.9091 time: 0.21s
Test loss: 0.4792 score: 0.8605 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.3807;  Loss pred: 0.3807; Loss self: 0.0000; time: 0.35s
Val loss: 0.4583 score: 0.9091 time: 0.24s
Test loss: 0.4683 score: 0.8605 time: 0.19s
Epoch 77/1000, LR 0.000267
Train loss: 0.3637;  Loss pred: 0.3637; Loss self: 0.0000; time: 0.40s
Val loss: 0.4463 score: 0.9091 time: 0.20s
Test loss: 0.4583 score: 0.8605 time: 0.31s
Epoch 78/1000, LR 0.000267
Train loss: 0.3486;  Loss pred: 0.3486; Loss self: 0.0000; time: 0.37s
Val loss: 0.4341 score: 0.9091 time: 0.24s
Test loss: 0.4490 score: 0.8605 time: 0.21s
Epoch 79/1000, LR 0.000267
Train loss: 0.3341;  Loss pred: 0.3341; Loss self: 0.0000; time: 0.40s
Val loss: 0.4223 score: 0.9091 time: 0.22s
Test loss: 0.4397 score: 0.8605 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.3104;  Loss pred: 0.3104; Loss self: 0.0000; time: 0.35s
Val loss: 0.4103 score: 0.8864 time: 0.24s
Test loss: 0.4317 score: 0.8605 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.3063;  Loss pred: 0.3063; Loss self: 0.0000; time: 0.39s
Val loss: 0.3988 score: 0.8864 time: 0.21s
Test loss: 0.4243 score: 0.8372 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.2973;  Loss pred: 0.2973; Loss self: 0.0000; time: 0.33s
Val loss: 0.3881 score: 0.9091 time: 0.24s
Test loss: 0.4182 score: 0.8140 time: 0.22s
Epoch 83/1000, LR 0.000266
Train loss: 0.2734;  Loss pred: 0.2734; Loss self: 0.0000; time: 0.41s
Val loss: 0.3780 score: 0.9091 time: 0.22s
Test loss: 0.4103 score: 0.8140 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2326;  Loss pred: 0.2326; Loss self: 0.0000; time: 0.35s
Val loss: 0.3684 score: 0.9091 time: 0.25s
Test loss: 0.4022 score: 0.8372 time: 0.21s
Epoch 85/1000, LR 0.000266
Train loss: 0.2504;  Loss pred: 0.2504; Loss self: 0.0000; time: 0.38s
Val loss: 0.3594 score: 0.9091 time: 0.21s
Test loss: 0.3949 score: 0.8372 time: 0.32s
Epoch 86/1000, LR 0.000266
Train loss: 0.2370;  Loss pred: 0.2370; Loss self: 0.0000; time: 0.44s
Val loss: 0.3514 score: 0.8864 time: 0.22s
Test loss: 0.3879 score: 0.8605 time: 0.26s
Epoch 87/1000, LR 0.000266
Train loss: 0.2074;  Loss pred: 0.2074; Loss self: 0.0000; time: 0.41s
Val loss: 0.3447 score: 0.8864 time: 0.23s
Test loss: 0.3814 score: 0.8605 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.1766;  Loss pred: 0.1766; Loss self: 0.0000; time: 0.50s
Val loss: 0.3385 score: 0.9091 time: 0.22s
Test loss: 0.3763 score: 0.8605 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.1896;  Loss pred: 0.1896; Loss self: 0.0000; time: 0.35s
Val loss: 0.3338 score: 0.9091 time: 0.24s
Test loss: 0.3720 score: 0.8605 time: 0.22s
Epoch 90/1000, LR 0.000266
Train loss: 0.1674;  Loss pred: 0.1674; Loss self: 0.0000; time: 0.40s
Val loss: 0.3269 score: 0.9091 time: 0.21s
Test loss: 0.3704 score: 0.8605 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.1993;  Loss pred: 0.1993; Loss self: 0.0000; time: 0.33s
Val loss: 0.3209 score: 0.9318 time: 0.25s
Test loss: 0.3695 score: 0.8605 time: 0.21s
Epoch 92/1000, LR 0.000266
Train loss: 0.1444;  Loss pred: 0.1444; Loss self: 0.0000; time: 0.38s
Val loss: 0.3138 score: 0.9091 time: 0.21s
Test loss: 0.3706 score: 0.8605 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.1396;  Loss pred: 0.1396; Loss self: 0.0000; time: 0.43s
Val loss: 0.3087 score: 0.9091 time: 0.23s
Test loss: 0.3716 score: 0.8605 time: 0.21s
Epoch 94/1000, LR 0.000265
Train loss: 0.1148;  Loss pred: 0.1148; Loss self: 0.0000; time: 0.42s
Val loss: 0.3030 score: 0.9091 time: 0.23s
Test loss: 0.3745 score: 0.8605 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.1115;  Loss pred: 0.1115; Loss self: 0.0000; time: 0.43s
Val loss: 0.2977 score: 0.9091 time: 0.22s
Test loss: 0.3787 score: 0.8140 time: 0.23s
Epoch 96/1000, LR 0.000265
Train loss: 0.1266;  Loss pred: 0.1266; Loss self: 0.0000; time: 0.39s
Val loss: 0.2927 score: 0.9091 time: 0.22s
Test loss: 0.3855 score: 0.8140 time: 0.21s
Epoch 97/1000, LR 0.000265
Train loss: 0.1055;  Loss pred: 0.1055; Loss self: 0.0000; time: 0.39s
Val loss: 0.2892 score: 0.9091 time: 0.22s
Test loss: 0.3921 score: 0.8140 time: 0.22s
Epoch 98/1000, LR 0.000265
Train loss: 0.1165;  Loss pred: 0.1165; Loss self: 0.0000; time: 0.38s
Val loss: 0.2869 score: 0.9091 time: 0.22s
Test loss: 0.4007 score: 0.8140 time: 0.22s
Epoch 99/1000, LR 0.000265
Train loss: 0.0881;  Loss pred: 0.0881; Loss self: 0.0000; time: 0.38s
Val loss: 0.2854 score: 0.8864 time: 0.22s
Test loss: 0.4096 score: 0.8140 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.39s
Val loss: 0.2840 score: 0.8864 time: 0.23s
Test loss: 0.4164 score: 0.8140 time: 0.31s
Epoch 101/1000, LR 0.000265
Train loss: 0.0743;  Loss pred: 0.0743; Loss self: 0.0000; time: 0.40s
Val loss: 0.2829 score: 0.8864 time: 0.22s
Test loss: 0.4229 score: 0.8140 time: 0.22s
Epoch 102/1000, LR 0.000264
Train loss: 0.0777;  Loss pred: 0.0777; Loss self: 0.0000; time: 0.36s
Val loss: 0.2820 score: 0.8864 time: 0.23s
Test loss: 0.4287 score: 0.8140 time: 0.22s
Epoch 103/1000, LR 0.000264
Train loss: 0.0598;  Loss pred: 0.0598; Loss self: 0.0000; time: 0.39s
Val loss: 0.2813 score: 0.8864 time: 0.22s
Test loss: 0.4337 score: 0.8140 time: 0.23s
Epoch 104/1000, LR 0.000264
Train loss: 0.0681;  Loss pred: 0.0681; Loss self: 0.0000; time: 0.34s
Val loss: 0.2809 score: 0.8864 time: 0.24s
Test loss: 0.4351 score: 0.8140 time: 0.22s
Epoch 105/1000, LR 0.000264
Train loss: 0.0773;  Loss pred: 0.0773; Loss self: 0.0000; time: 0.38s
Val loss: 0.2815 score: 0.9091 time: 0.22s
Test loss: 0.4340 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 106/1000, LR 0.000264
Train loss: 0.0494;  Loss pred: 0.0494; Loss self: 0.0000; time: 0.35s
Val loss: 0.2839 score: 0.9091 time: 0.26s
Test loss: 0.4349 score: 0.8140 time: 0.20s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 103,   Train_Loss: 0.0681,   Val_Loss: 0.2809,   Val_Precision: 0.9048,   Val_Recall: 0.8636,   Val_accuracy: 0.8837,   Val_Score: 0.8864,   Val_Loss: 0.2809,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4351


[0.2584658929845318, 0.2540021979948506, 0.24756741407327354, 0.24729462107643485, 0.24313744297251105, 0.269764123018831, 0.2370913838967681, 0.25280163099523634, 0.25585390406195074, 0.2417142370250076, 0.27445204893592745, 0.2516138489590958, 0.3156533610308543, 0.24448092002421618, 0.25520695897284895, 0.25933848309796304, 0.256443899939768, 0.2623331630602479, 0.25407878309488297, 0.2713687060168013, 0.23849671997595578, 0.24735160707496107, 0.2571315059904009, 0.2464434850262478, 0.25910851801745594, 0.25245045905467123, 0.26334086002316326, 0.3622643850976601, 0.24066599400248379, 0.25489355402532965, 0.25414506101515144, 0.25491191702894866, 0.24644650402478874, 0.2549056709976867, 0.26510797708760947, 0.25105250894557685, 0.25622216099873185, 0.24265428690705448, 0.26515696407295763, 0.24730178399477154, 0.2618436840130016, 0.25160567997954786, 0.256584299961105, 0.24354007607325912, 0.262669840012677, 0.26354348799213767, 0.2418337669223547, 0.2552400149870664, 0.2503000079886988, 0.266865401994437, 0.25382309197448194, 0.37353180500213057, 0.24581950006540865, 0.24016977206338197, 0.24192113406024873, 0.24626518494915217, 0.2569065879797563, 0.24769598001148552, 0.26377025805413723, 0.23447924107313156, 0.2588770049624145, 0.25528103101532906, 0.2495994990458712, 0.2523250199155882, 0.24226498790085316, 0.25841553905047476, 0.2872433140873909, 0.37441899499390274, 0.2823460700456053, 0.2381214339984581, 0.26469589106272906, 0.24824698909651488, 0.25775827397592366, 0.24459869193378836, 0.25149479100946337, 0.33031413704156876, 0.2577367479680106, 0.2349484460428357, 0.25831051904242486, 0.25207631499506533, 0.25854220404289663, 0.25225736503489316, 0.3334519200725481, 0.24881555000320077, 0.24562752293422818, 0.25017125194426626, 0.22510442591737956, 0.24844532401766628, 0.25539325608406216, 0.2530335020273924, 0.2509494569385424, 0.24949132406618446, 0.23923518892843276, 0.2504835099680349, 0.24867932498455048, 0.24846718600019813, 0.25673655001446605, 0.24249405798036605, 0.24574408796615899, 0.23275289894081652, 0.2574262310517952, 0.23335679795127362, 0.24683992296922952, 0.23467552103102207, 0.253699182998389, 0.23376991297118366, 0.254316347069107, 0.23739258502610028, 0.30053537199273705, 0.31317043909803033, 0.24196656607091427, 0.24354710394982249, 0.2392263519577682, 0.2845153498928994, 0.24285895901266485, 0.25038840097840875, 0.24346271099057049, 0.2555452010128647, 0.23015008901711553, 0.2386079569114372, 0.23124861402902752, 0.24810339999385178, 0.2429979039588943, 0.2529294940177351, 0.24524287600070238, 0.25320164707954973, 0.2302084401017055, 0.2606633650138974, 0.23567828896921128, 0.2571479449979961, 0.23489037505351007, 0.25618906202726066, 0.24592430307529867, 0.24413490598089993, 0.24376512505114079, 0.24987947300542146, 0.25988187396433204, 0.24933716701343656, 0.25874975300394, 0.23801289894618094, 0.2507786339847371, 0.25894974707625806, 0.23709594004321843, 0.24598104099277407, 0.30038189701735973, 0.2241986959706992, 0.2570331250317395, 0.22754332004114985, 0.25829290994443, 0.3112543999450281, 0.24934538104571402, 0.2607049710350111, 0.23713537701405585, 0.2542082149302587, 0.22985502996016294, 0.2562664339784533, 0.2278737019514665, 0.2454497340368107, 0.2418196490034461, 0.24298081803135574, 0.2587201870046556, 0.3055437229340896, 0.24997553892899305, 0.23850846593268216, 0.23792988702189177, 0.2466952980030328, 0.2428831789875403, 0.25053087901324034, 0.2440003149677068, 0.246515192091465, 0.3222992739174515, 0.24252659804187715, 0.2417452149093151, 0.2345643319422379, 0.24433313007466495, 0.22949085501022637, 0.2474419119535014, 0.21716240094974637, 0.2980951420031488, 0.20655052992515266, 0.23404859704896808, 0.2104844219284132, 0.23098605789709836, 0.22044231102336198, 0.23631474399007857, 0.2218096029246226, 0.23414966103155166, 0.20974706194829196, 0.21913242095615715, 0.2252126430394128, 0.2225733830127865, 0.2315183370374143, 0.2235867700073868, 0.22348590195178986, 0.23482662497553974, 0.20934334804769605, 0.23234290501568466, 0.2071625420358032, 0.23106337594799697, 0.20763443293981254, 0.23381374299060553, 0.3058066499652341, 0.20076202298514545, 0.2325230260612443, 0.22349390701856464, 0.2364962559659034, 0.22014820505864918, 0.22552069299854338, 0.20765619992744178, 0.23086854198481888, 0.2214040809776634, 0.24702477199025452, 0.21595241001341492, 0.2512244980316609, 0.21671045699622482, 0.3318469140212983, 0.2297075999667868, 0.22063789004459977, 0.22724773094523698, 0.21057162492070347, 0.23589389002881944, 0.21385955798905343, 0.23309838504064828, 0.30589558300562203, 0.22068567702081054, 0.22331926692277193, 0.2304031120147556, 0.23603558994363993, 0.21052900096401572, 0.22925837303046137, 0.21593812806531787, 0.22174101404380053, 0.2202855529030785, 0.23254179197829217, 0.20828876295126975, 0.2339435399044305, 0.20642237202264369, 0.2357885210076347, 0.209385373047553, 0.23631588893476874, 0.21008719399105757, 0.23648078995756805, 0.20937818300444633, 0.23666278494056314, 0.20766974799335003, 0.23606337804812938, 0.20755142206326127, 0.23792923300061375, 0.22084821504540741, 0.22770417702849954, 0.22126579098403454, 0.23291679797694087, 0.19912107498385012, 0.314639811986126, 0.21454613702371716, 0.229614740004763, 0.22389867005404085, 0.2306007120059803, 0.2205613220576197, 0.23742982104886323, 0.21723330405075103, 0.3196422689361498, 0.2647168911062181, 0.21763958304654807, 0.23556781793013215, 0.22334014205262065, 0.23821966396644711, 0.21380179899279028, 0.2343724239617586, 0.2100951949832961, 0.2198510430753231, 0.23121659306343645, 0.21840075100772083, 0.22123875201214105, 0.2226196259725839, 0.22014631307683885, 0.3103762329556048, 0.2286907669622451, 0.22307876904960722, 0.2300226790830493, 0.22381526604294777, 0.2366585920099169, 0.20834403298795223]
[0.005874224840557541, 0.005772777227155695, 0.005626532138028944, 0.005620332297191701, 0.005525850976647978, 0.006131002795882523, 0.005388440543108366, 0.005745491613528098, 0.005814861455953426, 0.005493505386931991, 0.006237546566725624, 0.005718496567252177, 0.007173940023428507, 0.005556384546004913, 0.00580015815847384, 0.0058940564340446144, 0.005828270453176546, 0.005962117342278361, 0.005774517797610976, 0.0061674705912909385, 0.00542037999945354, 0.005621627433521842, 0.005843897863418202, 0.005600988296051087, 0.00588882995494218, 0.00573751043306071, 0.005985019545980983, 0.008233281479492276, 0.005469681681874631, 0.005793035318757492, 0.005776024113980715, 0.005793452659748833, 0.0056010569096542895, 0.005793310704492879, 0.00602518129744567, 0.005705738839672201, 0.0058232309317893605, 0.005514870156978511, 0.0060262946380217645, 0.005620495090790262, 0.00595099281847731, 0.0057183109086260875, 0.005831461362752386, 0.005535001728937707, 0.005969769091197205, 0.005989624727094038, 0.005496221975508061, 0.005800909431524236, 0.0056886365451977, 0.00606512277260084, 0.00576870663578368, 0.008489359204593877, 0.005586806819668378, 0.005458403910531409, 0.005498207592278381, 0.005596936021571641, 0.005838786090449007, 0.005629454091170125, 0.0059947785921394825, 0.00532907366075299, 0.00588356829460033, 0.005801841613984751, 0.005672715887406163, 0.005734659543536095, 0.005506022452292117, 0.005873080432965336, 0.006528257138349793, 0.00850952261349779, 0.0064169561374001205, 0.005411850772692229, 0.006015815705971115, 0.005641977024920793, 0.005858142590361901, 0.0055590611803133716, 0.005715790704760531, 0.007507139478217472, 0.005857653362909332, 0.005339737410064448, 0.005870693614600565, 0.005729007158978758, 0.005875959182793105, 0.005733121932611208, 0.0075784527289215475, 0.005654898863709109, 0.005582443703050641, 0.005685710271460597, 0.005116009679940445, 0.005646484636765143, 0.005939378048466562, 0.00588450004714866, 0.005836033882291684, 0.005802123815492662, 0.005563609044847273, 0.00582519790623337, 0.005783240115919778, 0.005778306651167398, 0.005970617442196885, 0.005639396697217815, 0.005714978789910674, 0.00541285811490271, 0.0059866565360882605, 0.0054269022779365956, 0.005740463324865803, 0.005457570256535397, 0.005899980999962535, 0.005436509603981015, 0.00591433365276993, 0.0055207577913046575, 0.006989194697505513, 0.0072830334673960545, 0.005627129443509634, 0.005663886138367965, 0.005563403533901586, 0.006616636044020916, 0.005647882767736392, 0.00582298606926532, 0.005661923511408616, 0.00594291165146197, 0.005352327651560827, 0.005549022253754354, 0.0053778747448611055, 0.005769846511484925, 0.005651114045555682, 0.0058820812562263985, 0.005703322697690753, 0.005888410397198831, 0.005353684653528035, 0.006061938721253429, 0.005480890441144449, 0.005980184767395258, 0.0054625668617095365, 0.005957885163424667, 0.005719169838960434, 0.005677555953044184, 0.0056689563965381575, 0.005811150535009801, 0.006043764510798419, 0.005798538767754338, 0.006017436116370697, 0.005535183696422813, 0.005832061255459002, 0.006022087141308327, 0.005513859070772522, 0.00572048932541335, 0.006985625512031622, 0.005213923162109283, 0.005977514535621848, 0.005291705117236043, 0.00600681185917279, 0.007238474417326235, 0.005798729791760791, 0.0060629063031397935, 0.005514776209629205, 0.005911818951866482, 0.005345465813027045, 0.00595968451112682, 0.005299388417475965, 0.005708133349693272, 0.005623712767522002, 0.0056507166984036215, 0.0060167485349919905, 0.007105667975211386, 0.005813384626255652, 0.005546708510062376, 0.005533253186555623, 0.005737099953558903, 0.005648446022966053, 0.005826299511935822, 0.005674425929481554, 0.005732911443987558, 0.00749533195156864, 0.005640153442834353, 0.005621981742077096, 0.005454984463772974, 0.005682165815689882, 0.005336996628144799, 0.005754463068686079, 0.005050288394180148, 0.006932445162863925, 0.0048035006959337824, 0.0054429906290457694, 0.004894986556474726, 0.005371768788304613, 0.005126565372636325, 0.005495691720699501, 0.005158362858712153, 0.005445340954222132, 0.004877838649960278, 0.005096102812933887, 0.005237503326497972, 0.005176125186343872, 0.005384147372963124, 0.005199692325753181, 0.005197346557018368, 0.005461084301756738, 0.004868449954597583, 0.005403323372457783, 0.0048177335357163535, 0.0053735668825115575, 0.004828707742786338, 0.005437528906758268, 0.007111782557331026, 0.004668884255468499, 0.005407512233982426, 0.005197532721361969, 0.005499912929439614, 0.005119725699038353, 0.005244667279035892, 0.004829213951800971, 0.005369035860112067, 0.005148932115759614, 0.005744762139308245, 0.005022149070079417, 0.005842430186782812, 0.005039778069679647, 0.007717370093518565, 0.005342037208529926, 0.005131113721967437, 0.005284830952214814, 0.004897014533039616, 0.005485904419274871, 0.004973478092768684, 0.005420892675363913, 0.007113850767572605, 0.005132225046995594, 0.005193471323785393, 0.005358211907319898, 0.005489199766131161, 0.004896023278232924, 0.005331590070475845, 0.005021816931751578, 0.005156767768460477, 0.005122919834955314, 0.005407948650657957, 0.004843924719796971, 0.0054405474396379185, 0.004800520279596365, 0.005483453976921737, 0.004869427280175651, 0.005495718347320203, 0.004885748697466455, 0.005499553254827164, 0.004869260069870845, 0.005503785696292166, 0.004829529023101163, 0.005489846001119288, 0.004826777257285145, 0.00553323797675846, 0.005136005001055987, 0.005295445977406966, 0.005145716069396152, 0.0054166697203939735, 0.004630722674043026, 0.007317204929909907, 0.00498944504706319, 0.005339877674529372, 0.005206945815210252, 0.00536280725595303, 0.005129333071107435, 0.005521623745322401, 0.005051937303505838, 0.007433541138049995, 0.006156206769912049, 0.005061385652245304, 0.005478321347212376, 0.005193956791921411, 0.005539992185266212, 0.004972134860297448, 0.005450521487482759, 0.004885934767053398, 0.005112814955240072, 0.005377130071242708, 0.005079087232737694, 0.0051450872560963035, 0.005177200604013579, 0.005119681699461368, 0.0072180519292001115, 0.0053183899293545375, 0.005187878349990865, 0.005349364629838356, 0.005205006187045297, 0.0055036881862771374, 0.004845210069487261]
[170.23522713936276, 173.22684743417858, 177.72936783585396, 177.92542275474847, 180.9676019541532, 163.10545489745707, 185.58245043252197, 174.0495099923984, 171.97314288136144, 182.0331335942276, 160.31944440054838, 174.8711375865204, 139.39341515739196, 179.97314471674002, 172.4090917312372, 169.66244066207233, 171.5774873581881, 167.72564889134844, 173.17463293882622, 162.14102446018893, 184.48891039019693, 177.88443147921652, 171.11866486576167, 178.5399195897336, 169.8130201842139, 174.291622066218, 167.08383194362543, 121.45825482717096, 182.82599576384646, 172.62107772104574, 173.12947111483248, 172.60864267480758, 178.537732454806, 172.6128721569294, 165.97010955071883, 175.262140118816, 171.725973383769, 181.32793185250264, 165.93944705104352, 177.92026927282598, 168.03918783015297, 174.876815195812, 171.48360210141405, 180.66841691699395, 167.5106666143188, 166.9553679175433, 181.94316103973617, 172.3867631109079, 175.7890475256662, 164.87712409013295, 173.3490820623347, 117.7945208701814, 178.9931229552265, 183.20373801407524, 181.87745428244443, 178.66918545179232, 171.26847678762954, 177.63711788120156, 166.81183210189403, 187.64987381666282, 169.96488354146484, 172.35906571279045, 176.28240508573182, 174.3782682142246, 181.61931024885436, 170.26839857105398, 153.18024073003642, 117.51540543693976, 155.83712566954802, 184.77967002451746, 166.22849649590006, 177.24283448567198, 170.7025707508807, 179.88648938445908, 174.9539218024772, 133.20652998410046, 170.71682772012446, 187.27512669727525, 170.3376237371636, 174.5503142604308, 170.18498067998073, 174.42503608928826, 131.9530563519534, 176.83782223190624, 179.13302008823297, 175.8795211601788, 195.4648373557497, 177.10134080394798, 168.3677974090539, 169.93797127838428, 171.34924508137394, 172.35068257761566, 179.73944465529047, 171.667987267854, 172.91344989243248, 173.06108179599102, 167.4868654173982, 177.3239326279969, 174.97877713306966, 184.745282209928, 167.03814457567157, 184.26718388970468, 174.2019665326191, 183.23172272542126, 169.49207124672944, 183.94154942129154, 169.0807551129041, 181.13455395109472, 143.07800015313728, 137.30542424069566, 177.71050231542233, 176.5572215913485, 179.74608419222562, 151.13420072479943, 177.0574994425369, 171.73319463671137, 176.61842269416536, 168.2676873976408, 186.83460077568003, 180.21192820472484, 185.94706039882433, 173.31483567361664, 176.95625887898157, 170.0078520577157, 175.3363877525105, 169.82511960710295, 186.78724368664638, 164.96372629006547, 182.45210531725078, 167.21891361152075, 183.06412082744654, 167.84479266887843, 174.85055141879974, 176.13212591305617, 176.39931056987254, 172.08296256918655, 165.45978888047273, 172.4572413934693, 166.1837335139224, 180.6624774975876, 171.46596309563225, 166.05538520698744, 181.36118227988996, 174.81022043996947, 143.15110340193016, 191.79415747190484, 167.29361242715385, 188.9750048132536, 166.4776629341129, 138.15065749301664, 172.4515602401175, 164.93739965635467, 181.3310208769535, 169.1526767213126, 187.07443560165942, 167.7941169759884, 188.70102004643923, 175.1886192451575, 177.8184699928467, 176.96870209092395, 166.20272464176222, 140.73272259393053, 172.0168308636566, 180.28710147394324, 180.72550925913563, 174.30409232798337, 177.03984351343598, 171.63552919848848, 176.22928071093273, 174.43144024992029, 133.41637254514362, 177.30014088011566, 177.87322084588277, 183.31857893291664, 175.98923235199325, 187.37130069119257, 173.77815932848287, 198.0084941589435, 144.2492477772274, 208.18150413645432, 183.72252832177182, 204.29065299010352, 186.158421817632, 195.0623716489842, 181.96071592471316, 193.85995661609232, 183.6432297640856, 205.00883111583516, 196.2283801382508, 190.9306663235371, 193.1947091693786, 185.73042874375443, 192.31907146643505, 192.40587269471663, 183.11381856499028, 205.4041859987977, 185.0712850349238, 207.56648174634032, 186.09612978942005, 207.09474527505034, 183.9070682929348, 140.61172314234662, 214.18393459395259, 184.92792188535435, 192.39898113387127, 181.82106022938257, 195.32296431190284, 190.6698646065163, 207.07303714035436, 186.25317953810932, 194.21502896479177, 174.07161093016376, 199.1179445384696, 171.1616515781867, 198.42143566126612, 129.57782092630885, 187.19450295165387, 194.88946341586197, 189.2208112316083, 204.2060511058545, 182.2853486995642, 201.0665335902405, 184.47146252215154, 140.57084308801447, 194.8472623166438, 192.54944095293945, 186.62942363923526, 182.17591681944, 204.24739491045082, 187.56130662362602, 199.131114015183, 193.91992133447266, 195.20118061904492, 184.91299836553276, 206.444166217743, 183.80503269107646, 208.3107542010179, 182.36680825784427, 205.3629600489542, 181.95983432950405, 204.67692096373236, 181.8329514533317, 205.3700122093755, 181.69312091378993, 207.05952800297587, 182.15447205552155, 207.1775735022123, 180.7260060384807, 194.70386025605416, 188.84150726237272, 194.33641236978502, 184.61528053574335, 215.94901495729445, 136.66420574233015, 200.42309125913803, 187.27020747495578, 192.05116309811663, 186.46950230962364, 194.95711940267856, 181.10614669229173, 197.94386587221518, 134.52538721838906, 162.43768888456074, 197.57435388397752, 182.53766740223196, 192.531443764681, 180.50566978407159, 201.12085212833043, 183.46868318866768, 204.6691263139967, 195.5869728817606, 185.972812029985, 196.88576985927983, 194.36016343845705, 193.15457840763577, 195.32464295684787, 138.54153583386838, 188.02683016537756, 192.75702561563742, 186.93808876330374, 192.1227303223756, 181.69634000948562, 206.38940018256503]
Elapsed: 0.24537576893517293~0.026784848726573215
Time per graph: 0.005663770201779243~0.0005989442162357798
Speed: 178.2718011672342~16.44928262304208
Total Time: 0.2091
best val loss: 0.28086286783218384 test_score: 0.8140

Testing...
Test loss: 0.3695 score: 0.8605 time: 0.22s
test Score 0.8605
Epoch Time List: [0.8516632040264085, 0.8506711019435897, 0.8377671010093763, 0.832902148948051, 0.835638041025959, 0.9168454060563818, 0.9344245268730447, 0.872180426842533, 0.8452723560621962, 0.8951164608588442, 0.8851969609968364, 0.7856000908650458, 0.915079406928271, 0.8724290120881051, 0.9828251118306071, 0.8532070700312033, 0.8062625620514154, 0.8352010269882157, 0.8049368839710951, 0.8732562850927934, 0.7981671020388603, 0.9747800970217213, 0.9000448259757832, 0.7985607050359249, 0.8274504239670932, 0.806539198034443, 0.8451857570325956, 1.0039323681266978, 0.8263145699165761, 0.8561057879123837, 0.8089968389831483, 0.8345479749841616, 0.8038675599964336, 0.8328209288883954, 0.8253129030345008, 0.8420890839770436, 0.8227660550037399, 1.034503179951571, 0.9621517879422754, 0.7996889180503786, 0.8254970939597115, 0.7901595642324537, 0.860626777051948, 0.7796866209246218, 0.9012399460189044, 0.9442814760841429, 0.8343248169403523, 0.8562544139567763, 0.8313235610257834, 0.8811052669771016, 0.8066619629971683, 0.947619580081664, 0.8427049340680242, 0.8136909530730918, 0.8429130939766765, 0.8456644529942423, 0.8547585691558197, 0.806820135912858, 0.9338290852028877, 0.8189379859250039, 0.8386800430016592, 0.9111267930129543, 0.8734518779674545, 0.8363219000166282, 0.7829537549987435, 0.8434253531740978, 0.8878382419934496, 1.0552120089996606, 1.0284449410391971, 0.8280587800545618, 0.844239863101393, 0.7802747581154108, 0.8194473129697144, 0.7816790760261938, 0.8323860689997673, 0.8997237069997936, 0.8645716210594401, 0.7820782029302791, 0.8568525530863553, 0.7897054359782487, 0.8470446760766208, 0.7960526871029288, 0.9278415930457413, 1.018859110889025, 0.809772192966193, 0.8398127588443458, 0.7608444510260597, 0.8263930020621046, 0.9290778820868582, 0.9119667118648067, 0.8806683929869905, 0.7966394668910652, 0.8041017460636795, 0.8199696779483929, 0.9507708420278504, 0.8809091438306496, 0.9767137828748673, 0.7839486058801413, 0.8695876859128475, 0.8107808571076021, 0.8292628631461412, 0.7786838208558038, 0.8113718639360741, 0.8544086291221902, 0.8482224079780281, 0.8005864800652489, 0.8331844841595739, 0.7845006319694221, 0.8626600360730663, 0.8815129029098898, 0.8305924899177626, 0.8666362660005689, 0.8126015680609271, 0.9516527859959751, 0.7986768431728706, 0.8224886289099231, 0.8092636869987473, 0.8256095900433138, 0.8527917879400775, 0.8143719929503277, 0.7942004549549893, 0.8020031959749758, 0.7869341020705178, 0.8165621990337968, 0.7808824080275372, 0.8263644619146362, 0.8717333269305527, 0.828926409012638, 0.8018829819047824, 0.8336612670682371, 0.8084847859572619, 0.8141050328267738, 0.7932890770025551, 0.9064006999833509, 0.9582039510132745, 0.8309596730396152, 0.8717949509155005, 0.8128158840117976, 0.8562610780354589, 0.9217996690422297, 0.9587448291713372, 0.845193778979592, 0.9137873268919066, 0.8333780210232362, 0.9089862530818209, 0.8970323459943756, 0.912231559981592, 0.80429837689735, 0.8403967241756618, 1.0197406940860674, 0.8945658119628206, 0.8474319479428232, 0.8882024029735476, 0.8324568981770426, 0.8166835471056402, 0.8403466920135543, 0.8077145038405433, 0.9092842360259965, 0.9080249170074239, 0.817767666070722, 0.9294916230719537, 0.8981587179005146, 0.8038122471189126, 0.8979491780046374, 0.9160978909349069, 0.9609790738904849, 0.8682698600459844, 0.8235772469779477, 0.8242586110718548, 0.8013805040391162, 0.9143012970453128, 0.9427479020087048, 0.9152075401507318, 0.7933677749242634, 0.9068941930308938, 0.7916356059722602, 0.833957894006744, 0.8401073340792209, 0.9813307779841125, 0.7745384199079126, 0.8379068510839716, 0.7745404910529032, 0.814085068879649, 0.7843693750910461, 0.8335710399551317, 0.7935855619143695, 0.9223303040489554, 0.7875679340213537, 0.8812423248309642, 0.8653322621248662, 0.9282963941805065, 0.9540505370823666, 0.8320506798336282, 0.9428791250102222, 0.8588847740320489, 0.7799064250430092, 0.8292733950074762, 0.7769333269679919, 0.8312766789458692, 0.7831537281163037, 0.9671713779680431, 0.9159167188918218, 0.8274130399804562, 0.9190085759619251, 0.8188256800640374, 0.8644228779012337, 0.8007609751075506, 0.8186990030808374, 0.8741819970309734, 0.819098333013244, 0.7848727869568393, 0.8443255668971688, 0.7812939798459411, 0.8362778880400583, 0.7843781739939004, 0.9142103801714256, 0.9859336349181831, 0.8416556509910151, 0.8523831419879571, 0.7876462759450078, 0.8305169350933284, 0.7908933641156182, 0.8160771699622273, 0.8817707831040025, 0.8287100919988006, 0.9289163198554888, 0.8488016710616648, 0.8519700530450791, 0.8129445009399205, 0.8416713770711794, 0.7908014209242538, 0.9456107000587508, 0.8858743389137089, 0.8173251521075144, 0.7828216449124739, 0.824627575930208, 0.8257606610422954, 0.899307934101671, 0.7952202089363709, 0.8558955000480637, 0.7992931179469451, 0.8535710470750928, 0.8018297899980098, 0.8467550850473344, 0.8050258319126442, 0.8532200071495026, 0.866547574987635, 0.8522827700944617, 0.8132944200187922, 0.817360342014581, 0.7889329560566694, 0.8345368311274797, 0.7807646291330457, 0.9117202569032088, 0.8175646590534598, 0.8425177070312202, 0.8104795551626012, 0.8278723399853334, 0.7868688980815932, 0.8546237130649388, 0.8049888369860128, 0.9066716119414195, 0.9205624930327758, 0.8501241711201146, 0.9528736259089783, 0.8133997009135783, 0.8491441089427099, 0.7902319750282913, 0.8188962639542297, 0.8575915871188045, 0.8624574160203338, 0.8767593749798834, 0.8238252920564264, 0.8250420389231294, 0.8254673659102991, 0.8129816009895876, 0.9205759419128299, 0.8414238609839231, 0.8128830380737782, 0.8318206060212106, 0.7971504461020231, 0.8274424099363387, 0.8098546169931069]
Total Epoch List: [88, 89, 106]
Total Time List: [0.24889481300488114, 0.24803507199976593, 0.20914673199877143]
========================training times:5========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335a20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5116 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5116 time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5000 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.27s
Epoch 5/1000, LR 0.000090
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5000 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5000 time: 0.26s
Epoch 7/1000, LR 0.000150
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5116 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5000 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5116 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5000 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5000 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5116 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5116 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.5000 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.26s
Epoch 17/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.27s
Epoch 19/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.23s
Epoch 20/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5116 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.5000 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5116 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5116 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5116 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5116 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.32s
Val loss: 0.6916 score: 0.5349 time: 0.26s
Test loss: 0.6920 score: 0.5455 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.35s
Val loss: 0.6914 score: 0.6744 time: 0.26s
Test loss: 0.6917 score: 0.5455 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.30s
Val loss: 0.6912 score: 0.8605 time: 0.26s
Test loss: 0.6914 score: 0.8409 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.34s
Val loss: 0.6909 score: 0.8372 time: 0.25s
Test loss: 0.6911 score: 0.7727 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.28s
Val loss: 0.6906 score: 0.6977 time: 0.26s
Test loss: 0.6907 score: 0.6818 time: 0.24s
Epoch 31/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.33s
Val loss: 0.6902 score: 0.6744 time: 0.26s
Test loss: 0.6903 score: 0.6591 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.30s
Val loss: 0.6899 score: 0.6512 time: 0.36s
Test loss: 0.6898 score: 0.6591 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.33s
Val loss: 0.6894 score: 0.5581 time: 0.38s
Test loss: 0.6894 score: 0.6136 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.45s
Val loss: 0.6890 score: 0.5116 time: 0.26s
Test loss: 0.6889 score: 0.6136 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.29s
Val loss: 0.6884 score: 0.5116 time: 0.26s
Test loss: 0.6883 score: 0.6136 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.35s
Val loss: 0.6878 score: 0.5116 time: 0.24s
Test loss: 0.6877 score: 0.6136 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.31s
Val loss: 0.6871 score: 0.5116 time: 0.26s
Test loss: 0.6870 score: 0.5909 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6833;  Loss pred: 0.6833; Loss self: 0.0000; time: 0.33s
Val loss: 0.6863 score: 0.5116 time: 0.24s
Test loss: 0.6862 score: 0.5682 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.30s
Val loss: 0.6855 score: 0.5116 time: 0.26s
Test loss: 0.6854 score: 0.5682 time: 0.36s
Epoch 40/1000, LR 0.000269
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.34s
Val loss: 0.6845 score: 0.5116 time: 0.27s
Test loss: 0.6844 score: 0.5682 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6790;  Loss pred: 0.6790; Loss self: 0.0000; time: 0.36s
Val loss: 0.6833 score: 0.5116 time: 0.26s
Test loss: 0.6834 score: 0.5909 time: 0.26s
Epoch 42/1000, LR 0.000269
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.29s
Val loss: 0.6821 score: 0.5116 time: 0.27s
Test loss: 0.6822 score: 0.6136 time: 0.24s
Epoch 43/1000, LR 0.000269
Train loss: 0.6752;  Loss pred: 0.6752; Loss self: 0.0000; time: 0.39s
Val loss: 0.6806 score: 0.5349 time: 0.26s
Test loss: 0.6808 score: 0.6136 time: 0.28s
Epoch 44/1000, LR 0.000269
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.35s
Val loss: 0.6791 score: 0.5349 time: 0.25s
Test loss: 0.6793 score: 0.6136 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6713;  Loss pred: 0.6713; Loss self: 0.0000; time: 0.39s
Val loss: 0.6773 score: 0.5349 time: 0.25s
Test loss: 0.6776 score: 0.6136 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.46s
Val loss: 0.6754 score: 0.6279 time: 0.25s
Test loss: 0.6756 score: 0.6364 time: 0.25s
Epoch 47/1000, LR 0.000269
Train loss: 0.6661;  Loss pred: 0.6661; Loss self: 0.0000; time: 0.30s
Val loss: 0.6732 score: 0.6512 time: 0.27s
Test loss: 0.6735 score: 0.6591 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6625;  Loss pred: 0.6625; Loss self: 0.0000; time: 0.34s
Val loss: 0.6708 score: 0.6744 time: 0.24s
Test loss: 0.6711 score: 0.6591 time: 0.26s
Epoch 49/1000, LR 0.000269
Train loss: 0.6590;  Loss pred: 0.6590; Loss self: 0.0000; time: 0.28s
Val loss: 0.6681 score: 0.6977 time: 0.27s
Test loss: 0.6685 score: 0.6818 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6563;  Loss pred: 0.6563; Loss self: 0.0000; time: 0.33s
Val loss: 0.6652 score: 0.6977 time: 0.24s
Test loss: 0.6656 score: 0.7045 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6507;  Loss pred: 0.6507; Loss self: 0.0000; time: 0.28s
Val loss: 0.6620 score: 0.7209 time: 0.26s
Test loss: 0.6626 score: 0.7045 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6444;  Loss pred: 0.6444; Loss self: 0.0000; time: 0.35s
Val loss: 0.6585 score: 0.7209 time: 0.24s
Test loss: 0.6592 score: 0.7273 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.6442;  Loss pred: 0.6442; Loss self: 0.0000; time: 0.28s
Val loss: 0.6548 score: 0.7442 time: 0.27s
Test loss: 0.6557 score: 0.7500 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6357;  Loss pred: 0.6357; Loss self: 0.0000; time: 0.34s
Val loss: 0.6508 score: 0.7442 time: 0.24s
Test loss: 0.6518 score: 0.7500 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6281;  Loss pred: 0.6281; Loss self: 0.0000; time: 0.28s
Val loss: 0.6465 score: 0.7442 time: 0.26s
Test loss: 0.6476 score: 0.7500 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.6239;  Loss pred: 0.6239; Loss self: 0.0000; time: 0.45s
Val loss: 0.6418 score: 0.7442 time: 0.26s
Test loss: 0.6432 score: 0.7500 time: 0.37s
Epoch 57/1000, LR 0.000269
Train loss: 0.6160;  Loss pred: 0.6160; Loss self: 0.0000; time: 0.34s
Val loss: 0.6369 score: 0.7674 time: 0.24s
Test loss: 0.6386 score: 0.7500 time: 0.26s
Epoch 58/1000, LR 0.000269
Train loss: 0.6077;  Loss pred: 0.6077; Loss self: 0.0000; time: 0.27s
Val loss: 0.6317 score: 0.7907 time: 0.26s
Test loss: 0.6337 score: 0.7500 time: 0.24s
Epoch 59/1000, LR 0.000268
Train loss: 0.6029;  Loss pred: 0.6029; Loss self: 0.0000; time: 0.33s
Val loss: 0.6261 score: 0.8140 time: 0.23s
Test loss: 0.6286 score: 0.7727 time: 0.26s
Epoch 60/1000, LR 0.000268
Train loss: 0.5941;  Loss pred: 0.5941; Loss self: 0.0000; time: 0.29s
Val loss: 0.6204 score: 0.7907 time: 0.26s
Test loss: 0.6233 score: 0.7727 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5858;  Loss pred: 0.5858; Loss self: 0.0000; time: 0.35s
Val loss: 0.6146 score: 0.7907 time: 0.24s
Test loss: 0.6178 score: 0.7727 time: 0.26s
Epoch 62/1000, LR 0.000268
Train loss: 0.5752;  Loss pred: 0.5752; Loss self: 0.0000; time: 0.30s
Val loss: 0.6085 score: 0.7907 time: 0.26s
Test loss: 0.6122 score: 0.7727 time: 0.23s
Epoch 63/1000, LR 0.000268
Train loss: 0.5715;  Loss pred: 0.5715; Loss self: 0.0000; time: 0.35s
Val loss: 0.6023 score: 0.8140 time: 0.24s
Test loss: 0.6064 score: 0.7727 time: 0.36s
Epoch 64/1000, LR 0.000268
Train loss: 0.5612;  Loss pred: 0.5612; Loss self: 0.0000; time: 0.35s
Val loss: 0.5960 score: 0.8372 time: 0.25s
Test loss: 0.6005 score: 0.8182 time: 0.26s
Epoch 65/1000, LR 0.000268
Train loss: 0.5445;  Loss pred: 0.5445; Loss self: 0.0000; time: 0.40s
Val loss: 0.5897 score: 0.8372 time: 0.26s
Test loss: 0.5946 score: 0.8182 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5356;  Loss pred: 0.5356; Loss self: 0.0000; time: 0.34s
Val loss: 0.5833 score: 0.8372 time: 0.24s
Test loss: 0.5887 score: 0.8409 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.5286;  Loss pred: 0.5286; Loss self: 0.0000; time: 0.31s
Val loss: 0.5769 score: 0.8372 time: 0.26s
Test loss: 0.5826 score: 0.8409 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.5190;  Loss pred: 0.5190; Loss self: 0.0000; time: 0.33s
Val loss: 0.5705 score: 0.8372 time: 0.25s
Test loss: 0.5764 score: 0.8409 time: 0.26s
Epoch 69/1000, LR 0.000268
Train loss: 0.5170;  Loss pred: 0.5170; Loss self: 0.0000; time: 0.29s
Val loss: 0.5641 score: 0.8372 time: 0.27s
Test loss: 0.5702 score: 0.8182 time: 0.30s
Epoch 70/1000, LR 0.000268
Train loss: 0.5043;  Loss pred: 0.5043; Loss self: 0.0000; time: 0.36s
Val loss: 0.5576 score: 0.8372 time: 0.24s
Test loss: 0.5640 score: 0.8182 time: 0.36s
Epoch 71/1000, LR 0.000268
Train loss: 0.4876;  Loss pred: 0.4876; Loss self: 0.0000; time: 0.37s
Val loss: 0.5513 score: 0.8372 time: 0.25s
Test loss: 0.5579 score: 0.8182 time: 0.25s
Epoch 72/1000, LR 0.000267
Train loss: 0.4769;  Loss pred: 0.4769; Loss self: 0.0000; time: 0.31s
Val loss: 0.5449 score: 0.8372 time: 0.28s
Test loss: 0.5519 score: 0.8182 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.4672;  Loss pred: 0.4672; Loss self: 0.0000; time: 0.35s
Val loss: 0.5384 score: 0.8605 time: 0.24s
Test loss: 0.5455 score: 0.8182 time: 0.26s
Epoch 74/1000, LR 0.000267
Train loss: 0.4366;  Loss pred: 0.4366; Loss self: 0.0000; time: 0.28s
Val loss: 0.5321 score: 0.8605 time: 0.26s
Test loss: 0.5394 score: 0.8182 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.4376;  Loss pred: 0.4376; Loss self: 0.0000; time: 0.42s
Val loss: 0.5256 score: 0.8605 time: 0.25s
Test loss: 0.5329 score: 0.8182 time: 0.29s
Epoch 76/1000, LR 0.000267
Train loss: 0.4241;  Loss pred: 0.4241; Loss self: 0.0000; time: 0.37s
Val loss: 0.5192 score: 0.8605 time: 0.24s
Test loss: 0.5267 score: 0.8182 time: 0.38s
Epoch 77/1000, LR 0.000267
Train loss: 0.4126;  Loss pred: 0.4126; Loss self: 0.0000; time: 0.31s
Val loss: 0.5131 score: 0.8605 time: 0.27s
Test loss: 0.5209 score: 0.8182 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.3934;  Loss pred: 0.3934; Loss self: 0.0000; time: 0.42s
Val loss: 0.5076 score: 0.8605 time: 0.24s
Test loss: 0.5158 score: 0.8182 time: 0.29s
Epoch 79/1000, LR 0.000267
Train loss: 0.3815;  Loss pred: 0.3815; Loss self: 0.0000; time: 0.35s
Val loss: 0.5022 score: 0.8605 time: 0.25s
Test loss: 0.5110 score: 0.8182 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.3614;  Loss pred: 0.3614; Loss self: 0.0000; time: 0.32s
Val loss: 0.4972 score: 0.8605 time: 0.25s
Test loss: 0.5068 score: 0.8182 time: 0.24s
Epoch 81/1000, LR 0.000267
Train loss: 0.3454;  Loss pred: 0.3454; Loss self: 0.0000; time: 0.33s
Val loss: 0.4925 score: 0.8605 time: 0.25s
Test loss: 0.5030 score: 0.8182 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.3445;  Loss pred: 0.3445; Loss self: 0.0000; time: 0.32s
Val loss: 0.4879 score: 0.8605 time: 0.25s
Test loss: 0.4994 score: 0.8182 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.3466;  Loss pred: 0.3466; Loss self: 0.0000; time: 0.35s
Val loss: 0.4821 score: 0.8605 time: 0.25s
Test loss: 0.4945 score: 0.8182 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.3159;  Loss pred: 0.3159; Loss self: 0.0000; time: 0.33s
Val loss: 0.4757 score: 0.8605 time: 0.24s
Test loss: 0.4889 score: 0.8182 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.3046;  Loss pred: 0.3046; Loss self: 0.0000; time: 0.31s
Val loss: 0.4692 score: 0.8372 time: 0.26s
Test loss: 0.4835 score: 0.8182 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3011;  Loss pred: 0.3011; Loss self: 0.0000; time: 0.33s
Val loss: 0.4623 score: 0.8372 time: 0.24s
Test loss: 0.4777 score: 0.8182 time: 0.35s
Epoch 87/1000, LR 0.000266
Train loss: 0.2745;  Loss pred: 0.2745; Loss self: 0.0000; time: 0.43s
Val loss: 0.4573 score: 0.8372 time: 0.24s
Test loss: 0.4741 score: 0.8182 time: 0.25s
Epoch 88/1000, LR 0.000266
Train loss: 0.2927;  Loss pred: 0.2927; Loss self: 0.0000; time: 0.28s
Val loss: 0.4521 score: 0.8372 time: 0.25s
Test loss: 0.4705 score: 0.8182 time: 0.25s
Epoch 89/1000, LR 0.000266
Train loss: 0.2706;  Loss pred: 0.2706; Loss self: 0.0000; time: 0.35s
Val loss: 0.4475 score: 0.8372 time: 0.24s
Test loss: 0.4676 score: 0.8182 time: 0.26s
Epoch 90/1000, LR 0.000266
Train loss: 0.2445;  Loss pred: 0.2445; Loss self: 0.0000; time: 0.29s
Val loss: 0.4432 score: 0.8837 time: 0.27s
Test loss: 0.4651 score: 0.8182 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.2271;  Loss pred: 0.2271; Loss self: 0.0000; time: 0.34s
Val loss: 0.4400 score: 0.8837 time: 0.25s
Test loss: 0.4640 score: 0.8182 time: 0.26s
Epoch 92/1000, LR 0.000266
Train loss: 0.2179;  Loss pred: 0.2179; Loss self: 0.0000; time: 0.31s
Val loss: 0.4377 score: 0.8837 time: 0.27s
Test loss: 0.4642 score: 0.8182 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.2319;  Loss pred: 0.2319; Loss self: 0.0000; time: 0.32s
Val loss: 0.4364 score: 0.8837 time: 0.25s
Test loss: 0.4656 score: 0.8409 time: 0.25s
Epoch 94/1000, LR 0.000265
Train loss: 0.2182;  Loss pred: 0.2182; Loss self: 0.0000; time: 0.29s
Val loss: 0.4377 score: 0.8605 time: 0.26s
Test loss: 0.4703 score: 0.8409 time: 0.36s
     INFO: Early stopping counter 1 of 2
Epoch 95/1000, LR 0.000265
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.32s
Val loss: 0.4398 score: 0.8605 time: 0.35s
Test loss: 0.4764 score: 0.8182 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.2319,   Val_Loss: 0.4364,   Val_Precision: 0.9048,   Val_Recall: 0.8636,   Val_accuracy: 0.8837,   Val_Score: 0.8837,   Val_Loss: 0.4364,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8409,   Test_loss: 0.4656


[0.25712007097899914, 0.23332176508847624, 0.24219595710746944, 0.2744539009872824, 0.24909903097432107, 0.26644965005107224, 0.24126906204037368, 0.24368513911031187, 0.24955843295902014, 0.23095162399113178, 0.24934063700493425, 0.2409207959426567, 0.2694052540464327, 0.24828887998592108, 0.24752101895865053, 0.25945665303152055, 0.2521086579654366, 0.2771361330524087, 0.23865933495108038, 0.25794203602708876, 0.2337836279766634, 0.24903763399925083, 0.23902932705823332, 0.24868320603854954, 0.249000997049734, 0.24557937297504395, 0.2584755630232394, 0.24944539205171168, 0.2582385289715603, 0.24880599603056908, 0.25293041102122515, 0.23301660805009305, 0.24842520791571587, 0.2629505640361458, 0.2555404750164598, 0.2629542740760371, 0.23514532600529492, 0.2597851409809664, 0.3595562770497054, 0.24728567292913795, 0.26343834307044744, 0.24926511500962079, 0.287277038092725, 0.22953708504792303, 0.2521180759649724, 0.25241230498068035, 0.24655903794337064, 0.2627724129706621, 0.2487175449496135, 0.25885671889409423, 0.24881036393344402, 0.25781949097290635, 0.23825471091549844, 0.25702911510597914, 0.23656965501140803, 0.3733781510964036, 0.2617239939281717, 0.2431262549944222, 0.26047433400526643, 0.2515483469469473, 0.2639579640235752, 0.23461534397210926, 0.36883005301933736, 0.2611365159973502, 0.23631519102491438, 0.24519132496789098, 0.2550145830027759, 0.2599668470211327, 0.30255002598278224, 0.36834167095366865, 0.25699170492589474, 0.2518272050656378, 0.2608876349404454, 0.24783193797338754, 0.29176521697081625, 0.37968192098196596, 0.25526358105707914, 0.2975202149245888, 0.23438004800118506, 0.24521326692774892, 0.23367902101017535, 0.24884514010045677, 0.23565951408818364, 0.25711100397165865, 0.235582691966556, 0.35451121802907437, 0.2568977529881522, 0.2506360140396282, 0.2608437999151647, 0.2369368189247325, 0.2646928640315309, 0.23572257300838828, 0.2588460349943489, 0.3672608779743314, 0.24770997907035053]
[0.005843637976795435, 0.00530276738837446, 0.005504453570624305, 0.006237588658801873, 0.005661341613052751, 0.006055673864797096, 0.005483387773644857, 0.005538298616143452, 0.005671782567250458, 0.0052489005452529955, 0.005666832659203051, 0.00547547263506038, 0.00612284668287347, 0.005642929090589116, 0.005625477703605694, 0.00589674211435274, 0.005729742226487195, 0.006298548478463834, 0.005424075794342736, 0.005862319000615654, 0.005313264272196896, 0.0056599462272557, 0.005432484705868939, 0.005651891046330671, 0.005659113569312136, 0.005581349385796453, 0.005874444614164531, 0.00566921345572072, 0.005869057476626371, 0.005654681727967479, 0.005748418432300572, 0.0052958320011384785, 0.005646027452629906, 0.005976149182639678, 0.005807738068555905, 0.005976233501728116, 0.005344211954665793, 0.0059042077495674175, 0.008171733569311486, 0.00562012893020768, 0.005987235069782896, 0.005665116250218654, 0.006529023593016477, 0.005216751932907341, 0.005729956271931191, 0.005736643295015462, 0.005603614498712969, 0.005972100294787775, 0.00565267147612758, 0.0058831072475930505, 0.005654780998487364, 0.005859533885747872, 0.005414879793534055, 0.005841570797863163, 0.005376583068441091, 0.00848586707037281, 0.005948272589276629, 0.005525596704418686, 0.005919871227392418, 0.005717007885157893, 0.005999044636899436, 0.0053321669084570285, 0.00838250120498494, 0.005934920818121595, 0.005370799796020781, 0.005572530112906613, 0.005795785977335816, 0.005908337432298471, 0.006876136954154142, 0.008371401612583379, 0.005840720566497607, 0.005723345569673587, 0.005929264430464668, 0.005632544044849717, 0.0066310276584276425, 0.008629134567771953, 0.005801445024024526, 0.006761823066467928, 0.0053268192727542055, 0.005573028793812476, 0.005310886841140349, 0.0056555713659194725, 0.005355898047458719, 0.005843431908446787, 0.005354152090149, 0.008057073137024418, 0.0058385852951852776, 0.005696273046355186, 0.005928268179890107, 0.00538492770283483, 0.006015746909807521, 0.005357331204736097, 0.005882864431689747, 0.008346838135780259, 0.005629772251598875]
[171.12627509967433, 188.58077806549716, 181.6710754609166, 160.31836254365672, 176.636576336324, 165.13438839783132, 182.3690100500208, 180.560867029655, 176.31141323613463, 190.51608834623107, 176.46541906897139, 182.63263587454176, 163.32272418271577, 177.21293036762245, 177.76268126688018, 169.58516764129607, 174.5279212347887, 158.76673862545107, 184.36320544100644, 170.58095949657144, 188.20821791845978, 176.68012377652272, 184.0778307060227, 176.9319315964559, 176.70611973980047, 179.16814212434417, 170.22885833135408, 176.3913120947871, 170.3851093608332, 176.84461267804022, 173.96089233535326, 188.82774222917638, 177.11568149287024, 167.33183349989565, 172.18407376430633, 167.32947260357804, 187.11832698307268, 169.37073396058375, 122.37305481366245, 177.9318610690746, 167.02200403771036, 176.51888431440457, 153.16225860626574, 191.69015756566583, 174.52140165512395, 174.31796759420172, 178.45624466666627, 167.4452789871534, 176.9075036862146, 169.97820333959217, 176.84150814461194, 170.66203890932303, 184.67630642403304, 171.18683220715195, 185.991732531707, 117.84299609068317, 168.1160345278679, 180.97592956799113, 168.9225933450717, 174.91667321224656, 166.69320875679347, 187.54101609496888, 119.29613555024793, 168.4942445982793, 186.19200826307073, 179.4516996299197, 172.5391523963202, 169.2523508446569, 145.43049486468723, 119.45430959815154, 171.21175180610476, 174.72298113514609, 168.6549843960377, 177.53966805006692, 150.80618744351932, 115.88647646483516, 172.37084827295132, 147.88911069841924, 187.72928999390567, 179.43564208931818, 188.29246977238944, 176.8167945021452, 186.7100514496318, 171.13230985963605, 186.77093649242434, 124.11454916608999, 171.27436689580242, 175.553382336519, 168.68332701145397, 185.7035145473842, 166.23039748725, 186.66010403014837, 169.9852192094061, 119.8058454869654, 177.627078913538]
Elapsed: 0.2607206240651737~0.032985722567618884
Time per graph: 0.0059254687287539475~0.000749675512900429
Speed: 170.88121690911424~16.906941427407162
Total Time: 0.2482
best val loss: 0.4364320635795593 test_score: 0.8409

Testing...
Test loss: 0.4651 score: 0.8182 time: 0.24s
test Score 0.8182
Epoch Time List: [1.0460550060961396, 0.907733665080741, 0.8435050990665331, 1.0622012718813494, 0.7984094719868153, 0.8527098279446363, 0.807070963899605, 0.9175945681054145, 0.8772904530633241, 0.8222495758673176, 0.8469435110455379, 0.8304426420945674, 0.9153775400482118, 1.0184815380489454, 0.8232487239874899, 0.860969015979208, 0.8137134370626882, 0.8644737609429285, 0.8336923940805718, 0.8296770859742537, 0.7844027801183984, 0.8304127299925312, 0.8040466289967299, 0.9430571330012754, 0.9593064821092412, 0.8183031369699165, 0.8619577761273831, 0.8045638209441677, 0.8442684438778087, 0.7901164060458541, 0.8317910999758169, 0.8891613491578028, 0.957887097960338, 0.9655396499438211, 0.8054049761267379, 0.8499639170477167, 0.796928358846344, 0.8300930210389197, 0.9124319029506296, 0.8473527588648722, 0.8722431370988488, 0.8025498589267954, 0.9255360200768337, 0.8281911530066282, 0.8852933010784909, 0.9575252078939229, 0.8113203641260043, 0.8359653609804809, 0.7911765169119462, 0.8277496789814904, 0.7877416219562292, 0.8422055820701644, 0.7835703571327031, 0.8327499210136011, 0.7814761679619551, 1.074640849023126, 0.8385379491373897, 0.7673661310691386, 0.8198495759861544, 0.797011295100674, 0.8487920011393726, 0.794103313004598, 0.9493680681334808, 0.8645199830643833, 0.89650624897331, 0.8192098170984536, 0.8154348660027608, 0.8298302059993148, 0.8561289949575439, 0.9728769719367847, 0.8775338439736515, 0.8383038160391152, 0.8465807199245319, 0.788708480889909, 0.9541397029533982, 0.9870076520601287, 0.8263273440534249, 0.9522916621062905, 0.8268553089583293, 0.8192930119112134, 0.8118059100816026, 0.8198566479841247, 0.8239788690116256, 0.8202207629801705, 0.7994556890334934, 0.9253917450550944, 0.921310066129081, 0.7836800038348883, 0.8483302799286321, 0.7946052240440622, 0.8437139899469912, 0.8047115980880335, 0.8217122480273247, 0.9126264470396563, 0.9147438278887421]
Total Epoch List: [95]
Total Time List: [0.24818018602672964]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a52883370d0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4884 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.4884 time: 0.26s
Epoch 3/1000, LR 0.000030
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6982 score: 0.4884 time: 0.22s
Epoch 4/1000, LR 0.000060
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.4884 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.4884 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4884 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.4884 time: 0.26s
Epoch 8/1000, LR 0.000180
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6971 score: 0.4884 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.4884 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.24s
Epoch 11/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4884 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.4884 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6942 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.4884 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.4884 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.4884 time: 0.23s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.4884 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6941 score: 0.4884 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6909;  Loss pred: 0.6909; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.30s
Epoch 23/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.32s
Val loss: 0.6903 score: 0.5227 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.37s
Val loss: 0.6898 score: 0.5455 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.34s
Val loss: 0.6894 score: 0.5909 time: 0.35s
Test loss: 0.6913 score: 0.5349 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.36s
Val loss: 0.6889 score: 0.5909 time: 0.22s
Test loss: 0.6909 score: 0.6047 time: 0.26s
Epoch 31/1000, LR 0.000270
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 0.34s
Val loss: 0.6883 score: 0.6591 time: 0.24s
Test loss: 0.6905 score: 0.5814 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.38s
Val loss: 0.6878 score: 0.6591 time: 0.23s
Test loss: 0.6901 score: 0.5116 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.36s
Val loss: 0.6871 score: 0.7045 time: 0.23s
Test loss: 0.6896 score: 0.5116 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6825;  Loss pred: 0.6825; Loss self: 0.0000; time: 0.37s
Val loss: 0.6864 score: 0.7045 time: 0.22s
Test loss: 0.6892 score: 0.6047 time: 0.24s
Epoch 35/1000, LR 0.000270
Train loss: 0.6815;  Loss pred: 0.6815; Loss self: 0.0000; time: 0.35s
Val loss: 0.6857 score: 0.7500 time: 0.23s
Test loss: 0.6886 score: 0.6744 time: 0.23s
Epoch 36/1000, LR 0.000270
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.36s
Val loss: 0.6849 score: 0.6364 time: 0.35s
Test loss: 0.6881 score: 0.6744 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6785;  Loss pred: 0.6785; Loss self: 0.0000; time: 0.42s
Val loss: 0.6840 score: 0.6591 time: 0.21s
Test loss: 0.6875 score: 0.6744 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.32s
Val loss: 0.6831 score: 0.6364 time: 0.31s
Test loss: 0.6868 score: 0.6512 time: 0.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.50s
Val loss: 0.6821 score: 0.6591 time: 0.23s
Test loss: 0.6861 score: 0.6047 time: 0.30s
Epoch 40/1000, LR 0.000269
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.36s
Val loss: 0.6809 score: 0.6364 time: 0.32s
Test loss: 0.6854 score: 0.6279 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6716;  Loss pred: 0.6716; Loss self: 0.0000; time: 0.38s
Val loss: 0.6797 score: 0.6136 time: 0.24s
Test loss: 0.6846 score: 0.6047 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6690;  Loss pred: 0.6690; Loss self: 0.0000; time: 0.44s
Val loss: 0.6783 score: 0.6136 time: 0.24s
Test loss: 0.6837 score: 0.6047 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6662;  Loss pred: 0.6662; Loss self: 0.0000; time: 0.38s
Val loss: 0.6768 score: 0.6136 time: 0.23s
Test loss: 0.6827 score: 0.5814 time: 0.26s
Epoch 44/1000, LR 0.000269
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.40s
Val loss: 0.6752 score: 0.6136 time: 0.25s
Test loss: 0.6817 score: 0.5814 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6610;  Loss pred: 0.6610; Loss self: 0.0000; time: 0.44s
Val loss: 0.6734 score: 0.6136 time: 0.23s
Test loss: 0.6805 score: 0.5814 time: 0.26s
Epoch 46/1000, LR 0.000269
Train loss: 0.6581;  Loss pred: 0.6581; Loss self: 0.0000; time: 0.46s
Val loss: 0.6714 score: 0.6364 time: 0.24s
Test loss: 0.6792 score: 0.5814 time: 0.26s
Epoch 47/1000, LR 0.000269
Train loss: 0.6534;  Loss pred: 0.6534; Loss self: 0.0000; time: 0.34s
Val loss: 0.6692 score: 0.6364 time: 0.25s
Test loss: 0.6778 score: 0.5814 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6511;  Loss pred: 0.6511; Loss self: 0.0000; time: 0.37s
Val loss: 0.6667 score: 0.6591 time: 0.22s
Test loss: 0.6763 score: 0.5814 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6451;  Loss pred: 0.6451; Loss self: 0.0000; time: 0.32s
Val loss: 0.6640 score: 0.6591 time: 0.25s
Test loss: 0.6746 score: 0.6047 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.37s
Val loss: 0.6611 score: 0.6818 time: 0.22s
Test loss: 0.6727 score: 0.6047 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6377;  Loss pred: 0.6377; Loss self: 0.0000; time: 0.31s
Val loss: 0.6579 score: 0.6818 time: 0.24s
Test loss: 0.6707 score: 0.6279 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6295;  Loss pred: 0.6295; Loss self: 0.0000; time: 0.36s
Val loss: 0.6544 score: 0.7045 time: 0.21s
Test loss: 0.6685 score: 0.6047 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.6225;  Loss pred: 0.6225; Loss self: 0.0000; time: 0.40s
Val loss: 0.6507 score: 0.7045 time: 0.23s
Test loss: 0.6662 score: 0.6279 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6189;  Loss pred: 0.6189; Loss self: 0.0000; time: 0.41s
Val loss: 0.6467 score: 0.7273 time: 0.22s
Test loss: 0.6636 score: 0.6512 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6082;  Loss pred: 0.6082; Loss self: 0.0000; time: 0.36s
Val loss: 0.6423 score: 0.7273 time: 0.23s
Test loss: 0.6609 score: 0.6744 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.6042;  Loss pred: 0.6042; Loss self: 0.0000; time: 0.36s
Val loss: 0.6377 score: 0.8182 time: 0.23s
Test loss: 0.6580 score: 0.6977 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.6014;  Loss pred: 0.6014; Loss self: 0.0000; time: 0.36s
Val loss: 0.6327 score: 0.8409 time: 0.22s
Test loss: 0.6548 score: 0.6977 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.5843;  Loss pred: 0.5843; Loss self: 0.0000; time: 0.36s
Val loss: 0.6273 score: 0.8636 time: 0.24s
Test loss: 0.6515 score: 0.6977 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.36s
Val loss: 0.6216 score: 0.8636 time: 0.23s
Test loss: 0.6479 score: 0.6977 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5703;  Loss pred: 0.5703; Loss self: 0.0000; time: 0.46s
Val loss: 0.6154 score: 0.8409 time: 0.27s
Test loss: 0.6440 score: 0.7209 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.5607;  Loss pred: 0.5607; Loss self: 0.0000; time: 0.41s
Val loss: 0.6088 score: 0.8409 time: 0.22s
Test loss: 0.6399 score: 0.7209 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.32s
Val loss: 0.6018 score: 0.8409 time: 0.23s
Test loss: 0.6356 score: 0.7209 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5327;  Loss pred: 0.5327; Loss self: 0.0000; time: 0.36s
Val loss: 0.5944 score: 0.8409 time: 0.22s
Test loss: 0.6311 score: 0.7209 time: 0.25s
Epoch 64/1000, LR 0.000268
Train loss: 0.5232;  Loss pred: 0.5232; Loss self: 0.0000; time: 0.31s
Val loss: 0.5867 score: 0.8409 time: 0.24s
Test loss: 0.6264 score: 0.6977 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5082;  Loss pred: 0.5082; Loss self: 0.0000; time: 0.35s
Val loss: 0.5787 score: 0.8182 time: 0.22s
Test loss: 0.6215 score: 0.7209 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.4984;  Loss pred: 0.4984; Loss self: 0.0000; time: 0.31s
Val loss: 0.5705 score: 0.8182 time: 0.24s
Test loss: 0.6165 score: 0.7209 time: 0.33s
Epoch 67/1000, LR 0.000268
Train loss: 0.4825;  Loss pred: 0.4825; Loss self: 0.0000; time: 0.36s
Val loss: 0.5621 score: 0.8182 time: 0.35s
Test loss: 0.6113 score: 0.6977 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.4722;  Loss pred: 0.4722; Loss self: 0.0000; time: 0.51s
Val loss: 0.5535 score: 0.8182 time: 0.21s
Test loss: 0.6060 score: 0.6977 time: 0.29s
Epoch 69/1000, LR 0.000268
Train loss: 0.4510;  Loss pred: 0.4510; Loss self: 0.0000; time: 0.31s
Val loss: 0.5447 score: 0.8182 time: 0.23s
Test loss: 0.6006 score: 0.6977 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.4392;  Loss pred: 0.4392; Loss self: 0.0000; time: 0.37s
Val loss: 0.5357 score: 0.8182 time: 0.21s
Test loss: 0.5950 score: 0.6977 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.4250;  Loss pred: 0.4250; Loss self: 0.0000; time: 0.31s
Val loss: 0.5267 score: 0.8182 time: 0.24s
Test loss: 0.5895 score: 0.6977 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.4175;  Loss pred: 0.4175; Loss self: 0.0000; time: 0.35s
Val loss: 0.5178 score: 0.8409 time: 0.21s
Test loss: 0.5840 score: 0.6977 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.4028;  Loss pred: 0.4028; Loss self: 0.0000; time: 0.31s
Val loss: 0.5092 score: 0.8409 time: 0.24s
Test loss: 0.5787 score: 0.7209 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.3896;  Loss pred: 0.3896; Loss self: 0.0000; time: 0.35s
Val loss: 0.5006 score: 0.8409 time: 0.21s
Test loss: 0.5735 score: 0.7209 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.3733;  Loss pred: 0.3733; Loss self: 0.0000; time: 0.43s
Val loss: 0.4923 score: 0.8636 time: 0.24s
Test loss: 0.5684 score: 0.7209 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.3604;  Loss pred: 0.3604; Loss self: 0.0000; time: 0.37s
Val loss: 0.4843 score: 0.8636 time: 0.21s
Test loss: 0.5635 score: 0.7442 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.3495;  Loss pred: 0.3495; Loss self: 0.0000; time: 0.31s
Val loss: 0.4767 score: 0.8636 time: 0.24s
Test loss: 0.5588 score: 0.7674 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.3389;  Loss pred: 0.3389; Loss self: 0.0000; time: 0.36s
Val loss: 0.4693 score: 0.8636 time: 0.21s
Test loss: 0.5544 score: 0.7674 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.3348;  Loss pred: 0.3348; Loss self: 0.0000; time: 0.32s
Val loss: 0.4623 score: 0.8636 time: 0.23s
Test loss: 0.5502 score: 0.7907 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.3240;  Loss pred: 0.3240; Loss self: 0.0000; time: 0.35s
Val loss: 0.4555 score: 0.8636 time: 0.22s
Test loss: 0.5462 score: 0.7907 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.3150;  Loss pred: 0.3150; Loss self: 0.0000; time: 0.31s
Val loss: 0.4490 score: 0.8636 time: 0.24s
Test loss: 0.5426 score: 0.7907 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.3108;  Loss pred: 0.3108; Loss self: 0.0000; time: 0.36s
Val loss: 0.4432 score: 0.8636 time: 0.22s
Test loss: 0.5397 score: 0.7907 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.2962;  Loss pred: 0.2962; Loss self: 0.0000; time: 0.41s
Val loss: 0.4381 score: 0.8409 time: 0.24s
Test loss: 0.5374 score: 0.7907 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.36s
Val loss: 0.4331 score: 0.8409 time: 0.23s
Test loss: 0.5356 score: 0.7907 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.2794;  Loss pred: 0.2794; Loss self: 0.0000; time: 0.31s
Val loss: 0.4284 score: 0.8409 time: 0.24s
Test loss: 0.5343 score: 0.7907 time: 0.24s
Epoch 86/1000, LR 0.000266
Train loss: 0.2656;  Loss pred: 0.2656; Loss self: 0.0000; time: 0.39s
Val loss: 0.4240 score: 0.8409 time: 0.22s
Test loss: 0.5333 score: 0.7907 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.2539;  Loss pred: 0.2539; Loss self: 0.0000; time: 0.33s
Val loss: 0.4193 score: 0.8409 time: 0.24s
Test loss: 0.5324 score: 0.7907 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.2498;  Loss pred: 0.2498; Loss self: 0.0000; time: 0.37s
Val loss: 0.4148 score: 0.8409 time: 0.21s
Test loss: 0.5317 score: 0.7907 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.2345;  Loss pred: 0.2345; Loss self: 0.0000; time: 0.34s
Val loss: 0.4101 score: 0.8409 time: 0.24s
Test loss: 0.5309 score: 0.7907 time: 0.24s
Epoch 90/1000, LR 0.000266
Train loss: 0.2380;  Loss pred: 0.2380; Loss self: 0.0000; time: 0.37s
Val loss: 0.4059 score: 0.8409 time: 0.33s
Test loss: 0.5307 score: 0.7907 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.2210;  Loss pred: 0.2210; Loss self: 0.0000; time: 0.46s
Val loss: 0.4017 score: 0.8409 time: 0.22s
Test loss: 0.5307 score: 0.7907 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.2159;  Loss pred: 0.2159; Loss self: 0.0000; time: 0.39s
Val loss: 0.3979 score: 0.8409 time: 0.24s
Test loss: 0.5313 score: 0.7907 time: 0.24s
Epoch 93/1000, LR 0.000265
Train loss: 0.2164;  Loss pred: 0.2164; Loss self: 0.0000; time: 0.38s
Val loss: 0.3953 score: 0.8409 time: 0.21s
Test loss: 0.5329 score: 0.7907 time: 0.25s
Epoch 94/1000, LR 0.000265
Train loss: 0.1825;  Loss pred: 0.1825; Loss self: 0.0000; time: 0.31s
Val loss: 0.3936 score: 0.8409 time: 0.24s
Test loss: 0.5355 score: 0.7907 time: 0.24s
Epoch 95/1000, LR 0.000265
Train loss: 0.1836;  Loss pred: 0.1836; Loss self: 0.0000; time: 0.41s
Val loss: 0.3924 score: 0.8409 time: 0.26s
Test loss: 0.5388 score: 0.7907 time: 0.24s
Epoch 96/1000, LR 0.000265
Train loss: 0.1867;  Loss pred: 0.1867; Loss self: 0.0000; time: 0.37s
Val loss: 0.3916 score: 0.8409 time: 0.23s
Test loss: 0.5425 score: 0.8140 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.1789;  Loss pred: 0.1789; Loss self: 0.0000; time: 0.36s
Val loss: 0.3916 score: 0.8409 time: 0.47s
Test loss: 0.5470 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 98/1000, LR 0.000265
Train loss: 0.1804;  Loss pred: 0.1804; Loss self: 0.0000; time: 0.39s
Val loss: 0.3922 score: 0.8409 time: 0.23s
Test loss: 0.5521 score: 0.8140 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 095,   Train_Loss: 0.1867,   Val_Loss: 0.3916,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3916,   Test_Precision: 0.8889,   Test_Recall: 0.7273,   Test_accuracy: 0.8000,   Test_Score: 0.8140,   Test_loss: 0.5425


[0.25712007097899914, 0.23332176508847624, 0.24219595710746944, 0.2744539009872824, 0.24909903097432107, 0.26644965005107224, 0.24126906204037368, 0.24368513911031187, 0.24955843295902014, 0.23095162399113178, 0.24934063700493425, 0.2409207959426567, 0.2694052540464327, 0.24828887998592108, 0.24752101895865053, 0.25945665303152055, 0.2521086579654366, 0.2771361330524087, 0.23865933495108038, 0.25794203602708876, 0.2337836279766634, 0.24903763399925083, 0.23902932705823332, 0.24868320603854954, 0.249000997049734, 0.24557937297504395, 0.2584755630232394, 0.24944539205171168, 0.2582385289715603, 0.24880599603056908, 0.25293041102122515, 0.23301660805009305, 0.24842520791571587, 0.2629505640361458, 0.2555404750164598, 0.2629542740760371, 0.23514532600529492, 0.2597851409809664, 0.3595562770497054, 0.24728567292913795, 0.26343834307044744, 0.24926511500962079, 0.287277038092725, 0.22953708504792303, 0.2521180759649724, 0.25241230498068035, 0.24655903794337064, 0.2627724129706621, 0.2487175449496135, 0.25885671889409423, 0.24881036393344402, 0.25781949097290635, 0.23825471091549844, 0.25702911510597914, 0.23656965501140803, 0.3733781510964036, 0.2617239939281717, 0.2431262549944222, 0.26047433400526643, 0.2515483469469473, 0.2639579640235752, 0.23461534397210926, 0.36883005301933736, 0.2611365159973502, 0.23631519102491438, 0.24519132496789098, 0.2550145830027759, 0.2599668470211327, 0.30255002598278224, 0.36834167095366865, 0.25699170492589474, 0.2518272050656378, 0.2608876349404454, 0.24783193797338754, 0.29176521697081625, 0.37968192098196596, 0.25526358105707914, 0.2975202149245888, 0.23438004800118506, 0.24521326692774892, 0.23367902101017535, 0.24884514010045677, 0.23565951408818364, 0.25711100397165865, 0.235582691966556, 0.35451121802907437, 0.2568977529881522, 0.2506360140396282, 0.2608437999151647, 0.2369368189247325, 0.2646928640315309, 0.23572257300838828, 0.2588460349943489, 0.3672608779743314, 0.24770997907035053, 0.24040370795410126, 0.26284044003114104, 0.2283419700106606, 0.2544930559815839, 0.22996392904315144, 0.24430559494066983, 0.26008692500181496, 0.23992635204922408, 0.2513617370277643, 0.2449920770013705, 0.25600513094104826, 0.24751502997241914, 0.256301415967755, 0.2322615149896592, 0.23720126692205667, 0.2403569760499522, 0.24438791594002396, 0.22995038493536413, 0.2531386259943247, 0.22836878895759583, 0.2536262039793655, 0.30311288498342037, 0.24448485497850925, 0.24297881498932838, 0.25745558296330273, 0.2551837529754266, 0.2458718599518761, 0.25682031107135117, 0.23308542906306684, 0.2684047990478575, 0.23188625590410084, 0.2533312590094283, 0.23519769392441958, 0.2432399159297347, 0.23179883195552975, 0.24013908300548792, 0.26191234099678695, 0.22755432699341327, 0.30760300101246685, 0.2581905419938266, 0.24406603793613613, 0.23533476702868938, 0.26534024509601295, 0.22903464699629694, 0.26227089494932443, 0.2615764520596713, 0.24521923996508121, 0.2570672680158168, 0.2477094039786607, 0.25843274302314967, 0.24268926901277155, 0.25045226502697915, 0.231003628927283, 0.25598241307307035, 0.23243095504585654, 0.25528148899320513, 0.23846509691793472, 0.2560870529850945, 0.2380691630532965, 0.2442921930924058, 0.2488434580154717, 0.2505306400125846, 0.2513703510630876, 0.24648626591078937, 0.2548981789732352, 0.3346047989325598, 0.23073329299222678, 0.2984497779980302, 0.24406567204277962, 0.24745931406505406, 0.24012911098543555, 0.25414401199668646, 0.24350160895846784, 0.24393576895818114, 0.22841638093814254, 0.25649231497664005, 0.23110083304345608, 0.2480296939611435, 0.2439572470029816, 0.2542540659196675, 0.24519336898811162, 0.2527358280494809, 0.22981623897794634, 0.25567709608003497, 0.2410140410065651, 0.2496088040061295, 0.2411970989778638, 0.2454214550089091, 0.24798468500375748, 0.2426125550409779, 0.24548250902444124, 0.24231556500308216, 0.2555965030333027, 0.24053611897397786, 0.24040114192757756, 0.23321751703042537, 0.23117327596992254, 0.24516669393051416]
[0.005843637976795435, 0.00530276738837446, 0.005504453570624305, 0.006237588658801873, 0.005661341613052751, 0.006055673864797096, 0.005483387773644857, 0.005538298616143452, 0.005671782567250458, 0.0052489005452529955, 0.005666832659203051, 0.00547547263506038, 0.00612284668287347, 0.005642929090589116, 0.005625477703605694, 0.00589674211435274, 0.005729742226487195, 0.006298548478463834, 0.005424075794342736, 0.005862319000615654, 0.005313264272196896, 0.0056599462272557, 0.005432484705868939, 0.005651891046330671, 0.005659113569312136, 0.005581349385796453, 0.005874444614164531, 0.00566921345572072, 0.005869057476626371, 0.005654681727967479, 0.005748418432300572, 0.0052958320011384785, 0.005646027452629906, 0.005976149182639678, 0.005807738068555905, 0.005976233501728116, 0.005344211954665793, 0.0059042077495674175, 0.008171733569311486, 0.00562012893020768, 0.005987235069782896, 0.005665116250218654, 0.006529023593016477, 0.005216751932907341, 0.005729956271931191, 0.005736643295015462, 0.005603614498712969, 0.005972100294787775, 0.00565267147612758, 0.0058831072475930505, 0.005654780998487364, 0.005859533885747872, 0.005414879793534055, 0.005841570797863163, 0.005376583068441091, 0.00848586707037281, 0.005948272589276629, 0.005525596704418686, 0.005919871227392418, 0.005717007885157893, 0.005999044636899436, 0.0053321669084570285, 0.00838250120498494, 0.005934920818121595, 0.005370799796020781, 0.005572530112906613, 0.005795785977335816, 0.005908337432298471, 0.006876136954154142, 0.008371401612583379, 0.005840720566497607, 0.005723345569673587, 0.005929264430464668, 0.005632544044849717, 0.0066310276584276425, 0.008629134567771953, 0.005801445024024526, 0.006761823066467928, 0.0053268192727542055, 0.005573028793812476, 0.005310886841140349, 0.0056555713659194725, 0.005355898047458719, 0.005843431908446787, 0.005354152090149, 0.008057073137024418, 0.0058385852951852776, 0.005696273046355186, 0.005928268179890107, 0.00538492770283483, 0.006015746909807521, 0.005357331204736097, 0.005882864431689747, 0.008346838135780259, 0.005629772251598875, 0.005590783905909332, 0.006112568372817233, 0.0053102783723409435, 0.005918443162362416, 0.0053479983498407315, 0.005681525463736508, 0.006048533139577092, 0.005579682605795909, 0.005845621791343356, 0.005697490162822569, 0.005953607696303448, 0.005756163487730678, 0.005960498045761745, 0.005401430581154865, 0.005516308533071086, 0.005589697117440749, 0.0056834399055819525, 0.005347683370589863, 0.00588694479056569, 0.0053109020687812985, 0.005898283813473617, 0.007049136860079543, 0.005685694301825797, 0.005650670116030892, 0.005987339138681459, 0.005934505883149456, 0.005717950231438979, 0.005972565373752353, 0.005420591373559694, 0.006241972070880408, 0.005392703625676763, 0.00589142462812624, 0.005469713812195804, 0.005656742230924063, 0.005390670510593715, 0.005584629837336929, 0.006090984674343883, 0.005291961092870076, 0.0071535581630806245, 0.006004431209158759, 0.0056759543706078166, 0.00547290155880673, 0.006170703374325882, 0.005326387139448766, 0.006099323138356382, 0.006083173303713285, 0.0057027730224437495, 0.005978308558507368, 0.005760683813457226, 0.006010063791236039, 0.005643936488669106, 0.005824471279697189, 0.005372177416913558, 0.005953079373792334, 0.005405371047578059, 0.005936778813795468, 0.005545699928324063, 0.005955512860118476, 0.005536492164030151, 0.005681213792846646, 0.005787057163150504, 0.005826293953781037, 0.005845822117746223, 0.005732238742111381, 0.005927864627284539, 0.007781506951919995, 0.0053658905347029485, 0.006940692511582097, 0.005675945861459991, 0.005754867768954746, 0.00558439792989385, 0.005910325860388057, 0.005662828115313206, 0.005672924859492585, 0.00531200885902657, 0.00596493755759628, 0.005374437977754792, 0.005768132417701011, 0.005673424348906549, 0.005912885253945756, 0.00570217137181655, 0.005877577396499556, 0.005344563697161543, 0.005945978978605464, 0.005604977697827095, 0.0058048559071192905, 0.00560923485995032, 0.005707475697881607, 0.005767085697761801, 0.00564215244281344, 0.005708895558707936, 0.005635245697746097, 0.005944104721704715, 0.0055938632319529735, 0.005590724230873896, 0.005423663186754079, 0.005376122696974943, 0.005701551021639864]
[171.12627509967433, 188.58077806549716, 181.6710754609166, 160.31836254365672, 176.636576336324, 165.13438839783132, 182.3690100500208, 180.560867029655, 176.31141323613463, 190.51608834623107, 176.46541906897139, 182.63263587454176, 163.32272418271577, 177.21293036762245, 177.76268126688018, 169.58516764129607, 174.5279212347887, 158.76673862545107, 184.36320544100644, 170.58095949657144, 188.20821791845978, 176.68012377652272, 184.0778307060227, 176.9319315964559, 176.70611973980047, 179.16814212434417, 170.22885833135408, 176.3913120947871, 170.3851093608332, 176.84461267804022, 173.96089233535326, 188.82774222917638, 177.11568149287024, 167.33183349989565, 172.18407376430633, 167.32947260357804, 187.11832698307268, 169.37073396058375, 122.37305481366245, 177.9318610690746, 167.02200403771036, 176.51888431440457, 153.16225860626574, 191.69015756566583, 174.52140165512395, 174.31796759420172, 178.45624466666627, 167.4452789871534, 176.9075036862146, 169.97820333959217, 176.84150814461194, 170.66203890932303, 184.67630642403304, 171.18683220715195, 185.991732531707, 117.84299609068317, 168.1160345278679, 180.97592956799113, 168.9225933450717, 174.91667321224656, 166.69320875679347, 187.54101609496888, 119.29613555024793, 168.4942445982793, 186.19200826307073, 179.4516996299197, 172.5391523963202, 169.2523508446569, 145.43049486468723, 119.45430959815154, 171.21175180610476, 174.72298113514609, 168.6549843960377, 177.53966805006692, 150.80618744351932, 115.88647646483516, 172.37084827295132, 147.88911069841924, 187.72928999390567, 179.43564208931818, 188.29246977238944, 176.8167945021452, 186.7100514496318, 171.13230985963605, 186.77093649242434, 124.11454916608999, 171.27436689580242, 175.553382336519, 168.68332701145397, 185.7035145473842, 166.23039748725, 186.66010403014837, 169.9852192094061, 119.8058454869654, 177.627078913538, 178.86579356841582, 163.59735204714087, 188.31404493003393, 168.96335278834346, 186.98584677569713, 176.0090677024513, 165.32934133347894, 179.22166378446823, 171.06820038902902, 175.51588004929422, 167.96538351374625, 173.7268238005245, 167.7712151438515, 185.13613846837458, 181.28065063889233, 178.90056992172975, 175.94977981870744, 186.99686026656013, 169.86739906285203, 188.29192989233027, 169.5408413063596, 141.86133988448563, 175.8800151599566, 176.97016096604372, 167.0191009457703, 168.5060255546158, 174.88784608542142, 167.4322401550768, 184.48171630825246, 160.20577930252637, 185.4357423312882, 169.73823194238992, 182.8249218030938, 176.78019594621762, 185.5056802367731, 179.06289747519907, 164.17706716816184, 188.96586396814448, 139.7905737540488, 166.54366836190357, 176.18182506511477, 182.7184335868143, 162.05608005088152, 187.74452059515357, 163.95261856375026, 164.38788607084084, 175.3532879643666, 167.2713929388875, 173.59050286078076, 166.38758501335948, 177.18129925941275, 171.68940354908736, 186.14426188748686, 167.9802900667462, 185.0011759041152, 168.44151203279975, 180.3198898109521, 167.91165152149574, 180.6197806070902, 176.01872354445175, 172.79939903956205, 171.63569293496408, 171.06233817212632, 174.45191049939862, 168.69481050515896, 128.50981258241526, 186.3623556113709, 144.07784213625317, 176.18208918976117, 173.7659387057697, 179.07033355321946, 169.1954087848453, 176.59020892685015, 176.2759114157287, 188.25269809193253, 167.64634840586243, 186.06596710931936, 173.3663389784951, 176.26039204924484, 169.12217251851544, 175.37178993647618, 170.1381253772275, 187.1060121392308, 168.18088385413938, 178.4128419257893, 172.26956465423422, 178.27743443940173, 175.20880559704548, 173.39780478519657, 177.23732389998196, 175.16522937167284, 177.45455187516765, 168.2339135696131, 178.7673310079968, 178.86770277053856, 184.37723095384064, 186.00765949086016, 175.39087104624082]
Elapsed: 0.2543518050704055~0.0266990760493728
Time per graph: 0.005847328455158047~0.0005976324007062701
Speed: 172.43154391562973~13.995683200527605
Total Time: 0.2458
best val loss: 0.39160841703414917 test_score: 0.8140

Testing...
Test loss: 0.6515 score: 0.6977 time: 0.23s
test Score 0.6977
Epoch Time List: [1.0460550060961396, 0.907733665080741, 0.8435050990665331, 1.0622012718813494, 0.7984094719868153, 0.8527098279446363, 0.807070963899605, 0.9175945681054145, 0.8772904530633241, 0.8222495758673176, 0.8469435110455379, 0.8304426420945674, 0.9153775400482118, 1.0184815380489454, 0.8232487239874899, 0.860969015979208, 0.8137134370626882, 0.8644737609429285, 0.8336923940805718, 0.8296770859742537, 0.7844027801183984, 0.8304127299925312, 0.8040466289967299, 0.9430571330012754, 0.9593064821092412, 0.8183031369699165, 0.8619577761273831, 0.8045638209441677, 0.8442684438778087, 0.7901164060458541, 0.8317910999758169, 0.8891613491578028, 0.957887097960338, 0.9655396499438211, 0.8054049761267379, 0.8499639170477167, 0.796928358846344, 0.8300930210389197, 0.9124319029506296, 0.8473527588648722, 0.8722431370988488, 0.8025498589267954, 0.9255360200768337, 0.8281911530066282, 0.8852933010784909, 0.9575252078939229, 0.8113203641260043, 0.8359653609804809, 0.7911765169119462, 0.8277496789814904, 0.7877416219562292, 0.8422055820701644, 0.7835703571327031, 0.8327499210136011, 0.7814761679619551, 1.074640849023126, 0.8385379491373897, 0.7673661310691386, 0.8198495759861544, 0.797011295100674, 0.8487920011393726, 0.794103313004598, 0.9493680681334808, 0.8645199830643833, 0.89650624897331, 0.8192098170984536, 0.8154348660027608, 0.8298302059993148, 0.8561289949575439, 0.9728769719367847, 0.8775338439736515, 0.8383038160391152, 0.8465807199245319, 0.788708480889909, 0.9541397029533982, 0.9870076520601287, 0.8263273440534249, 0.9522916621062905, 0.8268553089583293, 0.8192930119112134, 0.8118059100816026, 0.8198566479841247, 0.8239788690116256, 0.8202207629801705, 0.7994556890334934, 0.9253917450550944, 0.921310066129081, 0.7836800038348883, 0.8483302799286321, 0.7946052240440622, 0.8437139899469912, 0.8047115980880335, 0.8217122480273247, 0.9126264470396563, 0.9147438278887421, 0.8014905280433595, 0.8628226480213925, 0.8338124150177464, 0.824787967139855, 0.8752797041088343, 0.9009455809136853, 0.9379644739674404, 0.8210217260057107, 0.8526763501577079, 0.8142131878994405, 0.8403114349348471, 0.801036490011029, 0.8181230209302157, 0.8931868160143495, 0.865299831959419, 0.8600391780491918, 0.8107994191814214, 0.8016051519662142, 0.8249033039901406, 0.7965222240891308, 0.8192773000337183, 0.9461604461539537, 0.8067775659728795, 0.902423272957094, 0.8554480999009684, 0.8205885249190032, 0.8064058730378747, 0.8477034820243716, 0.9185746690491214, 0.8444261810509488, 0.8089710128260776, 0.8611683959607035, 0.8209577009547502, 0.8345295939361677, 0.8032279500039294, 0.9480876290472224, 0.8871096960501745, 0.8458145341137424, 1.0412830179557204, 0.9382149969460443, 0.8571088298922405, 0.9087362070567906, 0.8608402239624411, 0.8715951311169192, 0.9348418469307944, 0.950979272951372, 0.8288628740701824, 0.8386272679781541, 0.81345271889586, 0.8367655189940706, 0.7961667620111257, 0.8226410250645131, 0.8644425859674811, 0.8853477251250297, 0.8191269011003897, 0.8350591839989647, 0.8168202750384808, 0.8598421089118347, 0.8238573630806059, 0.9729034071788192, 0.8754501839866862, 0.7999150169780478, 0.8260737509699538, 0.7920291960472241, 0.8162809528876096, 0.8793900599703193, 0.9351614291081205, 1.0100711459526792, 0.7820022699888796, 0.8289757568854839, 0.7841605830471963, 0.8122000599978492, 0.7868223951663822, 0.8025112790055573, 0.8865203651366755, 0.8337087768595666, 0.7765104139689356, 0.8180862830486149, 0.7925088660558686, 0.8200899180956185, 0.7915139840915799, 0.8226164060179144, 0.8672397730406374, 0.8420679941773415, 0.7830773231107742, 0.8540875810431316, 0.8104633879847825, 0.825219947961159, 0.8191610730718821, 0.9321693349629641, 0.9242206100607291, 0.862429357948713, 0.8464882089756429, 0.7838439479237422, 0.9114502978045493, 0.823948354111053, 1.0590263899648562, 0.8528143371222541]
Total Epoch List: [95, 98]
Total Time List: [0.24818018602672964, 0.2457790799671784]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288337e80>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7060;  Loss pred: 0.7060; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7028 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6996 score: 0.5116 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.7060;  Loss pred: 0.7060; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7027 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6995 score: 0.5116 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.7059;  Loss pred: 0.7059; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7026 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.5116 time: 0.22s
Epoch 4/1000, LR 0.000060
Train loss: 0.7058;  Loss pred: 0.7058; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7024 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6992 score: 0.5116 time: 0.22s
Epoch 5/1000, LR 0.000090
Train loss: 0.7056;  Loss pred: 0.7056; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7021 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.5116 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.7053;  Loss pred: 0.7053; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7018 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5116 time: 0.21s
Epoch 7/1000, LR 0.000150
Train loss: 0.7049;  Loss pred: 0.7049; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7015 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6985 score: 0.5116 time: 0.21s
Epoch 8/1000, LR 0.000180
Train loss: 0.7045;  Loss pred: 0.7045; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7011 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.5116 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.7041;  Loss pred: 0.7041; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7006 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.5116 time: 0.22s
Epoch 10/1000, LR 0.000240
Train loss: 0.7035;  Loss pred: 0.7035; Loss self: 0.0000; time: 0.59s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7001 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6973 score: 0.5116 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.7030;  Loss pred: 0.7030; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5116 time: 0.31s
Epoch 12/1000, LR 0.000270
Train loss: 0.7024;  Loss pred: 0.7024; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6990 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6965 score: 0.5116 time: 0.21s
Epoch 13/1000, LR 0.000270
Train loss: 0.7018;  Loss pred: 0.7018; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6985 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.5116 time: 0.28s
Epoch 14/1000, LR 0.000270
Train loss: 0.7012;  Loss pred: 0.7012; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5116 time: 0.27s
Epoch 15/1000, LR 0.000270
Train loss: 0.7006;  Loss pred: 0.7006; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.20s
Epoch 16/1000, LR 0.000270
Train loss: 0.7000;  Loss pred: 0.7000; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6969 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5116 time: 0.19s
Epoch 17/1000, LR 0.000270
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5116 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 0.18s
Epoch 19/1000, LR 0.000270
Train loss: 0.6981;  Loss pred: 0.6981; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.18s
Epoch 20/1000, LR 0.000270
Train loss: 0.6974;  Loss pred: 0.6974; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.19s
Epoch 21/1000, LR 0.000270
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.22s
Epoch 22/1000, LR 0.000270
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6910 score: 0.5116 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5116 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6900 score: 0.5116 time: 0.21s
Epoch 27/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6905 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6896 score: 0.5116 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6892 score: 0.5116 time: 0.26s
Epoch 29/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6890 score: 0.5116 time: 0.20s
Epoch 30/1000, LR 0.000270
Train loss: 0.6906;  Loss pred: 0.6906; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5116 time: 0.23s
Epoch 31/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.5116 time: 0.21s
Epoch 32/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.41s
Val loss: 0.6883 score: 0.5227 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6882 score: 0.5116 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.41s
Val loss: 0.6878 score: 0.5909 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5116 time: 0.30s
Epoch 34/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.43s
Val loss: 0.6872 score: 0.6136 time: 0.24s
Test loss: 0.6874 score: 0.5814 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6869;  Loss pred: 0.6869; Loss self: 0.0000; time: 0.52s
Val loss: 0.6865 score: 0.6591 time: 0.25s
Test loss: 0.6869 score: 0.5814 time: 0.21s
Epoch 36/1000, LR 0.000270
Train loss: 0.6864;  Loss pred: 0.6864; Loss self: 0.0000; time: 0.44s
Val loss: 0.6858 score: 0.7273 time: 0.28s
Test loss: 0.6864 score: 0.7442 time: 0.32s
Epoch 37/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.50s
Val loss: 0.6850 score: 0.6364 time: 0.38s
Test loss: 0.6858 score: 0.6977 time: 0.26s
Epoch 38/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.40s
Val loss: 0.6841 score: 0.6591 time: 0.22s
Test loss: 0.6851 score: 0.6977 time: 0.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.39s
Val loss: 0.6832 score: 0.7500 time: 0.23s
Test loss: 0.6844 score: 0.6744 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.41s
Val loss: 0.6822 score: 0.7045 time: 0.24s
Test loss: 0.6837 score: 0.6279 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.38s
Val loss: 0.6812 score: 0.6818 time: 0.23s
Test loss: 0.6830 score: 0.6279 time: 0.21s
Epoch 42/1000, LR 0.000269
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.40s
Val loss: 0.6800 score: 0.5682 time: 0.22s
Test loss: 0.6822 score: 0.6279 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6782;  Loss pred: 0.6782; Loss self: 0.0000; time: 0.41s
Val loss: 0.6788 score: 0.5455 time: 0.23s
Test loss: 0.6813 score: 0.5814 time: 0.21s
Epoch 44/1000, LR 0.000269
Train loss: 0.6771;  Loss pred: 0.6771; Loss self: 0.0000; time: 0.54s
Val loss: 0.6775 score: 0.5682 time: 0.23s
Test loss: 0.6803 score: 0.5581 time: 0.36s
Epoch 45/1000, LR 0.000269
Train loss: 0.6749;  Loss pred: 0.6749; Loss self: 0.0000; time: 0.48s
Val loss: 0.6760 score: 0.5227 time: 0.23s
Test loss: 0.6792 score: 0.5581 time: 0.23s
Epoch 46/1000, LR 0.000269
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.38s
Val loss: 0.6744 score: 0.5227 time: 0.23s
Test loss: 0.6779 score: 0.5349 time: 0.22s
Epoch 47/1000, LR 0.000269
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.39s
Val loss: 0.6727 score: 0.5227 time: 0.23s
Test loss: 0.6766 score: 0.5349 time: 0.22s
Epoch 48/1000, LR 0.000269
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.38s
Val loss: 0.6710 score: 0.5227 time: 0.22s
Test loss: 0.6751 score: 0.5349 time: 0.23s
Epoch 49/1000, LR 0.000269
Train loss: 0.6684;  Loss pred: 0.6684; Loss self: 0.0000; time: 0.38s
Val loss: 0.6691 score: 0.5000 time: 0.24s
Test loss: 0.6736 score: 0.5349 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6643;  Loss pred: 0.6643; Loss self: 0.0000; time: 0.41s
Val loss: 0.6671 score: 0.5000 time: 0.23s
Test loss: 0.6718 score: 0.5349 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6612;  Loss pred: 0.6612; Loss self: 0.0000; time: 0.38s
Val loss: 0.6648 score: 0.5000 time: 0.24s
Test loss: 0.6699 score: 0.5349 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6583;  Loss pred: 0.6583; Loss self: 0.0000; time: 0.52s
Val loss: 0.6624 score: 0.5000 time: 0.22s
Test loss: 0.6679 score: 0.5349 time: 0.22s
Epoch 53/1000, LR 0.000269
Train loss: 0.6547;  Loss pred: 0.6547; Loss self: 0.0000; time: 0.39s
Val loss: 0.6597 score: 0.5000 time: 0.22s
Test loss: 0.6655 score: 0.5116 time: 0.22s
Epoch 54/1000, LR 0.000269
Train loss: 0.6528;  Loss pred: 0.6528; Loss self: 0.0000; time: 0.39s
Val loss: 0.6569 score: 0.5000 time: 0.23s
Test loss: 0.6630 score: 0.5116 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.6484;  Loss pred: 0.6484; Loss self: 0.0000; time: 0.47s
Val loss: 0.6539 score: 0.5000 time: 0.20s
Test loss: 0.6603 score: 0.5116 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6439;  Loss pred: 0.6439; Loss self: 0.0000; time: 0.39s
Val loss: 0.6507 score: 0.5227 time: 0.23s
Test loss: 0.6575 score: 0.5349 time: 0.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.51s
Val loss: 0.6474 score: 0.5227 time: 0.22s
Test loss: 0.6545 score: 0.5349 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.6320;  Loss pred: 0.6320; Loss self: 0.0000; time: 0.35s
Val loss: 0.6439 score: 0.5227 time: 0.35s
Test loss: 0.6514 score: 0.5349 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.6271;  Loss pred: 0.6271; Loss self: 0.0000; time: 0.39s
Val loss: 0.6402 score: 0.5227 time: 0.29s
Test loss: 0.6480 score: 0.5349 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.6234;  Loss pred: 0.6234; Loss self: 0.0000; time: 0.47s
Val loss: 0.6363 score: 0.5227 time: 0.22s
Test loss: 0.6444 score: 0.5349 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.6186;  Loss pred: 0.6186; Loss self: 0.0000; time: 0.34s
Val loss: 0.6321 score: 0.5455 time: 0.24s
Test loss: 0.6404 score: 0.5349 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.6088;  Loss pred: 0.6088; Loss self: 0.0000; time: 0.39s
Val loss: 0.6277 score: 0.5455 time: 0.22s
Test loss: 0.6361 score: 0.5349 time: 0.23s
Epoch 63/1000, LR 0.000268
Train loss: 0.6049;  Loss pred: 0.6049; Loss self: 0.0000; time: 0.36s
Val loss: 0.6229 score: 0.5455 time: 0.24s
Test loss: 0.6314 score: 0.5349 time: 0.21s
Epoch 64/1000, LR 0.000268
Train loss: 0.5942;  Loss pred: 0.5942; Loss self: 0.0000; time: 0.39s
Val loss: 0.6177 score: 0.5682 time: 0.22s
Test loss: 0.6263 score: 0.5581 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.5917;  Loss pred: 0.5917; Loss self: 0.0000; time: 0.35s
Val loss: 0.6122 score: 0.5682 time: 0.23s
Test loss: 0.6209 score: 0.5581 time: 0.21s
Epoch 66/1000, LR 0.000268
Train loss: 0.5826;  Loss pred: 0.5826; Loss self: 0.0000; time: 0.39s
Val loss: 0.6063 score: 0.5682 time: 0.23s
Test loss: 0.6150 score: 0.5581 time: 0.23s
Epoch 67/1000, LR 0.000268
Train loss: 0.5673;  Loss pred: 0.5673; Loss self: 0.0000; time: 0.46s
Val loss: 0.6000 score: 0.6364 time: 0.22s
Test loss: 0.6087 score: 0.5581 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.5629;  Loss pred: 0.5629; Loss self: 0.0000; time: 0.39s
Val loss: 0.5934 score: 0.6591 time: 0.23s
Test loss: 0.6020 score: 0.5814 time: 0.21s
Epoch 69/1000, LR 0.000268
Train loss: 0.5549;  Loss pred: 0.5549; Loss self: 0.0000; time: 0.47s
Val loss: 0.5867 score: 0.6591 time: 0.24s
Test loss: 0.5949 score: 0.6047 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.5437;  Loss pred: 0.5437; Loss self: 0.0000; time: 0.34s
Val loss: 0.5799 score: 0.6818 time: 0.24s
Test loss: 0.5875 score: 0.6279 time: 0.21s
Epoch 71/1000, LR 0.000268
Train loss: 0.5267;  Loss pred: 0.5267; Loss self: 0.0000; time: 0.39s
Val loss: 0.5729 score: 0.6818 time: 0.22s
Test loss: 0.5800 score: 0.6279 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.5197;  Loss pred: 0.5197; Loss self: 0.0000; time: 0.38s
Val loss: 0.5658 score: 0.6818 time: 0.24s
Test loss: 0.5724 score: 0.6977 time: 0.20s
Epoch 73/1000, LR 0.000267
Train loss: 0.5166;  Loss pred: 0.5166; Loss self: 0.0000; time: 0.39s
Val loss: 0.5587 score: 0.6818 time: 0.22s
Test loss: 0.5647 score: 0.7209 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.5050;  Loss pred: 0.5050; Loss self: 0.0000; time: 0.37s
Val loss: 0.5514 score: 0.6818 time: 0.22s
Test loss: 0.5572 score: 0.7209 time: 0.29s
Epoch 75/1000, LR 0.000267
Train loss: 0.4936;  Loss pred: 0.4936; Loss self: 0.0000; time: 0.38s
Val loss: 0.5441 score: 0.7045 time: 0.22s
Test loss: 0.5498 score: 0.7209 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.4859;  Loss pred: 0.4859; Loss self: 0.0000; time: 0.43s
Val loss: 0.5367 score: 0.7045 time: 0.22s
Test loss: 0.5422 score: 0.7209 time: 0.20s
Epoch 77/1000, LR 0.000267
Train loss: 0.4686;  Loss pred: 0.4686; Loss self: 0.0000; time: 0.40s
Val loss: 0.5292 score: 0.7500 time: 0.23s
Test loss: 0.5350 score: 0.7442 time: 0.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.4571;  Loss pred: 0.4571; Loss self: 0.0000; time: 0.39s
Val loss: 0.5217 score: 0.7500 time: 0.22s
Test loss: 0.5280 score: 0.7442 time: 0.20s
Epoch 79/1000, LR 0.000267
Train loss: 0.4542;  Loss pred: 0.4542; Loss self: 0.0000; time: 0.39s
Val loss: 0.5143 score: 0.7500 time: 0.23s
Test loss: 0.5211 score: 0.7907 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.4252;  Loss pred: 0.4252; Loss self: 0.0000; time: 0.36s
Val loss: 0.5070 score: 0.7955 time: 0.28s
Test loss: 0.5148 score: 0.7907 time: 0.20s
Epoch 81/1000, LR 0.000267
Train loss: 0.4411;  Loss pred: 0.4411; Loss self: 0.0000; time: 0.38s
Val loss: 0.4999 score: 0.7955 time: 0.21s
Test loss: 0.5093 score: 0.7907 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.4351;  Loss pred: 0.4351; Loss self: 0.0000; time: 0.36s
Val loss: 0.4926 score: 0.7955 time: 0.31s
Test loss: 0.5032 score: 0.7907 time: 0.30s
Epoch 83/1000, LR 0.000266
Train loss: 0.4036;  Loss pred: 0.4036; Loss self: 0.0000; time: 0.46s
Val loss: 0.4854 score: 0.7955 time: 0.24s
Test loss: 0.4977 score: 0.7674 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.4078;  Loss pred: 0.4078; Loss self: 0.0000; time: 0.41s
Val loss: 0.4779 score: 0.7955 time: 0.22s
Test loss: 0.4919 score: 0.7674 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.3816;  Loss pred: 0.3816; Loss self: 0.0000; time: 0.34s
Val loss: 0.4704 score: 0.8182 time: 0.24s
Test loss: 0.4864 score: 0.7674 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.3658;  Loss pred: 0.3658; Loss self: 0.0000; time: 0.43s
Val loss: 0.4629 score: 0.8182 time: 0.24s
Test loss: 0.4809 score: 0.7907 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.3662;  Loss pred: 0.3662; Loss self: 0.0000; time: 0.35s
Val loss: 0.4551 score: 0.8182 time: 0.24s
Test loss: 0.4755 score: 0.7907 time: 0.20s
Epoch 88/1000, LR 0.000266
Train loss: 0.3626;  Loss pred: 0.3626; Loss self: 0.0000; time: 0.38s
Val loss: 0.4474 score: 0.8409 time: 0.21s
Test loss: 0.4707 score: 0.7907 time: 0.22s
Epoch 89/1000, LR 0.000266
Train loss: 0.3500;  Loss pred: 0.3500; Loss self: 0.0000; time: 0.33s
Val loss: 0.4400 score: 0.8409 time: 0.24s
Test loss: 0.4668 score: 0.7907 time: 0.21s
Epoch 90/1000, LR 0.000266
Train loss: 0.3331;  Loss pred: 0.3331; Loss self: 0.0000; time: 0.47s
Val loss: 0.4326 score: 0.8409 time: 0.27s
Test loss: 0.4631 score: 0.7907 time: 0.21s
Epoch 91/1000, LR 0.000266
Train loss: 0.3339;  Loss pred: 0.3339; Loss self: 0.0000; time: 0.47s
Val loss: 0.4253 score: 0.8636 time: 0.23s
Test loss: 0.4597 score: 0.7907 time: 0.22s
Epoch 92/1000, LR 0.000266
Train loss: 0.3111;  Loss pred: 0.3111; Loss self: 0.0000; time: 0.39s
Val loss: 0.4183 score: 0.8636 time: 0.24s
Test loss: 0.4575 score: 0.7907 time: 0.23s
Epoch 93/1000, LR 0.000265
Train loss: 0.3015;  Loss pred: 0.3015; Loss self: 0.0000; time: 0.40s
Val loss: 0.4118 score: 0.8636 time: 0.22s
Test loss: 0.4571 score: 0.7907 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.2887;  Loss pred: 0.2887; Loss self: 0.0000; time: 0.34s
Val loss: 0.4054 score: 0.8636 time: 0.25s
Test loss: 0.4573 score: 0.7907 time: 0.21s
Epoch 95/1000, LR 0.000265
Train loss: 0.2952;  Loss pred: 0.2952; Loss self: 0.0000; time: 0.39s
Val loss: 0.3990 score: 0.8636 time: 0.22s
Test loss: 0.4574 score: 0.7907 time: 0.23s
Epoch 96/1000, LR 0.000265
Train loss: 0.2759;  Loss pred: 0.2759; Loss self: 0.0000; time: 0.34s
Val loss: 0.3921 score: 0.8636 time: 0.24s
Test loss: 0.4555 score: 0.7907 time: 0.22s
Epoch 97/1000, LR 0.000265
Train loss: 0.2760;  Loss pred: 0.2760; Loss self: 0.0000; time: 0.40s
Val loss: 0.3846 score: 0.8636 time: 0.21s
Test loss: 0.4514 score: 0.7674 time: 0.31s
Epoch 98/1000, LR 0.000265
Train loss: 0.2575;  Loss pred: 0.2575; Loss self: 0.0000; time: 0.37s
Val loss: 0.3771 score: 0.8864 time: 0.23s
Test loss: 0.4464 score: 0.7674 time: 0.21s
Epoch 99/1000, LR 0.000265
Train loss: 0.2441;  Loss pred: 0.2441; Loss self: 0.0000; time: 0.43s
Val loss: 0.3693 score: 0.8864 time: 0.22s
Test loss: 0.4389 score: 0.7674 time: 0.24s
Epoch 100/1000, LR 0.000265
Train loss: 0.2497;  Loss pred: 0.2497; Loss self: 0.0000; time: 0.47s
Val loss: 0.3620 score: 0.8864 time: 0.20s
Test loss: 0.4307 score: 0.7907 time: 0.22s
Epoch 101/1000, LR 0.000265
Train loss: 0.2381;  Loss pred: 0.2381; Loss self: 0.0000; time: 0.38s
Val loss: 0.3553 score: 0.8864 time: 0.25s
Test loss: 0.4256 score: 0.7907 time: 0.21s
Epoch 102/1000, LR 0.000264
Train loss: 0.2504;  Loss pred: 0.2504; Loss self: 0.0000; time: 0.43s
Val loss: 0.3488 score: 0.8864 time: 0.31s
Test loss: 0.4184 score: 0.7907 time: 0.23s
Epoch 103/1000, LR 0.000264
Train loss: 0.2038;  Loss pred: 0.2038; Loss self: 0.0000; time: 0.36s
Val loss: 0.3426 score: 0.8864 time: 0.24s
Test loss: 0.4112 score: 0.8140 time: 0.22s
Epoch 104/1000, LR 0.000264
Train loss: 0.2027;  Loss pred: 0.2027; Loss self: 0.0000; time: 0.40s
Val loss: 0.3364 score: 0.9318 time: 0.22s
Test loss: 0.4059 score: 0.8140 time: 0.23s
Epoch 105/1000, LR 0.000264
Train loss: 0.2022;  Loss pred: 0.2022; Loss self: 0.0000; time: 0.45s
Val loss: 0.3303 score: 0.9318 time: 0.23s
Test loss: 0.4009 score: 0.8140 time: 0.20s
Epoch 106/1000, LR 0.000264
Train loss: 0.1872;  Loss pred: 0.1872; Loss self: 0.0000; time: 0.38s
Val loss: 0.3238 score: 0.9318 time: 0.23s
Test loss: 0.3999 score: 0.8140 time: 0.23s
Epoch 107/1000, LR 0.000264
Train loss: 0.1700;  Loss pred: 0.1700; Loss self: 0.0000; time: 0.37s
Val loss: 0.3173 score: 0.9318 time: 0.23s
Test loss: 0.4012 score: 0.8140 time: 0.20s
Epoch 108/1000, LR 0.000264
Train loss: 0.1569;  Loss pred: 0.1569; Loss self: 0.0000; time: 0.39s
Val loss: 0.3109 score: 0.9318 time: 0.23s
Test loss: 0.4056 score: 0.8140 time: 0.23s
Epoch 109/1000, LR 0.000264
Train loss: 0.1700;  Loss pred: 0.1700; Loss self: 0.0000; time: 0.37s
Val loss: 0.3050 score: 0.9091 time: 0.22s
Test loss: 0.4118 score: 0.8140 time: 0.21s
Epoch 110/1000, LR 0.000263
Train loss: 0.1689;  Loss pred: 0.1689; Loss self: 0.0000; time: 0.39s
Val loss: 0.3000 score: 0.9091 time: 0.22s
Test loss: 0.4206 score: 0.8140 time: 0.23s
Epoch 111/1000, LR 0.000263
Train loss: 0.1437;  Loss pred: 0.1437; Loss self: 0.0000; time: 0.41s
Val loss: 0.2961 score: 0.9091 time: 0.32s
Test loss: 0.4310 score: 0.8140 time: 0.22s
Epoch 112/1000, LR 0.000263
Train loss: 0.1257;  Loss pred: 0.1257; Loss self: 0.0000; time: 0.37s
Val loss: 0.2921 score: 0.9091 time: 0.25s
Test loss: 0.4373 score: 0.8140 time: 0.21s
Epoch 113/1000, LR 0.000263
Train loss: 0.1140;  Loss pred: 0.1140; Loss self: 0.0000; time: 0.40s
Val loss: 0.2881 score: 0.9091 time: 0.22s
Test loss: 0.4426 score: 0.8140 time: 0.23s
Epoch 114/1000, LR 0.000263
Train loss: 0.1217;  Loss pred: 0.1217; Loss self: 0.0000; time: 0.38s
Val loss: 0.2826 score: 0.9091 time: 0.24s
Test loss: 0.4397 score: 0.8140 time: 0.21s
Epoch 115/1000, LR 0.000263
Train loss: 0.1482;  Loss pred: 0.1482; Loss self: 0.0000; time: 0.38s
Val loss: 0.2775 score: 0.9091 time: 0.21s
Test loss: 0.4362 score: 0.8140 time: 0.23s
Epoch 116/1000, LR 0.000263
Train loss: 0.1182;  Loss pred: 0.1182; Loss self: 0.0000; time: 0.34s
Val loss: 0.2728 score: 0.9091 time: 0.24s
Test loss: 0.4330 score: 0.8140 time: 0.22s
Epoch 117/1000, LR 0.000262
Train loss: 0.1181;  Loss pred: 0.1181; Loss self: 0.0000; time: 0.39s
Val loss: 0.2699 score: 0.9091 time: 0.21s
Test loss: 0.4362 score: 0.8140 time: 0.23s
Epoch 118/1000, LR 0.000262
Train loss: 0.0900;  Loss pred: 0.0900; Loss self: 0.0000; time: 0.36s
Val loss: 0.2663 score: 0.9091 time: 0.23s
Test loss: 0.4334 score: 0.8140 time: 0.20s
Epoch 119/1000, LR 0.000262
Train loss: 0.0784;  Loss pred: 0.0784; Loss self: 0.0000; time: 0.38s
Val loss: 0.2632 score: 0.9091 time: 0.21s
Test loss: 0.4309 score: 0.8140 time: 0.22s
Epoch 120/1000, LR 0.000262
Train loss: 0.0797;  Loss pred: 0.0797; Loss self: 0.0000; time: 0.45s
Val loss: 0.2606 score: 0.9091 time: 0.25s
Test loss: 0.4277 score: 0.8140 time: 0.21s
Epoch 121/1000, LR 0.000262
Train loss: 0.0794;  Loss pred: 0.0794; Loss self: 0.0000; time: 0.39s
Val loss: 0.2587 score: 0.9091 time: 0.27s
Test loss: 0.4268 score: 0.8140 time: 0.23s
Epoch 122/1000, LR 0.000262
Train loss: 0.0823;  Loss pred: 0.0823; Loss self: 0.0000; time: 0.39s
Val loss: 0.2575 score: 0.9091 time: 0.23s
Test loss: 0.4288 score: 0.8140 time: 0.22s
Epoch 123/1000, LR 0.000262
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 0.40s
Val loss: 0.2566 score: 0.9091 time: 0.23s
Test loss: 0.4361 score: 0.8140 time: 0.31s
Epoch 124/1000, LR 0.000261
Train loss: 0.0588;  Loss pred: 0.0588; Loss self: 0.0000; time: 0.39s
Val loss: 0.2561 score: 0.9091 time: 0.22s
Test loss: 0.4443 score: 0.8140 time: 0.23s
Epoch 125/1000, LR 0.000261
Train loss: 0.0547;  Loss pred: 0.0547; Loss self: 0.0000; time: 0.37s
Val loss: 0.2558 score: 0.9091 time: 0.24s
Test loss: 0.4499 score: 0.8140 time: 0.22s
Epoch 126/1000, LR 0.000261
Train loss: 0.0657;  Loss pred: 0.0657; Loss self: 0.0000; time: 0.41s
Val loss: 0.2562 score: 0.9091 time: 0.22s
Test loss: 0.4614 score: 0.8140 time: 0.23s
     INFO: Early stopping counter 1 of 2
Epoch 127/1000, LR 0.000261
Train loss: 0.0458;  Loss pred: 0.0458; Loss self: 0.0000; time: 0.35s
Val loss: 0.2571 score: 0.9091 time: 0.24s
Test loss: 0.4743 score: 0.8140 time: 0.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 124,   Train_Loss: 0.0547,   Val_Loss: 0.2558,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2558,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4499


[0.25712007097899914, 0.23332176508847624, 0.24219595710746944, 0.2744539009872824, 0.24909903097432107, 0.26644965005107224, 0.24126906204037368, 0.24368513911031187, 0.24955843295902014, 0.23095162399113178, 0.24934063700493425, 0.2409207959426567, 0.2694052540464327, 0.24828887998592108, 0.24752101895865053, 0.25945665303152055, 0.2521086579654366, 0.2771361330524087, 0.23865933495108038, 0.25794203602708876, 0.2337836279766634, 0.24903763399925083, 0.23902932705823332, 0.24868320603854954, 0.249000997049734, 0.24557937297504395, 0.2584755630232394, 0.24944539205171168, 0.2582385289715603, 0.24880599603056908, 0.25293041102122515, 0.23301660805009305, 0.24842520791571587, 0.2629505640361458, 0.2555404750164598, 0.2629542740760371, 0.23514532600529492, 0.2597851409809664, 0.3595562770497054, 0.24728567292913795, 0.26343834307044744, 0.24926511500962079, 0.287277038092725, 0.22953708504792303, 0.2521180759649724, 0.25241230498068035, 0.24655903794337064, 0.2627724129706621, 0.2487175449496135, 0.25885671889409423, 0.24881036393344402, 0.25781949097290635, 0.23825471091549844, 0.25702911510597914, 0.23656965501140803, 0.3733781510964036, 0.2617239939281717, 0.2431262549944222, 0.26047433400526643, 0.2515483469469473, 0.2639579640235752, 0.23461534397210926, 0.36883005301933736, 0.2611365159973502, 0.23631519102491438, 0.24519132496789098, 0.2550145830027759, 0.2599668470211327, 0.30255002598278224, 0.36834167095366865, 0.25699170492589474, 0.2518272050656378, 0.2608876349404454, 0.24783193797338754, 0.29176521697081625, 0.37968192098196596, 0.25526358105707914, 0.2975202149245888, 0.23438004800118506, 0.24521326692774892, 0.23367902101017535, 0.24884514010045677, 0.23565951408818364, 0.25711100397165865, 0.235582691966556, 0.35451121802907437, 0.2568977529881522, 0.2506360140396282, 0.2608437999151647, 0.2369368189247325, 0.2646928640315309, 0.23572257300838828, 0.2588460349943489, 0.3672608779743314, 0.24770997907035053, 0.24040370795410126, 0.26284044003114104, 0.2283419700106606, 0.2544930559815839, 0.22996392904315144, 0.24430559494066983, 0.26008692500181496, 0.23992635204922408, 0.2513617370277643, 0.2449920770013705, 0.25600513094104826, 0.24751502997241914, 0.256301415967755, 0.2322615149896592, 0.23720126692205667, 0.2403569760499522, 0.24438791594002396, 0.22995038493536413, 0.2531386259943247, 0.22836878895759583, 0.2536262039793655, 0.30311288498342037, 0.24448485497850925, 0.24297881498932838, 0.25745558296330273, 0.2551837529754266, 0.2458718599518761, 0.25682031107135117, 0.23308542906306684, 0.2684047990478575, 0.23188625590410084, 0.2533312590094283, 0.23519769392441958, 0.2432399159297347, 0.23179883195552975, 0.24013908300548792, 0.26191234099678695, 0.22755432699341327, 0.30760300101246685, 0.2581905419938266, 0.24406603793613613, 0.23533476702868938, 0.26534024509601295, 0.22903464699629694, 0.26227089494932443, 0.2615764520596713, 0.24521923996508121, 0.2570672680158168, 0.2477094039786607, 0.25843274302314967, 0.24268926901277155, 0.25045226502697915, 0.231003628927283, 0.25598241307307035, 0.23243095504585654, 0.25528148899320513, 0.23846509691793472, 0.2560870529850945, 0.2380691630532965, 0.2442921930924058, 0.2488434580154717, 0.2505306400125846, 0.2513703510630876, 0.24648626591078937, 0.2548981789732352, 0.3346047989325598, 0.23073329299222678, 0.2984497779980302, 0.24406567204277962, 0.24745931406505406, 0.24012911098543555, 0.25414401199668646, 0.24350160895846784, 0.24393576895818114, 0.22841638093814254, 0.25649231497664005, 0.23110083304345608, 0.2480296939611435, 0.2439572470029816, 0.2542540659196675, 0.24519336898811162, 0.2527358280494809, 0.22981623897794634, 0.25567709608003497, 0.2410140410065651, 0.2496088040061295, 0.2411970989778638, 0.2454214550089091, 0.24798468500375748, 0.2426125550409779, 0.24548250902444124, 0.24231556500308216, 0.2555965030333027, 0.24053611897397786, 0.24040114192757756, 0.23321751703042537, 0.23117327596992254, 0.24516669393051416, 0.24191496800631285, 0.22009017900563776, 0.2276410380145535, 0.22478152997791767, 0.23656680900603533, 0.2115569479065016, 0.21566995698958635, 0.22745463496539742, 0.22514366393443197, 0.22366710391361266, 0.319202313083224, 0.21389059396460652, 0.28621715400367975, 0.2753450219752267, 0.20585786504670978, 0.19928059598896652, 0.21110116597265005, 0.18472106906119734, 0.1893717540660873, 0.19174908206332475, 0.2262451279675588, 0.22806915908586234, 0.23040237498935312, 0.21321749896742404, 0.23486123094335198, 0.21852377394679934, 0.22152006498072296, 0.26076450501568615, 0.2060732579557225, 0.23497435194440186, 0.21212106198072433, 0.23889207490719855, 0.3005970250815153, 0.23342379496898502, 0.21743146795779467, 0.3262332329759374, 0.2593904280802235, 0.22076206596102566, 0.21798452292568982, 0.22259548399597406, 0.2135953139513731, 0.224519838928245, 0.21087014605291188, 0.3609275280032307, 0.23259302403312176, 0.22104583599139005, 0.22335249104071409, 0.22978344303555787, 0.22001209598965943, 0.22816944890655577, 0.2217130089411512, 0.226281848968938, 0.22289762599393725, 0.22000968700740486, 0.25171373202465475, 0.2070846010465175, 0.23434380791150033, 0.23437942506279796, 0.21768724697176367, 0.23685314098838717, 0.21236049802973866, 0.23714369104709476, 0.21015549905132502, 0.22886882501188666, 0.21102781791705638, 0.23703281406778842, 0.23847510595805943, 0.21046942891553044, 0.2277526200050488, 0.2103675869293511, 0.22759522905107588, 0.2081726569449529, 0.23514818609692156, 0.2989218910224736, 0.22052216797601432, 0.20897876808885485, 0.22916292597074062, 0.206090864026919, 0.2319492109818384, 0.20646166102960706, 0.24606015195604414, 0.3073642859235406, 0.24652892688754946, 0.230772488983348, 0.21045393391977996, 0.2267577169695869, 0.2039833889575675, 0.22831368900369853, 0.2098149579251185, 0.2163103410275653, 0.22433597303461283, 0.23035436298232526, 0.23829407803714275, 0.21003366296645254, 0.23005016997922212, 0.2213290969375521, 0.31130343896802515, 0.21292213292326778, 0.2470031949924305, 0.22189430298749357, 0.2162408260628581, 0.2300203361082822, 0.22576192894484848, 0.23691018007230014, 0.20646745502017438, 0.23439181398134679, 0.2082511690678075, 0.23362096201162785, 0.2103699560975656, 0.23453980300109833, 0.22621692600660026, 0.21909420809242874, 0.2365906819468364, 0.21095106203574687, 0.23441240494139493, 0.22004841605667025, 0.23747040890157223, 0.20408765505999327, 0.22793431393802166, 0.21072381897829473, 0.23333067796193063, 0.22105888195801526, 0.31500729208346456, 0.23395217710640281, 0.22502002795226872, 0.23451706394553185, 0.30270236101932824]
[0.005843637976795435, 0.00530276738837446, 0.005504453570624305, 0.006237588658801873, 0.005661341613052751, 0.006055673864797096, 0.005483387773644857, 0.005538298616143452, 0.005671782567250458, 0.0052489005452529955, 0.005666832659203051, 0.00547547263506038, 0.00612284668287347, 0.005642929090589116, 0.005625477703605694, 0.00589674211435274, 0.005729742226487195, 0.006298548478463834, 0.005424075794342736, 0.005862319000615654, 0.005313264272196896, 0.0056599462272557, 0.005432484705868939, 0.005651891046330671, 0.005659113569312136, 0.005581349385796453, 0.005874444614164531, 0.00566921345572072, 0.005869057476626371, 0.005654681727967479, 0.005748418432300572, 0.0052958320011384785, 0.005646027452629906, 0.005976149182639678, 0.005807738068555905, 0.005976233501728116, 0.005344211954665793, 0.0059042077495674175, 0.008171733569311486, 0.00562012893020768, 0.005987235069782896, 0.005665116250218654, 0.006529023593016477, 0.005216751932907341, 0.005729956271931191, 0.005736643295015462, 0.005603614498712969, 0.005972100294787775, 0.00565267147612758, 0.0058831072475930505, 0.005654780998487364, 0.005859533885747872, 0.005414879793534055, 0.005841570797863163, 0.005376583068441091, 0.00848586707037281, 0.005948272589276629, 0.005525596704418686, 0.005919871227392418, 0.005717007885157893, 0.005999044636899436, 0.0053321669084570285, 0.00838250120498494, 0.005934920818121595, 0.005370799796020781, 0.005572530112906613, 0.005795785977335816, 0.005908337432298471, 0.006876136954154142, 0.008371401612583379, 0.005840720566497607, 0.005723345569673587, 0.005929264430464668, 0.005632544044849717, 0.0066310276584276425, 0.008629134567771953, 0.005801445024024526, 0.006761823066467928, 0.0053268192727542055, 0.005573028793812476, 0.005310886841140349, 0.0056555713659194725, 0.005355898047458719, 0.005843431908446787, 0.005354152090149, 0.008057073137024418, 0.0058385852951852776, 0.005696273046355186, 0.005928268179890107, 0.00538492770283483, 0.006015746909807521, 0.005357331204736097, 0.005882864431689747, 0.008346838135780259, 0.005629772251598875, 0.005590783905909332, 0.006112568372817233, 0.0053102783723409435, 0.005918443162362416, 0.0053479983498407315, 0.005681525463736508, 0.006048533139577092, 0.005579682605795909, 0.005845621791343356, 0.005697490162822569, 0.005953607696303448, 0.005756163487730678, 0.005960498045761745, 0.005401430581154865, 0.005516308533071086, 0.005589697117440749, 0.0056834399055819525, 0.005347683370589863, 0.00588694479056569, 0.0053109020687812985, 0.005898283813473617, 0.007049136860079543, 0.005685694301825797, 0.005650670116030892, 0.005987339138681459, 0.005934505883149456, 0.005717950231438979, 0.005972565373752353, 0.005420591373559694, 0.006241972070880408, 0.005392703625676763, 0.00589142462812624, 0.005469713812195804, 0.005656742230924063, 0.005390670510593715, 0.005584629837336929, 0.006090984674343883, 0.005291961092870076, 0.0071535581630806245, 0.006004431209158759, 0.0056759543706078166, 0.00547290155880673, 0.006170703374325882, 0.005326387139448766, 0.006099323138356382, 0.006083173303713285, 0.0057027730224437495, 0.005978308558507368, 0.005760683813457226, 0.006010063791236039, 0.005643936488669106, 0.005824471279697189, 0.005372177416913558, 0.005953079373792334, 0.005405371047578059, 0.005936778813795468, 0.005545699928324063, 0.005955512860118476, 0.005536492164030151, 0.005681213792846646, 0.005787057163150504, 0.005826293953781037, 0.005845822117746223, 0.005732238742111381, 0.005927864627284539, 0.007781506951919995, 0.0053658905347029485, 0.006940692511582097, 0.005675945861459991, 0.005754867768954746, 0.00558439792989385, 0.005910325860388057, 0.005662828115313206, 0.005672924859492585, 0.00531200885902657, 0.00596493755759628, 0.005374437977754792, 0.005768132417701011, 0.005673424348906549, 0.005912885253945756, 0.00570217137181655, 0.005877577396499556, 0.005344563697161543, 0.005945978978605464, 0.005604977697827095, 0.0058048559071192905, 0.00560923485995032, 0.005707475697881607, 0.005767085697761801, 0.00564215244281344, 0.005708895558707936, 0.005635245697746097, 0.005944104721704715, 0.0055938632319529735, 0.005590724230873896, 0.005423663186754079, 0.005376122696974943, 0.005701551021639864, 0.005625929488518903, 0.005118376255945065, 0.00529397762824543, 0.005227477441346923, 0.005501553697814775, 0.004919929021081433, 0.00501558039510666, 0.005289642673613894, 0.00523589916126586, 0.005201560556130527, 0.007423309606586604, 0.004974199859642012, 0.006656212883806506, 0.006403372604075039, 0.0047873922103885995, 0.004634432464859686, 0.0049093294412244195, 0.004295838815376683, 0.004403994280606682, 0.004459280978216854, 0.005261514603896717, 0.0053039339322293565, 0.005358194767194259, 0.0049585464876145124, 0.00546188909170586, 0.005081948231320915, 0.005151629418156348, 0.006064290814318283, 0.004792401347807501, 0.0054645198126605085, 0.004933047953040101, 0.005555629649004618, 0.006990628490267798, 0.005428460348115931, 0.005056545766460341, 0.007586819371533429, 0.0060323355367493834, 0.005134001533977341, 0.005069407509899763, 0.005176639162697071, 0.004967332882590072, 0.005221391602982442, 0.004903956884951439, 0.008393663441935597, 0.005409140093793529, 0.005140600837009071, 0.005194243977691026, 0.0053438010008269275, 0.0051165603718525444, 0.005306266253640832, 0.005156116487003516, 0.005262368580672977, 0.005183665720789239, 0.005116504349009415, 0.005853807721503599, 0.004815920954570174, 0.005449855997941868, 0.005450684303785999, 0.005062494115622411, 0.005508212581125283, 0.004938616233249736, 0.005514969559234762, 0.004887337187240117, 0.005322530814229922, 0.004907623672489684, 0.005512391024832289, 0.005545932696699057, 0.004894637881756522, 0.005296572558256949, 0.004892269463473281, 0.005292912303513393, 0.004841224580115184, 0.005468562467370269, 0.006951671884243571, 0.0051284225110701, 0.004859971350903602, 0.0053293703714125725, 0.004792810791323698, 0.005394167697252056, 0.004801433977432722, 0.0057223291152568405, 0.007148006649384665, 0.005733230857849988, 0.005366802069380186, 0.004894277533018138, 0.005273435278362487, 0.004743799743199244, 0.005309620674504617, 0.004879417626165547, 0.005030473047152682, 0.005217115651967741, 0.005357078208891285, 0.0055417227450498315, 0.004884503789917501, 0.005350003953005166, 0.005147188300873304, 0.0072396148597215155, 0.004951677509843436, 0.005744260348661175, 0.005160332627616129, 0.005028856420066468, 0.0053493101420530745, 0.005250277417322057, 0.005509539071448841, 0.004801568721399405, 0.005450972418170856, 0.004843050443437384, 0.005433045628177392, 0.004892324560408502, 0.005454414023281356, 0.005260858744339541, 0.00509521414168439, 0.0055021088824845675, 0.004905838651994113, 0.005451451277706859, 0.005117405024573727, 0.005522567648873772, 0.004746224536278913, 0.005300797998558643, 0.0049005539297277845, 0.005426294836323968, 0.00514090423158175, 0.007325750978685222, 0.005440748304800066, 0.005233023905866715, 0.005453885208035625, 0.007039589791147168]
[171.12627509967433, 188.58077806549716, 181.6710754609166, 160.31836254365672, 176.636576336324, 165.13438839783132, 182.3690100500208, 180.560867029655, 176.31141323613463, 190.51608834623107, 176.46541906897139, 182.63263587454176, 163.32272418271577, 177.21293036762245, 177.76268126688018, 169.58516764129607, 174.5279212347887, 158.76673862545107, 184.36320544100644, 170.58095949657144, 188.20821791845978, 176.68012377652272, 184.0778307060227, 176.9319315964559, 176.70611973980047, 179.16814212434417, 170.22885833135408, 176.3913120947871, 170.3851093608332, 176.84461267804022, 173.96089233535326, 188.82774222917638, 177.11568149287024, 167.33183349989565, 172.18407376430633, 167.32947260357804, 187.11832698307268, 169.37073396058375, 122.37305481366245, 177.9318610690746, 167.02200403771036, 176.51888431440457, 153.16225860626574, 191.69015756566583, 174.52140165512395, 174.31796759420172, 178.45624466666627, 167.4452789871534, 176.9075036862146, 169.97820333959217, 176.84150814461194, 170.66203890932303, 184.67630642403304, 171.18683220715195, 185.991732531707, 117.84299609068317, 168.1160345278679, 180.97592956799113, 168.9225933450717, 174.91667321224656, 166.69320875679347, 187.54101609496888, 119.29613555024793, 168.4942445982793, 186.19200826307073, 179.4516996299197, 172.5391523963202, 169.2523508446569, 145.43049486468723, 119.45430959815154, 171.21175180610476, 174.72298113514609, 168.6549843960377, 177.53966805006692, 150.80618744351932, 115.88647646483516, 172.37084827295132, 147.88911069841924, 187.72928999390567, 179.43564208931818, 188.29246977238944, 176.8167945021452, 186.7100514496318, 171.13230985963605, 186.77093649242434, 124.11454916608999, 171.27436689580242, 175.553382336519, 168.68332701145397, 185.7035145473842, 166.23039748725, 186.66010403014837, 169.9852192094061, 119.8058454869654, 177.627078913538, 178.86579356841582, 163.59735204714087, 188.31404493003393, 168.96335278834346, 186.98584677569713, 176.0090677024513, 165.32934133347894, 179.22166378446823, 171.06820038902902, 175.51588004929422, 167.96538351374625, 173.7268238005245, 167.7712151438515, 185.13613846837458, 181.28065063889233, 178.90056992172975, 175.94977981870744, 186.99686026656013, 169.86739906285203, 188.29192989233027, 169.5408413063596, 141.86133988448563, 175.8800151599566, 176.97016096604372, 167.0191009457703, 168.5060255546158, 174.88784608542142, 167.4322401550768, 184.48171630825246, 160.20577930252637, 185.4357423312882, 169.73823194238992, 182.8249218030938, 176.78019594621762, 185.5056802367731, 179.06289747519907, 164.17706716816184, 188.96586396814448, 139.7905737540488, 166.54366836190357, 176.18182506511477, 182.7184335868143, 162.05608005088152, 187.74452059515357, 163.95261856375026, 164.38788607084084, 175.3532879643666, 167.2713929388875, 173.59050286078076, 166.38758501335948, 177.18129925941275, 171.68940354908736, 186.14426188748686, 167.9802900667462, 185.0011759041152, 168.44151203279975, 180.3198898109521, 167.91165152149574, 180.6197806070902, 176.01872354445175, 172.79939903956205, 171.63569293496408, 171.06233817212632, 174.45191049939862, 168.69481050515896, 128.50981258241526, 186.3623556113709, 144.07784213625317, 176.18208918976117, 173.7659387057697, 179.07033355321946, 169.1954087848453, 176.59020892685015, 176.2759114157287, 188.25269809193253, 167.64634840586243, 186.06596710931936, 173.3663389784951, 176.26039204924484, 169.12217251851544, 175.37178993647618, 170.1381253772275, 187.1060121392308, 168.18088385413938, 178.4128419257893, 172.26956465423422, 178.27743443940173, 175.20880559704548, 173.39780478519657, 177.23732389998196, 175.16522937167284, 177.45455187516765, 168.2339135696131, 178.7673310079968, 178.86770277053856, 184.37723095384064, 186.00765949086016, 175.39087104624082, 177.74840620394312, 195.37446057008535, 188.89388475399122, 191.29685612614293, 181.7668344121046, 203.2549648003242, 199.37872015283176, 189.04868659432495, 190.98916331273165, 192.24999674788103, 134.71080326660675, 201.03735841285012, 150.2355795189242, 156.1677044005858, 208.8819875317524, 215.77615114308853, 203.69380624629528, 232.78340807866522, 227.0665982477713, 224.25139947110327, 190.05934132718983, 188.53930172913724, 186.63002064100698, 201.6720025712789, 183.08683739451024, 196.77492852776996, 194.11334139750244, 164.89974353454798, 208.6636588685326, 182.99869600310416, 202.71442919660404, 179.9975994042667, 143.04865455118642, 184.21429574355693, 197.76346268492597, 131.80754029179985, 165.77327204495413, 194.77984051658322, 197.26171116588196, 193.1755273201217, 201.31527796433463, 191.5198238394537, 203.91696408846025, 119.13749066991394, 184.8722685417973, 194.5297897476562, 192.5207988486759, 187.13271692663236, 195.443799608277, 188.45643098174008, 193.94441582547552, 190.02849851161798, 192.91367419574755, 195.44593960788984, 170.82897962749308, 207.64460410236342, 183.49108680626586, 183.46320283224043, 197.53109379704523, 181.5470963169886, 202.4858690714614, 181.32466358323046, 204.61039655925617, 187.88054684934363, 203.76460518063536, 181.40948192811203, 180.3123216037585, 204.30520585133326, 188.80134067852558, 204.40411295130238, 188.93190414966975, 206.55930817739667, 182.86341355096226, 143.85028762168233, 194.9917343669368, 205.76252981698602, 187.63942648162126, 208.64583300685985, 185.38541182348274, 208.27111331741978, 174.75401708961581, 139.89914238343425, 174.42172220063173, 186.3307025435895, 204.32024813748828, 189.62970951840737, 210.80147858973376, 188.3373712178223, 204.9424903983557, 198.78846196502616, 191.6767935981695, 186.66891932626137, 180.44930177953316, 204.72908672200865, 186.91574974225716, 194.28082703528327, 138.12889488964706, 201.9517624102338, 174.08681697950405, 193.7859576431918, 198.85236651611993, 186.93999290461747, 190.46612598045482, 181.50338658675318, 208.26526871171234, 183.45350577568377, 206.48143389773247, 184.05882601347986, 204.40181096989636, 183.33775099060108, 190.0830356024969, 196.2626049058298, 181.74849341557078, 203.83874622405742, 183.43739108325073, 195.41154065351682, 181.07519247934098, 210.6937824698892, 188.65084092468967, 204.05856446835347, 184.28781151107665, 194.51830941661416, 136.50477649452853, 183.79824685471232, 191.09410122871893, 183.35552763864993, 142.0537317753341]
Elapsed: 0.24494771304016466~0.029610716749288456
Time per graph: 0.005655548593286415~0.0006635511397612445
Speed: 178.88526571780287~17.99862229212111
Total Time: 0.3036
best val loss: 0.2558317482471466 test_score: 0.8140

Testing...
Test loss: 0.4059 score: 0.8140 time: 0.22s
test Score 0.8140
Epoch Time List: [1.0460550060961396, 0.907733665080741, 0.8435050990665331, 1.0622012718813494, 0.7984094719868153, 0.8527098279446363, 0.807070963899605, 0.9175945681054145, 0.8772904530633241, 0.8222495758673176, 0.8469435110455379, 0.8304426420945674, 0.9153775400482118, 1.0184815380489454, 0.8232487239874899, 0.860969015979208, 0.8137134370626882, 0.8644737609429285, 0.8336923940805718, 0.8296770859742537, 0.7844027801183984, 0.8304127299925312, 0.8040466289967299, 0.9430571330012754, 0.9593064821092412, 0.8183031369699165, 0.8619577761273831, 0.8045638209441677, 0.8442684438778087, 0.7901164060458541, 0.8317910999758169, 0.8891613491578028, 0.957887097960338, 0.9655396499438211, 0.8054049761267379, 0.8499639170477167, 0.796928358846344, 0.8300930210389197, 0.9124319029506296, 0.8473527588648722, 0.8722431370988488, 0.8025498589267954, 0.9255360200768337, 0.8281911530066282, 0.8852933010784909, 0.9575252078939229, 0.8113203641260043, 0.8359653609804809, 0.7911765169119462, 0.8277496789814904, 0.7877416219562292, 0.8422055820701644, 0.7835703571327031, 0.8327499210136011, 0.7814761679619551, 1.074640849023126, 0.8385379491373897, 0.7673661310691386, 0.8198495759861544, 0.797011295100674, 0.8487920011393726, 0.794103313004598, 0.9493680681334808, 0.8645199830643833, 0.89650624897331, 0.8192098170984536, 0.8154348660027608, 0.8298302059993148, 0.8561289949575439, 0.9728769719367847, 0.8775338439736515, 0.8383038160391152, 0.8465807199245319, 0.788708480889909, 0.9541397029533982, 0.9870076520601287, 0.8263273440534249, 0.9522916621062905, 0.8268553089583293, 0.8192930119112134, 0.8118059100816026, 0.8198566479841247, 0.8239788690116256, 0.8202207629801705, 0.7994556890334934, 0.9253917450550944, 0.921310066129081, 0.7836800038348883, 0.8483302799286321, 0.7946052240440622, 0.8437139899469912, 0.8047115980880335, 0.8217122480273247, 0.9126264470396563, 0.9147438278887421, 0.8014905280433595, 0.8628226480213925, 0.8338124150177464, 0.824787967139855, 0.8752797041088343, 0.9009455809136853, 0.9379644739674404, 0.8210217260057107, 0.8526763501577079, 0.8142131878994405, 0.8403114349348471, 0.801036490011029, 0.8181230209302157, 0.8931868160143495, 0.865299831959419, 0.8600391780491918, 0.8107994191814214, 0.8016051519662142, 0.8249033039901406, 0.7965222240891308, 0.8192773000337183, 0.9461604461539537, 0.8067775659728795, 0.902423272957094, 0.8554480999009684, 0.8205885249190032, 0.8064058730378747, 0.8477034820243716, 0.9185746690491214, 0.8444261810509488, 0.8089710128260776, 0.8611683959607035, 0.8209577009547502, 0.8345295939361677, 0.8032279500039294, 0.9480876290472224, 0.8871096960501745, 0.8458145341137424, 1.0412830179557204, 0.9382149969460443, 0.8571088298922405, 0.9087362070567906, 0.8608402239624411, 0.8715951311169192, 0.9348418469307944, 0.950979272951372, 0.8288628740701824, 0.8386272679781541, 0.81345271889586, 0.8367655189940706, 0.7961667620111257, 0.8226410250645131, 0.8644425859674811, 0.8853477251250297, 0.8191269011003897, 0.8350591839989647, 0.8168202750384808, 0.8598421089118347, 0.8238573630806059, 0.9729034071788192, 0.8754501839866862, 0.7999150169780478, 0.8260737509699538, 0.7920291960472241, 0.8162809528876096, 0.8793900599703193, 0.9351614291081205, 1.0100711459526792, 0.7820022699888796, 0.8289757568854839, 0.7841605830471963, 0.8122000599978492, 0.7868223951663822, 0.8025112790055573, 0.8865203651366755, 0.8337087768595666, 0.7765104139689356, 0.8180862830486149, 0.7925088660558686, 0.8200899180956185, 0.7915139840915799, 0.8226164060179144, 0.8672397730406374, 0.8420679941773415, 0.7830773231107742, 0.8540875810431316, 0.8104633879847825, 0.825219947961159, 0.8191610730718821, 0.9321693349629641, 0.9242206100607291, 0.862429357948713, 0.8464882089756429, 0.7838439479237422, 0.9114502978045493, 0.823948354111053, 1.0590263899648562, 0.8528143371222541, 0.879257126012817, 0.8215335799613968, 0.849782585981302, 0.8981905960245058, 0.8649117719614878, 0.7873810180462897, 0.9637759601464495, 0.9092157341074198, 0.8265780180227011, 1.0437313370639458, 0.9417137370910496, 0.8095915390877053, 0.8458870571339503, 0.9541767729679123, 0.8727228390052915, 0.8160255629336461, 0.8750404599122703, 0.7247312191175297, 0.7235465310513973, 0.7779238248476759, 0.9523181929253042, 0.8615591861307621, 0.8418184830807149, 0.8046662751585245, 0.8549373421119526, 0.8395369539503008, 0.8394805480493233, 0.8840068511199206, 0.9401444209506735, 0.8494605770101771, 0.8309650680748746, 0.8778038050513715, 0.9420587552012876, 0.9012565350858495, 0.9784920100355521, 1.0383688290603459, 1.1384814189514145, 0.8383131189038977, 0.8254130510613322, 0.8667645860696211, 0.8175536339404061, 0.8407011800445616, 0.8511301398975775, 1.1334974319906905, 0.9459571599727497, 0.8276941271033138, 0.8382786010624841, 0.8233083680970594, 0.8408381189219654, 0.8654402630636469, 0.8397805979475379, 0.9536274150013924, 0.8308126439806074, 0.8383979410864413, 0.9213194870389998, 0.8257536079036072, 0.9581992331659421, 0.9237642398802564, 0.8930739430943504, 0.9156961381668225, 0.7915391320129856, 0.8472695811651647, 0.7982989070005715, 0.8375126868486404, 0.7785817601252347, 0.8582693720236421, 0.9169295799219981, 0.8276170011376962, 0.9352418651105836, 0.789739300031215, 0.8379355370998383, 0.8253044239245355, 0.84797906386666, 0.8872886329190806, 0.8146256488980725, 0.8491557561792433, 0.8572467600461096, 0.8127876839134842, 0.8423301790608093, 0.8383787941420451, 0.8349810998188332, 0.971107963938266, 0.9402925830800086, 0.8557076050201431, 0.7858483929885551, 0.8941111390013248, 0.796037124004215, 0.8097262789960951, 0.773194159148261, 0.9456324690254405, 0.9219514629803598, 0.854074354050681, 0.8549087989376858, 0.7967743531335145, 0.8283564209705219, 0.7930870429845527, 0.9180249329656363, 0.8059236201224849, 0.8933305910322815, 0.8875241721980274, 0.8496128849219531, 0.9627871799748391, 0.8202954890439287, 0.8534959899261594, 0.8782169930636883, 0.8402981939725578, 0.7960690590552986, 0.8464296191232279, 0.7941594369476661, 0.8396055829944089, 0.9519764931173995, 0.8346323929727077, 0.8518222559941933, 0.8261798110324889, 0.8202581639634445, 0.7987615090096369, 0.838450025068596, 0.794431891059503, 0.8190275189699605, 0.905991644016467, 0.8853844980476424, 0.8369076671078801, 0.9422066040569916, 0.84026659198571, 0.8351647920208052, 0.8589859318453819, 0.8958339920500293]
Total Epoch List: [95, 98, 127]
Total Time List: [0.24818018602672964, 0.2457790799671784, 0.3035880490206182]
========================training times:6========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334c70>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7045 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7015 score: 0.5000 time: 0.26s
Epoch 2/1000, LR 0.000000
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7045 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7014 score: 0.5000 time: 0.24s
Epoch 3/1000, LR 0.000030
Train loss: 0.6984;  Loss pred: 0.6984; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7043 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.5000 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7041 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7011 score: 0.5000 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7039 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7008 score: 0.5000 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6980;  Loss pred: 0.6980; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7035 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7005 score: 0.5000 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7032 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7002 score: 0.5000 time: 0.26s
Epoch 8/1000, LR 0.000180
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7028 score: 0.4884 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6998 score: 0.5000 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7023 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6993 score: 0.5000 time: 0.26s
Epoch 10/1000, LR 0.000240
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7018 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6988 score: 0.5000 time: 0.23s
Epoch 11/1000, LR 0.000270
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7013 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6983 score: 0.5000 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7008 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6979 score: 0.5000 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7003 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6974 score: 0.5000 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6998 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.5000 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6964 score: 0.5000 time: 0.34s
Epoch 16/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6987 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5000 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5000 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5000 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6967 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.28s
Epoch 21/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6957 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5000 time: 0.26s
Epoch 25/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5000 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6902 score: 0.5000 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.5000 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5000 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6888;  Loss pred: 0.6888; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.5000 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.4884 time: 0.24s
Test loss: 0.6873 score: 0.5227 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.4884 time: 0.26s
Test loss: 0.6866 score: 0.5455 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.4884 time: 0.29s
Test loss: 0.6858 score: 0.6136 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.47s
Val loss: 0.6889 score: 0.5581 time: 0.26s
Test loss: 0.6850 score: 0.6591 time: 0.32s
Epoch 34/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.29s
Val loss: 0.6882 score: 0.6744 time: 0.28s
Test loss: 0.6841 score: 0.6591 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6853;  Loss pred: 0.6853; Loss self: 0.0000; time: 0.35s
Val loss: 0.6875 score: 0.6047 time: 0.24s
Test loss: 0.6833 score: 0.7045 time: 0.26s
Epoch 36/1000, LR 0.000270
Train loss: 0.6840;  Loss pred: 0.6840; Loss self: 0.0000; time: 0.32s
Val loss: 0.6867 score: 0.6744 time: 0.26s
Test loss: 0.6824 score: 0.7045 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6836;  Loss pred: 0.6836; Loss self: 0.0000; time: 0.34s
Val loss: 0.6859 score: 0.7209 time: 0.24s
Test loss: 0.6815 score: 0.7273 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.30s
Val loss: 0.6851 score: 0.6512 time: 0.25s
Test loss: 0.6806 score: 0.7500 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6814;  Loss pred: 0.6814; Loss self: 0.0000; time: 0.34s
Val loss: 0.6843 score: 0.6047 time: 0.24s
Test loss: 0.6795 score: 0.7045 time: 0.33s
Epoch 40/1000, LR 0.000269
Train loss: 0.6807;  Loss pred: 0.6807; Loss self: 0.0000; time: 0.42s
Val loss: 0.6834 score: 0.6047 time: 0.24s
Test loss: 0.6784 score: 0.6818 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.32s
Val loss: 0.6824 score: 0.5581 time: 0.26s
Test loss: 0.6772 score: 0.7045 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6769;  Loss pred: 0.6769; Loss self: 0.0000; time: 0.35s
Val loss: 0.6813 score: 0.5581 time: 0.25s
Test loss: 0.6759 score: 0.6136 time: 0.26s
Epoch 43/1000, LR 0.000269
Train loss: 0.6765;  Loss pred: 0.6765; Loss self: 0.0000; time: 0.28s
Val loss: 0.6802 score: 0.5581 time: 0.30s
Test loss: 0.6746 score: 0.6136 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6750;  Loss pred: 0.6750; Loss self: 0.0000; time: 0.34s
Val loss: 0.6790 score: 0.5581 time: 0.24s
Test loss: 0.6732 score: 0.6136 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.28s
Val loss: 0.6777 score: 0.5581 time: 0.26s
Test loss: 0.6716 score: 0.5682 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.33s
Val loss: 0.6763 score: 0.5581 time: 0.24s
Test loss: 0.6699 score: 0.5682 time: 0.33s
Epoch 47/1000, LR 0.000269
Train loss: 0.6701;  Loss pred: 0.6701; Loss self: 0.0000; time: 0.34s
Val loss: 0.6748 score: 0.5349 time: 0.25s
Test loss: 0.6682 score: 0.5682 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.34s
Val loss: 0.6732 score: 0.5349 time: 0.24s
Test loss: 0.6663 score: 0.5682 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.31s
Val loss: 0.6715 score: 0.5349 time: 0.26s
Test loss: 0.6643 score: 0.5909 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6641;  Loss pred: 0.6641; Loss self: 0.0000; time: 0.33s
Val loss: 0.6696 score: 0.5349 time: 0.25s
Test loss: 0.6621 score: 0.5909 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6615;  Loss pred: 0.6615; Loss self: 0.0000; time: 0.33s
Val loss: 0.6676 score: 0.5349 time: 0.25s
Test loss: 0.6598 score: 0.5909 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.6588;  Loss pred: 0.6588; Loss self: 0.0000; time: 0.33s
Val loss: 0.6653 score: 0.5581 time: 0.24s
Test loss: 0.6572 score: 0.5909 time: 0.33s
Epoch 53/1000, LR 0.000269
Train loss: 0.6533;  Loss pred: 0.6533; Loss self: 0.0000; time: 0.45s
Val loss: 0.6628 score: 0.5581 time: 0.25s
Test loss: 0.6544 score: 0.5682 time: 0.26s
Epoch 54/1000, LR 0.000269
Train loss: 0.6539;  Loss pred: 0.6539; Loss self: 0.0000; time: 0.29s
Val loss: 0.6600 score: 0.5581 time: 0.27s
Test loss: 0.6514 score: 0.5682 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6465;  Loss pred: 0.6465; Loss self: 0.0000; time: 0.35s
Val loss: 0.6570 score: 0.5581 time: 0.25s
Test loss: 0.6481 score: 0.5682 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6421;  Loss pred: 0.6421; Loss self: 0.0000; time: 0.28s
Val loss: 0.6537 score: 0.5581 time: 0.27s
Test loss: 0.6445 score: 0.5909 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.6374;  Loss pred: 0.6374; Loss self: 0.0000; time: 0.34s
Val loss: 0.6501 score: 0.5581 time: 0.24s
Test loss: 0.6407 score: 0.5909 time: 0.26s
Epoch 58/1000, LR 0.000269
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.31s
Val loss: 0.6461 score: 0.5814 time: 0.25s
Test loss: 0.6364 score: 0.6136 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.6298;  Loss pred: 0.6298; Loss self: 0.0000; time: 0.34s
Val loss: 0.6417 score: 0.6279 time: 0.24s
Test loss: 0.6319 score: 0.6136 time: 0.26s
Epoch 60/1000, LR 0.000268
Train loss: 0.6227;  Loss pred: 0.6227; Loss self: 0.0000; time: 0.29s
Val loss: 0.6371 score: 0.6279 time: 0.27s
Test loss: 0.6270 score: 0.6136 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.6131;  Loss pred: 0.6131; Loss self: 0.0000; time: 0.36s
Val loss: 0.6321 score: 0.6512 time: 0.24s
Test loss: 0.6218 score: 0.6364 time: 0.26s
Epoch 62/1000, LR 0.000268
Train loss: 0.6081;  Loss pred: 0.6081; Loss self: 0.0000; time: 0.29s
Val loss: 0.6268 score: 0.6744 time: 0.27s
Test loss: 0.6163 score: 0.6364 time: 0.33s
Epoch 63/1000, LR 0.000268
Train loss: 0.5996;  Loss pred: 0.5996; Loss self: 0.0000; time: 0.34s
Val loss: 0.6210 score: 0.6744 time: 0.26s
Test loss: 0.6105 score: 0.6364 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5955;  Loss pred: 0.5955; Loss self: 0.0000; time: 0.35s
Val loss: 0.6150 score: 0.6977 time: 0.27s
Test loss: 0.6043 score: 0.6364 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5899;  Loss pred: 0.5899; Loss self: 0.0000; time: 0.32s
Val loss: 0.6085 score: 0.6977 time: 0.26s
Test loss: 0.5978 score: 0.6591 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.5732;  Loss pred: 0.5732; Loss self: 0.0000; time: 0.35s
Val loss: 0.6018 score: 0.7209 time: 0.26s
Test loss: 0.5911 score: 0.6591 time: 0.26s
Epoch 67/1000, LR 0.000268
Train loss: 0.5706;  Loss pred: 0.5706; Loss self: 0.0000; time: 0.27s
Val loss: 0.5947 score: 0.7674 time: 0.25s
Test loss: 0.5840 score: 0.6818 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.5574;  Loss pred: 0.5574; Loss self: 0.0000; time: 0.35s
Val loss: 0.5872 score: 0.7442 time: 0.25s
Test loss: 0.5766 score: 0.7045 time: 0.23s
Epoch 69/1000, LR 0.000268
Train loss: 0.5457;  Loss pred: 0.5457; Loss self: 0.0000; time: 0.33s
Val loss: 0.5793 score: 0.7209 time: 0.24s
Test loss: 0.5689 score: 0.7045 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.5323;  Loss pred: 0.5323; Loss self: 0.0000; time: 0.28s
Val loss: 0.5711 score: 0.7209 time: 0.25s
Test loss: 0.5610 score: 0.6818 time: 0.33s
Epoch 71/1000, LR 0.000268
Train loss: 0.5294;  Loss pred: 0.5294; Loss self: 0.0000; time: 0.37s
Val loss: 0.5625 score: 0.7442 time: 0.26s
Test loss: 0.5529 score: 0.7500 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.5173;  Loss pred: 0.5173; Loss self: 0.0000; time: 0.34s
Val loss: 0.5537 score: 0.7674 time: 0.25s
Test loss: 0.5445 score: 0.7727 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.5185;  Loss pred: 0.5185; Loss self: 0.0000; time: 0.31s
Val loss: 0.5447 score: 0.7907 time: 0.25s
Test loss: 0.5360 score: 0.7727 time: 0.24s
Epoch 74/1000, LR 0.000267
Train loss: 0.4890;  Loss pred: 0.4890; Loss self: 0.0000; time: 0.34s
Val loss: 0.5357 score: 0.7907 time: 0.25s
Test loss: 0.5274 score: 0.7727 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.4863;  Loss pred: 0.4863; Loss self: 0.0000; time: 0.33s
Val loss: 0.5266 score: 0.7907 time: 0.25s
Test loss: 0.5189 score: 0.7955 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.4711;  Loss pred: 0.4711; Loss self: 0.0000; time: 0.31s
Val loss: 0.5176 score: 0.7907 time: 0.26s
Test loss: 0.5105 score: 0.7955 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.4706;  Loss pred: 0.4706; Loss self: 0.0000; time: 0.33s
Val loss: 0.5088 score: 0.7907 time: 0.33s
Test loss: 0.5023 score: 0.8182 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.4493;  Loss pred: 0.4493; Loss self: 0.0000; time: 0.44s
Val loss: 0.5002 score: 0.7907 time: 0.23s
Test loss: 0.4943 score: 0.8409 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.4288;  Loss pred: 0.4288; Loss self: 0.0000; time: 0.32s
Val loss: 0.4918 score: 0.7907 time: 0.26s
Test loss: 0.4866 score: 0.8409 time: 0.23s
Epoch 80/1000, LR 0.000267
Train loss: 0.4251;  Loss pred: 0.4251; Loss self: 0.0000; time: 0.35s
Val loss: 0.4839 score: 0.8140 time: 0.25s
Test loss: 0.4793 score: 0.8409 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.4151;  Loss pred: 0.4151; Loss self: 0.0000; time: 0.29s
Val loss: 0.4762 score: 0.8140 time: 0.26s
Test loss: 0.4721 score: 0.8409 time: 0.22s
Epoch 82/1000, LR 0.000267
Train loss: 0.4041;  Loss pred: 0.4041; Loss self: 0.0000; time: 0.34s
Val loss: 0.4686 score: 0.8372 time: 0.25s
Test loss: 0.4649 score: 0.8409 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.3864;  Loss pred: 0.3864; Loss self: 0.0000; time: 0.30s
Val loss: 0.4615 score: 0.8372 time: 0.36s
Test loss: 0.4584 score: 0.8409 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.3831;  Loss pred: 0.3831; Loss self: 0.0000; time: 0.33s
Val loss: 0.4543 score: 0.8372 time: 0.24s
Test loss: 0.4520 score: 0.8409 time: 0.25s
Epoch 85/1000, LR 0.000266
Train loss: 0.3864;  Loss pred: 0.3864; Loss self: 0.0000; time: 0.29s
Val loss: 0.4471 score: 0.8372 time: 0.26s
Test loss: 0.4455 score: 0.8409 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.3639;  Loss pred: 0.3639; Loss self: 0.0000; time: 0.32s
Val loss: 0.4404 score: 0.8372 time: 0.24s
Test loss: 0.4394 score: 0.8409 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.3593;  Loss pred: 0.3593; Loss self: 0.0000; time: 0.27s
Val loss: 0.4343 score: 0.8372 time: 0.26s
Test loss: 0.4338 score: 0.8409 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.3250;  Loss pred: 0.3250; Loss self: 0.0000; time: 0.32s
Val loss: 0.4287 score: 0.8372 time: 0.23s
Test loss: 0.4287 score: 0.8409 time: 0.25s
Epoch 89/1000, LR 0.000266
Train loss: 0.3316;  Loss pred: 0.3316; Loss self: 0.0000; time: 0.27s
Val loss: 0.4238 score: 0.8372 time: 0.26s
Test loss: 0.4239 score: 0.8409 time: 0.24s
Epoch 90/1000, LR 0.000266
Train loss: 0.3256;  Loss pred: 0.3256; Loss self: 0.0000; time: 0.35s
Val loss: 0.4196 score: 0.8372 time: 0.24s
Test loss: 0.4196 score: 0.8409 time: 0.25s
Epoch 91/1000, LR 0.000266
Train loss: 0.2906;  Loss pred: 0.2906; Loss self: 0.0000; time: 0.28s
Val loss: 0.4165 score: 0.8372 time: 0.26s
Test loss: 0.4161 score: 0.8182 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.3047;  Loss pred: 0.3047; Loss self: 0.0000; time: 0.34s
Val loss: 0.4136 score: 0.8372 time: 0.24s
Test loss: 0.4127 score: 0.8182 time: 0.25s
Epoch 93/1000, LR 0.000265
Train loss: 0.2746;  Loss pred: 0.2746; Loss self: 0.0000; time: 0.29s
Val loss: 0.4115 score: 0.8605 time: 0.35s
Test loss: 0.4100 score: 0.8182 time: 0.24s
Epoch 94/1000, LR 0.000265
Train loss: 0.2937;  Loss pred: 0.2937; Loss self: 0.0000; time: 0.38s
Val loss: 0.4108 score: 0.8605 time: 0.30s
Test loss: 0.4086 score: 0.8182 time: 0.24s
Epoch 95/1000, LR 0.000265
Train loss: 0.2576;  Loss pred: 0.2576; Loss self: 0.0000; time: 0.39s
Val loss: 0.4113 score: 0.8605 time: 0.28s
Test loss: 0.4083 score: 0.8182 time: 0.34s
     INFO: Early stopping counter 1 of 2
Epoch 96/1000, LR 0.000265
Train loss: 0.2598;  Loss pred: 0.2598; Loss self: 0.0000; time: 0.33s
Val loss: 0.4110 score: 0.8605 time: 0.26s
Test loss: 0.4074 score: 0.8182 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 093,   Train_Loss: 0.2937,   Val_Loss: 0.4108,   Val_Precision: 0.8636,   Val_Recall: 0.8636,   Val_accuracy: 0.8636,   Val_Score: 0.8605,   Val_Loss: 0.4108,   Test_Precision: 0.8500,   Test_Recall: 0.7727,   Test_accuracy: 0.8095,   Test_Score: 0.8182,   Test_loss: 0.4086


[0.25985806703101844, 0.24895759299397469, 0.2591656920267269, 0.24886473501101136, 0.25864786200691015, 0.24501386901829392, 0.25953609799034894, 0.2296513250330463, 0.25985878508072346, 0.23440212104469538, 0.2496032090857625, 0.24609784805215895, 0.2612411450827494, 0.24714532808866352, 0.33994701702613384, 0.24574634293094277, 0.2509509079391137, 0.23499916004948318, 0.2497311090119183, 0.2856965189566836, 0.24046997795812786, 0.25820702698547393, 0.24146963900420815, 0.2683923071017489, 0.24177011707797647, 0.25006920006126165, 0.23421581799630076, 0.24860897497273982, 0.23664122400805354, 0.25655326002743095, 0.23835388699080795, 0.24702775501646101, 0.3230723360320553, 0.23650814802385867, 0.2640517979161814, 0.23359224398154765, 0.2455878050532192, 0.23371440800838172, 0.3355090649565682, 0.2505819300422445, 0.23156033200211823, 0.26704143988899887, 0.23675602197181433, 0.24816211592406034, 0.24782337294891477, 0.3353333770064637, 0.2336018669884652, 0.25138394604437053, 0.23510743293445557, 0.2544571169419214, 0.2334717740304768, 0.33138925407547504, 0.2632385359611362, 0.2530858899699524, 0.25768184300977737, 0.24121216405183077, 0.26465308805927634, 0.23705290502402931, 0.2658188220812008, 0.2330612090881914, 0.2645582129480317, 0.33119193697348237, 0.24482900509610772, 0.24503532599192113, 0.2494602509541437, 0.2605548599967733, 0.2549003940075636, 0.23333191499114037, 0.2547797750448808, 0.33913106797263026, 0.2427645130082965, 0.25005593698006123, 0.24841550597921014, 0.23369896598160267, 0.24915092694573104, 0.23911164107266814, 0.24565634701866657, 0.24901866202708334, 0.23756947403308004, 0.2569520119577646, 0.2291708419797942, 0.2561341340187937, 0.23975539696402848, 0.2541689029894769, 0.23457509302534163, 0.24977108906023204, 0.24798604694660753, 0.25891605392098427, 0.24512501491699368, 0.256134863011539, 0.24417099403217435, 0.25410399108659476, 0.2462450129678473, 0.2411395040107891, 0.3422756379004568, 0.2339929190929979]
[0.005905865159795873, 0.005658127113499425, 0.005890129364243793, 0.005656016704795713, 0.005878360500157049, 0.005568497023143043, 0.0058985476815988395, 0.005219348296205598, 0.005905881479107352, 0.005327320932833986, 0.005672800206494602, 0.00559313291027634, 0.0059372987518806685, 0.005616939274742353, 0.007726068568775769, 0.005585144157521427, 0.005703429725888947, 0.005340890001124618, 0.005675707022998144, 0.0064931027035609904, 0.005465226771775633, 0.005868341522397135, 0.005487946341004731, 0.006099825161403383, 0.0054947753881358285, 0.005683390910483219, 0.005323086772643199, 0.005650203976653178, 0.005378209636546671, 0.005830755909714339, 0.005417133795245635, 0.005614267159465023, 0.007342553091637621, 0.005375185182360424, 0.006001177225367759, 0.005308914635944265, 0.0055815410239368, 0.005311691091099585, 0.007625206021740186, 0.005695043864596466, 0.00526273481822996, 0.0060691236338408835, 0.005380818681177599, 0.005640048089183189, 0.0056323493852026086, 0.007621213113783266, 0.005309133340646936, 0.005713271501008421, 0.005343350748510354, 0.005783116294134577, 0.005306176682510836, 0.007531573956260796, 0.005982693999116732, 0.005751952044771646, 0.005856405522949485, 0.005482094637541609, 0.0060148429104380985, 0.0053875660232733935, 0.006041336865481836, 0.005296845661095259, 0.006012686657909812, 0.007527089476670054, 0.005564295570366085, 0.005568984681634571, 0.005669551158048721, 0.005921701363563029, 0.005793190772899173, 0.005302998067980463, 0.0057904494328382, 0.007707524272105234, 0.005517375295643102, 0.005683089476819573, 0.0056458069540729575, 0.005311340135945516, 0.005662521066948433, 0.005434355478924276, 0.0055830987958787855, 0.005659515046070076, 0.005399306228024547, 0.00583981845358556, 0.005208428226813505, 0.0058212303186089475, 0.005448986294637011, 0.005776565977033566, 0.00533125211421231, 0.005676615660459819, 0.005636046521513807, 0.00588445577093146, 0.005571023066295311, 0.005821246886625886, 0.005549340773458508, 0.005775090706513517, 0.005596477567451075, 0.0054804432729724795, 0.007778991770464927, 0.0053180208884772255]
[169.32320209534944, 176.73692724473318, 169.77555808375448, 176.80287244415388, 170.11545990983092, 179.58167093273707, 169.53325699470204, 191.59480135230442, 169.32273421632323, 187.71161201058484, 176.27978486799745, 178.79067349940607, 168.42676135898418, 178.0329020996706, 129.43193437881365, 179.04640807763494, 175.33309746253394, 187.23471177826775, 176.18950307828936, 154.0095768778728, 182.975024049204, 170.4058968250904, 182.21752507458382, 163.93912506336994, 181.99106048250363, 175.95129663797434, 187.86092406745536, 176.98476092757568, 185.9354818013558, 171.50434960481684, 184.59946491955824, 178.1176370123592, 136.1924098497691, 186.04010207530496, 166.6339723767646, 188.36241841777814, 179.16199051685462, 188.2639601680955, 131.14399757185643, 175.59127265314876, 190.01527428971514, 164.76843451072418, 185.84532563753828, 177.30345276981907, 177.54580399916503, 131.21270656917602, 188.3546590069532, 175.031065795402, 187.1484854852145, 172.91715212682686, 188.45961222814176, 132.77437170602656, 167.14877948757484, 173.85402246337753, 170.75320281037605, 182.41202790480105, 166.25538104488314, 185.6125745244078, 165.52627709169195, 188.7916061713651, 166.31500307511945, 132.85347584872804, 179.71726831438042, 179.56594553003632, 176.38080548587345, 168.87038683732436, 172.61644561716292, 188.57257483045422, 172.698166454732, 129.74334749994372, 181.2456007460048, 175.96062917517708, 177.12259879494943, 188.27640000539748, 176.59978447354476, 184.01446204213877, 179.11200151753698, 176.6935844961473, 185.20898014815353, 171.23819994541358, 191.9965019104806, 171.78499136226617, 183.5203735021719, 173.11323093612944, 187.5731964230601, 176.16130099584683, 177.42933742346153, 169.93924993707756, 179.5002440485303, 171.78450244009844, 180.2015844445558, 173.1574534183742, 178.6838217338646, 182.4669922105809, 128.5513636608762, 188.03987817474376]
Elapsed: 0.25505454570399405~0.026182411567202628
Time per graph: 0.005796694220545319~0.0005950548083455141
Speed: 174.01652102023834~14.753580026570404
Total Time: 0.2345
best val loss: 0.4108055830001831 test_score: 0.8182

Testing...
Test loss: 0.4100 score: 0.8182 time: 0.26s
test Score 0.8182
Epoch Time List: [0.8287606338271871, 0.7797295178752393, 0.8415411140304059, 0.7848680029856041, 0.8190869839163497, 0.7828194231260568, 0.8158358570653945, 0.8444983069784939, 0.832006320124492, 0.7809111628448591, 0.8033154952572659, 0.7811211838852614, 0.8168682709801942, 0.7885311690624803, 0.9001350739272311, 0.908893796033226, 0.8143452570075169, 0.8174770348705351, 0.8495812191395089, 0.9733270900323987, 0.8317684991052374, 0.9437226001173258, 0.9031989470822737, 0.8966858380008489, 0.8291906190570444, 0.8288307529874146, 0.8034000629559159, 0.8213034190703183, 0.7918130890466273, 0.8252260171575472, 0.778523197863251, 0.9436698950594291, 1.0426965518854558, 0.8012131149880588, 0.8504672070266679, 0.8165121519705281, 0.8168687430443242, 0.7846049630315974, 0.9106899669859558, 0.9056238549528643, 0.8098139410139993, 0.8698281919350848, 0.8105040369555354, 0.8245376029517502, 0.7838184660067782, 0.8948803229723126, 0.8171274490887299, 0.8229766939766705, 0.801829966949299, 0.8355589989805594, 0.8155895791715011, 0.8993711930233985, 0.9619848111178726, 0.8037525009131059, 0.8542700550751761, 0.7937045778380707, 0.8402147139422596, 0.798424387932755, 0.8435678930254653, 0.7820348719833419, 0.8568497040541843, 0.8839769391342998, 0.8364966559456661, 0.8610648959875107, 0.8263929898384959, 0.8627087119966745, 0.7683155231643468, 0.8233119450742379, 0.8166341320611537, 0.8677156920311972, 0.8635922829853371, 0.8352536540478468, 0.812151139951311, 0.816301562008448, 0.828947363072075, 0.8065826180391014, 0.9060432750266045, 0.9194098099833354, 0.8135657351231202, 0.8509885468520224, 0.7684649080038071, 0.8385721021331847, 0.8874087309231982, 0.8241106348577887, 0.7819792840164155, 0.8011053670197725, 0.7777428050758317, 0.8081645137863234, 0.7727063259808347, 0.8373134619323537, 0.7762239499716088, 0.8269910300150514, 0.8729120121570304, 0.9102414628723636, 1.0111943939700723, 0.8164572240784764]
Total Epoch List: [96]
Total Time List: [0.23446863901335746]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288337700>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6982 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5116 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6958 score: 0.5116 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.7004;  Loss pred: 0.7004; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6980 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6957 score: 0.5116 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.7003;  Loss pred: 0.7003; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6979 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.5116 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.7002;  Loss pred: 0.7002; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5116 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.7000;  Loss pred: 0.7000; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5116 time: 0.30s
Epoch 7/1000, LR 0.000150
Train loss: 0.6997;  Loss pred: 0.6997; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6994;  Loss pred: 0.6994; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6971 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5116 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6991;  Loss pred: 0.6991; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5116 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6987;  Loss pred: 0.6987; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.5116 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6983;  Loss pred: 0.6983; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5116 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6979;  Loss pred: 0.6979; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.30s
Epoch 13/1000, LR 0.000270
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6971;  Loss pred: 0.6971; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.26s
Epoch 16/1000, LR 0.000270
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 20/1000, LR 0.000270
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.29s
Epoch 21/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5116 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.51s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6914 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6912 score: 0.5116 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.5116 time: 0.26s
Epoch 32/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.5116 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.5116 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5116 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6886 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5116 time: 0.34s
Epoch 36/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5116 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.5116 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6864 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.5116 time: 0.25s
Epoch 39/1000, LR 0.000269
Train loss: 0.6817;  Loss pred: 0.6817; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6855 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.5116 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6845 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.5116 time: 0.24s
Epoch 41/1000, LR 0.000269
Train loss: 0.6774;  Loss pred: 0.6774; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6864 score: 0.5116 time: 0.31s
Epoch 42/1000, LR 0.000269
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6856 score: 0.5116 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6736;  Loss pred: 0.6736; Loss self: 0.0000; time: 0.38s
Val loss: 0.6806 score: 0.5227 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.5116 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6719;  Loss pred: 0.6719; Loss self: 0.0000; time: 0.37s
Val loss: 0.6791 score: 0.5227 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6836 score: 0.5116 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6689;  Loss pred: 0.6689; Loss self: 0.0000; time: 0.37s
Val loss: 0.6773 score: 0.5455 time: 0.22s
Test loss: 0.6825 score: 0.5349 time: 0.23s
Epoch 46/1000, LR 0.000269
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.34s
Val loss: 0.6754 score: 0.6136 time: 0.22s
Test loss: 0.6812 score: 0.5814 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.35s
Val loss: 0.6732 score: 0.6136 time: 0.23s
Test loss: 0.6798 score: 0.6512 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6580;  Loss pred: 0.6580; Loss self: 0.0000; time: 0.35s
Val loss: 0.6709 score: 0.6818 time: 0.22s
Test loss: 0.6783 score: 0.6512 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6538;  Loss pred: 0.6538; Loss self: 0.0000; time: 0.34s
Val loss: 0.6684 score: 0.7500 time: 0.23s
Test loss: 0.6767 score: 0.6744 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6500;  Loss pred: 0.6500; Loss self: 0.0000; time: 0.45s
Val loss: 0.6656 score: 0.7955 time: 0.22s
Test loss: 0.6749 score: 0.6977 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6440;  Loss pred: 0.6440; Loss self: 0.0000; time: 0.41s
Val loss: 0.6626 score: 0.8182 time: 0.21s
Test loss: 0.6729 score: 0.7442 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6386;  Loss pred: 0.6386; Loss self: 0.0000; time: 0.35s
Val loss: 0.6593 score: 0.8182 time: 0.23s
Test loss: 0.6708 score: 0.7674 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.6355;  Loss pred: 0.6355; Loss self: 0.0000; time: 0.40s
Val loss: 0.6558 score: 0.8182 time: 0.23s
Test loss: 0.6685 score: 0.7674 time: 0.25s
Epoch 54/1000, LR 0.000269
Train loss: 0.6282;  Loss pred: 0.6282; Loss self: 0.0000; time: 0.34s
Val loss: 0.6519 score: 0.7955 time: 0.24s
Test loss: 0.6661 score: 0.7442 time: 0.24s
Epoch 55/1000, LR 0.000269
Train loss: 0.6180;  Loss pred: 0.6180; Loss self: 0.0000; time: 0.37s
Val loss: 0.6477 score: 0.7955 time: 0.22s
Test loss: 0.6633 score: 0.7442 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6145;  Loss pred: 0.6145; Loss self: 0.0000; time: 0.33s
Val loss: 0.6433 score: 0.8182 time: 0.24s
Test loss: 0.6604 score: 0.7674 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.6062;  Loss pred: 0.6062; Loss self: 0.0000; time: 0.37s
Val loss: 0.6384 score: 0.8182 time: 0.21s
Test loss: 0.6572 score: 0.7674 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.6014;  Loss pred: 0.6014; Loss self: 0.0000; time: 0.43s
Val loss: 0.6333 score: 0.8182 time: 0.24s
Test loss: 0.6538 score: 0.7907 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.5868;  Loss pred: 0.5868; Loss self: 0.0000; time: 0.37s
Val loss: 0.6278 score: 0.8182 time: 0.22s
Test loss: 0.6501 score: 0.7907 time: 0.24s
Epoch 60/1000, LR 0.000268
Train loss: 0.5807;  Loss pred: 0.5807; Loss self: 0.0000; time: 0.33s
Val loss: 0.6219 score: 0.8182 time: 0.24s
Test loss: 0.6462 score: 0.7907 time: 0.22s
Epoch 61/1000, LR 0.000268
Train loss: 0.5700;  Loss pred: 0.5700; Loss self: 0.0000; time: 0.35s
Val loss: 0.6156 score: 0.8182 time: 0.22s
Test loss: 0.6420 score: 0.7907 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.5576;  Loss pred: 0.5576; Loss self: 0.0000; time: 0.36s
Val loss: 0.6090 score: 0.8182 time: 0.24s
Test loss: 0.6376 score: 0.7907 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.36s
Val loss: 0.6021 score: 0.8182 time: 0.21s
Test loss: 0.6329 score: 0.7907 time: 0.26s
Epoch 64/1000, LR 0.000268
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.44s
Val loss: 0.5948 score: 0.8182 time: 0.24s
Test loss: 0.6281 score: 0.7907 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.5269;  Loss pred: 0.5269; Loss self: 0.0000; time: 0.37s
Val loss: 0.5872 score: 0.8182 time: 0.37s
Test loss: 0.6230 score: 0.7907 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.37s
Val loss: 0.5792 score: 0.8182 time: 0.23s
Test loss: 0.6177 score: 0.7907 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.4987;  Loss pred: 0.4987; Loss self: 0.0000; time: 0.32s
Val loss: 0.5708 score: 0.8182 time: 0.26s
Test loss: 0.6120 score: 0.7907 time: 0.24s
Epoch 68/1000, LR 0.000268
Train loss: 0.4787;  Loss pred: 0.4787; Loss self: 0.0000; time: 0.36s
Val loss: 0.5621 score: 0.8182 time: 0.22s
Test loss: 0.6062 score: 0.7907 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4743;  Loss pred: 0.4743; Loss self: 0.0000; time: 0.31s
Val loss: 0.5531 score: 0.8182 time: 0.24s
Test loss: 0.6001 score: 0.7907 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.4603;  Loss pred: 0.4603; Loss self: 0.0000; time: 0.37s
Val loss: 0.5441 score: 0.8182 time: 0.21s
Test loss: 0.5941 score: 0.7907 time: 0.25s
Epoch 71/1000, LR 0.000268
Train loss: 0.4462;  Loss pred: 0.4462; Loss self: 0.0000; time: 0.32s
Val loss: 0.5350 score: 0.8636 time: 0.24s
Test loss: 0.5882 score: 0.7907 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.4302;  Loss pred: 0.4302; Loss self: 0.0000; time: 0.36s
Val loss: 0.5259 score: 0.8636 time: 0.30s
Test loss: 0.5823 score: 0.7907 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.4096;  Loss pred: 0.4096; Loss self: 0.0000; time: 0.34s
Val loss: 0.5168 score: 0.8636 time: 0.24s
Test loss: 0.5766 score: 0.7907 time: 0.22s
Epoch 74/1000, LR 0.000267
Train loss: 0.4034;  Loss pred: 0.4034; Loss self: 0.0000; time: 0.35s
Val loss: 0.5079 score: 0.8636 time: 0.21s
Test loss: 0.5712 score: 0.8140 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.3855;  Loss pred: 0.3855; Loss self: 0.0000; time: 0.31s
Val loss: 0.4991 score: 0.8636 time: 0.24s
Test loss: 0.5662 score: 0.8140 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.3679;  Loss pred: 0.3679; Loss self: 0.0000; time: 0.36s
Val loss: 0.4901 score: 0.8409 time: 0.21s
Test loss: 0.5611 score: 0.8140 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.3486;  Loss pred: 0.3486; Loss self: 0.0000; time: 0.30s
Val loss: 0.4811 score: 0.8182 time: 0.24s
Test loss: 0.5561 score: 0.8140 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.3410;  Loss pred: 0.3410; Loss self: 0.0000; time: 0.35s
Val loss: 0.4722 score: 0.8182 time: 0.21s
Test loss: 0.5513 score: 0.8140 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.3195;  Loss pred: 0.3195; Loss self: 0.0000; time: 0.32s
Val loss: 0.4636 score: 0.8182 time: 0.24s
Test loss: 0.5470 score: 0.8140 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.3103;  Loss pred: 0.3103; Loss self: 0.0000; time: 0.36s
Val loss: 0.4556 score: 0.8182 time: 0.21s
Test loss: 0.5435 score: 0.8372 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.2951;  Loss pred: 0.2951; Loss self: 0.0000; time: 0.41s
Val loss: 0.4479 score: 0.8182 time: 0.30s
Test loss: 0.5406 score: 0.8372 time: 0.31s
Epoch 82/1000, LR 0.000267
Train loss: 0.2854;  Loss pred: 0.2854; Loss self: 0.0000; time: 0.41s
Val loss: 0.4415 score: 0.8182 time: 0.24s
Test loss: 0.5394 score: 0.8372 time: 0.25s
Epoch 83/1000, LR 0.000266
Train loss: 0.2747;  Loss pred: 0.2747; Loss self: 0.0000; time: 0.36s
Val loss: 0.4362 score: 0.8182 time: 0.21s
Test loss: 0.5398 score: 0.8372 time: 0.26s
Epoch 84/1000, LR 0.000266
Train loss: 0.2557;  Loss pred: 0.2557; Loss self: 0.0000; time: 0.31s
Val loss: 0.4320 score: 0.8182 time: 0.23s
Test loss: 0.5415 score: 0.8140 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2571;  Loss pred: 0.2571; Loss self: 0.0000; time: 0.37s
Val loss: 0.4293 score: 0.8182 time: 0.21s
Test loss: 0.5456 score: 0.8140 time: 0.26s
Epoch 86/1000, LR 0.000266
Train loss: 0.2373;  Loss pred: 0.2373; Loss self: 0.0000; time: 0.32s
Val loss: 0.4271 score: 0.8182 time: 0.23s
Test loss: 0.5508 score: 0.7907 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.2258;  Loss pred: 0.2258; Loss self: 0.0000; time: 0.36s
Val loss: 0.4260 score: 0.8182 time: 0.22s
Test loss: 0.5576 score: 0.7907 time: 0.25s
Epoch 88/1000, LR 0.000266
Train loss: 0.2146;  Loss pred: 0.2146; Loss self: 0.0000; time: 0.32s
Val loss: 0.4252 score: 0.8182 time: 0.24s
Test loss: 0.5644 score: 0.7907 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.2140;  Loss pred: 0.2140; Loss self: 0.0000; time: 0.45s
Val loss: 0.4256 score: 0.8182 time: 0.31s
Test loss: 0.5733 score: 0.7907 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 90/1000, LR 0.000266
Train loss: 0.1916;  Loss pred: 0.1916; Loss self: 0.0000; time: 0.37s
Val loss: 0.4255 score: 0.8182 time: 0.24s
Test loss: 0.5802 score: 0.7907 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.2146,   Val_Loss: 0.4252,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4252,   Test_Precision: 0.9333,   Test_Recall: 0.6364,   Test_accuracy: 0.7568,   Test_Score: 0.7907,   Test_loss: 0.5644


[0.25985806703101844, 0.24895759299397469, 0.2591656920267269, 0.24886473501101136, 0.25864786200691015, 0.24501386901829392, 0.25953609799034894, 0.2296513250330463, 0.25985878508072346, 0.23440212104469538, 0.2496032090857625, 0.24609784805215895, 0.2612411450827494, 0.24714532808866352, 0.33994701702613384, 0.24574634293094277, 0.2509509079391137, 0.23499916004948318, 0.2497311090119183, 0.2856965189566836, 0.24046997795812786, 0.25820702698547393, 0.24146963900420815, 0.2683923071017489, 0.24177011707797647, 0.25006920006126165, 0.23421581799630076, 0.24860897497273982, 0.23664122400805354, 0.25655326002743095, 0.23835388699080795, 0.24702775501646101, 0.3230723360320553, 0.23650814802385867, 0.2640517979161814, 0.23359224398154765, 0.2455878050532192, 0.23371440800838172, 0.3355090649565682, 0.2505819300422445, 0.23156033200211823, 0.26704143988899887, 0.23675602197181433, 0.24816211592406034, 0.24782337294891477, 0.3353333770064637, 0.2336018669884652, 0.25138394604437053, 0.23510743293445557, 0.2544571169419214, 0.2334717740304768, 0.33138925407547504, 0.2632385359611362, 0.2530858899699524, 0.25768184300977737, 0.24121216405183077, 0.26465308805927634, 0.23705290502402931, 0.2658188220812008, 0.2330612090881914, 0.2645582129480317, 0.33119193697348237, 0.24482900509610772, 0.24503532599192113, 0.2494602509541437, 0.2605548599967733, 0.2549003940075636, 0.23333191499114037, 0.2547797750448808, 0.33913106797263026, 0.2427645130082965, 0.25005593698006123, 0.24841550597921014, 0.23369896598160267, 0.24915092694573104, 0.23911164107266814, 0.24565634701866657, 0.24901866202708334, 0.23756947403308004, 0.2569520119577646, 0.2291708419797942, 0.2561341340187937, 0.23975539696402848, 0.2541689029894769, 0.23457509302534163, 0.24977108906023204, 0.24798604694660753, 0.25891605392098427, 0.24512501491699368, 0.256134863011539, 0.24417099403217435, 0.25410399108659476, 0.2462450129678473, 0.2411395040107891, 0.3422756379004568, 0.2339929190929979, 0.25145989609882236, 0.23186418402474374, 0.2498678530100733, 0.233841649023816, 0.2563203910831362, 0.30254258296918124, 0.2356313419295475, 0.24249887699261308, 0.23091835097875446, 0.25601121003273875, 0.2331246470566839, 0.3067132029682398, 0.2517608009511605, 0.24393645906820893, 0.2639833290595561, 0.2316776120569557, 0.2567196899326518, 0.2316070629749447, 0.25627458200324327, 0.29404005501419306, 0.23834972397889942, 0.2518831220222637, 0.24798159394413233, 0.25242475199047476, 0.2471705200150609, 0.25216435303445905, 0.24819537007715553, 0.2395213160198182, 0.24441766098607332, 0.22940199507866055, 0.2605507739353925, 0.24604189093224704, 0.25910553405992687, 0.2495950929587707, 0.34447782998904586, 0.25737040501553565, 0.23910107393749058, 0.25453918194398284, 0.24525264592375606, 0.24748973303940147, 0.31281801394652575, 0.25678708706982434, 0.23939989192876965, 0.23509636893868446, 0.23921534302644432, 0.24285230308305472, 0.22982613497879356, 0.2529961139662191, 0.2257999200373888, 0.25529815908521414, 0.24371208401862532, 0.2409132180036977, 0.2523284249473363, 0.2438950870418921, 0.25435642106458545, 0.24764788197353482, 0.2499800130026415, 0.22997507196851075, 0.24403058900497854, 0.2246139149647206, 0.24666860804427415, 0.2426009209593758, 0.2663370770169422, 0.23179832904133946, 0.2301271960604936, 0.25088795297779143, 0.24306539807002991, 0.2541133959311992, 0.2427659350214526, 0.25491100701037794, 0.2434003329835832, 0.2516615920467302, 0.2270732120377943, 0.25886365899350494, 0.22802863200195134, 0.25260026892647147, 0.24243377009406686, 0.2543850539950654, 0.2426295259501785, 0.2527506910264492, 0.3180956250289455, 0.2507301039295271, 0.26110060804057866, 0.24224158807191998, 0.26269826001953334, 0.23013232403900474, 0.25011957495007664, 0.24888919410295784, 0.2513999039074406, 0.2548081249697134]
[0.005905865159795873, 0.005658127113499425, 0.005890129364243793, 0.005656016704795713, 0.005878360500157049, 0.005568497023143043, 0.0058985476815988395, 0.005219348296205598, 0.005905881479107352, 0.005327320932833986, 0.005672800206494602, 0.00559313291027634, 0.0059372987518806685, 0.005616939274742353, 0.007726068568775769, 0.005585144157521427, 0.005703429725888947, 0.005340890001124618, 0.005675707022998144, 0.0064931027035609904, 0.005465226771775633, 0.005868341522397135, 0.005487946341004731, 0.006099825161403383, 0.0054947753881358285, 0.005683390910483219, 0.005323086772643199, 0.005650203976653178, 0.005378209636546671, 0.005830755909714339, 0.005417133795245635, 0.005614267159465023, 0.007342553091637621, 0.005375185182360424, 0.006001177225367759, 0.005308914635944265, 0.0055815410239368, 0.005311691091099585, 0.007625206021740186, 0.005695043864596466, 0.00526273481822996, 0.0060691236338408835, 0.005380818681177599, 0.005640048089183189, 0.0056323493852026086, 0.007621213113783266, 0.005309133340646936, 0.005713271501008421, 0.005343350748510354, 0.005783116294134577, 0.005306176682510836, 0.007531573956260796, 0.005982693999116732, 0.005751952044771646, 0.005856405522949485, 0.005482094637541609, 0.0060148429104380985, 0.0053875660232733935, 0.006041336865481836, 0.005296845661095259, 0.006012686657909812, 0.007527089476670054, 0.005564295570366085, 0.005568984681634571, 0.005669551158048721, 0.005921701363563029, 0.005793190772899173, 0.005302998067980463, 0.0057904494328382, 0.007707524272105234, 0.005517375295643102, 0.005683089476819573, 0.0056458069540729575, 0.005311340135945516, 0.005662521066948433, 0.005434355478924276, 0.0055830987958787855, 0.005659515046070076, 0.005399306228024547, 0.00583981845358556, 0.005208428226813505, 0.0058212303186089475, 0.005448986294637011, 0.005776565977033566, 0.00533125211421231, 0.005676615660459819, 0.005636046521513807, 0.00588445577093146, 0.005571023066295311, 0.005821246886625886, 0.005549340773458508, 0.005775090706513517, 0.005596477567451075, 0.0054804432729724795, 0.007778991770464927, 0.0053180208884772255, 0.005847904560437729, 0.005392190326156831, 0.005810880302559844, 0.00543817788427479, 0.005960939327514795, 0.007035874022539098, 0.00547979864952436, 0.005639508767270072, 0.005370194208808243, 0.005953749070528808, 0.0054215034199228814, 0.007132865185307902, 0.005854902347701407, 0.005672940908562999, 0.0061391471874315375, 0.005387851443185016, 0.005970225347270972, 0.005386210766859179, 0.0059598740000754245, 0.006838140814283559, 0.0055430168367185916, 0.005857747023773574, 0.00576701381265424, 0.005870343069545925, 0.00574815162825723, 0.005864287279871141, 0.005771985350631524, 0.005570263163251586, 0.005684131650838914, 0.005334930118108385, 0.006059320324078896, 0.0057219044402848146, 0.006025710094416904, 0.0058045370455528066, 0.008011112325326648, 0.005985358256175247, 0.005560490091569548, 0.005919515859162392, 0.0057035499052036295, 0.005755575186962825, 0.007274837533640134, 0.005971792722554054, 0.005567439347180689, 0.005467357417178708, 0.005563147512242891, 0.005647727978675691, 0.005344793836716129, 0.005883630557353933, 0.005251160931102065, 0.005937166490353817, 0.005667722884154077, 0.005602632976830179, 0.005868102905752007, 0.005671978768416096, 0.00591526560615315, 0.005759253069151973, 0.005813488674480035, 0.005348257487639785, 0.005675129976859966, 0.0052235794177842, 0.005736479256843585, 0.005641881882776182, 0.0061938855120219115, 0.005390658814914871, 0.0053517952572207815, 0.005834603557623056, 0.005652683676047207, 0.005909613858865097, 0.005645719419103549, 0.00592816295372972, 0.005660472860083331, 0.005852595163877447, 0.00528077237297196, 0.006020085092872208, 0.005302991441905845, 0.005874424858755151, 0.005637994653350392, 0.005915931488257335, 0.00564254711512043, 0.0058779230471267254, 0.007397572675091756, 0.0058309326495238865, 0.006072107163734387, 0.005633525303998139, 0.00610926186091938, 0.005351914512534994, 0.005816734301164573, 0.005788120793092043, 0.005846509393196293, 0.005925770348132869]
[169.32320209534944, 176.73692724473318, 169.77555808375448, 176.80287244415388, 170.11545990983092, 179.58167093273707, 169.53325699470204, 191.59480135230442, 169.32273421632323, 187.71161201058484, 176.27978486799745, 178.79067349940607, 168.42676135898418, 178.0329020996706, 129.43193437881365, 179.04640807763494, 175.33309746253394, 187.23471177826775, 176.18950307828936, 154.0095768778728, 182.975024049204, 170.4058968250904, 182.21752507458382, 163.93912506336994, 181.99106048250363, 175.95129663797434, 187.86092406745536, 176.98476092757568, 185.9354818013558, 171.50434960481684, 184.59946491955824, 178.1176370123592, 136.1924098497691, 186.04010207530496, 166.6339723767646, 188.36241841777814, 179.16199051685462, 188.2639601680955, 131.14399757185643, 175.59127265314876, 190.01527428971514, 164.76843451072418, 185.84532563753828, 177.30345276981907, 177.54580399916503, 131.21270656917602, 188.3546590069532, 175.031065795402, 187.1484854852145, 172.91715212682686, 188.45961222814176, 132.77437170602656, 167.14877948757484, 173.85402246337753, 170.75320281037605, 182.41202790480105, 166.25538104488314, 185.6125745244078, 165.52627709169195, 188.7916061713651, 166.31500307511945, 132.85347584872804, 179.71726831438042, 179.56594553003632, 176.38080548587345, 168.87038683732436, 172.61644561716292, 188.57257483045422, 172.698166454732, 129.74334749994372, 181.2456007460048, 175.96062917517708, 177.12259879494943, 188.27640000539748, 176.59978447354476, 184.01446204213877, 179.11200151753698, 176.6935844961473, 185.20898014815353, 171.23819994541358, 191.9965019104806, 171.78499136226617, 183.5203735021719, 173.11323093612944, 187.5731964230601, 176.16130099584683, 177.42933742346153, 169.93924993707756, 179.5002440485303, 171.78450244009844, 180.2015844445558, 173.1574534183742, 178.6838217338646, 182.4669922105809, 128.5513636608762, 188.03987817474376, 171.00142276007796, 185.45339454157005, 172.09096521218547, 183.88512131823273, 167.75879522614014, 142.12875284528207, 182.48845695941745, 177.32040879228424, 186.21300480339994, 167.96139510649223, 184.45068139682638, 140.19611671054363, 170.797041626594, 176.27541272121377, 162.88907391685692, 185.60274175058774, 167.49786512783916, 185.65927760437836, 167.78878210971314, 146.23857963135134, 180.40717346837246, 170.71409808950705, 173.39996616719648, 170.3477953763528, 173.96896683868235, 170.52370599790493, 173.25061296120373, 179.52473172133935, 175.9283671504004, 187.44387983746856, 165.03501160454238, 174.76698718691355, 165.95554454678225, 172.27902796591817, 124.82661076147446, 167.07437670389643, 179.84026291425906, 168.93273433032064, 175.3294030245358, 173.74458112633792, 137.4601144528415, 167.45390311744003, 179.6157870146161, 182.90371813958063, 179.7543562882859, 177.06235211322718, 187.09795560878086, 169.96308491023504, 190.43407983882324, 168.43051338120827, 176.43770177893103, 178.48750830824073, 170.4128260974742, 176.30531439370156, 169.05411634598195, 173.6336271375632, 172.01375215363967, 186.97678679664793, 176.20741799349895, 191.43960874709776, 174.32295232428604, 177.24582342867723, 161.44954537165205, 185.50608271352672, 186.85318700314144, 171.3912505149514, 176.90712187512273, 169.21579376965306, 177.12534502091572, 168.68632117658765, 176.66368600613316, 170.86437247053365, 189.36623837796876, 166.11060883242362, 188.5728104514175, 170.2294308028497, 177.36802914592116, 169.03508804740596, 177.22492689875514, 170.1281204232207, 135.1794762851176, 171.49915118324222, 164.68747553937985, 177.50874382161652, 163.68589573757615, 186.8490234023448, 171.91777176409607, 172.76764527676607, 171.04222925968804, 168.75443043706318]
Elapsed: 0.2527415303279325~0.023431547609567716
Time per graph: 0.005808132282548525~0.0005342204941635529
Speed: 173.39720563364045~13.423907245202185
Total Time: 0.2556
best val loss: 0.4252234399318695 test_score: 0.7907

Testing...
Test loss: 0.5882 score: 0.7907 time: 0.23s
test Score 0.7907
Epoch Time List: [0.8287606338271871, 0.7797295178752393, 0.8415411140304059, 0.7848680029856041, 0.8190869839163497, 0.7828194231260568, 0.8158358570653945, 0.8444983069784939, 0.832006320124492, 0.7809111628448591, 0.8033154952572659, 0.7811211838852614, 0.8168682709801942, 0.7885311690624803, 0.9001350739272311, 0.908893796033226, 0.8143452570075169, 0.8174770348705351, 0.8495812191395089, 0.9733270900323987, 0.8317684991052374, 0.9437226001173258, 0.9031989470822737, 0.8966858380008489, 0.8291906190570444, 0.8288307529874146, 0.8034000629559159, 0.8213034190703183, 0.7918130890466273, 0.8252260171575472, 0.778523197863251, 0.9436698950594291, 1.0426965518854558, 0.8012131149880588, 0.8504672070266679, 0.8165121519705281, 0.8168687430443242, 0.7846049630315974, 0.9106899669859558, 0.9056238549528643, 0.8098139410139993, 0.8698281919350848, 0.8105040369555354, 0.8245376029517502, 0.7838184660067782, 0.8948803229723126, 0.8171274490887299, 0.8229766939766705, 0.801829966949299, 0.8355589989805594, 0.8155895791715011, 0.8993711930233985, 0.9619848111178726, 0.8037525009131059, 0.8542700550751761, 0.7937045778380707, 0.8402147139422596, 0.798424387932755, 0.8435678930254653, 0.7820348719833419, 0.8568497040541843, 0.8839769391342998, 0.8364966559456661, 0.8610648959875107, 0.8263929898384959, 0.8627087119966745, 0.7683155231643468, 0.8233119450742379, 0.8166341320611537, 0.8677156920311972, 0.8635922829853371, 0.8352536540478468, 0.812151139951311, 0.816301562008448, 0.828947363072075, 0.8065826180391014, 0.9060432750266045, 0.9194098099833354, 0.8135657351231202, 0.8509885468520224, 0.7684649080038071, 0.8385721021331847, 0.8874087309231982, 0.8241106348577887, 0.7819792840164155, 0.8011053670197725, 0.7777428050758317, 0.8081645137863234, 0.7727063259808347, 0.8373134619323537, 0.7762239499716088, 0.8269910300150514, 0.8729120121570304, 0.9102414628723636, 1.0111943939700723, 0.8164572240784764, 0.8354457509703934, 0.7994388011284173, 0.8160916249034926, 0.8742738700238988, 0.8903559548780322, 0.9438148101326078, 0.8006275099469349, 0.9200878539122641, 0.794234658125788, 0.8675068410811946, 0.8952910641673952, 0.9073101619724184, 0.9701953999465331, 0.9172384069534019, 0.8707653909223154, 0.8077637739479542, 0.8432233439525589, 0.8044577459804714, 0.8462570449337363, 0.9543485860340297, 0.7972380919381976, 0.9044145749649033, 0.792119700810872, 0.8408049719873816, 0.7975754669168964, 0.8387983881402761, 0.7873333861352876, 0.9923242530785501, 0.8385293289320543, 0.8023434149799868, 0.8569386809831485, 0.801907611079514, 0.8266944921342656, 0.7964941839454696, 1.0367686122190207, 0.8443629461107776, 0.7934448780724779, 0.8208803021116182, 0.8088263790123165, 0.828119719051756, 0.8664947099750862, 0.8717458150349557, 0.8387147549074143, 0.8282061161007732, 0.8164384710835293, 0.8040979189099744, 0.8079267659923062, 0.8214548949617893, 0.7938018519198522, 0.9283575940644369, 0.8595146499574184, 0.816525190952234, 0.8759590451372787, 0.8110116070602089, 0.8321073480183259, 0.8043214969802648, 0.8323198959697038, 0.8908493018243462, 0.8294511139392853, 0.7942762209568173, 0.8031887399265543, 0.8327844389714301, 0.8382741222158074, 0.9044979449827224, 0.9688818259164691, 0.8402091930620372, 0.820466848090291, 0.8302697080653161, 0.7959638200700283, 0.8342849489999935, 0.7921453492017463, 0.9044560610782355, 0.7997523549711332, 0.8201283300295472, 0.7748863600427285, 0.8194109119940549, 0.7797090378589928, 0.8153693650383502, 0.793485120870173, 0.8207585780182853, 1.021148114115931, 0.8949670908041298, 0.8342337540816516, 0.7856435659341514, 0.8430973008507863, 0.7748321660328656, 0.8234377308981493, 0.80068671295885, 1.0117430458776653, 0.8507136601256207]
Total Epoch List: [96, 90]
Total Time List: [0.23446863901335746, 0.25562496192287654]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334d30>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.4884 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.22s
Epoch 4/1000, LR 0.000060
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.4884 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.57s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.4884 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.4884 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.4884 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.4884 time: 0.21s
Epoch 11/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.22s
Epoch 12/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6931 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.4884 time: 0.32s
Epoch 14/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.4884 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6916;  Loss pred: 0.6916; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.4884 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6899;  Loss pred: 0.6899; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6902 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6898 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.21s
Epoch 22/1000, LR 0.000270
Train loss: 0.6890;  Loss pred: 0.6890; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6893 score: 0.5000 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.21s
Epoch 23/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6887 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6887 score: 0.4884 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6862;  Loss pred: 0.6862; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6881 score: 0.4884 time: 0.21s
Epoch 27/1000, LR 0.000270
Train loss: 0.6855;  Loss pred: 0.6855; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6859 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.4884 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6845;  Loss pred: 0.6845; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6851 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6866 score: 0.4884 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.4884 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6831 score: 0.5000 time: 0.31s
Test loss: 0.6847 score: 0.5116 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.54s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6820 score: 0.5000 time: 0.23s
Test loss: 0.6837 score: 0.5116 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6808 score: 0.5000 time: 0.24s
Test loss: 0.6825 score: 0.5349 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6776;  Loss pred: 0.6776; Loss self: 0.0000; time: 0.40s
Val loss: 0.6794 score: 0.5455 time: 0.22s
Test loss: 0.6812 score: 0.5349 time: 0.23s
Epoch 34/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.37s
Val loss: 0.6779 score: 0.5455 time: 0.24s
Test loss: 0.6797 score: 0.5349 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6738;  Loss pred: 0.6738; Loss self: 0.0000; time: 0.38s
Val loss: 0.6762 score: 0.5455 time: 0.22s
Test loss: 0.6781 score: 0.5581 time: 0.23s
Epoch 36/1000, LR 0.000270
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.59s
Val loss: 0.6744 score: 0.5682 time: 0.21s
Test loss: 0.6762 score: 0.5581 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.39s
Val loss: 0.6723 score: 0.5455 time: 0.32s
Test loss: 0.6742 score: 0.5581 time: 0.21s
Epoch 38/1000, LR 0.000270
Train loss: 0.6658;  Loss pred: 0.6658; Loss self: 0.0000; time: 0.39s
Val loss: 0.6701 score: 0.5455 time: 0.36s
Test loss: 0.6719 score: 0.5581 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6630;  Loss pred: 0.6630; Loss self: 0.0000; time: 0.49s
Val loss: 0.6677 score: 0.5455 time: 0.22s
Test loss: 0.6694 score: 0.5581 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6599;  Loss pred: 0.6599; Loss self: 0.0000; time: 0.38s
Val loss: 0.6650 score: 0.5455 time: 0.23s
Test loss: 0.6667 score: 0.5581 time: 0.21s
Epoch 41/1000, LR 0.000269
Train loss: 0.6561;  Loss pred: 0.6561; Loss self: 0.0000; time: 0.56s
Val loss: 0.6621 score: 0.5455 time: 0.23s
Test loss: 0.6637 score: 0.5814 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6527;  Loss pred: 0.6527; Loss self: 0.0000; time: 0.36s
Val loss: 0.6589 score: 0.5909 time: 0.22s
Test loss: 0.6604 score: 0.5349 time: 0.21s
Epoch 43/1000, LR 0.000269
Train loss: 0.6471;  Loss pred: 0.6471; Loss self: 0.0000; time: 0.40s
Val loss: 0.6554 score: 0.6136 time: 0.22s
Test loss: 0.6568 score: 0.5581 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6416;  Loss pred: 0.6416; Loss self: 0.0000; time: 0.44s
Val loss: 0.6517 score: 0.6136 time: 0.24s
Test loss: 0.6528 score: 0.5581 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6352;  Loss pred: 0.6352; Loss self: 0.0000; time: 0.41s
Val loss: 0.6476 score: 0.6591 time: 0.23s
Test loss: 0.6485 score: 0.5581 time: 0.21s
Epoch 46/1000, LR 0.000269
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 0.42s
Val loss: 0.6432 score: 0.7045 time: 0.20s
Test loss: 0.6439 score: 0.6744 time: 0.20s
Epoch 47/1000, LR 0.000269
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.42s
Val loss: 0.6386 score: 0.7273 time: 0.24s
Test loss: 0.6390 score: 0.7209 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6180;  Loss pred: 0.6180; Loss self: 0.0000; time: 0.42s
Val loss: 0.6336 score: 0.7273 time: 0.22s
Test loss: 0.6338 score: 0.7209 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6091;  Loss pred: 0.6091; Loss self: 0.0000; time: 0.40s
Val loss: 0.6284 score: 0.7273 time: 0.26s
Test loss: 0.6285 score: 0.7442 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6042;  Loss pred: 0.6042; Loss self: 0.0000; time: 0.41s
Val loss: 0.6228 score: 0.7727 time: 0.21s
Test loss: 0.6227 score: 0.7674 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.5959;  Loss pred: 0.5959; Loss self: 0.0000; time: 0.36s
Val loss: 0.6170 score: 0.7955 time: 0.23s
Test loss: 0.6167 score: 0.7907 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.5857;  Loss pred: 0.5857; Loss self: 0.0000; time: 0.38s
Val loss: 0.6107 score: 0.8409 time: 0.21s
Test loss: 0.6104 score: 0.7907 time: 0.31s
Epoch 53/1000, LR 0.000269
Train loss: 0.5689;  Loss pred: 0.5689; Loss self: 0.0000; time: 0.39s
Val loss: 0.6041 score: 0.8636 time: 0.23s
Test loss: 0.6040 score: 0.7674 time: 0.21s
Epoch 54/1000, LR 0.000269
Train loss: 0.5713;  Loss pred: 0.5713; Loss self: 0.0000; time: 0.38s
Val loss: 0.5970 score: 0.8636 time: 0.34s
Test loss: 0.5972 score: 0.7674 time: 0.22s
Epoch 55/1000, LR 0.000269
Train loss: 0.5494;  Loss pred: 0.5494; Loss self: 0.0000; time: 0.52s
Val loss: 0.5895 score: 0.8636 time: 0.22s
Test loss: 0.5902 score: 0.7674 time: 0.21s
Epoch 56/1000, LR 0.000269
Train loss: 0.5450;  Loss pred: 0.5450; Loss self: 0.0000; time: 0.39s
Val loss: 0.5817 score: 0.8636 time: 0.24s
Test loss: 0.5829 score: 0.7674 time: 0.20s
Epoch 57/1000, LR 0.000269
Train loss: 0.5341;  Loss pred: 0.5341; Loss self: 0.0000; time: 0.47s
Val loss: 0.5734 score: 0.8636 time: 0.29s
Test loss: 0.5753 score: 0.7674 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.5240;  Loss pred: 0.5240; Loss self: 0.0000; time: 0.36s
Val loss: 0.5648 score: 0.8636 time: 0.23s
Test loss: 0.5673 score: 0.7674 time: 0.21s
Epoch 59/1000, LR 0.000268
Train loss: 0.5096;  Loss pred: 0.5096; Loss self: 0.0000; time: 0.39s
Val loss: 0.5557 score: 0.8636 time: 0.24s
Test loss: 0.5588 score: 0.7674 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.4978;  Loss pred: 0.4978; Loss self: 0.0000; time: 0.37s
Val loss: 0.5464 score: 0.8636 time: 0.24s
Test loss: 0.5501 score: 0.7674 time: 0.21s
Epoch 61/1000, LR 0.000268
Train loss: 0.4805;  Loss pred: 0.4805; Loss self: 0.0000; time: 0.49s
Val loss: 0.5366 score: 0.8636 time: 0.23s
Test loss: 0.5412 score: 0.7907 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.4672;  Loss pred: 0.4672; Loss self: 0.0000; time: 0.45s
Val loss: 0.5266 score: 0.8636 time: 0.24s
Test loss: 0.5318 score: 0.7907 time: 0.22s
Epoch 63/1000, LR 0.000268
Train loss: 0.4593;  Loss pred: 0.4593; Loss self: 0.0000; time: 0.37s
Val loss: 0.5163 score: 0.8864 time: 0.24s
Test loss: 0.5218 score: 0.7907 time: 0.22s
Epoch 64/1000, LR 0.000268
Train loss: 0.4453;  Loss pred: 0.4453; Loss self: 0.0000; time: 0.38s
Val loss: 0.5059 score: 0.8864 time: 0.21s
Test loss: 0.5117 score: 0.7907 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.4174;  Loss pred: 0.4174; Loss self: 0.0000; time: 0.35s
Val loss: 0.4952 score: 0.8864 time: 0.24s
Test loss: 0.5016 score: 0.7907 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.4075;  Loss pred: 0.4075; Loss self: 0.0000; time: 0.39s
Val loss: 0.4845 score: 0.8864 time: 0.21s
Test loss: 0.4914 score: 0.7907 time: 0.23s
Epoch 67/1000, LR 0.000268
Train loss: 0.4046;  Loss pred: 0.4046; Loss self: 0.0000; time: 0.34s
Val loss: 0.4738 score: 0.8864 time: 0.23s
Test loss: 0.4812 score: 0.7907 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.3764;  Loss pred: 0.3764; Loss self: 0.0000; time: 0.39s
Val loss: 0.4629 score: 0.8864 time: 0.21s
Test loss: 0.4721 score: 0.7907 time: 0.23s
Epoch 69/1000, LR 0.000268
Train loss: 0.3718;  Loss pred: 0.3718; Loss self: 0.0000; time: 0.42s
Val loss: 0.4521 score: 0.8864 time: 0.25s
Test loss: 0.4627 score: 0.7907 time: 0.21s
Epoch 70/1000, LR 0.000268
Train loss: 0.3746;  Loss pred: 0.3746; Loss self: 0.0000; time: 0.41s
Val loss: 0.4416 score: 0.8864 time: 0.23s
Test loss: 0.4530 score: 0.7907 time: 0.22s
Epoch 71/1000, LR 0.000268
Train loss: 0.3337;  Loss pred: 0.3337; Loss self: 0.0000; time: 0.39s
Val loss: 0.4311 score: 0.8864 time: 0.23s
Test loss: 0.4440 score: 0.7907 time: 0.21s
Epoch 72/1000, LR 0.000267
Train loss: 0.3156;  Loss pred: 0.3156; Loss self: 0.0000; time: 0.39s
Val loss: 0.4206 score: 0.8864 time: 0.23s
Test loss: 0.4356 score: 0.8140 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.2951;  Loss pred: 0.2951; Loss self: 0.0000; time: 0.37s
Val loss: 0.4105 score: 0.8864 time: 0.23s
Test loss: 0.4268 score: 0.8140 time: 0.21s
Epoch 74/1000, LR 0.000267
Train loss: 0.2771;  Loss pred: 0.2771; Loss self: 0.0000; time: 0.39s
Val loss: 0.4005 score: 0.8864 time: 0.26s
Test loss: 0.4187 score: 0.8372 time: 0.30s
Epoch 75/1000, LR 0.000267
Train loss: 0.2915;  Loss pred: 0.2915; Loss self: 0.0000; time: 0.44s
Val loss: 0.3911 score: 0.8864 time: 0.23s
Test loss: 0.4106 score: 0.8372 time: 0.23s
Epoch 76/1000, LR 0.000267
Train loss: 0.2519;  Loss pred: 0.2519; Loss self: 0.0000; time: 0.44s
Val loss: 0.3825 score: 0.8864 time: 0.24s
Test loss: 0.4023 score: 0.8372 time: 0.21s
Epoch 77/1000, LR 0.000267
Train loss: 0.2317;  Loss pred: 0.2317; Loss self: 0.0000; time: 0.39s
Val loss: 0.3741 score: 0.9091 time: 0.28s
Test loss: 0.3952 score: 0.8605 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.2415;  Loss pred: 0.2415; Loss self: 0.0000; time: 0.35s
Val loss: 0.3651 score: 0.9091 time: 0.25s
Test loss: 0.3904 score: 0.8372 time: 0.21s
Epoch 79/1000, LR 0.000267
Train loss: 0.2275;  Loss pred: 0.2275; Loss self: 0.0000; time: 0.41s
Val loss: 0.3567 score: 0.8864 time: 0.23s
Test loss: 0.3864 score: 0.8372 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.2195;  Loss pred: 0.2195; Loss self: 0.0000; time: 0.47s
Val loss: 0.3492 score: 0.8864 time: 0.20s
Test loss: 0.3825 score: 0.8372 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.2095;  Loss pred: 0.2095; Loss self: 0.0000; time: 0.39s
Val loss: 0.3435 score: 0.9091 time: 0.23s
Test loss: 0.3778 score: 0.8605 time: 0.21s
Epoch 82/1000, LR 0.000267
Train loss: 0.2216;  Loss pred: 0.2216; Loss self: 0.0000; time: 0.44s
Val loss: 0.3384 score: 0.9091 time: 0.21s
Test loss: 0.3741 score: 0.8605 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.1580;  Loss pred: 0.1580; Loss self: 0.0000; time: 0.43s
Val loss: 0.3322 score: 0.9091 time: 0.23s
Test loss: 0.3731 score: 0.8605 time: 0.21s
Epoch 84/1000, LR 0.000266
Train loss: 0.1528;  Loss pred: 0.1528; Loss self: 0.0000; time: 0.39s
Val loss: 0.3253 score: 0.9091 time: 0.24s
Test loss: 0.3742 score: 0.8605 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.1382;  Loss pred: 0.1382; Loss self: 0.0000; time: 0.36s
Val loss: 0.3188 score: 0.8864 time: 0.23s
Test loss: 0.3760 score: 0.8605 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.1412;  Loss pred: 0.1412; Loss self: 0.0000; time: 0.38s
Val loss: 0.3130 score: 0.8864 time: 0.22s
Test loss: 0.3786 score: 0.8372 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.1293;  Loss pred: 0.1293; Loss self: 0.0000; time: 0.36s
Val loss: 0.3080 score: 0.8864 time: 0.23s
Test loss: 0.3816 score: 0.8140 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.1306;  Loss pred: 0.1306; Loss self: 0.0000; time: 0.37s
Val loss: 0.3059 score: 0.8864 time: 0.21s
Test loss: 0.3818 score: 0.8372 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.0965;  Loss pred: 0.0965; Loss self: 0.0000; time: 0.34s
Val loss: 0.3047 score: 0.9091 time: 0.23s
Test loss: 0.3824 score: 0.8605 time: 0.20s
Epoch 90/1000, LR 0.000266
Train loss: 0.1117;  Loss pred: 0.1117; Loss self: 0.0000; time: 0.38s
Val loss: 0.3028 score: 0.9091 time: 0.22s
Test loss: 0.3849 score: 0.8372 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.0912;  Loss pred: 0.0912; Loss self: 0.0000; time: 0.34s
Val loss: 0.3007 score: 0.9091 time: 0.32s
Test loss: 0.3885 score: 0.8372 time: 0.21s
Epoch 92/1000, LR 0.000266
Train loss: 0.0905;  Loss pred: 0.0905; Loss self: 0.0000; time: 0.38s
Val loss: 0.2980 score: 0.9091 time: 0.30s
Test loss: 0.3934 score: 0.8372 time: 0.22s
Epoch 93/1000, LR 0.000265
Train loss: 0.0746;  Loss pred: 0.0746; Loss self: 0.0000; time: 0.45s
Val loss: 0.2953 score: 0.9091 time: 0.23s
Test loss: 0.3992 score: 0.8372 time: 0.26s
Epoch 94/1000, LR 0.000265
Train loss: 0.0675;  Loss pred: 0.0675; Loss self: 0.0000; time: 0.39s
Val loss: 0.2926 score: 0.9091 time: 0.24s
Test loss: 0.4058 score: 0.8372 time: 0.21s
Epoch 95/1000, LR 0.000265
Train loss: 0.0759;  Loss pred: 0.0759; Loss self: 0.0000; time: 0.41s
Val loss: 0.2885 score: 0.9091 time: 0.21s
Test loss: 0.4152 score: 0.8140 time: 0.23s
Epoch 96/1000, LR 0.000265
Train loss: 0.0740;  Loss pred: 0.0740; Loss self: 0.0000; time: 0.34s
Val loss: 0.2859 score: 0.9091 time: 0.24s
Test loss: 0.4239 score: 0.8140 time: 0.22s
Epoch 97/1000, LR 0.000265
Train loss: 0.0690;  Loss pred: 0.0690; Loss self: 0.0000; time: 0.40s
Val loss: 0.2846 score: 0.9091 time: 0.22s
Test loss: 0.4312 score: 0.8140 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.0671;  Loss pred: 0.0671; Loss self: 0.0000; time: 0.34s
Val loss: 0.2840 score: 0.9091 time: 0.24s
Test loss: 0.4374 score: 0.8140 time: 0.21s
Epoch 99/1000, LR 0.000265
Train loss: 0.0532;  Loss pred: 0.0532; Loss self: 0.0000; time: 0.37s
Val loss: 0.2836 score: 0.9091 time: 0.31s
Test loss: 0.4441 score: 0.8140 time: 0.23s
Epoch 100/1000, LR 0.000265
Train loss: 0.0695;  Loss pred: 0.0695; Loss self: 0.0000; time: 0.36s
Val loss: 0.2819 score: 0.9091 time: 0.30s
Test loss: 0.4551 score: 0.8140 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.0632;  Loss pred: 0.0632; Loss self: 0.0000; time: 0.41s
Val loss: 0.2819 score: 0.9091 time: 0.24s
Test loss: 0.4621 score: 0.8140 time: 0.20s
     INFO: Early stopping counter 1 of 2
Epoch 102/1000, LR 0.000264
Train loss: 0.0538;  Loss pred: 0.0538; Loss self: 0.0000; time: 0.40s
Val loss: 0.2815 score: 0.9091 time: 0.22s
Test loss: 0.4714 score: 0.8140 time: 0.22s
Epoch 103/1000, LR 0.000264
Train loss: 0.0473;  Loss pred: 0.0473; Loss self: 0.0000; time: 0.37s
Val loss: 0.2816 score: 0.9091 time: 0.24s
Test loss: 0.4800 score: 0.8140 time: 0.20s
     INFO: Early stopping counter 1 of 2
Epoch 104/1000, LR 0.000264
Train loss: 0.0450;  Loss pred: 0.0450; Loss self: 0.0000; time: 0.40s
Val loss: 0.2820 score: 0.9091 time: 0.28s
Test loss: 0.4886 score: 0.8140 time: 0.22s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 101,   Train_Loss: 0.0538,   Val_Loss: 0.2815,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2815,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4714


[0.25985806703101844, 0.24895759299397469, 0.2591656920267269, 0.24886473501101136, 0.25864786200691015, 0.24501386901829392, 0.25953609799034894, 0.2296513250330463, 0.25985878508072346, 0.23440212104469538, 0.2496032090857625, 0.24609784805215895, 0.2612411450827494, 0.24714532808866352, 0.33994701702613384, 0.24574634293094277, 0.2509509079391137, 0.23499916004948318, 0.2497311090119183, 0.2856965189566836, 0.24046997795812786, 0.25820702698547393, 0.24146963900420815, 0.2683923071017489, 0.24177011707797647, 0.25006920006126165, 0.23421581799630076, 0.24860897497273982, 0.23664122400805354, 0.25655326002743095, 0.23835388699080795, 0.24702775501646101, 0.3230723360320553, 0.23650814802385867, 0.2640517979161814, 0.23359224398154765, 0.2455878050532192, 0.23371440800838172, 0.3355090649565682, 0.2505819300422445, 0.23156033200211823, 0.26704143988899887, 0.23675602197181433, 0.24816211592406034, 0.24782337294891477, 0.3353333770064637, 0.2336018669884652, 0.25138394604437053, 0.23510743293445557, 0.2544571169419214, 0.2334717740304768, 0.33138925407547504, 0.2632385359611362, 0.2530858899699524, 0.25768184300977737, 0.24121216405183077, 0.26465308805927634, 0.23705290502402931, 0.2658188220812008, 0.2330612090881914, 0.2645582129480317, 0.33119193697348237, 0.24482900509610772, 0.24503532599192113, 0.2494602509541437, 0.2605548599967733, 0.2549003940075636, 0.23333191499114037, 0.2547797750448808, 0.33913106797263026, 0.2427645130082965, 0.25005593698006123, 0.24841550597921014, 0.23369896598160267, 0.24915092694573104, 0.23911164107266814, 0.24565634701866657, 0.24901866202708334, 0.23756947403308004, 0.2569520119577646, 0.2291708419797942, 0.2561341340187937, 0.23975539696402848, 0.2541689029894769, 0.23457509302534163, 0.24977108906023204, 0.24798604694660753, 0.25891605392098427, 0.24512501491699368, 0.256134863011539, 0.24417099403217435, 0.25410399108659476, 0.2462450129678473, 0.2411395040107891, 0.3422756379004568, 0.2339929190929979, 0.25145989609882236, 0.23186418402474374, 0.2498678530100733, 0.233841649023816, 0.2563203910831362, 0.30254258296918124, 0.2356313419295475, 0.24249887699261308, 0.23091835097875446, 0.25601121003273875, 0.2331246470566839, 0.3067132029682398, 0.2517608009511605, 0.24393645906820893, 0.2639833290595561, 0.2316776120569557, 0.2567196899326518, 0.2316070629749447, 0.25627458200324327, 0.29404005501419306, 0.23834972397889942, 0.2518831220222637, 0.24798159394413233, 0.25242475199047476, 0.2471705200150609, 0.25216435303445905, 0.24819537007715553, 0.2395213160198182, 0.24441766098607332, 0.22940199507866055, 0.2605507739353925, 0.24604189093224704, 0.25910553405992687, 0.2495950929587707, 0.34447782998904586, 0.25737040501553565, 0.23910107393749058, 0.25453918194398284, 0.24525264592375606, 0.24748973303940147, 0.31281801394652575, 0.25678708706982434, 0.23939989192876965, 0.23509636893868446, 0.23921534302644432, 0.24285230308305472, 0.22982613497879356, 0.2529961139662191, 0.2257999200373888, 0.25529815908521414, 0.24371208401862532, 0.2409132180036977, 0.2523284249473363, 0.2438950870418921, 0.25435642106458545, 0.24764788197353482, 0.2499800130026415, 0.22997507196851075, 0.24403058900497854, 0.2246139149647206, 0.24666860804427415, 0.2426009209593758, 0.2663370770169422, 0.23179832904133946, 0.2301271960604936, 0.25088795297779143, 0.24306539807002991, 0.2541133959311992, 0.2427659350214526, 0.25491100701037794, 0.2434003329835832, 0.2516615920467302, 0.2270732120377943, 0.25886365899350494, 0.22802863200195134, 0.25260026892647147, 0.24243377009406686, 0.2543850539950654, 0.2426295259501785, 0.2527506910264492, 0.3180956250289455, 0.2507301039295271, 0.26110060804057866, 0.24224158807191998, 0.26269826001953334, 0.23013232403900474, 0.25011957495007664, 0.24888919410295784, 0.2513999039074406, 0.2548081249697134, 0.21738335909321904, 0.22978684504050761, 0.22625362302642316, 0.23747245804406703, 0.23090535390656441, 0.22504068701528013, 0.23415833595208824, 0.22214213898405433, 0.23463022499345243, 0.21022720402106643, 0.22579027200117707, 0.2387711990159005, 0.32336775003932416, 0.23637301695998758, 0.21125241601839662, 0.23464393592439592, 0.21570318203885108, 0.2354843720095232, 0.2072234220104292, 0.24627808202058077, 0.21435763000044972, 0.21264812699519098, 0.23386663699056953, 0.21265741903334856, 0.2310383280273527, 0.21197860897518694, 0.22574395698029548, 0.24159963591955602, 0.22849247697740793, 0.22353037900757045, 0.22995516401715577, 0.22609122993890196, 0.23044599406421185, 0.22931129205971956, 0.2351700379513204, 0.22627354005817324, 0.21685252210590988, 0.2313379340339452, 0.2571883429773152, 0.20965109101962298, 0.23874724190682173, 0.2163634350290522, 0.23360490007326007, 0.2310013819951564, 0.21319756493903697, 0.20474267995450646, 0.2536259340122342, 0.2219342510215938, 0.23975467402487993, 0.2333154969383031, 0.22578899597283453, 0.31043005594983697, 0.21173973404802382, 0.22448796697426587, 0.21220693201757967, 0.20645417901687324, 0.23321651900187135, 0.21192340401466936, 0.235656835953705, 0.21002202294766903, 0.22943037399090827, 0.2276096340501681, 0.22458181390538812, 0.2333877149503678, 0.2231842209585011, 0.2318362760124728, 0.22191529406700283, 0.23404802498407662, 0.21039456501603127, 0.22179503506049514, 0.2170630469918251, 0.22328844701405615, 0.21261985891032964, 0.3077550559537485, 0.2322758639929816, 0.21148264594376087, 0.2303194550331682, 0.21059324603993446, 0.21750262600835413, 0.22099232103209943, 0.2130121870432049, 0.23002049699425697, 0.21110851306002587, 0.23441704106517136, 0.21012865600641817, 0.22817655000835657, 0.21534314495511353, 0.23122085200157017, 0.20626364601776004, 0.2360239049885422, 0.21053841500543058, 0.2278772839345038, 0.2675000010058284, 0.21887832297943532, 0.22942424099892378, 0.22150032396893948, 0.23831022798549384, 0.2107953450176865, 0.2312260220060125, 0.23228193097747862, 0.20868907100521028, 0.22309682192280889, 0.20765634905546904, 0.2289576450129971]
[0.005905865159795873, 0.005658127113499425, 0.005890129364243793, 0.005656016704795713, 0.005878360500157049, 0.005568497023143043, 0.0058985476815988395, 0.005219348296205598, 0.005905881479107352, 0.005327320932833986, 0.005672800206494602, 0.00559313291027634, 0.0059372987518806685, 0.005616939274742353, 0.007726068568775769, 0.005585144157521427, 0.005703429725888947, 0.005340890001124618, 0.005675707022998144, 0.0064931027035609904, 0.005465226771775633, 0.005868341522397135, 0.005487946341004731, 0.006099825161403383, 0.0054947753881358285, 0.005683390910483219, 0.005323086772643199, 0.005650203976653178, 0.005378209636546671, 0.005830755909714339, 0.005417133795245635, 0.005614267159465023, 0.007342553091637621, 0.005375185182360424, 0.006001177225367759, 0.005308914635944265, 0.0055815410239368, 0.005311691091099585, 0.007625206021740186, 0.005695043864596466, 0.00526273481822996, 0.0060691236338408835, 0.005380818681177599, 0.005640048089183189, 0.0056323493852026086, 0.007621213113783266, 0.005309133340646936, 0.005713271501008421, 0.005343350748510354, 0.005783116294134577, 0.005306176682510836, 0.007531573956260796, 0.005982693999116732, 0.005751952044771646, 0.005856405522949485, 0.005482094637541609, 0.0060148429104380985, 0.0053875660232733935, 0.006041336865481836, 0.005296845661095259, 0.006012686657909812, 0.007527089476670054, 0.005564295570366085, 0.005568984681634571, 0.005669551158048721, 0.005921701363563029, 0.005793190772899173, 0.005302998067980463, 0.0057904494328382, 0.007707524272105234, 0.005517375295643102, 0.005683089476819573, 0.0056458069540729575, 0.005311340135945516, 0.005662521066948433, 0.005434355478924276, 0.0055830987958787855, 0.005659515046070076, 0.005399306228024547, 0.00583981845358556, 0.005208428226813505, 0.0058212303186089475, 0.005448986294637011, 0.005776565977033566, 0.00533125211421231, 0.005676615660459819, 0.005636046521513807, 0.00588445577093146, 0.005571023066295311, 0.005821246886625886, 0.005549340773458508, 0.005775090706513517, 0.005596477567451075, 0.0054804432729724795, 0.007778991770464927, 0.0053180208884772255, 0.005847904560437729, 0.005392190326156831, 0.005810880302559844, 0.00543817788427479, 0.005960939327514795, 0.007035874022539098, 0.00547979864952436, 0.005639508767270072, 0.005370194208808243, 0.005953749070528808, 0.0054215034199228814, 0.007132865185307902, 0.005854902347701407, 0.005672940908562999, 0.0061391471874315375, 0.005387851443185016, 0.005970225347270972, 0.005386210766859179, 0.0059598740000754245, 0.006838140814283559, 0.0055430168367185916, 0.005857747023773574, 0.00576701381265424, 0.005870343069545925, 0.00574815162825723, 0.005864287279871141, 0.005771985350631524, 0.005570263163251586, 0.005684131650838914, 0.005334930118108385, 0.006059320324078896, 0.0057219044402848146, 0.006025710094416904, 0.0058045370455528066, 0.008011112325326648, 0.005985358256175247, 0.005560490091569548, 0.005919515859162392, 0.0057035499052036295, 0.005755575186962825, 0.007274837533640134, 0.005971792722554054, 0.005567439347180689, 0.005467357417178708, 0.005563147512242891, 0.005647727978675691, 0.005344793836716129, 0.005883630557353933, 0.005251160931102065, 0.005937166490353817, 0.005667722884154077, 0.005602632976830179, 0.005868102905752007, 0.005671978768416096, 0.00591526560615315, 0.005759253069151973, 0.005813488674480035, 0.005348257487639785, 0.005675129976859966, 0.0052235794177842, 0.005736479256843585, 0.005641881882776182, 0.0061938855120219115, 0.005390658814914871, 0.0053517952572207815, 0.005834603557623056, 0.005652683676047207, 0.005909613858865097, 0.005645719419103549, 0.00592816295372972, 0.005660472860083331, 0.005852595163877447, 0.00528077237297196, 0.006020085092872208, 0.005302991441905845, 0.005874424858755151, 0.005637994653350392, 0.005915931488257335, 0.00564254711512043, 0.0058779230471267254, 0.007397572675091756, 0.0058309326495238865, 0.006072107163734387, 0.005633525303998139, 0.00610926186091938, 0.005351914512534994, 0.005816734301164573, 0.005788120793092043, 0.005846509393196293, 0.005925770348132869, 0.005055426955656256, 0.0053438801172211075, 0.0052617121634051895, 0.005522615303350396, 0.005369891951315452, 0.005233504349192561, 0.005445542696560192, 0.0051660962554431235, 0.005456516860312847, 0.004889004744675963, 0.005250936558166909, 0.0055528185817651276, 0.007520180233472655, 0.005497046906046223, 0.004912846884148759, 0.005456835719171998, 0.0050163530706709555, 0.005476380744407517, 0.004819149349079749, 0.005727397256292576, 0.004985061162801156, 0.004945305278957929, 0.005438758999780687, 0.0049455213728685716, 0.005372984372729132, 0.004929735092446208, 0.005249859464658034, 0.0056185961841757216, 0.005313778534358324, 0.005198380907152801, 0.005347794512026879, 0.005257935579974464, 0.005359209164283996, 0.005332820745574874, 0.005469070650030708, 0.005262175350190076, 0.005043081909439764, 0.005379951954277796, 0.0059811242552864, 0.004875606767898209, 0.005552261439693528, 0.005031707791373307, 0.005432672094726978, 0.005372125162678056, 0.004958082905558999, 0.004761457673360615, 0.005898277535168237, 0.005161261651664972, 0.005575690093601859, 0.0054259417892628635, 0.005250906883089175, 0.007219303626740395, 0.004924179861581949, 0.005220650394750369, 0.004935044930641388, 0.004801259977136587, 0.0054236399767877055, 0.004928451256155101, 0.005480391533807093, 0.004884233091806256, 0.00533559009281182, 0.005293247303492282, 0.0052228328815206535, 0.0054276212779155305, 0.005190330719965142, 0.005391541302615647, 0.00516082079225588, 0.0054429773252110845, 0.004892896860837936, 0.005158024071174306, 0.005047977837019189, 0.005192754581722236, 0.004944647881635573, 0.00715709432450578, 0.005401764278906549, 0.004918201068459555, 0.005356266396120191, 0.004897517349765917, 0.005058200604845445, 0.00513935630307208, 0.00495377179170244, 0.005349313883587371, 0.004909500303721532, 0.005451559094538869, 0.004886712930381818, 0.005306431395543176, 0.0050079801152351985, 0.0053772291163155855, 0.0047968289771572106, 0.005488928022989353, 0.004896242209428618, 0.005299471719407065, 0.006220930255949497, 0.005090193557661287, 0.00533544746509125, 0.005151170324859058, 0.005542098325244043, 0.004902217325992709, 0.005377349348977035, 0.00540190537156927, 0.004853234209423495, 0.005188298184251369, 0.004829217419894629, 0.005324596395651095]
[169.32320209534944, 176.73692724473318, 169.77555808375448, 176.80287244415388, 170.11545990983092, 179.58167093273707, 169.53325699470204, 191.59480135230442, 169.32273421632323, 187.71161201058484, 176.27978486799745, 178.79067349940607, 168.42676135898418, 178.0329020996706, 129.43193437881365, 179.04640807763494, 175.33309746253394, 187.23471177826775, 176.18950307828936, 154.0095768778728, 182.975024049204, 170.4058968250904, 182.21752507458382, 163.93912506336994, 181.99106048250363, 175.95129663797434, 187.86092406745536, 176.98476092757568, 185.9354818013558, 171.50434960481684, 184.59946491955824, 178.1176370123592, 136.1924098497691, 186.04010207530496, 166.6339723767646, 188.36241841777814, 179.16199051685462, 188.2639601680955, 131.14399757185643, 175.59127265314876, 190.01527428971514, 164.76843451072418, 185.84532563753828, 177.30345276981907, 177.54580399916503, 131.21270656917602, 188.3546590069532, 175.031065795402, 187.1484854852145, 172.91715212682686, 188.45961222814176, 132.77437170602656, 167.14877948757484, 173.85402246337753, 170.75320281037605, 182.41202790480105, 166.25538104488314, 185.6125745244078, 165.52627709169195, 188.7916061713651, 166.31500307511945, 132.85347584872804, 179.71726831438042, 179.56594553003632, 176.38080548587345, 168.87038683732436, 172.61644561716292, 188.57257483045422, 172.698166454732, 129.74334749994372, 181.2456007460048, 175.96062917517708, 177.12259879494943, 188.27640000539748, 176.59978447354476, 184.01446204213877, 179.11200151753698, 176.6935844961473, 185.20898014815353, 171.23819994541358, 191.9965019104806, 171.78499136226617, 183.5203735021719, 173.11323093612944, 187.5731964230601, 176.16130099584683, 177.42933742346153, 169.93924993707756, 179.5002440485303, 171.78450244009844, 180.2015844445558, 173.1574534183742, 178.6838217338646, 182.4669922105809, 128.5513636608762, 188.03987817474376, 171.00142276007796, 185.45339454157005, 172.09096521218547, 183.88512131823273, 167.75879522614014, 142.12875284528207, 182.48845695941745, 177.32040879228424, 186.21300480339994, 167.96139510649223, 184.45068139682638, 140.19611671054363, 170.797041626594, 176.27541272121377, 162.88907391685692, 185.60274175058774, 167.49786512783916, 185.65927760437836, 167.78878210971314, 146.23857963135134, 180.40717346837246, 170.71409808950705, 173.39996616719648, 170.3477953763528, 173.96896683868235, 170.52370599790493, 173.25061296120373, 179.52473172133935, 175.9283671504004, 187.44387983746856, 165.03501160454238, 174.76698718691355, 165.95554454678225, 172.27902796591817, 124.82661076147446, 167.07437670389643, 179.84026291425906, 168.93273433032064, 175.3294030245358, 173.74458112633792, 137.4601144528415, 167.45390311744003, 179.6157870146161, 182.90371813958063, 179.7543562882859, 177.06235211322718, 187.09795560878086, 169.96308491023504, 190.43407983882324, 168.43051338120827, 176.43770177893103, 178.48750830824073, 170.4128260974742, 176.30531439370156, 169.05411634598195, 173.6336271375632, 172.01375215363967, 186.97678679664793, 176.20741799349895, 191.43960874709776, 174.32295232428604, 177.24582342867723, 161.44954537165205, 185.50608271352672, 186.85318700314144, 171.3912505149514, 176.90712187512273, 169.21579376965306, 177.12534502091572, 168.68632117658765, 176.66368600613316, 170.86437247053365, 189.36623837796876, 166.11060883242362, 188.5728104514175, 170.2294308028497, 177.36802914592116, 169.03508804740596, 177.22492689875514, 170.1281204232207, 135.1794762851176, 171.49915118324222, 164.68747553937985, 177.50874382161652, 163.68589573757615, 186.8490234023448, 171.91777176409607, 172.76764527676607, 171.04222925968804, 168.75443043706318, 197.80722949248658, 187.12994641803718, 190.05220524127571, 181.07362998710622, 186.22348625749015, 191.0765585117518, 183.63642628891225, 193.5697576185066, 183.26709613477954, 204.54060738823821, 190.44221710214262, 180.08872166000432, 132.9755363507055, 181.9158572032733, 203.54796792598768, 183.25638730273826, 199.34800958223747, 182.60235120087307, 207.50550098451652, 174.5993782605738, 200.59934418900687, 202.211985629069, 183.86547373037197, 202.20315000276014, 186.11630532103408, 202.85065652559945, 190.48128940059885, 177.98040065887125, 188.1900033157398, 192.3675886513112, 186.99297397292628, 190.1887128112849, 186.59469510248206, 187.51802239551947, 182.84642199573437, 190.03547648101065, 198.29144518318756, 185.8752658943104, 167.1926476224186, 205.10267698046596, 180.1067926756701, 198.73968073314316, 184.07148131959096, 186.14607249796288, 201.69085895655368, 210.019718456135, 169.5410217707697, 193.75107628527414, 179.3499967201381, 184.29980247463254, 190.44329337100856, 138.51751522071837, 203.07950320862946, 191.54701510094435, 202.63240032346252, 208.27866117684962, 184.37801997917208, 202.90349808189913, 182.4687148411319, 204.74043339118901, 187.42069435716468, 188.91994699364196, 191.46697255778267, 184.24277391440407, 192.6659501972383, 185.47571907032616, 193.76762733179183, 183.72297738007185, 204.37790299727354, 193.87268965814155, 198.0991264792272, 192.57601803864543, 202.23886997373484, 139.7215063347727, 185.12470155443822, 203.32637606319196, 186.69721146139213, 204.18508574508596, 197.6987624891867, 194.57689660517295, 201.86638425189437, 186.93986215095256, 203.68671720866854, 183.43376319661579, 204.63653466991482, 188.4505660131385, 199.6813040366945, 185.96938653140157, 208.47105551647985, 182.18493589489353, 204.2382621665071, 188.69805387165755, 160.7476629469736, 196.45618357574887, 187.42570450609756, 194.1306415697604, 180.43707298462007, 203.98932432019382, 185.9652284243418, 185.1198662722033, 206.04816434745848, 192.74142782221222, 207.0728884312315, 187.80766196978942]
Elapsed: 0.24366806753437387~0.025036392995600752
Time per graph: 0.005622073531659669~0.0005602504322421593
Speed: 179.40388163137752~15.59953786569896
Total Time: 0.2298
best val loss: 0.281544953584671 test_score: 0.8140

Testing...
Test loss: 0.3952 score: 0.8605 time: 0.21s
test Score 0.8605
Epoch Time List: [0.8287606338271871, 0.7797295178752393, 0.8415411140304059, 0.7848680029856041, 0.8190869839163497, 0.7828194231260568, 0.8158358570653945, 0.8444983069784939, 0.832006320124492, 0.7809111628448591, 0.8033154952572659, 0.7811211838852614, 0.8168682709801942, 0.7885311690624803, 0.9001350739272311, 0.908893796033226, 0.8143452570075169, 0.8174770348705351, 0.8495812191395089, 0.9733270900323987, 0.8317684991052374, 0.9437226001173258, 0.9031989470822737, 0.8966858380008489, 0.8291906190570444, 0.8288307529874146, 0.8034000629559159, 0.8213034190703183, 0.7918130890466273, 0.8252260171575472, 0.778523197863251, 0.9436698950594291, 1.0426965518854558, 0.8012131149880588, 0.8504672070266679, 0.8165121519705281, 0.8168687430443242, 0.7846049630315974, 0.9106899669859558, 0.9056238549528643, 0.8098139410139993, 0.8698281919350848, 0.8105040369555354, 0.8245376029517502, 0.7838184660067782, 0.8948803229723126, 0.8171274490887299, 0.8229766939766705, 0.801829966949299, 0.8355589989805594, 0.8155895791715011, 0.8993711930233985, 0.9619848111178726, 0.8037525009131059, 0.8542700550751761, 0.7937045778380707, 0.8402147139422596, 0.798424387932755, 0.8435678930254653, 0.7820348719833419, 0.8568497040541843, 0.8839769391342998, 0.8364966559456661, 0.8610648959875107, 0.8263929898384959, 0.8627087119966745, 0.7683155231643468, 0.8233119450742379, 0.8166341320611537, 0.8677156920311972, 0.8635922829853371, 0.8352536540478468, 0.812151139951311, 0.816301562008448, 0.828947363072075, 0.8065826180391014, 0.9060432750266045, 0.9194098099833354, 0.8135657351231202, 0.8509885468520224, 0.7684649080038071, 0.8385721021331847, 0.8874087309231982, 0.8241106348577887, 0.7819792840164155, 0.8011053670197725, 0.7777428050758317, 0.8081645137863234, 0.7727063259808347, 0.8373134619323537, 0.7762239499716088, 0.8269910300150514, 0.8729120121570304, 0.9102414628723636, 1.0111943939700723, 0.8164572240784764, 0.8354457509703934, 0.7994388011284173, 0.8160916249034926, 0.8742738700238988, 0.8903559548780322, 0.9438148101326078, 0.8006275099469349, 0.9200878539122641, 0.794234658125788, 0.8675068410811946, 0.8952910641673952, 0.9073101619724184, 0.9701953999465331, 0.9172384069534019, 0.8707653909223154, 0.8077637739479542, 0.8432233439525589, 0.8044577459804714, 0.8462570449337363, 0.9543485860340297, 0.7972380919381976, 0.9044145749649033, 0.792119700810872, 0.8408049719873816, 0.7975754669168964, 0.8387983881402761, 0.7873333861352876, 0.9923242530785501, 0.8385293289320543, 0.8023434149799868, 0.8569386809831485, 0.801907611079514, 0.8266944921342656, 0.7964941839454696, 1.0367686122190207, 0.8443629461107776, 0.7934448780724779, 0.8208803021116182, 0.8088263790123165, 0.828119719051756, 0.8664947099750862, 0.8717458150349557, 0.8387147549074143, 0.8282061161007732, 0.8164384710835293, 0.8040979189099744, 0.8079267659923062, 0.8214548949617893, 0.7938018519198522, 0.9283575940644369, 0.8595146499574184, 0.816525190952234, 0.8759590451372787, 0.8110116070602089, 0.8321073480183259, 0.8043214969802648, 0.8323198959697038, 0.8908493018243462, 0.8294511139392853, 0.7942762209568173, 0.8031887399265543, 0.8327844389714301, 0.8382741222158074, 0.9044979449827224, 0.9688818259164691, 0.8402091930620372, 0.820466848090291, 0.8302697080653161, 0.7959638200700283, 0.8342849489999935, 0.7921453492017463, 0.9044560610782355, 0.7997523549711332, 0.8201283300295472, 0.7748863600427285, 0.8194109119940549, 0.7797090378589928, 0.8153693650383502, 0.793485120870173, 0.8207585780182853, 1.021148114115931, 0.8949670908041298, 0.8342337540816516, 0.7856435659341514, 0.8430973008507863, 0.7748321660328656, 0.8234377308981493, 0.80068671295885, 1.0117430458776653, 0.8507136601256207, 0.8325272498186678, 0.8950104140676558, 0.8444890090031549, 0.8554005129262805, 1.0327227560337633, 0.8368291520746425, 0.8502876161364838, 0.8299234800506383, 0.8706316731404513, 0.8010755010182038, 0.8671302780276164, 0.8975822961656377, 0.9668283319333568, 0.8305338399950415, 0.7936634529614821, 0.8392398320138454, 0.804906957084313, 0.8332149951020256, 0.8136995569802821, 0.9070423520170152, 0.8185151590732858, 1.0157095800386742, 0.8399965189164504, 0.8099459400400519, 0.9099184509832412, 0.8322817590087652, 0.8786121711600572, 0.9730249689891934, 0.8388113230466843, 0.997388155083172, 0.9950099958805367, 0.8232380480039865, 0.8481121830409393, 0.8295521329855546, 0.8343840471934527, 1.0175487078959122, 0.9208273600088432, 0.9715607500402257, 0.9646324351197109, 0.8215651450445876, 1.0202889099018648, 0.7929328529862687, 0.8485738540766761, 0.9117780029773712, 0.8468430039938539, 0.8208473970880732, 0.9088805570499972, 0.8525676869321615, 0.896770520019345, 0.8510174680268392, 0.8080219371477142, 0.8977942039491609, 0.8192607810487971, 0.9449749219929799, 0.9415436889976263, 0.8323309239931405, 0.9816316500073299, 0.7923246190184727, 0.8594079129397869, 0.8208937949966639, 0.9420292399590835, 0.9088678020052612, 0.828424509963952, 0.8243198129348457, 0.8087122640572488, 0.8313978330697864, 0.7934156720293686, 0.832315473118797, 0.8793347870232537, 0.8620646459748968, 0.8315452869283035, 0.8398722830461338, 0.8089535980252549, 0.9546290410216898, 0.9011748511111364, 0.891778243938461, 0.8968511869898066, 0.803637552075088, 0.8538985550403595, 0.8903460210422054, 0.8328110309084877, 0.8818720921408385, 0.8703054809011519, 0.8582056240411475, 0.7955660088919103, 0.8242620340315625, 0.8074364918284118, 0.8101195669732988, 0.7737570960307494, 0.8288792859530076, 0.8627435290254653, 0.9047003160230815, 0.9422469701385126, 0.8484485780354589, 0.8462798699038103, 0.7960617978824303, 0.8444157550111413, 0.78893460216932, 0.906218794058077, 0.8902447861619294, 0.8503112919861451, 0.8378324740333483, 0.808547162800096, 0.9056195409502834]
Total Epoch List: [96, 90, 104]
Total Time List: [0.23446863901335746, 0.25562496192287654, 0.2297845979919657]
========================training times:7========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288337640>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6944 score: 0.5000 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6955 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5000 time: 0.27s
Epoch 5/1000, LR 0.000090
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5000 time: 0.24s
Epoch 6/1000, LR 0.000120
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5000 time: 0.25s
Epoch 7/1000, LR 0.000150
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.25s
Epoch 9/1000, LR 0.000210
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.33s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5000 time: 0.25s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5000 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5000 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5000 time: 0.23s
Epoch 18/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5000 time: 0.24s
Epoch 19/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5000 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5000 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.25s
Test loss: 0.6910 score: 0.5455 time: 0.25s
Epoch 22/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.4884 time: 0.26s
Test loss: 0.6907 score: 0.5682 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.4884 time: 0.25s
Test loss: 0.6904 score: 0.6364 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.28s
Val loss: 0.6913 score: 0.5814 time: 0.26s
Test loss: 0.6901 score: 0.6591 time: 0.24s
Epoch 25/1000, LR 0.000270
Train loss: 0.6901;  Loss pred: 0.6901; Loss self: 0.0000; time: 0.33s
Val loss: 0.6910 score: 0.6512 time: 0.25s
Test loss: 0.6897 score: 0.6591 time: 0.26s
Epoch 26/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.38s
Val loss: 0.6907 score: 0.6744 time: 0.34s
Test loss: 0.6893 score: 0.7045 time: 0.26s
Epoch 27/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.35s
Val loss: 0.6903 score: 0.6977 time: 0.25s
Test loss: 0.6888 score: 0.7045 time: 0.24s
Epoch 28/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.34s
Val loss: 0.6899 score: 0.6744 time: 0.24s
Test loss: 0.6884 score: 0.7273 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.34s
Val loss: 0.6895 score: 0.6744 time: 0.25s
Test loss: 0.6878 score: 0.7273 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6876;  Loss pred: 0.6876; Loss self: 0.0000; time: 0.33s
Val loss: 0.6890 score: 0.6977 time: 0.25s
Test loss: 0.6873 score: 0.7273 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.28s
Val loss: 0.6885 score: 0.6977 time: 0.26s
Test loss: 0.6866 score: 0.7273 time: 0.24s
Epoch 32/1000, LR 0.000270
Train loss: 0.6863;  Loss pred: 0.6863; Loss self: 0.0000; time: 0.33s
Val loss: 0.6880 score: 0.7209 time: 0.24s
Test loss: 0.6860 score: 0.7273 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6854;  Loss pred: 0.6854; Loss self: 0.0000; time: 0.29s
Val loss: 0.6874 score: 0.7209 time: 0.26s
Test loss: 0.6852 score: 0.7273 time: 0.34s
Epoch 34/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.32s
Val loss: 0.6867 score: 0.7209 time: 0.24s
Test loss: 0.6844 score: 0.7273 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6838;  Loss pred: 0.6838; Loss self: 0.0000; time: 0.27s
Val loss: 0.6859 score: 0.7442 time: 0.25s
Test loss: 0.6834 score: 0.7273 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6822;  Loss pred: 0.6822; Loss self: 0.0000; time: 0.32s
Val loss: 0.6851 score: 0.7442 time: 0.23s
Test loss: 0.6824 score: 0.7727 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6812;  Loss pred: 0.6812; Loss self: 0.0000; time: 0.28s
Val loss: 0.6841 score: 0.7674 time: 0.26s
Test loss: 0.6813 score: 0.7727 time: 0.24s
Epoch 38/1000, LR 0.000270
Train loss: 0.6795;  Loss pred: 0.6795; Loss self: 0.0000; time: 0.33s
Val loss: 0.6830 score: 0.7674 time: 0.25s
Test loss: 0.6800 score: 0.7727 time: 0.25s
Epoch 39/1000, LR 0.000269
Train loss: 0.6784;  Loss pred: 0.6784; Loss self: 0.0000; time: 0.29s
Val loss: 0.6818 score: 0.7674 time: 0.26s
Test loss: 0.6787 score: 0.7727 time: 0.23s
Epoch 40/1000, LR 0.000269
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.39s
Val loss: 0.6805 score: 0.7907 time: 0.23s
Test loss: 0.6773 score: 0.7955 time: 0.34s
Epoch 41/1000, LR 0.000269
Train loss: 0.6744;  Loss pred: 0.6744; Loss self: 0.0000; time: 0.39s
Val loss: 0.6790 score: 0.7907 time: 0.25s
Test loss: 0.6757 score: 0.7955 time: 0.23s
Epoch 42/1000, LR 0.000269
Train loss: 0.6723;  Loss pred: 0.6723; Loss self: 0.0000; time: 0.32s
Val loss: 0.6774 score: 0.7907 time: 0.24s
Test loss: 0.6740 score: 0.8182 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6711;  Loss pred: 0.6711; Loss self: 0.0000; time: 0.29s
Val loss: 0.6756 score: 0.7907 time: 0.26s
Test loss: 0.6722 score: 0.8182 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6682;  Loss pred: 0.6682; Loss self: 0.0000; time: 0.32s
Val loss: 0.6737 score: 0.7907 time: 0.23s
Test loss: 0.6702 score: 0.8409 time: 0.25s
Epoch 45/1000, LR 0.000269
Train loss: 0.6657;  Loss pred: 0.6657; Loss self: 0.0000; time: 0.26s
Val loss: 0.6716 score: 0.7907 time: 0.26s
Test loss: 0.6680 score: 0.8409 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6626;  Loss pred: 0.6626; Loss self: 0.0000; time: 0.33s
Val loss: 0.6693 score: 0.7907 time: 0.25s
Test loss: 0.6657 score: 0.8409 time: 0.35s
Epoch 47/1000, LR 0.000269
Train loss: 0.6587;  Loss pred: 0.6587; Loss self: 0.0000; time: 0.28s
Val loss: 0.6668 score: 0.7907 time: 0.26s
Test loss: 0.6631 score: 0.8409 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.32s
Val loss: 0.6641 score: 0.7907 time: 0.23s
Test loss: 0.6603 score: 0.8409 time: 0.25s
Epoch 49/1000, LR 0.000269
Train loss: 0.6522;  Loss pred: 0.6522; Loss self: 0.0000; time: 0.27s
Val loss: 0.6611 score: 0.7907 time: 0.27s
Test loss: 0.6573 score: 0.8409 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6474;  Loss pred: 0.6474; Loss self: 0.0000; time: 0.33s
Val loss: 0.6580 score: 0.7907 time: 0.25s
Test loss: 0.6540 score: 0.8409 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6420;  Loss pred: 0.6420; Loss self: 0.0000; time: 0.27s
Val loss: 0.6545 score: 0.7907 time: 0.26s
Test loss: 0.6505 score: 0.8409 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6388;  Loss pred: 0.6388; Loss self: 0.0000; time: 0.34s
Val loss: 0.6508 score: 0.7907 time: 0.25s
Test loss: 0.6466 score: 0.8409 time: 0.24s
Epoch 53/1000, LR 0.000269
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.32s
Val loss: 0.6467 score: 0.7907 time: 0.26s
Test loss: 0.6424 score: 0.8409 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6284;  Loss pred: 0.6284; Loss self: 0.0000; time: 0.34s
Val loss: 0.6423 score: 0.7907 time: 0.25s
Test loss: 0.6378 score: 0.8409 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6211;  Loss pred: 0.6211; Loss self: 0.0000; time: 0.39s
Val loss: 0.6375 score: 0.8140 time: 0.25s
Test loss: 0.6329 score: 0.8182 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.6124;  Loss pred: 0.6124; Loss self: 0.0000; time: 0.38s
Val loss: 0.6323 score: 0.8140 time: 0.25s
Test loss: 0.6275 score: 0.8182 time: 0.36s
Epoch 57/1000, LR 0.000269
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.28s
Val loss: 0.6268 score: 0.8140 time: 0.26s
Test loss: 0.6218 score: 0.8182 time: 0.24s
Epoch 58/1000, LR 0.000269
Train loss: 0.5994;  Loss pred: 0.5994; Loss self: 0.0000; time: 0.33s
Val loss: 0.6209 score: 0.8140 time: 0.24s
Test loss: 0.6156 score: 0.8182 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.28s
Val loss: 0.6146 score: 0.8140 time: 0.26s
Test loss: 0.6091 score: 0.8182 time: 0.25s
Epoch 60/1000, LR 0.000268
Train loss: 0.5804;  Loss pred: 0.5804; Loss self: 0.0000; time: 0.33s
Val loss: 0.6080 score: 0.8140 time: 0.24s
Test loss: 0.6021 score: 0.8182 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.5755;  Loss pred: 0.5755; Loss self: 0.0000; time: 0.28s
Val loss: 0.6010 score: 0.8140 time: 0.26s
Test loss: 0.5949 score: 0.8409 time: 0.24s
Epoch 62/1000, LR 0.000268
Train loss: 0.5640;  Loss pred: 0.5640; Loss self: 0.0000; time: 0.34s
Val loss: 0.5937 score: 0.8372 time: 0.24s
Test loss: 0.5872 score: 0.8409 time: 0.26s
Epoch 63/1000, LR 0.000268
Train loss: 0.5551;  Loss pred: 0.5551; Loss self: 0.0000; time: 0.28s
Val loss: 0.5859 score: 0.8372 time: 0.27s
Test loss: 0.5792 score: 0.8409 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5377;  Loss pred: 0.5377; Loss self: 0.0000; time: 0.34s
Val loss: 0.5778 score: 0.8372 time: 0.24s
Test loss: 0.5709 score: 0.8409 time: 0.34s
Epoch 65/1000, LR 0.000268
Train loss: 0.5369;  Loss pred: 0.5369; Loss self: 0.0000; time: 0.44s
Val loss: 0.5694 score: 0.8372 time: 0.25s
Test loss: 0.5623 score: 0.8409 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.5234;  Loss pred: 0.5234; Loss self: 0.0000; time: 0.33s
Val loss: 0.5606 score: 0.8372 time: 0.26s
Test loss: 0.5535 score: 0.8409 time: 0.22s
Epoch 67/1000, LR 0.000268
Train loss: 0.5026;  Loss pred: 0.5026; Loss self: 0.0000; time: 0.35s
Val loss: 0.5513 score: 0.8140 time: 0.25s
Test loss: 0.5444 score: 0.8409 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.4910;  Loss pred: 0.4910; Loss self: 0.0000; time: 0.29s
Val loss: 0.5416 score: 0.8140 time: 0.26s
Test loss: 0.5350 score: 0.8409 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.4774;  Loss pred: 0.4774; Loss self: 0.0000; time: 0.35s
Val loss: 0.5315 score: 0.8140 time: 0.25s
Test loss: 0.5255 score: 0.8409 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.4675;  Loss pred: 0.4675; Loss self: 0.0000; time: 0.29s
Val loss: 0.5212 score: 0.8140 time: 0.26s
Test loss: 0.5158 score: 0.8409 time: 0.22s
Epoch 71/1000, LR 0.000268
Train loss: 0.4546;  Loss pred: 0.4546; Loss self: 0.0000; time: 0.35s
Val loss: 0.5108 score: 0.8140 time: 0.33s
Test loss: 0.5061 score: 0.8409 time: 0.25s
Epoch 72/1000, LR 0.000267
Train loss: 0.4319;  Loss pred: 0.4319; Loss self: 0.0000; time: 0.28s
Val loss: 0.5005 score: 0.8140 time: 0.26s
Test loss: 0.4965 score: 0.8409 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.4201;  Loss pred: 0.4201; Loss self: 0.0000; time: 0.33s
Val loss: 0.4903 score: 0.8140 time: 0.24s
Test loss: 0.4871 score: 0.8409 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.4062;  Loss pred: 0.4062; Loss self: 0.0000; time: 0.39s
Val loss: 0.4803 score: 0.8140 time: 0.26s
Test loss: 0.4779 score: 0.8409 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.3842;  Loss pred: 0.3842; Loss self: 0.0000; time: 0.32s
Val loss: 0.4704 score: 0.8140 time: 0.23s
Test loss: 0.4691 score: 0.8409 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3770;  Loss pred: 0.3770; Loss self: 0.0000; time: 0.43s
Val loss: 0.4607 score: 0.8140 time: 0.26s
Test loss: 0.4606 score: 0.8409 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.3714;  Loss pred: 0.3714; Loss self: 0.0000; time: 0.40s
Val loss: 0.4514 score: 0.8140 time: 0.35s
Test loss: 0.4525 score: 0.8409 time: 0.30s
Epoch 78/1000, LR 0.000267
Train loss: 0.3396;  Loss pred: 0.3396; Loss self: 0.0000; time: 0.32s
Val loss: 0.4425 score: 0.8140 time: 0.24s
Test loss: 0.4449 score: 0.8409 time: 0.26s
Epoch 79/1000, LR 0.000267
Train loss: 0.3097;  Loss pred: 0.3097; Loss self: 0.0000; time: 0.26s
Val loss: 0.4341 score: 0.8372 time: 0.26s
Test loss: 0.4378 score: 0.8636 time: 0.29s
Epoch 80/1000, LR 0.000267
Train loss: 0.3092;  Loss pred: 0.3092; Loss self: 0.0000; time: 0.31s
Val loss: 0.4262 score: 0.8605 time: 0.24s
Test loss: 0.4310 score: 0.8636 time: 0.25s
Epoch 81/1000, LR 0.000267
Train loss: 0.2916;  Loss pred: 0.2916; Loss self: 0.0000; time: 0.28s
Val loss: 0.4189 score: 0.8605 time: 0.26s
Test loss: 0.4247 score: 0.8636 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.3083;  Loss pred: 0.3083; Loss self: 0.0000; time: 0.34s
Val loss: 0.4125 score: 0.8605 time: 0.25s
Test loss: 0.4192 score: 0.8409 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2718;  Loss pred: 0.2718; Loss self: 0.0000; time: 0.29s
Val loss: 0.4070 score: 0.8605 time: 0.25s
Test loss: 0.4146 score: 0.8636 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2717;  Loss pred: 0.2717; Loss self: 0.0000; time: 0.33s
Val loss: 0.4023 score: 0.8605 time: 0.25s
Test loss: 0.4108 score: 0.8636 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2347;  Loss pred: 0.2347; Loss self: 0.0000; time: 0.32s
Val loss: 0.3985 score: 0.8605 time: 0.26s
Test loss: 0.4078 score: 0.8636 time: 0.23s
Epoch 86/1000, LR 0.000266
Train loss: 0.2268;  Loss pred: 0.2268; Loss self: 0.0000; time: 0.33s
Val loss: 0.3956 score: 0.8605 time: 0.26s
Test loss: 0.4057 score: 0.8636 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.2330;  Loss pred: 0.2330; Loss self: 0.0000; time: 0.30s
Val loss: 0.3936 score: 0.8605 time: 0.35s
Test loss: 0.4046 score: 0.8636 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.2290;  Loss pred: 0.2290; Loss self: 0.0000; time: 0.33s
Val loss: 0.3921 score: 0.8605 time: 0.23s
Test loss: 0.4039 score: 0.8636 time: 0.25s
Epoch 89/1000, LR 0.000266
Train loss: 0.2136;  Loss pred: 0.2136; Loss self: 0.0000; time: 0.26s
Val loss: 0.3906 score: 0.8605 time: 0.26s
Test loss: 0.4033 score: 0.8636 time: 0.24s
Epoch 90/1000, LR 0.000266
Train loss: 0.2004;  Loss pred: 0.2004; Loss self: 0.0000; time: 0.35s
Val loss: 0.3896 score: 0.8605 time: 0.23s
Test loss: 0.4035 score: 0.8636 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.1613;  Loss pred: 0.1613; Loss self: 0.0000; time: 0.32s
Val loss: 0.3896 score: 0.8605 time: 0.26s
Test loss: 0.4044 score: 0.8636 time: 0.24s
Epoch 92/1000, LR 0.000266
Train loss: 0.2113;  Loss pred: 0.2113; Loss self: 0.0000; time: 0.36s
Val loss: 0.3902 score: 0.8605 time: 0.25s
Test loss: 0.4061 score: 0.8636 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 93/1000, LR 0.000265
Train loss: 0.1466;  Loss pred: 0.1466; Loss self: 0.0000; time: 0.28s
Val loss: 0.3914 score: 0.8605 time: 0.26s
Test loss: 0.4083 score: 0.8636 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.1613,   Val_Loss: 0.3896,   Val_Precision: 0.9000,   Val_Recall: 0.8182,   Val_accuracy: 0.8571,   Val_Score: 0.8605,   Val_Loss: 0.3896,   Test_Precision: 0.9444,   Test_Recall: 0.7727,   Test_accuracy: 0.8500,   Test_Score: 0.8636,   Test_loss: 0.4044


[0.2514687420334667, 0.23251657199580222, 0.24458059796597809, 0.27918873401358724, 0.24399058206472546, 0.25478168996050954, 0.24712166201788932, 0.25280240701977164, 0.3379805749282241, 0.2536935689859092, 0.2451093300478533, 0.25547526497393847, 0.2590720159932971, 0.2582047020550817, 0.24700368102639914, 0.24181767308618873, 0.22991992998868227, 0.2450813609175384, 0.24900227796752006, 0.24262488004751503, 0.2542982690501958, 0.24631918605882674, 0.2509135160362348, 0.23946545703802258, 0.2615683920448646, 0.2650777400704101, 0.24607847491279244, 0.2512346380390227, 0.25617723201867193, 0.25474920601118356, 0.2409827479859814, 0.24876219604630023, 0.3416238990612328, 0.2562221890548244, 0.24579727300442755, 0.2518847859464586, 0.24408856208901852, 0.25090344494674355, 0.23068957997020334, 0.3414085269905627, 0.2355697649763897, 0.2547475609462708, 0.23513500101398677, 0.2495683419983834, 0.2430214729392901, 0.35203872493002564, 0.2328117749420926, 0.2529702769825235, 0.24332378699909896, 0.25846279598772526, 0.2404329190030694, 0.24785813910420984, 0.23303952999413013, 0.23055433796253055, 0.23986885999329388, 0.36073893401771784, 0.24654571700375527, 0.2580471629044041, 0.2580863810144365, 0.24845388706307858, 0.24843055894598365, 0.259369443054311, 0.24287427705712616, 0.34273971093352884, 0.25337594898883253, 0.22798258601687849, 0.25022471603006124, 0.224414078053087, 0.2510993559844792, 0.2282417039386928, 0.250872582080774, 0.2301256579812616, 0.2512671850854531, 0.23231405089609325, 0.25272661296185106, 0.23156074795406312, 0.3049970540450886, 0.25993977999314666, 0.29750615800730884, 0.25848772400058806, 0.2411239199573174, 0.24809637700673193, 0.23109782801475376, 0.24595153308473527, 0.2374484270112589, 0.2491922010667622, 0.22574883000925183, 0.2550901479553431, 0.24714231700636446, 0.24868038296699524, 0.2428560679545626, 0.25636656302958727, 0.2424944629892707]
[0.005715198682578789, 0.005284467545359142, 0.005558649953772229, 0.006345198500308801, 0.005545240501471033, 0.005790492953647944, 0.0056164014094974846, 0.005745509250449355, 0.007681376702914184, 0.005765762931497937, 0.005570666591996665, 0.005806256022134965, 0.005888000363484025, 0.005868288683070039, 0.005613720023327253, 0.00549585620650429, 0.005225452954288234, 0.005570030929944055, 0.005659142681080001, 0.005514201819261705, 0.005779506114777178, 0.00559816331951879, 0.005702579909914427, 0.00544239675086415, 0.005944736182837832, 0.00602449409250932, 0.005592692611654374, 0.0057098781372505155, 0.00582220981860618, 0.005789754682072354, 0.005476880636045032, 0.005653686273779551, 0.0077641795241189275, 0.005823231569427828, 0.005586301659191536, 0.005724654226055877, 0.005547467320204966, 0.005702351021516899, 0.005242944999322804, 0.00775928470433097, 0.005353858294917948, 0.005789717294233428, 0.005343977295772426, 0.005672007772690532, 0.0055232152940747755, 0.008000880112046038, 0.005291176703229377, 0.005749324476875534, 0.00553008606816134, 0.005874154454266483, 0.0054643845227970314, 0.005633139525095678, 0.005296352954412048, 0.0052398713173302395, 0.005451564999847588, 0.008198612136766315, 0.005603311750085347, 0.005864708247827366, 0.00586559956850992, 0.005646679251433604, 0.005646149066954174, 0.005894760069416158, 0.0055198699331165035, 0.007789538884852928, 0.00575854429520074, 0.005181422409474511, 0.0056869253643195734, 0.005100319955751977, 0.0057068035451017995, 0.00518731145315211, 0.005701649592744864, 0.005230128590483218, 0.005710617842851207, 0.005279864793093028, 0.005743786658223888, 0.005262744271683253, 0.006931751228297468, 0.005907722272571515, 0.006761503591075201, 0.005874721000013365, 0.005480089089939032, 0.005638554022880271, 0.005252223363971676, 0.0055898075701076196, 0.0053965551593467935, 0.005663459115153687, 0.0051306552274829964, 0.0057975033626214345, 0.0056168708410537374, 0.005651826885613528, 0.005519456089876423, 0.005826512796126984, 0.005511237795210697]
[174.9720448124096, 189.23382373276328, 179.89979730984442, 157.59948249866937, 180.33482943340718, 172.6968684712778, 178.0499517554022, 174.04897571469235, 130.18499660622254, 173.43758525642355, 179.5117305057697, 172.2280237364213, 169.83694603719144, 170.40743119625168, 178.13499708653083, 181.95527001170632, 191.37096989445797, 179.5322167106968, 176.7052107279893, 181.34990933898928, 173.02516515090736, 178.6300154040448, 175.35922613928017, 183.74257625396731, 168.21604344477927, 165.98904151028563, 178.8047492394169, 175.1350862422313, 171.75609109865385, 172.71888964422675, 182.5856845260956, 176.87575001070746, 128.7966097246417, 171.72595457993384, 179.00930902194114, 174.68303944865005, 180.26244090844907, 175.36626493645548, 190.73249864897753, 128.87785899154258, 186.78118562630462, 172.72000499851734, 187.12654351864316, 176.30441284209442, 181.05395983256082, 124.98624976199943, 188.9938771823038, 173.9334775802129, 180.82901200351174, 170.23726696081096, 183.0032267729457, 177.52090029813616, 188.80916898994897, 190.84438136727158, 183.4335644953252, 121.97186344692952, 178.46588671150923, 170.51146582960183, 170.48555536736666, 177.0952369476477, 177.11186653799277, 169.6421886937027, 181.16368902109306, 128.37730381506154, 173.65499833584948, 192.9971967102018, 175.84194198751322, 196.06613088503047, 175.22944185774696, 192.778091123166, 175.38783885850555, 191.19988786119094, 175.11240071016874, 189.3987893985793, 174.1011739299564, 190.01493296579218, 144.26368850597277, 169.26997476554018, 147.896098335279, 170.2208496365572, 182.47878521462601, 177.3504334519407, 190.3955583571774, 178.8970349082602, 185.30339642095709, 176.57053395588315, 194.90687946509735, 172.48804139508664, 178.0350711807355, 176.93394016463157, 181.17727249142575, 171.62924634177804, 181.447441964672]
Elapsed: 0.2542021067021915~0.0272170533226076
Time per graph: 0.005777320606867991~0.0006185693936956272
Speed: 174.69067436067868~15.07875074595936
Total Time: 0.2433
best val loss: 0.3895757496356964 test_score: 0.8636

Testing...
Test loss: 0.4310 score: 0.8636 time: 0.25s
test Score 0.8636
Epoch Time List: [0.8492227299138904, 0.8035539700649679, 0.9200324779376388, 0.9774291620124131, 0.772653921158053, 0.8216311109717935, 0.7898320778040215, 0.835273390985094, 0.8780643520876765, 0.8158374711638317, 0.7860900680534542, 0.810779923107475, 0.7951239629182965, 0.8218133290065452, 0.7895649379352108, 0.9490686999633908, 0.801375670125708, 0.9116965311113745, 0.8234068170422688, 0.8086350531084463, 0.8665526129771024, 0.7774529920425266, 0.8160915249027312, 0.776920035132207, 0.8322437390452251, 0.9753848239779472, 0.8411793978884816, 0.8215584130957723, 0.8421639289008453, 0.8296927510527894, 0.7752176669891924, 0.8160718792350963, 0.8881849329918623, 0.8076067661168054, 0.7637769840657711, 0.8040499539347365, 0.7827169599477202, 0.8287257850170135, 0.7740882179932669, 0.964860048959963, 0.8716039760038257, 0.8063112569507211, 0.7783151939511299, 0.7956681159557775, 0.763312506955117, 0.928236442967318, 0.7677529149223119, 0.797460851026699, 0.7759310408728197, 0.8319732900708914, 0.7702829899499193, 0.8265446278965101, 0.8098759420681745, 0.8108954649651423, 0.8759739899542183, 0.9852900529513136, 0.7854804210364819, 0.8237137031974271, 0.7902935699094087, 0.8161649609683082, 0.7822141267824918, 0.8376901769079268, 0.7893723848974332, 0.9149987490382046, 0.935276222997345, 0.8096641659503803, 0.840977763873525, 0.7647117978194728, 0.8519624940818176, 0.7747242889599875, 0.9254963538842276, 0.7710260800085962, 0.8196517250034958, 0.8737408079905435, 0.7965204410720617, 0.9179249219596386, 1.0546731369104236, 0.8089775808621198, 0.8177844180027023, 0.8042388590984046, 0.774023319943808, 0.8276781280292198, 0.7755435100989416, 0.8167096800170839, 0.8060140220914036, 0.8329703392228112, 0.8755327719263732, 0.8162045649951324, 0.7695101008284837, 0.8327302599791437, 0.8129989539738744, 0.8605154609540477, 0.7816360059659928]
Total Epoch List: [93]
Total Time List: [0.24327162595000118]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288336dd0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.33s
Epoch 2/1000, LR 0.000000
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6950 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5116 time: 0.24s
Epoch 3/1000, LR 0.000030
Train loss: 0.6964;  Loss pred: 0.6964; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6962;  Loss pred: 0.6962; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6960;  Loss pred: 0.6960; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6958;  Loss pred: 0.6958; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5116 time: 0.25s
Epoch 8/1000, LR 0.000180
Train loss: 0.6956;  Loss pred: 0.6956; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.33s
Epoch 9/1000, LR 0.000210
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6947;  Loss pred: 0.6947; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.25s
Epoch 12/1000, LR 0.000270
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.24s
Epoch 14/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.24s
Epoch 15/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5116 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.28s
Epoch 19/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.5116 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.5116 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.5116 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5116 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5116 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6894 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6906 score: 0.5116 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6871;  Loss pred: 0.6871; Loss self: 0.0000; time: 0.39s
Val loss: 0.6889 score: 0.5455 time: 0.22s
Test loss: 0.6903 score: 0.5814 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.43s
Val loss: 0.6884 score: 0.5682 time: 0.22s
Test loss: 0.6900 score: 0.5814 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.41s
Val loss: 0.6878 score: 0.5909 time: 0.23s
Test loss: 0.6896 score: 0.5581 time: 0.23s
Epoch 29/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.42s
Val loss: 0.6871 score: 0.5909 time: 0.23s
Test loss: 0.6892 score: 0.5581 time: 0.25s
Epoch 30/1000, LR 0.000270
Train loss: 0.6828;  Loss pred: 0.6828; Loss self: 0.0000; time: 0.34s
Val loss: 0.6863 score: 0.5682 time: 0.24s
Test loss: 0.6887 score: 0.5581 time: 0.23s
Epoch 31/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.37s
Val loss: 0.6854 score: 0.5682 time: 0.21s
Test loss: 0.6881 score: 0.5581 time: 0.25s
Epoch 32/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.42s
Val loss: 0.6845 score: 0.5682 time: 0.23s
Test loss: 0.6875 score: 0.5581 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6781;  Loss pred: 0.6781; Loss self: 0.0000; time: 0.37s
Val loss: 0.6834 score: 0.5682 time: 0.21s
Test loss: 0.6868 score: 0.5581 time: 0.25s
Epoch 34/1000, LR 0.000270
Train loss: 0.6764;  Loss pred: 0.6764; Loss self: 0.0000; time: 0.30s
Val loss: 0.6822 score: 0.5455 time: 0.23s
Test loss: 0.6860 score: 0.5581 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6745;  Loss pred: 0.6745; Loss self: 0.0000; time: 0.36s
Val loss: 0.6809 score: 0.5682 time: 0.21s
Test loss: 0.6852 score: 0.5581 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.35s
Val loss: 0.6796 score: 0.5682 time: 0.24s
Test loss: 0.6843 score: 0.5581 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6691;  Loss pred: 0.6691; Loss self: 0.0000; time: 0.37s
Val loss: 0.6781 score: 0.5682 time: 0.21s
Test loss: 0.6833 score: 0.5581 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6670;  Loss pred: 0.6670; Loss self: 0.0000; time: 0.32s
Val loss: 0.6765 score: 0.5682 time: 0.24s
Test loss: 0.6823 score: 0.5581 time: 0.31s
Epoch 39/1000, LR 0.000269
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.34s
Val loss: 0.6748 score: 0.5682 time: 0.21s
Test loss: 0.6812 score: 0.5581 time: 0.24s
Epoch 40/1000, LR 0.000269
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.31s
Val loss: 0.6729 score: 0.5909 time: 0.24s
Test loss: 0.6800 score: 0.5581 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6579;  Loss pred: 0.6579; Loss self: 0.0000; time: 0.36s
Val loss: 0.6709 score: 0.6136 time: 0.21s
Test loss: 0.6787 score: 0.5814 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6540;  Loss pred: 0.6540; Loss self: 0.0000; time: 0.31s
Val loss: 0.6686 score: 0.6136 time: 0.24s
Test loss: 0.6773 score: 0.5814 time: 0.23s
Epoch 43/1000, LR 0.000269
Train loss: 0.6506;  Loss pred: 0.6506; Loss self: 0.0000; time: 0.36s
Val loss: 0.6661 score: 0.6136 time: 0.21s
Test loss: 0.6758 score: 0.6047 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6453;  Loss pred: 0.6453; Loss self: 0.0000; time: 0.31s
Val loss: 0.6634 score: 0.6818 time: 0.24s
Test loss: 0.6741 score: 0.6279 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6402;  Loss pred: 0.6402; Loss self: 0.0000; time: 0.36s
Val loss: 0.6604 score: 0.6818 time: 0.23s
Test loss: 0.6723 score: 0.6744 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6362;  Loss pred: 0.6362; Loss self: 0.0000; time: 0.30s
Val loss: 0.6571 score: 0.7045 time: 0.23s
Test loss: 0.6703 score: 0.6744 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6310;  Loss pred: 0.6310; Loss self: 0.0000; time: 0.45s
Val loss: 0.6536 score: 0.7727 time: 0.21s
Test loss: 0.6682 score: 0.7442 time: 0.27s
Epoch 48/1000, LR 0.000269
Train loss: 0.6248;  Loss pred: 0.6248; Loss self: 0.0000; time: 0.42s
Val loss: 0.6498 score: 0.7727 time: 0.23s
Test loss: 0.6659 score: 0.7442 time: 0.23s
Epoch 49/1000, LR 0.000269
Train loss: 0.6172;  Loss pred: 0.6172; Loss self: 0.0000; time: 0.35s
Val loss: 0.6458 score: 0.8182 time: 0.22s
Test loss: 0.6634 score: 0.7907 time: 0.25s
Epoch 50/1000, LR 0.000269
Train loss: 0.6105;  Loss pred: 0.6105; Loss self: 0.0000; time: 0.40s
Val loss: 0.6414 score: 0.8409 time: 0.29s
Test loss: 0.6608 score: 0.7674 time: 0.35s
Epoch 51/1000, LR 0.000269
Train loss: 0.6044;  Loss pred: 0.6044; Loss self: 0.0000; time: 0.35s
Val loss: 0.6368 score: 0.8409 time: 0.24s
Test loss: 0.6580 score: 0.7442 time: 0.23s
Epoch 52/1000, LR 0.000269
Train loss: 0.5905;  Loss pred: 0.5905; Loss self: 0.0000; time: 0.36s
Val loss: 0.6319 score: 0.8409 time: 0.23s
Test loss: 0.6549 score: 0.7209 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.5859;  Loss pred: 0.5859; Loss self: 0.0000; time: 0.33s
Val loss: 0.6266 score: 0.8409 time: 0.23s
Test loss: 0.6517 score: 0.7442 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.5774;  Loss pred: 0.5774; Loss self: 0.0000; time: 0.36s
Val loss: 0.6208 score: 0.8182 time: 0.21s
Test loss: 0.6482 score: 0.7442 time: 0.24s
Epoch 55/1000, LR 0.000269
Train loss: 0.5671;  Loss pred: 0.5671; Loss self: 0.0000; time: 0.40s
Val loss: 0.6147 score: 0.8409 time: 0.22s
Test loss: 0.6445 score: 0.7442 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.5596;  Loss pred: 0.5596; Loss self: 0.0000; time: 0.37s
Val loss: 0.6083 score: 0.8409 time: 0.26s
Test loss: 0.6406 score: 0.7442 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5489;  Loss pred: 0.5489; Loss self: 0.0000; time: 0.37s
Val loss: 0.6016 score: 0.8409 time: 0.22s
Test loss: 0.6365 score: 0.7442 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.5384;  Loss pred: 0.5384; Loss self: 0.0000; time: 0.35s
Val loss: 0.5945 score: 0.8636 time: 0.24s
Test loss: 0.6322 score: 0.7442 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.34s
Val loss: 0.5872 score: 0.8636 time: 0.23s
Test loss: 0.6278 score: 0.7674 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.36s
Val loss: 0.5794 score: 0.8636 time: 0.22s
Test loss: 0.6232 score: 0.7674 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5040;  Loss pred: 0.5040; Loss self: 0.0000; time: 0.32s
Val loss: 0.5714 score: 0.8636 time: 0.23s
Test loss: 0.6183 score: 0.7907 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.4899;  Loss pred: 0.4899; Loss self: 0.0000; time: 0.35s
Val loss: 0.5631 score: 0.8636 time: 0.30s
Test loss: 0.6134 score: 0.7907 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.4764;  Loss pred: 0.4764; Loss self: 0.0000; time: 0.40s
Val loss: 0.5545 score: 0.8409 time: 0.22s
Test loss: 0.6084 score: 0.7907 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.4611;  Loss pred: 0.4611; Loss self: 0.0000; time: 0.35s
Val loss: 0.5457 score: 0.8409 time: 0.28s
Test loss: 0.6032 score: 0.7907 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.4489;  Loss pred: 0.4489; Loss self: 0.0000; time: 0.38s
Val loss: 0.5367 score: 0.8409 time: 0.23s
Test loss: 0.5979 score: 0.7907 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.4334;  Loss pred: 0.4334; Loss self: 0.0000; time: 0.35s
Val loss: 0.5276 score: 0.8409 time: 0.24s
Test loss: 0.5926 score: 0.7907 time: 0.23s
Epoch 67/1000, LR 0.000268
Train loss: 0.4165;  Loss pred: 0.4165; Loss self: 0.0000; time: 0.37s
Val loss: 0.5183 score: 0.8409 time: 0.23s
Test loss: 0.5872 score: 0.7907 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.3952;  Loss pred: 0.3952; Loss self: 0.0000; time: 0.33s
Val loss: 0.5089 score: 0.8409 time: 0.24s
Test loss: 0.5816 score: 0.7907 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.3854;  Loss pred: 0.3854; Loss self: 0.0000; time: 0.36s
Val loss: 0.4995 score: 0.8409 time: 0.30s
Test loss: 0.5762 score: 0.7907 time: 0.30s
Epoch 70/1000, LR 0.000268
Train loss: 0.3672;  Loss pred: 0.3672; Loss self: 0.0000; time: 0.35s
Val loss: 0.4900 score: 0.8409 time: 0.23s
Test loss: 0.5708 score: 0.7907 time: 0.22s
Epoch 71/1000, LR 0.000268
Train loss: 0.3554;  Loss pred: 0.3554; Loss self: 0.0000; time: 0.45s
Val loss: 0.4806 score: 0.8409 time: 0.23s
Test loss: 0.5655 score: 0.7907 time: 0.23s
Epoch 72/1000, LR 0.000267
Train loss: 0.3405;  Loss pred: 0.3405; Loss self: 0.0000; time: 0.43s
Val loss: 0.4714 score: 0.8409 time: 0.23s
Test loss: 0.5604 score: 0.7907 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.3326;  Loss pred: 0.3326; Loss self: 0.0000; time: 0.45s
Val loss: 0.4632 score: 0.8409 time: 0.29s
Test loss: 0.5566 score: 0.7907 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.3068;  Loss pred: 0.3068; Loss self: 0.0000; time: 0.42s
Val loss: 0.4552 score: 0.8409 time: 0.25s
Test loss: 0.5531 score: 0.7907 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.2909;  Loss pred: 0.2909; Loss self: 0.0000; time: 0.47s
Val loss: 0.4475 score: 0.8409 time: 0.22s
Test loss: 0.5501 score: 0.7674 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.2866;  Loss pred: 0.2866; Loss self: 0.0000; time: 0.33s
Val loss: 0.4402 score: 0.8409 time: 0.23s
Test loss: 0.5473 score: 0.7674 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.2746;  Loss pred: 0.2746; Loss self: 0.0000; time: 0.42s
Val loss: 0.4331 score: 0.8409 time: 0.22s
Test loss: 0.5448 score: 0.7907 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.2682;  Loss pred: 0.2682; Loss self: 0.0000; time: 0.42s
Val loss: 0.4268 score: 0.8409 time: 0.24s
Test loss: 0.5432 score: 0.7907 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.2568;  Loss pred: 0.2568; Loss self: 0.0000; time: 0.36s
Val loss: 0.4214 score: 0.8409 time: 0.21s
Test loss: 0.5426 score: 0.7674 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.2322;  Loss pred: 0.2322; Loss self: 0.0000; time: 0.31s
Val loss: 0.4173 score: 0.8409 time: 0.24s
Test loss: 0.5435 score: 0.7674 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.2240;  Loss pred: 0.2240; Loss self: 0.0000; time: 0.34s
Val loss: 0.4134 score: 0.8182 time: 0.21s
Test loss: 0.5447 score: 0.7674 time: 0.25s
Epoch 82/1000, LR 0.000267
Train loss: 0.2177;  Loss pred: 0.2177; Loss self: 0.0000; time: 0.33s
Val loss: 0.4095 score: 0.8182 time: 0.24s
Test loss: 0.5456 score: 0.7674 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2156;  Loss pred: 0.2156; Loss self: 0.0000; time: 0.35s
Val loss: 0.4065 score: 0.8182 time: 0.25s
Test loss: 0.5475 score: 0.7907 time: 0.25s
Epoch 84/1000, LR 0.000266
Train loss: 0.2064;  Loss pred: 0.2064; Loss self: 0.0000; time: 0.33s
Val loss: 0.4044 score: 0.8182 time: 0.25s
Test loss: 0.5503 score: 0.7907 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.1789;  Loss pred: 0.1789; Loss self: 0.0000; time: 0.37s
Val loss: 0.4025 score: 0.8182 time: 0.21s
Test loss: 0.5532 score: 0.7907 time: 0.25s
Epoch 86/1000, LR 0.000266
Train loss: 0.1750;  Loss pred: 0.1750; Loss self: 0.0000; time: 0.46s
Val loss: 0.4010 score: 0.8182 time: 0.22s
Test loss: 0.5565 score: 0.7907 time: 0.22s
Epoch 87/1000, LR 0.000266
Train loss: 0.1647;  Loss pred: 0.1647; Loss self: 0.0000; time: 0.36s
Val loss: 0.3989 score: 0.8409 time: 0.24s
Test loss: 0.5587 score: 0.7907 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.1530;  Loss pred: 0.1530; Loss self: 0.0000; time: 0.35s
Val loss: 0.3968 score: 0.8409 time: 0.23s
Test loss: 0.5608 score: 0.7907 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.1672;  Loss pred: 0.1672; Loss self: 0.0000; time: 0.36s
Val loss: 0.3949 score: 0.8409 time: 0.21s
Test loss: 0.5634 score: 0.7907 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.1471;  Loss pred: 0.1471; Loss self: 0.0000; time: 0.32s
Val loss: 0.3940 score: 0.8409 time: 0.24s
Test loss: 0.5675 score: 0.7907 time: 0.22s
Epoch 91/1000, LR 0.000266
Train loss: 0.1302;  Loss pred: 0.1302; Loss self: 0.0000; time: 0.36s
Val loss: 0.3933 score: 0.8409 time: 0.21s
Test loss: 0.5720 score: 0.7907 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 0.1309;  Loss pred: 0.1309; Loss self: 0.0000; time: 0.32s
Val loss: 0.3947 score: 0.8409 time: 0.23s
Test loss: 0.5788 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 93/1000, LR 0.000265
Train loss: 0.1108;  Loss pred: 0.1108; Loss self: 0.0000; time: 0.43s
Val loss: 0.3983 score: 0.8409 time: 0.29s
Test loss: 0.5878 score: 0.7907 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 090,   Train_Loss: 0.1302,   Val_Loss: 0.3933,   Val_Precision: 0.9412,   Val_Recall: 0.7273,   Val_accuracy: 0.8205,   Val_Score: 0.8409,   Val_Loss: 0.3933,   Test_Precision: 0.8824,   Test_Recall: 0.6818,   Test_accuracy: 0.7692,   Test_Score: 0.7907,   Test_loss: 0.5720


[0.2514687420334667, 0.23251657199580222, 0.24458059796597809, 0.27918873401358724, 0.24399058206472546, 0.25478168996050954, 0.24712166201788932, 0.25280240701977164, 0.3379805749282241, 0.2536935689859092, 0.2451093300478533, 0.25547526497393847, 0.2590720159932971, 0.2582047020550817, 0.24700368102639914, 0.24181767308618873, 0.22991992998868227, 0.2450813609175384, 0.24900227796752006, 0.24262488004751503, 0.2542982690501958, 0.24631918605882674, 0.2509135160362348, 0.23946545703802258, 0.2615683920448646, 0.2650777400704101, 0.24607847491279244, 0.2512346380390227, 0.25617723201867193, 0.25474920601118356, 0.2409827479859814, 0.24876219604630023, 0.3416238990612328, 0.2562221890548244, 0.24579727300442755, 0.2518847859464586, 0.24408856208901852, 0.25090344494674355, 0.23068957997020334, 0.3414085269905627, 0.2355697649763897, 0.2547475609462708, 0.23513500101398677, 0.2495683419983834, 0.2430214729392901, 0.35203872493002564, 0.2328117749420926, 0.2529702769825235, 0.24332378699909896, 0.25846279598772526, 0.2404329190030694, 0.24785813910420984, 0.23303952999413013, 0.23055433796253055, 0.23986885999329388, 0.36073893401771784, 0.24654571700375527, 0.2580471629044041, 0.2580863810144365, 0.24845388706307858, 0.24843055894598365, 0.259369443054311, 0.24287427705712616, 0.34273971093352884, 0.25337594898883253, 0.22798258601687849, 0.25022471603006124, 0.224414078053087, 0.2510993559844792, 0.2282417039386928, 0.250872582080774, 0.2301256579812616, 0.2512671850854531, 0.23231405089609325, 0.25272661296185106, 0.23156074795406312, 0.3049970540450886, 0.25993977999314666, 0.29750615800730884, 0.25848772400058806, 0.2411239199573174, 0.24809637700673193, 0.23109782801475376, 0.24595153308473527, 0.2374484270112589, 0.2491922010667622, 0.22574883000925183, 0.2550901479553431, 0.24714231700636446, 0.24868038296699524, 0.2428560679545626, 0.25636656302958727, 0.2424944629892707, 0.3358056510332972, 0.2424693520879373, 0.2542250470723957, 0.2432427379535511, 0.25381210597697645, 0.23927866900339723, 0.25572142098098993, 0.3319718310376629, 0.25286178407259285, 0.22333295608405024, 0.2548868280136958, 0.2454686079872772, 0.24703226995188743, 0.24151606089435518, 0.24721219495404512, 0.24841094494331628, 0.25556586601305753, 0.2832463609520346, 0.24107529898174107, 0.25157933006994426, 0.23827584297396243, 0.2497651280136779, 0.24273170600645244, 0.24942220805678517, 0.2396239050431177, 0.24156139395199716, 0.23899131396319717, 0.23205167392734438, 0.2510599480010569, 0.2384340959833935, 0.25180885300505906, 0.22362357401289046, 0.25700750399846584, 0.23774698306806386, 0.2502954580122605, 0.24028813501354307, 0.24994655407499522, 0.31252238107845187, 0.24757994909305125, 0.25201441708486527, 0.25274987798184156, 0.23637744295410812, 0.24610379908699542, 0.2471287560183555, 0.2532165199518204, 0.2380414200015366, 0.27302287600468844, 0.23608834901824594, 0.25143251894041896, 0.357193015050143, 0.23232561396434903, 0.25384618795942515, 0.2422945509897545, 0.24535922205541283, 0.24092970590572804, 0.25349596107844263, 0.2335845569614321, 0.2513192229671404, 0.2368854940868914, 0.2520970939658582, 0.2344483289634809, 0.2434006059775129, 0.23999456292949617, 0.23815488803666085, 0.24340428202413023, 0.23063844302669168, 0.2520022800890729, 0.24172892002388835, 0.2996946539497003, 0.22600206406787038, 0.2390082839410752, 0.25780325499363244, 0.23184903000947088, 0.25702829600777477, 0.24491688492707908, 0.23350506799761206, 0.24924355500843376, 0.2310052199754864, 0.25657366891391575, 0.2282227820251137, 0.25189505598973483, 0.24254120595287532, 0.25251851708162576, 0.24379398894961923, 0.25285740103572607, 0.22765516897197813, 0.239557969965972, 0.22921569808386266, 0.2537334189983085, 0.22838918503839523, 0.2542655060533434, 0.24397383199539036, 0.23650421004276723]
[0.005715198682578789, 0.005284467545359142, 0.005558649953772229, 0.006345198500308801, 0.005545240501471033, 0.005790492953647944, 0.0056164014094974846, 0.005745509250449355, 0.007681376702914184, 0.005765762931497937, 0.005570666591996665, 0.005806256022134965, 0.005888000363484025, 0.005868288683070039, 0.005613720023327253, 0.00549585620650429, 0.005225452954288234, 0.005570030929944055, 0.005659142681080001, 0.005514201819261705, 0.005779506114777178, 0.00559816331951879, 0.005702579909914427, 0.00544239675086415, 0.005944736182837832, 0.00602449409250932, 0.005592692611654374, 0.0057098781372505155, 0.00582220981860618, 0.005789754682072354, 0.005476880636045032, 0.005653686273779551, 0.0077641795241189275, 0.005823231569427828, 0.005586301659191536, 0.005724654226055877, 0.005547467320204966, 0.005702351021516899, 0.005242944999322804, 0.00775928470433097, 0.005353858294917948, 0.005789717294233428, 0.005343977295772426, 0.005672007772690532, 0.0055232152940747755, 0.008000880112046038, 0.005291176703229377, 0.005749324476875534, 0.00553008606816134, 0.005874154454266483, 0.0054643845227970314, 0.005633139525095678, 0.005296352954412048, 0.0052398713173302395, 0.005451564999847588, 0.008198612136766315, 0.005603311750085347, 0.005864708247827366, 0.00586559956850992, 0.005646679251433604, 0.005646149066954174, 0.005894760069416158, 0.0055198699331165035, 0.007789538884852928, 0.00575854429520074, 0.005181422409474511, 0.0056869253643195734, 0.005100319955751977, 0.0057068035451017995, 0.00518731145315211, 0.005701649592744864, 0.005230128590483218, 0.005710617842851207, 0.005279864793093028, 0.005743786658223888, 0.005262744271683253, 0.006931751228297468, 0.005907722272571515, 0.006761503591075201, 0.005874721000013365, 0.005480089089939032, 0.005638554022880271, 0.005252223363971676, 0.0055898075701076196, 0.0053965551593467935, 0.005663459115153687, 0.0051306552274829964, 0.0057975033626214345, 0.0056168708410537374, 0.005651826885613528, 0.005519456089876423, 0.005826512796126984, 0.005511237795210697, 0.0078094337449604, 0.005638822141579937, 0.0059122103970324575, 0.005656807859384909, 0.0059026071157436385, 0.005564620209381331, 0.00594700979025558, 0.007720275140410766, 0.005880506606339369, 0.005193789676373261, 0.005927600651481297, 0.005708572278773889, 0.00574493651050901, 0.005616652578938492, 0.00574912081288477, 0.0057769987196120065, 0.005943392232861803, 0.00658712467330313, 0.0056064023019009555, 0.005850682094649866, 0.00554129867381308, 0.0058084913491553, 0.005644923395498894, 0.005800516466436864, 0.00557264895449111, 0.005617706836092957, 0.005557937534027841, 0.00539655055644987, 0.0058386034418850445, 0.005544978976357988, 0.005856019837326955, 0.005200548232857918, 0.0059769186976387405, 0.005528999606234043, 0.005820824604936291, 0.005588096163105653, 0.0058127105598836095, 0.007267962350661672, 0.005757673234722122, 0.005860800397322449, 0.0058779041391125945, 0.0054971498361420495, 0.0057233441648138475, 0.005747180372519895, 0.005888756277949311, 0.005535846976779921, 0.0063493692094113595, 0.0054904267213545565, 0.005847267882335324, 0.008306814303491698, 0.005402921254984861, 0.005903399719986631, 0.005634756999761733, 0.005706028419893322, 0.00560301641641228, 0.005895254908800992, 0.005432198999103072, 0.005844633092259078, 0.005508964978764916, 0.005862723115485075, 0.005452286720080951, 0.00566047920877937, 0.00558126890533712, 0.005538485768294439, 0.005660564698235586, 0.005363684721550969, 0.005860518141606346, 0.005621602791253217, 0.006969643115109309, 0.005255861955066753, 0.005558332184676168, 0.005995424534735638, 0.005391837907196997, 0.0059774022327389485, 0.005695741509932072, 0.005430350418549118, 0.005796361744382181, 0.005372214418034567, 0.005966829509625948, 0.005307506558723574, 0.00585802455790081, 0.005640493161694775, 0.005872523653061065, 0.005669627649991145, 0.005880404675249444, 0.005294306255162282, 0.005571115580604, 0.005330597629857271, 0.005900777186007174, 0.005311376396241749, 0.0059131513035661255, 0.005673810046404427, 0.005500097907971331]
[174.9720448124096, 189.23382373276328, 179.89979730984442, 157.59948249866937, 180.33482943340718, 172.6968684712778, 178.0499517554022, 174.04897571469235, 130.18499660622254, 173.43758525642355, 179.5117305057697, 172.2280237364213, 169.83694603719144, 170.40743119625168, 178.13499708653083, 181.95527001170632, 191.37096989445797, 179.5322167106968, 176.7052107279893, 181.34990933898928, 173.02516515090736, 178.6300154040448, 175.35922613928017, 183.74257625396731, 168.21604344477927, 165.98904151028563, 178.8047492394169, 175.1350862422313, 171.75609109865385, 172.71888964422675, 182.5856845260956, 176.87575001070746, 128.7966097246417, 171.72595457993384, 179.00930902194114, 174.68303944865005, 180.26244090844907, 175.36626493645548, 190.73249864897753, 128.87785899154258, 186.78118562630462, 172.72000499851734, 187.12654351864316, 176.30441284209442, 181.05395983256082, 124.98624976199943, 188.9938771823038, 173.9334775802129, 180.82901200351174, 170.23726696081096, 183.0032267729457, 177.52090029813616, 188.80916898994897, 190.84438136727158, 183.4335644953252, 121.97186344692952, 178.46588671150923, 170.51146582960183, 170.48555536736666, 177.0952369476477, 177.11186653799277, 169.6421886937027, 181.16368902109306, 128.37730381506154, 173.65499833584948, 192.9971967102018, 175.84194198751322, 196.06613088503047, 175.22944185774696, 192.778091123166, 175.38783885850555, 191.19988786119094, 175.11240071016874, 189.3987893985793, 174.1011739299564, 190.01493296579218, 144.26368850597277, 169.26997476554018, 147.896098335279, 170.2208496365572, 182.47878521462601, 177.3504334519407, 190.3955583571774, 178.8970349082602, 185.30339642095709, 176.57053395588315, 194.90687946509735, 172.48804139508664, 178.0350711807355, 176.93394016463157, 181.17727249142575, 171.62924634177804, 181.447441964672, 128.05025724756575, 177.3420006682124, 169.14147718794555, 176.7781449993839, 169.41666290693232, 179.70678363891057, 168.15173259652963, 129.52906234722536, 170.05337583023356, 192.53763866277382, 168.70232304703282, 175.1751490855756, 174.0663274817285, 178.04198959177796, 173.93963921558714, 173.10026339544729, 168.2540813091331, 151.81130608517046, 178.3675066737418, 170.92024208843034, 180.46311142291844, 172.16174388301792, 177.15032249992487, 172.39844172260044, 179.44787266638787, 178.00857701849873, 179.92285697304325, 185.30355447236872, 171.27383456567503, 180.34333480139045, 170.76444885413179, 192.287419561237, 167.31028989821513, 180.86454534604826, 171.79696484102274, 178.95182380759852, 172.0367786590811, 137.5901458692835, 173.6812700605199, 170.6251590579433, 170.12866769055086, 181.91245096237168, 174.72302402288352, 173.99836705691254, 169.81514479458778, 180.64083132978465, 157.4959601526635, 182.13520564996952, 171.02004220142086, 120.38309314073129, 185.08505913858667, 169.39391662983385, 177.4699423670418, 175.2532455873565, 178.47529360628207, 169.62794916757642, 184.0875122883225, 171.09713889900968, 181.52230116812163, 170.56920142769886, 183.4092833593962, 176.6634878631841, 179.17072568278232, 180.55476565898033, 176.66081977858192, 186.43899705403246, 170.63337674881828, 177.88521123476087, 143.47936952928418, 190.26374903853412, 179.9100821568225, 166.79385991872783, 185.46551606553402, 167.29675552414395, 175.56976528099608, 184.15017870378617, 172.5220136526499, 186.1429798190839, 167.59319139029475, 188.41239081586636, 170.70601021146015, 177.28946234544043, 170.28454189005234, 176.37842583922804, 170.05632353994082, 188.88215977776824, 179.49726325577032, 187.59622643414102, 169.46920184875867, 188.27511465909006, 169.11456322738033, 176.2484101197068, 181.814945248646]
Elapsed: 0.2516305412995749~0.02470707729980703
Time per graph: 0.005784694906886998~0.0005635795677820108
Speed: 174.19981016418663~13.784821481044789
Total Time: 0.2371
best val loss: 0.39333823323249817 test_score: 0.7907

Testing...
Test loss: 0.6322 score: 0.7442 time: 0.22s
test Score 0.7442
Epoch Time List: [0.8492227299138904, 0.8035539700649679, 0.9200324779376388, 0.9774291620124131, 0.772653921158053, 0.8216311109717935, 0.7898320778040215, 0.835273390985094, 0.8780643520876765, 0.8158374711638317, 0.7860900680534542, 0.810779923107475, 0.7951239629182965, 0.8218133290065452, 0.7895649379352108, 0.9490686999633908, 0.801375670125708, 0.9116965311113745, 0.8234068170422688, 0.8086350531084463, 0.8665526129771024, 0.7774529920425266, 0.8160915249027312, 0.776920035132207, 0.8322437390452251, 0.9753848239779472, 0.8411793978884816, 0.8215584130957723, 0.8421639289008453, 0.8296927510527894, 0.7752176669891924, 0.8160718792350963, 0.8881849329918623, 0.8076067661168054, 0.7637769840657711, 0.8040499539347365, 0.7827169599477202, 0.8287257850170135, 0.7740882179932669, 0.964860048959963, 0.8716039760038257, 0.8063112569507211, 0.7783151939511299, 0.7956681159557775, 0.763312506955117, 0.928236442967318, 0.7677529149223119, 0.797460851026699, 0.7759310408728197, 0.8319732900708914, 0.7702829899499193, 0.8265446278965101, 0.8098759420681745, 0.8108954649651423, 0.8759739899542183, 0.9852900529513136, 0.7854804210364819, 0.8237137031974271, 0.7902935699094087, 0.8161649609683082, 0.7822141267824918, 0.8376901769079268, 0.7893723848974332, 0.9149987490382046, 0.935276222997345, 0.8096641659503803, 0.840977763873525, 0.7647117978194728, 0.8519624940818176, 0.7747242889599875, 0.9254963538842276, 0.7710260800085962, 0.8196517250034958, 0.8737408079905435, 0.7965204410720617, 0.9179249219596386, 1.0546731369104236, 0.8089775808621198, 0.8177844180027023, 0.8042388590984046, 0.774023319943808, 0.8276781280292198, 0.7755435100989416, 0.8167096800170839, 0.8060140220914036, 0.8329703392228112, 0.8755327719263732, 0.8162045649951324, 0.7695101008284837, 0.8327302599791437, 0.8129989539738744, 0.8605154609540477, 0.7816360059659928, 0.9745072419755161, 0.7955837449990213, 0.8163368299137801, 0.7893245450686663, 0.8287142909830436, 0.775273010134697, 0.8345665071392432, 0.8756016800180078, 0.8347890828736126, 0.7659835068043321, 0.8246758540626615, 0.7890828680247068, 0.8261280979495496, 0.7740078829228878, 0.8218134450726211, 0.779791948851198, 0.8995910241501406, 0.957244059885852, 0.8216309839626774, 0.8565018259687349, 0.7978534569265321, 0.8270456989994273, 0.8029945759335533, 0.8204697601031512, 0.9194195840973407, 0.8385863251751289, 0.8786045870510861, 0.8641842181095853, 0.8939185729250312, 0.8187263109721243, 0.8262941759312525, 0.8713417699327692, 0.8299418981187046, 0.7588709208648652, 0.8097759059164673, 0.8233086658874527, 0.8275479458970949, 0.8614688070956618, 0.7990886199986562, 0.7892655320465565, 0.8265552369412035, 0.7809006149182096, 0.809759630006738, 0.7875965629937127, 0.8391908800695091, 0.7690786650637165, 0.9280995919834822, 0.8827549089910462, 0.8199754699598998, 1.047784941853024, 0.8149262270890176, 0.8492604900384322, 0.7967496230266988, 0.8110647399444133, 0.8611878649098799, 0.8733338110614568, 0.8211681480752304, 0.8456986959790811, 0.8102534059435129, 0.8246262830216438, 0.7793089739279822, 0.8929247681517154, 0.8601790198590606, 0.8655646621482447, 0.8401798999402672, 0.8130144658498466, 0.8404353618388996, 0.8098513489821926, 0.9499032421736047, 0.8010161600541323, 0.9222067039227113, 0.9121564121451229, 0.9745761769590899, 0.923153750016354, 0.9353976319544017, 0.7876885880250484, 0.8801252798875794, 0.884659313946031, 0.8285589640727267, 0.7704576540272683, 0.8068888910347596, 0.8008950870716944, 0.8505068977829069, 0.823297681985423, 0.8268024530261755, 0.9065006768796593, 0.8308245250955224, 0.8119359369156882, 0.8172959110233933, 0.7783945109695196, 0.826430524000898, 0.7849318649386987, 0.9568347767926753]
Total Epoch List: [93, 93]
Total Time List: [0.24327162595000118, 0.23710686201229692]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288334e50>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6997 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7023 score: 0.4884 time: 0.21s
Epoch 2/1000, LR 0.000000
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6996 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7022 score: 0.4884 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6970;  Loss pred: 0.6970; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6995 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7021 score: 0.4884 time: 0.22s
Epoch 4/1000, LR 0.000060
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6994 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7020 score: 0.4884 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6993 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7018 score: 0.4884 time: 0.21s
Epoch 6/1000, LR 0.000120
Train loss: 0.6967;  Loss pred: 0.6967; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6991 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7016 score: 0.4884 time: 0.22s
Epoch 7/1000, LR 0.000150
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6988 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7013 score: 0.4884 time: 0.31s
Epoch 8/1000, LR 0.000180
Train loss: 0.6963;  Loss pred: 0.6963; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6986 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7010 score: 0.4884 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6983 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7007 score: 0.4884 time: 0.21s
Epoch 10/1000, LR 0.000240
Train loss: 0.6959;  Loss pred: 0.6959; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6981 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7003 score: 0.4884 time: 0.21s
Epoch 11/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6978 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7000 score: 0.4884 time: 0.20s
Epoch 12/1000, LR 0.000270
Train loss: 0.6954;  Loss pred: 0.6954; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6997 score: 0.4884 time: 0.23s
Epoch 13/1000, LR 0.000270
Train loss: 0.6952;  Loss pred: 0.6952; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6973 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6994 score: 0.4884 time: 0.21s
Epoch 14/1000, LR 0.000270
Train loss: 0.6950;  Loss pred: 0.6950; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6990 score: 0.4884 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6948;  Loss pred: 0.6948; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6987 score: 0.4884 time: 0.22s
Epoch 16/1000, LR 0.000270
Train loss: 0.6946;  Loss pred: 0.6946; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6965 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6984 score: 0.4884 time: 0.23s
Epoch 17/1000, LR 0.000270
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6963 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6981 score: 0.4884 time: 0.21s
Epoch 18/1000, LR 0.000270
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6960 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6978 score: 0.4884 time: 0.23s
Epoch 19/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6958 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6975 score: 0.4884 time: 0.20s
Epoch 20/1000, LR 0.000270
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.4884 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6953 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6969 score: 0.4884 time: 0.21s
Epoch 22/1000, LR 0.000270
Train loss: 0.6932;  Loss pred: 0.6932; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6951 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6966 score: 0.4884 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6963 score: 0.4884 time: 0.20s
Epoch 24/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6960 score: 0.4884 time: 0.21s
Epoch 25/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6956 score: 0.4884 time: 0.22s
Epoch 26/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.4884 time: 0.20s
Epoch 27/1000, LR 0.000270
Train loss: 0.6918;  Loss pred: 0.6918; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6949 score: 0.4884 time: 0.22s
Epoch 28/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6946 score: 0.4884 time: 0.21s
Epoch 29/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.4884 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.4884 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6904;  Loss pred: 0.6904; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.4884 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6916 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.4884 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6893;  Loss pred: 0.6893; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.19s
Epoch 34/1000, LR 0.000270
Train loss: 0.6884;  Loss pred: 0.6884; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6904 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.23s
Epoch 35/1000, LR 0.000270
Train loss: 0.6878;  Loss pred: 0.6878; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6897 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.21s
Epoch 36/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.22s
Epoch 37/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6881 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.4884 time: 0.22s
Epoch 38/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6885 score: 0.4884 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6834;  Loss pred: 0.6834; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6863 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6876 score: 0.4884 time: 0.22s
Epoch 40/1000, LR 0.000269
Train loss: 0.6823;  Loss pred: 0.6823; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4884 time: 0.21s
Epoch 41/1000, LR 0.000269
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6841 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.4884 time: 0.27s
Epoch 42/1000, LR 0.000269
Train loss: 0.6796;  Loss pred: 0.6796; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6828 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6840 score: 0.4884 time: 0.21s
Epoch 43/1000, LR 0.000269
Train loss: 0.6780;  Loss pred: 0.6780; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6813 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6826 score: 0.4884 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6757;  Loss pred: 0.6757; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6797 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6810 score: 0.4884 time: 0.22s
Epoch 45/1000, LR 0.000269
Train loss: 0.6732;  Loss pred: 0.6732; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6780 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6793 score: 0.4884 time: 0.23s
Epoch 46/1000, LR 0.000269
Train loss: 0.6704;  Loss pred: 0.6704; Loss self: 0.0000; time: 0.34s
Val loss: 0.6760 score: 0.5227 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6773 score: 0.4884 time: 0.31s
Epoch 47/1000, LR 0.000269
Train loss: 0.6687;  Loss pred: 0.6687; Loss self: 0.0000; time: 0.39s
Val loss: 0.6737 score: 0.5227 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6752 score: 0.4884 time: 0.23s
Epoch 48/1000, LR 0.000269
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 0.43s
Val loss: 0.6712 score: 0.5227 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6727 score: 0.4884 time: 0.23s
Epoch 49/1000, LR 0.000269
Train loss: 0.6624;  Loss pred: 0.6624; Loss self: 0.0000; time: 0.40s
Val loss: 0.6684 score: 0.5227 time: 0.22s
Test loss: 0.6700 score: 0.5581 time: 0.22s
Epoch 50/1000, LR 0.000269
Train loss: 0.6570;  Loss pred: 0.6570; Loss self: 0.0000; time: 0.46s
Val loss: 0.6653 score: 0.5227 time: 0.22s
Test loss: 0.6671 score: 0.5581 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6519;  Loss pred: 0.6519; Loss self: 0.0000; time: 0.40s
Val loss: 0.6620 score: 0.5909 time: 0.24s
Test loss: 0.6639 score: 0.5814 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6485;  Loss pred: 0.6485; Loss self: 0.0000; time: 0.43s
Val loss: 0.6585 score: 0.6136 time: 0.25s
Test loss: 0.6604 score: 0.5814 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.6423;  Loss pred: 0.6423; Loss self: 0.0000; time: 0.38s
Val loss: 0.6547 score: 0.6136 time: 0.22s
Test loss: 0.6567 score: 0.6279 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.6378;  Loss pred: 0.6378; Loss self: 0.0000; time: 0.37s
Val loss: 0.6506 score: 0.6364 time: 0.30s
Test loss: 0.6527 score: 0.6279 time: 0.21s
Epoch 55/1000, LR 0.000269
Train loss: 0.6337;  Loss pred: 0.6337; Loss self: 0.0000; time: 0.45s
Val loss: 0.6462 score: 0.7500 time: 0.22s
Test loss: 0.6484 score: 0.6279 time: 0.25s
Epoch 56/1000, LR 0.000269
Train loss: 0.6287;  Loss pred: 0.6287; Loss self: 0.0000; time: 0.36s
Val loss: 0.6416 score: 0.8182 time: 0.23s
Test loss: 0.6439 score: 0.6977 time: 0.19s
Epoch 57/1000, LR 0.000269
Train loss: 0.6212;  Loss pred: 0.6212; Loss self: 0.0000; time: 0.56s
Val loss: 0.6365 score: 0.8182 time: 0.22s
Test loss: 0.6391 score: 0.7209 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.6125;  Loss pred: 0.6125; Loss self: 0.0000; time: 0.34s
Val loss: 0.6312 score: 0.8409 time: 0.24s
Test loss: 0.6340 score: 0.7209 time: 0.19s
Epoch 59/1000, LR 0.000268
Train loss: 0.6045;  Loss pred: 0.6045; Loss self: 0.0000; time: 0.46s
Val loss: 0.6255 score: 0.8636 time: 0.21s
Test loss: 0.6286 score: 0.7442 time: 0.18s
Epoch 60/1000, LR 0.000268
Train loss: 0.5944;  Loss pred: 0.5944; Loss self: 0.0000; time: 0.30s
Val loss: 0.6195 score: 0.8636 time: 0.19s
Test loss: 0.6228 score: 0.7674 time: 0.18s
Epoch 61/1000, LR 0.000268
Train loss: 0.5914;  Loss pred: 0.5914; Loss self: 0.0000; time: 0.31s
Val loss: 0.6131 score: 0.8864 time: 0.22s
Test loss: 0.6167 score: 0.7674 time: 0.20s
Epoch 62/1000, LR 0.000268
Train loss: 0.5818;  Loss pred: 0.5818; Loss self: 0.0000; time: 0.50s
Val loss: 0.6062 score: 0.8864 time: 0.23s
Test loss: 0.6102 score: 0.7907 time: 0.25s
Epoch 63/1000, LR 0.000268
Train loss: 0.5735;  Loss pred: 0.5735; Loss self: 0.0000; time: 0.51s
Val loss: 0.5989 score: 0.8864 time: 0.22s
Test loss: 0.6033 score: 0.7907 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5621;  Loss pred: 0.5621; Loss self: 0.0000; time: 0.35s
Val loss: 0.5912 score: 0.8864 time: 0.24s
Test loss: 0.5961 score: 0.7907 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.5525;  Loss pred: 0.5525; Loss self: 0.0000; time: 0.40s
Val loss: 0.5831 score: 0.8636 time: 0.22s
Test loss: 0.5887 score: 0.8140 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.5452;  Loss pred: 0.5452; Loss self: 0.0000; time: 0.34s
Val loss: 0.5745 score: 0.8864 time: 0.24s
Test loss: 0.5805 score: 0.8140 time: 0.20s
Epoch 67/1000, LR 0.000268
Train loss: 0.5315;  Loss pred: 0.5315; Loss self: 0.0000; time: 0.50s
Val loss: 0.5656 score: 0.9091 time: 0.24s
Test loss: 0.5718 score: 0.8372 time: 0.31s
Epoch 68/1000, LR 0.000268
Train loss: 0.5165;  Loss pred: 0.5165; Loss self: 0.0000; time: 0.52s
Val loss: 0.5562 score: 0.9091 time: 0.22s
Test loss: 0.5628 score: 0.8372 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.4988;  Loss pred: 0.4988; Loss self: 0.0000; time: 0.32s
Val loss: 0.5464 score: 0.9318 time: 0.23s
Test loss: 0.5532 score: 0.8372 time: 0.20s
Epoch 70/1000, LR 0.000268
Train loss: 0.4863;  Loss pred: 0.4863; Loss self: 0.0000; time: 0.46s
Val loss: 0.5362 score: 0.9318 time: 0.22s
Test loss: 0.5435 score: 0.8372 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.4721;  Loss pred: 0.4721; Loss self: 0.0000; time: 0.37s
Val loss: 0.5255 score: 0.9318 time: 0.23s
Test loss: 0.5336 score: 0.8372 time: 0.20s
Epoch 72/1000, LR 0.000267
Train loss: 0.4466;  Loss pred: 0.4466; Loss self: 0.0000; time: 0.38s
Val loss: 0.5143 score: 0.9318 time: 0.21s
Test loss: 0.5240 score: 0.8372 time: 0.23s
Epoch 73/1000, LR 0.000267
Train loss: 0.4333;  Loss pred: 0.4333; Loss self: 0.0000; time: 0.33s
Val loss: 0.5029 score: 0.9318 time: 0.24s
Test loss: 0.5136 score: 0.8372 time: 0.20s
Epoch 74/1000, LR 0.000267
Train loss: 0.4220;  Loss pred: 0.4220; Loss self: 0.0000; time: 0.37s
Val loss: 0.4914 score: 0.9318 time: 0.23s
Test loss: 0.5021 score: 0.8372 time: 0.23s
Epoch 75/1000, LR 0.000267
Train loss: 0.4031;  Loss pred: 0.4031; Loss self: 0.0000; time: 0.32s
Val loss: 0.4799 score: 0.9318 time: 0.24s
Test loss: 0.4902 score: 0.8605 time: 0.20s
Epoch 76/1000, LR 0.000267
Train loss: 0.3889;  Loss pred: 0.3889; Loss self: 0.0000; time: 0.38s
Val loss: 0.4684 score: 0.9318 time: 0.21s
Test loss: 0.4781 score: 0.8605 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.3730;  Loss pred: 0.3730; Loss self: 0.0000; time: 0.41s
Val loss: 0.4569 score: 0.9091 time: 0.23s
Test loss: 0.4663 score: 0.8605 time: 0.20s
Epoch 78/1000, LR 0.000267
Train loss: 0.3498;  Loss pred: 0.3498; Loss self: 0.0000; time: 0.36s
Val loss: 0.4455 score: 0.9091 time: 0.21s
Test loss: 0.4549 score: 0.8605 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.3223;  Loss pred: 0.3223; Loss self: 0.0000; time: 0.33s
Val loss: 0.4346 score: 0.9091 time: 0.23s
Test loss: 0.4433 score: 0.8837 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.3230;  Loss pred: 0.3230; Loss self: 0.0000; time: 0.39s
Val loss: 0.4238 score: 0.9091 time: 0.21s
Test loss: 0.4327 score: 0.8837 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.2989;  Loss pred: 0.2989; Loss self: 0.0000; time: 0.33s
Val loss: 0.4131 score: 0.8636 time: 0.24s
Test loss: 0.4226 score: 0.8837 time: 0.21s
Epoch 82/1000, LR 0.000267
Train loss: 0.2696;  Loss pred: 0.2696; Loss self: 0.0000; time: 0.39s
Val loss: 0.4025 score: 0.8636 time: 0.22s
Test loss: 0.4135 score: 0.8837 time: 0.23s
Epoch 83/1000, LR 0.000266
Train loss: 0.2747;  Loss pred: 0.2747; Loss self: 0.0000; time: 0.34s
Val loss: 0.3923 score: 0.8636 time: 0.24s
Test loss: 0.4053 score: 0.8837 time: 0.21s
Epoch 84/1000, LR 0.000266
Train loss: 0.2495;  Loss pred: 0.2495; Loss self: 0.0000; time: 0.39s
Val loss: 0.3815 score: 0.8636 time: 0.21s
Test loss: 0.3995 score: 0.8837 time: 0.22s
Epoch 85/1000, LR 0.000266
Train loss: 0.2246;  Loss pred: 0.2246; Loss self: 0.0000; time: 0.37s
Val loss: 0.3702 score: 0.9091 time: 0.32s
Test loss: 0.3966 score: 0.8605 time: 0.20s
Epoch 86/1000, LR 0.000266
Train loss: 0.2428;  Loss pred: 0.2428; Loss self: 0.0000; time: 0.39s
Val loss: 0.3600 score: 0.9091 time: 0.22s
Test loss: 0.3950 score: 0.8605 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.1934;  Loss pred: 0.1934; Loss self: 0.0000; time: 0.34s
Val loss: 0.3512 score: 0.9091 time: 0.25s
Test loss: 0.3928 score: 0.8605 time: 0.20s
Epoch 88/1000, LR 0.000266
Train loss: 0.1893;  Loss pred: 0.1893; Loss self: 0.0000; time: 0.38s
Val loss: 0.3447 score: 0.9091 time: 0.23s
Test loss: 0.3864 score: 0.8605 time: 0.22s
Epoch 89/1000, LR 0.000266
Train loss: 0.1915;  Loss pred: 0.1915; Loss self: 0.0000; time: 0.33s
Val loss: 0.3414 score: 0.8636 time: 0.22s
Test loss: 0.3781 score: 0.8837 time: 0.20s
Epoch 90/1000, LR 0.000266
Train loss: 0.1597;  Loss pred: 0.1597; Loss self: 0.0000; time: 0.39s
Val loss: 0.3402 score: 0.8636 time: 0.21s
Test loss: 0.3710 score: 0.8837 time: 0.23s
Epoch 91/1000, LR 0.000266
Train loss: 0.1768;  Loss pred: 0.1768; Loss self: 0.0000; time: 0.33s
Val loss: 0.3440 score: 0.8864 time: 0.24s
Test loss: 0.3634 score: 0.8837 time: 0.20s
     INFO: Early stopping counter 1 of 2
Epoch 92/1000, LR 0.000266
Train loss: 0.1436;  Loss pred: 0.1436; Loss self: 0.0000; time: 0.38s
Val loss: 0.3492 score: 0.8864 time: 0.21s
Test loss: 0.3583 score: 0.8837 time: 0.23s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 089,   Train_Loss: 0.1597,   Val_Loss: 0.3402,   Val_Precision: 0.9444,   Val_Recall: 0.7727,   Val_accuracy: 0.8500,   Val_Score: 0.8636,   Val_Loss: 0.3402,   Test_Precision: 0.9000,   Test_Recall: 0.8571,   Test_accuracy: 0.8780,   Test_Score: 0.8837,   Test_loss: 0.3710


[0.2514687420334667, 0.23251657199580222, 0.24458059796597809, 0.27918873401358724, 0.24399058206472546, 0.25478168996050954, 0.24712166201788932, 0.25280240701977164, 0.3379805749282241, 0.2536935689859092, 0.2451093300478533, 0.25547526497393847, 0.2590720159932971, 0.2582047020550817, 0.24700368102639914, 0.24181767308618873, 0.22991992998868227, 0.2450813609175384, 0.24900227796752006, 0.24262488004751503, 0.2542982690501958, 0.24631918605882674, 0.2509135160362348, 0.23946545703802258, 0.2615683920448646, 0.2650777400704101, 0.24607847491279244, 0.2512346380390227, 0.25617723201867193, 0.25474920601118356, 0.2409827479859814, 0.24876219604630023, 0.3416238990612328, 0.2562221890548244, 0.24579727300442755, 0.2518847859464586, 0.24408856208901852, 0.25090344494674355, 0.23068957997020334, 0.3414085269905627, 0.2355697649763897, 0.2547475609462708, 0.23513500101398677, 0.2495683419983834, 0.2430214729392901, 0.35203872493002564, 0.2328117749420926, 0.2529702769825235, 0.24332378699909896, 0.25846279598772526, 0.2404329190030694, 0.24785813910420984, 0.23303952999413013, 0.23055433796253055, 0.23986885999329388, 0.36073893401771784, 0.24654571700375527, 0.2580471629044041, 0.2580863810144365, 0.24845388706307858, 0.24843055894598365, 0.259369443054311, 0.24287427705712616, 0.34273971093352884, 0.25337594898883253, 0.22798258601687849, 0.25022471603006124, 0.224414078053087, 0.2510993559844792, 0.2282417039386928, 0.250872582080774, 0.2301256579812616, 0.2512671850854531, 0.23231405089609325, 0.25272661296185106, 0.23156074795406312, 0.3049970540450886, 0.25993977999314666, 0.29750615800730884, 0.25848772400058806, 0.2411239199573174, 0.24809637700673193, 0.23109782801475376, 0.24595153308473527, 0.2374484270112589, 0.2491922010667622, 0.22574883000925183, 0.2550901479553431, 0.24714231700636446, 0.24868038296699524, 0.2428560679545626, 0.25636656302958727, 0.2424944629892707, 0.3358056510332972, 0.2424693520879373, 0.2542250470723957, 0.2432427379535511, 0.25381210597697645, 0.23927866900339723, 0.25572142098098993, 0.3319718310376629, 0.25286178407259285, 0.22333295608405024, 0.2548868280136958, 0.2454686079872772, 0.24703226995188743, 0.24151606089435518, 0.24721219495404512, 0.24841094494331628, 0.25556586601305753, 0.2832463609520346, 0.24107529898174107, 0.25157933006994426, 0.23827584297396243, 0.2497651280136779, 0.24273170600645244, 0.24942220805678517, 0.2396239050431177, 0.24156139395199716, 0.23899131396319717, 0.23205167392734438, 0.2510599480010569, 0.2384340959833935, 0.25180885300505906, 0.22362357401289046, 0.25700750399846584, 0.23774698306806386, 0.2502954580122605, 0.24028813501354307, 0.24994655407499522, 0.31252238107845187, 0.24757994909305125, 0.25201441708486527, 0.25274987798184156, 0.23637744295410812, 0.24610379908699542, 0.2471287560183555, 0.2532165199518204, 0.2380414200015366, 0.27302287600468844, 0.23608834901824594, 0.25143251894041896, 0.357193015050143, 0.23232561396434903, 0.25384618795942515, 0.2422945509897545, 0.24535922205541283, 0.24092970590572804, 0.25349596107844263, 0.2335845569614321, 0.2513192229671404, 0.2368854940868914, 0.2520970939658582, 0.2344483289634809, 0.2434006059775129, 0.23999456292949617, 0.23815488803666085, 0.24340428202413023, 0.23063844302669168, 0.2520022800890729, 0.24172892002388835, 0.2996946539497003, 0.22600206406787038, 0.2390082839410752, 0.25780325499363244, 0.23184903000947088, 0.25702829600777477, 0.24491688492707908, 0.23350506799761206, 0.24924355500843376, 0.2310052199754864, 0.25657366891391575, 0.2282227820251137, 0.25189505598973483, 0.24254120595287532, 0.25251851708162576, 0.24379398894961923, 0.25285740103572607, 0.22765516897197813, 0.239557969965972, 0.22921569808386266, 0.2537334189983085, 0.22838918503839523, 0.2542655060533434, 0.24397383199539036, 0.23650421004276723, 0.21752514597028494, 0.23052186192944646, 0.22398264904040843, 0.2345332329859957, 0.21851921710185707, 0.22801069705747068, 0.3104022489860654, 0.23123001295607537, 0.21000663097947836, 0.21575848094653338, 0.20271198893897235, 0.2341432519024238, 0.21878970798570663, 0.23192583699710667, 0.22214642900507897, 0.22982445999514312, 0.2099232979817316, 0.23677005001809448, 0.20359436003491282, 0.23208948294632137, 0.2098043089499697, 0.22243636404164135, 0.2085793809965253, 0.21472688799258322, 0.22410975897219032, 0.20887053792830557, 0.22390561702195555, 0.21557376603595912, 0.24785103905014694, 0.22308141400571913, 0.22477813996374607, 0.2248328410787508, 0.1948596970178187, 0.23407871997915208, 0.2103141020052135, 0.22469840908888727, 0.2244987990707159, 0.23250820499379188, 0.2275827240664512, 0.21825444500427693, 0.2778653500135988, 0.21531607303768396, 0.23192708694841713, 0.2247515949420631, 0.23584883904550225, 0.31241367803886533, 0.2335075120208785, 0.2321884889388457, 0.2272480649407953, 0.22379671002272516, 0.21598039695527405, 0.2305730130756274, 0.20766125398222357, 0.20996218698564917, 0.25988945993594825, 0.19464644591789693, 0.20992430509068072, 0.19453638698905706, 0.18391751893796027, 0.17940449400339276, 0.20027025695890188, 0.252651521936059, 0.230251160915941, 0.23326795594766736, 0.22559702093712986, 0.20490527304355055, 0.31147828500252217, 0.2488100939663127, 0.20215429097879678, 0.2319533759728074, 0.20504053600598127, 0.23534831893630326, 0.20750304800458252, 0.2327542450511828, 0.20145940105430782, 0.23079553397838026, 0.2085237669525668, 0.23290221102070063, 0.21986913192085922, 0.2327974420040846, 0.21698558097705245, 0.23051801696419716, 0.2157002289313823, 0.22921831894200295, 0.20307858497835696, 0.23376686999108642, 0.2035885970108211, 0.22070603596512228, 0.20303264795802534, 0.23649586795363575, 0.20578708592802286, 0.2297257330501452]
[0.005715198682578789, 0.005284467545359142, 0.005558649953772229, 0.006345198500308801, 0.005545240501471033, 0.005790492953647944, 0.0056164014094974846, 0.005745509250449355, 0.007681376702914184, 0.005765762931497937, 0.005570666591996665, 0.005806256022134965, 0.005888000363484025, 0.005868288683070039, 0.005613720023327253, 0.00549585620650429, 0.005225452954288234, 0.005570030929944055, 0.005659142681080001, 0.005514201819261705, 0.005779506114777178, 0.00559816331951879, 0.005702579909914427, 0.00544239675086415, 0.005944736182837832, 0.00602449409250932, 0.005592692611654374, 0.0057098781372505155, 0.00582220981860618, 0.005789754682072354, 0.005476880636045032, 0.005653686273779551, 0.0077641795241189275, 0.005823231569427828, 0.005586301659191536, 0.005724654226055877, 0.005547467320204966, 0.005702351021516899, 0.005242944999322804, 0.00775928470433097, 0.005353858294917948, 0.005789717294233428, 0.005343977295772426, 0.005672007772690532, 0.0055232152940747755, 0.008000880112046038, 0.005291176703229377, 0.005749324476875534, 0.00553008606816134, 0.005874154454266483, 0.0054643845227970314, 0.005633139525095678, 0.005296352954412048, 0.0052398713173302395, 0.005451564999847588, 0.008198612136766315, 0.005603311750085347, 0.005864708247827366, 0.00586559956850992, 0.005646679251433604, 0.005646149066954174, 0.005894760069416158, 0.0055198699331165035, 0.007789538884852928, 0.00575854429520074, 0.005181422409474511, 0.0056869253643195734, 0.005100319955751977, 0.0057068035451017995, 0.00518731145315211, 0.005701649592744864, 0.005230128590483218, 0.005710617842851207, 0.005279864793093028, 0.005743786658223888, 0.005262744271683253, 0.006931751228297468, 0.005907722272571515, 0.006761503591075201, 0.005874721000013365, 0.005480089089939032, 0.005638554022880271, 0.005252223363971676, 0.0055898075701076196, 0.0053965551593467935, 0.005663459115153687, 0.0051306552274829964, 0.0057975033626214345, 0.0056168708410537374, 0.005651826885613528, 0.005519456089876423, 0.005826512796126984, 0.005511237795210697, 0.0078094337449604, 0.005638822141579937, 0.0059122103970324575, 0.005656807859384909, 0.0059026071157436385, 0.005564620209381331, 0.00594700979025558, 0.007720275140410766, 0.005880506606339369, 0.005193789676373261, 0.005927600651481297, 0.005708572278773889, 0.00574493651050901, 0.005616652578938492, 0.00574912081288477, 0.0057769987196120065, 0.005943392232861803, 0.00658712467330313, 0.0056064023019009555, 0.005850682094649866, 0.00554129867381308, 0.0058084913491553, 0.005644923395498894, 0.005800516466436864, 0.00557264895449111, 0.005617706836092957, 0.005557937534027841, 0.00539655055644987, 0.0058386034418850445, 0.005544978976357988, 0.005856019837326955, 0.005200548232857918, 0.0059769186976387405, 0.005528999606234043, 0.005820824604936291, 0.005588096163105653, 0.0058127105598836095, 0.007267962350661672, 0.005757673234722122, 0.005860800397322449, 0.0058779041391125945, 0.0054971498361420495, 0.0057233441648138475, 0.005747180372519895, 0.005888756277949311, 0.005535846976779921, 0.0063493692094113595, 0.0054904267213545565, 0.005847267882335324, 0.008306814303491698, 0.005402921254984861, 0.005903399719986631, 0.005634756999761733, 0.005706028419893322, 0.00560301641641228, 0.005895254908800992, 0.005432198999103072, 0.005844633092259078, 0.005508964978764916, 0.005862723115485075, 0.005452286720080951, 0.00566047920877937, 0.00558126890533712, 0.005538485768294439, 0.005660564698235586, 0.005363684721550969, 0.005860518141606346, 0.005621602791253217, 0.006969643115109309, 0.005255861955066753, 0.005558332184676168, 0.005995424534735638, 0.005391837907196997, 0.0059774022327389485, 0.005695741509932072, 0.005430350418549118, 0.005796361744382181, 0.005372214418034567, 0.005966829509625948, 0.005307506558723574, 0.00585802455790081, 0.005640493161694775, 0.005872523653061065, 0.005669627649991145, 0.005880404675249444, 0.005294306255162282, 0.005571115580604, 0.005330597629857271, 0.005900777186007174, 0.005311376396241749, 0.0059131513035661255, 0.005673810046404427, 0.005500097907971331, 0.005058724324890348, 0.005360973533242941, 0.00520889881489322, 0.005454261232232458, 0.005081842258182722, 0.005302574350173736, 0.007218656953164312, 0.005377442161769195, 0.004883875139057636, 0.005017639091779846, 0.004714232300906334, 0.0054451919047075305, 0.005088132743853642, 0.005393624116211783, 0.005166196023373929, 0.0053447548836079795, 0.004881937162365851, 0.005506280232978941, 0.004734752558951461, 0.005397429835960962, 0.004879169975580691, 0.005172938698642823, 0.00485068327898896, 0.004993648557967052, 0.00521185485981838, 0.004857454370425711, 0.005207107372603617, 0.005013343396185096, 0.005763977652328999, 0.00518793986059812, 0.005227398603808048, 0.005228670722761646, 0.004531620860879504, 0.005443691162305863, 0.004891025628028221, 0.005225544397415983, 0.005220902303970138, 0.00540716755799516, 0.005292621489917469, 0.005075684767541324, 0.006461984884037182, 0.005007350535760092, 0.00539365318484691, 0.005226781277722398, 0.0054848567219884245, 0.007265434372996868, 0.0054304072562995, 0.005399732300903388, 0.005284838719553379, 0.0052045746516912825, 0.00502279992919242, 0.005362163094782033, 0.004829331487958687, 0.004882841557805794, 0.006043940928742983, 0.004526661532974347, 0.004881960583504203, 0.004524102023001327, 0.004277151603208378, 0.004172197534962622, 0.004657447836253532, 0.005875616789210674, 0.005354678160835838, 0.0054248361848294735, 0.005246442347375113, 0.004765238907989548, 0.007243681046570283, 0.005786281255030528, 0.004701262580902251, 0.005394264557507149, 0.004768384558278634, 0.005473216719448913, 0.004825652279176338, 0.005412889419794949, 0.0046851023501001815, 0.005367337999497216, 0.004849389929129461, 0.005416330488853503, 0.005113235626066494, 0.005413894000094991, 0.005046176301791917, 0.005360884115446446, 0.005016284393753076, 0.0053306585800465805, 0.004722757790194348, 0.00543643883700201, 0.004734618535135375, 0.0051326985108167975, 0.004721689487395938, 0.005499903905898506, 0.004785746184372624, 0.005342458908142912]
[174.9720448124096, 189.23382373276328, 179.89979730984442, 157.59948249866937, 180.33482943340718, 172.6968684712778, 178.0499517554022, 174.04897571469235, 130.18499660622254, 173.43758525642355, 179.5117305057697, 172.2280237364213, 169.83694603719144, 170.40743119625168, 178.13499708653083, 181.95527001170632, 191.37096989445797, 179.5322167106968, 176.7052107279893, 181.34990933898928, 173.02516515090736, 178.6300154040448, 175.35922613928017, 183.74257625396731, 168.21604344477927, 165.98904151028563, 178.8047492394169, 175.1350862422313, 171.75609109865385, 172.71888964422675, 182.5856845260956, 176.87575001070746, 128.7966097246417, 171.72595457993384, 179.00930902194114, 174.68303944865005, 180.26244090844907, 175.36626493645548, 190.73249864897753, 128.87785899154258, 186.78118562630462, 172.72000499851734, 187.12654351864316, 176.30441284209442, 181.05395983256082, 124.98624976199943, 188.9938771823038, 173.9334775802129, 180.82901200351174, 170.23726696081096, 183.0032267729457, 177.52090029813616, 188.80916898994897, 190.84438136727158, 183.4335644953252, 121.97186344692952, 178.46588671150923, 170.51146582960183, 170.48555536736666, 177.0952369476477, 177.11186653799277, 169.6421886937027, 181.16368902109306, 128.37730381506154, 173.65499833584948, 192.9971967102018, 175.84194198751322, 196.06613088503047, 175.22944185774696, 192.778091123166, 175.38783885850555, 191.19988786119094, 175.11240071016874, 189.3987893985793, 174.1011739299564, 190.01493296579218, 144.26368850597277, 169.26997476554018, 147.896098335279, 170.2208496365572, 182.47878521462601, 177.3504334519407, 190.3955583571774, 178.8970349082602, 185.30339642095709, 176.57053395588315, 194.90687946509735, 172.48804139508664, 178.0350711807355, 176.93394016463157, 181.17727249142575, 171.62924634177804, 181.447441964672, 128.05025724756575, 177.3420006682124, 169.14147718794555, 176.7781449993839, 169.41666290693232, 179.70678363891057, 168.15173259652963, 129.52906234722536, 170.05337583023356, 192.53763866277382, 168.70232304703282, 175.1751490855756, 174.0663274817285, 178.04198959177796, 173.93963921558714, 173.10026339544729, 168.2540813091331, 151.81130608517046, 178.3675066737418, 170.92024208843034, 180.46311142291844, 172.16174388301792, 177.15032249992487, 172.39844172260044, 179.44787266638787, 178.00857701849873, 179.92285697304325, 185.30355447236872, 171.27383456567503, 180.34333480139045, 170.76444885413179, 192.287419561237, 167.31028989821513, 180.86454534604826, 171.79696484102274, 178.95182380759852, 172.0367786590811, 137.5901458692835, 173.6812700605199, 170.6251590579433, 170.12866769055086, 181.91245096237168, 174.72302402288352, 173.99836705691254, 169.81514479458778, 180.64083132978465, 157.4959601526635, 182.13520564996952, 171.02004220142086, 120.38309314073129, 185.08505913858667, 169.39391662983385, 177.4699423670418, 175.2532455873565, 178.47529360628207, 169.62794916757642, 184.0875122883225, 171.09713889900968, 181.52230116812163, 170.56920142769886, 183.4092833593962, 176.6634878631841, 179.17072568278232, 180.55476565898033, 176.66081977858192, 186.43899705403246, 170.63337674881828, 177.88521123476087, 143.47936952928418, 190.26374903853412, 179.9100821568225, 166.79385991872783, 185.46551606553402, 167.29675552414395, 175.56976528099608, 184.15017870378617, 172.5220136526499, 186.1429798190839, 167.59319139029475, 188.41239081586636, 170.70601021146015, 177.28946234544043, 170.28454189005234, 176.37842583922804, 170.05632353994082, 188.88215977776824, 179.49726325577032, 187.59622643414102, 169.46920184875867, 188.27511465909006, 169.11456322738033, 176.2484101197068, 181.814945248646, 197.67829511478192, 186.53328426247305, 191.97915635082262, 183.34288685888532, 196.77903193272317, 188.58764327693686, 138.5299241241334, 185.96201872880712, 204.75543938515884, 199.29691667905158, 212.12361550527436, 183.64825657209073, 196.53575296516763, 185.40409536405568, 193.56601946105056, 187.09931919739404, 204.83672090432782, 181.6108076030471, 211.2042789035328, 185.27336721218524, 204.9528926036207, 193.3137155215779, 206.1565232122169, 200.25438081832232, 191.87027016229067, 205.86914950522927, 192.0452044567669, 199.4676847313013, 173.49130415103866, 192.75474019945744, 191.299741188958, 191.25319857048223, 220.67159427055827, 183.6988855878473, 204.45609490767322, 191.3676210452823, 191.53777293238542, 184.93970998205472, 188.94228538825539, 197.01775145590904, 154.75121312497427, 199.70641017809325, 185.40309614445175, 191.32233527012178, 182.32016818070502, 137.63801978814337, 184.14825128261938, 185.19436599342112, 189.22053312622373, 192.1386601064583, 199.0921426489673, 186.49190304806444, 207.0679974015804, 204.79878123454216, 165.4549592376608, 220.91335804886808, 204.83573820299344, 221.03833974473272, 233.80045711961196, 239.68184430868726, 214.7098658230821, 170.1948979784196, 186.75258717022598, 184.33736354961184, 190.60535383569353, 209.85306703581406, 138.05135725481412, 172.822570477474, 212.70881657669148, 185.38208301413565, 209.7146293001577, 182.7079122313081, 207.22587168478788, 184.74421375448716, 213.44250888747754, 186.31209737372137, 206.21150590369524, 184.6268432212441, 195.57088175286756, 184.70993336449777, 198.1698498415317, 186.53639557674373, 199.3507388148345, 187.59408147862695, 211.74069144012753, 183.94394381735785, 211.21025750629084, 194.8292887050683, 211.7885986932001, 181.82135853819656, 208.95383112155005, 187.17972701218383]
Elapsed: 0.24258751863958783~0.027145118609173884
Time per graph: 0.005596623671186774~0.0006101081073767186
Speed: 180.53990553438373~17.443274361168026
Total Time: 0.2305
best val loss: 0.3401727080345154 test_score: 0.8837

Testing...
Test loss: 0.5532 score: 0.8372 time: 0.21s
test Score 0.8372
Epoch Time List: [0.8492227299138904, 0.8035539700649679, 0.9200324779376388, 0.9774291620124131, 0.772653921158053, 0.8216311109717935, 0.7898320778040215, 0.835273390985094, 0.8780643520876765, 0.8158374711638317, 0.7860900680534542, 0.810779923107475, 0.7951239629182965, 0.8218133290065452, 0.7895649379352108, 0.9490686999633908, 0.801375670125708, 0.9116965311113745, 0.8234068170422688, 0.8086350531084463, 0.8665526129771024, 0.7774529920425266, 0.8160915249027312, 0.776920035132207, 0.8322437390452251, 0.9753848239779472, 0.8411793978884816, 0.8215584130957723, 0.8421639289008453, 0.8296927510527894, 0.7752176669891924, 0.8160718792350963, 0.8881849329918623, 0.8076067661168054, 0.7637769840657711, 0.8040499539347365, 0.7827169599477202, 0.8287257850170135, 0.7740882179932669, 0.964860048959963, 0.8716039760038257, 0.8063112569507211, 0.7783151939511299, 0.7956681159557775, 0.763312506955117, 0.928236442967318, 0.7677529149223119, 0.797460851026699, 0.7759310408728197, 0.8319732900708914, 0.7702829899499193, 0.8265446278965101, 0.8098759420681745, 0.8108954649651423, 0.8759739899542183, 0.9852900529513136, 0.7854804210364819, 0.8237137031974271, 0.7902935699094087, 0.8161649609683082, 0.7822141267824918, 0.8376901769079268, 0.7893723848974332, 0.9149987490382046, 0.935276222997345, 0.8096641659503803, 0.840977763873525, 0.7647117978194728, 0.8519624940818176, 0.7747242889599875, 0.9254963538842276, 0.7710260800085962, 0.8196517250034958, 0.8737408079905435, 0.7965204410720617, 0.9179249219596386, 1.0546731369104236, 0.8089775808621198, 0.8177844180027023, 0.8042388590984046, 0.774023319943808, 0.8276781280292198, 0.7755435100989416, 0.8167096800170839, 0.8060140220914036, 0.8329703392228112, 0.8755327719263732, 0.8162045649951324, 0.7695101008284837, 0.8327302599791437, 0.8129989539738744, 0.8605154609540477, 0.7816360059659928, 0.9745072419755161, 0.7955837449990213, 0.8163368299137801, 0.7893245450686663, 0.8287142909830436, 0.775273010134697, 0.8345665071392432, 0.8756016800180078, 0.8347890828736126, 0.7659835068043321, 0.8246758540626615, 0.7890828680247068, 0.8261280979495496, 0.7740078829228878, 0.8218134450726211, 0.779791948851198, 0.8995910241501406, 0.957244059885852, 0.8216309839626774, 0.8565018259687349, 0.7978534569265321, 0.8270456989994273, 0.8029945759335533, 0.8204697601031512, 0.9194195840973407, 0.8385863251751289, 0.8786045870510861, 0.8641842181095853, 0.8939185729250312, 0.8187263109721243, 0.8262941759312525, 0.8713417699327692, 0.8299418981187046, 0.7588709208648652, 0.8097759059164673, 0.8233086658874527, 0.8275479458970949, 0.8614688070956618, 0.7990886199986562, 0.7892655320465565, 0.8265552369412035, 0.7809006149182096, 0.809759630006738, 0.7875965629937127, 0.8391908800695091, 0.7690786650637165, 0.9280995919834822, 0.8827549089910462, 0.8199754699598998, 1.047784941853024, 0.8149262270890176, 0.8492604900384322, 0.7967496230266988, 0.8110647399444133, 0.8611878649098799, 0.8733338110614568, 0.8211681480752304, 0.8456986959790811, 0.8102534059435129, 0.8246262830216438, 0.7793089739279822, 0.8929247681517154, 0.8601790198590606, 0.8655646621482447, 0.8401798999402672, 0.8130144658498466, 0.8404353618388996, 0.8098513489821926, 0.9499032421736047, 0.8010161600541323, 0.9222067039227113, 0.9121564121451229, 0.9745761769590899, 0.923153750016354, 0.9353976319544017, 0.7876885880250484, 0.8801252798875794, 0.884659313946031, 0.8285589640727267, 0.7704576540272683, 0.8068888910347596, 0.8008950870716944, 0.8505068977829069, 0.823297681985423, 0.8268024530261755, 0.9065006768796593, 0.8308245250955224, 0.8119359369156882, 0.8172959110233933, 0.7783945109695196, 0.826430524000898, 0.7849318649386987, 0.9568347767926753, 0.8411535389022902, 0.8895682709990069, 0.8139480840181932, 0.8513052760390565, 0.7930049479473382, 0.8067412019008771, 0.8787040211027488, 0.8277416210621595, 0.8035451330943033, 0.8179297680035233, 0.7564490890363231, 0.8209080820670351, 0.7794966280926019, 0.8215816270094365, 0.7926602251827717, 0.8938431589631364, 0.8094050629297271, 0.8480199979385361, 0.7989704050123692, 0.8330484080361202, 0.7985833530547097, 0.8273158092051744, 0.7989737930474803, 0.9206922618905082, 0.8240535719087347, 0.8569167339010164, 0.8720962250372395, 0.940716783166863, 0.8824669569730759, 0.8342882940778509, 0.9463559910655022, 0.8219731971621513, 0.8074500460643321, 0.9122716101119295, 0.810785045963712, 0.848272294853814, 0.7925909409532323, 0.9826090269489214, 0.8572576910955831, 0.8832879129331559, 0.9187953589716926, 0.8412225799402222, 0.8650274290703237, 0.8297394409310073, 0.8272376201348379, 0.8827284559374675, 0.8585325960302725, 0.881380297942087, 0.835873925127089, 0.9010181840276346, 0.852125477977097, 0.9056294329930097, 0.8015756001695991, 0.8748319528531283, 0.9207272488856688, 0.779601562069729, 0.9927692158380523, 0.7732584530021995, 0.8530025938525796, 0.6637979201041162, 0.7253730590455234, 0.9781826708931476, 0.9535665909061208, 0.8169651238713413, 0.8478654510108754, 0.7794205560348928, 1.0415132360067219, 0.9856576279271394, 0.7574050240218639, 0.905504340888001, 0.794201674987562, 0.818621956044808, 0.7749190960312262, 0.8333599588368088, 0.761191519908607, 0.8174105240032077, 0.8515087650157511, 0.8016519631491974, 0.7776295731309801, 0.8239437469746917, 0.7804808941436931, 0.8329103039577603, 0.7898886208422482, 0.8229229440912604, 0.8910089031560346, 0.8435482219792902, 0.7877349689370021, 0.8255329800304025, 0.7544236418325454, 0.8356355410069227, 0.7727767878677696, 0.8143709609284997]
Total Epoch List: [93, 93, 92]
Total Time List: [0.24327162595000118, 0.23710686201229692, 0.2305127769941464]
========================training times:8========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288337af0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.4884 time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.32s
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6948 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5000 time: 0.25s
Epoch 4/1000, LR 0.000060
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6938 score: 0.5000 time: 0.23s
Epoch 5/1000, LR 0.000090
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6946 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5000 time: 0.25s
Epoch 6/1000, LR 0.000120
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6945 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6936 score: 0.5000 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5000 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6943 score: 0.4884 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5000 time: 0.32s
Epoch 9/1000, LR 0.000210
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.25s
Epoch 10/1000, LR 0.000240
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6940 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5000 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5000 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5000 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5000 time: 0.23s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.4884 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5000 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5000 time: 0.33s
Epoch 16/1000, LR 0.000270
Train loss: 0.6921;  Loss pred: 0.6921; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.5000 time: 0.25s
Epoch 17/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.4884 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.5000 time: 0.24s
Epoch 18/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.4884 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.5000 time: 0.25s
Epoch 19/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5000 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.4884 time: 0.26s
Test loss: 0.6911 score: 0.5227 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.4884 time: 0.26s
Test loss: 0.6907 score: 0.5455 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6910;  Loss pred: 0.6910; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.4884 time: 0.24s
Test loss: 0.6904 score: 0.6364 time: 0.25s
Epoch 23/1000, LR 0.000270
Train loss: 0.6907;  Loss pred: 0.6907; Loss self: 0.0000; time: 0.29s
Val loss: 0.6915 score: 0.6047 time: 0.25s
Test loss: 0.6900 score: 0.6364 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6903;  Loss pred: 0.6903; Loss self: 0.0000; time: 0.32s
Val loss: 0.6912 score: 0.6512 time: 0.25s
Test loss: 0.6897 score: 0.6818 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.28s
Val loss: 0.6909 score: 0.6279 time: 0.26s
Test loss: 0.6892 score: 0.7045 time: 0.25s
Epoch 26/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.33s
Val loss: 0.6905 score: 0.6744 time: 0.26s
Test loss: 0.6888 score: 0.7045 time: 0.25s
Epoch 27/1000, LR 0.000270
Train loss: 0.6892;  Loss pred: 0.6892; Loss self: 0.0000; time: 0.28s
Val loss: 0.6901 score: 0.6512 time: 0.26s
Test loss: 0.6883 score: 0.6818 time: 0.23s
Epoch 28/1000, LR 0.000270
Train loss: 0.6887;  Loss pred: 0.6887; Loss self: 0.0000; time: 0.34s
Val loss: 0.6897 score: 0.6977 time: 0.25s
Test loss: 0.6877 score: 0.7045 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6880;  Loss pred: 0.6880; Loss self: 0.0000; time: 0.31s
Val loss: 0.6893 score: 0.7209 time: 0.26s
Test loss: 0.6871 score: 0.7727 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.33s
Val loss: 0.6887 score: 0.6744 time: 0.26s
Test loss: 0.6864 score: 0.7955 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.33s
Val loss: 0.6882 score: 0.6744 time: 0.26s
Test loss: 0.6857 score: 0.7727 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6860;  Loss pred: 0.6860; Loss self: 0.0000; time: 0.42s
Val loss: 0.6876 score: 0.6279 time: 0.25s
Test loss: 0.6848 score: 0.7500 time: 0.25s
Epoch 33/1000, LR 0.000270
Train loss: 0.6852;  Loss pred: 0.6852; Loss self: 0.0000; time: 0.28s
Val loss: 0.6868 score: 0.6279 time: 0.26s
Test loss: 0.6839 score: 0.7273 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6844;  Loss pred: 0.6844; Loss self: 0.0000; time: 0.34s
Val loss: 0.6860 score: 0.6279 time: 0.24s
Test loss: 0.6829 score: 0.7045 time: 0.25s
Epoch 35/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.27s
Val loss: 0.6852 score: 0.6512 time: 0.26s
Test loss: 0.6818 score: 0.7045 time: 0.24s
Epoch 36/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.32s
Val loss: 0.6842 score: 0.6279 time: 0.25s
Test loss: 0.6807 score: 0.7045 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6810;  Loss pred: 0.6810; Loss self: 0.0000; time: 0.28s
Val loss: 0.6832 score: 0.6279 time: 0.26s
Test loss: 0.6794 score: 0.7045 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6794;  Loss pred: 0.6794; Loss self: 0.0000; time: 0.35s
Val loss: 0.6820 score: 0.6279 time: 0.25s
Test loss: 0.6779 score: 0.7045 time: 0.25s
Epoch 39/1000, LR 0.000269
Train loss: 0.6779;  Loss pred: 0.6779; Loss self: 0.0000; time: 0.28s
Val loss: 0.6807 score: 0.6279 time: 0.26s
Test loss: 0.6763 score: 0.7045 time: 0.32s
Epoch 40/1000, LR 0.000269
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.36s
Val loss: 0.6793 score: 0.6279 time: 0.25s
Test loss: 0.6746 score: 0.7045 time: 0.27s
Epoch 41/1000, LR 0.000269
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.41s
Val loss: 0.6776 score: 0.6279 time: 0.25s
Test loss: 0.6727 score: 0.7045 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6720;  Loss pred: 0.6720; Loss self: 0.0000; time: 0.32s
Val loss: 0.6759 score: 0.6279 time: 0.25s
Test loss: 0.6707 score: 0.7045 time: 0.25s
Epoch 43/1000, LR 0.000269
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.33s
Val loss: 0.6739 score: 0.6279 time: 0.25s
Test loss: 0.6684 score: 0.6818 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.33s
Val loss: 0.6717 score: 0.6279 time: 0.24s
Test loss: 0.6659 score: 0.6818 time: 0.26s
Epoch 45/1000, LR 0.000269
Train loss: 0.6640;  Loss pred: 0.6640; Loss self: 0.0000; time: 0.30s
Val loss: 0.6694 score: 0.6279 time: 0.27s
Test loss: 0.6632 score: 0.6818 time: 0.23s
Epoch 46/1000, LR 0.000269
Train loss: 0.6606;  Loss pred: 0.6606; Loss self: 0.0000; time: 0.33s
Val loss: 0.6667 score: 0.6744 time: 0.24s
Test loss: 0.6603 score: 0.7045 time: 0.32s
Epoch 47/1000, LR 0.000269
Train loss: 0.6558;  Loss pred: 0.6558; Loss self: 0.0000; time: 0.42s
Val loss: 0.6639 score: 0.6744 time: 0.24s
Test loss: 0.6572 score: 0.7045 time: 0.26s
Epoch 48/1000, LR 0.000269
Train loss: 0.6537;  Loss pred: 0.6537; Loss self: 0.0000; time: 0.29s
Val loss: 0.6608 score: 0.6977 time: 0.25s
Test loss: 0.6537 score: 0.7045 time: 0.24s
Epoch 49/1000, LR 0.000269
Train loss: 0.6504;  Loss pred: 0.6504; Loss self: 0.0000; time: 0.41s
Val loss: 0.6574 score: 0.6977 time: 0.26s
Test loss: 0.6500 score: 0.7273 time: 0.24s
Epoch 50/1000, LR 0.000269
Train loss: 0.6443;  Loss pred: 0.6443; Loss self: 0.0000; time: 0.38s
Val loss: 0.6536 score: 0.7209 time: 0.26s
Test loss: 0.6461 score: 0.7500 time: 0.25s
Epoch 51/1000, LR 0.000269
Train loss: 0.6390;  Loss pred: 0.6390; Loss self: 0.0000; time: 0.32s
Val loss: 0.6495 score: 0.7209 time: 0.33s
Test loss: 0.6417 score: 0.7727 time: 0.25s
Epoch 52/1000, LR 0.000269
Train loss: 0.6339;  Loss pred: 0.6339; Loss self: 0.0000; time: 0.50s
Val loss: 0.6451 score: 0.7907 time: 0.25s
Test loss: 0.6371 score: 0.7727 time: 0.38s
Epoch 53/1000, LR 0.000269
Train loss: 0.6302;  Loss pred: 0.6302; Loss self: 0.0000; time: 0.31s
Val loss: 0.6403 score: 0.7907 time: 0.26s
Test loss: 0.6320 score: 0.7955 time: 0.23s
Epoch 54/1000, LR 0.000269
Train loss: 0.6209;  Loss pred: 0.6209; Loss self: 0.0000; time: 0.35s
Val loss: 0.6350 score: 0.7674 time: 0.25s
Test loss: 0.6266 score: 0.7727 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6147;  Loss pred: 0.6147; Loss self: 0.0000; time: 0.32s
Val loss: 0.6294 score: 0.7674 time: 0.26s
Test loss: 0.6208 score: 0.7727 time: 0.26s
Epoch 56/1000, LR 0.000269
Train loss: 0.6093;  Loss pred: 0.6093; Loss self: 0.0000; time: 0.33s
Val loss: 0.6232 score: 0.7442 time: 0.25s
Test loss: 0.6145 score: 0.8409 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5989;  Loss pred: 0.5989; Loss self: 0.0000; time: 0.30s
Val loss: 0.6167 score: 0.7442 time: 0.26s
Test loss: 0.6079 score: 0.8409 time: 0.25s
Epoch 58/1000, LR 0.000269
Train loss: 0.5903;  Loss pred: 0.5903; Loss self: 0.0000; time: 0.33s
Val loss: 0.6097 score: 0.7674 time: 0.26s
Test loss: 0.6008 score: 0.8409 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5870;  Loss pred: 0.5870; Loss self: 0.0000; time: 0.28s
Val loss: 0.6023 score: 0.7674 time: 0.26s
Test loss: 0.5933 score: 0.8409 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5680;  Loss pred: 0.5680; Loss self: 0.0000; time: 0.34s
Val loss: 0.5945 score: 0.7674 time: 0.25s
Test loss: 0.5855 score: 0.8409 time: 0.24s
Epoch 61/1000, LR 0.000268
Train loss: 0.5609;  Loss pred: 0.5609; Loss self: 0.0000; time: 0.31s
Val loss: 0.5864 score: 0.7674 time: 0.25s
Test loss: 0.5773 score: 0.8409 time: 0.23s
Epoch 62/1000, LR 0.000268
Train loss: 0.5563;  Loss pred: 0.5563; Loss self: 0.0000; time: 0.34s
Val loss: 0.5782 score: 0.7674 time: 0.25s
Test loss: 0.5689 score: 0.8409 time: 0.34s
Epoch 63/1000, LR 0.000268
Train loss: 0.5388;  Loss pred: 0.5388; Loss self: 0.0000; time: 0.50s
Val loss: 0.5698 score: 0.7674 time: 0.25s
Test loss: 0.5604 score: 0.8409 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5268;  Loss pred: 0.5268; Loss self: 0.0000; time: 0.33s
Val loss: 0.5613 score: 0.7674 time: 0.25s
Test loss: 0.5519 score: 0.8409 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5129;  Loss pred: 0.5129; Loss self: 0.0000; time: 0.34s
Val loss: 0.5527 score: 0.7674 time: 0.24s
Test loss: 0.5434 score: 0.8409 time: 0.24s
Epoch 66/1000, LR 0.000268
Train loss: 0.5102;  Loss pred: 0.5102; Loss self: 0.0000; time: 0.32s
Val loss: 0.5440 score: 0.7907 time: 0.25s
Test loss: 0.5350 score: 0.7955 time: 0.25s
Epoch 67/1000, LR 0.000268
Train loss: 0.4903;  Loss pred: 0.4903; Loss self: 0.0000; time: 0.34s
Val loss: 0.5355 score: 0.7907 time: 0.25s
Test loss: 0.5269 score: 0.7955 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.4854;  Loss pred: 0.4854; Loss self: 0.0000; time: 0.32s
Val loss: 0.5264 score: 0.7907 time: 0.26s
Test loss: 0.5185 score: 0.7955 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.4687;  Loss pred: 0.4687; Loss self: 0.0000; time: 0.34s
Val loss: 0.5174 score: 0.7907 time: 0.25s
Test loss: 0.5100 score: 0.7955 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.4608;  Loss pred: 0.4608; Loss self: 0.0000; time: 0.30s
Val loss: 0.5079 score: 0.8140 time: 0.26s
Test loss: 0.5011 score: 0.8182 time: 0.32s
Epoch 71/1000, LR 0.000268
Train loss: 0.4343;  Loss pred: 0.4343; Loss self: 0.0000; time: 0.32s
Val loss: 0.4986 score: 0.8140 time: 0.25s
Test loss: 0.4922 score: 0.8182 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.4222;  Loss pred: 0.4222; Loss self: 0.0000; time: 0.28s
Val loss: 0.4892 score: 0.8140 time: 0.26s
Test loss: 0.4836 score: 0.8182 time: 0.25s
Epoch 73/1000, LR 0.000267
Train loss: 0.4177;  Loss pred: 0.4177; Loss self: 0.0000; time: 0.33s
Val loss: 0.4806 score: 0.8372 time: 0.53s
Test loss: 0.4757 score: 0.8182 time: 0.26s
Epoch 74/1000, LR 0.000267
Train loss: 0.4017;  Loss pred: 0.4017; Loss self: 0.0000; time: 0.36s
Val loss: 0.4720 score: 0.8372 time: 0.25s
Test loss: 0.4678 score: 0.8182 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.3930;  Loss pred: 0.3930; Loss self: 0.0000; time: 0.37s
Val loss: 0.4643 score: 0.8372 time: 0.29s
Test loss: 0.4607 score: 0.8182 time: 0.25s
Epoch 76/1000, LR 0.000267
Train loss: 0.3771;  Loss pred: 0.3771; Loss self: 0.0000; time: 0.34s
Val loss: 0.4569 score: 0.8372 time: 0.24s
Test loss: 0.4539 score: 0.8182 time: 0.30s
Epoch 77/1000, LR 0.000267
Train loss: 0.3533;  Loss pred: 0.3533; Loss self: 0.0000; time: 0.46s
Val loss: 0.4505 score: 0.8372 time: 0.35s
Test loss: 0.4480 score: 0.8182 time: 0.26s
Epoch 78/1000, LR 0.000267
Train loss: 0.3619;  Loss pred: 0.3619; Loss self: 0.0000; time: 0.30s
Val loss: 0.4446 score: 0.8372 time: 0.28s
Test loss: 0.4425 score: 0.8182 time: 0.24s
Epoch 79/1000, LR 0.000267
Train loss: 0.3335;  Loss pred: 0.3335; Loss self: 0.0000; time: 0.32s
Val loss: 0.4392 score: 0.8372 time: 0.25s
Test loss: 0.4376 score: 0.8182 time: 0.24s
Epoch 80/1000, LR 0.000267
Train loss: 0.3360;  Loss pred: 0.3360; Loss self: 0.0000; time: 0.30s
Val loss: 0.4339 score: 0.8372 time: 0.26s
Test loss: 0.4325 score: 0.8182 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.3249;  Loss pred: 0.3249; Loss self: 0.0000; time: 0.36s
Val loss: 0.4274 score: 0.8372 time: 0.26s
Test loss: 0.4262 score: 0.8182 time: 0.25s
Epoch 82/1000, LR 0.000267
Train loss: 0.3088;  Loss pred: 0.3088; Loss self: 0.0000; time: 0.28s
Val loss: 0.4210 score: 0.8372 time: 0.26s
Test loss: 0.4200 score: 0.8182 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.3360;  Loss pred: 0.3360; Loss self: 0.0000; time: 0.34s
Val loss: 0.4145 score: 0.8372 time: 0.34s
Test loss: 0.4138 score: 0.8409 time: 0.25s
Epoch 84/1000, LR 0.000266
Train loss: 0.3084;  Loss pred: 0.3084; Loss self: 0.0000; time: 0.46s
Val loss: 0.4090 score: 0.8605 time: 0.25s
Test loss: 0.4085 score: 0.8636 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2765;  Loss pred: 0.2765; Loss self: 0.0000; time: 0.32s
Val loss: 0.4045 score: 0.8605 time: 0.26s
Test loss: 0.4041 score: 0.8636 time: 0.24s
Epoch 86/1000, LR 0.000266
Train loss: 0.2667;  Loss pred: 0.2667; Loss self: 0.0000; time: 0.33s
Val loss: 0.4003 score: 0.8605 time: 0.26s
Test loss: 0.4001 score: 0.8636 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.2465;  Loss pred: 0.2465; Loss self: 0.0000; time: 0.33s
Val loss: 0.3973 score: 0.8605 time: 0.24s
Test loss: 0.3972 score: 0.8636 time: 0.25s
Epoch 88/1000, LR 0.000266
Train loss: 0.2449;  Loss pred: 0.2449; Loss self: 0.0000; time: 0.32s
Val loss: 0.3956 score: 0.8605 time: 0.26s
Test loss: 0.3953 score: 0.8636 time: 0.22s
Epoch 89/1000, LR 0.000266
Train loss: 0.2337;  Loss pred: 0.2337; Loss self: 0.0000; time: 0.33s
Val loss: 0.3952 score: 0.8605 time: 0.23s
Test loss: 0.3944 score: 0.8636 time: 0.25s
Epoch 90/1000, LR 0.000266
Train loss: 0.2233;  Loss pred: 0.2233; Loss self: 0.0000; time: 0.27s
Val loss: 0.3947 score: 0.8837 time: 0.26s
Test loss: 0.3936 score: 0.8636 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.2314;  Loss pred: 0.2314; Loss self: 0.0000; time: 0.34s
Val loss: 0.3930 score: 0.8837 time: 0.24s
Test loss: 0.3921 score: 0.8636 time: 0.25s
Epoch 92/1000, LR 0.000266
Train loss: 0.2090;  Loss pred: 0.2090; Loss self: 0.0000; time: 0.28s
Val loss: 0.3918 score: 0.8837 time: 0.27s
Test loss: 0.3910 score: 0.8636 time: 0.24s
Epoch 93/1000, LR 0.000265
Train loss: 0.2068;  Loss pred: 0.2068; Loss self: 0.0000; time: 0.34s
Val loss: 0.3903 score: 0.8605 time: 0.32s
Test loss: 0.3903 score: 0.8636 time: 0.25s
Epoch 94/1000, LR 0.000265
Train loss: 0.1880;  Loss pred: 0.1880; Loss self: 0.0000; time: 0.29s
Val loss: 0.3901 score: 0.8605 time: 0.26s
Test loss: 0.3908 score: 0.8409 time: 0.23s
Epoch 95/1000, LR 0.000265
Train loss: 0.1611;  Loss pred: 0.1611; Loss self: 0.0000; time: 0.32s
Val loss: 0.3898 score: 0.8605 time: 0.23s
Test loss: 0.3922 score: 0.8409 time: 0.25s
Epoch 96/1000, LR 0.000265
Train loss: 0.1462;  Loss pred: 0.1462; Loss self: 0.0000; time: 0.27s
Val loss: 0.3905 score: 0.8605 time: 0.26s
Test loss: 0.3947 score: 0.8409 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 97/1000, LR 0.000265
Train loss: 0.1907;  Loss pred: 0.1907; Loss self: 0.0000; time: 0.35s
Val loss: 0.3928 score: 0.8605 time: 0.24s
Test loss: 0.3969 score: 0.8409 time: 0.25s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 094,   Train_Loss: 0.1611,   Val_Loss: 0.3898,   Val_Precision: 0.9000,   Val_Recall: 0.8182,   Val_accuracy: 0.8571,   Val_Score: 0.8605,   Val_Loss: 0.3898,   Test_Precision: 0.9412,   Test_Recall: 0.7273,   Test_accuracy: 0.8205,   Test_Score: 0.8409,   Test_loss: 0.3922


[0.2497985300142318, 0.32745955989230424, 0.25860901991836727, 0.22938486700877547, 0.2560368539998308, 0.23099367099348456, 0.24871338508091867, 0.3258730289526284, 0.2521289469441399, 0.25767703901510686, 0.24736589996609837, 0.25654872797895223, 0.23171718197409064, 0.24934374098666012, 0.33275324502028525, 0.2500969209941104, 0.24809519504196942, 0.25609403708949685, 0.24471755407284945, 0.25662262504920363, 0.24853688501752913, 0.2572587139438838, 0.2541353029664606, 0.25713114405516535, 0.2508298170287162, 0.25474247604142874, 0.23634351103100926, 0.2528658659430221, 0.23292927793227136, 0.2529120879480615, 0.23483967501670122, 0.2575533849885687, 0.24814328702632338, 0.24999670893885195, 0.2469879719428718, 0.25520091399084777, 0.2346758390776813, 0.25641757098492235, 0.32292912795674056, 0.2756702930200845, 0.24478805402759463, 0.25107796606607735, 0.23320562997832894, 0.2598322540288791, 0.23266700096428394, 0.3234885160345584, 0.2620240399846807, 0.24345859105233103, 0.2450440980028361, 0.2539227369707078, 0.2492432571016252, 0.38411563099361956, 0.23875079897698015, 0.25360120506957173, 0.2659267050912604, 0.253873456036672, 0.2505510658957064, 0.2551003940170631, 0.2385756679577753, 0.2484913250664249, 0.23582218994852155, 0.34310909803025424, 0.24119721597526222, 0.24683881900273263, 0.24486084200907499, 0.252111655077897, 0.2507264600135386, 0.23981428798288107, 0.25302124209702015, 0.3285796850686893, 0.2484757910715416, 0.251142728026025, 0.2622546700295061, 0.25552358699496835, 0.25563455803785473, 0.3021004729671404, 0.25950399599969387, 0.2444445510627702, 0.24512262002099305, 0.23292591399513185, 0.2541733719408512, 0.24245301797054708, 0.2570711220614612, 0.24241339298896492, 0.2464832979021594, 0.2320937339682132, 0.25882383494172245, 0.22883157501928508, 0.25482657097745687, 0.24171577801462263, 0.25847092701587826, 0.24828070006333292, 0.25162430095952004, 0.23141626897267997, 0.25869310600683093, 0.24723731097765267, 0.25864988600369543]
[0.005677239318505268, 0.0074422627248250965, 0.005877477725417438, 0.005213292432017624, 0.005819019409087064, 0.005249856158942831, 0.005652576933657242, 0.007406205203468827, 0.005730203339639543, 0.005856296341252429, 0.005621952271956781, 0.005830652908612551, 0.005266299590320242, 0.005666903204242276, 0.007562573750461029, 0.0056840209316843275, 0.0056385271600447595, 0.005820319024761292, 0.00556176259256476, 0.005832332387481901, 0.005648565568580208, 0.005846788953270086, 0.005775802340146832, 0.005843889637617394, 0.00570067765974355, 0.005789601728214289, 0.005371443432522938, 0.005746951498705047, 0.00529384722573344, 0.00574800199881958, 0.005337265341288664, 0.005853486022467471, 0.005639620159689168, 0.005681743384973908, 0.005613362998701632, 0.005800020772519268, 0.005333541797220029, 0.005827672067839144, 0.007339298362653194, 0.0062652339322746475, 0.005563364864263514, 0.005706317410592667, 0.00530012795405293, 0.0059052785006563436, 0.0052878863855519076, 0.007352011728058146, 0.005955091817833652, 0.005533149796643887, 0.005569184045519002, 0.005770971294788813, 0.005664619479582391, 0.008729900704400445, 0.005426154522204094, 0.005763663751581175, 0.0060437887520741, 0.005769851273560727, 0.005694342406720601, 0.0057977362276605245, 0.005422174271767621, 0.005647530115146021, 0.005359595226102762, 0.007797934046142142, 0.005481754908528687, 0.005609973159153014, 0.005565019136569886, 0.005729810342679478, 0.005698328636671332, 0.00545032472688366, 0.005750482774932276, 0.007467720115197484, 0.0056471770698077635, 0.00570778927331875, 0.005960333409761502, 0.005807354249885644, 0.005809876319042153, 0.006865919840162282, 0.005897818090902133, 0.005555557978699322, 0.005570968636840751, 0.005293770772616633, 0.005776667544110255, 0.005510295862966979, 0.005842525501396845, 0.005509395295203748, 0.005601893134139987, 0.005274857590186663, 0.005882359885039146, 0.005200717614074661, 0.005791512976760383, 0.005493540409423242, 0.005874339250360869, 0.005642743183257567, 0.0057187341127163645, 0.0052594606584699995, 0.005879388772882521, 0.005619029794946651, 0.005878406500083987]
[176.14194926404565, 134.3677369335952, 170.14101060314553, 191.81736168461677, 171.8502602755347, 190.48140934234112, 176.91046256189486, 135.02191372332385, 174.51387686059056, 170.75638624293725, 177.87415325245001, 171.5073793061638, 189.88665244910428, 176.4632223206838, 132.23011543378843, 175.93179406249536, 177.35127837746583, 171.8118879301488, 179.79911644140466, 171.45799202842556, 177.0360966618565, 171.03405099660796, 173.13611877766894, 171.1189057306888, 175.41774148391085, 172.72345265594532, 186.1696976915394, 174.00529658642998, 188.89853774755545, 173.9734955216372, 187.3618671839451, 170.8383681385236, 177.31690640227725, 176.00231693754895, 178.1463269400713, 172.41317561103236, 187.49267147793313, 171.59510493369135, 136.25280654736795, 159.61095959220492, 179.74733356489668, 175.24437006320306, 188.67469024692406, 169.34002348726733, 189.11147613388593, 136.01719325114917, 167.92352336286575, 180.7288862135174, 179.55951748525993, 173.28105598151217, 176.534364506638, 114.54884011406158, 184.29257698208744, 173.5007528372322, 165.45912523114268, 173.3146926303481, 175.6129028735216, 172.48111344374067, 184.42786046306873, 177.0685555652224, 186.58125433236336, 128.23909436560677, 182.4233327988029, 178.2539722794286, 179.69390139714247, 174.5258464405581, 175.49005397206227, 183.47530653862, 173.89844281583422, 133.90967853293134, 177.07962538423487, 175.19917994774843, 167.77584931108984, 172.19545372485095, 172.12070362366427, 145.64690868519727, 169.55422913815903, 179.9999214901762, 179.5019978010667, 188.90126583734087, 173.11018720812052, 181.47845866511372, 171.15885925716842, 181.50812320011937, 178.5110811746183, 189.57857779144567, 169.99979932260555, 192.28115698758722, 172.66645244734877, 182.03197309419417, 170.2319116041125, 177.2187688723941, 174.86387376821162, 190.1335640546277, 170.08570765251918, 177.9666662204439, 170.11412871595604]
Elapsed: 0.25653955523049476~0.026551606531458764
Time per graph: 0.0058304444370567~0.0006034456029876992
Speed: 173.00138164539797~14.51192161539705
Total Time: 0.2591
best val loss: 0.3898244798183441 test_score: 0.8409

Testing...
Test loss: 0.3936 score: 0.8636 time: 0.24s
test Score 0.8636
Epoch Time List: [0.8247207490494475, 0.9594387949910015, 0.7528684469871223, 0.8008802239783108, 0.8374526519328356, 0.7968455370282754, 0.8217005981132388, 0.9292726720450446, 0.8317684089997783, 0.9221130850492045, 0.7786981880199164, 0.8132974099135026, 0.7683717970503494, 0.8316199580440298, 0.8939715939341113, 0.8122237080242485, 0.7855531779350713, 0.827003040118143, 0.7827148829819635, 0.8732538809999824, 0.7805667839711532, 0.8952319369418547, 0.7890561139211059, 0.8291767020709813, 0.7886414459208027, 0.8437333110487089, 0.773493014043197, 0.8423521219519898, 0.7985178369563073, 0.835178000968881, 0.8113870199304074, 0.9287382970796898, 0.7756555470405146, 0.8210850391769782, 0.7774627038743347, 0.8269702589605004, 0.7698092651553452, 0.8602708399994299, 0.8550174020929262, 0.8806576308561489, 0.8983630340080708, 0.825564495055005, 0.8105708009097725, 0.8243050900055096, 0.7977071148343384, 0.8888733449857682, 0.9180228960467502, 0.7743497571209446, 0.9097699569538236, 0.8939186189090833, 0.8956154871266335, 1.1280851961346343, 0.797158862114884, 0.850575581076555, 0.8471479009604082, 0.8296630670083687, 0.8014388380106539, 0.8436017360072583, 0.7709037190070376, 0.8368428669637069, 0.8006790579529479, 0.9299801111919805, 0.988941426970996, 0.8212239319691435, 0.8243341819616035, 0.8220522220944986, 0.840741929016076, 0.8142986509483308, 0.8444773720111698, 0.882736028986983, 0.8072191709652543, 0.7866654510144144, 1.1222343500703573, 0.8611294019501656, 0.9047578909667209, 0.8811050849035382, 1.0740919369272888, 0.8233068980043754, 0.8193058430915698, 0.7875462149968371, 0.8648026889422908, 0.7721956020686775, 0.9307035650126636, 0.9469739039195701, 0.8278783999849111, 0.8171831460203975, 0.8303379679564387, 0.8011122940806672, 0.8110714779468253, 0.7642453399021178, 0.828912140103057, 0.7931228970410302, 0.9119933691108599, 0.7773288340540603, 0.8046813590917736, 0.7802793070441112, 0.8405244681052864]
Total Epoch List: [97]
Total Time List: [0.2591159560251981]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a52883372e0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6999;  Loss pred: 0.6999; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6977 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6955 score: 0.5116 time: 0.24s
Epoch 2/1000, LR 0.000000
Train loss: 0.6999;  Loss pred: 0.6999; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5116 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6976 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6954 score: 0.5116 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6998;  Loss pred: 0.6998; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6975 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6953 score: 0.5116 time: 0.25s
Epoch 5/1000, LR 0.000090
Train loss: 0.6996;  Loss pred: 0.6996; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6974 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6952 score: 0.5116 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6995;  Loss pred: 0.6995; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6972 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6951 score: 0.5116 time: 0.23s
Epoch 7/1000, LR 0.000150
Train loss: 0.6993;  Loss pred: 0.6993; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6970 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6950 score: 0.5116 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6990;  Loss pred: 0.6990; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6968 score: 0.5000 time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6948 score: 0.5116 time: 0.23s
Epoch 9/1000, LR 0.000210
Train loss: 0.6988;  Loss pred: 0.6988; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6966 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6947 score: 0.5116 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6985;  Loss pred: 0.6985; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6964 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6945 score: 0.5116 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6982;  Loss pred: 0.6982; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6961 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6943 score: 0.5116 time: 0.26s
Epoch 12/1000, LR 0.000270
Train loss: 0.6978;  Loss pred: 0.6978; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6959 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6942 score: 0.5116 time: 0.22s
Epoch 13/1000, LR 0.000270
Train loss: 0.6975;  Loss pred: 0.6975; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6956 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6940 score: 0.5116 time: 0.26s
Epoch 14/1000, LR 0.000270
Train loss: 0.6972;  Loss pred: 0.6972; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6954 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6939 score: 0.5116 time: 0.22s
Epoch 15/1000, LR 0.000270
Train loss: 0.6968;  Loss pred: 0.6968; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6952 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6937 score: 0.5116 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6965;  Loss pred: 0.6965; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6949 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6935 score: 0.5116 time: 0.22s
Epoch 17/1000, LR 0.000270
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6947 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6934 score: 0.5116 time: 0.25s
Epoch 18/1000, LR 0.000270
Train loss: 0.6957;  Loss pred: 0.6957; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6944 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6932 score: 0.5116 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6941 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.24s
Epoch 20/1000, LR 0.000270
Train loss: 0.6949;  Loss pred: 0.6949; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6939 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.22s
Epoch 21/1000, LR 0.000270
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.23s
Epoch 22/1000, LR 0.000270
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.25s
Epoch 24/1000, LR 0.000270
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.5116 time: 0.23s
Epoch 25/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.5116 time: 0.24s
Epoch 26/1000, LR 0.000270
Train loss: 0.6919;  Loss pred: 0.6919; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6919 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.5116 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6914 score: 0.5116 time: 0.25s
Epoch 28/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6910 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.5116 time: 0.24s
Epoch 29/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6908 score: 0.5116 time: 0.24s
Epoch 30/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6901 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6905 score: 0.5116 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6882;  Loss pred: 0.6882; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.5116 time: 0.24s
Epoch 32/1000, LR 0.000270
Train loss: 0.6870;  Loss pred: 0.6870; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6889 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6897 score: 0.5116 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6859;  Loss pred: 0.6859; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6882 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6893 score: 0.5116 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6888 score: 0.5116 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6835;  Loss pred: 0.6835; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6866 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.5116 time: 0.23s
Epoch 36/1000, LR 0.000270
Train loss: 0.6821;  Loss pred: 0.6821; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6856 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.5116 time: 0.25s
Epoch 37/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6871 score: 0.5116 time: 0.25s
Epoch 38/1000, LR 0.000270
Train loss: 0.6786;  Loss pred: 0.6786; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6834 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5116 time: 0.22s
Epoch 39/1000, LR 0.000269
Train loss: 0.6761;  Loss pred: 0.6761; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6855 score: 0.5116 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6808 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6847 score: 0.5116 time: 0.32s
Epoch 41/1000, LR 0.000269
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.35s
Val loss: 0.6793 score: 0.5227 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6837 score: 0.5116 time: 0.24s
Epoch 42/1000, LR 0.000269
Train loss: 0.6692;  Loss pred: 0.6692; Loss self: 0.0000; time: 0.38s
Val loss: 0.6776 score: 0.5455 time: 0.22s
Test loss: 0.6827 score: 0.5349 time: 0.24s
Epoch 43/1000, LR 0.000269
Train loss: 0.6672;  Loss pred: 0.6672; Loss self: 0.0000; time: 0.35s
Val loss: 0.6758 score: 0.6364 time: 0.23s
Test loss: 0.6815 score: 0.5814 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6629;  Loss pred: 0.6629; Loss self: 0.0000; time: 0.38s
Val loss: 0.6739 score: 0.6818 time: 0.22s
Test loss: 0.6803 score: 0.6512 time: 0.23s
Epoch 45/1000, LR 0.000269
Train loss: 0.6609;  Loss pred: 0.6609; Loss self: 0.0000; time: 0.35s
Val loss: 0.6717 score: 0.8182 time: 0.22s
Test loss: 0.6789 score: 0.6977 time: 0.24s
Epoch 46/1000, LR 0.000269
Train loss: 0.6575;  Loss pred: 0.6575; Loss self: 0.0000; time: 0.35s
Val loss: 0.6694 score: 0.8409 time: 0.23s
Test loss: 0.6774 score: 0.7674 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6512;  Loss pred: 0.6512; Loss self: 0.0000; time: 0.37s
Val loss: 0.6669 score: 0.7955 time: 0.25s
Test loss: 0.6759 score: 0.7674 time: 0.24s
Epoch 48/1000, LR 0.000269
Train loss: 0.6483;  Loss pred: 0.6483; Loss self: 0.0000; time: 0.36s
Val loss: 0.6643 score: 0.8409 time: 0.23s
Test loss: 0.6742 score: 0.7442 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6451;  Loss pred: 0.6451; Loss self: 0.0000; time: 0.46s
Val loss: 0.6615 score: 0.8636 time: 0.25s
Test loss: 0.6724 score: 0.8372 time: 0.23s
Epoch 50/1000, LR 0.000269
Train loss: 0.6384;  Loss pred: 0.6384; Loss self: 0.0000; time: 0.37s
Val loss: 0.6586 score: 0.8636 time: 0.23s
Test loss: 0.6706 score: 0.7907 time: 0.26s
Epoch 51/1000, LR 0.000269
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.34s
Val loss: 0.6554 score: 0.8409 time: 0.26s
Test loss: 0.6687 score: 0.7907 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6284;  Loss pred: 0.6284; Loss self: 0.0000; time: 0.37s
Val loss: 0.6522 score: 0.8182 time: 0.21s
Test loss: 0.6667 score: 0.7442 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.6215;  Loss pred: 0.6215; Loss self: 0.0000; time: 0.32s
Val loss: 0.6486 score: 0.8182 time: 0.24s
Test loss: 0.6646 score: 0.7209 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.6155;  Loss pred: 0.6155; Loss self: 0.0000; time: 0.36s
Val loss: 0.6449 score: 0.8409 time: 0.21s
Test loss: 0.6624 score: 0.6744 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.6106;  Loss pred: 0.6106; Loss self: 0.0000; time: 0.32s
Val loss: 0.6410 score: 0.8409 time: 0.24s
Test loss: 0.6601 score: 0.6977 time: 0.24s
Epoch 56/1000, LR 0.000269
Train loss: 0.6043;  Loss pred: 0.6043; Loss self: 0.0000; time: 0.37s
Val loss: 0.6369 score: 0.8409 time: 0.21s
Test loss: 0.6577 score: 0.6744 time: 0.25s
Epoch 57/1000, LR 0.000269
Train loss: 0.5934;  Loss pred: 0.5934; Loss self: 0.0000; time: 0.40s
Val loss: 0.6324 score: 0.8182 time: 0.24s
Test loss: 0.6550 score: 0.6744 time: 0.23s
Epoch 58/1000, LR 0.000269
Train loss: 0.5846;  Loss pred: 0.5846; Loss self: 0.0000; time: 0.42s
Val loss: 0.6277 score: 0.8182 time: 0.23s
Test loss: 0.6522 score: 0.6744 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5755;  Loss pred: 0.5755; Loss self: 0.0000; time: 0.30s
Val loss: 0.6227 score: 0.8182 time: 0.24s
Test loss: 0.6493 score: 0.6744 time: 0.23s
Epoch 60/1000, LR 0.000268
Train loss: 0.5709;  Loss pred: 0.5709; Loss self: 0.0000; time: 0.41s
Val loss: 0.6173 score: 0.8182 time: 0.22s
Test loss: 0.6461 score: 0.6744 time: 0.27s
Epoch 61/1000, LR 0.000268
Train loss: 0.5597;  Loss pred: 0.5597; Loss self: 0.0000; time: 0.34s
Val loss: 0.6118 score: 0.8182 time: 0.23s
Test loss: 0.6430 score: 0.6744 time: 0.22s
Epoch 62/1000, LR 0.000268
Train loss: 0.5519;  Loss pred: 0.5519; Loss self: 0.0000; time: 0.42s
Val loss: 0.6060 score: 0.8182 time: 0.23s
Test loss: 0.6397 score: 0.6744 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.5398;  Loss pred: 0.5398; Loss self: 0.0000; time: 0.42s
Val loss: 0.6001 score: 0.8182 time: 0.22s
Test loss: 0.6366 score: 0.6512 time: 0.24s
Epoch 64/1000, LR 0.000268
Train loss: 0.5262;  Loss pred: 0.5262; Loss self: 0.0000; time: 0.35s
Val loss: 0.5941 score: 0.8182 time: 0.33s
Test loss: 0.6335 score: 0.6512 time: 0.24s
Epoch 65/1000, LR 0.000268
Train loss: 0.5161;  Loss pred: 0.5161; Loss self: 0.0000; time: 0.37s
Val loss: 0.5880 score: 0.8182 time: 0.21s
Test loss: 0.6307 score: 0.6512 time: 0.25s
Epoch 66/1000, LR 0.000268
Train loss: 0.5054;  Loss pred: 0.5054; Loss self: 0.0000; time: 0.31s
Val loss: 0.5819 score: 0.8182 time: 0.24s
Test loss: 0.6280 score: 0.6512 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.4922;  Loss pred: 0.4922; Loss self: 0.0000; time: 0.36s
Val loss: 0.5754 score: 0.8182 time: 0.21s
Test loss: 0.6253 score: 0.6512 time: 0.25s
Epoch 68/1000, LR 0.000268
Train loss: 0.4773;  Loss pred: 0.4773; Loss self: 0.0000; time: 0.32s
Val loss: 0.5690 score: 0.8182 time: 0.24s
Test loss: 0.6229 score: 0.6512 time: 0.24s
Epoch 69/1000, LR 0.000268
Train loss: 0.4643;  Loss pred: 0.4643; Loss self: 0.0000; time: 0.37s
Val loss: 0.5626 score: 0.8182 time: 0.22s
Test loss: 0.6206 score: 0.6512 time: 0.25s
Epoch 70/1000, LR 0.000268
Train loss: 0.4523;  Loss pred: 0.4523; Loss self: 0.0000; time: 0.32s
Val loss: 0.5561 score: 0.8182 time: 0.24s
Test loss: 0.6186 score: 0.6512 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.4330;  Loss pred: 0.4330; Loss self: 0.0000; time: 0.37s
Val loss: 0.5493 score: 0.8182 time: 0.30s
Test loss: 0.6167 score: 0.6512 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.4242;  Loss pred: 0.4242; Loss self: 0.0000; time: 0.44s
Val loss: 0.5421 score: 0.8182 time: 0.22s
Test loss: 0.6142 score: 0.6512 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.4090;  Loss pred: 0.4090; Loss self: 0.0000; time: 0.38s
Val loss: 0.5348 score: 0.8182 time: 0.23s
Test loss: 0.6116 score: 0.6512 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.3843;  Loss pred: 0.3843; Loss self: 0.0000; time: 0.45s
Val loss: 0.5276 score: 0.8182 time: 0.23s
Test loss: 0.6089 score: 0.6512 time: 0.25s
Epoch 75/1000, LR 0.000267
Train loss: 0.3673;  Loss pred: 0.3673; Loss self: 0.0000; time: 0.34s
Val loss: 0.5201 score: 0.8409 time: 0.24s
Test loss: 0.6058 score: 0.6512 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.3531;  Loss pred: 0.3531; Loss self: 0.0000; time: 0.38s
Val loss: 0.5132 score: 0.8409 time: 0.22s
Test loss: 0.6032 score: 0.6744 time: 0.25s
Epoch 77/1000, LR 0.000267
Train loss: 0.3387;  Loss pred: 0.3387; Loss self: 0.0000; time: 0.31s
Val loss: 0.5061 score: 0.8409 time: 0.24s
Test loss: 0.6001 score: 0.6744 time: 0.24s
Epoch 78/1000, LR 0.000267
Train loss: 0.3229;  Loss pred: 0.3229; Loss self: 0.0000; time: 0.37s
Val loss: 0.4994 score: 0.8409 time: 0.23s
Test loss: 0.5975 score: 0.6977 time: 0.25s
Epoch 79/1000, LR 0.000267
Train loss: 0.3090;  Loss pred: 0.3090; Loss self: 0.0000; time: 0.33s
Val loss: 0.4929 score: 0.8182 time: 0.23s
Test loss: 0.5951 score: 0.7209 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.2769;  Loss pred: 0.2769; Loss self: 0.0000; time: 0.46s
Val loss: 0.4869 score: 0.8182 time: 0.26s
Test loss: 0.5934 score: 0.7442 time: 0.23s
Epoch 81/1000, LR 0.000267
Train loss: 0.2687;  Loss pred: 0.2687; Loss self: 0.0000; time: 0.40s
Val loss: 0.4810 score: 0.8182 time: 0.22s
Test loss: 0.5915 score: 0.7442 time: 0.24s
Epoch 82/1000, LR 0.000267
Train loss: 0.2516;  Loss pred: 0.2516; Loss self: 0.0000; time: 0.35s
Val loss: 0.4755 score: 0.8182 time: 0.22s
Test loss: 0.5901 score: 0.7442 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2318;  Loss pred: 0.2318; Loss self: 0.0000; time: 0.47s
Val loss: 0.4702 score: 0.8182 time: 0.21s
Test loss: 0.5889 score: 0.8140 time: 0.23s
Epoch 84/1000, LR 0.000266
Train loss: 0.2214;  Loss pred: 0.2214; Loss self: 0.0000; time: 0.35s
Val loss: 0.4651 score: 0.8182 time: 0.28s
Test loss: 0.5874 score: 0.8140 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2193;  Loss pred: 0.2193; Loss self: 0.0000; time: 0.44s
Val loss: 0.4601 score: 0.8182 time: 0.22s
Test loss: 0.5859 score: 0.8140 time: 0.24s
Epoch 86/1000, LR 0.000266
Train loss: 0.2076;  Loss pred: 0.2076; Loss self: 0.0000; time: 0.33s
Val loss: 0.4548 score: 0.8182 time: 0.22s
Test loss: 0.5845 score: 0.8140 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.1726;  Loss pred: 0.1726; Loss self: 0.0000; time: 0.45s
Val loss: 0.4532 score: 0.8182 time: 0.23s
Test loss: 0.5882 score: 0.8140 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.52s
Val loss: 0.4551 score: 0.8182 time: 0.22s
Test loss: 0.5960 score: 0.8140 time: 0.26s
     INFO: Early stopping counter 1 of 2
Epoch 89/1000, LR 0.000266
Train loss: 0.1639;  Loss pred: 0.1639; Loss self: 0.0000; time: 0.31s
Val loss: 0.4609 score: 0.8182 time: 0.24s
Test loss: 0.6082 score: 0.8372 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 086,   Train_Loss: 0.1726,   Val_Loss: 0.4532,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.4532,   Test_Precision: 0.8889,   Test_Recall: 0.7273,   Test_accuracy: 0.8000,   Test_Score: 0.8140,   Test_loss: 0.5882


[0.2497985300142318, 0.32745955989230424, 0.25860901991836727, 0.22938486700877547, 0.2560368539998308, 0.23099367099348456, 0.24871338508091867, 0.3258730289526284, 0.2521289469441399, 0.25767703901510686, 0.24736589996609837, 0.25654872797895223, 0.23171718197409064, 0.24934374098666012, 0.33275324502028525, 0.2500969209941104, 0.24809519504196942, 0.25609403708949685, 0.24471755407284945, 0.25662262504920363, 0.24853688501752913, 0.2572587139438838, 0.2541353029664606, 0.25713114405516535, 0.2508298170287162, 0.25474247604142874, 0.23634351103100926, 0.2528658659430221, 0.23292927793227136, 0.2529120879480615, 0.23483967501670122, 0.2575533849885687, 0.24814328702632338, 0.24999670893885195, 0.2469879719428718, 0.25520091399084777, 0.2346758390776813, 0.25641757098492235, 0.32292912795674056, 0.2756702930200845, 0.24478805402759463, 0.25107796606607735, 0.23320562997832894, 0.2598322540288791, 0.23266700096428394, 0.3234885160345584, 0.2620240399846807, 0.24345859105233103, 0.2450440980028361, 0.2539227369707078, 0.2492432571016252, 0.38411563099361956, 0.23875079897698015, 0.25360120506957173, 0.2659267050912604, 0.253873456036672, 0.2505510658957064, 0.2551003940170631, 0.2385756679577753, 0.2484913250664249, 0.23582218994852155, 0.34310909803025424, 0.24119721597526222, 0.24683881900273263, 0.24486084200907499, 0.252111655077897, 0.2507264600135386, 0.23981428798288107, 0.25302124209702015, 0.3285796850686893, 0.2484757910715416, 0.251142728026025, 0.2622546700295061, 0.25552358699496835, 0.25563455803785473, 0.3021004729671404, 0.25950399599969387, 0.2444445510627702, 0.24512262002099305, 0.23292591399513185, 0.2541733719408512, 0.24245301797054708, 0.2570711220614612, 0.24241339298896492, 0.2464832979021594, 0.2320937339682132, 0.25882383494172245, 0.22883157501928508, 0.25482657097745687, 0.24171577801462263, 0.25847092701587826, 0.24828070006333292, 0.25162430095952004, 0.23141626897267997, 0.25869310600683093, 0.24723731097765267, 0.25864988600369543, 0.2413100799312815, 0.25171287008561194, 0.24160026700701565, 0.2528091899584979, 0.22962563193868846, 0.23839664901606739, 0.22977831005118787, 0.23907785397022963, 0.23890278697945178, 0.2281447669956833, 0.26915517391171306, 0.22865108796395361, 0.26681728300172836, 0.227431857958436, 0.2449376389849931, 0.2292591850273311, 0.2521258459892124, 0.22697411803528666, 0.2401684180367738, 0.22784099297132343, 0.23907131899613887, 0.22846162703353912, 0.25515378592535853, 0.2376494479831308, 0.24420176399871707, 0.24231249396689236, 0.2496304550440982, 0.24122482095845044, 0.24481899698730558, 0.22689327702391893, 0.24414298892952502, 0.23458016000222415, 0.24716039199847728, 0.22453731892164797, 0.23843483196105808, 0.24953836505301297, 0.2535892160376534, 0.2288960110163316, 0.2504239580594003, 0.32908025407232344, 0.24541229207534343, 0.24146251298952848, 0.2416575860697776, 0.23789329396095127, 0.24748750100843608, 0.23005351808387786, 0.24709174397867173, 0.22826743801124394, 0.22962212399579585, 0.266580912983045, 0.24573359999340028, 0.2522175550693646, 0.24233591207303107, 0.2535888010170311, 0.24096664402168244, 0.25291049701627344, 0.23257419501896948, 0.2546249870210886, 0.23455452697817236, 0.2708364389836788, 0.22481738298665732, 0.24178545200265944, 0.24748341599479318, 0.24074248794931918, 0.25517577095888555, 0.24733015103265643, 0.25405953207518905, 0.24645568802952766, 0.2568706440506503, 0.2429547639330849, 0.24350356694776565, 0.24492015107534826, 0.23723668104503304, 0.2591209770180285, 0.2446586670121178, 0.25674472691025585, 0.24405030498746783, 0.2560999769484624, 0.22918542905244976, 0.23367390595376492, 0.2455861900234595, 0.24117685202509165, 0.23911398299969733, 0.24093195993918926, 0.24857717205304652, 0.2305697980336845, 0.2415757739217952, 0.2628975809784606, 0.24832868098746985]
[0.005677239318505268, 0.0074422627248250965, 0.005877477725417438, 0.005213292432017624, 0.005819019409087064, 0.005249856158942831, 0.005652576933657242, 0.007406205203468827, 0.005730203339639543, 0.005856296341252429, 0.005621952271956781, 0.005830652908612551, 0.005266299590320242, 0.005666903204242276, 0.007562573750461029, 0.0056840209316843275, 0.0056385271600447595, 0.005820319024761292, 0.00556176259256476, 0.005832332387481901, 0.005648565568580208, 0.005846788953270086, 0.005775802340146832, 0.005843889637617394, 0.00570067765974355, 0.005789601728214289, 0.005371443432522938, 0.005746951498705047, 0.00529384722573344, 0.00574800199881958, 0.005337265341288664, 0.005853486022467471, 0.005639620159689168, 0.005681743384973908, 0.005613362998701632, 0.005800020772519268, 0.005333541797220029, 0.005827672067839144, 0.007339298362653194, 0.0062652339322746475, 0.005563364864263514, 0.005706317410592667, 0.00530012795405293, 0.0059052785006563436, 0.0052878863855519076, 0.007352011728058146, 0.005955091817833652, 0.005533149796643887, 0.005569184045519002, 0.005770971294788813, 0.005664619479582391, 0.008729900704400445, 0.005426154522204094, 0.005763663751581175, 0.0060437887520741, 0.005769851273560727, 0.005694342406720601, 0.0057977362276605245, 0.005422174271767621, 0.005647530115146021, 0.005359595226102762, 0.007797934046142142, 0.005481754908528687, 0.005609973159153014, 0.005565019136569886, 0.005729810342679478, 0.005698328636671332, 0.00545032472688366, 0.005750482774932276, 0.007467720115197484, 0.0056471770698077635, 0.00570778927331875, 0.005960333409761502, 0.005807354249885644, 0.005809876319042153, 0.006865919840162282, 0.005897818090902133, 0.005555557978699322, 0.005570968636840751, 0.005293770772616633, 0.005776667544110255, 0.005510295862966979, 0.005842525501396845, 0.005509395295203748, 0.005601893134139987, 0.005274857590186663, 0.005882359885039146, 0.005200717614074661, 0.005791512976760383, 0.005493540409423242, 0.005874339250360869, 0.005642743183257567, 0.0057187341127163645, 0.0052594606584699995, 0.005879388772882521, 0.005619029794946651, 0.005878406500083987, 0.005611862323983291, 0.00585378767640958, 0.0056186108606282705, 0.005879283487406927, 0.005340130975318336, 0.00554410811665273, 0.0053436816290973925, 0.005559950092330922, 0.005555878766963995, 0.005305692255713566, 0.006259422649109606, 0.00531746716195241, 0.0062050530930634505, 0.0052891129757775815, 0.0056962241624417, 0.005331608954123979, 0.005863391767190986, 0.0052784678612857365, 0.005585312047366833, 0.005298627743519149, 0.005559798116189276, 0.005313061093803235, 0.005933808975008338, 0.005526731348444902, 0.005679110790667839, 0.005635174278299822, 0.00580535941963019, 0.005609879557173266, 0.005693465046216409, 0.005276587837765557, 0.005677743928593605, 0.005455352558191259, 0.005747916092987844, 0.00522179811445693, 0.0055449960921176294, 0.0058032177919305344, 0.005897423628782637, 0.005323163046891433, 0.005823812978125589, 0.007653029164472638, 0.005707262606403336, 0.0056154072788262436, 0.005619943862087851, 0.005532402185138402, 0.005755523279265955, 0.005350081815904136, 0.00574631962741097, 0.005308545070028928, 0.005340049395251067, 0.006199556115884767, 0.005714734883567448, 0.005865524536496852, 0.005635718885419327, 0.005897413977140257, 0.005603875442364708, 0.0058816394654947314, 0.0054087022097434765, 0.0059215113260718275, 0.005454756441352846, 0.00629852183682974, 0.0052283112322478445, 0.005622917488433941, 0.005755428278948678, 0.0055986625104492834, 0.005934320254857803, 0.005751863977503638, 0.0059083612110509085, 0.005731527628593667, 0.005973735908154658, 0.00565011078914151, 0.005662873649948039, 0.005695817466868564, 0.00551713211732635, 0.006026069232977407, 0.005689736442142274, 0.00597080760256409, 0.005675588488080647, 0.005955813417406103, 0.005329893698894181, 0.005434276882645695, 0.005711306744731616, 0.005608764000583527, 0.005560790302318542, 0.005603068835795099, 0.005780864466349919, 0.005362088326364756, 0.005618041253995237, 0.006113897232057223, 0.005775085604359764]
[176.14194926404565, 134.3677369335952, 170.14101060314553, 191.81736168461677, 171.8502602755347, 190.48140934234112, 176.91046256189486, 135.02191372332385, 174.51387686059056, 170.75638624293725, 177.87415325245001, 171.5073793061638, 189.88665244910428, 176.4632223206838, 132.23011543378843, 175.93179406249536, 177.35127837746583, 171.8118879301488, 179.79911644140466, 171.45799202842556, 177.0360966618565, 171.03405099660796, 173.13611877766894, 171.1189057306888, 175.41774148391085, 172.72345265594532, 186.1696976915394, 174.00529658642998, 188.89853774755545, 173.9734955216372, 187.3618671839451, 170.8383681385236, 177.31690640227725, 176.00231693754895, 178.1463269400713, 172.41317561103236, 187.49267147793313, 171.59510493369135, 136.25280654736795, 159.61095959220492, 179.74733356489668, 175.24437006320306, 188.67469024692406, 169.34002348726733, 189.11147613388593, 136.01719325114917, 167.92352336286575, 180.7288862135174, 179.55951748525993, 173.28105598151217, 176.534364506638, 114.54884011406158, 184.29257698208744, 173.5007528372322, 165.45912523114268, 173.3146926303481, 175.6129028735216, 172.48111344374067, 184.42786046306873, 177.0685555652224, 186.58125433236336, 128.23909436560677, 182.4233327988029, 178.2539722794286, 179.69390139714247, 174.5258464405581, 175.49005397206227, 183.47530653862, 173.89844281583422, 133.90967853293134, 177.07962538423487, 175.19917994774843, 167.77584931108984, 172.19545372485095, 172.12070362366427, 145.64690868519727, 169.55422913815903, 179.9999214901762, 179.5019978010667, 188.90126583734087, 173.11018720812052, 181.47845866511372, 171.15885925716842, 181.50812320011937, 178.5110811746183, 189.57857779144567, 169.99979932260555, 192.28115698758722, 172.66645244734877, 182.03197309419417, 170.2319116041125, 177.2187688723941, 174.86387376821162, 190.1335640546277, 170.08570765251918, 177.9666662204439, 170.11412871595604, 178.19396525932618, 170.82956459625981, 177.9799357537604, 170.08875352616352, 187.2613246045689, 180.371662846242, 187.1368972572775, 179.8577295467711, 179.98952855957458, 188.4768191979332, 159.7591433040951, 188.05945942745248, 161.15897559650008, 189.06761957622666, 175.5548889022913, 187.56064231351846, 170.5497499920727, 189.4489132602995, 179.04102609118232, 188.72811007021934, 179.8626459274041, 188.21541524646247, 168.52581608402633, 180.93877501054533, 176.083904128661, 177.4568009104606, 172.25462330869797, 178.25694648315854, 175.63996474599415, 189.51641302032485, 176.12629462979376, 183.30620969647347, 173.9760956531616, 191.50491422321116, 180.3427781349618, 172.31819239845103, 169.56557014480964, 187.8582322560963, 171.70881066339683, 130.66721405456826, 175.21534735024062, 178.08147305194652, 177.93772047190743, 180.7533086958651, 173.74614808743115, 186.9130294470095, 174.02443039016163, 188.3755316773736, 187.26418540047686, 161.30187086100526, 174.98624527193212, 170.48773622507832, 177.43965239060975, 169.56584765394317, 178.44793487737172, 170.02062194845627, 184.88723564750072, 168.87580634982476, 183.32624210660228, 158.76741018069313, 191.2663488416819, 177.8436198035895, 173.74901597812388, 178.6140882994127, 168.51129650129772, 173.85668435678292, 169.25166967273688, 174.4735548357407, 167.3994323443249, 176.98767994458075, 176.5887889815758, 175.56742395921233, 181.25358949798155, 165.94565401398688, 175.7550653125656, 167.4815312371751, 176.1931828038817, 167.9031779399704, 187.62100268669053, 184.0171234545463, 175.09127852788646, 178.2924009453708, 179.83055386624724, 178.47362388473954, 172.98450877389405, 186.4945034722979, 177.99798093131764, 163.56179406429388, 173.15760639895515]
Elapsed: 0.2505395596581561~0.022345504989054358
Time per graph: 0.0057557895706454795~0.000495951941074985
Speed: 174.79008766356068~12.349885361763466
Total Time: 0.2489
best val loss: 0.4531848728656769 test_score: 0.8140

Testing...
Test loss: 0.6724 score: 0.8372 time: 0.25s
test Score 0.8372
Epoch Time List: [0.8247207490494475, 0.9594387949910015, 0.7528684469871223, 0.8008802239783108, 0.8374526519328356, 0.7968455370282754, 0.8217005981132388, 0.9292726720450446, 0.8317684089997783, 0.9221130850492045, 0.7786981880199164, 0.8132974099135026, 0.7683717970503494, 0.8316199580440298, 0.8939715939341113, 0.8122237080242485, 0.7855531779350713, 0.827003040118143, 0.7827148829819635, 0.8732538809999824, 0.7805667839711532, 0.8952319369418547, 0.7890561139211059, 0.8291767020709813, 0.7886414459208027, 0.8437333110487089, 0.773493014043197, 0.8423521219519898, 0.7985178369563073, 0.835178000968881, 0.8113870199304074, 0.9287382970796898, 0.7756555470405146, 0.8210850391769782, 0.7774627038743347, 0.8269702589605004, 0.7698092651553452, 0.8602708399994299, 0.8550174020929262, 0.8806576308561489, 0.8983630340080708, 0.825564495055005, 0.8105708009097725, 0.8243050900055096, 0.7977071148343384, 0.8888733449857682, 0.9180228960467502, 0.7743497571209446, 0.9097699569538236, 0.8939186189090833, 0.8956154871266335, 1.1280851961346343, 0.797158862114884, 0.850575581076555, 0.8471479009604082, 0.8296630670083687, 0.8014388380106539, 0.8436017360072583, 0.7709037190070376, 0.8368428669637069, 0.8006790579529479, 0.9299801111919805, 0.988941426970996, 0.8212239319691435, 0.8243341819616035, 0.8220522220944986, 0.840741929016076, 0.8142986509483308, 0.8444773720111698, 0.882736028986983, 0.8072191709652543, 0.7866654510144144, 1.1222343500703573, 0.8611294019501656, 0.9047578909667209, 0.8811050849035382, 1.0740919369272888, 0.8233068980043754, 0.8193058430915698, 0.7875462149968371, 0.8648026889422908, 0.7721956020686775, 0.9307035650126636, 0.9469739039195701, 0.8278783999849111, 0.8171831460203975, 0.8303379679564387, 0.8011122940806672, 0.8110714779468253, 0.7642453399021178, 0.828912140103057, 0.7931228970410302, 0.9119933691108599, 0.7773288340540603, 0.8046813590917736, 0.7802793070441112, 0.8405244681052864, 0.7960834520636126, 0.8403372941538692, 0.8934082119958475, 0.848209252115339, 0.7951096849283203, 0.8178311639931053, 0.8814818609971553, 0.8658556620357558, 0.8142064539715648, 0.9059506360208616, 0.8534197120461613, 0.8454104620032012, 0.8809470180422068, 0.7971502779982984, 0.858454964007251, 0.8126257440308109, 0.8267992479959503, 0.7765154190128669, 0.899220593040809, 0.8089490691199899, 0.8088542360346764, 0.7684056849684566, 0.8179148220224306, 0.7700231760973111, 0.7984804550651461, 0.8049162570387125, 0.8992394719971344, 0.7709511689608917, 0.7996443558949977, 0.7735787429846823, 0.8196737258695066, 0.7874342320719734, 0.8426383751211688, 0.8529537671711296, 0.894829077180475, 0.8490355571266264, 0.8213914389489219, 0.7923868710640818, 0.8144133180612698, 0.8669919540407136, 0.9348576570628211, 0.8393233169335872, 0.8109355359338224, 0.8370277570793405, 0.8215348580852151, 0.8004561670823023, 0.8629096240038052, 0.8095641051186249, 0.935330364969559, 0.8596518120029941, 0.8398514830041677, 0.831519890925847, 0.792153254034929, 0.8178386429790407, 0.7917564738309011, 0.8302881309064105, 0.8691932499641553, 0.8954914020141587, 0.7714617289602757, 0.8926027819979936, 0.7900326879462227, 0.8851078419247642, 0.8840164471184835, 0.9111782860709354, 0.8318207709817216, 0.796443049213849, 0.8272587009705603, 0.7988031810382381, 0.8360161200398579, 0.7977994851535186, 0.9124379379209131, 0.9055363670922816, 0.8417486670659855, 0.9293455659644678, 0.8183746428694576, 0.8477610810659826, 0.7913225519005209, 0.8569914470426738, 0.7893851139815524, 0.9487766709644347, 0.8626759110484272, 0.8092558379285038, 0.9126394271152094, 0.872535330010578, 0.9054140510270372, 0.7743889710400254, 0.9153909710003063, 0.9928418180206791, 0.7939139609225094]
Total Epoch List: [97, 89]
Total Time List: [0.2591159560251981, 0.24894950597081333]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288336b90>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.21s
Epoch 3/1000, LR 0.000030
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.22s
Epoch 4/1000, LR 0.000060
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.23s
Epoch 6/1000, LR 0.000120
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.21s
Epoch 7/1000, LR 0.000150
Train loss: 0.6941;  Loss pred: 0.6941; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.23s
Epoch 8/1000, LR 0.000180
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.21s
Epoch 9/1000, LR 0.000210
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.20s
Epoch 11/1000, LR 0.000270
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.23s
Epoch 12/1000, LR 0.000270
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.20s
Epoch 13/1000, LR 0.000270
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.33s
Epoch 14/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.23s
Epoch 15/1000, LR 0.000270
Train loss: 0.6928;  Loss pred: 0.6928; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6924 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.21s
Epoch 16/1000, LR 0.000270
Train loss: 0.6926;  Loss pred: 0.6926; Loss self: 0.0000; time: 0.41s
Val loss: 0.6923 score: 0.5455 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.5116 time: 0.22s
Epoch 17/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.39s
Val loss: 0.6921 score: 0.6818 time: 0.24s
Test loss: 0.6922 score: 0.6977 time: 0.20s
Epoch 18/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.39s
Val loss: 0.6919 score: 0.6364 time: 0.22s
Test loss: 0.6921 score: 0.6047 time: 0.22s
Epoch 19/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.36s
Val loss: 0.6918 score: 0.5000 time: 0.24s
Test loss: 0.6920 score: 0.5116 time: 0.21s
Epoch 20/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.40s
Val loss: 0.6916 score: 0.5455 time: 0.21s
Test loss: 0.6919 score: 0.5349 time: 0.23s
Epoch 21/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.22s
Epoch 22/1000, LR 0.000270
Train loss: 0.6912;  Loss pred: 0.6912; Loss self: 0.0000; time: 0.48s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6917 score: 0.4884 time: 0.23s
Epoch 23/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6908 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6915 score: 0.4884 time: 0.22s
Epoch 25/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.22s
Epoch 26/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.23s
Epoch 27/1000, LR 0.000270
Train loss: 0.6897;  Loss pred: 0.6897; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6900 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6909 score: 0.4884 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6894;  Loss pred: 0.6894; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6896 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.22s
Epoch 29/1000, LR 0.000270
Train loss: 0.6889;  Loss pred: 0.6889; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6892 score: 0.5000 time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6904 score: 0.4884 time: 0.39s
Epoch 30/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6888 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6901 score: 0.4884 time: 0.21s
Epoch 31/1000, LR 0.000270
Train loss: 0.6877;  Loss pred: 0.6877; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6883 score: 0.5000 time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6898 score: 0.4884 time: 0.22s
Epoch 32/1000, LR 0.000270
Train loss: 0.6873;  Loss pred: 0.6873; Loss self: 0.0000; time: 0.45s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6878 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4884 time: 0.23s
Epoch 33/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6872 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4884 time: 0.21s
Epoch 34/1000, LR 0.000270
Train loss: 0.6858;  Loss pred: 0.6858; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6865 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6884 score: 0.4884 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6847;  Loss pred: 0.6847; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6858 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6878 score: 0.4884 time: 0.19s
Epoch 36/1000, LR 0.000270
Train loss: 0.6842;  Loss pred: 0.6842; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6850 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6872 score: 0.4884 time: 0.24s
Epoch 37/1000, LR 0.000270
Train loss: 0.6829;  Loss pred: 0.6829; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6842 score: 0.5000 time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6865 score: 0.4884 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6824;  Loss pred: 0.6824; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6833 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6857 score: 0.4884 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6802;  Loss pred: 0.6802; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6822 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6848 score: 0.4884 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 0.6792;  Loss pred: 0.6792; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6811 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6838 score: 0.4884 time: 0.25s
Epoch 41/1000, LR 0.000269
Train loss: 0.6777;  Loss pred: 0.6777; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6799 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6827 score: 0.4884 time: 0.22s
Epoch 42/1000, LR 0.000269
Train loss: 0.6766;  Loss pred: 0.6766; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6785 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6815 score: 0.4884 time: 0.21s
Epoch 43/1000, LR 0.000269
Train loss: 0.6742;  Loss pred: 0.6742; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6770 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6801 score: 0.4884 time: 0.23s
Epoch 44/1000, LR 0.000269
Train loss: 0.6728;  Loss pred: 0.6728; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6754 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6786 score: 0.4884 time: 0.20s
Epoch 45/1000, LR 0.000269
Train loss: 0.6710;  Loss pred: 0.6710; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6736 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6769 score: 0.4884 time: 0.22s
Epoch 46/1000, LR 0.000269
Train loss: 0.6683;  Loss pred: 0.6683; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6716 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6750 score: 0.4884 time: 0.22s
Epoch 47/1000, LR 0.000269
Train loss: 0.6656;  Loss pred: 0.6656; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6694 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6730 score: 0.4884 time: 0.21s
Epoch 48/1000, LR 0.000269
Train loss: 0.6625;  Loss pred: 0.6625; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6671 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6707 score: 0.4884 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6601;  Loss pred: 0.6601; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6645 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6682 score: 0.4884 time: 0.21s
Epoch 50/1000, LR 0.000269
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6617 score: 0.5000 time: 0.21s
Test loss: 0.6655 score: 0.5116 time: 0.22s
Epoch 51/1000, LR 0.000269
Train loss: 0.6508;  Loss pred: 0.6508; Loss self: 0.0000; time: 0.32s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6586 score: 0.5000 time: 0.24s
Test loss: 0.6624 score: 0.5116 time: 0.22s
Epoch 52/1000, LR 0.000269
Train loss: 0.6448;  Loss pred: 0.6448; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6552 score: 0.5000 time: 0.22s
Test loss: 0.6591 score: 0.5116 time: 0.30s
Epoch 53/1000, LR 0.000269
Train loss: 0.6416;  Loss pred: 0.6416; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6516 score: 0.5000 time: 0.22s
Test loss: 0.6555 score: 0.5116 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.6343;  Loss pred: 0.6343; Loss self: 0.0000; time: 0.38s
Val loss: 0.6475 score: 0.5227 time: 0.21s
Test loss: 0.6515 score: 0.5349 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6290;  Loss pred: 0.6290; Loss self: 0.0000; time: 0.33s
Val loss: 0.6432 score: 0.5227 time: 0.24s
Test loss: 0.6473 score: 0.5349 time: 0.22s
Epoch 56/1000, LR 0.000269
Train loss: 0.6221;  Loss pred: 0.6221; Loss self: 0.0000; time: 0.40s
Val loss: 0.6384 score: 0.5455 time: 0.22s
Test loss: 0.6426 score: 0.5349 time: 0.23s
Epoch 57/1000, LR 0.000269
Train loss: 0.6159;  Loss pred: 0.6159; Loss self: 0.0000; time: 0.33s
Val loss: 0.6333 score: 0.5909 time: 0.23s
Test loss: 0.6376 score: 0.5349 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.6089;  Loss pred: 0.6089; Loss self: 0.0000; time: 0.40s
Val loss: 0.6277 score: 0.6364 time: 0.21s
Test loss: 0.6322 score: 0.5349 time: 0.23s
Epoch 59/1000, LR 0.000268
Train loss: 0.6014;  Loss pred: 0.6014; Loss self: 0.0000; time: 0.33s
Val loss: 0.6218 score: 0.6364 time: 0.23s
Test loss: 0.6263 score: 0.5349 time: 0.20s
Epoch 60/1000, LR 0.000268
Train loss: 0.5905;  Loss pred: 0.5905; Loss self: 0.0000; time: 0.38s
Val loss: 0.6154 score: 0.6591 time: 0.21s
Test loss: 0.6201 score: 0.5814 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5825;  Loss pred: 0.5825; Loss self: 0.0000; time: 0.42s
Val loss: 0.6086 score: 0.6818 time: 0.23s
Test loss: 0.6135 score: 0.6279 time: 0.20s
Epoch 62/1000, LR 0.000268
Train loss: 0.5725;  Loss pred: 0.5725; Loss self: 0.0000; time: 0.50s
Val loss: 0.6015 score: 0.6818 time: 0.24s
Test loss: 0.6064 score: 0.6512 time: 0.22s
Epoch 63/1000, LR 0.000268
Train loss: 0.5642;  Loss pred: 0.5642; Loss self: 0.0000; time: 0.39s
Val loss: 0.5940 score: 0.6818 time: 0.21s
Test loss: 0.5989 score: 0.6512 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5562;  Loss pred: 0.5562; Loss self: 0.0000; time: 0.34s
Val loss: 0.5862 score: 0.7045 time: 0.24s
Test loss: 0.5910 score: 0.6977 time: 0.22s
Epoch 65/1000, LR 0.000268
Train loss: 0.5436;  Loss pred: 0.5436; Loss self: 0.0000; time: 0.40s
Val loss: 0.5781 score: 0.7273 time: 0.21s
Test loss: 0.5831 score: 0.7209 time: 0.23s
Epoch 66/1000, LR 0.000268
Train loss: 0.5325;  Loss pred: 0.5325; Loss self: 0.0000; time: 0.34s
Val loss: 0.5697 score: 0.7273 time: 0.27s
Test loss: 0.5749 score: 0.7209 time: 0.20s
Epoch 67/1000, LR 0.000268
Train loss: 0.5154;  Loss pred: 0.5154; Loss self: 0.0000; time: 0.38s
Val loss: 0.5611 score: 0.7500 time: 0.21s
Test loss: 0.5666 score: 0.7442 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.5076;  Loss pred: 0.5076; Loss self: 0.0000; time: 0.34s
Val loss: 0.5523 score: 0.7727 time: 0.24s
Test loss: 0.5581 score: 0.7442 time: 0.20s
Epoch 69/1000, LR 0.000268
Train loss: 0.4913;  Loss pred: 0.4913; Loss self: 0.0000; time: 0.50s
Val loss: 0.5434 score: 0.7955 time: 0.23s
Test loss: 0.5499 score: 0.7907 time: 0.31s
Epoch 70/1000, LR 0.000268
Train loss: 0.4813;  Loss pred: 0.4813; Loss self: 0.0000; time: 0.37s
Val loss: 0.5343 score: 0.7955 time: 0.21s
Test loss: 0.5414 score: 0.7907 time: 0.23s
Epoch 71/1000, LR 0.000268
Train loss: 0.4740;  Loss pred: 0.4740; Loss self: 0.0000; time: 0.49s
Val loss: 0.5250 score: 0.7955 time: 0.22s
Test loss: 0.5331 score: 0.7907 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.4508;  Loss pred: 0.4508; Loss self: 0.0000; time: 0.38s
Val loss: 0.5159 score: 0.7955 time: 0.23s
Test loss: 0.5255 score: 0.7907 time: 0.22s
Epoch 73/1000, LR 0.000267
Train loss: 0.4360;  Loss pred: 0.4360; Loss self: 0.0000; time: 0.41s
Val loss: 0.5068 score: 0.7955 time: 0.22s
Test loss: 0.5182 score: 0.8140 time: 0.23s
Epoch 74/1000, LR 0.000267
Train loss: 0.4264;  Loss pred: 0.4264; Loss self: 0.0000; time: 0.38s
Val loss: 0.4974 score: 0.8182 time: 0.23s
Test loss: 0.5108 score: 0.7907 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.4155;  Loss pred: 0.4155; Loss self: 0.0000; time: 0.38s
Val loss: 0.4879 score: 0.8182 time: 0.22s
Test loss: 0.5033 score: 0.8140 time: 0.22s
Epoch 76/1000, LR 0.000267
Train loss: 0.4037;  Loss pred: 0.4037; Loss self: 0.0000; time: 0.51s
Val loss: 0.4780 score: 0.8409 time: 0.24s
Test loss: 0.4953 score: 0.7907 time: 0.22s
Epoch 77/1000, LR 0.000267
Train loss: 0.3897;  Loss pred: 0.3897; Loss self: 0.0000; time: 0.39s
Val loss: 0.4681 score: 0.8182 time: 0.22s
Test loss: 0.4874 score: 0.7907 time: 0.23s
Epoch 78/1000, LR 0.000267
Train loss: 0.3792;  Loss pred: 0.3792; Loss self: 0.0000; time: 0.36s
Val loss: 0.4579 score: 0.8409 time: 0.24s
Test loss: 0.4789 score: 0.7907 time: 0.21s
Epoch 79/1000, LR 0.000267
Train loss: 0.3699;  Loss pred: 0.3699; Loss self: 0.0000; time: 0.38s
Val loss: 0.4482 score: 0.8409 time: 0.22s
Test loss: 0.4713 score: 0.7907 time: 0.22s
Epoch 80/1000, LR 0.000267
Train loss: 0.3415;  Loss pred: 0.3415; Loss self: 0.0000; time: 0.36s
Val loss: 0.4385 score: 0.8636 time: 0.24s
Test loss: 0.4637 score: 0.7907 time: 0.21s
Epoch 81/1000, LR 0.000267
Train loss: 0.3340;  Loss pred: 0.3340; Loss self: 0.0000; time: 0.39s
Val loss: 0.4293 score: 0.8636 time: 0.23s
Test loss: 0.4575 score: 0.7907 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.3083;  Loss pred: 0.3083; Loss self: 0.0000; time: 0.35s
Val loss: 0.4202 score: 0.8864 time: 0.24s
Test loss: 0.4512 score: 0.7907 time: 0.20s
Epoch 83/1000, LR 0.000266
Train loss: 0.3011;  Loss pred: 0.3011; Loss self: 0.0000; time: 0.51s
Val loss: 0.4115 score: 0.8864 time: 0.23s
Test loss: 0.4458 score: 0.7907 time: 0.20s
Epoch 84/1000, LR 0.000266
Train loss: 0.2883;  Loss pred: 0.2883; Loss self: 0.0000; time: 0.43s
Val loss: 0.4030 score: 0.8864 time: 0.21s
Test loss: 0.4423 score: 0.7907 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.3021;  Loss pred: 0.3021; Loss self: 0.0000; time: 0.35s
Val loss: 0.3945 score: 0.9091 time: 0.24s
Test loss: 0.4372 score: 0.8140 time: 0.21s
Epoch 86/1000, LR 0.000266
Train loss: 0.2849;  Loss pred: 0.2849; Loss self: 0.0000; time: 0.40s
Val loss: 0.3862 score: 0.9091 time: 0.21s
Test loss: 0.4328 score: 0.8140 time: 0.23s
Epoch 87/1000, LR 0.000266
Train loss: 0.2779;  Loss pred: 0.2779; Loss self: 0.0000; time: 0.35s
Val loss: 0.3781 score: 0.9091 time: 0.24s
Test loss: 0.4276 score: 0.8140 time: 0.22s
Epoch 88/1000, LR 0.000266
Train loss: 0.2537;  Loss pred: 0.2537; Loss self: 0.0000; time: 0.41s
Val loss: 0.3701 score: 0.9091 time: 0.25s
Test loss: 0.4215 score: 0.8140 time: 0.24s
Epoch 89/1000, LR 0.000266
Train loss: 0.2310;  Loss pred: 0.2310; Loss self: 0.0000; time: 0.44s
Val loss: 0.3622 score: 0.9091 time: 0.23s
Test loss: 0.4171 score: 0.8140 time: 0.28s
Epoch 90/1000, LR 0.000266
Train loss: 0.2295;  Loss pred: 0.2295; Loss self: 0.0000; time: 0.39s
Val loss: 0.3544 score: 0.9091 time: 0.23s
Test loss: 0.4176 score: 0.8140 time: 0.21s
Epoch 91/1000, LR 0.000266
Train loss: 0.2110;  Loss pred: 0.2110; Loss self: 0.0000; time: 0.50s
Val loss: 0.3468 score: 0.9091 time: 0.34s
Test loss: 0.4189 score: 0.8140 time: 0.23s
Epoch 92/1000, LR 0.000266
Train loss: 0.2028;  Loss pred: 0.2028; Loss self: 0.0000; time: 0.38s
Val loss: 0.3397 score: 0.9091 time: 0.23s
Test loss: 0.4218 score: 0.8140 time: 0.21s
Epoch 93/1000, LR 0.000265
Train loss: 0.2064;  Loss pred: 0.2064; Loss self: 0.0000; time: 0.40s
Val loss: 0.3331 score: 0.8864 time: 0.25s
Test loss: 0.4261 score: 0.8140 time: 0.23s
Epoch 94/1000, LR 0.000265
Train loss: 0.1897;  Loss pred: 0.1897; Loss self: 0.0000; time: 0.40s
Val loss: 0.3263 score: 0.8864 time: 0.22s
Test loss: 0.4261 score: 0.8140 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.1828;  Loss pred: 0.1828; Loss self: 0.0000; time: 0.40s
Val loss: 0.3193 score: 0.9091 time: 0.26s
Test loss: 0.4221 score: 0.8140 time: 0.21s
Epoch 96/1000, LR 0.000265
Train loss: 0.1970;  Loss pred: 0.1970; Loss self: 0.0000; time: 0.49s
Val loss: 0.3127 score: 0.9091 time: 0.22s
Test loss: 0.4110 score: 0.8140 time: 0.23s
Epoch 97/1000, LR 0.000265
Train loss: 0.1573;  Loss pred: 0.1573; Loss self: 0.0000; time: 0.40s
Val loss: 0.3077 score: 0.9091 time: 0.26s
Test loss: 0.3996 score: 0.8140 time: 0.21s
Epoch 98/1000, LR 0.000265
Train loss: 0.1338;  Loss pred: 0.1338; Loss self: 0.0000; time: 0.40s
Val loss: 0.3037 score: 0.9091 time: 0.24s
Test loss: 0.3918 score: 0.8140 time: 0.33s
Epoch 99/1000, LR 0.000265
Train loss: 0.1414;  Loss pred: 0.1414; Loss self: 0.0000; time: 0.50s
Val loss: 0.3015 score: 0.9318 time: 0.34s
Test loss: 0.3834 score: 0.8372 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.1250;  Loss pred: 0.1250; Loss self: 0.0000; time: 0.40s
Val loss: 0.2995 score: 0.9318 time: 0.22s
Test loss: 0.3782 score: 0.8372 time: 0.23s
Epoch 101/1000, LR 0.000265
Train loss: 0.1218;  Loss pred: 0.1218; Loss self: 0.0000; time: 0.39s
Val loss: 0.2956 score: 0.9318 time: 0.28s
Test loss: 0.3788 score: 0.8372 time: 0.21s
Epoch 102/1000, LR 0.000264
Train loss: 0.1236;  Loss pred: 0.1236; Loss self: 0.0000; time: 0.46s
Val loss: 0.2899 score: 0.9318 time: 0.22s
Test loss: 0.3840 score: 0.8372 time: 0.24s
Epoch 103/1000, LR 0.000264
Train loss: 0.1330;  Loss pred: 0.1330; Loss self: 0.0000; time: 0.33s
Val loss: 0.2862 score: 0.9318 time: 0.25s
Test loss: 0.3861 score: 0.8372 time: 0.20s
Epoch 104/1000, LR 0.000264
Train loss: 0.0921;  Loss pred: 0.0921; Loss self: 0.0000; time: 0.37s
Val loss: 0.2830 score: 0.9318 time: 0.21s
Test loss: 0.3885 score: 0.8372 time: 0.23s
Epoch 105/1000, LR 0.000264
Train loss: 0.1017;  Loss pred: 0.1017; Loss self: 0.0000; time: 0.34s
Val loss: 0.2784 score: 0.9318 time: 0.24s
Test loss: 0.3955 score: 0.8372 time: 0.22s
Epoch 106/1000, LR 0.000264
Train loss: 0.0906;  Loss pred: 0.0906; Loss self: 0.0000; time: 0.39s
Val loss: 0.2742 score: 0.9091 time: 0.21s
Test loss: 0.4038 score: 0.8372 time: 0.33s
Epoch 107/1000, LR 0.000264
Train loss: 0.0845;  Loss pred: 0.0845; Loss self: 0.0000; time: 0.45s
Val loss: 0.2707 score: 0.9091 time: 0.23s
Test loss: 0.4127 score: 0.8140 time: 0.23s
Epoch 108/1000, LR 0.000264
Train loss: 0.0819;  Loss pred: 0.0819; Loss self: 0.0000; time: 0.37s
Val loss: 0.2674 score: 0.9091 time: 0.24s
Test loss: 0.4252 score: 0.8140 time: 0.21s
Epoch 109/1000, LR 0.000264
Train loss: 0.0750;  Loss pred: 0.0750; Loss self: 0.0000; time: 0.44s
Val loss: 0.2650 score: 0.9091 time: 0.22s
Test loss: 0.4369 score: 0.8140 time: 0.23s
Epoch 110/1000, LR 0.000263
Train loss: 0.0646;  Loss pred: 0.0646; Loss self: 0.0000; time: 0.36s
Val loss: 0.2635 score: 0.9091 time: 0.24s
Test loss: 0.4459 score: 0.8140 time: 0.20s
Epoch 111/1000, LR 0.000263
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.45s
Val loss: 0.2624 score: 0.9091 time: 0.23s
Test loss: 0.4547 score: 0.8140 time: 0.29s
Epoch 112/1000, LR 0.000263
Train loss: 0.0629;  Loss pred: 0.0629; Loss self: 0.0000; time: 0.40s
Val loss: 0.2616 score: 0.9091 time: 0.22s
Test loss: 0.4617 score: 0.8140 time: 0.23s
Epoch 113/1000, LR 0.000263
Train loss: 0.0822;  Loss pred: 0.0822; Loss self: 0.0000; time: 0.36s
Val loss: 0.2615 score: 0.9091 time: 0.25s
Test loss: 0.4586 score: 0.8140 time: 0.21s
Epoch 114/1000, LR 0.000263
Train loss: 0.0677;  Loss pred: 0.0677; Loss self: 0.0000; time: 0.51s
Val loss: 0.2614 score: 0.9091 time: 0.23s
Test loss: 0.4632 score: 0.8140 time: 0.21s
Epoch 115/1000, LR 0.000263
Train loss: 0.0471;  Loss pred: 0.0471; Loss self: 0.0000; time: 0.42s
Val loss: 0.2615 score: 0.9091 time: 0.22s
Test loss: 0.4674 score: 0.8140 time: 0.22s
     INFO: Early stopping counter 1 of 2
Epoch 116/1000, LR 0.000263
Train loss: 0.1048;  Loss pred: 0.1048; Loss self: 0.0000; time: 0.37s
Val loss: 0.2614 score: 0.9091 time: 0.23s
Test loss: 0.4785 score: 0.8140 time: 0.20s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 113,   Train_Loss: 0.0677,   Val_Loss: 0.2614,   Val_Precision: 0.9500,   Val_Recall: 0.8636,   Val_accuracy: 0.9048,   Val_Score: 0.9091,   Val_Loss: 0.2614,   Test_Precision: 0.8095,   Test_Recall: 0.8095,   Test_accuracy: 0.8095,   Test_Score: 0.8140,   Test_loss: 0.4632


[0.2497985300142318, 0.32745955989230424, 0.25860901991836727, 0.22938486700877547, 0.2560368539998308, 0.23099367099348456, 0.24871338508091867, 0.3258730289526284, 0.2521289469441399, 0.25767703901510686, 0.24736589996609837, 0.25654872797895223, 0.23171718197409064, 0.24934374098666012, 0.33275324502028525, 0.2500969209941104, 0.24809519504196942, 0.25609403708949685, 0.24471755407284945, 0.25662262504920363, 0.24853688501752913, 0.2572587139438838, 0.2541353029664606, 0.25713114405516535, 0.2508298170287162, 0.25474247604142874, 0.23634351103100926, 0.2528658659430221, 0.23292927793227136, 0.2529120879480615, 0.23483967501670122, 0.2575533849885687, 0.24814328702632338, 0.24999670893885195, 0.2469879719428718, 0.25520091399084777, 0.2346758390776813, 0.25641757098492235, 0.32292912795674056, 0.2756702930200845, 0.24478805402759463, 0.25107796606607735, 0.23320562997832894, 0.2598322540288791, 0.23266700096428394, 0.3234885160345584, 0.2620240399846807, 0.24345859105233103, 0.2450440980028361, 0.2539227369707078, 0.2492432571016252, 0.38411563099361956, 0.23875079897698015, 0.25360120506957173, 0.2659267050912604, 0.253873456036672, 0.2505510658957064, 0.2551003940170631, 0.2385756679577753, 0.2484913250664249, 0.23582218994852155, 0.34310909803025424, 0.24119721597526222, 0.24683881900273263, 0.24486084200907499, 0.252111655077897, 0.2507264600135386, 0.23981428798288107, 0.25302124209702015, 0.3285796850686893, 0.2484757910715416, 0.251142728026025, 0.2622546700295061, 0.25552358699496835, 0.25563455803785473, 0.3021004729671404, 0.25950399599969387, 0.2444445510627702, 0.24512262002099305, 0.23292591399513185, 0.2541733719408512, 0.24245301797054708, 0.2570711220614612, 0.24241339298896492, 0.2464832979021594, 0.2320937339682132, 0.25882383494172245, 0.22883157501928508, 0.25482657097745687, 0.24171577801462263, 0.25847092701587826, 0.24828070006333292, 0.25162430095952004, 0.23141626897267997, 0.25869310600683093, 0.24723731097765267, 0.25864988600369543, 0.2413100799312815, 0.25171287008561194, 0.24160026700701565, 0.2528091899584979, 0.22962563193868846, 0.23839664901606739, 0.22977831005118787, 0.23907785397022963, 0.23890278697945178, 0.2281447669956833, 0.26915517391171306, 0.22865108796395361, 0.26681728300172836, 0.227431857958436, 0.2449376389849931, 0.2292591850273311, 0.2521258459892124, 0.22697411803528666, 0.2401684180367738, 0.22784099297132343, 0.23907131899613887, 0.22846162703353912, 0.25515378592535853, 0.2376494479831308, 0.24420176399871707, 0.24231249396689236, 0.2496304550440982, 0.24122482095845044, 0.24481899698730558, 0.22689327702391893, 0.24414298892952502, 0.23458016000222415, 0.24716039199847728, 0.22453731892164797, 0.23843483196105808, 0.24953836505301297, 0.2535892160376534, 0.2288960110163316, 0.2504239580594003, 0.32908025407232344, 0.24541229207534343, 0.24146251298952848, 0.2416575860697776, 0.23789329396095127, 0.24748750100843608, 0.23005351808387786, 0.24709174397867173, 0.22826743801124394, 0.22962212399579585, 0.266580912983045, 0.24573359999340028, 0.2522175550693646, 0.24233591207303107, 0.2535888010170311, 0.24096664402168244, 0.25291049701627344, 0.23257419501896948, 0.2546249870210886, 0.23455452697817236, 0.2708364389836788, 0.22481738298665732, 0.24178545200265944, 0.24748341599479318, 0.24074248794931918, 0.25517577095888555, 0.24733015103265643, 0.25405953207518905, 0.24645568802952766, 0.2568706440506503, 0.2429547639330849, 0.24350356694776565, 0.24492015107534826, 0.23723668104503304, 0.2591209770180285, 0.2446586670121178, 0.25674472691025585, 0.24405030498746783, 0.2560999769484624, 0.22918542905244976, 0.23367390595376492, 0.2455861900234595, 0.24117685202509165, 0.23911398299969733, 0.24093195993918926, 0.24857717205304652, 0.2305697980336845, 0.2415757739217952, 0.2628975809784606, 0.24832868098746985, 0.23110025201458484, 0.21551656699739397, 0.22955728694796562, 0.2406867960235104, 0.23366781196091324, 0.2190537479473278, 0.23444930999539793, 0.21725149196572602, 0.2368118220474571, 0.20857100805733353, 0.23517176799941808, 0.20430254703387618, 0.3301939599914476, 0.2348960629897192, 0.21653041196987033, 0.22750658891163766, 0.20566243294160813, 0.22377274499740452, 0.21511299000121653, 0.23131312103942037, 0.2197441910393536, 0.23866896599065512, 0.2458211110206321, 0.22178532800171524, 0.21984337095636874, 0.23635690694209188, 0.26319316297303885, 0.22745288896840066, 0.38953063904773444, 0.2138305699918419, 0.22734960506204516, 0.2294356330530718, 0.21342598902992904, 0.22206151904538274, 0.19819737202487886, 0.24108315096236765, 0.20618649700190872, 0.2359325890429318, 0.21087745996192098, 0.2509099820163101, 0.22075031592976302, 0.21722025203052908, 0.23039719101507217, 0.2016648059943691, 0.2234623630065471, 0.22106334299314767, 0.21400476689450443, 0.22447904897853732, 0.21776902792043984, 0.22770155395846814, 0.2222806609934196, 0.30836350994650275, 0.20485360699240118, 0.23317084996961057, 0.22130165004637092, 0.23455513000953943, 0.21987540507689118, 0.2304433030076325, 0.20513064600527287, 0.23394302104134113, 0.208489014999941, 0.22839871398173273, 0.2355326151009649, 0.2214358999626711, 0.23774261691141874, 0.2089978609001264, 0.22795380000025034, 0.20783601701259613, 0.31784483103547245, 0.23362391197588295, 0.22384160803630948, 0.2197566459653899, 0.2299494449980557, 0.22233003901783377, 0.22207842394709587, 0.22429858497343957, 0.23458010202739388, 0.21906727994792163, 0.22688071697484702, 0.20952813397161663, 0.23670220596250147, 0.20695049304049462, 0.2027450129389763, 0.23258681094739586, 0.21686848800163716, 0.23204880708362907, 0.22644195100292563, 0.24316549499053508, 0.28532840800471604, 0.21655412996187806, 0.23026404192205518, 0.21555523201823235, 0.23018471605610102, 0.22102055908180773, 0.212492426042445, 0.2396113600116223, 0.21012478100601584, 0.3373470170190558, 0.2269557979889214, 0.22987198003102094, 0.2126739489613101, 0.24553991307038814, 0.20816361997276545, 0.2327129829209298, 0.22103295603301376, 0.33674774202518165, 0.22991737292613834, 0.21894934098236263, 0.23386536596808583, 0.20784234500024468, 0.2986278859898448, 0.2378363290335983, 0.21721234696451575, 0.21743759501259774, 0.22428876801859587, 0.20756410504691303]
[0.005677239318505268, 0.0074422627248250965, 0.005877477725417438, 0.005213292432017624, 0.005819019409087064, 0.005249856158942831, 0.005652576933657242, 0.007406205203468827, 0.005730203339639543, 0.005856296341252429, 0.005621952271956781, 0.005830652908612551, 0.005266299590320242, 0.005666903204242276, 0.007562573750461029, 0.0056840209316843275, 0.0056385271600447595, 0.005820319024761292, 0.00556176259256476, 0.005832332387481901, 0.005648565568580208, 0.005846788953270086, 0.005775802340146832, 0.005843889637617394, 0.00570067765974355, 0.005789601728214289, 0.005371443432522938, 0.005746951498705047, 0.00529384722573344, 0.00574800199881958, 0.005337265341288664, 0.005853486022467471, 0.005639620159689168, 0.005681743384973908, 0.005613362998701632, 0.005800020772519268, 0.005333541797220029, 0.005827672067839144, 0.007339298362653194, 0.0062652339322746475, 0.005563364864263514, 0.005706317410592667, 0.00530012795405293, 0.0059052785006563436, 0.0052878863855519076, 0.007352011728058146, 0.005955091817833652, 0.005533149796643887, 0.005569184045519002, 0.005770971294788813, 0.005664619479582391, 0.008729900704400445, 0.005426154522204094, 0.005763663751581175, 0.0060437887520741, 0.005769851273560727, 0.005694342406720601, 0.0057977362276605245, 0.005422174271767621, 0.005647530115146021, 0.005359595226102762, 0.007797934046142142, 0.005481754908528687, 0.005609973159153014, 0.005565019136569886, 0.005729810342679478, 0.005698328636671332, 0.00545032472688366, 0.005750482774932276, 0.007467720115197484, 0.0056471770698077635, 0.00570778927331875, 0.005960333409761502, 0.005807354249885644, 0.005809876319042153, 0.006865919840162282, 0.005897818090902133, 0.005555557978699322, 0.005570968636840751, 0.005293770772616633, 0.005776667544110255, 0.005510295862966979, 0.005842525501396845, 0.005509395295203748, 0.005601893134139987, 0.005274857590186663, 0.005882359885039146, 0.005200717614074661, 0.005791512976760383, 0.005493540409423242, 0.005874339250360869, 0.005642743183257567, 0.0057187341127163645, 0.0052594606584699995, 0.005879388772882521, 0.005619029794946651, 0.005878406500083987, 0.005611862323983291, 0.00585378767640958, 0.0056186108606282705, 0.005879283487406927, 0.005340130975318336, 0.00554410811665273, 0.0053436816290973925, 0.005559950092330922, 0.005555878766963995, 0.005305692255713566, 0.006259422649109606, 0.00531746716195241, 0.0062050530930634505, 0.0052891129757775815, 0.0056962241624417, 0.005331608954123979, 0.005863391767190986, 0.0052784678612857365, 0.005585312047366833, 0.005298627743519149, 0.005559798116189276, 0.005313061093803235, 0.005933808975008338, 0.005526731348444902, 0.005679110790667839, 0.005635174278299822, 0.00580535941963019, 0.005609879557173266, 0.005693465046216409, 0.005276587837765557, 0.005677743928593605, 0.005455352558191259, 0.005747916092987844, 0.00522179811445693, 0.0055449960921176294, 0.0058032177919305344, 0.005897423628782637, 0.005323163046891433, 0.005823812978125589, 0.007653029164472638, 0.005707262606403336, 0.0056154072788262436, 0.005619943862087851, 0.005532402185138402, 0.005755523279265955, 0.005350081815904136, 0.00574631962741097, 0.005308545070028928, 0.005340049395251067, 0.006199556115884767, 0.005714734883567448, 0.005865524536496852, 0.005635718885419327, 0.005897413977140257, 0.005603875442364708, 0.0058816394654947314, 0.0054087022097434765, 0.0059215113260718275, 0.005454756441352846, 0.00629852183682974, 0.0052283112322478445, 0.005622917488433941, 0.005755428278948678, 0.0055986625104492834, 0.005934320254857803, 0.005751863977503638, 0.0059083612110509085, 0.005731527628593667, 0.005973735908154658, 0.00565011078914151, 0.005662873649948039, 0.005695817466868564, 0.00551713211732635, 0.006026069232977407, 0.005689736442142274, 0.00597080760256409, 0.005675588488080647, 0.005955813417406103, 0.005329893698894181, 0.005434276882645695, 0.005711306744731616, 0.005608764000583527, 0.005560790302318542, 0.005603068835795099, 0.005780864466349919, 0.005362088326364756, 0.005618041253995237, 0.006113897232057223, 0.005775085604359764, 0.0053744244654554615, 0.005012013185985906, 0.005338541556929433, 0.0055973673493839625, 0.005434135161881703, 0.00509427320807739, 0.005452309534776696, 0.005052360278272698, 0.005507251675522258, 0.0048504885594728726, 0.005469110883707397, 0.004751222024043632, 0.007678929302126689, 0.005462699139295796, 0.005035590976043496, 0.0052908509049218065, 0.004782847277711817, 0.0052040173255210355, 0.005002627674446896, 0.005379374907893497, 0.005110330024171014, 0.005550441069550119, 0.00571677002373563, 0.005157798325621285, 0.00511263653386904, 0.005496672254467253, 0.006120771231931136, 0.005289602069032573, 0.009058852070877544, 0.004972803953298649, 0.00528720011772198, 0.005335712396583065, 0.00496339509371928, 0.005164221373148436, 0.0046092412098809035, 0.0056065849061015735, 0.004795034813997877, 0.005486804396347251, 0.004904126975858627, 0.005835115860844421, 0.00513372827743635, 0.005051633768151839, 0.005358074209652841, 0.004689879209171374, 0.005196799139687141, 0.00514100797658483, 0.004976855044058243, 0.005220442999500868, 0.0050643959981497635, 0.005295384975778329, 0.005169317697521386, 0.007171244417360529, 0.004764037371916306, 0.0054225779062700135, 0.005146550001078393, 0.005454770465338126, 0.005113381513416074, 0.0053591465815728484, 0.004770480139657508, 0.005440535373054445, 0.004848581744184674, 0.00531159799957518, 0.005477502676766626, 0.005149672092155142, 0.0055288980677074126, 0.004860415369770382, 0.005301251162796519, 0.00483339574447898, 0.007391740256638894, 0.0054331142319972775, 0.0052056187915420805, 0.005110619673613719, 0.00534766151158269, 0.005170466023670553, 0.005164614510397578, 0.005216246162173014, 0.005455351209939393, 0.005094587905765619, 0.005276295743601093, 0.004872747301665503, 0.0055047024642442205, 0.004812802163732433, 0.004715000300906425, 0.005408995603427811, 0.005043453209340399, 0.005396483885665792, 0.005266091883788968, 0.005655011511407793, 0.0066355443722026985, 0.0050361425572529785, 0.005354977719117563, 0.0050129123725170316, 0.005353132931537233, 0.005140013001902506, 0.004941684326568488, 0.005572357209572612, 0.004886622814093392, 0.007845279465559437, 0.005278041813695847, 0.005345860000721417, 0.004945905789797909, 0.0057102305365206545, 0.00484101441797129, 0.0054119298353704605, 0.005140301303093343, 0.007831342837794922, 0.005346915649445077, 0.005091845139124712, 0.005438729441118275, 0.004833542906982434, 0.006944834557903367, 0.005531077419386007, 0.005051449929407343, 0.005056688256106924, 0.005216017860897579, 0.004827072210393326]
[176.14194926404565, 134.3677369335952, 170.14101060314553, 191.81736168461677, 171.8502602755347, 190.48140934234112, 176.91046256189486, 135.02191372332385, 174.51387686059056, 170.75638624293725, 177.87415325245001, 171.5073793061638, 189.88665244910428, 176.4632223206838, 132.23011543378843, 175.93179406249536, 177.35127837746583, 171.8118879301488, 179.79911644140466, 171.45799202842556, 177.0360966618565, 171.03405099660796, 173.13611877766894, 171.1189057306888, 175.41774148391085, 172.72345265594532, 186.1696976915394, 174.00529658642998, 188.89853774755545, 173.9734955216372, 187.3618671839451, 170.8383681385236, 177.31690640227725, 176.00231693754895, 178.1463269400713, 172.41317561103236, 187.49267147793313, 171.59510493369135, 136.25280654736795, 159.61095959220492, 179.74733356489668, 175.24437006320306, 188.67469024692406, 169.34002348726733, 189.11147613388593, 136.01719325114917, 167.92352336286575, 180.7288862135174, 179.55951748525993, 173.28105598151217, 176.534364506638, 114.54884011406158, 184.29257698208744, 173.5007528372322, 165.45912523114268, 173.3146926303481, 175.6129028735216, 172.48111344374067, 184.42786046306873, 177.0685555652224, 186.58125433236336, 128.23909436560677, 182.4233327988029, 178.2539722794286, 179.69390139714247, 174.5258464405581, 175.49005397206227, 183.47530653862, 173.89844281583422, 133.90967853293134, 177.07962538423487, 175.19917994774843, 167.77584931108984, 172.19545372485095, 172.12070362366427, 145.64690868519727, 169.55422913815903, 179.9999214901762, 179.5019978010667, 188.90126583734087, 173.11018720812052, 181.47845866511372, 171.15885925716842, 181.50812320011937, 178.5110811746183, 189.57857779144567, 169.99979932260555, 192.28115698758722, 172.66645244734877, 182.03197309419417, 170.2319116041125, 177.2187688723941, 174.86387376821162, 190.1335640546277, 170.08570765251918, 177.9666662204439, 170.11412871595604, 178.19396525932618, 170.82956459625981, 177.9799357537604, 170.08875352616352, 187.2613246045689, 180.371662846242, 187.1368972572775, 179.8577295467711, 179.98952855957458, 188.4768191979332, 159.7591433040951, 188.05945942745248, 161.15897559650008, 189.06761957622666, 175.5548889022913, 187.56064231351846, 170.5497499920727, 189.4489132602995, 179.04102609118232, 188.72811007021934, 179.8626459274041, 188.21541524646247, 168.52581608402633, 180.93877501054533, 176.083904128661, 177.4568009104606, 172.25462330869797, 178.25694648315854, 175.63996474599415, 189.51641302032485, 176.12629462979376, 183.30620969647347, 173.9760956531616, 191.50491422321116, 180.3427781349618, 172.31819239845103, 169.56557014480964, 187.8582322560963, 171.70881066339683, 130.66721405456826, 175.21534735024062, 178.08147305194652, 177.93772047190743, 180.7533086958651, 173.74614808743115, 186.9130294470095, 174.02443039016163, 188.3755316773736, 187.26418540047686, 161.30187086100526, 174.98624527193212, 170.48773622507832, 177.43965239060975, 169.56584765394317, 178.44793487737172, 170.02062194845627, 184.88723564750072, 168.87580634982476, 183.32624210660228, 158.76741018069313, 191.2663488416819, 177.8436198035895, 173.74901597812388, 178.6140882994127, 168.51129650129772, 173.85668435678292, 169.25166967273688, 174.4735548357407, 167.3994323443249, 176.98767994458075, 176.5887889815758, 175.56742395921233, 181.25358949798155, 165.94565401398688, 175.7550653125656, 167.4815312371751, 176.1931828038817, 167.9031779399704, 187.62100268669053, 184.0171234545463, 175.09127852788646, 178.2924009453708, 179.83055386624724, 178.47362388473954, 172.98450877389405, 186.4945034722979, 177.99798093131764, 163.56179406429388, 173.15760639895515, 186.0664349136506, 199.5206243263886, 187.31707702115736, 178.65541737403717, 184.02192257097363, 196.29885543131402, 183.40851589985084, 197.9272943579313, 181.57877266525486, 206.1647992235807, 182.84507688059904, 210.4721679895161, 130.22648870110692, 183.05968798583908, 198.58642307475657, 189.0055149862098, 209.08047904017843, 192.1592372676965, 199.89494823049424, 185.89520476303608, 195.68207831395736, 180.16586204041135, 174.92395108567771, 193.88117504178385, 195.59379849817716, 181.92825653508456, 163.37810418124297, 189.05013778907798, 110.38926258822642, 201.09379122751503, 189.13602241915058, 187.416398350179, 201.4749946594838, 193.6400335585027, 216.95544981596626, 178.36169731625986, 208.54905934796466, 182.25544921297603, 203.90989159185003, 171.376202949171, 194.79020819921035, 197.95575964048044, 186.63421984683407, 213.2251078118244, 192.42614022988815, 194.514384057482, 200.93010367940653, 191.55462478866468, 197.45691299916948, 188.84368267351846, 193.449127818065, 139.44581188435677, 209.9059939989001, 184.41413240807861, 194.30492267450288, 183.32577078254982, 195.56530201712533, 186.59687410649465, 209.6225056440113, 183.80544035293653, 206.2458782301416, 188.26725969848994, 182.5649495785004, 194.18712145252323, 180.86786693368276, 205.74373256647047, 188.63471457792227, 206.89388017570573, 135.28613902549534, 184.05650190652958, 192.1001210508859, 195.67098783793867, 186.99762463163842, 193.40616405213171, 193.62529342446874, 191.70874397220055, 183.30625499932015, 196.2867298586183, 189.526904592633, 205.2230370448721, 181.66286125281016, 207.7791619060606, 212.08906387720847, 184.87720703013252, 198.2768469325769, 185.30584380251972, 189.89414200659505, 176.83429962657212, 150.70353597349927, 198.5646729876252, 186.7421402016194, 199.48483549850891, 186.80649496085553, 194.55203705318715, 202.36015372807134, 179.45726779362334, 204.64030845923364, 127.46518519703126, 189.46420572211596, 187.0606412934591, 202.1874339100301, 175.1242780137066, 206.56827550186625, 184.7769705853083, 194.54112532240427, 127.69202175313919, 187.02370966031296, 196.39246141171918, 183.86647301109113, 206.88758106510673, 143.9919110617227, 180.79660148944504, 197.96296389645184, 197.75789001671353, 191.7171349232916, 207.16491413715906]
Elapsed: 0.24297683044692922~0.026961360848160972
Time per graph: 0.005607072977404754~0.000603466370915749
Speed: 180.06669684415553~16.29868132521065
Total Time: 0.2084
best val loss: 0.2613794207572937 test_score: 0.8140

Testing...
Test loss: 0.3834 score: 0.8372 time: 0.22s
test Score 0.8372
Epoch Time List: [0.8247207490494475, 0.9594387949910015, 0.7528684469871223, 0.8008802239783108, 0.8374526519328356, 0.7968455370282754, 0.8217005981132388, 0.9292726720450446, 0.8317684089997783, 0.9221130850492045, 0.7786981880199164, 0.8132974099135026, 0.7683717970503494, 0.8316199580440298, 0.8939715939341113, 0.8122237080242485, 0.7855531779350713, 0.827003040118143, 0.7827148829819635, 0.8732538809999824, 0.7805667839711532, 0.8952319369418547, 0.7890561139211059, 0.8291767020709813, 0.7886414459208027, 0.8437333110487089, 0.773493014043197, 0.8423521219519898, 0.7985178369563073, 0.835178000968881, 0.8113870199304074, 0.9287382970796898, 0.7756555470405146, 0.8210850391769782, 0.7774627038743347, 0.8269702589605004, 0.7698092651553452, 0.8602708399994299, 0.8550174020929262, 0.8806576308561489, 0.8983630340080708, 0.825564495055005, 0.8105708009097725, 0.8243050900055096, 0.7977071148343384, 0.8888733449857682, 0.9180228960467502, 0.7743497571209446, 0.9097699569538236, 0.8939186189090833, 0.8956154871266335, 1.1280851961346343, 0.797158862114884, 0.850575581076555, 0.8471479009604082, 0.8296630670083687, 0.8014388380106539, 0.8436017360072583, 0.7709037190070376, 0.8368428669637069, 0.8006790579529479, 0.9299801111919805, 0.988941426970996, 0.8212239319691435, 0.8243341819616035, 0.8220522220944986, 0.840741929016076, 0.8142986509483308, 0.8444773720111698, 0.882736028986983, 0.8072191709652543, 0.7866654510144144, 1.1222343500703573, 0.8611294019501656, 0.9047578909667209, 0.8811050849035382, 1.0740919369272888, 0.8233068980043754, 0.8193058430915698, 0.7875462149968371, 0.8648026889422908, 0.7721956020686775, 0.9307035650126636, 0.9469739039195701, 0.8278783999849111, 0.8171831460203975, 0.8303379679564387, 0.8011122940806672, 0.8110714779468253, 0.7642453399021178, 0.828912140103057, 0.7931228970410302, 0.9119933691108599, 0.7773288340540603, 0.8046813590917736, 0.7802793070441112, 0.8405244681052864, 0.7960834520636126, 0.8403372941538692, 0.8934082119958475, 0.848209252115339, 0.7951096849283203, 0.8178311639931053, 0.8814818609971553, 0.8658556620357558, 0.8142064539715648, 0.9059506360208616, 0.8534197120461613, 0.8454104620032012, 0.8809470180422068, 0.7971502779982984, 0.858454964007251, 0.8126257440308109, 0.8267992479959503, 0.7765154190128669, 0.899220593040809, 0.8089490691199899, 0.8088542360346764, 0.7684056849684566, 0.8179148220224306, 0.7700231760973111, 0.7984804550651461, 0.8049162570387125, 0.8992394719971344, 0.7709511689608917, 0.7996443558949977, 0.7735787429846823, 0.8196737258695066, 0.7874342320719734, 0.8426383751211688, 0.8529537671711296, 0.894829077180475, 0.8490355571266264, 0.8213914389489219, 0.7923868710640818, 0.8144133180612698, 0.8669919540407136, 0.9348576570628211, 0.8393233169335872, 0.8109355359338224, 0.8370277570793405, 0.8215348580852151, 0.8004561670823023, 0.8629096240038052, 0.8095641051186249, 0.935330364969559, 0.8596518120029941, 0.8398514830041677, 0.831519890925847, 0.792153254034929, 0.8178386429790407, 0.7917564738309011, 0.8302881309064105, 0.8691932499641553, 0.8954914020141587, 0.7714617289602757, 0.8926027819979936, 0.7900326879462227, 0.8851078419247642, 0.8840164471184835, 0.9111782860709354, 0.8318207709817216, 0.796443049213849, 0.8272587009705603, 0.7988031810382381, 0.8360161200398579, 0.7977994851535186, 0.9124379379209131, 0.9055363670922816, 0.8417486670659855, 0.9293455659644678, 0.8183746428694576, 0.8477610810659826, 0.7913225519005209, 0.8569914470426738, 0.7893851139815524, 0.9487766709644347, 0.8626759110484272, 0.8092558379285038, 0.9126394271152094, 0.872535330010578, 0.9054140510270372, 0.7743889710400254, 0.9153909710003063, 0.9928418180206791, 0.7939139609225094, 0.8507150000659749, 0.8667677239282057, 0.9936643940163776, 0.8504394680494443, 1.0055951989488676, 0.8402867129771039, 0.8519916847581044, 0.8226474660914391, 0.8574803830124438, 0.7832955550402403, 0.8250063469167799, 0.7942315380787477, 0.9198323029559106, 0.8653337651630864, 0.8397428389871493, 0.8513153509702533, 0.8292029470903799, 0.8377670190529898, 0.8126108620781451, 0.8299234671285376, 0.8083833049749956, 0.9392890969756991, 0.8609816532116383, 0.8418399260845035, 0.8450635190820321, 0.8209246151382104, 0.8623845679685473, 0.8313984600827098, 1.054291272885166, 0.8872952909441665, 0.9286707530263811, 0.8975053740432486, 0.8413772059138864, 0.8522444610716775, 0.8175363439368084, 0.8841105761239305, 0.8657515761442482, 0.8603557220194489, 0.816984754987061, 0.8864968279376626, 0.8541559932054952, 0.8678465819684789, 0.8212440350325778, 0.8837021011859179, 0.874216858879663, 0.899441123008728, 0.8410543160280213, 0.8153185530100018, 0.8443149519152939, 0.8114383600186557, 0.7776202679378912, 0.8925669321324676, 0.7582864199066535, 0.814657800947316, 0.7812165629584342, 0.8427445420529693, 0.7690330561017618, 0.8448420080821961, 0.7649325361708179, 0.8192870750790462, 0.8509741020388901, 0.9600599199766293, 0.8277086110319942, 0.795464699040167, 0.843824896030128, 0.8097393759526312, 0.8112111670197919, 0.7788838450796902, 1.0500037990277633, 0.815211845911108, 0.9278236269019544, 0.8286155450623482, 0.8609454580582678, 0.8235878149280325, 0.8238798410166055, 0.9637850639410317, 0.8408109060255811, 0.807793115847744, 0.8227948190178722, 0.8064413198735565, 0.8493640550877899, 0.7950924509204924, 0.942093945806846, 0.8709724780637771, 0.8051347659202293, 0.8415388069115579, 0.8089677219977602, 0.9000181101728231, 0.9489775489782915, 0.8363632389809936, 1.058082074043341, 0.8199187450809404, 0.8771818620152771, 0.8443311700830236, 0.8691006531007588, 0.9403416230343282, 0.857913153944537, 0.9692304959753528, 1.0610826801275834, 0.847694227937609, 0.8761199199361727, 0.918437733897008, 0.7832305660704151, 0.8131274438928813, 0.7954932290595025, 0.9356667120009661, 0.9120658851461485, 0.8284178250469267, 0.8880898251663893, 0.8063497379189357, 0.9723757650936022, 0.8555539129301906, 0.8196649590972811, 0.9559076639125124, 0.855940924026072, 0.8114943800028414]
Total Epoch List: [97, 89, 116]
Total Time List: [0.2591159560251981, 0.24894950597081333, 0.2084274550434202]
========================training times:9========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a52883363b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.23s
Epoch 2/1000, LR 0.000000
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.25s
     INFO: Early stopping counter 1 of 2
Epoch 3/1000, LR 0.000030
Train loss: 0.6930;  Loss pred: 0.6930; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.4884 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.6931,   Val_Loss: 0.6935,   Val_Precision: 0.0000,   Val_Recall: 0.0000,   Val_accuracy: 0.0000,   Val_Score: 0.4884,   Val_Loss: 0.6935,   Test_Precision: 0.0000,   Test_Recall: 0.0000,   Test_accuracy: 0.0000,   Test_Score: 0.5000,   Test_loss: 0.6933


[0.23468184401281178, 0.25102296005934477, 0.24650598608423024]
[0.0053336782730184495, 0.005705067274076017, 0.005602408774641596]
[187.48787399845872, 175.28277090509124, 178.49465118045998]
Elapsed: 0.24407026338546225~0.006889972288794523
Time per graph: 0.005547051440578688~0.00015659027929078455
Speed: 180.42176536133664~5.165685638875965
Total Time: 0.2468
best val loss: 0.6934770345687866 test_score: 0.5000

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6933 score: 0.5000 time: 0.25s
test Score 0.5000
Epoch Time List: [0.8162464839406312, 0.8219865409191698, 0.8088915530825034]
Total Epoch List: [3]
Total Time List: [0.2467775100376457]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335de0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 2/1000, LR 0.000000
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.25s
Epoch 3/1000, LR 0.000030
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.24s
Epoch 4/1000, LR 0.000060
Train loss: 0.6939;  Loss pred: 0.6939; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 5/1000, LR 0.000090
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.55s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.26s
Epoch 6/1000, LR 0.000120
Train loss: 0.6937;  Loss pred: 0.6937; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6932 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 7/1000, LR 0.000150
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.24s
Epoch 8/1000, LR 0.000180
Train loss: 0.6935;  Loss pred: 0.6935; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 9/1000, LR 0.000210
Train loss: 0.6933;  Loss pred: 0.6933; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6929 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.37s
Epoch 10/1000, LR 0.000240
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.25s
Epoch 11/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6927 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.24s
Epoch 12/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.25s
Epoch 13/1000, LR 0.000270
Train loss: 0.6924;  Loss pred: 0.6924; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.22s
Epoch 14/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.43s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6923 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.32s
Val loss: 0.6922 score: 0.5682 time: 0.25s
Test loss: 0.6925 score: 0.5814 time: 0.25s
Epoch 16/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.39s
Val loss: 0.6920 score: 0.5455 time: 0.22s
Test loss: 0.6925 score: 0.5581 time: 0.24s
Epoch 17/1000, LR 0.000270
Train loss: 0.6914;  Loss pred: 0.6914; Loss self: 0.0000; time: 0.35s
Val loss: 0.6919 score: 0.6364 time: 0.23s
Test loss: 0.6924 score: 0.6279 time: 0.34s
Epoch 18/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.35s
Val loss: 0.6917 score: 0.6591 time: 0.24s
Test loss: 0.6923 score: 0.5814 time: 0.24s
Epoch 19/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.41s
Val loss: 0.6915 score: 0.6136 time: 0.22s
Test loss: 0.6923 score: 0.5349 time: 0.26s
Epoch 20/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.32s
Val loss: 0.6913 score: 0.6591 time: 0.25s
Test loss: 0.6921 score: 0.4884 time: 0.24s
Epoch 21/1000, LR 0.000270
Train loss: 0.6900;  Loss pred: 0.6900; Loss self: 0.0000; time: 0.40s
Val loss: 0.6911 score: 0.6591 time: 0.32s
Test loss: 0.6920 score: 0.4884 time: 0.24s
Epoch 22/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.45s
Val loss: 0.6908 score: 0.6591 time: 0.23s
Test loss: 0.6919 score: 0.5116 time: 0.24s
Epoch 23/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.35s
Val loss: 0.6905 score: 0.6591 time: 0.24s
Test loss: 0.6917 score: 0.4884 time: 0.24s
Epoch 24/1000, LR 0.000270
Train loss: 0.6886;  Loss pred: 0.6886; Loss self: 0.0000; time: 0.37s
Val loss: 0.6902 score: 0.6591 time: 0.22s
Test loss: 0.6915 score: 0.4884 time: 0.25s
Epoch 25/1000, LR 0.000270
Train loss: 0.6881;  Loss pred: 0.6881; Loss self: 0.0000; time: 0.43s
Val loss: 0.6898 score: 0.6591 time: 0.23s
Test loss: 0.6913 score: 0.5116 time: 0.23s
Epoch 26/1000, LR 0.000270
Train loss: 0.6875;  Loss pred: 0.6875; Loss self: 0.0000; time: 0.45s
Val loss: 0.6894 score: 0.6591 time: 0.24s
Test loss: 0.6910 score: 0.5349 time: 0.24s
Epoch 27/1000, LR 0.000270
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.38s
Val loss: 0.6889 score: 0.6818 time: 0.22s
Test loss: 0.6907 score: 0.5349 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6861;  Loss pred: 0.6861; Loss self: 0.0000; time: 0.32s
Val loss: 0.6884 score: 0.6591 time: 0.24s
Test loss: 0.6904 score: 0.5814 time: 0.25s
Epoch 29/1000, LR 0.000270
Train loss: 0.6849;  Loss pred: 0.6849; Loss self: 0.0000; time: 0.36s
Val loss: 0.6879 score: 0.6591 time: 0.22s
Test loss: 0.6901 score: 0.6047 time: 0.26s
Epoch 30/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.32s
Val loss: 0.6872 score: 0.7273 time: 0.24s
Test loss: 0.6897 score: 0.6279 time: 0.25s
Epoch 31/1000, LR 0.000270
Train loss: 0.6830;  Loss pred: 0.6830; Loss self: 0.0000; time: 0.37s
Val loss: 0.6866 score: 0.7273 time: 0.22s
Test loss: 0.6892 score: 0.6279 time: 0.36s
Epoch 32/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.40s
Val loss: 0.6858 score: 0.7045 time: 0.23s
Test loss: 0.6887 score: 0.6512 time: 0.24s
Epoch 33/1000, LR 0.000270
Train loss: 0.6808;  Loss pred: 0.6808; Loss self: 0.0000; time: 0.36s
Val loss: 0.6850 score: 0.6591 time: 0.24s
Test loss: 0.6882 score: 0.6744 time: 0.24s
Epoch 34/1000, LR 0.000270
Train loss: 0.6791;  Loss pred: 0.6791; Loss self: 0.0000; time: 0.38s
Val loss: 0.6841 score: 0.6136 time: 0.22s
Test loss: 0.6876 score: 0.6279 time: 0.26s
Epoch 35/1000, LR 0.000270
Train loss: 0.6775;  Loss pred: 0.6775; Loss self: 0.0000; time: 0.32s
Val loss: 0.6832 score: 0.6364 time: 0.24s
Test loss: 0.6870 score: 0.6512 time: 0.25s
Epoch 36/1000, LR 0.000270
Train loss: 0.6759;  Loss pred: 0.6759; Loss self: 0.0000; time: 0.36s
Val loss: 0.6821 score: 0.6591 time: 0.22s
Test loss: 0.6863 score: 0.6512 time: 0.26s
Epoch 37/1000, LR 0.000270
Train loss: 0.6740;  Loss pred: 0.6740; Loss self: 0.0000; time: 0.36s
Val loss: 0.6809 score: 0.6364 time: 0.24s
Test loss: 0.6855 score: 0.6047 time: 0.23s
Epoch 38/1000, LR 0.000270
Train loss: 0.6724;  Loss pred: 0.6724; Loss self: 0.0000; time: 0.37s
Val loss: 0.6797 score: 0.6364 time: 0.23s
Test loss: 0.6847 score: 0.6047 time: 0.26s
Epoch 39/1000, LR 0.000269
Train loss: 0.6702;  Loss pred: 0.6702; Loss self: 0.0000; time: 0.37s
Val loss: 0.6784 score: 0.6364 time: 0.26s
Test loss: 0.6838 score: 0.6047 time: 0.25s
Epoch 40/1000, LR 0.000269
Train loss: 0.6677;  Loss pred: 0.6677; Loss self: 0.0000; time: 0.43s
Val loss: 0.6769 score: 0.6364 time: 0.35s
Test loss: 0.6828 score: 0.6047 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6650;  Loss pred: 0.6650; Loss self: 0.0000; time: 0.38s
Val loss: 0.6754 score: 0.6364 time: 0.22s
Test loss: 0.6818 score: 0.6047 time: 0.25s
Epoch 42/1000, LR 0.000269
Train loss: 0.6617;  Loss pred: 0.6617; Loss self: 0.0000; time: 0.32s
Val loss: 0.6737 score: 0.6136 time: 0.24s
Test loss: 0.6807 score: 0.6047 time: 0.22s
Epoch 43/1000, LR 0.000269
Train loss: 0.6604;  Loss pred: 0.6604; Loss self: 0.0000; time: 0.36s
Val loss: 0.6719 score: 0.6136 time: 0.21s
Test loss: 0.6795 score: 0.6047 time: 0.24s
Epoch 44/1000, LR 0.000269
Train loss: 0.6559;  Loss pred: 0.6559; Loss self: 0.0000; time: 0.31s
Val loss: 0.6699 score: 0.6364 time: 0.23s
Test loss: 0.6782 score: 0.6047 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6535;  Loss pred: 0.6535; Loss self: 0.0000; time: 0.37s
Val loss: 0.6677 score: 0.6364 time: 0.21s
Test loss: 0.6769 score: 0.6279 time: 0.25s
Epoch 46/1000, LR 0.000269
Train loss: 0.6497;  Loss pred: 0.6497; Loss self: 0.0000; time: 0.31s
Val loss: 0.6653 score: 0.6591 time: 0.23s
Test loss: 0.6754 score: 0.6512 time: 0.24s
Epoch 47/1000, LR 0.000269
Train loss: 0.6462;  Loss pred: 0.6462; Loss self: 0.0000; time: 0.36s
Val loss: 0.6627 score: 0.6818 time: 0.21s
Test loss: 0.6738 score: 0.6744 time: 0.25s
Epoch 48/1000, LR 0.000269
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.42s
Val loss: 0.6599 score: 0.6818 time: 0.24s
Test loss: 0.6721 score: 0.6744 time: 0.23s
Epoch 49/1000, LR 0.000269
Train loss: 0.6366;  Loss pred: 0.6366; Loss self: 0.0000; time: 0.37s
Val loss: 0.6569 score: 0.7727 time: 0.25s
Test loss: 0.6703 score: 0.7209 time: 0.29s
Epoch 50/1000, LR 0.000269
Train loss: 0.6328;  Loss pred: 0.6328; Loss self: 0.0000; time: 0.39s
Val loss: 0.6536 score: 0.7727 time: 0.22s
Test loss: 0.6683 score: 0.7442 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.6253;  Loss pred: 0.6253; Loss self: 0.0000; time: 0.38s
Val loss: 0.6501 score: 0.8182 time: 0.24s
Test loss: 0.6662 score: 0.7674 time: 0.24s
Epoch 52/1000, LR 0.000269
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.38s
Val loss: 0.6463 score: 0.8409 time: 0.22s
Test loss: 0.6638 score: 0.7209 time: 0.25s
Epoch 53/1000, LR 0.000269
Train loss: 0.6114;  Loss pred: 0.6114; Loss self: 0.0000; time: 0.33s
Val loss: 0.6421 score: 0.8636 time: 0.24s
Test loss: 0.6614 score: 0.6512 time: 0.24s
Epoch 54/1000, LR 0.000269
Train loss: 0.6037;  Loss pred: 0.6037; Loss self: 0.0000; time: 0.37s
Val loss: 0.6376 score: 0.8636 time: 0.21s
Test loss: 0.6587 score: 0.6977 time: 0.25s
Epoch 55/1000, LR 0.000269
Train loss: 0.5948;  Loss pred: 0.5948; Loss self: 0.0000; time: 0.43s
Val loss: 0.6327 score: 0.8409 time: 0.23s
Test loss: 0.6558 score: 0.6977 time: 0.23s
Epoch 56/1000, LR 0.000269
Train loss: 0.5907;  Loss pred: 0.5907; Loss self: 0.0000; time: 0.36s
Val loss: 0.6274 score: 0.8409 time: 0.23s
Test loss: 0.6528 score: 0.6744 time: 0.24s
Epoch 57/1000, LR 0.000269
Train loss: 0.5835;  Loss pred: 0.5835; Loss self: 0.0000; time: 0.35s
Val loss: 0.6219 score: 0.8409 time: 0.24s
Test loss: 0.6496 score: 0.6744 time: 0.22s
Epoch 58/1000, LR 0.000269
Train loss: 0.5730;  Loss pred: 0.5730; Loss self: 0.0000; time: 0.35s
Val loss: 0.6160 score: 0.8409 time: 0.22s
Test loss: 0.6461 score: 0.6744 time: 0.25s
Epoch 59/1000, LR 0.000268
Train loss: 0.5610;  Loss pred: 0.5610; Loss self: 0.0000; time: 0.34s
Val loss: 0.6097 score: 0.8409 time: 0.24s
Test loss: 0.6422 score: 0.6744 time: 0.22s
Epoch 60/1000, LR 0.000268
Train loss: 0.5510;  Loss pred: 0.5510; Loss self: 0.0000; time: 0.35s
Val loss: 0.6030 score: 0.8409 time: 0.21s
Test loss: 0.6381 score: 0.6744 time: 0.25s
Epoch 61/1000, LR 0.000268
Train loss: 0.5449;  Loss pred: 0.5449; Loss self: 0.0000; time: 0.31s
Val loss: 0.5961 score: 0.8636 time: 0.24s
Test loss: 0.6339 score: 0.6744 time: 0.33s
Epoch 62/1000, LR 0.000268
Train loss: 0.5306;  Loss pred: 0.5306; Loss self: 0.0000; time: 0.35s
Val loss: 0.5887 score: 0.8636 time: 0.22s
Test loss: 0.6294 score: 0.7442 time: 0.24s
Epoch 63/1000, LR 0.000268
Train loss: 0.5210;  Loss pred: 0.5210; Loss self: 0.0000; time: 0.35s
Val loss: 0.5811 score: 0.8636 time: 0.23s
Test loss: 0.6247 score: 0.7442 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5061;  Loss pred: 0.5061; Loss self: 0.0000; time: 0.36s
Val loss: 0.5731 score: 0.8636 time: 0.22s
Test loss: 0.6198 score: 0.7442 time: 0.23s
Epoch 65/1000, LR 0.000268
Train loss: 0.5009;  Loss pred: 0.5009; Loss self: 0.0000; time: 0.32s
Val loss: 0.5648 score: 0.8636 time: 0.24s
Test loss: 0.6147 score: 0.7442 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.4813;  Loss pred: 0.4813; Loss self: 0.0000; time: 0.34s
Val loss: 0.5563 score: 0.8636 time: 0.21s
Test loss: 0.6095 score: 0.7674 time: 0.24s
Epoch 67/1000, LR 0.000268
Train loss: 0.4683;  Loss pred: 0.4683; Loss self: 0.0000; time: 0.32s
Val loss: 0.5476 score: 0.8636 time: 0.24s
Test loss: 0.6041 score: 0.7907 time: 0.23s
Epoch 68/1000, LR 0.000268
Train loss: 0.4523;  Loss pred: 0.4523; Loss self: 0.0000; time: 0.36s
Val loss: 0.5388 score: 0.8636 time: 0.21s
Test loss: 0.5986 score: 0.7907 time: 0.25s
Epoch 69/1000, LR 0.000268
Train loss: 0.4382;  Loss pred: 0.4382; Loss self: 0.0000; time: 0.35s
Val loss: 0.5298 score: 0.8636 time: 0.24s
Test loss: 0.5930 score: 0.7907 time: 0.24s
Epoch 70/1000, LR 0.000268
Train loss: 0.4237;  Loss pred: 0.4237; Loss self: 0.0000; time: 0.47s
Val loss: 0.5207 score: 0.8636 time: 0.22s
Test loss: 0.5872 score: 0.7907 time: 0.24s
Epoch 71/1000, LR 0.000268
Train loss: 0.4139;  Loss pred: 0.4139; Loss self: 0.0000; time: 0.41s
Val loss: 0.5116 score: 0.8636 time: 0.22s
Test loss: 0.5815 score: 0.7907 time: 0.24s
Epoch 72/1000, LR 0.000267
Train loss: 0.3944;  Loss pred: 0.3944; Loss self: 0.0000; time: 0.35s
Val loss: 0.5024 score: 0.8636 time: 0.24s
Test loss: 0.5757 score: 0.7907 time: 0.24s
Epoch 73/1000, LR 0.000267
Train loss: 0.3879;  Loss pred: 0.3879; Loss self: 0.0000; time: 0.37s
Val loss: 0.4935 score: 0.8636 time: 0.23s
Test loss: 0.5701 score: 0.7907 time: 0.25s
Epoch 74/1000, LR 0.000267
Train loss: 0.3702;  Loss pred: 0.3702; Loss self: 0.0000; time: 0.33s
Val loss: 0.4847 score: 0.8636 time: 0.25s
Test loss: 0.5648 score: 0.7907 time: 0.24s
Epoch 75/1000, LR 0.000267
Train loss: 0.3510;  Loss pred: 0.3510; Loss self: 0.0000; time: 0.37s
Val loss: 0.4759 score: 0.8636 time: 0.22s
Test loss: 0.5595 score: 0.7907 time: 0.24s
Epoch 76/1000, LR 0.000267
Train loss: 0.3418;  Loss pred: 0.3418; Loss self: 0.0000; time: 0.32s
Val loss: 0.4675 score: 0.8409 time: 0.23s
Test loss: 0.5546 score: 0.7907 time: 0.24s
Epoch 77/1000, LR 0.000267
Train loss: 0.3193;  Loss pred: 0.3193; Loss self: 0.0000; time: 0.38s
Val loss: 0.4593 score: 0.8409 time: 0.22s
Test loss: 0.5500 score: 0.7907 time: 0.25s
Epoch 78/1000, LR 0.000267
Train loss: 0.3188;  Loss pred: 0.3188; Loss self: 0.0000; time: 0.41s
Val loss: 0.4520 score: 0.8409 time: 0.23s
Test loss: 0.5463 score: 0.7907 time: 0.23s
Epoch 79/1000, LR 0.000267
Train loss: 0.3029;  Loss pred: 0.3029; Loss self: 0.0000; time: 0.39s
Val loss: 0.4453 score: 0.8409 time: 0.22s
Test loss: 0.5432 score: 0.7907 time: 0.25s
Epoch 80/1000, LR 0.000267
Train loss: 0.2848;  Loss pred: 0.2848; Loss self: 0.0000; time: 0.46s
Val loss: 0.4388 score: 0.8409 time: 0.22s
Test loss: 0.5406 score: 0.7907 time: 0.40s
Epoch 81/1000, LR 0.000267
Train loss: 0.2846;  Loss pred: 0.2846; Loss self: 0.0000; time: 0.35s
Val loss: 0.4339 score: 0.8409 time: 0.23s
Test loss: 0.5401 score: 0.7907 time: 0.23s
Epoch 82/1000, LR 0.000267
Train loss: 0.2554;  Loss pred: 0.2554; Loss self: 0.0000; time: 0.45s
Val loss: 0.4287 score: 0.8409 time: 0.21s
Test loss: 0.5392 score: 0.7674 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2671;  Loss pred: 0.2671; Loss self: 0.0000; time: 0.34s
Val loss: 0.4240 score: 0.8182 time: 0.23s
Test loss: 0.5390 score: 0.7674 time: 0.24s
Epoch 84/1000, LR 0.000266
Train loss: 0.2481;  Loss pred: 0.2481; Loss self: 0.0000; time: 0.36s
Val loss: 0.4195 score: 0.8182 time: 0.21s
Test loss: 0.5390 score: 0.7674 time: 0.24s
Epoch 85/1000, LR 0.000266
Train loss: 0.2286;  Loss pred: 0.2286; Loss self: 0.0000; time: 0.33s
Val loss: 0.4156 score: 0.8182 time: 0.33s
Test loss: 0.5396 score: 0.7674 time: 0.22s
Epoch 86/1000, LR 0.000266
Train loss: 0.2347;  Loss pred: 0.2347; Loss self: 0.0000; time: 0.35s
Val loss: 0.4126 score: 0.8182 time: 0.23s
Test loss: 0.5414 score: 0.7442 time: 0.25s
Epoch 87/1000, LR 0.000266
Train loss: 0.2082;  Loss pred: 0.2082; Loss self: 0.0000; time: 0.30s
Val loss: 0.4097 score: 0.8182 time: 0.23s
Test loss: 0.5432 score: 0.7442 time: 0.24s
Epoch 88/1000, LR 0.000266
Train loss: 0.2010;  Loss pred: 0.2010; Loss self: 0.0000; time: 0.36s
Val loss: 0.4064 score: 0.8182 time: 0.21s
Test loss: 0.5441 score: 0.7674 time: 0.25s
Epoch 89/1000, LR 0.000266
Train loss: 0.1842;  Loss pred: 0.1842; Loss self: 0.0000; time: 0.32s
Val loss: 0.4026 score: 0.8182 time: 0.23s
Test loss: 0.5443 score: 0.7674 time: 0.22s
Epoch 90/1000, LR 0.000266
Train loss: 0.1812;  Loss pred: 0.1812; Loss self: 0.0000; time: 0.43s
Val loss: 0.4006 score: 0.8182 time: 0.21s
Test loss: 0.5466 score: 0.7907 time: 0.25s
Epoch 91/1000, LR 0.000266
Train loss: 0.1652;  Loss pred: 0.1652; Loss self: 0.0000; time: 0.32s
Val loss: 0.3992 score: 0.8182 time: 0.24s
Test loss: 0.5497 score: 0.7907 time: 0.23s
Epoch 92/1000, LR 0.000266
Train loss: 0.1722;  Loss pred: 0.1722; Loss self: 0.0000; time: 0.36s
Val loss: 0.3976 score: 0.8182 time: 0.32s
Test loss: 0.5524 score: 0.7907 time: 0.25s
Epoch 93/1000, LR 0.000265
Train loss: 0.1741;  Loss pred: 0.1741; Loss self: 0.0000; time: 0.42s
Val loss: 0.3975 score: 0.8182 time: 0.23s
Test loss: 0.5568 score: 0.7907 time: 0.22s
Epoch 94/1000, LR 0.000265
Train loss: 0.1773;  Loss pred: 0.1773; Loss self: 0.0000; time: 0.36s
Val loss: 0.3992 score: 0.8182 time: 0.27s
Test loss: 0.5631 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 1 of 2
Epoch 95/1000, LR 0.000265
Train loss: 0.1391;  Loss pred: 0.1391; Loss self: 0.0000; time: 0.41s
Val loss: 0.4012 score: 0.8182 time: 0.24s
Test loss: 0.5698 score: 0.7907 time: 0.24s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 092,   Train_Loss: 0.1741,   Val_Loss: 0.3975,   Val_Precision: 0.9375,   Val_Recall: 0.6818,   Val_accuracy: 0.7895,   Val_Score: 0.8182,   Val_Loss: 0.3975,   Test_Precision: 0.8824,   Test_Recall: 0.6818,   Test_accuracy: 0.7692,   Test_Score: 0.7907,   Test_loss: 0.5568


[0.23468184401281178, 0.25102296005934477, 0.24650598608423024, 0.2508760100463405, 0.25733099796343595, 0.2404125159373507, 0.24052650504745543, 0.26913144101854414, 0.24596615100745112, 0.24488305510021746, 0.24438242195174098, 0.3725928779458627, 0.2537640519440174, 0.24343412299640477, 0.2524806510191411, 0.22814964200370014, 0.25440057704690844, 0.25646022893488407, 0.2483890700386837, 0.3457996170036495, 0.24092389491852373, 0.2624215000541881, 0.2450473200296983, 0.2415760630974546, 0.24650537699926645, 0.24586349702440202, 0.2558956299908459, 0.23553229705430567, 0.24693414405919611, 0.26384557597339153, 0.2507684590527788, 0.2620564189273864, 0.25298753299284726, 0.3606757080415264, 0.2489127640146762, 0.24424883199390024, 0.26140037702862173, 0.2503615020541474, 0.26065148995257914, 0.23473681102041155, 0.26141359901521355, 0.25322777801193297, 0.22919227601960301, 0.24992068705614656, 0.22677756601478904, 0.2489689220674336, 0.24267618905287236, 0.2572813150472939, 0.24270898604299873, 0.25256513501517475, 0.2332909689284861, 0.29722194594796747, 0.238997268024832, 0.24179001001175493, 0.25033164396882057, 0.24423006700817496, 0.2582209960091859, 0.235044569009915, 0.24134671094361693, 0.22892710601445287, 0.2543337279930711, 0.22688213305082172, 0.25318970205262303, 0.33264023694209754, 0.24236291204579175, 0.23023807303979993, 0.2385168339824304, 0.22832115704659373, 0.24732943391427398, 0.23757801298052073, 0.2533960920991376, 0.24834971106611192, 0.24560222506988794, 0.2425267540384084, 0.24034681194461882, 0.2504175390349701, 0.24544032802805305, 0.24889367795549333, 0.24784845300018787, 0.2527881939895451, 0.2317594800842926, 0.25178492604754865, 0.4000416590133682, 0.234819530043751, 0.24645066901575774, 0.24402819492388517, 0.24411627696827054, 0.2266990179196, 0.2555487169884145, 0.24729900201782584, 0.25072399503551424, 0.22028259502258152, 0.24993317504413426, 0.23884775396436453, 0.2524173560086638, 0.2271501439390704, 0.24912126001436263, 0.24378823104780167]
[0.0053336782730184495, 0.005705067274076017, 0.005602408774641596, 0.005834325815031175, 0.005984441813103161, 0.0055909887427290856, 0.005593639652266405, 0.006258870721361492, 0.00572014304668491, 0.005694954769772499, 0.005683312138412581, 0.008664950649903785, 0.005901489580093428, 0.005661258674334995, 0.005871643046956769, 0.005305805627993027, 0.005916292489462987, 0.0059641913705786994, 0.005776490000899621, 0.008041851558224407, 0.00560288127717497, 0.006102825582655537, 0.0056987748844115884, 0.005618047979010572, 0.005732683186029452, 0.0057177557447535355, 0.005951061162577812, 0.00547749528033269, 0.0057426545130045605, 0.006135943627288175, 0.005831824629134391, 0.006094335323892707, 0.005883430999833657, 0.008387807163756427, 0.005788668930573866, 0.005680205395206982, 0.006079078535549343, 0.005822360512887149, 0.006061662557036724, 0.00545899560512585, 0.006079386023609618, 0.005889018093300767, 0.005330052930688442, 0.005812109001305734, 0.005273896884064861, 0.005789974931800781, 0.0056436323035551714, 0.005983286396448695, 0.005644395024255785, 0.005873607791050575, 0.00542537137042991, 0.006912138277859708, 0.005558076000577489, 0.005623023488645463, 0.005821666138809781, 0.005679769000190116, 0.006005139442074091, 0.005466152767672441, 0.005612714207991091, 0.005323886186382625, 0.005914737860303979, 0.0052763286756005055, 0.005888132605874954, 0.007735819463769711, 0.005636346791762599, 0.005354373791623254, 0.005546903115870474, 0.005309794349920785, 0.005751847300331953, 0.005525070069314435, 0.005892932374398549, 0.005775574675956091, 0.005711679652788092, 0.00564015707066066, 0.005589460742898112, 0.005823663698487677, 0.00570791460530356, 0.005788225068732403, 0.005763917511632276, 0.005878795209059188, 0.005389755350797502, 0.00585546339645462, 0.009303294395659726, 0.005460919303343047, 0.005731410907343203, 0.005675074300555469, 0.005677122720192338, 0.005272070184176744, 0.005942993418335221, 0.005751139581809903, 0.005830790582221261, 0.005122851047036779, 0.005812399419631029, 0.0055545989294038265, 0.005870171069968925, 0.005282561486955125, 0.005793517674752619, 0.005669493745297714]
[187.48787399845872, 175.28277090509124, 178.49465118045998, 171.39940958108056, 167.09996207339876, 178.85924046984894, 178.7744763992483, 159.77323138933122, 174.82080287826835, 175.59401969402256, 175.9537353651866, 115.40746628615871, 169.4487444954819, 176.6391641020477, 170.31008050094135, 188.4727919025296, 169.02477383953146, 167.66732283826283, 173.11550783334891, 124.34947260090861, 178.47959835840214, 163.85852527754327, 175.4763120640889, 177.99776786102063, 174.4383855778041, 174.89379481059052, 168.0372580084241, 182.56519610168652, 174.13549739679522, 162.97411787695273, 171.4729203282693, 164.08680304799148, 169.96884981370107, 119.22067120486307, 172.75128565710943, 176.04997186260388, 164.49861507006077, 171.75164570909186, 164.9712418978425, 183.18388076023123, 164.49029492722573, 169.80759511294093, 187.61539763374125, 172.05458462243953, 189.61311189483268, 172.7123194450486, 177.19084912212588, 167.13223030633088, 177.1669055235641, 170.25311113276365, 184.31917959576646, 144.6730316728621, 179.91837461310337, 177.8402672546707, 171.77213123465827, 176.0634983511702, 166.5240265685844, 182.94402708869274, 178.16691941596667, 187.83271561247656, 169.0692002956504, 189.52572166785816, 169.8331316455472, 129.26878718970184, 177.41988506837066, 186.76320311526771, 180.2807763378557, 188.33121098464366, 173.85718844488233, 180.99317971619544, 169.694803277301, 173.14294353478505, 175.07984704847047, 177.3000268382357, 178.90813550673658, 171.71321212447174, 175.1953329979466, 172.76453284478032, 173.49311435874648, 170.10288068191352, 185.5371783901163, 170.780676488471, 107.48880530605706, 183.11935123960237, 174.47710802217637, 176.20914670705216, 176.1455669160029, 189.6788102330914, 168.265372281049, 173.8785828052006, 171.50332976271054, 195.20380171475645, 172.04598786218307, 180.0310000252943, 170.3527866702007, 189.30210324468956, 172.60670565619697, 176.38259162546947]
Elapsed: 0.2521273842145575~0.028207494263591884
Time per graph: 0.005859478523660393~0.0006574640496238118
Speed: 172.27167782425843~14.537842791937619
Total Time: 0.2446
best val loss: 0.39746809005737305 test_score: 0.7907

Testing...
Test loss: 0.6614 score: 0.6512 time: 0.25s
test Score 0.6512
Epoch Time List: [0.8162464839406312, 0.8219865409191698, 0.8088915530825034, 0.9324288050411269, 0.8398615999612957, 0.8303632360184565, 0.8312249730806798, 1.049406121019274, 0.8100718049099669, 0.8434168830281124, 0.7811744039645419, 0.9555258869659156, 0.9838238109368831, 0.8119234750047326, 0.8291357088601217, 0.7661242100875825, 0.8943514609709382, 0.8179002840770409, 0.8501789979636669, 0.9158482679631561, 0.8253506600158289, 0.8914776530582458, 0.8099420509533957, 0.9668285449733958, 0.9284389670938253, 0.831494536017999, 0.8381367269903421, 0.8887598408618942, 0.9253373668761924, 0.8612187780672684, 0.8091769120655954, 0.8324428829364479, 0.7996253238525242, 0.9411252980353311, 0.8710428000194952, 0.8383907821262255, 0.8560775249497965, 0.8117943060351536, 0.8309120449703187, 0.8305285018868744, 0.8582504939986393, 0.8819917980581522, 1.008828318095766, 0.8409999300492927, 0.7850240260595456, 0.8145561559358612, 0.7777390778064728, 0.8351059580454603, 0.7845446121646091, 0.823646969976835, 0.8900417539989576, 0.9125833939760923, 0.8368142211111262, 0.8584061119472608, 0.8453135630115867, 0.8163206939352676, 0.8354629811365157, 0.8851404250599444, 0.8260606919648126, 0.8171316758962348, 0.8245038730092347, 0.7962292318698019, 0.8114878538763151, 0.8762747930595651, 0.8005517810815945, 0.8027848950587213, 0.8133258799789473, 0.7828062641201541, 0.801995322923176, 0.7964417460607365, 0.8197084579151124, 0.8329761192435399, 0.9333100350340828, 0.8690042940434068, 0.8222712279530242, 0.8454865139210597, 0.824582515982911, 0.8366669419920072, 0.8017024140572175, 0.8387865618569776, 0.8686352248769253, 0.8635732369730249, 1.075357815832831, 0.8049577199853957, 0.9027087721042335, 0.8105645158793777, 0.8119195150211453, 0.8888602040242404, 0.8278359980322421, 0.7734517618082464, 0.8178681778954342, 0.7590211320202798, 0.886723374016583, 0.7929608990671113, 0.9300410180585459, 0.8661429119529203, 0.8704048690851778, 0.8911651249509305]
Total Epoch List: [3, 95]
Total Time List: [0.2467775100376457, 0.24458533606957644]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: FAGNN
FAGNN(
  (conv1): NeighborFilterConv()
  (conv2): NeighborFilterConv()
  (ffn): Sequential(
    (0): Linear(in_features=14951, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)
Total number of parameters: 1922370
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x7a5288335ba0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.22s
Epoch 2/1000, LR 0.000000
Train loss: 0.6945;  Loss pred: 0.6945; Loss self: 0.0000; time: 0.50s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6937 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6930 score: 0.5116 time: 0.23s
Epoch 3/1000, LR 0.000030
Train loss: 0.6944;  Loss pred: 0.6944; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6936 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.22s
Epoch 4/1000, LR 0.000060
Train loss: 0.6943;  Loss pred: 0.6943; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6935 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6929 score: 0.5116 time: 0.32s
Epoch 5/1000, LR 0.000090
Train loss: 0.6942;  Loss pred: 0.6942; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6934 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6928 score: 0.5116 time: 0.22s
Epoch 6/1000, LR 0.000120
Train loss: 0.6940;  Loss pred: 0.6940; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6933 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6927 score: 0.5116 time: 0.21s
Epoch 7/1000, LR 0.000150
Train loss: 0.6938;  Loss pred: 0.6938; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6931 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6926 score: 0.5116 time: 0.22s
Epoch 8/1000, LR 0.000180
Train loss: 0.6936;  Loss pred: 0.6936; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6930 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.22s
Epoch 9/1000, LR 0.000210
Train loss: 0.6934;  Loss pred: 0.6934; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6928 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6925 score: 0.5116 time: 0.23s
Epoch 10/1000, LR 0.000240
Train loss: 0.6931;  Loss pred: 0.6931; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6926 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.22s
Epoch 11/1000, LR 0.000270
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6925 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.5116 time: 0.29s
Epoch 12/1000, LR 0.000270
Train loss: 0.6927;  Loss pred: 0.6927; Loss self: 0.0000; time: 0.38s
Val loss: 0.6924 score: 0.6364 time: 0.25s
Test loss: 0.6924 score: 0.6512 time: 0.24s
Epoch 13/1000, LR 0.000270
Train loss: 0.6925;  Loss pred: 0.6925; Loss self: 0.0000; time: 0.60s
Val loss: 0.6923 score: 0.5000 time: 0.24s
Test loss: 0.6924 score: 0.5349 time: 0.22s
Epoch 14/1000, LR 0.000270
Train loss: 0.6923;  Loss pred: 0.6923; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6922 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.32s
Epoch 15/1000, LR 0.000270
Train loss: 0.6922;  Loss pred: 0.6922; Loss self: 0.0000; time: 0.42s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6921 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6924 score: 0.4884 time: 0.24s
Epoch 16/1000, LR 0.000270
Train loss: 0.6920;  Loss pred: 0.6920; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6920 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.21s
Epoch 17/1000, LR 0.000270
Train loss: 0.6917;  Loss pred: 0.6917; Loss self: 0.0000; time: 0.41s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6918 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6923 score: 0.4884 time: 0.22s
Epoch 18/1000, LR 0.000270
Train loss: 0.6915;  Loss pred: 0.6915; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6917 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6922 score: 0.4884 time: 0.20s
Epoch 19/1000, LR 0.000270
Train loss: 0.6913;  Loss pred: 0.6913; Loss self: 0.0000; time: 0.47s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6915 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6921 score: 0.4884 time: 0.22s
Epoch 20/1000, LR 0.000270
Train loss: 0.6911;  Loss pred: 0.6911; Loss self: 0.0000; time: 0.52s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6913 score: 0.5000 time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6920 score: 0.4884 time: 0.30s
Epoch 21/1000, LR 0.000270
Train loss: 0.6908;  Loss pred: 0.6908; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6911 score: 0.5000 time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6919 score: 0.4884 time: 0.26s
Epoch 22/1000, LR 0.000270
Train loss: 0.6905;  Loss pred: 0.6905; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6909 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6918 score: 0.4884 time: 0.22s
Epoch 23/1000, LR 0.000270
Train loss: 0.6902;  Loss pred: 0.6902; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6906 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6916 score: 0.4884 time: 0.23s
Epoch 24/1000, LR 0.000270
Train loss: 0.6898;  Loss pred: 0.6898; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6903 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6913 score: 0.4884 time: 0.22s
Epoch 25/1000, LR 0.000270
Train loss: 0.6896;  Loss pred: 0.6896; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6899 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6911 score: 0.4884 time: 0.22s
Epoch 26/1000, LR 0.000270
Train loss: 0.6891;  Loss pred: 0.6891; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6895 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6907 score: 0.4884 time: 0.19s
Epoch 27/1000, LR 0.000270
Train loss: 0.6885;  Loss pred: 0.6885; Loss self: 0.0000; time: 0.53s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6890 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6903 score: 0.4884 time: 0.26s
Epoch 28/1000, LR 0.000270
Train loss: 0.6879;  Loss pred: 0.6879; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6885 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6899 score: 0.4884 time: 0.21s
Epoch 29/1000, LR 0.000270
Train loss: 0.6872;  Loss pred: 0.6872; Loss self: 0.0000; time: 0.39s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6880 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6894 score: 0.4884 time: 0.22s
Epoch 30/1000, LR 0.000270
Train loss: 0.6866;  Loss pred: 0.6866; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6874 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6889 score: 0.4884 time: 0.22s
Epoch 31/1000, LR 0.000270
Train loss: 0.6857;  Loss pred: 0.6857; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6883 score: 0.4884 time: 0.23s
Epoch 32/1000, LR 0.000270
Train loss: 0.6846;  Loss pred: 0.6846; Loss self: 0.0000; time: 0.46s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6860 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6877 score: 0.4884 time: 0.22s
Epoch 33/1000, LR 0.000270
Train loss: 0.6843;  Loss pred: 0.6843; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6852 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6869 score: 0.4884 time: 0.20s
Epoch 34/1000, LR 0.000270
Train loss: 0.6832;  Loss pred: 0.6832; Loss self: 0.0000; time: 0.44s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6844 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6862 score: 0.4884 time: 0.22s
Epoch 35/1000, LR 0.000270
Train loss: 0.6820;  Loss pred: 0.6820; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6835 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6853 score: 0.4884 time: 0.30s
Epoch 36/1000, LR 0.000270
Train loss: 0.6805;  Loss pred: 0.6805; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6825 score: 0.5000 time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6844 score: 0.4884 time: 0.23s
Epoch 37/1000, LR 0.000270
Train loss: 0.6799;  Loss pred: 0.6799; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6814 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6833 score: 0.4884 time: 0.20s
Epoch 38/1000, LR 0.000270
Train loss: 0.6770;  Loss pred: 0.6770; Loss self: 0.0000; time: 0.38s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6802 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6821 score: 0.4884 time: 0.23s
Epoch 39/1000, LR 0.000269
Train loss: 0.6767;  Loss pred: 0.6767; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6788 score: 0.5000 time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6808 score: 0.4884 time: 0.21s
Epoch 40/1000, LR 0.000269
Train loss: 0.6743;  Loss pred: 0.6743; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6773 score: 0.5000 time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6793 score: 0.4884 time: 0.22s
Epoch 41/1000, LR 0.000269
Train loss: 0.6722;  Loss pred: 0.6722; Loss self: 0.0000; time: 0.35s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6756 score: 0.5000 time: 0.24s
Test loss: 0.6777 score: 0.5116 time: 0.22s
Epoch 42/1000, LR 0.000269
Train loss: 0.6700;  Loss pred: 0.6700; Loss self: 0.0000; time: 0.40s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6737 score: 0.5000 time: 0.23s
Test loss: 0.6758 score: 0.5116 time: 0.24s
Epoch 43/1000, LR 0.000269
Train loss: 0.6668;  Loss pred: 0.6668; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6717 score: 0.5000 time: 0.24s
Test loss: 0.6737 score: 0.5116 time: 0.22s
Epoch 44/1000, LR 0.000269
Train loss: 0.6652;  Loss pred: 0.6652; Loss self: 0.0000; time: 0.49s
Val loss: 0.6695 score: 0.5227 time: 0.23s
Test loss: 0.6715 score: 0.5349 time: 0.24s
Epoch 45/1000, LR 0.000269
Train loss: 0.6625;  Loss pred: 0.6625; Loss self: 0.0000; time: 0.39s
Val loss: 0.6671 score: 0.5455 time: 0.22s
Test loss: 0.6691 score: 0.5349 time: 0.21s
Epoch 46/1000, LR 0.000269
Train loss: 0.6562;  Loss pred: 0.6562; Loss self: 0.0000; time: 0.39s
Val loss: 0.6645 score: 0.5455 time: 0.23s
Test loss: 0.6665 score: 0.5349 time: 0.23s
Epoch 47/1000, LR 0.000269
Train loss: 0.6534;  Loss pred: 0.6534; Loss self: 0.0000; time: 0.38s
Val loss: 0.6615 score: 0.5682 time: 0.23s
Test loss: 0.6634 score: 0.5349 time: 0.20s
Epoch 48/1000, LR 0.000269
Train loss: 0.6505;  Loss pred: 0.6505; Loss self: 0.0000; time: 0.38s
Val loss: 0.6583 score: 0.5682 time: 0.23s
Test loss: 0.6600 score: 0.5581 time: 0.22s
Epoch 49/1000, LR 0.000269
Train loss: 0.6474;  Loss pred: 0.6474; Loss self: 0.0000; time: 0.33s
Val loss: 0.6547 score: 0.5682 time: 0.23s
Test loss: 0.6562 score: 0.5581 time: 0.20s
Epoch 50/1000, LR 0.000269
Train loss: 0.6398;  Loss pred: 0.6398; Loss self: 0.0000; time: 0.38s
Val loss: 0.6508 score: 0.5909 time: 0.22s
Test loss: 0.6522 score: 0.5581 time: 0.23s
Epoch 51/1000, LR 0.000269
Train loss: 0.6337;  Loss pred: 0.6337; Loss self: 0.0000; time: 0.35s
Val loss: 0.6467 score: 0.6364 time: 0.24s
Test loss: 0.6478 score: 0.5814 time: 0.21s
Epoch 52/1000, LR 0.000269
Train loss: 0.6317;  Loss pred: 0.6317; Loss self: 0.0000; time: 0.49s
Val loss: 0.6423 score: 0.6364 time: 0.21s
Test loss: 0.6432 score: 0.5814 time: 0.23s
Epoch 53/1000, LR 0.000269
Train loss: 0.6208;  Loss pred: 0.6208; Loss self: 0.0000; time: 0.36s
Val loss: 0.6375 score: 0.6818 time: 0.24s
Test loss: 0.6382 score: 0.6512 time: 0.20s
Epoch 54/1000, LR 0.000269
Train loss: 0.6154;  Loss pred: 0.6154; Loss self: 0.0000; time: 0.38s
Val loss: 0.6324 score: 0.6818 time: 0.21s
Test loss: 0.6329 score: 0.6744 time: 0.23s
Epoch 55/1000, LR 0.000269
Train loss: 0.6046;  Loss pred: 0.6046; Loss self: 0.0000; time: 0.35s
Val loss: 0.6268 score: 0.6818 time: 0.24s
Test loss: 0.6273 score: 0.7209 time: 0.21s
Epoch 56/1000, LR 0.000269
Train loss: 0.5984;  Loss pred: 0.5984; Loss self: 0.0000; time: 0.38s
Val loss: 0.6208 score: 0.7273 time: 0.22s
Test loss: 0.6213 score: 0.7209 time: 0.23s
Epoch 57/1000, LR 0.000269
Train loss: 0.5913;  Loss pred: 0.5913; Loss self: 0.0000; time: 0.32s
Val loss: 0.6144 score: 0.7500 time: 0.24s
Test loss: 0.6149 score: 0.7209 time: 0.21s
Epoch 58/1000, LR 0.000269
Train loss: 0.5823;  Loss pred: 0.5823; Loss self: 0.0000; time: 0.38s
Val loss: 0.6077 score: 0.7727 time: 0.21s
Test loss: 0.6082 score: 0.7442 time: 0.22s
Epoch 59/1000, LR 0.000268
Train loss: 0.5701;  Loss pred: 0.5701; Loss self: 0.0000; time: 0.45s
Val loss: 0.6004 score: 0.7955 time: 0.24s
Test loss: 0.6009 score: 0.7674 time: 0.21s
Epoch 60/1000, LR 0.000268
Train loss: 0.5604;  Loss pred: 0.5604; Loss self: 0.0000; time: 0.42s
Val loss: 0.5926 score: 0.7955 time: 0.25s
Test loss: 0.5930 score: 0.7907 time: 0.23s
Epoch 61/1000, LR 0.000268
Train loss: 0.5429;  Loss pred: 0.5429; Loss self: 0.0000; time: 0.38s
Val loss: 0.5845 score: 0.8182 time: 0.22s
Test loss: 0.5846 score: 0.7907 time: 0.21s
Epoch 62/1000, LR 0.000268
Train loss: 0.5414;  Loss pred: 0.5414; Loss self: 0.0000; time: 0.46s
Val loss: 0.5759 score: 0.8182 time: 0.24s
Test loss: 0.5758 score: 0.7907 time: 0.21s
Epoch 63/1000, LR 0.000268
Train loss: 0.5246;  Loss pred: 0.5246; Loss self: 0.0000; time: 0.43s
Val loss: 0.5669 score: 0.8182 time: 0.23s
Test loss: 0.5667 score: 0.7907 time: 0.23s
Epoch 64/1000, LR 0.000268
Train loss: 0.5039;  Loss pred: 0.5039; Loss self: 0.0000; time: 0.37s
Val loss: 0.5576 score: 0.8182 time: 0.24s
Test loss: 0.5577 score: 0.7907 time: 0.21s
Epoch 65/1000, LR 0.000268
Train loss: 0.5065;  Loss pred: 0.5065; Loss self: 0.0000; time: 0.40s
Val loss: 0.5480 score: 0.8409 time: 0.21s
Test loss: 0.5484 score: 0.7907 time: 0.22s
Epoch 66/1000, LR 0.000268
Train loss: 0.4750;  Loss pred: 0.4750; Loss self: 0.0000; time: 0.46s
Val loss: 0.5382 score: 0.8636 time: 0.23s
Test loss: 0.5394 score: 0.7907 time: 0.20s
Epoch 67/1000, LR 0.000268
Train loss: 0.4723;  Loss pred: 0.4723; Loss self: 0.0000; time: 0.41s
Val loss: 0.5280 score: 0.8636 time: 0.22s
Test loss: 0.5302 score: 0.7907 time: 0.22s
Epoch 68/1000, LR 0.000268
Train loss: 0.4463;  Loss pred: 0.4463; Loss self: 0.0000; time: 0.37s
Val loss: 0.5178 score: 0.8636 time: 0.26s
Test loss: 0.5216 score: 0.7907 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.4347;  Loss pred: 0.4347; Loss self: 0.0000; time: 0.38s
Val loss: 0.5076 score: 0.8182 time: 0.22s
Test loss: 0.5134 score: 0.7907 time: 0.22s
Epoch 70/1000, LR 0.000268
Train loss: 0.4347;  Loss pred: 0.4347; Loss self: 0.0000; time: 0.38s
Val loss: 0.4974 score: 0.8182 time: 0.23s
Test loss: 0.5053 score: 0.7907 time: 0.22s
Epoch 71/1000, LR 0.000268
Train loss: 0.4119;  Loss pred: 0.4119; Loss self: 0.0000; time: 0.39s
Val loss: 0.4870 score: 0.8409 time: 0.22s
Test loss: 0.4968 score: 0.7907 time: 0.22s
Epoch 72/1000, LR 0.000267
Train loss: 0.3964;  Loss pred: 0.3964; Loss self: 0.0000; time: 0.41s
Val loss: 0.4764 score: 0.8636 time: 0.23s
Test loss: 0.4877 score: 0.7907 time: 0.29s
Epoch 73/1000, LR 0.000267
Train loss: 0.3934;  Loss pred: 0.3934; Loss self: 0.0000; time: 0.39s
Val loss: 0.4658 score: 0.8636 time: 0.23s
Test loss: 0.4786 score: 0.7907 time: 0.21s
Epoch 74/1000, LR 0.000267
Train loss: 0.3688;  Loss pred: 0.3688; Loss self: 0.0000; time: 0.47s
Val loss: 0.4554 score: 0.8864 time: 0.32s
Test loss: 0.4695 score: 0.7907 time: 0.22s
Epoch 75/1000, LR 0.000267
Train loss: 0.3539;  Loss pred: 0.3539; Loss self: 0.0000; time: 0.33s
Val loss: 0.4449 score: 0.9091 time: 0.24s
Test loss: 0.4600 score: 0.7674 time: 0.21s
Epoch 76/1000, LR 0.000267
Train loss: 0.3518;  Loss pred: 0.3518; Loss self: 0.0000; time: 0.38s
Val loss: 0.4346 score: 0.9091 time: 0.22s
Test loss: 0.4496 score: 0.7907 time: 0.23s
Epoch 77/1000, LR 0.000267
Train loss: 0.3307;  Loss pred: 0.3307; Loss self: 0.0000; time: 0.34s
Val loss: 0.4249 score: 0.9091 time: 0.24s
Test loss: 0.4417 score: 0.7907 time: 0.22s
Epoch 78/1000, LR 0.000267
Train loss: 0.3221;  Loss pred: 0.3221; Loss self: 0.0000; time: 0.38s
Val loss: 0.4154 score: 0.9091 time: 0.22s
Test loss: 0.4346 score: 0.7907 time: 0.22s
Epoch 79/1000, LR 0.000267
Train loss: 0.3010;  Loss pred: 0.3010; Loss self: 0.0000; time: 0.36s
Val loss: 0.4062 score: 0.9091 time: 0.23s
Test loss: 0.4280 score: 0.7907 time: 0.21s
Epoch 80/1000, LR 0.000267
Train loss: 0.2774;  Loss pred: 0.2774; Loss self: 0.0000; time: 0.39s
Val loss: 0.3972 score: 0.9091 time: 0.21s
Test loss: 0.4220 score: 0.7907 time: 0.22s
Epoch 81/1000, LR 0.000267
Train loss: 0.2737;  Loss pred: 0.2737; Loss self: 0.0000; time: 0.37s
Val loss: 0.3884 score: 0.9091 time: 0.23s
Test loss: 0.4171 score: 0.7907 time: 0.22s
Epoch 82/1000, LR 0.000267
Train loss: 0.2634;  Loss pred: 0.2634; Loss self: 0.0000; time: 0.41s
Val loss: 0.3798 score: 0.9091 time: 0.32s
Test loss: 0.4125 score: 0.7907 time: 0.24s
Epoch 83/1000, LR 0.000266
Train loss: 0.2408;  Loss pred: 0.2408; Loss self: 0.0000; time: 0.43s
Val loss: 0.3713 score: 0.9091 time: 0.22s
Test loss: 0.4105 score: 0.7907 time: 0.20s
Epoch 84/1000, LR 0.000266
Train loss: 0.2531;  Loss pred: 0.2531; Loss self: 0.0000; time: 0.40s
Val loss: 0.3631 score: 0.9091 time: 0.23s
Test loss: 0.4062 score: 0.7907 time: 0.23s
Epoch 85/1000, LR 0.000266
Train loss: 0.2260;  Loss pred: 0.2260; Loss self: 0.0000; time: 0.35s
Val loss: 0.3556 score: 0.9091 time: 0.24s
Test loss: 0.3993 score: 0.8140 time: 0.20s
Epoch 86/1000, LR 0.000266
Train loss: 0.2038;  Loss pred: 0.2038; Loss self: 0.0000; time: 0.38s
Val loss: 0.3485 score: 0.9091 time: 0.22s
Test loss: 0.3930 score: 0.8140 time: 0.24s
Epoch 87/1000, LR 0.000266
Train loss: 0.2005;  Loss pred: 0.2005; Loss self: 0.0000; time: 0.34s
Val loss: 0.3415 score: 0.9091 time: 0.25s
Test loss: 0.3885 score: 0.8140 time: 0.21s
Epoch 88/1000, LR 0.000266
Train loss: 0.1876;  Loss pred: 0.1876; Loss self: 0.0000; time: 0.39s
Val loss: 0.3346 score: 0.9091 time: 0.21s
Test loss: 0.3857 score: 0.8140 time: 0.23s
Epoch 89/1000, LR 0.000266
Train loss: 0.1706;  Loss pred: 0.1706; Loss self: 0.0000; time: 0.34s
Val loss: 0.3281 score: 0.9091 time: 0.24s
Test loss: 0.3829 score: 0.8372 time: 0.30s
Epoch 90/1000, LR 0.000266
Train loss: 0.1612;  Loss pred: 0.1612; Loss self: 0.0000; time: 0.38s
Val loss: 0.3217 score: 0.9091 time: 0.27s
Test loss: 0.3816 score: 0.8372 time: 0.24s
Epoch 91/1000, LR 0.000266
Train loss: 0.1556;  Loss pred: 0.1556; Loss self: 0.0000; time: 0.40s
Val loss: 0.3155 score: 0.9091 time: 0.22s
Test loss: 0.3810 score: 0.8372 time: 0.21s
Epoch 92/1000, LR 0.000266
Train loss: 0.1585;  Loss pred: 0.1585; Loss self: 0.0000; time: 0.38s
Val loss: 0.3108 score: 0.9091 time: 0.25s
Test loss: 0.3782 score: 0.8372 time: 0.20s
Epoch 93/1000, LR 0.000265
Train loss: 0.1461;  Loss pred: 0.1461; Loss self: 0.0000; time: 0.47s
Val loss: 0.3067 score: 0.9318 time: 0.22s
Test loss: 0.3759 score: 0.8372 time: 0.22s
Epoch 94/1000, LR 0.000265
Train loss: 0.1158;  Loss pred: 0.1158; Loss self: 0.0000; time: 0.35s
Val loss: 0.3022 score: 0.9318 time: 0.24s
Test loss: 0.3759 score: 0.8372 time: 0.22s
Epoch 95/1000, LR 0.000265
Train loss: 0.1141;  Loss pred: 0.1141; Loss self: 0.0000; time: 0.41s
Val loss: 0.2976 score: 0.9318 time: 0.21s
Test loss: 0.3774 score: 0.8372 time: 0.22s
Epoch 96/1000, LR 0.000265
Train loss: 0.1147;  Loss pred: 0.1147; Loss self: 0.0000; time: 0.33s
Val loss: 0.2939 score: 0.9318 time: 0.24s
Test loss: 0.3781 score: 0.8372 time: 0.20s
Epoch 97/1000, LR 0.000265
Train loss: 0.1401;  Loss pred: 0.1401; Loss self: 0.0000; time: 0.51s
Val loss: 0.2909 score: 0.9318 time: 0.21s
Test loss: 0.3791 score: 0.8372 time: 0.23s
Epoch 98/1000, LR 0.000265
Train loss: 0.0992;  Loss pred: 0.0992; Loss self: 0.0000; time: 0.34s
Val loss: 0.2859 score: 0.9091 time: 0.27s
Test loss: 0.3855 score: 0.8372 time: 0.20s
Epoch 99/1000, LR 0.000265
Train loss: 0.1101;  Loss pred: 0.1101; Loss self: 0.0000; time: 0.44s
Val loss: 0.2835 score: 0.9318 time: 0.22s
Test loss: 0.3877 score: 0.8372 time: 0.22s
Epoch 100/1000, LR 0.000265
Train loss: 0.0761;  Loss pred: 0.0761; Loss self: 0.0000; time: 0.38s
Val loss: 0.2812 score: 0.9318 time: 0.22s
Test loss: 0.3908 score: 0.8372 time: 0.20s
Epoch 101/1000, LR 0.000265
Train loss: 0.0839;  Loss pred: 0.0839; Loss self: 0.0000; time: 0.47s
Val loss: 0.2774 score: 0.9091 time: 0.24s
Test loss: 0.3981 score: 0.8372 time: 0.21s
Epoch 102/1000, LR 0.000264
Train loss: 0.0890;  Loss pred: 0.0890; Loss self: 0.0000; time: 0.45s
Val loss: 0.2733 score: 0.9091 time: 0.23s
Test loss: 0.4081 score: 0.8372 time: 0.22s
Epoch 103/1000, LR 0.000264
Train loss: 0.0719;  Loss pred: 0.0719; Loss self: 0.0000; time: 0.37s
Val loss: 0.2698 score: 0.9091 time: 0.33s
Test loss: 0.4200 score: 0.8372 time: 0.22s
Epoch 104/1000, LR 0.000264
Train loss: 0.0689;  Loss pred: 0.0689; Loss self: 0.0000; time: 0.47s
Val loss: 0.2687 score: 0.9091 time: 0.23s
Test loss: 0.4234 score: 0.8372 time: 0.25s
Epoch 105/1000, LR 0.000264
Train loss: 0.0651;  Loss pred: 0.0651; Loss self: 0.0000; time: 0.40s
Val loss: 0.2675 score: 0.9091 time: 0.24s
Test loss: 0.4289 score: 0.8605 time: 0.20s
Epoch 106/1000, LR 0.000264
Train loss: 0.1004;  Loss pred: 0.1004; Loss self: 0.0000; time: 0.39s
Val loss: 0.2661 score: 0.9091 time: 0.23s
Test loss: 0.4378 score: 0.8605 time: 0.21s
Epoch 107/1000, LR 0.000264
Train loss: 0.0883;  Loss pred: 0.0883; Loss self: 0.0000; time: 0.39s
Val loss: 0.2649 score: 0.8864 time: 0.21s
Test loss: 0.4518 score: 0.8372 time: 0.22s
Epoch 108/1000, LR 0.000264
Train loss: 0.1425;  Loss pred: 0.1425; Loss self: 0.0000; time: 0.42s
Val loss: 0.2646 score: 0.8864 time: 0.23s
Test loss: 0.4606 score: 0.8372 time: 0.21s
Epoch 109/1000, LR 0.000264
Train loss: 0.0498;  Loss pred: 0.0498; Loss self: 0.0000; time: 0.39s
Val loss: 0.2645 score: 0.8864 time: 0.22s
Test loss: 0.4661 score: 0.8372 time: 0.23s
Epoch 110/1000, LR 0.000263
Train loss: 0.0540;  Loss pred: 0.0540; Loss self: 0.0000; time: 0.38s
Val loss: 0.2648 score: 0.8864 time: 0.23s
Test loss: 0.4639 score: 0.8372 time: 0.21s
     INFO: Early stopping counter 1 of 2
Epoch 111/1000, LR 0.000263
Train loss: 0.0465;  Loss pred: 0.0465; Loss self: 0.0000; time: 0.42s
Val loss: 0.2658 score: 0.9091 time: 0.22s
Test loss: 0.4585 score: 0.8605 time: 0.30s
     INFO: Early stopping counter 2 of 2
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 108,   Train_Loss: 0.0498,   Val_Loss: 0.2645,   Val_Precision: 0.9048,   Val_Recall: 0.8636,   Val_accuracy: 0.8837,   Val_Score: 0.8864,   Val_Loss: 0.2645,   Test_Precision: 0.8182,   Test_Recall: 0.8571,   Test_accuracy: 0.8372,   Test_Score: 0.8372,   Test_loss: 0.4661


[0.23468184401281178, 0.25102296005934477, 0.24650598608423024, 0.2508760100463405, 0.25733099796343595, 0.2404125159373507, 0.24052650504745543, 0.26913144101854414, 0.24596615100745112, 0.24488305510021746, 0.24438242195174098, 0.3725928779458627, 0.2537640519440174, 0.24343412299640477, 0.2524806510191411, 0.22814964200370014, 0.25440057704690844, 0.25646022893488407, 0.2483890700386837, 0.3457996170036495, 0.24092389491852373, 0.2624215000541881, 0.2450473200296983, 0.2415760630974546, 0.24650537699926645, 0.24586349702440202, 0.2558956299908459, 0.23553229705430567, 0.24693414405919611, 0.26384557597339153, 0.2507684590527788, 0.2620564189273864, 0.25298753299284726, 0.3606757080415264, 0.2489127640146762, 0.24424883199390024, 0.26140037702862173, 0.2503615020541474, 0.26065148995257914, 0.23473681102041155, 0.26141359901521355, 0.25322777801193297, 0.22919227601960301, 0.24992068705614656, 0.22677756601478904, 0.2489689220674336, 0.24267618905287236, 0.2572813150472939, 0.24270898604299873, 0.25256513501517475, 0.2332909689284861, 0.29722194594796747, 0.238997268024832, 0.24179001001175493, 0.25033164396882057, 0.24423006700817496, 0.2582209960091859, 0.235044569009915, 0.24134671094361693, 0.22892710601445287, 0.2543337279930711, 0.22688213305082172, 0.25318970205262303, 0.33264023694209754, 0.24236291204579175, 0.23023807303979993, 0.2385168339824304, 0.22832115704659373, 0.24732943391427398, 0.23757801298052073, 0.2533960920991376, 0.24834971106611192, 0.24560222506988794, 0.2425267540384084, 0.24034681194461882, 0.2504175390349701, 0.24544032802805305, 0.24889367795549333, 0.24784845300018787, 0.2527881939895451, 0.2317594800842926, 0.25178492604754865, 0.4000416590133682, 0.234819530043751, 0.24645066901575774, 0.24402819492388517, 0.24411627696827054, 0.2266990179196, 0.2555487169884145, 0.24729900201782584, 0.25072399503551424, 0.22028259502258152, 0.24993317504413426, 0.23884775396436453, 0.2524173560086638, 0.2271501439390704, 0.24912126001436263, 0.24378823104780167, 0.22085213800892234, 0.23358190106227994, 0.22147515299730003, 0.3213374959304929, 0.21999513998162, 0.21708667499478906, 0.22381247801240534, 0.2242548509966582, 0.2317955499747768, 0.21968973602633923, 0.29647067200858146, 0.24341119697783142, 0.2245029719779268, 0.3197630239883438, 0.24640670896042138, 0.21122506901156157, 0.22928268904797733, 0.20682163001038134, 0.22862039308529347, 0.30441404902376235, 0.26355198095552623, 0.2218575180741027, 0.2302130290772766, 0.2198511390015483, 0.22711682110093534, 0.1984006909187883, 0.26286730205174536, 0.21009562502149493, 0.22280164097901434, 0.22109774698037654, 0.2309997190022841, 0.2225996849592775, 0.20691071904730052, 0.22429993003606796, 0.30090237699914724, 0.23657749604899436, 0.2065312840277329, 0.2332672670017928, 0.21919950505252928, 0.22890625894069672, 0.22019404801540077, 0.24728360201697797, 0.2211184430634603, 0.2441404330311343, 0.21205284690950066, 0.23205031594261527, 0.20843571703881025, 0.22341767395846546, 0.2070921229897067, 0.22979786596260965, 0.2164476019097492, 0.23843313194811344, 0.20726356201339513, 0.23343332193326205, 0.21352267090696841, 0.23918717505875975, 0.2186636079568416, 0.22755373804830015, 0.2161115730414167, 0.2341222909744829, 0.2096631060121581, 0.21262713603209704, 0.2310659970389679, 0.21733400807715952, 0.22900946799200028, 0.20771243202034384, 0.22401523601729423, 0.22222013608552516, 0.22371921606827527, 0.22054806898813695, 0.22648960805963725, 0.297392460051924, 0.21205421502236277, 0.22540392400696874, 0.21326502703595906, 0.23100691905710846, 0.22011971403844655, 0.2284990210318938, 0.2164872030261904, 0.22610084002371877, 0.22561731794849038, 0.24441424699034542, 0.2081829139497131, 0.2293583860155195, 0.20568259991705418, 0.2409931030124426, 0.21003260498400778, 0.2337366360006854, 0.3040129829896614, 0.24067780293989927, 0.2151601529913023, 0.20666829496622086, 0.2252680560341105, 0.22176775301340967, 0.22858784801792353, 0.2081412150291726, 0.23271388304419816, 0.2078593340702355, 0.2275835599284619, 0.20609286590479314, 0.21215999708510935, 0.22899189009331167, 0.2212381480494514, 0.25908467697445303, 0.2079556209500879, 0.2108402419835329, 0.22411361301783472, 0.21150037401821464, 0.2295904840575531, 0.2141412500059232, 0.3047166720498353]
[0.0053336782730184495, 0.005705067274076017, 0.005602408774641596, 0.005834325815031175, 0.005984441813103161, 0.0055909887427290856, 0.005593639652266405, 0.006258870721361492, 0.00572014304668491, 0.005694954769772499, 0.005683312138412581, 0.008664950649903785, 0.005901489580093428, 0.005661258674334995, 0.005871643046956769, 0.005305805627993027, 0.005916292489462987, 0.0059641913705786994, 0.005776490000899621, 0.008041851558224407, 0.00560288127717497, 0.006102825582655537, 0.0056987748844115884, 0.005618047979010572, 0.005732683186029452, 0.0057177557447535355, 0.005951061162577812, 0.00547749528033269, 0.0057426545130045605, 0.006135943627288175, 0.005831824629134391, 0.006094335323892707, 0.005883430999833657, 0.008387807163756427, 0.005788668930573866, 0.005680205395206982, 0.006079078535549343, 0.005822360512887149, 0.006061662557036724, 0.00545899560512585, 0.006079386023609618, 0.005889018093300767, 0.005330052930688442, 0.005812109001305734, 0.005273896884064861, 0.005789974931800781, 0.0056436323035551714, 0.005983286396448695, 0.005644395024255785, 0.005873607791050575, 0.00542537137042991, 0.006912138277859708, 0.005558076000577489, 0.005623023488645463, 0.005821666138809781, 0.005679769000190116, 0.006005139442074091, 0.005466152767672441, 0.005612714207991091, 0.005323886186382625, 0.005914737860303979, 0.0052763286756005055, 0.005888132605874954, 0.007735819463769711, 0.005636346791762599, 0.005354373791623254, 0.005546903115870474, 0.005309794349920785, 0.005751847300331953, 0.005525070069314435, 0.005892932374398549, 0.005775574675956091, 0.005711679652788092, 0.00564015707066066, 0.005589460742898112, 0.005823663698487677, 0.00570791460530356, 0.005788225068732403, 0.005763917511632276, 0.005878795209059188, 0.005389755350797502, 0.00585546339645462, 0.009303294395659726, 0.005460919303343047, 0.005731410907343203, 0.005675074300555469, 0.005677122720192338, 0.005272070184176744, 0.005942993418335221, 0.005751139581809903, 0.005830790582221261, 0.005122851047036779, 0.005812399419631029, 0.0055545989294038265, 0.005870171069968925, 0.005282561486955125, 0.005793517674752619, 0.005669493745297714, 0.0051360962327656355, 0.00543213723400651, 0.005150584953425582, 0.007472965021639369, 0.005116166046084186, 0.005048527325460211, 0.005204941349125705, 0.005215229092945539, 0.0053905941854599255, 0.005109063628519517, 0.006894666790897243, 0.005660725511112358, 0.005220999348323879, 0.007436349395077762, 0.005730388580474916, 0.004912210907245618, 0.005332155559255287, 0.004809805349078636, 0.005316753327564964, 0.007079396488924705, 0.006129115836175029, 0.005159477164514016, 0.005353791373890153, 0.005112817186082519, 0.005281786537231054, 0.004613969556250891, 0.006113193070970822, 0.004885944767941742, 0.005181433511139868, 0.005141808069311082, 0.005372086488425212, 0.005176736859518082, 0.004811877187146524, 0.005216277442699255, 0.006997729697654587, 0.005501802233697543, 0.0048030531169240215, 0.00542482016283239, 0.005097662908198355, 0.005323401370713877, 0.005120791814311645, 0.005750781442255302, 0.005142289373568844, 0.005677684489096147, 0.004931461556034899, 0.005396518975409658, 0.004847342256716517, 0.005195759859499197, 0.004816095883481551, 0.005344136417735108, 0.005033665160691842, 0.005544956556932871, 0.004820082837520817, 0.005428681905424699, 0.004965643509464382, 0.005562492443226971, 0.005085200185042827, 0.0052919473964720964, 0.0050258505358469, 0.0054447044412670445, 0.004875886186329258, 0.004944817117025512, 0.0053736278381155324, 0.005054279257608361, 0.005325801581209309, 0.004830521674891717, 0.005209656651564982, 0.005167910141523841, 0.005202772466704076, 0.005129024860189232, 0.005267200187433424, 0.006916103722137767, 0.004931493372613087, 0.005241951721092296, 0.004959651791533932, 0.0053722539315606615, 0.005119063117173176, 0.0053139307216719485, 0.005034586116888149, 0.0052581590703190415, 0.0052469143708951255, 0.005684052255589428, 0.0048414631151096075, 0.005333915953849291, 0.004783316277140795, 0.005604490767731223, 0.0048844791856745996, 0.005435735720946172, 0.00707006937185259, 0.005597158207904634, 0.005003724488169822, 0.00480623941781909, 0.005238792000793267, 0.0051573896049630155, 0.005315996465533105, 0.004840493372771456, 0.005411950768469725, 0.004833938001633384, 0.005292640928568882, 0.004792857346623097, 0.004933953420583939, 0.005325392792867713, 0.005145073210452359, 0.006025225045917512, 0.004836177231397393, 0.00490326144147751, 0.005211944488786854, 0.004918613349260806, 0.0053393135827337936, 0.0049800290699051905, 0.0070864342337171]
[187.48787399845872, 175.28277090509124, 178.49465118045998, 171.39940958108056, 167.09996207339876, 178.85924046984894, 178.7744763992483, 159.77323138933122, 174.82080287826835, 175.59401969402256, 175.9537353651866, 115.40746628615871, 169.4487444954819, 176.6391641020477, 170.31008050094135, 188.4727919025296, 169.02477383953146, 167.66732283826283, 173.11550783334891, 124.34947260090861, 178.47959835840214, 163.85852527754327, 175.4763120640889, 177.99776786102063, 174.4383855778041, 174.89379481059052, 168.0372580084241, 182.56519610168652, 174.13549739679522, 162.97411787695273, 171.4729203282693, 164.08680304799148, 169.96884981370107, 119.22067120486307, 172.75128565710943, 176.04997186260388, 164.49861507006077, 171.75164570909186, 164.9712418978425, 183.18388076023123, 164.49029492722573, 169.80759511294093, 187.61539763374125, 172.05458462243953, 189.61311189483268, 172.7123194450486, 177.19084912212588, 167.13223030633088, 177.1669055235641, 170.25311113276365, 184.31917959576646, 144.6730316728621, 179.91837461310337, 177.8402672546707, 171.77213123465827, 176.0634983511702, 166.5240265685844, 182.94402708869274, 178.16691941596667, 187.83271561247656, 169.0692002956504, 189.52572166785816, 169.8331316455472, 129.26878718970184, 177.41988506837066, 186.76320311526771, 180.2807763378557, 188.33121098464366, 173.85718844488233, 180.99317971619544, 169.694803277301, 173.14294353478505, 175.07984704847047, 177.3000268382357, 178.90813550673658, 171.71321212447174, 175.1953329979466, 172.76453284478032, 173.49311435874648, 170.10288068191352, 185.5371783901163, 170.780676488471, 107.48880530605706, 183.11935123960237, 174.47710802217637, 176.20914670705216, 176.1455669160029, 189.6788102330914, 168.265372281049, 173.8785828052006, 171.50332976271054, 195.20380171475645, 172.04598786218307, 180.0310000252943, 170.3527866702007, 189.30210324468956, 172.60670565619697, 176.38259162546947, 194.70040176048835, 184.08960542081945, 194.1527047981053, 133.8156939185869, 195.4588633348561, 198.07756510634368, 192.12512359394287, 191.74613083683428, 185.5083068017445, 195.73058249223172, 145.03964155603003, 176.65580110481199, 191.53421275967685, 134.47458515893777, 174.50823551604998, 203.57432098955243, 187.5414152657738, 207.90862153944744, 188.08470854110377, 141.2549786644159, 163.1556698762061, 193.81808817331833, 186.78352034352497, 195.58688754256204, 189.3298778644402, 216.73311620472765, 163.58063427582735, 204.6687073831293, 192.9967831971676, 194.48411658313483, 186.14741258440586, 193.1718816577231, 207.81910283811854, 191.70759434960812, 142.90349058997916, 181.7586233607564, 208.2009038118699, 184.33790798290408, 196.168326154274, 187.84982201443478, 195.28229935167232, 173.88941138542503, 194.4659134003541, 176.12813849034328, 202.77964020144196, 185.30463888975552, 206.29861623952627, 192.46463020644433, 207.63706209210704, 187.12097181527577, 198.66239967828074, 180.34406396740798, 207.46531412608346, 184.2067775237915, 201.38376790319867, 179.77552512770148, 196.64909219135845, 188.9663530417281, 198.97129707051488, 183.66469856852115, 205.09092332871614, 202.23194838832308, 186.09401881293812, 197.85214647462732, 187.76516262420236, 207.01697814499846, 191.95122958815332, 193.50181652058177, 192.20521489256933, 194.96883467301146, 189.8541852245937, 144.59008137762572, 202.77833192547158, 190.7686398514987, 201.62705811464193, 186.14161071673243, 195.34824578451676, 188.18461368374125, 198.6260591800334, 190.18062911880003, 190.58820657471483, 175.93082453044784, 206.54913116638724, 187.47951948480494, 209.05997890604579, 178.4283428135281, 204.7301179894145, 183.96773708967862, 141.44132785757478, 178.66209295062296, 199.85113136510105, 208.06287682892136, 190.8836998774867, 193.8965400321295, 188.11148699658077, 206.59051112953875, 184.77625587912704, 206.87067141988598, 188.94159144674828, 208.6438063308039, 202.67722752065401, 187.77957587265635, 194.36069402636923, 165.96890446068335, 206.7748868895473, 203.94588620970373, 191.86697060021118, 203.30933313761795, 187.2899923379267, 200.80204070355714, 141.11469421984074]
Elapsed: 0.24006349720009582~0.02869945538830374
Time per graph: 0.005581020336096394~0.0006673346443745751
Speed: 181.27943918215414~17.955197216672456
Total Time: 0.3054
best val loss: 0.26451241970062256 test_score: 0.8372

Testing...
Test loss: 0.3759 score: 0.8372 time: 0.30s
test Score 0.8372
Epoch Time List: [0.8162464839406312, 0.8219865409191698, 0.8088915530825034, 0.9324288050411269, 0.8398615999612957, 0.8303632360184565, 0.8312249730806798, 1.049406121019274, 0.8100718049099669, 0.8434168830281124, 0.7811744039645419, 0.9555258869659156, 0.9838238109368831, 0.8119234750047326, 0.8291357088601217, 0.7661242100875825, 0.8943514609709382, 0.8179002840770409, 0.8501789979636669, 0.9158482679631561, 0.8253506600158289, 0.8914776530582458, 0.8099420509533957, 0.9668285449733958, 0.9284389670938253, 0.831494536017999, 0.8381367269903421, 0.8887598408618942, 0.9253373668761924, 0.8612187780672684, 0.8091769120655954, 0.8324428829364479, 0.7996253238525242, 0.9411252980353311, 0.8710428000194952, 0.8383907821262255, 0.8560775249497965, 0.8117943060351536, 0.8309120449703187, 0.8305285018868744, 0.8582504939986393, 0.8819917980581522, 1.008828318095766, 0.8409999300492927, 0.7850240260595456, 0.8145561559358612, 0.7777390778064728, 0.8351059580454603, 0.7845446121646091, 0.823646969976835, 0.8900417539989576, 0.9125833939760923, 0.8368142211111262, 0.8584061119472608, 0.8453135630115867, 0.8163206939352676, 0.8354629811365157, 0.8851404250599444, 0.8260606919648126, 0.8171316758962348, 0.8245038730092347, 0.7962292318698019, 0.8114878538763151, 0.8762747930595651, 0.8005517810815945, 0.8027848950587213, 0.8133258799789473, 0.7828062641201541, 0.801995322923176, 0.7964417460607365, 0.8197084579151124, 0.8329761192435399, 0.9333100350340828, 0.8690042940434068, 0.8222712279530242, 0.8454865139210597, 0.824582515982911, 0.8366669419920072, 0.8017024140572175, 0.8387865618569776, 0.8686352248769253, 0.8635732369730249, 1.075357815832831, 0.8049577199853957, 0.9027087721042335, 0.8105645158793777, 0.8119195150211453, 0.8888602040242404, 0.8278359980322421, 0.7734517618082464, 0.8178681778954342, 0.7590211320202798, 0.886723374016583, 0.7929608990671113, 0.9300410180585459, 0.8661429119529203, 0.8704048690851778, 0.8911651249509305, 0.8225432289764285, 0.9574600988999009, 0.8209133519558236, 0.9385939129861072, 0.8146892721997574, 0.8284676080802456, 0.8119181381771341, 0.8253562131430954, 0.8319889520062134, 0.822202431038022, 0.9164327260805294, 0.8684469629079103, 1.0650292999343947, 0.9841821680311114, 0.8817926790798083, 0.8312772038625553, 0.8678559290710837, 0.9144684289349243, 0.9317893692059442, 1.1777434181421995, 0.9464979440672323, 0.8280711648985744, 0.8090727539965883, 0.8006318439729512, 0.815429674112238, 0.7691781980684027, 1.013622370082885, 0.8119661449454725, 0.8448627589968964, 0.8335406720871106, 0.8620251689571887, 0.8942586540943012, 0.8081036519724876, 0.8868246100610122, 0.9007746878778562, 0.8499937229789793, 0.7653269809670746, 0.8221759367734194, 0.7879934528609738, 0.8338270329404622, 0.8103344648843631, 0.875381947029382, 0.7876574898837134, 0.9567062321584672, 0.8262377260252833, 0.8400789470179006, 0.8129635519580916, 0.8257914858404547, 0.7684633390745148, 0.8217587269609794, 0.8052465419750661, 0.9344488299684599, 0.8048945618793368, 0.8189360230462626, 0.7975972979329526, 0.8316235520178452, 0.7791164730442688, 0.8133076269878075, 0.9045314110117033, 0.8982409810414538, 0.8060400519752875, 0.9078908629016951, 0.8853557569673285, 0.8255057070637122, 0.8359436788596213, 0.89269194600638, 0.8501771999290213, 0.8489744769176468, 0.8196883131749928, 0.8281088711228222, 0.8286060448735952, 0.9338003189768642, 0.8316283620661125, 1.0120046840747818, 0.7747782960068434, 0.8288227979792282, 0.7887743269093335, 0.8188258971786126, 0.804936348926276, 0.8187866169027984, 0.8226525268983096, 0.9614976670127362, 0.8538335360353813, 0.8534252952085808, 0.7933685530442744, 0.8333419989794493, 0.7966434858972207, 0.8312821821309626, 0.8788790710968897, 0.8901533089810982, 0.8282967599807307, 0.8350893661845475, 0.9084202060475945, 0.8088389498880133, 0.8446947099873796, 0.7757059998111799, 0.9537903970340267, 0.8131493310211226, 0.886998696019873, 0.8028406709199771, 0.9167604520916939, 0.8987145110731944, 0.9217445279937238, 0.9481387259438634, 0.8430947929155082, 0.8283390488941222, 0.8157939208904281, 0.8593939170241356, 0.834745294880122, 0.8163787740049884, 0.9416677780682221]
Total Epoch List: [3, 95, 111]
Total Time List: [0.2467775100376457, 0.24458533606957644, 0.30541634396649897]
T-times Epoch Time: 1.130314225147064 ~ 0.4392542228354725
T-times Total Epoch: 92.03333333333333 ~ 11.199652772395323
T-times Total Time: 0.2798540983038644 ~ 0.06614592117264725
T-times Inference Elapsed: 0.29486572199262984 ~ 0.07936486794706078
T-times Time Per Graph: 0.006815151906696047 ~ 0.0018380356639169644
T-times Speed: 167.57984813839226 ~ 18.796645022667576
T-times cross validation test micro f1 score:0.755280117987674 ~ 0.046083501128046885
T-times cross validation test precision:0.8206535649879303 ~ 0.12714467363639534
T-times cross validation test recall:0.704978354978355 ~ 0.0938940814103356
T-times cross validation test f1_score:0.755280117987674 ~ 0.10820144535852028
