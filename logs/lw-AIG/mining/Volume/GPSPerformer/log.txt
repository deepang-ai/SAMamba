Namespace(seed=60, model='GPSPerformer', dataset='mining/Volume', num_heads=1, num_layers=2, dim_hidden=64, dropout=0.1, epochs=1000, lr=0.0003, weight_decay=0.0001, batch_size=64, abs_pe=None, abs_pe_dim=20, num_class=2, outdir='./outdir/mining/Volume/seed60/edge_attr/khopgnn_gat_1_0.1_0.0003_0.0001_2_1_64_BN', warmup=10, layer_norm=False, use_edge_attr=True, edge_dim=128, gnn_type='gat', k_hop=1, global_pool='mean', se='khopgnn', ie='mamba', aggr='add', not_extract_node_feature=False, k_ford=3, early_stop=1, early_stop_mindelta=-0.0, patience=20, training_times=3, Lambda=0.01, order='sd', use_cuda=True, batch_norm=True, save_logs=True)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 132], edge_attr=[132, 2], x=[32, 14887], y=[1, 1], num_nodes=32)
Data(edge_index=[2, 118], edge_attr=[118, 2], x=[28, 14887], y=[1, 1], num_nodes=32)
========================training times:0========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdd16fb0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8154;  Loss pred: 0.8154; Loss self: 0.0000; time: 0.37s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.6637 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.6668 score: 0.5000 time: 0.07s
Epoch 2/1000, LR 0.000000
Train loss: 0.7407;  Loss pred: 0.7407; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.2609 score: 0.4884 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3157 score: 0.5000 time: 0.22s
Epoch 3/1000, LR 0.000030
Train loss: 0.7795;  Loss pred: 0.7795; Loss self: 0.0000; time: 0.30s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.0277 score: 0.4884 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0978 score: 0.5000 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.7551;  Loss pred: 0.7551; Loss self: 0.0000; time: 0.21s
Val loss: 0.9002 score: 0.5116 time: 0.06s
Test loss: 0.9726 score: 0.5227 time: 0.09s
Epoch 5/1000, LR 0.000090
Train loss: 0.7571;  Loss pred: 0.7571; Loss self: 0.0000; time: 0.18s
Val loss: 0.8276 score: 0.4884 time: 0.17s
Test loss: 0.8899 score: 0.5000 time: 0.04s
Epoch 6/1000, LR 0.000120
Train loss: 0.7278;  Loss pred: 0.7278; Loss self: 0.0000; time: 0.15s
Val loss: 0.7833 score: 0.4884 time: 0.15s
Test loss: 0.8302 score: 0.5000 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.7337;  Loss pred: 0.7337; Loss self: 0.0000; time: 0.24s
Val loss: 0.7591 score: 0.4884 time: 0.10s
Test loss: 0.7967 score: 0.4318 time: 0.09s
Epoch 8/1000, LR 0.000180
Train loss: 0.6868;  Loss pred: 0.6868; Loss self: 0.0000; time: 0.16s
Val loss: 0.7312 score: 0.5581 time: 0.19s
Test loss: 0.7720 score: 0.4545 time: 0.05s
Epoch 9/1000, LR 0.000210
Train loss: 0.6393;  Loss pred: 0.6393; Loss self: 0.0000; time: 0.29s
Val loss: 0.7115 score: 0.5814 time: 0.07s
Test loss: 0.7572 score: 0.4545 time: 0.08s
Epoch 10/1000, LR 0.000240
Train loss: 0.6789;  Loss pred: 0.6789; Loss self: 0.0000; time: 0.15s
Val loss: 0.6996 score: 0.6512 time: 0.06s
Test loss: 0.7500 score: 0.5455 time: 0.07s
Epoch 11/1000, LR 0.000270
Train loss: 0.6109;  Loss pred: 0.6109; Loss self: 0.0000; time: 0.14s
Val loss: 0.6903 score: 0.6279 time: 0.13s
Test loss: 0.7490 score: 0.5455 time: 0.09s
Epoch 12/1000, LR 0.000270
Train loss: 0.5448;  Loss pred: 0.5448; Loss self: 0.0000; time: 0.14s
Val loss: 0.6840 score: 0.6512 time: 0.17s
Test loss: 0.7458 score: 0.5455 time: 0.05s
Epoch 13/1000, LR 0.000270
Train loss: 0.4938;  Loss pred: 0.4938; Loss self: 0.0000; time: 0.20s
Val loss: 0.6791 score: 0.6047 time: 0.07s
Test loss: 0.7456 score: 0.5455 time: 0.10s
Epoch 14/1000, LR 0.000270
Train loss: 0.5097;  Loss pred: 0.5097; Loss self: 0.0000; time: 0.18s
Val loss: 0.6766 score: 0.6047 time: 0.06s
Test loss: 0.7460 score: 0.5682 time: 0.25s
Epoch 15/1000, LR 0.000270
Train loss: 0.4918;  Loss pred: 0.4918; Loss self: 0.0000; time: 0.61s
Val loss: 0.6737 score: 0.6279 time: 0.30s
Test loss: 0.7434 score: 0.5455 time: 0.05s
Epoch 16/1000, LR 0.000270
Train loss: 0.4485;  Loss pred: 0.4485; Loss self: 0.0000; time: 0.27s
Val loss: 0.6703 score: 0.6279 time: 0.05s
Test loss: 0.7372 score: 0.5682 time: 0.07s
Epoch 17/1000, LR 0.000270
Train loss: 0.4796;  Loss pred: 0.4796; Loss self: 0.0000; time: 0.21s
Val loss: 0.6659 score: 0.6279 time: 0.08s
Test loss: 0.7300 score: 0.5682 time: 0.11s
Epoch 18/1000, LR 0.000270
Train loss: 0.4139;  Loss pred: 0.4139; Loss self: 0.0000; time: 0.38s
Val loss: 0.6613 score: 0.6512 time: 0.18s
Test loss: 0.7232 score: 0.5682 time: 0.29s
Epoch 19/1000, LR 0.000270
Train loss: 0.3883;  Loss pred: 0.3883; Loss self: 0.0000; time: 0.15s
Val loss: 0.6543 score: 0.6279 time: 0.08s
Test loss: 0.7145 score: 0.6136 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.3593;  Loss pred: 0.3593; Loss self: 0.0000; time: 0.21s
Val loss: 0.6482 score: 0.6512 time: 0.07s
Test loss: 0.7075 score: 0.6364 time: 0.25s
Epoch 21/1000, LR 0.000270
Train loss: 0.3760;  Loss pred: 0.3760; Loss self: 0.0000; time: 0.14s
Val loss: 0.6442 score: 0.6279 time: 0.18s
Test loss: 0.7031 score: 0.5909 time: 0.06s
Epoch 22/1000, LR 0.000270
Train loss: 0.3398;  Loss pred: 0.3398; Loss self: 0.0000; time: 0.22s
Val loss: 0.6413 score: 0.6047 time: 0.07s
Test loss: 0.7016 score: 0.5227 time: 0.09s
Epoch 23/1000, LR 0.000270
Train loss: 0.3112;  Loss pred: 0.3112; Loss self: 0.0000; time: 0.30s
Val loss: 0.6380 score: 0.6047 time: 0.28s
Test loss: 0.7009 score: 0.5227 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.3262;  Loss pred: 0.3262; Loss self: 0.0000; time: 0.15s
Val loss: 0.6337 score: 0.6512 time: 0.08s
Test loss: 0.7005 score: 0.5000 time: 0.33s
Epoch 25/1000, LR 0.000270
Train loss: 0.3084;  Loss pred: 0.3084; Loss self: 0.0000; time: 0.14s
Val loss: 0.6298 score: 0.6279 time: 0.09s
Test loss: 0.6972 score: 0.5000 time: 0.08s
Epoch 26/1000, LR 0.000270
Train loss: 0.2780;  Loss pred: 0.2780; Loss self: 0.0000; time: 0.37s
Val loss: 0.6266 score: 0.6047 time: 0.08s
Test loss: 0.6944 score: 0.4773 time: 0.17s
Epoch 27/1000, LR 0.000270
Train loss: 0.2999;  Loss pred: 0.2999; Loss self: 0.0000; time: 0.20s
Val loss: 0.6218 score: 0.6047 time: 0.12s
Test loss: 0.6902 score: 0.4773 time: 0.34s
Epoch 28/1000, LR 0.000270
Train loss: 0.2587;  Loss pred: 0.2587; Loss self: 0.0000; time: 0.16s
Val loss: 0.6173 score: 0.5814 time: 0.07s
Test loss: 0.6823 score: 0.4773 time: 0.08s
Epoch 29/1000, LR 0.000270
Train loss: 0.2177;  Loss pred: 0.2177; Loss self: 0.0000; time: 0.23s
Val loss: 0.6131 score: 0.5814 time: 0.06s
Test loss: 0.6736 score: 0.5227 time: 0.23s
Epoch 30/1000, LR 0.000270
Train loss: 0.2394;  Loss pred: 0.2394; Loss self: 0.0000; time: 0.15s
Val loss: 0.6112 score: 0.5814 time: 0.22s
Test loss: 0.6645 score: 0.5227 time: 0.15s
Epoch 31/1000, LR 0.000270
Train loss: 0.2018;  Loss pred: 0.2018; Loss self: 0.0000; time: 0.15s
Val loss: 0.6102 score: 0.5581 time: 0.06s
Test loss: 0.6620 score: 0.5227 time: 0.08s
Epoch 32/1000, LR 0.000270
Train loss: 0.2026;  Loss pred: 0.2026; Loss self: 0.0000; time: 0.22s
Val loss: 0.6108 score: 0.5581 time: 0.05s
Test loss: 0.6645 score: 0.5227 time: 0.30s
     INFO: Early stopping counter 1 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1944;  Loss pred: 0.1944; Loss self: 0.0000; time: 0.15s
Val loss: 0.6113 score: 0.5581 time: 0.18s
Test loss: 0.6720 score: 0.5227 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1730;  Loss pred: 0.1730; Loss self: 0.0000; time: 0.15s
Val loss: 0.6125 score: 0.5581 time: 0.08s
Test loss: 0.6742 score: 0.5227 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1742;  Loss pred: 0.1742; Loss self: 0.0000; time: 0.15s
Val loss: 0.6121 score: 0.5581 time: 0.07s
Test loss: 0.6700 score: 0.5227 time: 0.25s
     INFO: Early stopping counter 4 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1691;  Loss pred: 0.1691; Loss self: 0.0000; time: 0.26s
Val loss: 0.6113 score: 0.5814 time: 0.05s
Test loss: 0.6667 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1610;  Loss pred: 0.1610; Loss self: 0.0000; time: 0.15s
Val loss: 0.6100 score: 0.5814 time: 0.09s
Test loss: 0.6597 score: 0.5455 time: 0.09s
Epoch 38/1000, LR 0.000270
Train loss: 0.1424;  Loss pred: 0.1424; Loss self: 0.0000; time: 0.15s
Val loss: 0.6097 score: 0.5814 time: 0.07s
Test loss: 0.6505 score: 0.5455 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.1461;  Loss pred: 0.1461; Loss self: 0.0000; time: 0.20s
Val loss: 0.6095 score: 0.5581 time: 0.05s
Test loss: 0.6448 score: 0.5455 time: 0.22s
Epoch 40/1000, LR 0.000269
Train loss: 0.1305;  Loss pred: 0.1305; Loss self: 0.0000; time: 0.14s
Val loss: 0.6094 score: 0.5814 time: 0.15s
Test loss: 0.6426 score: 0.5455 time: 0.07s
Epoch 41/1000, LR 0.000269
Train loss: 0.1203;  Loss pred: 0.1203; Loss self: 0.0000; time: 0.32s
Val loss: 0.6091 score: 0.5581 time: 0.07s
Test loss: 0.6410 score: 0.5227 time: 0.08s
Epoch 42/1000, LR 0.000269
Train loss: 0.1159;  Loss pred: 0.1159; Loss self: 0.0000; time: 0.33s
Val loss: 0.6062 score: 0.5349 time: 0.16s
Test loss: 0.6412 score: 0.5455 time: 0.05s
Epoch 43/1000, LR 0.000269
Train loss: 0.1133;  Loss pred: 0.1133; Loss self: 0.0000; time: 0.19s
Val loss: 0.6045 score: 0.5349 time: 0.06s
Test loss: 0.6449 score: 0.5455 time: 0.10s
Epoch 44/1000, LR 0.000269
Train loss: 0.1135;  Loss pred: 0.1135; Loss self: 0.0000; time: 0.14s
Val loss: 0.6044 score: 0.5581 time: 0.07s
Test loss: 0.6470 score: 0.5455 time: 0.20s
Epoch 45/1000, LR 0.000269
Train loss: 0.0937;  Loss pred: 0.0937; Loss self: 0.0000; time: 0.14s
Val loss: 0.6038 score: 0.5814 time: 0.16s
Test loss: 0.6501 score: 0.5455 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0879;  Loss pred: 0.0879; Loss self: 0.0000; time: 0.16s
Val loss: 0.6023 score: 0.5581 time: 0.06s
Test loss: 0.6472 score: 0.5455 time: 0.08s
Epoch 47/1000, LR 0.000269
Train loss: 0.0870;  Loss pred: 0.0870; Loss self: 0.0000; time: 0.20s
Val loss: 0.5985 score: 0.5814 time: 0.09s
Test loss: 0.6437 score: 0.5682 time: 0.05s
Epoch 48/1000, LR 0.000269
Train loss: 0.0818;  Loss pred: 0.0818; Loss self: 0.0000; time: 0.30s
Val loss: 0.5920 score: 0.5814 time: 0.15s
Test loss: 0.6408 score: 0.5909 time: 0.23s
Epoch 49/1000, LR 0.000269
Train loss: 0.0867;  Loss pred: 0.0867; Loss self: 0.0000; time: 0.14s
Val loss: 0.5836 score: 0.5814 time: 0.07s
Test loss: 0.6383 score: 0.6136 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 0.0786;  Loss pred: 0.0786; Loss self: 0.0000; time: 0.18s
Val loss: 0.5775 score: 0.5814 time: 0.06s
Test loss: 0.6322 score: 0.6136 time: 0.06s
Epoch 51/1000, LR 0.000269
Train loss: 0.0675;  Loss pred: 0.0675; Loss self: 0.0000; time: 0.16s
Val loss: 0.5718 score: 0.5814 time: 0.06s
Test loss: 0.6255 score: 0.6136 time: 0.14s
Epoch 52/1000, LR 0.000269
Train loss: 0.0726;  Loss pred: 0.0726; Loss self: 0.0000; time: 0.15s
Val loss: 0.5686 score: 0.5814 time: 0.15s
Test loss: 0.6195 score: 0.6136 time: 0.06s
Epoch 53/1000, LR 0.000269
Train loss: 0.0645;  Loss pred: 0.0645; Loss self: 0.0000; time: 0.18s
Val loss: 0.5662 score: 0.6512 time: 0.06s
Test loss: 0.6163 score: 0.5909 time: 0.11s
Epoch 54/1000, LR 0.000269
Train loss: 0.0585;  Loss pred: 0.0585; Loss self: 0.0000; time: 0.20s
Val loss: 0.5664 score: 0.6512 time: 0.17s
Test loss: 0.6125 score: 0.5909 time: 0.04s
     INFO: Early stopping counter 1 of 20
Epoch 55/1000, LR 0.000269
Train loss: 0.0531;  Loss pred: 0.0531; Loss self: 0.0000; time: 0.14s
Val loss: 0.5662 score: 0.6512 time: 0.15s
Test loss: 0.6106 score: 0.6364 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0489;  Loss pred: 0.0489; Loss self: 0.0000; time: 0.25s
Val loss: 0.5674 score: 0.6279 time: 0.08s
Test loss: 0.6086 score: 0.6364 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0497;  Loss pred: 0.0497; Loss self: 0.0000; time: 0.18s
Val loss: 0.5658 score: 0.6279 time: 0.17s
Test loss: 0.6051 score: 0.6591 time: 0.04s
Epoch 58/1000, LR 0.000269
Train loss: 0.0423;  Loss pred: 0.0423; Loss self: 0.0000; time: 0.14s
Val loss: 0.5637 score: 0.6279 time: 0.16s
Test loss: 0.6022 score: 0.7045 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.0432;  Loss pred: 0.0432; Loss self: 0.0000; time: 0.17s
Val loss: 0.5619 score: 0.6279 time: 0.05s
Test loss: 0.5987 score: 0.7273 time: 0.10s
Epoch 60/1000, LR 0.000268
Train loss: 0.0445;  Loss pred: 0.0445; Loss self: 0.0000; time: 0.19s
Val loss: 0.5545 score: 0.6512 time: 0.18s
Test loss: 0.5953 score: 0.7273 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0382;  Loss pred: 0.0382; Loss self: 0.0000; time: 0.14s
Val loss: 0.5444 score: 0.6512 time: 0.15s
Test loss: 0.5867 score: 0.7500 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0345;  Loss pred: 0.0345; Loss self: 0.0000; time: 0.31s
Val loss: 0.5372 score: 0.6512 time: 0.08s
Test loss: 0.5836 score: 0.7500 time: 0.12s
Epoch 63/1000, LR 0.000268
Train loss: 0.0361;  Loss pred: 0.0361; Loss self: 0.0000; time: 0.17s
Val loss: 0.5295 score: 0.7209 time: 0.18s
Test loss: 0.5793 score: 0.7500 time: 0.05s
Epoch 64/1000, LR 0.000268
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.31s
Val loss: 0.5201 score: 0.7442 time: 0.08s
Test loss: 0.5715 score: 0.7500 time: 0.08s
Epoch 65/1000, LR 0.000268
Train loss: 0.0290;  Loss pred: 0.0290; Loss self: 0.0000; time: 0.19s
Val loss: 0.5120 score: 0.7442 time: 0.06s
Test loss: 0.5631 score: 0.7500 time: 0.21s
Epoch 66/1000, LR 0.000268
Train loss: 0.0336;  Loss pred: 0.0336; Loss self: 0.0000; time: 0.18s
Val loss: 0.5029 score: 0.7442 time: 0.13s
Test loss: 0.5545 score: 0.7500 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0297;  Loss pred: 0.0297; Loss self: 0.0000; time: 0.19s
Val loss: 0.4979 score: 0.7674 time: 0.05s
Test loss: 0.5514 score: 0.7273 time: 0.13s
Epoch 68/1000, LR 0.000268
Train loss: 0.0309;  Loss pred: 0.0309; Loss self: 0.0000; time: 0.17s
Val loss: 0.4934 score: 0.7674 time: 0.06s
Test loss: 0.5435 score: 0.7273 time: 0.22s
Epoch 69/1000, LR 0.000268
Train loss: 0.0302;  Loss pred: 0.0302; Loss self: 0.0000; time: 0.15s
Val loss: 0.4952 score: 0.7674 time: 0.17s
Test loss: 0.5331 score: 0.7273 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0252;  Loss pred: 0.0252; Loss self: 0.0000; time: 0.20s
Val loss: 0.5016 score: 0.7209 time: 0.09s
Test loss: 0.5207 score: 0.7500 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0240;  Loss pred: 0.0240; Loss self: 0.0000; time: 0.18s
Val loss: 0.5097 score: 0.6977 time: 0.06s
Test loss: 0.5072 score: 0.7273 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0217;  Loss pred: 0.0217; Loss self: 0.0000; time: 0.16s
Val loss: 0.5337 score: 0.6744 time: 0.17s
Test loss: 0.5187 score: 0.7500 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0231;  Loss pred: 0.0231; Loss self: 0.0000; time: 0.15s
Val loss: 0.5710 score: 0.6977 time: 0.05s
Test loss: 0.5399 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0203;  Loss pred: 0.0203; Loss self: 0.0000; time: 0.15s
Val loss: 0.6104 score: 0.6977 time: 0.14s
Test loss: 0.5704 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0179;  Loss pred: 0.0179; Loss self: 0.0000; time: 0.15s
Val loss: 0.6259 score: 0.6977 time: 0.05s
Test loss: 0.5845 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.14s
Val loss: 0.6242 score: 0.6977 time: 0.05s
Test loss: 0.5835 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0181;  Loss pred: 0.0181; Loss self: 0.0000; time: 0.14s
Val loss: 0.6213 score: 0.6977 time: 0.05s
Test loss: 0.5801 score: 0.7273 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0188;  Loss pred: 0.0188; Loss self: 0.0000; time: 0.16s
Val loss: 0.6177 score: 0.6977 time: 0.07s
Test loss: 0.5766 score: 0.7273 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0185;  Loss pred: 0.0185; Loss self: 0.0000; time: 0.22s
Val loss: 0.6076 score: 0.6977 time: 0.06s
Test loss: 0.5694 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0168;  Loss pred: 0.0168; Loss self: 0.0000; time: 0.15s
Val loss: 0.6021 score: 0.6977 time: 0.05s
Test loss: 0.5668 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0129;  Loss pred: 0.0129; Loss self: 0.0000; time: 0.14s
Val loss: 0.5951 score: 0.6977 time: 0.06s
Test loss: 0.5656 score: 0.7500 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0163;  Loss pred: 0.0163; Loss self: 0.0000; time: 0.14s
Val loss: 0.5834 score: 0.7209 time: 0.05s
Test loss: 0.5630 score: 0.7500 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0153;  Loss pred: 0.0153; Loss self: 0.0000; time: 0.16s
Val loss: 0.5646 score: 0.7674 time: 0.05s
Test loss: 0.5606 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0125;  Loss pred: 0.0125; Loss self: 0.0000; time: 0.15s
Val loss: 0.5434 score: 0.7674 time: 0.15s
Test loss: 0.5601 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0118;  Loss pred: 0.0118; Loss self: 0.0000; time: 0.14s
Val loss: 0.5160 score: 0.7674 time: 0.06s
Test loss: 0.5613 score: 0.7273 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.14s
Val loss: 0.4891 score: 0.7907 time: 0.05s
Test loss: 0.5679 score: 0.6818 time: 0.05s
Epoch 87/1000, LR 0.000266
Train loss: 0.0113;  Loss pred: 0.0113; Loss self: 0.0000; time: 0.16s
Val loss: 0.4701 score: 0.7907 time: 0.06s
Test loss: 0.5765 score: 0.6818 time: 0.05s
Epoch 88/1000, LR 0.000266
Train loss: 0.0107;  Loss pred: 0.0107; Loss self: 0.0000; time: 0.15s
Val loss: 0.4663 score: 0.7907 time: 0.06s
Test loss: 0.5868 score: 0.6818 time: 0.06s
Epoch 89/1000, LR 0.000266
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.14s
Val loss: 0.4676 score: 0.7907 time: 0.05s
Test loss: 0.5998 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0091;  Loss pred: 0.0091; Loss self: 0.0000; time: 0.16s
Val loss: 0.4748 score: 0.7907 time: 0.15s
Test loss: 0.6172 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0092;  Loss pred: 0.0092; Loss self: 0.0000; time: 0.21s
Val loss: 0.4851 score: 0.7907 time: 0.06s
Test loss: 0.6279 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0097;  Loss pred: 0.0097; Loss self: 0.0000; time: 0.26s
Val loss: 0.4937 score: 0.7907 time: 0.06s
Test loss: 0.6354 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0078;  Loss pred: 0.0078; Loss self: 0.0000; time: 0.14s
Val loss: 0.4980 score: 0.7907 time: 0.06s
Test loss: 0.6462 score: 0.7045 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.15s
Val loss: 0.5015 score: 0.7907 time: 0.06s
Test loss: 0.6524 score: 0.7045 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.14s
Val loss: 0.5063 score: 0.7907 time: 0.05s
Test loss: 0.6615 score: 0.7045 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0076;  Loss pred: 0.0076; Loss self: 0.0000; time: 0.14s
Val loss: 0.5242 score: 0.7907 time: 0.06s
Test loss: 0.6652 score: 0.6818 time: 0.13s
     INFO: Early stopping counter 8 of 20
Epoch 97/1000, LR 0.000265
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.14s
Val loss: 0.5373 score: 0.7907 time: 0.07s
Test loss: 0.6668 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 98/1000, LR 0.000265
Train loss: 0.0065;  Loss pred: 0.0065; Loss self: 0.0000; time: 0.15s
Val loss: 0.5527 score: 0.7907 time: 0.06s
Test loss: 0.6669 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 99/1000, LR 0.000265
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.14s
Val loss: 0.5734 score: 0.7907 time: 0.06s
Test loss: 0.6716 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 100/1000, LR 0.000265
Train loss: 0.0057;  Loss pred: 0.0057; Loss self: 0.0000; time: 0.14s
Val loss: 0.5951 score: 0.7907 time: 0.05s
Test loss: 0.6778 score: 0.6818 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 101/1000, LR 0.000265
Train loss: 0.0050;  Loss pred: 0.0050; Loss self: 0.0000; time: 0.15s
Val loss: 0.6122 score: 0.7907 time: 0.05s
Test loss: 0.6842 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 102/1000, LR 0.000264
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.13s
Val loss: 0.6234 score: 0.7907 time: 0.06s
Test loss: 0.6914 score: 0.6818 time: 0.07s
     INFO: Early stopping counter 14 of 20
Epoch 103/1000, LR 0.000264
Train loss: 0.0046;  Loss pred: 0.0046; Loss self: 0.0000; time: 0.15s
Val loss: 0.6287 score: 0.7907 time: 0.11s
Test loss: 0.6971 score: 0.6818 time: 0.15s
     INFO: Early stopping counter 15 of 20
Epoch 104/1000, LR 0.000264
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.14s
Val loss: 0.6291 score: 0.7907 time: 0.14s
Test loss: 0.7030 score: 0.6818 time: 0.12s
     INFO: Early stopping counter 16 of 20
Epoch 105/1000, LR 0.000264
Train loss: 0.0045;  Loss pred: 0.0045; Loss self: 0.0000; time: 0.14s
Val loss: 0.6219 score: 0.7907 time: 0.07s
Test loss: 0.7087 score: 0.6818 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 106/1000, LR 0.000264
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.15s
Val loss: 0.6125 score: 0.7907 time: 0.05s
Test loss: 0.7183 score: 0.6818 time: 0.09s
     INFO: Early stopping counter 18 of 20
Epoch 107/1000, LR 0.000264
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.29s
Val loss: 0.6071 score: 0.7907 time: 0.05s
Test loss: 0.7321 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 108/1000, LR 0.000264
Train loss: 0.0036;  Loss pred: 0.0036; Loss self: 0.0000; time: 0.15s
Val loss: 0.6057 score: 0.7674 time: 0.19s
Test loss: 0.7482 score: 0.6818 time: 0.14s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 087,   Train_Loss: 0.0107,   Val_Loss: 0.4663,   Val_Precision: 0.7826,   Val_Recall: 0.8182,   Val_accuracy: 0.8000,   Val_Score: 0.7907,   Val_Loss: 0.4663,   Test_Precision: 0.6667,   Test_Recall: 0.7273,   Test_accuracy: 0.6957,   Test_Score: 0.6818,   Test_loss: 0.5868


[0.07856516598258168, 0.22984841105062515, 0.05368598096538335, 0.10103782603982836, 0.04899851290974766, 0.06356270692776889, 0.09287575795315206, 0.05298591998871416, 0.08793567004613578, 0.0715876059839502, 0.09063413797412068, 0.058199896942824125, 0.10292995593044907, 0.25159456802066416, 0.062214098987169564, 0.07040424004662782, 0.11551012494601309, 0.290749154984951, 0.08965806802734733, 0.25348921096883714, 0.06599524198099971, 0.09853447391651571, 0.055188406025990844, 0.33381057099904865, 0.08695056196302176, 0.1763326080981642, 0.3419660080689937, 0.08674130495637655, 0.23746134305838495, 0.15278216695878655, 0.08538491895888001, 0.30309886997565627, 0.07480480999220163, 0.0865406000521034, 0.25746188103221357, 0.05153274803888053, 0.09671454201452434, 0.09369500598404557, 0.2226523238932714, 0.0763115129666403, 0.08063876698724926, 0.057032065000385046, 0.10124755499418825, 0.2020421460038051, 0.05572603899054229, 0.08145165897440165, 0.05610707099549472, 0.23850354098249227, 0.05882806796580553, 0.06560157297644764, 0.14972390397451818, 0.06263132090680301, 0.1120987239992246, 0.04882568004541099, 0.05291660106740892, 0.11100260901730508, 0.04876806796528399, 0.05910450709052384, 0.10626579006202519, 0.052192916977219284, 0.06394858798012137, 0.12508494302164763, 0.053125990903936327, 0.08917159598786384, 0.21428580593783408, 0.058731961995363235, 0.1401435409206897, 0.2241834499873221, 0.061367291025817394, 0.1154532179934904, 0.08771693310700357, 0.1384170079836622, 0.05809714808128774, 0.05643188499379903, 0.05140331306029111, 0.056781591032631695, 0.0605594739317894, 0.0874942879891023, 0.055020050960592926, 0.056304297992028296, 0.06009591603651643, 0.06735498004127294, 0.056217588018625975, 0.056302704964764416, 0.05569308705162257, 0.059427063912153244, 0.05852280801627785, 0.0641747280023992, 0.061033198959194124, 0.060505144065245986, 0.055319509003311396, 0.05466314300429076, 0.057015468017198145, 0.07573073299136013, 0.05653421103488654, 0.13825018308125436, 0.06628958892542869, 0.057971312082372606, 0.06370394304394722, 0.06025497103109956, 0.05881310999393463, 0.07905955100432038, 0.15553215495310724, 0.12191667396109551, 0.0924320180201903, 0.09501464595086873, 0.05123734101653099, 0.1484126199502498]
[0.0017855719541495835, 0.005223827523877844, 0.0012201359310314399, 0.0022963142281779174, 0.0011136025661306287, 0.001444606975631111, 0.002110812680753456, 0.0012042254542889582, 0.001998537955593995, 0.0016269910450897771, 0.002059866772139106, 0.0013227249305187302, 0.0023393171802374786, 0.005718058364106004, 0.0014139567951629447, 0.0016000963646960868, 0.0026252301124093883, 0.006607935340567069, 0.0020376833642578936, 0.005761118431109935, 0.0014998918632045388, 0.0022394198617389934, 0.0012542819551361556, 0.0075866038863420145, 0.0019761491355232215, 0.0040075592749582774, 0.007771954728840766, 0.0019713932944631033, 0.005396848705872385, 0.003472321976336058, 0.0019405663399745456, 0.006888610681264915, 0.0017001093180045825, 0.0019668318193659866, 0.0058514063870957625, 0.0011711988190654665, 0.0021980577730573714, 0.0021294319541828536, 0.0050602800884834405, 0.001734352567423643, 0.0018326992497102103, 0.0012961832954632964, 0.0023010807953224603, 0.004591866954631934, 0.0012665008861486885, 0.0018511740676000375, 0.0012751607044430618, 0.00542053502232937, 0.0013370015446773984, 0.00149094484037381, 0.0034028159994208677, 0.0014234391115182502, 0.00254769827270965, 0.0011096745464866135, 0.0012026500242592936, 0.0025227865685751153, 0.0011083651810291815, 0.00134328425205736, 0.0024151315923187544, 0.0011862026585731655, 0.001453376999548213, 0.002842839614128355, 0.0012074088841803712, 0.00202662718154236, 0.004870131953132593, 0.0013348173180764372, 0.0031850804754702203, 0.005095078408802775, 0.001394711159677668, 0.0026239367725793272, 0.0019935666615228083, 0.003145841090537777, 0.0013203897291201758, 0.0012825428407681598, 0.0011682571150066162, 0.001290490705287084, 0.0013763516802679408, 0.0019885065452068707, 0.0012504557036498393, 0.0012796431361824614, 0.0013658162735571916, 0.0015307950009380213, 0.0012776724549687722, 0.0012796069310173732, 0.0012657519784459675, 0.0013506150889125738, 0.0013300638185517694, 0.0014585165455090728, 0.0013871181581635028, 0.0013751169105737724, 0.0012572615682570772, 0.0012423441591884264, 0.0012958060912999579, 0.001721153022530912, 0.0012848684326110576, 0.003142049615483054, 0.0015065815664870156, 0.0013175298200539228, 0.0014478168873624368, 0.0013694311597977173, 0.0013366615907712417, 0.0017968079773709178, 0.00353482170347971, 0.002770833499115807, 0.0021007276822770523, 0.0021594237716106527, 0.001164485023102977, 0.0033730140897784045]
[560.0446387366513, 191.43051630802356, 819.5808143725853, 435.48047028105617, 897.9864364668654, 692.2298015092487, 473.75118082152574, 830.4092862664622, 500.3657784937015, 614.6315328642881, 485.4682902436121, 756.0150844120192, 427.4751660219431, 174.88453882830316, 707.2351881054186, 624.9623598075822, 380.91898888140446, 151.33319992719294, 490.75338079534805, 173.57740722704435, 666.7147309296597, 446.5442220484115, 797.2689042564177, 131.81128407142444, 506.03468231421294, 249.52843648467584, 128.66775925612683, 507.25545369796123, 185.29331735979298, 287.99172623248063, 515.313483183016, 145.16715289480925, 588.1974702507364, 508.4318802216413, 170.899085424202, 853.8259975346705, 454.9470956848681, 469.6088071918405, 197.61751968549603, 576.5840341710257, 545.6432637041357, 771.4958243174772, 434.57839552298975, 217.77634454135793, 789.5770235431158, 540.1977142519364, 784.2148809288781, 184.48363415799304, 747.9422921992864, 670.7156246969404, 293.874250082929, 702.5239027845686, 392.51115829208146, 901.1651237438414, 831.4970937749702, 396.3870794527045, 902.2297137405939, 744.4440731501248, 414.05611320744043, 843.0262677061093, 688.0527215656043, 351.76096288731736, 828.2198459048386, 493.4306660384138, 205.333245510273, 749.1661866067697, 313.96380961217875, 196.2678333413473, 716.9943346772318, 381.1067440535166, 501.6135247948713, 317.88001085237636, 757.3521498583124, 779.7010502986902, 855.9759552539397, 774.8990332925637, 726.5584910720821, 502.88997157711987, 799.708455950253, 781.4678731316325, 732.1628972801417, 653.255334246083, 782.6732087015534, 781.4899839632262, 790.0441927238813, 740.4033970960106, 751.8436228788202, 685.6281494228543, 720.9191186163737, 727.2108955323271, 795.379438334606, 804.9299323411799, 771.7204037810905, 581.0058646206397, 778.2898035465315, 318.26359299748407, 663.754304608783, 758.9961037535171, 690.6950794183316, 730.2302075174871, 748.1325167898399, 556.5424979152174, 282.89970015053126, 360.9022340458592, 476.0255260291829, 463.0865016615652, 858.7487002068283, 296.47074497269494]
Elapsed: 0.1017808915704437~0.0675474237955588
Time per graph: 0.002313202081146448~0.001535168722626336
Speed: 563.9687379305111~223.65685699330083
Total Time: 0.1490
best val loss: 0.46628424525260925 test_score: 0.6818

Testing...
Test loss: 0.5679 score: 0.6818 time: 0.05s
test Score 0.6818
Epoch Time List: [0.5015501460293308, 0.4905716220382601, 0.5272030089981854, 0.36476052389480174, 0.3934759619878605, 0.3546397000318393, 0.4282281700288877, 0.3921746331034228, 0.44455980614293367, 0.2806669590063393, 0.3641173259820789, 0.35842599091120064, 0.37148971389979124, 0.48238724702969193, 0.96832497313153, 0.3902712940471247, 0.39926322910469025, 0.8439590280177072, 0.3145792519208044, 0.5255473280558363, 0.38930032297503203, 0.37845590291544795, 0.6258267950033769, 0.5534375469433144, 0.3074622710701078, 0.6276429401477799, 0.6546293209539726, 0.3078678820747882, 0.521954876021482, 0.5180077441036701, 0.28783837088849396, 0.5714149920968339, 0.3989961630431935, 0.3078362609958276, 0.46783072815742344, 0.3584914219100028, 0.3282528199488297, 0.3013641070574522, 0.4689446260454133, 0.3613846091320738, 0.46496967202983797, 0.531445375061594, 0.33753582707140595, 0.41232844605110586, 0.3509979099035263, 0.2985185629222542, 0.34670187905430794, 0.6814234550110996, 0.26461407192982733, 0.3016084110131487, 0.36404126905836165, 0.35991819004993886, 0.3542097869794816, 0.42089505889452994, 0.3353788099484518, 0.43488420790527016, 0.39854862401261926, 0.3559423309052363, 0.32311189209576696, 0.41247220302466303, 0.3529194399015978, 0.509855404146947, 0.40255576313938946, 0.46639947197400033, 0.4571771890623495, 0.36135739798191935, 0.373717593960464, 0.4446426120121032, 0.3727087799925357, 0.39472979912534356, 0.3271973947994411, 0.46433222701307386, 0.2587679949356243, 0.33832550107035786, 0.2538047609850764, 0.24664129910524935, 0.2497895749984309, 0.31044440891128033, 0.3279552039457485, 0.2525615659542382, 0.25084751890972257, 0.25890579796396196, 0.262121393927373, 0.3519219938898459, 0.24547295307274908, 0.24790406308602542, 0.266067449003458, 0.26903724600560963, 0.24692482501268387, 0.36214305891189724, 0.32353165303356946, 0.364098938065581, 0.25519125908613205, 0.28257878101430833, 0.23996547993738204, 0.3282199710374698, 0.27216782909817994, 0.25987914588768035, 0.25685594184324145, 0.2527221179334447, 0.2587113860063255, 0.26890606503002346, 0.41109480103477836, 0.39687668695114553, 0.3028386060614139, 0.2938752858899534, 0.3822735839057714, 0.48400732199661434]
Total Epoch List: [108]
Total Time List: [0.14896774396765977]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdd15c00>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.8190;  Loss pred: 0.8190; Loss self: 0.0000; time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1061 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0117 score: 0.5116 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.7716;  Loss pred: 0.7716; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9561 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8830 score: 0.5116 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.7825;  Loss pred: 0.7825; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8735 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8162 score: 0.5116 time: 0.12s
Epoch 4/1000, LR 0.000060
Train loss: 0.7989;  Loss pred: 0.7989; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8202 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7797 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.8255;  Loss pred: 0.8255; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7860 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7560 score: 0.5116 time: 0.11s
Epoch 6/1000, LR 0.000120
Train loss: 0.7565;  Loss pred: 0.7565; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7669 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7432 score: 0.5116 time: 0.16s
Epoch 7/1000, LR 0.000150
Train loss: 0.7897;  Loss pred: 0.7897; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7534 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7347 score: 0.5116 time: 0.06s
Epoch 8/1000, LR 0.000180
Train loss: 0.7293;  Loss pred: 0.7293; Loss self: 0.0000; time: 0.36s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7436 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7284 score: 0.5116 time: 0.11s
Epoch 9/1000, LR 0.000210
Train loss: 0.7322;  Loss pred: 0.7322; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7380 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7255 score: 0.5116 time: 0.17s
Epoch 10/1000, LR 0.000240
Train loss: 0.7149;  Loss pred: 0.7149; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7340 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7223 score: 0.5116 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.7213;  Loss pred: 0.7213; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7299 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7189 score: 0.5116 time: 0.08s
Epoch 12/1000, LR 0.000270
Train loss: 0.7000;  Loss pred: 0.7000; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7315 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7176 score: 0.5116 time: 0.22s
     INFO: Early stopping counter 1 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7333 score: 0.5000 time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7161 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.6349;  Loss pred: 0.6349; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7348 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7130 score: 0.5116 time: 0.14s
     INFO: Early stopping counter 3 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.6549;  Loss pred: 0.6549; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7353 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7075 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.6422;  Loss pred: 0.6422; Loss self: 0.0000; time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7372 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7036 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.6092;  Loss pred: 0.6092; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7394 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7006 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.6259;  Loss pred: 0.6259; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7421 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7006 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.5827;  Loss pred: 0.5827; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7452 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7012 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.5767;  Loss pred: 0.5767; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7480 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7014 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 9 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.5728;  Loss pred: 0.5728; Loss self: 0.0000; time: 0.34s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7505 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7019 score: 0.5116 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7524 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7030 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 11 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.5128;  Loss pred: 0.5128; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7539 score: 0.5000 time: 0.13s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7042 score: 0.5116 time: 0.06s
     INFO: Early stopping counter 12 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.5383;  Loss pred: 0.5383; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7554 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7061 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.5088;  Loss pred: 0.5088; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7585 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7086 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 14 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.4942;  Loss pred: 0.4942; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7626 score: 0.5000 time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7114 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.4908;  Loss pred: 0.4908; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7663 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7147 score: 0.5116 time: 0.12s
     INFO: Early stopping counter 16 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.4743;  Loss pred: 0.4743; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7703 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7200 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 17 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.4797;  Loss pred: 0.4797; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7781 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7277 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.4537;  Loss pred: 0.4537; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7830 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7345 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.4499;  Loss pred: 0.4499; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7859 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7404 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 010,   Train_Loss: 0.7213,   Val_Loss: 0.7299,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.7299,   Test_Precision: 0.5116,   Test_Recall: 1.0000,   Test_accuracy: 0.6769,   Test_Score: 0.5116,   Test_loss: 0.7189


[0.07856516598258168, 0.22984841105062515, 0.05368598096538335, 0.10103782603982836, 0.04899851290974766, 0.06356270692776889, 0.09287575795315206, 0.05298591998871416, 0.08793567004613578, 0.0715876059839502, 0.09063413797412068, 0.058199896942824125, 0.10292995593044907, 0.25159456802066416, 0.062214098987169564, 0.07040424004662782, 0.11551012494601309, 0.290749154984951, 0.08965806802734733, 0.25348921096883714, 0.06599524198099971, 0.09853447391651571, 0.055188406025990844, 0.33381057099904865, 0.08695056196302176, 0.1763326080981642, 0.3419660080689937, 0.08674130495637655, 0.23746134305838495, 0.15278216695878655, 0.08538491895888001, 0.30309886997565627, 0.07480480999220163, 0.0865406000521034, 0.25746188103221357, 0.05153274803888053, 0.09671454201452434, 0.09369500598404557, 0.2226523238932714, 0.0763115129666403, 0.08063876698724926, 0.057032065000385046, 0.10124755499418825, 0.2020421460038051, 0.05572603899054229, 0.08145165897440165, 0.05610707099549472, 0.23850354098249227, 0.05882806796580553, 0.06560157297644764, 0.14972390397451818, 0.06263132090680301, 0.1120987239992246, 0.04882568004541099, 0.05291660106740892, 0.11100260901730508, 0.04876806796528399, 0.05910450709052384, 0.10626579006202519, 0.052192916977219284, 0.06394858798012137, 0.12508494302164763, 0.053125990903936327, 0.08917159598786384, 0.21428580593783408, 0.058731961995363235, 0.1401435409206897, 0.2241834499873221, 0.061367291025817394, 0.1154532179934904, 0.08771693310700357, 0.1384170079836622, 0.05809714808128774, 0.05643188499379903, 0.05140331306029111, 0.056781591032631695, 0.0605594739317894, 0.0874942879891023, 0.055020050960592926, 0.056304297992028296, 0.06009591603651643, 0.06735498004127294, 0.056217588018625975, 0.056302704964764416, 0.05569308705162257, 0.059427063912153244, 0.05852280801627785, 0.0641747280023992, 0.061033198959194124, 0.060505144065245986, 0.055319509003311396, 0.05466314300429076, 0.057015468017198145, 0.07573073299136013, 0.05653421103488654, 0.13825018308125436, 0.06628958892542869, 0.057971312082372606, 0.06370394304394722, 0.06025497103109956, 0.05881310999393463, 0.07905955100432038, 0.15553215495310724, 0.12191667396109551, 0.0924320180201903, 0.09501464595086873, 0.05123734101653099, 0.1484126199502498, 0.09506955300457776, 0.06196803797502071, 0.12859928398393095, 0.05685212498065084, 0.11267908103764057, 0.16926929797045887, 0.06408685899805278, 0.12101535592228174, 0.17588867107406259, 0.055009072995744646, 0.08340493799187243, 0.22148355597164482, 0.05331256706267595, 0.1473112270468846, 0.06271044397726655, 0.05749147303868085, 0.05674606398679316, 0.06189394008833915, 0.05109709792304784, 0.08707237592898309, 0.07913508405908942, 0.12930502404924482, 0.06131662707775831, 0.0811653450364247, 0.12784295505844057, 0.05459740199148655, 0.1284959240583703, 0.09276735899038613, 0.05135038401931524, 0.08842673199251294, 0.09241203195415437]
[0.0017855719541495835, 0.005223827523877844, 0.0012201359310314399, 0.0022963142281779174, 0.0011136025661306287, 0.001444606975631111, 0.002110812680753456, 0.0012042254542889582, 0.001998537955593995, 0.0016269910450897771, 0.002059866772139106, 0.0013227249305187302, 0.0023393171802374786, 0.005718058364106004, 0.0014139567951629447, 0.0016000963646960868, 0.0026252301124093883, 0.006607935340567069, 0.0020376833642578936, 0.005761118431109935, 0.0014998918632045388, 0.0022394198617389934, 0.0012542819551361556, 0.0075866038863420145, 0.0019761491355232215, 0.0040075592749582774, 0.007771954728840766, 0.0019713932944631033, 0.005396848705872385, 0.003472321976336058, 0.0019405663399745456, 0.006888610681264915, 0.0017001093180045825, 0.0019668318193659866, 0.0058514063870957625, 0.0011711988190654665, 0.0021980577730573714, 0.0021294319541828536, 0.0050602800884834405, 0.001734352567423643, 0.0018326992497102103, 0.0012961832954632964, 0.0023010807953224603, 0.004591866954631934, 0.0012665008861486885, 0.0018511740676000375, 0.0012751607044430618, 0.00542053502232937, 0.0013370015446773984, 0.00149094484037381, 0.0034028159994208677, 0.0014234391115182502, 0.00254769827270965, 0.0011096745464866135, 0.0012026500242592936, 0.0025227865685751153, 0.0011083651810291815, 0.00134328425205736, 0.0024151315923187544, 0.0011862026585731655, 0.001453376999548213, 0.002842839614128355, 0.0012074088841803712, 0.00202662718154236, 0.004870131953132593, 0.0013348173180764372, 0.0031850804754702203, 0.005095078408802775, 0.001394711159677668, 0.0026239367725793272, 0.0019935666615228083, 0.003145841090537777, 0.0013203897291201758, 0.0012825428407681598, 0.0011682571150066162, 0.001290490705287084, 0.0013763516802679408, 0.0019885065452068707, 0.0012504557036498393, 0.0012796431361824614, 0.0013658162735571916, 0.0015307950009380213, 0.0012776724549687722, 0.0012796069310173732, 0.0012657519784459675, 0.0013506150889125738, 0.0013300638185517694, 0.0014585165455090728, 0.0013871181581635028, 0.0013751169105737724, 0.0012572615682570772, 0.0012423441591884264, 0.0012958060912999579, 0.001721153022530912, 0.0012848684326110576, 0.003142049615483054, 0.0015065815664870156, 0.0013175298200539228, 0.0014478168873624368, 0.0013694311597977173, 0.0013366615907712417, 0.0017968079773709178, 0.00353482170347971, 0.002770833499115807, 0.0021007276822770523, 0.0021594237716106527, 0.001164485023102977, 0.0033730140897784045, 0.002210919837315762, 0.0014411171622097838, 0.002990681022882115, 0.0013221424414104847, 0.0026204437450614085, 0.003936495301638579, 0.0014903920697221576, 0.0028143106028437614, 0.004090434211024711, 0.0012792807673428987, 0.0019396497207412192, 0.005150780371433601, 0.001239827140992464, 0.0034258424894624325, 0.0014583824180759662, 0.0013370110008995547, 0.0013196759066696083, 0.001439393955542771, 0.0011883046028615778, 0.00202493897509263, 0.001840350792071847, 0.003007093582540577, 0.0014259680715757747, 0.0018875661636377837, 0.002973091978103269, 0.0012697070230578268, 0.0029882773036830302, 0.0021573804416368868, 0.0011941949771933776, 0.0020564356277328593, 0.0021491170221896367]
[560.0446387366513, 191.43051630802356, 819.5808143725853, 435.48047028105617, 897.9864364668654, 692.2298015092487, 473.75118082152574, 830.4092862664622, 500.3657784937015, 614.6315328642881, 485.4682902436121, 756.0150844120192, 427.4751660219431, 174.88453882830316, 707.2351881054186, 624.9623598075822, 380.91898888140446, 151.33319992719294, 490.75338079534805, 173.57740722704435, 666.7147309296597, 446.5442220484115, 797.2689042564177, 131.81128407142444, 506.03468231421294, 249.52843648467584, 128.66775925612683, 507.25545369796123, 185.29331735979298, 287.99172623248063, 515.313483183016, 145.16715289480925, 588.1974702507364, 508.4318802216413, 170.899085424202, 853.8259975346705, 454.9470956848681, 469.6088071918405, 197.61751968549603, 576.5840341710257, 545.6432637041357, 771.4958243174772, 434.57839552298975, 217.77634454135793, 789.5770235431158, 540.1977142519364, 784.2148809288781, 184.48363415799304, 747.9422921992864, 670.7156246969404, 293.874250082929, 702.5239027845686, 392.51115829208146, 901.1651237438414, 831.4970937749702, 396.3870794527045, 902.2297137405939, 744.4440731501248, 414.05611320744043, 843.0262677061093, 688.0527215656043, 351.76096288731736, 828.2198459048386, 493.4306660384138, 205.333245510273, 749.1661866067697, 313.96380961217875, 196.2678333413473, 716.9943346772318, 381.1067440535166, 501.6135247948713, 317.88001085237636, 757.3521498583124, 779.7010502986902, 855.9759552539397, 774.8990332925637, 726.5584910720821, 502.88997157711987, 799.708455950253, 781.4678731316325, 732.1628972801417, 653.255334246083, 782.6732087015534, 781.4899839632262, 790.0441927238813, 740.4033970960106, 751.8436228788202, 685.6281494228543, 720.9191186163737, 727.2108955323271, 795.379438334606, 804.9299323411799, 771.7204037810905, 581.0058646206397, 778.2898035465315, 318.26359299748407, 663.754304608783, 758.9961037535171, 690.6950794183316, 730.2302075174871, 748.1325167898399, 556.5424979152174, 282.89970015053126, 360.9022340458592, 476.0255260291829, 463.0865016615652, 858.7487002068283, 296.47074497269494, 452.30043311478994, 693.9061071666217, 334.372001677498, 756.3481578680603, 381.61475585371346, 254.03307342542666, 670.9643860265725, 355.32680685263927, 244.47282327747936, 781.6892315805134, 515.5570046007375, 194.1453387424619, 806.5640498880466, 291.89900092485436, 685.6912066447517, 747.9370022589117, 757.7618072331437, 694.7368343108799, 841.5350723980047, 493.8420427974899, 543.3746676492099, 332.5470167626571, 701.2779738433721, 529.782753719618, 336.3501726031247, 787.5832627842814, 334.64096480186333, 463.52510697708095, 837.384195292984, 486.27828973302775, 465.30737492421235]
Elapsed: 0.10001519553132168~0.062884419294802
Time per graph: 0.002284136917197574~0.0014316444847050328
Speed: 558.8587957714332~218.52886885186913
Total Time: 0.0931
best val loss: 0.7299020290374756 test_score: 0.5116

Testing...
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0117 score: 0.5116 time: 0.10s
test Score 0.5116
Epoch Time List: [0.5015501460293308, 0.4905716220382601, 0.5272030089981854, 0.36476052389480174, 0.3934759619878605, 0.3546397000318393, 0.4282281700288877, 0.3921746331034228, 0.44455980614293367, 0.2806669590063393, 0.3641173259820789, 0.35842599091120064, 0.37148971389979124, 0.48238724702969193, 0.96832497313153, 0.3902712940471247, 0.39926322910469025, 0.8439590280177072, 0.3145792519208044, 0.5255473280558363, 0.38930032297503203, 0.37845590291544795, 0.6258267950033769, 0.5534375469433144, 0.3074622710701078, 0.6276429401477799, 0.6546293209539726, 0.3078678820747882, 0.521954876021482, 0.5180077441036701, 0.28783837088849396, 0.5714149920968339, 0.3989961630431935, 0.3078362609958276, 0.46783072815742344, 0.3584914219100028, 0.3282528199488297, 0.3013641070574522, 0.4689446260454133, 0.3613846091320738, 0.46496967202983797, 0.531445375061594, 0.33753582707140595, 0.41232844605110586, 0.3509979099035263, 0.2985185629222542, 0.34670187905430794, 0.6814234550110996, 0.26461407192982733, 0.3016084110131487, 0.36404126905836165, 0.35991819004993886, 0.3542097869794816, 0.42089505889452994, 0.3353788099484518, 0.43488420790527016, 0.39854862401261926, 0.3559423309052363, 0.32311189209576696, 0.41247220302466303, 0.3529194399015978, 0.509855404146947, 0.40255576313938946, 0.46639947197400033, 0.4571771890623495, 0.36135739798191935, 0.373717593960464, 0.4446426120121032, 0.3727087799925357, 0.39472979912534356, 0.3271973947994411, 0.46433222701307386, 0.2587679949356243, 0.33832550107035786, 0.2538047609850764, 0.24664129910524935, 0.2497895749984309, 0.31044440891128033, 0.3279552039457485, 0.2525615659542382, 0.25084751890972257, 0.25890579796396196, 0.262121393927373, 0.3519219938898459, 0.24547295307274908, 0.24790406308602542, 0.266067449003458, 0.26903724600560963, 0.24692482501268387, 0.36214305891189724, 0.32353165303356946, 0.364098938065581, 0.25519125908613205, 0.28257878101430833, 0.23996547993738204, 0.3282199710374698, 0.27216782909817994, 0.25987914588768035, 0.25685594184324145, 0.2527221179334447, 0.2587113860063255, 0.26890606503002346, 0.41109480103477836, 0.39687668695114553, 0.3028386060614139, 0.2938752858899534, 0.3822735839057714, 0.48400732199661434, 0.4031816410133615, 0.31022944196593016, 0.3636285449611023, 0.3347554550273344, 0.34269084804691374, 0.4210484790382907, 0.35521472396794707, 0.5360760150942951, 0.43553370411973447, 0.348870117100887, 0.37661213404498994, 0.4617791709024459, 0.33327067201025784, 0.4091860209591687, 0.2567484830506146, 0.5145021630451083, 0.28828738804440945, 0.3081699140602723, 0.3407641659723595, 0.40774372802115977, 0.4595158831216395, 0.39778589888010174, 0.341138610849157, 0.3055993299931288, 0.33915374404750764, 0.5214929840294644, 0.3771810280159116, 0.3307950870366767, 0.387328915996477, 0.2842293670400977, 0.2915751190157607]
Total Epoch List: [108, 31]
Total Time List: [0.14896774396765977, 0.09307875798549503]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdcbb310>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7153;  Loss pred: 0.7153; Loss self: 0.0000; time: 0.16s
Val loss: 0.6766 score: 0.5682 time: 0.05s
Test loss: 0.7054 score: 0.5349 time: 0.16s
Epoch 2/1000, LR 0.000000
Train loss: 0.6664;  Loss pred: 0.6664; Loss self: 0.0000; time: 0.16s
Val loss: 0.6845 score: 0.5227 time: 0.14s
Test loss: 0.7041 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7037;  Loss pred: 0.7037; Loss self: 0.0000; time: 0.20s
Val loss: 0.6924 score: 0.5000 time: 0.05s
Test loss: 0.7037 score: 0.4651 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6680;  Loss pred: 0.6680; Loss self: 0.0000; time: 0.20s
Val loss: 0.6990 score: 0.5227 time: 0.12s
Test loss: 0.7034 score: 0.4651 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.6969;  Loss pred: 0.6969; Loss self: 0.0000; time: 0.17s
Val loss: 0.7030 score: 0.5000 time: 0.13s
Test loss: 0.7062 score: 0.4651 time: 0.04s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6461;  Loss pred: 0.6461; Loss self: 0.0000; time: 0.22s
Val loss: 0.7060 score: 0.4773 time: 0.05s
Test loss: 0.7073 score: 0.4651 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6306;  Loss pred: 0.6306; Loss self: 0.0000; time: 0.21s
Val loss: 0.7071 score: 0.4773 time: 0.06s
Test loss: 0.7080 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6694;  Loss pred: 0.6694; Loss self: 0.0000; time: 0.16s
Val loss: 0.7087 score: 0.4091 time: 0.05s
Test loss: 0.7082 score: 0.3953 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6070;  Loss pred: 0.6070; Loss self: 0.0000; time: 0.26s
Val loss: 0.7078 score: 0.3409 time: 0.05s
Test loss: 0.7072 score: 0.3721 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5928;  Loss pred: 0.5928; Loss self: 0.0000; time: 0.22s
Val loss: 0.7069 score: 0.3409 time: 0.05s
Test loss: 0.7044 score: 0.3953 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.22s
Val loss: 0.7054 score: 0.3636 time: 0.05s
Test loss: 0.7006 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.5583;  Loss pred: 0.5583; Loss self: 0.0000; time: 0.32s
Val loss: 0.7028 score: 0.3636 time: 0.06s
Test loss: 0.6974 score: 0.4419 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.5498;  Loss pred: 0.5498; Loss self: 0.0000; time: 0.17s
Val loss: 0.6997 score: 0.4091 time: 0.13s
Test loss: 0.6938 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4919;  Loss pred: 0.4919; Loss self: 0.0000; time: 0.26s
Val loss: 0.6965 score: 0.4318 time: 0.07s
Test loss: 0.6898 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5020;  Loss pred: 0.5020; Loss self: 0.0000; time: 0.17s
Val loss: 0.6942 score: 0.4545 time: 0.16s
Test loss: 0.6859 score: 0.5349 time: 0.05s
     INFO: Early stopping counter 14 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4994;  Loss pred: 0.4994; Loss self: 0.0000; time: 0.30s
Val loss: 0.6931 score: 0.4091 time: 0.05s
Test loss: 0.6833 score: 0.5581 time: 0.08s
     INFO: Early stopping counter 15 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4703;  Loss pred: 0.4703; Loss self: 0.0000; time: 0.28s
Val loss: 0.6931 score: 0.4318 time: 0.08s
Test loss: 0.6805 score: 0.5814 time: 0.18s
     INFO: Early stopping counter 16 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4384;  Loss pred: 0.4384; Loss self: 0.0000; time: 0.16s
Val loss: 0.6927 score: 0.4318 time: 0.19s
Test loss: 0.6779 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4388;  Loss pred: 0.4388; Loss self: 0.0000; time: 0.41s
Val loss: 0.6920 score: 0.4318 time: 0.07s
Test loss: 0.6758 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4269;  Loss pred: 0.4269; Loss self: 0.0000; time: 0.27s
Val loss: 0.6915 score: 0.4318 time: 0.09s
Test loss: 0.6742 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3873;  Loss pred: 0.3873; Loss self: 0.0000; time: 0.31s
Val loss: 0.6904 score: 0.4545 time: 0.10s
Test loss: 0.6722 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 000,   Train_Loss: 0.7153,   Val_Loss: 0.6766,   Val_Precision: 0.5652,   Val_Recall: 0.5909,   Val_accuracy: 0.5778,   Val_Score: 0.5682,   Val_Loss: 0.6766,   Test_Precision: 0.5217,   Test_Recall: 0.5714,   Test_accuracy: 0.5455,   Test_Score: 0.5349,   Test_loss: 0.7054


[0.07856516598258168, 0.22984841105062515, 0.05368598096538335, 0.10103782603982836, 0.04899851290974766, 0.06356270692776889, 0.09287575795315206, 0.05298591998871416, 0.08793567004613578, 0.0715876059839502, 0.09063413797412068, 0.058199896942824125, 0.10292995593044907, 0.25159456802066416, 0.062214098987169564, 0.07040424004662782, 0.11551012494601309, 0.290749154984951, 0.08965806802734733, 0.25348921096883714, 0.06599524198099971, 0.09853447391651571, 0.055188406025990844, 0.33381057099904865, 0.08695056196302176, 0.1763326080981642, 0.3419660080689937, 0.08674130495637655, 0.23746134305838495, 0.15278216695878655, 0.08538491895888001, 0.30309886997565627, 0.07480480999220163, 0.0865406000521034, 0.25746188103221357, 0.05153274803888053, 0.09671454201452434, 0.09369500598404557, 0.2226523238932714, 0.0763115129666403, 0.08063876698724926, 0.057032065000385046, 0.10124755499418825, 0.2020421460038051, 0.05572603899054229, 0.08145165897440165, 0.05610707099549472, 0.23850354098249227, 0.05882806796580553, 0.06560157297644764, 0.14972390397451818, 0.06263132090680301, 0.1120987239992246, 0.04882568004541099, 0.05291660106740892, 0.11100260901730508, 0.04876806796528399, 0.05910450709052384, 0.10626579006202519, 0.052192916977219284, 0.06394858798012137, 0.12508494302164763, 0.053125990903936327, 0.08917159598786384, 0.21428580593783408, 0.058731961995363235, 0.1401435409206897, 0.2241834499873221, 0.061367291025817394, 0.1154532179934904, 0.08771693310700357, 0.1384170079836622, 0.05809714808128774, 0.05643188499379903, 0.05140331306029111, 0.056781591032631695, 0.0605594739317894, 0.0874942879891023, 0.055020050960592926, 0.056304297992028296, 0.06009591603651643, 0.06735498004127294, 0.056217588018625975, 0.056302704964764416, 0.05569308705162257, 0.059427063912153244, 0.05852280801627785, 0.0641747280023992, 0.061033198959194124, 0.060505144065245986, 0.055319509003311396, 0.05466314300429076, 0.057015468017198145, 0.07573073299136013, 0.05653421103488654, 0.13825018308125436, 0.06628958892542869, 0.057971312082372606, 0.06370394304394722, 0.06025497103109956, 0.05881310999393463, 0.07905955100432038, 0.15553215495310724, 0.12191667396109551, 0.0924320180201903, 0.09501464595086873, 0.05123734101653099, 0.1484126199502498, 0.09506955300457776, 0.06196803797502071, 0.12859928398393095, 0.05685212498065084, 0.11267908103764057, 0.16926929797045887, 0.06408685899805278, 0.12101535592228174, 0.17588867107406259, 0.055009072995744646, 0.08340493799187243, 0.22148355597164482, 0.05331256706267595, 0.1473112270468846, 0.06271044397726655, 0.05749147303868085, 0.05674606398679316, 0.06189394008833915, 0.05109709792304784, 0.08707237592898309, 0.07913508405908942, 0.12930502404924482, 0.06131662707775831, 0.0811653450364247, 0.12784295505844057, 0.05459740199148655, 0.1284959240583703, 0.09276735899038613, 0.05135038401931524, 0.08842673199251294, 0.09241203195415437, 0.165623364970088, 0.0536605209344998, 0.10929513000883162, 0.08016488305293024, 0.04949747596401721, 0.10081413597799838, 0.08781063195783645, 0.05143603193573654, 0.0552590120350942, 0.058676974033005536, 0.05681496299803257, 0.11933958006557077, 0.05167428299318999, 0.06029212404973805, 0.056012585991993546, 0.0796907339245081, 0.18567260901909322, 0.05405890103429556, 0.06477306899614632, 0.051026600995101035, 0.05326567904558033]
[0.0017855719541495835, 0.005223827523877844, 0.0012201359310314399, 0.0022963142281779174, 0.0011136025661306287, 0.001444606975631111, 0.002110812680753456, 0.0012042254542889582, 0.001998537955593995, 0.0016269910450897771, 0.002059866772139106, 0.0013227249305187302, 0.0023393171802374786, 0.005718058364106004, 0.0014139567951629447, 0.0016000963646960868, 0.0026252301124093883, 0.006607935340567069, 0.0020376833642578936, 0.005761118431109935, 0.0014998918632045388, 0.0022394198617389934, 0.0012542819551361556, 0.0075866038863420145, 0.0019761491355232215, 0.0040075592749582774, 0.007771954728840766, 0.0019713932944631033, 0.005396848705872385, 0.003472321976336058, 0.0019405663399745456, 0.006888610681264915, 0.0017001093180045825, 0.0019668318193659866, 0.0058514063870957625, 0.0011711988190654665, 0.0021980577730573714, 0.0021294319541828536, 0.0050602800884834405, 0.001734352567423643, 0.0018326992497102103, 0.0012961832954632964, 0.0023010807953224603, 0.004591866954631934, 0.0012665008861486885, 0.0018511740676000375, 0.0012751607044430618, 0.00542053502232937, 0.0013370015446773984, 0.00149094484037381, 0.0034028159994208677, 0.0014234391115182502, 0.00254769827270965, 0.0011096745464866135, 0.0012026500242592936, 0.0025227865685751153, 0.0011083651810291815, 0.00134328425205736, 0.0024151315923187544, 0.0011862026585731655, 0.001453376999548213, 0.002842839614128355, 0.0012074088841803712, 0.00202662718154236, 0.004870131953132593, 0.0013348173180764372, 0.0031850804754702203, 0.005095078408802775, 0.001394711159677668, 0.0026239367725793272, 0.0019935666615228083, 0.003145841090537777, 0.0013203897291201758, 0.0012825428407681598, 0.0011682571150066162, 0.001290490705287084, 0.0013763516802679408, 0.0019885065452068707, 0.0012504557036498393, 0.0012796431361824614, 0.0013658162735571916, 0.0015307950009380213, 0.0012776724549687722, 0.0012796069310173732, 0.0012657519784459675, 0.0013506150889125738, 0.0013300638185517694, 0.0014585165455090728, 0.0013871181581635028, 0.0013751169105737724, 0.0012572615682570772, 0.0012423441591884264, 0.0012958060912999579, 0.001721153022530912, 0.0012848684326110576, 0.003142049615483054, 0.0015065815664870156, 0.0013175298200539228, 0.0014478168873624368, 0.0013694311597977173, 0.0013366615907712417, 0.0017968079773709178, 0.00353482170347971, 0.002770833499115807, 0.0021007276822770523, 0.0021594237716106527, 0.001164485023102977, 0.0033730140897784045, 0.002210919837315762, 0.0014411171622097838, 0.002990681022882115, 0.0013221424414104847, 0.0026204437450614085, 0.003936495301638579, 0.0014903920697221576, 0.0028143106028437614, 0.004090434211024711, 0.0012792807673428987, 0.0019396497207412192, 0.005150780371433601, 0.001239827140992464, 0.0034258424894624325, 0.0014583824180759662, 0.0013370110008995547, 0.0013196759066696083, 0.001439393955542771, 0.0011883046028615778, 0.00202493897509263, 0.001840350792071847, 0.003007093582540577, 0.0014259680715757747, 0.0018875661636377837, 0.002973091978103269, 0.0012697070230578268, 0.0029882773036830302, 0.0021573804416368868, 0.0011941949771933776, 0.0020564356277328593, 0.0021491170221896367, 0.00385170616209507, 0.0012479190914999954, 0.002541747209507712, 0.0018642996058820984, 0.0011511040921864469, 0.002344514790186009, 0.002042107719949685, 0.0011961867892031754, 0.0012850933031417257, 0.001364580791465245, 0.0013212782092565714, 0.0027753390712923435, 0.0012017275114695346, 0.00140214241976135, 0.001302618278883571, 0.0018532728819653046, 0.004317967651606819, 0.0012571837449836176, 0.0015063504417708448, 0.0011866651394209543, 0.0012387367219902402]
[560.0446387366513, 191.43051630802356, 819.5808143725853, 435.48047028105617, 897.9864364668654, 692.2298015092487, 473.75118082152574, 830.4092862664622, 500.3657784937015, 614.6315328642881, 485.4682902436121, 756.0150844120192, 427.4751660219431, 174.88453882830316, 707.2351881054186, 624.9623598075822, 380.91898888140446, 151.33319992719294, 490.75338079534805, 173.57740722704435, 666.7147309296597, 446.5442220484115, 797.2689042564177, 131.81128407142444, 506.03468231421294, 249.52843648467584, 128.66775925612683, 507.25545369796123, 185.29331735979298, 287.99172623248063, 515.313483183016, 145.16715289480925, 588.1974702507364, 508.4318802216413, 170.899085424202, 853.8259975346705, 454.9470956848681, 469.6088071918405, 197.61751968549603, 576.5840341710257, 545.6432637041357, 771.4958243174772, 434.57839552298975, 217.77634454135793, 789.5770235431158, 540.1977142519364, 784.2148809288781, 184.48363415799304, 747.9422921992864, 670.7156246969404, 293.874250082929, 702.5239027845686, 392.51115829208146, 901.1651237438414, 831.4970937749702, 396.3870794527045, 902.2297137405939, 744.4440731501248, 414.05611320744043, 843.0262677061093, 688.0527215656043, 351.76096288731736, 828.2198459048386, 493.4306660384138, 205.333245510273, 749.1661866067697, 313.96380961217875, 196.2678333413473, 716.9943346772318, 381.1067440535166, 501.6135247948713, 317.88001085237636, 757.3521498583124, 779.7010502986902, 855.9759552539397, 774.8990332925637, 726.5584910720821, 502.88997157711987, 799.708455950253, 781.4678731316325, 732.1628972801417, 653.255334246083, 782.6732087015534, 781.4899839632262, 790.0441927238813, 740.4033970960106, 751.8436228788202, 685.6281494228543, 720.9191186163737, 727.2108955323271, 795.379438334606, 804.9299323411799, 771.7204037810905, 581.0058646206397, 778.2898035465315, 318.26359299748407, 663.754304608783, 758.9961037535171, 690.6950794183316, 730.2302075174871, 748.1325167898399, 556.5424979152174, 282.89970015053126, 360.9022340458592, 476.0255260291829, 463.0865016615652, 858.7487002068283, 296.47074497269494, 452.30043311478994, 693.9061071666217, 334.372001677498, 756.3481578680603, 381.61475585371346, 254.03307342542666, 670.9643860265725, 355.32680685263927, 244.47282327747936, 781.6892315805134, 515.5570046007375, 194.1453387424619, 806.5640498880466, 291.89900092485436, 685.6912066447517, 747.9370022589117, 757.7618072331437, 694.7368343108799, 841.5350723980047, 493.8420427974899, 543.3746676492099, 332.5470167626571, 701.2779738433721, 529.782753719618, 336.3501726031247, 787.5832627842814, 334.64096480186333, 463.52510697708095, 837.384195292984, 486.27828973302775, 465.30737492421235, 259.62520449796386, 801.3340021891986, 393.43015554787644, 536.3944705265585, 868.7311658327662, 426.5274862781574, 489.6901325188854, 835.9898378965859, 778.153615426409, 732.8257925470508, 756.8428760833486, 360.3163340810633, 832.1353971310419, 713.1943131498751, 767.6846058517353, 539.5859453463482, 231.59043343640522, 795.4286745992177, 663.8561467970321, 842.6977137694973, 807.274041568196]
Elapsed: 0.09716857168023126~0.06061452357990436
Time per graph: 0.0022234223319873812~0.0013802460046199129
Speed: 569.4667559831527~218.04624244844047
Total Time: 0.0539
best val loss: 0.6765625476837158 test_score: 0.5349

Testing...
Test loss: 0.7054 score: 0.5349 time: 0.20s
test Score 0.5349
Epoch Time List: [0.5015501460293308, 0.4905716220382601, 0.5272030089981854, 0.36476052389480174, 0.3934759619878605, 0.3546397000318393, 0.4282281700288877, 0.3921746331034228, 0.44455980614293367, 0.2806669590063393, 0.3641173259820789, 0.35842599091120064, 0.37148971389979124, 0.48238724702969193, 0.96832497313153, 0.3902712940471247, 0.39926322910469025, 0.8439590280177072, 0.3145792519208044, 0.5255473280558363, 0.38930032297503203, 0.37845590291544795, 0.6258267950033769, 0.5534375469433144, 0.3074622710701078, 0.6276429401477799, 0.6546293209539726, 0.3078678820747882, 0.521954876021482, 0.5180077441036701, 0.28783837088849396, 0.5714149920968339, 0.3989961630431935, 0.3078362609958276, 0.46783072815742344, 0.3584914219100028, 0.3282528199488297, 0.3013641070574522, 0.4689446260454133, 0.3613846091320738, 0.46496967202983797, 0.531445375061594, 0.33753582707140595, 0.41232844605110586, 0.3509979099035263, 0.2985185629222542, 0.34670187905430794, 0.6814234550110996, 0.26461407192982733, 0.3016084110131487, 0.36404126905836165, 0.35991819004993886, 0.3542097869794816, 0.42089505889452994, 0.3353788099484518, 0.43488420790527016, 0.39854862401261926, 0.3559423309052363, 0.32311189209576696, 0.41247220302466303, 0.3529194399015978, 0.509855404146947, 0.40255576313938946, 0.46639947197400033, 0.4571771890623495, 0.36135739798191935, 0.373717593960464, 0.4446426120121032, 0.3727087799925357, 0.39472979912534356, 0.3271973947994411, 0.46433222701307386, 0.2587679949356243, 0.33832550107035786, 0.2538047609850764, 0.24664129910524935, 0.2497895749984309, 0.31044440891128033, 0.3279552039457485, 0.2525615659542382, 0.25084751890972257, 0.25890579796396196, 0.262121393927373, 0.3519219938898459, 0.24547295307274908, 0.24790406308602542, 0.266067449003458, 0.26903724600560963, 0.24692482501268387, 0.36214305891189724, 0.32353165303356946, 0.364098938065581, 0.25519125908613205, 0.28257878101430833, 0.23996547993738204, 0.3282199710374698, 0.27216782909817994, 0.25987914588768035, 0.25685594184324145, 0.2527221179334447, 0.2587113860063255, 0.26890606503002346, 0.41109480103477836, 0.39687668695114553, 0.3028386060614139, 0.2938752858899534, 0.3822735839057714, 0.48400732199661434, 0.4031816410133615, 0.31022944196593016, 0.3636285449611023, 0.3347554550273344, 0.34269084804691374, 0.4210484790382907, 0.35521472396794707, 0.5360760150942951, 0.43553370411973447, 0.348870117100887, 0.37661213404498994, 0.4617791709024459, 0.33327067201025784, 0.4091860209591687, 0.2567484830506146, 0.5145021630451083, 0.28828738804440945, 0.3081699140602723, 0.3407641659723595, 0.40774372802115977, 0.4595158831216395, 0.39778589888010174, 0.341138610849157, 0.3055993299931288, 0.33915374404750764, 0.5214929840294644, 0.3771810280159116, 0.3307950870366767, 0.387328915996477, 0.2842293670400977, 0.2915751190157607, 0.3779380340129137, 0.3489649291150272, 0.3532525821356103, 0.39625499094836414, 0.34246356994844973, 0.3663794060703367, 0.3497648519696668, 0.25759991409722716, 0.3599247740348801, 0.3174684059340507, 0.3226210268912837, 0.4909027380635962, 0.35362980293575674, 0.38623046304564923, 0.38274710706900805, 0.42229640495497733, 0.5412914510816336, 0.4025197410956025, 0.5382789369905367, 0.4077822620747611, 0.45665288798045367]
Total Epoch List: [108, 31, 21]
Total Time List: [0.14896774396765977, 0.09307875798549503, 0.05393277504481375]
========================training times:1========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdcbb4c0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7468;  Loss pred: 0.7468; Loss self: 0.0000; time: 0.16s
Val loss: 0.6763 score: 0.5349 time: 0.35s
Test loss: 0.6665 score: 0.6136 time: 0.06s
Epoch 2/1000, LR 0.000000
Train loss: 0.7134;  Loss pred: 0.7134; Loss self: 0.0000; time: 0.19s
Val loss: 0.6820 score: 0.5581 time: 0.05s
Test loss: 0.6736 score: 0.5682 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 3/1000, LR 0.000030
Train loss: 0.7869;  Loss pred: 0.7869; Loss self: 0.0000; time: 0.20s
Val loss: 0.6841 score: 0.5581 time: 0.12s
Test loss: 0.6777 score: 0.5455 time: 0.13s
     INFO: Early stopping counter 2 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.6646;  Loss pred: 0.6646; Loss self: 0.0000; time: 0.19s
Val loss: 0.6858 score: 0.5349 time: 0.23s
Test loss: 0.6806 score: 0.5682 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.7272;  Loss pred: 0.7272; Loss self: 0.0000; time: 0.15s
Val loss: 0.6860 score: 0.5349 time: 0.06s
Test loss: 0.6831 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6961;  Loss pred: 0.6961; Loss self: 0.0000; time: 0.41s
Val loss: 0.6836 score: 0.5581 time: 0.10s
Test loss: 0.6835 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6667;  Loss pred: 0.6667; Loss self: 0.0000; time: 0.16s
Val loss: 0.6813 score: 0.5581 time: 0.22s
Test loss: 0.6815 score: 0.5682 time: 0.16s
     INFO: Early stopping counter 6 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6929;  Loss pred: 0.6929; Loss self: 0.0000; time: 0.15s
Val loss: 0.6804 score: 0.6512 time: 0.10s
Test loss: 0.6773 score: 0.6136 time: 0.16s
     INFO: Early stopping counter 7 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.6288;  Loss pred: 0.6288; Loss self: 0.0000; time: 0.24s
Val loss: 0.6779 score: 0.6279 time: 0.06s
Test loss: 0.6723 score: 0.6364 time: 0.11s
     INFO: Early stopping counter 8 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6309;  Loss pred: 0.6309; Loss self: 0.0000; time: 0.18s
Val loss: 0.6752 score: 0.5581 time: 0.07s
Test loss: 0.6675 score: 0.6136 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.5544;  Loss pred: 0.5544; Loss self: 0.0000; time: 0.17s
Val loss: 0.6734 score: 0.4884 time: 0.17s
Test loss: 0.6648 score: 0.5909 time: 0.06s
Epoch 12/1000, LR 0.000270
Train loss: 0.5505;  Loss pred: 0.5505; Loss self: 0.0000; time: 0.18s
Val loss: 0.6712 score: 0.5581 time: 0.05s
Test loss: 0.6628 score: 0.5909 time: 0.10s
Epoch 13/1000, LR 0.000270
Train loss: 0.5796;  Loss pred: 0.5796; Loss self: 0.0000; time: 0.21s
Val loss: 0.6690 score: 0.5814 time: 0.06s
Test loss: 0.6593 score: 0.5909 time: 0.10s
Epoch 14/1000, LR 0.000270
Train loss: 0.5237;  Loss pred: 0.5237; Loss self: 0.0000; time: 0.19s
Val loss: 0.6668 score: 0.6047 time: 0.08s
Test loss: 0.6559 score: 0.6364 time: 0.16s
Epoch 15/1000, LR 0.000270
Train loss: 0.4854;  Loss pred: 0.4854; Loss self: 0.0000; time: 0.14s
Val loss: 0.6636 score: 0.6279 time: 0.07s
Test loss: 0.6516 score: 0.6591 time: 0.11s
Epoch 16/1000, LR 0.000270
Train loss: 0.4843;  Loss pred: 0.4843; Loss self: 0.0000; time: 0.21s
Val loss: 0.6610 score: 0.6279 time: 0.07s
Test loss: 0.6492 score: 0.6364 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.4400;  Loss pred: 0.4400; Loss self: 0.0000; time: 0.18s
Val loss: 0.6574 score: 0.6279 time: 0.07s
Test loss: 0.6478 score: 0.6364 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.4084;  Loss pred: 0.4084; Loss self: 0.0000; time: 0.14s
Val loss: 0.6549 score: 0.6047 time: 0.16s
Test loss: 0.6464 score: 0.6364 time: 0.06s
Epoch 19/1000, LR 0.000270
Train loss: 0.4338;  Loss pred: 0.4338; Loss self: 0.0000; time: 0.18s
Val loss: 0.6527 score: 0.6047 time: 0.05s
Test loss: 0.6446 score: 0.6591 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.4007;  Loss pred: 0.4007; Loss self: 0.0000; time: 0.14s
Val loss: 0.6545 score: 0.6512 time: 0.07s
Test loss: 0.6459 score: 0.6364 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3666;  Loss pred: 0.3666; Loss self: 0.0000; time: 0.35s
Val loss: 0.6573 score: 0.6512 time: 0.07s
Test loss: 0.6478 score: 0.6364 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3335;  Loss pred: 0.3335; Loss self: 0.0000; time: 0.14s
Val loss: 0.6624 score: 0.5814 time: 0.16s
Test loss: 0.6515 score: 0.6136 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3249;  Loss pred: 0.3249; Loss self: 0.0000; time: 0.18s
Val loss: 0.6694 score: 0.5814 time: 0.06s
Test loss: 0.6555 score: 0.6364 time: 0.13s
     INFO: Early stopping counter 4 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2975;  Loss pred: 0.2975; Loss self: 0.0000; time: 0.18s
Val loss: 0.6724 score: 0.5814 time: 0.13s
Test loss: 0.6580 score: 0.5909 time: 0.08s
     INFO: Early stopping counter 5 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2763;  Loss pred: 0.2763; Loss self: 0.0000; time: 0.17s
Val loss: 0.6721 score: 0.5581 time: 0.16s
Test loss: 0.6602 score: 0.5909 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2644;  Loss pred: 0.2644; Loss self: 0.0000; time: 0.16s
Val loss: 0.6705 score: 0.5581 time: 0.06s
Test loss: 0.6612 score: 0.5909 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2489;  Loss pred: 0.2489; Loss self: 0.0000; time: 0.18s
Val loss: 0.6662 score: 0.5814 time: 0.06s
Test loss: 0.6592 score: 0.6136 time: 0.06s
     INFO: Early stopping counter 8 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2548;  Loss pred: 0.2548; Loss self: 0.0000; time: 0.17s
Val loss: 0.6614 score: 0.5814 time: 0.07s
Test loss: 0.6573 score: 0.6818 time: 0.09s
     INFO: Early stopping counter 9 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2153;  Loss pred: 0.2153; Loss self: 0.0000; time: 0.18s
Val loss: 0.6565 score: 0.5814 time: 0.07s
Test loss: 0.6553 score: 0.6818 time: 0.13s
     INFO: Early stopping counter 10 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2032;  Loss pred: 0.2032; Loss self: 0.0000; time: 0.15s
Val loss: 0.6547 score: 0.6047 time: 0.07s
Test loss: 0.6538 score: 0.5909 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1956;  Loss pred: 0.1956; Loss self: 0.0000; time: 0.19s
Val loss: 0.6536 score: 0.6279 time: 0.08s
Test loss: 0.6528 score: 0.6136 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1832;  Loss pred: 0.1832; Loss self: 0.0000; time: 0.18s
Val loss: 0.6530 score: 0.6279 time: 0.06s
Test loss: 0.6518 score: 0.6364 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1625;  Loss pred: 0.1625; Loss self: 0.0000; time: 0.29s
Val loss: 0.6507 score: 0.6047 time: 0.06s
Test loss: 0.6506 score: 0.6364 time: 0.10s
Epoch 34/1000, LR 0.000270
Train loss: 0.1736;  Loss pred: 0.1736; Loss self: 0.0000; time: 0.20s
Val loss: 0.6472 score: 0.6047 time: 0.05s
Test loss: 0.6482 score: 0.6818 time: 0.30s
Epoch 35/1000, LR 0.000270
Train loss: 0.1704;  Loss pred: 0.1704; Loss self: 0.0000; time: 0.18s
Val loss: 0.6429 score: 0.6279 time: 0.17s
Test loss: 0.6434 score: 0.6591 time: 0.06s
Epoch 36/1000, LR 0.000270
Train loss: 0.1389;  Loss pred: 0.1389; Loss self: 0.0000; time: 0.19s
Val loss: 0.6398 score: 0.6512 time: 0.05s
Test loss: 0.6395 score: 0.6591 time: 0.11s
Epoch 37/1000, LR 0.000270
Train loss: 0.1694;  Loss pred: 0.1694; Loss self: 0.0000; time: 0.18s
Val loss: 0.6364 score: 0.6512 time: 0.06s
Test loss: 0.6352 score: 0.6818 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.1423;  Loss pred: 0.1423; Loss self: 0.0000; time: 0.16s
Val loss: 0.6349 score: 0.6977 time: 0.21s
Test loss: 0.6331 score: 0.6591 time: 0.12s
Epoch 39/1000, LR 0.000269
Train loss: 0.1192;  Loss pred: 0.1192; Loss self: 0.0000; time: 0.14s
Val loss: 0.6337 score: 0.6977 time: 0.06s
Test loss: 0.6321 score: 0.6591 time: 0.06s
Epoch 40/1000, LR 0.000269
Train loss: 0.1168;  Loss pred: 0.1168; Loss self: 0.0000; time: 0.14s
Val loss: 0.6302 score: 0.6744 time: 0.05s
Test loss: 0.6305 score: 0.6818 time: 0.06s
Epoch 41/1000, LR 0.000269
Train loss: 0.1144;  Loss pred: 0.1144; Loss self: 0.0000; time: 0.14s
Val loss: 0.6285 score: 0.6512 time: 0.05s
Test loss: 0.6313 score: 0.6818 time: 0.05s
Epoch 42/1000, LR 0.000269
Train loss: 0.0980;  Loss pred: 0.0980; Loss self: 0.0000; time: 0.17s
Val loss: 0.6287 score: 0.5814 time: 0.07s
Test loss: 0.6341 score: 0.6591 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0973;  Loss pred: 0.0973; Loss self: 0.0000; time: 0.15s
Val loss: 0.6270 score: 0.5814 time: 0.08s
Test loss: 0.6340 score: 0.6591 time: 0.06s
Epoch 44/1000, LR 0.000269
Train loss: 0.1065;  Loss pred: 0.1065; Loss self: 0.0000; time: 0.14s
Val loss: 0.6249 score: 0.6279 time: 0.06s
Test loss: 0.6334 score: 0.6818 time: 0.05s
Epoch 45/1000, LR 0.000269
Train loss: 0.0997;  Loss pred: 0.0997; Loss self: 0.0000; time: 0.15s
Val loss: 0.6230 score: 0.6512 time: 0.11s
Test loss: 0.6336 score: 0.6364 time: 0.08s
Epoch 46/1000, LR 0.000269
Train loss: 0.0799;  Loss pred: 0.0799; Loss self: 0.0000; time: 0.15s
Val loss: 0.6223 score: 0.6512 time: 0.05s
Test loss: 0.6362 score: 0.6364 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.0762;  Loss pred: 0.0762; Loss self: 0.0000; time: 0.14s
Val loss: 0.6227 score: 0.6512 time: 0.06s
Test loss: 0.6364 score: 0.6364 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 48/1000, LR 0.000269
Train loss: 0.0721;  Loss pred: 0.0721; Loss self: 0.0000; time: 0.15s
Val loss: 0.6224 score: 0.6512 time: 0.05s
Test loss: 0.6348 score: 0.5682 time: 0.06s
     INFO: Early stopping counter 2 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0642;  Loss pred: 0.0642; Loss self: 0.0000; time: 0.14s
Val loss: 0.6215 score: 0.6744 time: 0.05s
Test loss: 0.6343 score: 0.6136 time: 0.06s
Epoch 50/1000, LR 0.000269
Train loss: 0.0661;  Loss pred: 0.0661; Loss self: 0.0000; time: 0.15s
Val loss: 0.6197 score: 0.6512 time: 0.05s
Test loss: 0.6324 score: 0.6136 time: 0.20s
Epoch 51/1000, LR 0.000269
Train loss: 0.0562;  Loss pred: 0.0562; Loss self: 0.0000; time: 0.17s
Val loss: 0.6180 score: 0.6279 time: 0.14s
Test loss: 0.6328 score: 0.6364 time: 0.05s
Epoch 52/1000, LR 0.000269
Train loss: 0.0602;  Loss pred: 0.0602; Loss self: 0.0000; time: 0.14s
Val loss: 0.6118 score: 0.6279 time: 0.05s
Test loss: 0.6287 score: 0.6364 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0600;  Loss pred: 0.0600; Loss self: 0.0000; time: 0.14s
Val loss: 0.6022 score: 0.6744 time: 0.05s
Test loss: 0.6224 score: 0.6364 time: 0.05s
Epoch 54/1000, LR 0.000269
Train loss: 0.0456;  Loss pred: 0.0456; Loss self: 0.0000; time: 0.14s
Val loss: 0.5936 score: 0.6744 time: 0.06s
Test loss: 0.6172 score: 0.6364 time: 0.07s
Epoch 55/1000, LR 0.000269
Train loss: 0.0493;  Loss pred: 0.0493; Loss self: 0.0000; time: 0.15s
Val loss: 0.5852 score: 0.6744 time: 0.06s
Test loss: 0.6118 score: 0.6364 time: 0.06s
Epoch 56/1000, LR 0.000269
Train loss: 0.0452;  Loss pred: 0.0452; Loss self: 0.0000; time: 0.15s
Val loss: 0.5813 score: 0.6977 time: 0.10s
Test loss: 0.6091 score: 0.7045 time: 0.07s
Epoch 57/1000, LR 0.000269
Train loss: 0.0365;  Loss pred: 0.0365; Loss self: 0.0000; time: 0.15s
Val loss: 0.5786 score: 0.6977 time: 0.11s
Test loss: 0.6071 score: 0.7045 time: 0.05s
Epoch 58/1000, LR 0.000269
Train loss: 0.0347;  Loss pred: 0.0347; Loss self: 0.0000; time: 0.14s
Val loss: 0.5746 score: 0.6977 time: 0.05s
Test loss: 0.6025 score: 0.6818 time: 0.07s
Epoch 59/1000, LR 0.000268
Train loss: 0.0303;  Loss pred: 0.0303; Loss self: 0.0000; time: 0.14s
Val loss: 0.5719 score: 0.6744 time: 0.05s
Test loss: 0.5991 score: 0.6591 time: 0.05s
Epoch 60/1000, LR 0.000268
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.14s
Val loss: 0.5601 score: 0.6977 time: 0.06s
Test loss: 0.5891 score: 0.6818 time: 0.05s
Epoch 61/1000, LR 0.000268
Train loss: 0.0342;  Loss pred: 0.0342; Loss self: 0.0000; time: 0.14s
Val loss: 0.5430 score: 0.7674 time: 0.06s
Test loss: 0.5743 score: 0.6818 time: 0.06s
Epoch 62/1000, LR 0.000268
Train loss: 0.0322;  Loss pred: 0.0322; Loss self: 0.0000; time: 0.14s
Val loss: 0.5283 score: 0.7674 time: 0.07s
Test loss: 0.5607 score: 0.6818 time: 0.07s
Epoch 63/1000, LR 0.000268
Train loss: 0.0236;  Loss pred: 0.0236; Loss self: 0.0000; time: 0.15s
Val loss: 0.5130 score: 0.7674 time: 0.06s
Test loss: 0.5454 score: 0.7045 time: 0.06s
Epoch 64/1000, LR 0.000268
Train loss: 0.0270;  Loss pred: 0.0270; Loss self: 0.0000; time: 0.14s
Val loss: 0.5005 score: 0.7442 time: 0.09s
Test loss: 0.5334 score: 0.7045 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0238;  Loss pred: 0.0238; Loss self: 0.0000; time: 0.14s
Val loss: 0.4839 score: 0.7442 time: 0.05s
Test loss: 0.5166 score: 0.7500 time: 0.05s
Epoch 66/1000, LR 0.000268
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.14s
Val loss: 0.4700 score: 0.7674 time: 0.06s
Test loss: 0.5052 score: 0.7500 time: 0.06s
Epoch 67/1000, LR 0.000268
Train loss: 0.0226;  Loss pred: 0.0226; Loss self: 0.0000; time: 0.14s
Val loss: 0.4574 score: 0.7442 time: 0.06s
Test loss: 0.4973 score: 0.7500 time: 0.06s
Epoch 68/1000, LR 0.000268
Train loss: 0.0224;  Loss pred: 0.0224; Loss self: 0.0000; time: 0.15s
Val loss: 0.4493 score: 0.7442 time: 0.05s
Test loss: 0.4929 score: 0.7727 time: 0.11s
Epoch 69/1000, LR 0.000268
Train loss: 0.0200;  Loss pred: 0.0200; Loss self: 0.0000; time: 0.15s
Val loss: 0.4410 score: 0.7442 time: 0.06s
Test loss: 0.4877 score: 0.7727 time: 0.06s
Epoch 70/1000, LR 0.000268
Train loss: 0.0159;  Loss pred: 0.0159; Loss self: 0.0000; time: 0.14s
Val loss: 0.4348 score: 0.7209 time: 0.06s
Test loss: 0.4856 score: 0.7727 time: 0.06s
Epoch 71/1000, LR 0.000268
Train loss: 0.0176;  Loss pred: 0.0176; Loss self: 0.0000; time: 0.15s
Val loss: 0.4285 score: 0.7442 time: 0.09s
Test loss: 0.4845 score: 0.7727 time: 0.11s
Epoch 72/1000, LR 0.000267
Train loss: 0.0194;  Loss pred: 0.0194; Loss self: 0.0000; time: 0.20s
Val loss: 0.4234 score: 0.7442 time: 0.24s
Test loss: 0.4836 score: 0.7727 time: 0.06s
Epoch 73/1000, LR 0.000267
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.15s
Val loss: 0.4202 score: 0.7442 time: 0.08s
Test loss: 0.4862 score: 0.7727 time: 0.09s
Epoch 74/1000, LR 0.000267
Train loss: 0.0142;  Loss pred: 0.0142; Loss self: 0.0000; time: 0.19s
Val loss: 0.4155 score: 0.7442 time: 0.06s
Test loss: 0.4873 score: 0.7727 time: 0.15s
Epoch 75/1000, LR 0.000267
Train loss: 0.0120;  Loss pred: 0.0120; Loss self: 0.0000; time: 0.18s
Val loss: 0.4118 score: 0.7442 time: 0.17s
Test loss: 0.4881 score: 0.7727 time: 0.05s
Epoch 76/1000, LR 0.000267
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.14s
Val loss: 0.4089 score: 0.7442 time: 0.07s
Test loss: 0.4850 score: 0.7727 time: 0.06s
Epoch 77/1000, LR 0.000267
Train loss: 0.0134;  Loss pred: 0.0134; Loss self: 0.0000; time: 0.14s
Val loss: 0.4092 score: 0.7674 time: 0.07s
Test loss: 0.4862 score: 0.7727 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0133;  Loss pred: 0.0133; Loss self: 0.0000; time: 0.32s
Val loss: 0.4127 score: 0.7442 time: 0.06s
Test loss: 0.4941 score: 0.7273 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0143;  Loss pred: 0.0143; Loss self: 0.0000; time: 0.19s
Val loss: 0.4190 score: 0.7442 time: 0.11s
Test loss: 0.5078 score: 0.6818 time: 0.09s
     INFO: Early stopping counter 3 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0094;  Loss pred: 0.0094; Loss self: 0.0000; time: 0.21s
Val loss: 0.4248 score: 0.7674 time: 0.07s
Test loss: 0.5204 score: 0.6818 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.18s
Val loss: 0.4277 score: 0.7674 time: 0.07s
Test loss: 0.5278 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0102;  Loss pred: 0.0102; Loss self: 0.0000; time: 0.15s
Val loss: 0.4280 score: 0.7674 time: 0.31s
Test loss: 0.5291 score: 0.6818 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.15s
Val loss: 0.4263 score: 0.7674 time: 0.05s
Test loss: 0.5265 score: 0.7273 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0079;  Loss pred: 0.0079; Loss self: 0.0000; time: 0.16s
Val loss: 0.4305 score: 0.7442 time: 0.05s
Test loss: 0.5297 score: 0.7727 time: 0.09s
     INFO: Early stopping counter 8 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0077;  Loss pred: 0.0077; Loss self: 0.0000; time: 0.29s
Val loss: 0.4352 score: 0.7674 time: 0.05s
Test loss: 0.5358 score: 0.7727 time: 0.17s
     INFO: Early stopping counter 9 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.14s
Val loss: 0.4404 score: 0.7907 time: 0.08s
Test loss: 0.5452 score: 0.7955 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0089;  Loss pred: 0.0089; Loss self: 0.0000; time: 0.20s
Val loss: 0.4453 score: 0.7907 time: 0.05s
Test loss: 0.5549 score: 0.7955 time: 0.16s
     INFO: Early stopping counter 11 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0071;  Loss pred: 0.0071; Loss self: 0.0000; time: 0.21s
Val loss: 0.4487 score: 0.7907 time: 0.06s
Test loss: 0.5604 score: 0.8182 time: 0.09s
     INFO: Early stopping counter 12 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0066;  Loss pred: 0.0066; Loss self: 0.0000; time: 0.22s
Val loss: 0.4565 score: 0.7674 time: 0.10s
Test loss: 0.5662 score: 0.8182 time: 0.09s
     INFO: Early stopping counter 13 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.21s
Val loss: 0.4706 score: 0.7674 time: 0.07s
Test loss: 0.5764 score: 0.8182 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 91/1000, LR 0.000266
Train loss: 0.0069;  Loss pred: 0.0069; Loss self: 0.0000; time: 0.18s
Val loss: 0.4861 score: 0.7674 time: 0.06s
Test loss: 0.5880 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 92/1000, LR 0.000266
Train loss: 0.0059;  Loss pred: 0.0059; Loss self: 0.0000; time: 0.15s
Val loss: 0.5044 score: 0.7674 time: 0.15s
Test loss: 0.6018 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 93/1000, LR 0.000265
Train loss: 0.0061;  Loss pred: 0.0061; Loss self: 0.0000; time: 0.14s
Val loss: 0.5193 score: 0.7674 time: 0.07s
Test loss: 0.6148 score: 0.8182 time: 0.07s
     INFO: Early stopping counter 17 of 20
Epoch 94/1000, LR 0.000265
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.21s
Val loss: 0.5320 score: 0.7674 time: 0.05s
Test loss: 0.6272 score: 0.8182 time: 0.12s
     INFO: Early stopping counter 18 of 20
Epoch 95/1000, LR 0.000265
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.20s
Val loss: 0.5464 score: 0.7674 time: 0.06s
Test loss: 0.6394 score: 0.8182 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 96/1000, LR 0.000265
Train loss: 0.0052;  Loss pred: 0.0052; Loss self: 0.0000; time: 0.37s
Val loss: 0.5585 score: 0.7674 time: 0.10s
Test loss: 0.6523 score: 0.8182 time: 0.08s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 075,   Train_Loss: 0.0110,   Val_Loss: 0.4089,   Val_Precision: 0.7895,   Val_Recall: 0.6818,   Val_accuracy: 0.7317,   Val_Score: 0.7442,   Val_Loss: 0.4089,   Test_Precision: 0.8000,   Test_Recall: 0.7273,   Test_accuracy: 0.7619,   Test_Score: 0.7727,   Test_loss: 0.4850


[0.06113242998253554, 0.11343773291446269, 0.14132661803159863, 0.05262730398681015, 0.10299825191032141, 0.09975217399187386, 0.16877738700713962, 0.1624851309461519, 0.11636502202600241, 0.09409018000587821, 0.06207356904633343, 0.10940970794763416, 0.10664870403707027, 0.1647164709866047, 0.11684316699393094, 0.05691265605855733, 0.09337871090974659, 0.06773503904696554, 0.08559224603231996, 0.08530414593406022, 0.08964969904627651, 0.05612020299304277, 0.13096557394601405, 0.08523298997897655, 0.0574891259893775, 0.096315000904724, 0.06727673800196499, 0.09511876595206559, 0.1354906519409269, 0.0956640390213579, 0.10692029097117484, 0.057214832049794495, 0.10857600998133421, 0.30633669695816934, 0.061056191101670265, 0.11729220196139067, 0.08619520300999284, 0.12362447008490562, 0.06313893501646817, 0.06579352100379765, 0.05526144301984459, 0.05524656700436026, 0.06239294190891087, 0.055379385012201965, 0.08134239399805665, 0.055422416888177395, 0.06589120195712894, 0.06261756492312998, 0.05979219090659171, 0.20870858302805573, 0.059032420977018774, 0.05577976198401302, 0.05597905605100095, 0.0723285130225122, 0.061976026045158505, 0.07805083098355681, 0.05573054892010987, 0.07030142203439027, 0.05925756797660142, 0.05513758200686425, 0.06310411402955651, 0.07644447998609394, 0.06898134900256991, 0.05919210298452526, 0.059863141970708966, 0.06278133403975517, 0.06279321492183954, 0.11637154500931501, 0.06062521191779524, 0.061195839080028236, 0.11759852699469775, 0.06806300801690668, 0.09276503999717534, 0.15575748099945486, 0.054418081999756396, 0.06446138094179332, 0.08846866199746728, 0.1539271930232644, 0.09635936201084405, 0.10416059393901378, 0.05648687807843089, 0.053203944000415504, 0.0888657970353961, 0.09133656194899231, 0.17951862304471433, 0.08701515302527696, 0.16244034201372415, 0.09165172302164137, 0.09208151802886277, 0.09559025801718235, 0.05495428992435336, 0.05273962905630469, 0.07859981199726462, 0.12235939293168485, 0.0570620991056785, 0.0812391460640356]
[0.0013893734086939896, 0.0025781302935105155, 0.0032119685916272415, 0.0011960750906093215, 0.002340869361598214, 0.0022670948634516785, 0.0038358497047077185, 0.0036928438851398164, 0.002644659591500055, 0.0021384131819517775, 0.0014107629328712144, 0.00248658427153714, 0.002423834182660688, 0.0037435561587864704, 0.0026555265225893395, 0.001293469455876303, 0.002122243429766968, 0.0015394327056128532, 0.0019452783189163629, 0.0019387305894104595, 0.002037493160142648, 0.0012754591589327902, 0.0029764903169548647, 0.0019371134086131033, 0.001306571045213125, 0.002188977293289182, 0.0015290167727719315, 0.002161790135274218, 0.00307933299865743, 0.002174182705030861, 0.0024300066129812462, 0.001300337092040784, 0.0024676365904848685, 0.006962197658140212, 0.0013876407068561423, 0.002665731862758879, 0.0019589818865907464, 0.0028096470473842187, 0.001434975795828822, 0.0014953072955408557, 0.0012559418868146497, 0.0012556037955536422, 0.0014180214070207016, 0.0012586223866409537, 0.0018486907726831057, 0.0012596003838222134, 0.001497527317207476, 0.0014231264755256814, 0.0013589134296952661, 0.004743376887001266, 0.0013416459312958812, 0.0012677218632730232, 0.0012722512738863852, 0.0016438298414207318, 0.001408546046480875, 0.001773882522353564, 0.0012666033845479515, 0.001597759591690688, 0.0013467629085591232, 0.0012531268637923693, 0.001434184409762648, 0.0017373745451384987, 0.001567757931876589, 0.0013452750678301197, 0.0013605259538797493, 0.0014268485009035264, 0.0014271185209508985, 0.0026448078411207957, 0.0013778457254044372, 0.0013908145245460962, 0.00267269379533404, 0.0015468865458387882, 0.0021082963635721667, 0.0035399427499876106, 0.0012367745909035545, 0.0014650313850407574, 0.002010651409033347, 0.003498345295983282, 0.0021899855002464556, 0.0023672862258866767, 0.001283792683600702, 0.0012091805454639887, 0.0020196772053499112, 0.002075830953386189, 0.00407996870556169, 0.00197761711421084, 0.003691825954857367, 0.002082993705037304, 0.002092761773383245, 0.0021725058640268717, 0.0012489611346443946, 0.0011986279330978339, 0.001786359363574196, 0.0027808952939019286, 0.0012968658887654203, 0.0018463442287280816]
[719.7489125259707, 387.87799147200906, 311.3355474915718, 836.0679089893644, 427.1917162080571, 441.09314352972785, 260.69843111233087, 270.79400892738755, 378.1204973275213, 467.63647383022476, 708.8363159391911, 402.158097534264, 412.56947655647025, 267.12568413135944, 376.5731546996278, 773.1145064593098, 471.19947974573523, 649.5899407320288, 514.0652575396308, 515.8014246342942, 490.7991936179008, 784.0313764626741, 335.96615258707186, 516.2320365723763, 765.3621314077737, 456.8343413454914, 654.015062363976, 462.579592571391, 324.74565122901413, 459.94294669260813, 411.52151383372285, 769.0313581923385, 405.2460576472117, 143.63280807329542, 720.6476395936911, 375.1315028980663, 510.4692426433402, 355.91659134943654, 696.8758657161976, 668.7588584514315, 796.2151835991585, 796.4295771812818, 705.2079715080077, 794.5194767024824, 540.9233468227061, 793.902584378019, 667.7674513909748, 702.6782349970794, 735.8820496933702, 210.82027083709002, 745.3531342908861, 788.8165606122664, 786.008252084723, 608.3354704984054, 709.9519412222338, 563.7351895621663, 789.5131279448603, 625.8763866607982, 742.5211918480006, 798.0038006476653, 697.2604033295106, 575.5811277413891, 637.8535739908591, 743.3424017981424, 735.009866697762, 700.8452539752943, 700.7126495238065, 378.09930250971576, 725.7706589077462, 719.0031325897737, 374.1543463548983, 646.4598213036737, 474.3166175677797, 282.490444232043, 808.554774131822, 682.5792335992719, 497.3512541792443, 285.8494274845243, 456.6240278245963, 422.42462658922705, 778.9419684144499, 827.0063587702516, 495.1286261740766, 481.7347955857171, 245.09991918242667, 505.6590544318008, 270.8686737207347, 480.0782631179825, 477.83747424980857, 460.2979520370266, 800.6654268587156, 834.2872482668718, 559.7977766350289, 359.5964228472912, 771.0897546638162, 541.610813650326]
Elapsed: 0.0894289899636836~0.04033781516864394
Time per graph: 0.0020324770446291727~0.0009167685265600895
Speed: 564.956068417236~181.22700177505692
Total Time: 0.0818
best val loss: 0.4088519811630249 test_score: 0.7727

Testing...
Test loss: 0.5452 score: 0.7955 time: 0.06s
test Score 0.7955
Epoch Time List: [0.5694545840378851, 0.34612133994232863, 0.44936126994434744, 0.4647056310204789, 0.3055782449664548, 0.6012817961163819, 0.541969764046371, 0.4025109460344538, 0.4073727980721742, 0.337119824020192, 0.3988798060454428, 0.3387120591942221, 0.37164870102424175, 0.43003343301825225, 0.32545030303299427, 0.33141688897740096, 0.3431315190391615, 0.36248998704832047, 0.31434848392382264, 0.2873671550769359, 0.503582744859159, 0.35211948305368423, 0.3659928230335936, 0.3890576738631353, 0.38311899604741484, 0.30673852493055165, 0.30307944503147155, 0.32784671301487833, 0.3789958319393918, 0.3064443301409483, 0.3732401089509949, 0.28758417605422437, 0.4500546180643141, 0.5530716290231794, 0.41215661086607724, 0.3556946530006826, 0.31965220405254513, 0.4961317889392376, 0.2536585567286238, 0.25344654999207705, 0.2467510470887646, 0.28990242211148143, 0.2841140360105783, 0.2483561870176345, 0.33516570506617427, 0.25033975986298174, 0.2616872328799218, 0.26027403702028096, 0.24595566501375288, 0.4098585620522499, 0.3639565600315109, 0.2472788579761982, 0.24270789406728, 0.2644021598389372, 0.26380844390951097, 0.3152856188826263, 0.30708154710009694, 0.2546051051467657, 0.2475301680387929, 0.24750416784081608, 0.26056074199732393, 0.2797216458711773, 0.2746688270708546, 0.2918405419914052, 0.24385933799203485, 0.25820080505218357, 0.2529100519604981, 0.3152186118531972, 0.2702978750457987, 0.2583225261187181, 0.3574946220032871, 0.5026628320338205, 0.3142301777843386, 0.4062447580508888, 0.3992301808902994, 0.26715681108180434, 0.29537670500576496, 0.5225820419145748, 0.3902483769925311, 0.37906606611795723, 0.30269589193630964, 0.5089736121008173, 0.2865167971467599, 0.2953018631087616, 0.5171952330274507, 0.3021846718620509, 0.40847639611456543, 0.3624878420960158, 0.4020374509273097, 0.37277960195206106, 0.29150991793721914, 0.34071803803090006, 0.27902834699489176, 0.3785099539672956, 0.31327784003224224, 0.5444416018435732]
Total Epoch List: [96]
Total Time List: [0.0818010080838576]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdcbb040>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6695;  Loss pred: 0.6695; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.4992 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.3867 score: 0.5116 time: 0.09s
Epoch 2/1000, LR 0.000000
Train loss: 0.6649;  Loss pred: 0.6649; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 1.1490 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 1.0671 score: 0.5116 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.6472;  Loss pred: 0.6472; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9635 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.9004 score: 0.5116 time: 0.05s
Epoch 4/1000, LR 0.000060
Train loss: 0.7184;  Loss pred: 0.7184; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8499 score: 0.5000 time: 0.10s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8028 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6126;  Loss pred: 0.6126; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7794 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7426 score: 0.5116 time: 0.08s
Epoch 6/1000, LR 0.000120
Train loss: 0.6182;  Loss pred: 0.6182; Loss self: 0.0000; time: 0.23s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7363 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7051 score: 0.5116 time: 0.10s
Epoch 7/1000, LR 0.000150
Train loss: 0.5749;  Loss pred: 0.5749; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7153 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6873 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.5203;  Loss pred: 0.5203; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7058 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6802 score: 0.5116 time: 0.08s
Epoch 9/1000, LR 0.000210
Train loss: 0.5355;  Loss pred: 0.5355; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7016 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6768 score: 0.5116 time: 0.05s
Epoch 10/1000, LR 0.000240
Train loss: 0.5380;  Loss pred: 0.5380; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6962 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6774 score: 0.5116 time: 0.09s
Epoch 11/1000, LR 0.000270
Train loss: 0.4716;  Loss pred: 0.4716; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6907 score: 0.5000 time: 0.08s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6795 score: 0.5116 time: 0.12s
Epoch 12/1000, LR 0.000270
Train loss: 0.4443;  Loss pred: 0.4443; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6846 score: 0.5000 time: 0.09s
Test loss: 0.6823 score: 0.4651 time: 0.11s
Epoch 13/1000, LR 0.000270
Train loss: 0.4271;  Loss pred: 0.4271; Loss self: 0.0000; time: 0.28s
Val loss: 0.6821 score: 0.5682 time: 0.05s
Test loss: 0.6835 score: 0.5116 time: 0.09s
Epoch 14/1000, LR 0.000270
Train loss: 0.4124;  Loss pred: 0.4124; Loss self: 0.0000; time: 0.17s
Val loss: 0.6790 score: 0.5682 time: 0.17s
Test loss: 0.6811 score: 0.6047 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.3345;  Loss pred: 0.3345; Loss self: 0.0000; time: 0.17s
Val loss: 0.6750 score: 0.6364 time: 0.05s
Test loss: 0.6764 score: 0.6279 time: 0.08s
Epoch 16/1000, LR 0.000270
Train loss: 0.3743;  Loss pred: 0.3743; Loss self: 0.0000; time: 0.20s
Val loss: 0.6715 score: 0.6591 time: 0.05s
Test loss: 0.6720 score: 0.6047 time: 0.11s
Epoch 17/1000, LR 0.000270
Train loss: 0.3214;  Loss pred: 0.3214; Loss self: 0.0000; time: 0.23s
Val loss: 0.6679 score: 0.6364 time: 0.08s
Test loss: 0.6671 score: 0.5581 time: 0.16s
Epoch 18/1000, LR 0.000270
Train loss: 0.2853;  Loss pred: 0.2853; Loss self: 0.0000; time: 0.14s
Val loss: 0.6655 score: 0.5909 time: 0.09s
Test loss: 0.6646 score: 0.5581 time: 0.08s
Epoch 19/1000, LR 0.000270
Train loss: 0.2581;  Loss pred: 0.2581; Loss self: 0.0000; time: 0.21s
Val loss: 0.6630 score: 0.6364 time: 0.05s
Test loss: 0.6650 score: 0.6047 time: 0.08s
Epoch 20/1000, LR 0.000270
Train loss: 0.2541;  Loss pred: 0.2541; Loss self: 0.0000; time: 0.25s
Val loss: 0.6593 score: 0.6136 time: 0.17s
Test loss: 0.6685 score: 0.6047 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.2118;  Loss pred: 0.2118; Loss self: 0.0000; time: 0.16s
Val loss: 0.6568 score: 0.6591 time: 0.24s
Test loss: 0.6715 score: 0.6279 time: 0.07s
Epoch 22/1000, LR 0.000270
Train loss: 0.1944;  Loss pred: 0.1944; Loss self: 0.0000; time: 0.23s
Val loss: 0.6530 score: 0.6364 time: 0.07s
Test loss: 0.6751 score: 0.6047 time: 0.10s
Epoch 23/1000, LR 0.000270
Train loss: 0.2020;  Loss pred: 0.2020; Loss self: 0.0000; time: 0.19s
Val loss: 0.6490 score: 0.6136 time: 0.15s
Test loss: 0.6822 score: 0.5814 time: 0.05s
Epoch 24/1000, LR 0.000270
Train loss: 0.1950;  Loss pred: 0.1950; Loss self: 0.0000; time: 0.15s
Val loss: 0.6450 score: 0.5682 time: 0.06s
Test loss: 0.6895 score: 0.5814 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.1913;  Loss pred: 0.1913; Loss self: 0.0000; time: 0.15s
Val loss: 0.6411 score: 0.5909 time: 0.07s
Test loss: 0.6993 score: 0.5581 time: 0.07s
Epoch 26/1000, LR 0.000270
Train loss: 0.1679;  Loss pred: 0.1679; Loss self: 0.0000; time: 0.15s
Val loss: 0.6361 score: 0.5909 time: 0.06s
Test loss: 0.7143 score: 0.5581 time: 0.08s
Epoch 27/1000, LR 0.000270
Train loss: 0.1440;  Loss pred: 0.1440; Loss self: 0.0000; time: 0.21s
Val loss: 0.6335 score: 0.5909 time: 0.17s
Test loss: 0.7303 score: 0.4884 time: 0.05s
Epoch 28/1000, LR 0.000270
Train loss: 0.1496;  Loss pred: 0.1496; Loss self: 0.0000; time: 0.15s
Val loss: 0.6344 score: 0.6136 time: 0.05s
Test loss: 0.7471 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.1444;  Loss pred: 0.1444; Loss self: 0.0000; time: 0.15s
Val loss: 0.6369 score: 0.6136 time: 0.06s
Test loss: 0.7618 score: 0.4884 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.1347;  Loss pred: 0.1347; Loss self: 0.0000; time: 0.21s
Val loss: 0.6406 score: 0.6591 time: 0.14s
Test loss: 0.7755 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1182;  Loss pred: 0.1182; Loss self: 0.0000; time: 0.29s
Val loss: 0.6431 score: 0.6818 time: 0.08s
Test loss: 0.7820 score: 0.4884 time: 0.08s
     INFO: Early stopping counter 4 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1299;  Loss pred: 0.1299; Loss self: 0.0000; time: 0.20s
Val loss: 0.6452 score: 0.6818 time: 0.06s
Test loss: 0.7916 score: 0.4884 time: 0.16s
     INFO: Early stopping counter 5 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1008;  Loss pred: 0.1008; Loss self: 0.0000; time: 0.17s
Val loss: 0.6455 score: 0.6818 time: 0.16s
Test loss: 0.7957 score: 0.4884 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1064;  Loss pred: 0.1064; Loss self: 0.0000; time: 0.21s
Val loss: 0.6443 score: 0.6818 time: 0.05s
Test loss: 0.7968 score: 0.4884 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1025;  Loss pred: 0.1025; Loss self: 0.0000; time: 0.29s
Val loss: 0.6410 score: 0.6591 time: 0.05s
Test loss: 0.7919 score: 0.4884 time: 0.12s
     INFO: Early stopping counter 8 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.0832;  Loss pred: 0.0832; Loss self: 0.0000; time: 0.18s
Val loss: 0.6380 score: 0.6364 time: 0.07s
Test loss: 0.7887 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.0779;  Loss pred: 0.0779; Loss self: 0.0000; time: 0.17s
Val loss: 0.6358 score: 0.6136 time: 0.13s
Test loss: 0.7880 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.0810;  Loss pred: 0.0810; Loss self: 0.0000; time: 0.17s
Val loss: 0.6339 score: 0.6136 time: 0.07s
Test loss: 0.7864 score: 0.5349 time: 0.08s
     INFO: Early stopping counter 11 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.0768;  Loss pred: 0.0768; Loss self: 0.0000; time: 0.15s
Val loss: 0.6314 score: 0.6136 time: 0.06s
Test loss: 0.7860 score: 0.5349 time: 0.10s
Epoch 40/1000, LR 0.000269
Train loss: 0.0711;  Loss pred: 0.0711; Loss self: 0.0000; time: 0.25s
Val loss: 0.6302 score: 0.6136 time: 0.06s
Test loss: 0.7849 score: 0.5581 time: 0.05s
Epoch 41/1000, LR 0.000269
Train loss: 0.0727;  Loss pred: 0.0727; Loss self: 0.0000; time: 0.31s
Val loss: 0.6258 score: 0.6136 time: 0.06s
Test loss: 0.7731 score: 0.5814 time: 0.10s
Epoch 42/1000, LR 0.000269
Train loss: 0.0695;  Loss pred: 0.0695; Loss self: 0.0000; time: 0.16s
Val loss: 0.6222 score: 0.6136 time: 0.06s
Test loss: 0.7764 score: 0.5814 time: 0.10s
Epoch 43/1000, LR 0.000269
Train loss: 0.0613;  Loss pred: 0.0613; Loss self: 0.0000; time: 0.21s
Val loss: 0.6209 score: 0.6364 time: 0.12s
Test loss: 0.7918 score: 0.5814 time: 0.05s
Epoch 44/1000, LR 0.000269
Train loss: 0.0486;  Loss pred: 0.0486; Loss self: 0.0000; time: 0.30s
Val loss: 0.6208 score: 0.6364 time: 0.09s
Test loss: 0.8072 score: 0.5349 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.0503;  Loss pred: 0.0503; Loss self: 0.0000; time: 0.21s
Val loss: 0.6180 score: 0.6818 time: 0.06s
Test loss: 0.8098 score: 0.5349 time: 0.09s
Epoch 46/1000, LR 0.000269
Train loss: 0.0612;  Loss pred: 0.0612; Loss self: 0.0000; time: 0.24s
Val loss: 0.6129 score: 0.6364 time: 0.19s
Test loss: 0.8010 score: 0.5581 time: 0.05s
Epoch 47/1000, LR 0.000269
Train loss: 0.0504;  Loss pred: 0.0504; Loss self: 0.0000; time: 0.14s
Val loss: 0.6048 score: 0.6364 time: 0.07s
Test loss: 0.7760 score: 0.5814 time: 0.07s
Epoch 48/1000, LR 0.000269
Train loss: 0.0472;  Loss pred: 0.0472; Loss self: 0.0000; time: 0.20s
Val loss: 0.6058 score: 0.6364 time: 0.15s
Test loss: 0.7884 score: 0.5581 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0463;  Loss pred: 0.0463; Loss self: 0.0000; time: 0.19s
Val loss: 0.6115 score: 0.6591 time: 0.05s
Test loss: 0.8179 score: 0.5581 time: 0.22s
     INFO: Early stopping counter 2 of 20
Epoch 50/1000, LR 0.000269
Train loss: 0.0426;  Loss pred: 0.0426; Loss self: 0.0000; time: 0.23s
Val loss: 0.6225 score: 0.6591 time: 0.13s
Test loss: 0.8598 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 51/1000, LR 0.000269
Train loss: 0.0487;  Loss pred: 0.0487; Loss self: 0.0000; time: 0.16s
Val loss: 0.6237 score: 0.6591 time: 0.18s
Test loss: 0.8737 score: 0.5581 time: 0.07s
     INFO: Early stopping counter 4 of 20
Epoch 52/1000, LR 0.000269
Train loss: 0.0325;  Loss pred: 0.0325; Loss self: 0.0000; time: 0.14s
Val loss: 0.6195 score: 0.6818 time: 0.05s
Test loss: 0.8736 score: 0.5581 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 53/1000, LR 0.000269
Train loss: 0.0375;  Loss pred: 0.0375; Loss self: 0.0000; time: 0.17s
Val loss: 0.6120 score: 0.6818 time: 0.05s
Test loss: 0.8625 score: 0.5814 time: 0.14s
     INFO: Early stopping counter 6 of 20
Epoch 54/1000, LR 0.000269
Train loss: 0.0362;  Loss pred: 0.0362; Loss self: 0.0000; time: 0.18s
Val loss: 0.6003 score: 0.6591 time: 0.07s
Test loss: 0.8391 score: 0.5814 time: 0.08s
Epoch 55/1000, LR 0.000269
Train loss: 0.0268;  Loss pred: 0.0268; Loss self: 0.0000; time: 0.17s
Val loss: 0.5972 score: 0.6818 time: 0.08s
Test loss: 0.8322 score: 0.5814 time: 0.08s
Epoch 56/1000, LR 0.000269
Train loss: 0.0364;  Loss pred: 0.0364; Loss self: 0.0000; time: 0.15s
Val loss: 0.6112 score: 0.6591 time: 0.08s
Test loss: 0.8694 score: 0.6047 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0363;  Loss pred: 0.0363; Loss self: 0.0000; time: 0.16s
Val loss: 0.6232 score: 0.6591 time: 0.14s
Test loss: 0.8880 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0280;  Loss pred: 0.0280; Loss self: 0.0000; time: 0.13s
Val loss: 0.6468 score: 0.6364 time: 0.05s
Test loss: 0.9254 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 59/1000, LR 0.000268
Train loss: 0.0296;  Loss pred: 0.0296; Loss self: 0.0000; time: 0.14s
Val loss: 0.6702 score: 0.6364 time: 0.07s
Test loss: 0.9595 score: 0.5814 time: 0.06s
     INFO: Early stopping counter 4 of 20
Epoch 60/1000, LR 0.000268
Train loss: 0.0241;  Loss pred: 0.0241; Loss self: 0.0000; time: 0.14s
Val loss: 0.6769 score: 0.6364 time: 0.12s
Test loss: 0.9592 score: 0.5814 time: 0.09s
     INFO: Early stopping counter 5 of 20
Epoch 61/1000, LR 0.000268
Train loss: 0.0213;  Loss pred: 0.0213; Loss self: 0.0000; time: 0.17s
Val loss: 0.6703 score: 0.6591 time: 0.06s
Test loss: 0.9319 score: 0.5814 time: 0.07s
     INFO: Early stopping counter 6 of 20
Epoch 62/1000, LR 0.000268
Train loss: 0.0257;  Loss pred: 0.0257; Loss self: 0.0000; time: 0.18s
Val loss: 0.6519 score: 0.6591 time: 0.09s
Test loss: 0.8874 score: 0.6047 time: 0.06s
     INFO: Early stopping counter 7 of 20
Epoch 63/1000, LR 0.000268
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.61s
Val loss: 0.6271 score: 0.6591 time: 0.05s
Test loss: 0.8354 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 64/1000, LR 0.000268
Train loss: 0.0169;  Loss pred: 0.0169; Loss self: 0.0000; time: 0.14s
Val loss: 0.6170 score: 0.6818 time: 0.05s
Test loss: 0.8095 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 65/1000, LR 0.000268
Train loss: 0.0150;  Loss pred: 0.0150; Loss self: 0.0000; time: 0.15s
Val loss: 0.6104 score: 0.6818 time: 0.05s
Test loss: 0.7945 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 10 of 20
Epoch 66/1000, LR 0.000268
Train loss: 0.0170;  Loss pred: 0.0170; Loss self: 0.0000; time: 0.15s
Val loss: 0.6039 score: 0.7500 time: 0.05s
Test loss: 0.7797 score: 0.6279 time: 0.06s
     INFO: Early stopping counter 11 of 20
Epoch 67/1000, LR 0.000268
Train loss: 0.0199;  Loss pred: 0.0199; Loss self: 0.0000; time: 0.16s
Val loss: 0.6572 score: 0.6818 time: 0.05s
Test loss: 0.8633 score: 0.6279 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 68/1000, LR 0.000268
Train loss: 0.0128;  Loss pred: 0.0128; Loss self: 0.0000; time: 0.32s
Val loss: 0.6877 score: 0.6818 time: 0.05s
Test loss: 0.9168 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 13 of 20
Epoch 69/1000, LR 0.000268
Train loss: 0.0147;  Loss pred: 0.0147; Loss self: 0.0000; time: 0.15s
Val loss: 0.7097 score: 0.6818 time: 0.30s
Test loss: 0.9598 score: 0.6512 time: 0.18s
     INFO: Early stopping counter 14 of 20
Epoch 70/1000, LR 0.000268
Train loss: 0.0132;  Loss pred: 0.0132; Loss self: 0.0000; time: 0.15s
Val loss: 0.7049 score: 0.6818 time: 0.06s
Test loss: 0.9513 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 71/1000, LR 0.000268
Train loss: 0.0106;  Loss pred: 0.0106; Loss self: 0.0000; time: 0.19s
Val loss: 0.6830 score: 0.7273 time: 0.05s
Test loss: 0.9096 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.15s
Val loss: 0.6668 score: 0.7273 time: 0.05s
Test loss: 0.8793 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 17 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0124;  Loss pred: 0.0124; Loss self: 0.0000; time: 0.17s
Val loss: 0.6584 score: 0.7500 time: 0.06s
Test loss: 0.8610 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 18 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0100;  Loss pred: 0.0100; Loss self: 0.0000; time: 0.15s
Val loss: 0.6485 score: 0.7273 time: 0.08s
Test loss: 0.8443 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 19 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0082;  Loss pred: 0.0082; Loss self: 0.0000; time: 0.15s
Val loss: 0.6448 score: 0.7500 time: 0.08s
Test loss: 0.8373 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 054,   Train_Loss: 0.0268,   Val_Loss: 0.5972,   Val_Precision: 0.8333,   Val_Recall: 0.4545,   Val_accuracy: 0.5882,   Val_Score: 0.6818,   Val_Loss: 0.5972,   Test_Precision: 0.6667,   Test_Recall: 0.3636,   Test_accuracy: 0.4706,   Test_Score: 0.5814,   Test_loss: 0.8322


[0.06113242998253554, 0.11343773291446269, 0.14132661803159863, 0.05262730398681015, 0.10299825191032141, 0.09975217399187386, 0.16877738700713962, 0.1624851309461519, 0.11636502202600241, 0.09409018000587821, 0.06207356904633343, 0.10940970794763416, 0.10664870403707027, 0.1647164709866047, 0.11684316699393094, 0.05691265605855733, 0.09337871090974659, 0.06773503904696554, 0.08559224603231996, 0.08530414593406022, 0.08964969904627651, 0.05612020299304277, 0.13096557394601405, 0.08523298997897655, 0.0574891259893775, 0.096315000904724, 0.06727673800196499, 0.09511876595206559, 0.1354906519409269, 0.0956640390213579, 0.10692029097117484, 0.057214832049794495, 0.10857600998133421, 0.30633669695816934, 0.061056191101670265, 0.11729220196139067, 0.08619520300999284, 0.12362447008490562, 0.06313893501646817, 0.06579352100379765, 0.05526144301984459, 0.05524656700436026, 0.06239294190891087, 0.055379385012201965, 0.08134239399805665, 0.055422416888177395, 0.06589120195712894, 0.06261756492312998, 0.05979219090659171, 0.20870858302805573, 0.059032420977018774, 0.05577976198401302, 0.05597905605100095, 0.0723285130225122, 0.061976026045158505, 0.07805083098355681, 0.05573054892010987, 0.07030142203439027, 0.05925756797660142, 0.05513758200686425, 0.06310411402955651, 0.07644447998609394, 0.06898134900256991, 0.05919210298452526, 0.059863141970708966, 0.06278133403975517, 0.06279321492183954, 0.11637154500931501, 0.06062521191779524, 0.061195839080028236, 0.11759852699469775, 0.06806300801690668, 0.09276503999717534, 0.15575748099945486, 0.054418081999756396, 0.06446138094179332, 0.08846866199746728, 0.1539271930232644, 0.09635936201084405, 0.10416059393901378, 0.05648687807843089, 0.053203944000415504, 0.0888657970353961, 0.09133656194899231, 0.17951862304471433, 0.08701515302527696, 0.16244034201372415, 0.09165172302164137, 0.09208151802886277, 0.09559025801718235, 0.05495428992435336, 0.05273962905630469, 0.07859981199726462, 0.12235939293168485, 0.0570620991056785, 0.0812391460640356, 0.09362184989731759, 0.11074552789796144, 0.05395229393616319, 0.0577835509320721, 0.0899269999936223, 0.10386388609185815, 0.05707719502970576, 0.08095760096330196, 0.05673734494484961, 0.097058444051072, 0.12110627302899957, 0.11447730707004666, 0.09695881605148315, 0.05275801301468164, 0.08394310297444463, 0.12030472105834633, 0.16850648797117174, 0.09002174204215407, 0.08977780095301569, 0.05611190898343921, 0.07043686090037227, 0.10328210995066911, 0.05592938605695963, 0.07764384103938937, 0.07489345490466803, 0.08740920294076204, 0.05328547803219408, 0.10173921391833574, 0.1093389040324837, 0.05466619797516614, 0.0903754070168361, 0.16284860100131482, 0.053605639026500285, 0.06198699993547052, 0.12054554501082748, 0.053718492039479315, 0.053461486008018255, 0.08319251099601388, 0.10241719195619226, 0.052195356925949454, 0.10092775395605713, 0.09966422605793923, 0.05276789504569024, 0.09382707590702921, 0.0946229750989005, 0.052990154013969004, 0.07170598593074828, 0.09099068003706634, 0.22758207994047552, 0.06472716899588704, 0.07495637296233326, 0.05416708497796208, 0.14103591500315815, 0.08776511310134083, 0.08189427002798766, 0.11666148994117975, 0.051865482004359365, 0.05471009400207549, 0.06215855199843645, 0.09492775506805629, 0.07289848395157605, 0.06817008799407631, 0.05613096302840859, 0.054642326082102954, 0.076380446087569, 0.06685269204899669, 0.10498324502259493, 0.06330618599895388, 0.18480368796736002, 0.05459345004055649, 0.056064141914248466, 0.060348146012984216, 0.05774252291303128, 0.07316461100708693, 0.05923729296773672]
[0.0013893734086939896, 0.0025781302935105155, 0.0032119685916272415, 0.0011960750906093215, 0.002340869361598214, 0.0022670948634516785, 0.0038358497047077185, 0.0036928438851398164, 0.002644659591500055, 0.0021384131819517775, 0.0014107629328712144, 0.00248658427153714, 0.002423834182660688, 0.0037435561587864704, 0.0026555265225893395, 0.001293469455876303, 0.002122243429766968, 0.0015394327056128532, 0.0019452783189163629, 0.0019387305894104595, 0.002037493160142648, 0.0012754591589327902, 0.0029764903169548647, 0.0019371134086131033, 0.001306571045213125, 0.002188977293289182, 0.0015290167727719315, 0.002161790135274218, 0.00307933299865743, 0.002174182705030861, 0.0024300066129812462, 0.001300337092040784, 0.0024676365904848685, 0.006962197658140212, 0.0013876407068561423, 0.002665731862758879, 0.0019589818865907464, 0.0028096470473842187, 0.001434975795828822, 0.0014953072955408557, 0.0012559418868146497, 0.0012556037955536422, 0.0014180214070207016, 0.0012586223866409537, 0.0018486907726831057, 0.0012596003838222134, 0.001497527317207476, 0.0014231264755256814, 0.0013589134296952661, 0.004743376887001266, 0.0013416459312958812, 0.0012677218632730232, 0.0012722512738863852, 0.0016438298414207318, 0.001408546046480875, 0.001773882522353564, 0.0012666033845479515, 0.001597759591690688, 0.0013467629085591232, 0.0012531268637923693, 0.001434184409762648, 0.0017373745451384987, 0.001567757931876589, 0.0013452750678301197, 0.0013605259538797493, 0.0014268485009035264, 0.0014271185209508985, 0.0026448078411207957, 0.0013778457254044372, 0.0013908145245460962, 0.00267269379533404, 0.0015468865458387882, 0.0021082963635721667, 0.0035399427499876106, 0.0012367745909035545, 0.0014650313850407574, 0.002010651409033347, 0.003498345295983282, 0.0021899855002464556, 0.0023672862258866767, 0.001283792683600702, 0.0012091805454639887, 0.0020196772053499112, 0.002075830953386189, 0.00407996870556169, 0.00197761711421084, 0.003691825954857367, 0.002082993705037304, 0.002092761773383245, 0.0021725058640268717, 0.0012489611346443946, 0.0011986279330978339, 0.001786359363574196, 0.0027808952939019286, 0.0012968658887654203, 0.0018463442287280816, 0.0021772523231934323, 0.0025754773929758472, 0.0012547045101433299, 0.0013438035100481885, 0.0020913255812470303, 0.0024154392114385617, 0.0013273766285978085, 0.0018827349061233012, 0.0013194731382523165, 0.0022571731174667905, 0.0028164249541627806, 0.0026622629551173644, 0.002254856187243794, 0.0012269305352251543, 0.0019521651854522008, 0.002797784210659217, 0.003918755534213296, 0.0020935288847012574, 0.002087855836116644, 0.001304928115893935, 0.001638066532566797, 0.002401909533736491, 0.0013006833966734798, 0.0018056707218462645, 0.001741708253596931, 0.002032772161413071, 0.0012391971635393971, 0.0023660282306589707, 0.0025427652100577605, 0.0012713069296550266, 0.002101753651554328, 0.0037871767674724378, 0.0012466427680581462, 0.0014415581380341981, 0.0028033847676936625, 0.001249267256732077, 0.0012432903722794943, 0.0019347095580468344, 0.002381795161771913, 0.0012138455099058012, 0.002347157068745515, 0.0023177726990218426, 0.001227160349899773, 0.0021820250210937025, 0.002200534304625593, 0.0012323291631155583, 0.001667581068156937, 0.0021160623264434033, 0.0052926065102436165, 0.0015052829999043498, 0.0017431714642403085, 0.0012596996506502808, 0.003279905000073445, 0.0020410491418916474, 0.00190451790762762, 0.0027130579056088315, 0.0012061740001013807, 0.0012723277674901278, 0.001445547720893871, 0.0022076222108850297, 0.0016953135802692107, 0.00158535088358317, 0.0013053712332188043, 0.0012707517693512315, 0.0017762894438969534, 0.0015547137685813183, 0.002441470814478952, 0.0014722368836966018, 0.004297760185287442, 0.001269615117222244, 0.0013038172538197318, 0.001403445256115912, 0.001342849370070495, 0.0017015025815601613, 0.0013776114643659703]
[719.7489125259707, 387.87799147200906, 311.3355474915718, 836.0679089893644, 427.1917162080571, 441.09314352972785, 260.69843111233087, 270.79400892738755, 378.1204973275213, 467.63647383022476, 708.8363159391911, 402.158097534264, 412.56947655647025, 267.12568413135944, 376.5731546996278, 773.1145064593098, 471.19947974573523, 649.5899407320288, 514.0652575396308, 515.8014246342942, 490.7991936179008, 784.0313764626741, 335.96615258707186, 516.2320365723763, 765.3621314077737, 456.8343413454914, 654.015062363976, 462.579592571391, 324.74565122901413, 459.94294669260813, 411.52151383372285, 769.0313581923385, 405.2460576472117, 143.63280807329542, 720.6476395936911, 375.1315028980663, 510.4692426433402, 355.91659134943654, 696.8758657161976, 668.7588584514315, 796.2151835991585, 796.4295771812818, 705.2079715080077, 794.5194767024824, 540.9233468227061, 793.902584378019, 667.7674513909748, 702.6782349970794, 735.8820496933702, 210.82027083709002, 745.3531342908861, 788.8165606122664, 786.008252084723, 608.3354704984054, 709.9519412222338, 563.7351895621663, 789.5131279448603, 625.8763866607982, 742.5211918480006, 798.0038006476653, 697.2604033295106, 575.5811277413891, 637.8535739908591, 743.3424017981424, 735.009866697762, 700.8452539752943, 700.7126495238065, 378.09930250971576, 725.7706589077462, 719.0031325897737, 374.1543463548983, 646.4598213036737, 474.3166175677797, 282.490444232043, 808.554774131822, 682.5792335992719, 497.3512541792443, 285.8494274845243, 456.6240278245963, 422.42462658922705, 778.9419684144499, 827.0063587702516, 495.1286261740766, 481.7347955857171, 245.09991918242667, 505.6590544318008, 270.8686737207347, 480.0782631179825, 477.83747424980857, 460.2979520370266, 800.6654268587156, 834.2872482668718, 559.7977766350289, 359.5964228472912, 771.0897546638162, 541.610813650326, 459.29449212081863, 388.2775297221869, 797.0004028165691, 744.1564131382127, 478.16562326164103, 414.00338094388667, 753.3656826972787, 531.1422212164103, 757.87825535011, 443.0320351866909, 355.06005530946703, 375.6202962888448, 443.48726347037774, 815.0420674113304, 512.2517333328836, 357.425707168595, 255.18305269857908, 477.66238493656994, 478.9602724007867, 766.325736889311, 610.4758140885966, 416.3354139505689, 768.8266049659104, 553.8108293507239, 574.1489700900399, 491.9390470719824, 806.9740872742141, 422.6492258384789, 393.27264508911713, 786.5921097994435, 475.79315456902447, 264.0489370839165, 802.1544147387681, 693.6938397529111, 356.7116478351623, 800.4692307520104, 804.3173359145082, 516.8734479244209, 419.85138606800257, 823.8280669486544, 426.04732905006233, 431.44869228204504, 814.8894315903166, 458.2898868404209, 454.43508783206346, 811.4715044735395, 599.6709959685687, 472.57587241334323, 188.94282015195012, 664.3269073413724, 573.667031909457, 793.8400232815663, 304.8868793387636, 489.94410740801595, 525.0672603260838, 368.5877835237697, 829.0677795375698, 785.9609964912278, 691.7793065881204, 452.97605499226364, 589.8613752868087, 630.7751869667018, 766.0656022993434, 786.935752614013, 562.9713127192418, 643.2052125662356, 409.58916816436147, 679.2385186608867, 232.6793392109937, 787.640274942435, 766.9786521618327, 712.5322456591845, 744.6851614842724, 587.715828842921, 725.8940752647108]
Elapsed: 0.08703574380219223~0.03744898953257383
Time per graph: 0.0019975512571590464~0.0008566642475004334
Speed: 569.5118528754745~178.93835425087502
Total Time: 0.0600
best val loss: 0.5972374081611633 test_score: 0.5814

Testing...
Test loss: 0.7797 score: 0.6279 time: 0.23s
test Score 0.6279
Epoch Time List: [0.5694545840378851, 0.34612133994232863, 0.44936126994434744, 0.4647056310204789, 0.3055782449664548, 0.6012817961163819, 0.541969764046371, 0.4025109460344538, 0.4073727980721742, 0.337119824020192, 0.3988798060454428, 0.3387120591942221, 0.37164870102424175, 0.43003343301825225, 0.32545030303299427, 0.33141688897740096, 0.3431315190391615, 0.36248998704832047, 0.31434848392382264, 0.2873671550769359, 0.503582744859159, 0.35211948305368423, 0.3659928230335936, 0.3890576738631353, 0.38311899604741484, 0.30673852493055165, 0.30307944503147155, 0.32784671301487833, 0.3789958319393918, 0.3064443301409483, 0.3732401089509949, 0.28758417605422437, 0.4500546180643141, 0.5530716290231794, 0.41215661086607724, 0.3556946530006826, 0.31965220405254513, 0.4961317889392376, 0.2536585567286238, 0.25344654999207705, 0.2467510470887646, 0.28990242211148143, 0.2841140360105783, 0.2483561870176345, 0.33516570506617427, 0.25033975986298174, 0.2616872328799218, 0.26027403702028096, 0.24595566501375288, 0.4098585620522499, 0.3639565600315109, 0.2472788579761982, 0.24270789406728, 0.2644021598389372, 0.26380844390951097, 0.3152856188826263, 0.30708154710009694, 0.2546051051467657, 0.2475301680387929, 0.24750416784081608, 0.26056074199732393, 0.2797216458711773, 0.2746688270708546, 0.2918405419914052, 0.24385933799203485, 0.25820080505218357, 0.2529100519604981, 0.3152186118531972, 0.2702978750457987, 0.2583225261187181, 0.3574946220032871, 0.5026628320338205, 0.3142301777843386, 0.4062447580508888, 0.3992301808902994, 0.26715681108180434, 0.29537670500576496, 0.5225820419145748, 0.3902483769925311, 0.37906606611795723, 0.30269589193630964, 0.5089736121008173, 0.2865167971467599, 0.2953018631087616, 0.5171952330274507, 0.3021846718620509, 0.40847639611456543, 0.3624878420960158, 0.4020374509273097, 0.37277960195206106, 0.29150991793721914, 0.34071803803090006, 0.27902834699489176, 0.3785099539672956, 0.31327784003224224, 0.5444416018435732, 0.3061554579762742, 0.350044006947428, 0.28620257193688303, 0.3364454668480903, 0.3212006529793143, 0.38306200492661446, 0.48262620996683836, 0.3936747530242428, 0.3107836061390117, 0.33660195709671825, 0.3459550269180909, 0.34748315007891506, 0.42119870614260435, 0.3886190268676728, 0.2999012280488387, 0.3697442429838702, 0.4657567589310929, 0.31952746014576405, 0.34604463493451476, 0.4678365859435871, 0.4616390719311312, 0.39703974698204547, 0.39165430911816657, 0.281270264997147, 0.2881344669731334, 0.2936276299878955, 0.42879173904657364, 0.3032997730188072, 0.3133124361047521, 0.39189289696514606, 0.44846686406526715, 0.41652619594242424, 0.37954779900610447, 0.3124366579577327, 0.45399242802523077, 0.29697712091729045, 0.35521138284821063, 0.3154456769116223, 0.30212898296304047, 0.3633291310397908, 0.47378651495091617, 0.3159602879313752, 0.3689145991811529, 0.4776769569143653, 0.354720102972351, 0.4724950570380315, 0.27734893816523254, 0.44006995810195804, 0.4594946230063215, 0.41355666099116206, 0.4147341769421473, 0.2436891309916973, 0.3625479128677398, 0.32985865499358624, 0.3269039060687646, 0.3362277818378061, 0.35177848604507744, 0.22764932096470147, 0.2678069288376719, 0.34930602088570595, 0.29953924915753305, 0.3260038511361927, 0.7099362469743937, 0.24429349100682884, 0.26529812696389854, 0.2649286249652505, 0.3122087309602648, 0.43176960991695523, 0.6294775152346119, 0.2642007569083944, 0.2952941640978679, 0.2597958849510178, 0.279815916903317, 0.290477231843397, 0.2864235130837187]
Total Epoch List: [96, 75]
Total Time List: [0.0818010080838576, 0.05995860602706671]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdcbb2b0>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7655;  Loss pred: 0.7655; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7590 score: 0.5000 time: 0.09s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8218 score: 0.4884 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7970;  Loss pred: 0.7970; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7506 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8081 score: 0.4884 time: 0.10s
Epoch 3/1000, LR 0.000030
Train loss: 0.7529;  Loss pred: 0.7529; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7465 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8039 score: 0.4884 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.7139;  Loss pred: 0.7139; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7447 score: 0.5000 time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8031 score: 0.4884 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.7619;  Loss pred: 0.7619; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7417 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7986 score: 0.4884 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.7237;  Loss pred: 0.7237; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7315 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7861 score: 0.4884 time: 0.10s
Epoch 7/1000, LR 0.000150
Train loss: 0.7222;  Loss pred: 0.7222; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7232 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7737 score: 0.4884 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.6565;  Loss pred: 0.6565; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7137 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7608 score: 0.4884 time: 0.07s
Epoch 9/1000, LR 0.000210
Train loss: 0.6707;  Loss pred: 0.6707; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7033 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7459 score: 0.4884 time: 0.15s
Epoch 10/1000, LR 0.000240
Train loss: 0.7148;  Loss pred: 0.7148; Loss self: 0.0000; time: 0.33s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6938 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7334 score: 0.4884 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.6316;  Loss pred: 0.6316; Loss self: 0.0000; time: 0.21s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6867 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7216 score: 0.4884 time: 0.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.6273;  Loss pred: 0.6273; Loss self: 0.0000; time: 0.25s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6821 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7112 score: 0.4884 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.6097;  Loss pred: 0.6097; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6782 score: 0.5000 time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7037 score: 0.4884 time: 0.05s
Epoch 14/1000, LR 0.000270
Train loss: 0.6032;  Loss pred: 0.6032; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6723 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6977 score: 0.4884 time: 0.08s
Epoch 15/1000, LR 0.000270
Train loss: 0.5795;  Loss pred: 0.5795; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.6664 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6895 score: 0.4884 time: 0.14s
Epoch 16/1000, LR 0.000270
Train loss: 0.5354;  Loss pred: 0.5354; Loss self: 0.0000; time: 0.19s
Val loss: 0.6606 score: 0.5227 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6820 score: 0.4884 time: 0.05s
Epoch 17/1000, LR 0.000270
Train loss: 0.5390;  Loss pred: 0.5390; Loss self: 0.0000; time: 0.26s
Val loss: 0.6556 score: 0.5227 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6767 score: 0.4884 time: 0.09s
Epoch 18/1000, LR 0.000270
Train loss: 0.5118;  Loss pred: 0.5118; Loss self: 0.0000; time: 0.17s
Val loss: 0.6525 score: 0.5227 time: 0.05s
Test loss: 0.6716 score: 0.5116 time: 0.12s
Epoch 19/1000, LR 0.000270
Train loss: 0.4575;  Loss pred: 0.4575; Loss self: 0.0000; time: 0.19s
Val loss: 0.6487 score: 0.5227 time: 0.17s
Test loss: 0.6660 score: 0.5116 time: 0.04s
Epoch 20/1000, LR 0.000270
Train loss: 0.4252;  Loss pred: 0.4252; Loss self: 0.0000; time: 0.21s
Val loss: 0.6448 score: 0.5455 time: 0.06s
Test loss: 0.6599 score: 0.5116 time: 0.10s
Epoch 21/1000, LR 0.000270
Train loss: 0.4411;  Loss pred: 0.4411; Loss self: 0.0000; time: 0.22s
Val loss: 0.6411 score: 0.5455 time: 0.18s
Test loss: 0.6529 score: 0.5116 time: 0.05s
Epoch 22/1000, LR 0.000270
Train loss: 0.3707;  Loss pred: 0.3707; Loss self: 0.0000; time: 0.19s
Val loss: 0.6389 score: 0.5455 time: 0.22s
Test loss: 0.6472 score: 0.5349 time: 0.14s
Epoch 23/1000, LR 0.000270
Train loss: 0.3836;  Loss pred: 0.3836; Loss self: 0.0000; time: 0.24s
Val loss: 0.6386 score: 0.5455 time: 0.07s
Test loss: 0.6436 score: 0.5814 time: 0.08s
Epoch 24/1000, LR 0.000270
Train loss: 0.3661;  Loss pred: 0.3661; Loss self: 0.0000; time: 0.40s
Val loss: 0.6369 score: 0.5455 time: 0.16s
Test loss: 0.6403 score: 0.5814 time: 0.05s
Epoch 25/1000, LR 0.000270
Train loss: 0.3196;  Loss pred: 0.3196; Loss self: 0.0000; time: 0.20s
Val loss: 0.6347 score: 0.5455 time: 0.05s
Test loss: 0.6364 score: 0.5814 time: 0.11s
Epoch 26/1000, LR 0.000270
Train loss: 0.2867;  Loss pred: 0.2867; Loss self: 0.0000; time: 0.23s
Val loss: 0.6311 score: 0.5455 time: 0.16s
Test loss: 0.6304 score: 0.5581 time: 0.05s
Epoch 27/1000, LR 0.000270
Train loss: 0.2564;  Loss pred: 0.2564; Loss self: 0.0000; time: 0.32s
Val loss: 0.6289 score: 0.5455 time: 0.06s
Test loss: 0.6249 score: 0.5581 time: 0.08s
Epoch 28/1000, LR 0.000270
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 0.23s
Val loss: 0.6274 score: 0.5682 time: 0.05s
Test loss: 0.6192 score: 0.5581 time: 0.11s
Epoch 29/1000, LR 0.000270
Train loss: 0.2373;  Loss pred: 0.2373; Loss self: 0.0000; time: 0.25s
Val loss: 0.6276 score: 0.5909 time: 0.17s
Test loss: 0.6118 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2168;  Loss pred: 0.2168; Loss self: 0.0000; time: 0.17s
Val loss: 0.6279 score: 0.5682 time: 0.05s
Test loss: 0.6037 score: 0.6279 time: 0.07s
     INFO: Early stopping counter 2 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1977;  Loss pred: 0.1977; Loss self: 0.0000; time: 0.28s
Val loss: 0.6278 score: 0.5682 time: 0.06s
Test loss: 0.5970 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.1657;  Loss pred: 0.1657; Loss self: 0.0000; time: 0.20s
Val loss: 0.6265 score: 0.5682 time: 0.05s
Test loss: 0.5913 score: 0.6279 time: 0.05s
Epoch 33/1000, LR 0.000270
Train loss: 0.1537;  Loss pred: 0.1537; Loss self: 0.0000; time: 0.15s
Val loss: 0.6213 score: 0.5455 time: 0.17s
Test loss: 0.5862 score: 0.6279 time: 0.05s
Epoch 34/1000, LR 0.000270
Train loss: 0.1436;  Loss pred: 0.1436; Loss self: 0.0000; time: 0.22s
Val loss: 0.6112 score: 0.5682 time: 0.22s
Test loss: 0.5863 score: 0.6047 time: 0.09s
Epoch 35/1000, LR 0.000270
Train loss: 0.1550;  Loss pred: 0.1550; Loss self: 0.0000; time: 0.22s
Val loss: 0.6024 score: 0.5909 time: 0.05s
Test loss: 0.5864 score: 0.6047 time: 0.16s
Epoch 36/1000, LR 0.000270
Train loss: 0.1328;  Loss pred: 0.1328; Loss self: 0.0000; time: 0.16s
Val loss: 0.5943 score: 0.6136 time: 0.16s
Test loss: 0.5856 score: 0.6047 time: 0.05s
Epoch 37/1000, LR 0.000270
Train loss: 0.1234;  Loss pred: 0.1234; Loss self: 0.0000; time: 0.25s
Val loss: 0.5887 score: 0.6136 time: 0.11s
Test loss: 0.5850 score: 0.6047 time: 0.08s
Epoch 38/1000, LR 0.000270
Train loss: 0.1073;  Loss pred: 0.1073; Loss self: 0.0000; time: 0.23s
Val loss: 0.5868 score: 0.6136 time: 0.05s
Test loss: 0.5890 score: 0.5814 time: 0.09s
Epoch 39/1000, LR 0.000269
Train loss: 0.1060;  Loss pred: 0.1060; Loss self: 0.0000; time: 0.16s
Val loss: 0.5872 score: 0.6136 time: 0.16s
Test loss: 0.5944 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.0816;  Loss pred: 0.0816; Loss self: 0.0000; time: 0.20s
Val loss: 0.5858 score: 0.6136 time: 0.05s
Test loss: 0.5977 score: 0.5814 time: 0.10s
Epoch 41/1000, LR 0.000269
Train loss: 0.0820;  Loss pred: 0.0820; Loss self: 0.0000; time: 0.19s
Val loss: 0.5861 score: 0.6364 time: 0.07s
Test loss: 0.6020 score: 0.5814 time: 0.10s
     INFO: Early stopping counter 1 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0750;  Loss pred: 0.0750; Loss self: 0.0000; time: 0.18s
Val loss: 0.5866 score: 0.6364 time: 0.17s
Test loss: 0.6062 score: 0.6047 time: 0.05s
     INFO: Early stopping counter 2 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0658;  Loss pred: 0.0658; Loss self: 0.0000; time: 0.20s
Val loss: 0.5855 score: 0.6364 time: 0.05s
Test loss: 0.6084 score: 0.6279 time: 0.09s
Epoch 44/1000, LR 0.000269
Train loss: 0.0718;  Loss pred: 0.0718; Loss self: 0.0000; time: 0.19s
Val loss: 0.5825 score: 0.6136 time: 0.06s
Test loss: 0.6064 score: 0.6279 time: 0.09s
Epoch 45/1000, LR 0.000269
Train loss: 0.0483;  Loss pred: 0.0483; Loss self: 0.0000; time: 0.20s
Val loss: 0.5794 score: 0.6136 time: 0.16s
Test loss: 0.6047 score: 0.6279 time: 0.05s
Epoch 46/1000, LR 0.000269
Train loss: 0.0550;  Loss pred: 0.0550; Loss self: 0.0000; time: 0.22s
Val loss: 0.5764 score: 0.6136 time: 0.05s
Test loss: 0.6038 score: 0.6279 time: 0.09s
Epoch 47/1000, LR 0.000269
Train loss: 0.0447;  Loss pred: 0.0447; Loss self: 0.0000; time: 0.16s
Val loss: 0.5748 score: 0.6136 time: 0.05s
Test loss: 0.6072 score: 0.6744 time: 0.19s
Epoch 48/1000, LR 0.000269
Train loss: 0.0383;  Loss pred: 0.0383; Loss self: 0.0000; time: 0.24s
Val loss: 0.5751 score: 0.6364 time: 0.15s
Test loss: 0.6122 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 1 of 20
Epoch 49/1000, LR 0.000269
Train loss: 0.0329;  Loss pred: 0.0329; Loss self: 0.0000; time: 0.16s
Val loss: 0.5710 score: 0.6364 time: 0.17s
Test loss: 0.6143 score: 0.6512 time: 0.05s
Epoch 50/1000, LR 0.000269
Train loss: 0.0283;  Loss pred: 0.0283; Loss self: 0.0000; time: 0.21s
Val loss: 0.5644 score: 0.6591 time: 0.06s
Test loss: 0.6128 score: 0.6512 time: 0.12s
Epoch 51/1000, LR 0.000269
Train loss: 0.0298;  Loss pred: 0.0298; Loss self: 0.0000; time: 0.23s
Val loss: 0.5543 score: 0.6818 time: 0.05s
Test loss: 0.6074 score: 0.6512 time: 0.15s
Epoch 52/1000, LR 0.000269
Train loss: 0.0341;  Loss pred: 0.0341; Loss self: 0.0000; time: 0.15s
Val loss: 0.5406 score: 0.6818 time: 0.14s
Test loss: 0.5983 score: 0.6512 time: 0.05s
Epoch 53/1000, LR 0.000269
Train loss: 0.0233;  Loss pred: 0.0233; Loss self: 0.0000; time: 0.21s
Val loss: 0.5292 score: 0.7045 time: 0.05s
Test loss: 0.5905 score: 0.6744 time: 0.11s
Epoch 54/1000, LR 0.000269
Train loss: 0.0191;  Loss pred: 0.0191; Loss self: 0.0000; time: 0.19s
Val loss: 0.5196 score: 0.7273 time: 0.19s
Test loss: 0.5855 score: 0.6744 time: 0.04s
Epoch 55/1000, LR 0.000269
Train loss: 0.0251;  Loss pred: 0.0251; Loss self: 0.0000; time: 0.16s
Val loss: 0.5163 score: 0.7273 time: 0.15s
Test loss: 0.5919 score: 0.6744 time: 0.05s
Epoch 56/1000, LR 0.000269
Train loss: 0.0232;  Loss pred: 0.0232; Loss self: 0.0000; time: 0.19s
Val loss: 0.5165 score: 0.7273 time: 0.05s
Test loss: 0.6029 score: 0.6744 time: 0.09s
     INFO: Early stopping counter 1 of 20
Epoch 57/1000, LR 0.000269
Train loss: 0.0127;  Loss pred: 0.0127; Loss self: 0.0000; time: 0.24s
Val loss: 0.5164 score: 0.7273 time: 0.11s
Test loss: 0.6126 score: 0.6744 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 58/1000, LR 0.000269
Train loss: 0.0210;  Loss pred: 0.0210; Loss self: 0.0000; time: 0.16s
Val loss: 0.5060 score: 0.7273 time: 0.16s
Test loss: 0.6102 score: 0.6744 time: 0.05s
Epoch 59/1000, LR 0.000268
Train loss: 0.0140;  Loss pred: 0.0140; Loss self: 0.0000; time: 0.17s
Val loss: 0.4948 score: 0.7045 time: 0.05s
Test loss: 0.6074 score: 0.6744 time: 0.09s
Epoch 60/1000, LR 0.000268
Train loss: 0.0110;  Loss pred: 0.0110; Loss self: 0.0000; time: 0.21s
Val loss: 0.4771 score: 0.7273 time: 0.05s
Test loss: 0.5960 score: 0.6744 time: 0.20s
Epoch 61/1000, LR 0.000268
Train loss: 0.0086;  Loss pred: 0.0086; Loss self: 0.0000; time: 0.32s
Val loss: 0.4584 score: 0.7500 time: 0.13s
Test loss: 0.5823 score: 0.6977 time: 0.05s
Epoch 62/1000, LR 0.000268
Train loss: 0.0112;  Loss pred: 0.0112; Loss self: 0.0000; time: 0.34s
Val loss: 0.4444 score: 0.7500 time: 0.06s
Test loss: 0.5744 score: 0.6977 time: 0.08s
Epoch 63/1000, LR 0.000268
Train loss: 0.0081;  Loss pred: 0.0081; Loss self: 0.0000; time: 0.21s
Val loss: 0.4334 score: 0.7500 time: 0.05s
Test loss: 0.5701 score: 0.7209 time: 0.13s
Epoch 64/1000, LR 0.000268
Train loss: 0.0099;  Loss pred: 0.0099; Loss self: 0.0000; time: 0.23s
Val loss: 0.4184 score: 0.7500 time: 0.16s
Test loss: 0.5591 score: 0.7442 time: 0.05s
Epoch 65/1000, LR 0.000268
Train loss: 0.0073;  Loss pred: 0.0073; Loss self: 0.0000; time: 0.22s
Val loss: 0.4084 score: 0.7500 time: 0.05s
Test loss: 0.5554 score: 0.7209 time: 0.11s
Epoch 66/1000, LR 0.000268
Train loss: 0.0088;  Loss pred: 0.0088; Loss self: 0.0000; time: 0.19s
Val loss: 0.3990 score: 0.7500 time: 0.18s
Test loss: 0.5552 score: 0.7209 time: 0.05s
Epoch 67/1000, LR 0.000268
Train loss: 0.0053;  Loss pred: 0.0053; Loss self: 0.0000; time: 0.16s
Val loss: 0.3933 score: 0.7273 time: 0.15s
Test loss: 0.5593 score: 0.7209 time: 0.05s
Epoch 68/1000, LR 0.000268
Train loss: 0.0083;  Loss pred: 0.0083; Loss self: 0.0000; time: 0.17s
Val loss: 0.3792 score: 0.7273 time: 0.07s
Test loss: 0.5498 score: 0.6977 time: 0.05s
Epoch 69/1000, LR 0.000268
Train loss: 0.0060;  Loss pred: 0.0060; Loss self: 0.0000; time: 0.21s
Val loss: 0.3644 score: 0.7500 time: 0.05s
Test loss: 0.5397 score: 0.6744 time: 0.09s
Epoch 70/1000, LR 0.000268
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.24s
Val loss: 0.3558 score: 0.7727 time: 0.17s
Test loss: 0.5396 score: 0.6744 time: 0.05s
Epoch 71/1000, LR 0.000268
Train loss: 0.0047;  Loss pred: 0.0047; Loss self: 0.0000; time: 0.15s
Val loss: 0.3560 score: 0.7727 time: 0.06s
Test loss: 0.5534 score: 0.6744 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 72/1000, LR 0.000267
Train loss: 0.0051;  Loss pred: 0.0051; Loss self: 0.0000; time: 0.21s
Val loss: 0.3597 score: 0.7727 time: 0.05s
Test loss: 0.5733 score: 0.6744 time: 0.09s
     INFO: Early stopping counter 2 of 20
Epoch 73/1000, LR 0.000267
Train loss: 0.0048;  Loss pred: 0.0048; Loss self: 0.0000; time: 0.41s
Val loss: 0.3682 score: 0.7727 time: 0.30s
Test loss: 0.6002 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 74/1000, LR 0.000267
Train loss: 0.0042;  Loss pred: 0.0042; Loss self: 0.0000; time: 0.20s
Val loss: 0.3759 score: 0.7727 time: 0.06s
Test loss: 0.6277 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 75/1000, LR 0.000267
Train loss: 0.0031;  Loss pred: 0.0031; Loss self: 0.0000; time: 0.16s
Val loss: 0.3826 score: 0.7727 time: 0.05s
Test loss: 0.6532 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 76/1000, LR 0.000267
Train loss: 0.0034;  Loss pred: 0.0034; Loss self: 0.0000; time: 0.19s
Val loss: 0.3942 score: 0.7727 time: 0.05s
Test loss: 0.6849 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 6 of 20
Epoch 77/1000, LR 0.000267
Train loss: 0.0033;  Loss pred: 0.0033; Loss self: 0.0000; time: 0.15s
Val loss: 0.4087 score: 0.7500 time: 0.06s
Test loss: 0.7182 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 78/1000, LR 0.000267
Train loss: 0.0030;  Loss pred: 0.0030; Loss self: 0.0000; time: 0.15s
Val loss: 0.4380 score: 0.7500 time: 0.06s
Test loss: 0.7759 score: 0.6512 time: 0.07s
     INFO: Early stopping counter 8 of 20
Epoch 79/1000, LR 0.000267
Train loss: 0.0027;  Loss pred: 0.0027; Loss self: 0.0000; time: 0.16s
Val loss: 0.4563 score: 0.7500 time: 0.05s
Test loss: 0.8126 score: 0.6512 time: 0.06s
     INFO: Early stopping counter 9 of 20
Epoch 80/1000, LR 0.000267
Train loss: 0.0038;  Loss pred: 0.0038; Loss self: 0.0000; time: 0.15s
Val loss: 0.4573 score: 0.7500 time: 0.06s
Test loss: 0.8245 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 81/1000, LR 0.000267
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.16s
Val loss: 0.4496 score: 0.7500 time: 0.06s
Test loss: 0.8224 score: 0.6512 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 82/1000, LR 0.000267
Train loss: 0.0026;  Loss pred: 0.0026; Loss self: 0.0000; time: 0.16s
Val loss: 0.4367 score: 0.7727 time: 0.05s
Test loss: 0.8128 score: 0.6279 time: 0.05s
     INFO: Early stopping counter 12 of 20
Epoch 83/1000, LR 0.000266
Train loss: 0.0020;  Loss pred: 0.0020; Loss self: 0.0000; time: 0.15s
Val loss: 0.4230 score: 0.7727 time: 0.07s
Test loss: 0.8017 score: 0.6744 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 84/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.18s
Val loss: 0.4027 score: 0.7727 time: 0.08s
Test loss: 0.7787 score: 0.6744 time: 0.13s
     INFO: Early stopping counter 14 of 20
Epoch 85/1000, LR 0.000266
Train loss: 0.0028;  Loss pred: 0.0028; Loss self: 0.0000; time: 0.16s
Val loss: 0.3857 score: 0.7727 time: 0.05s
Test loss: 0.7586 score: 0.6977 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 86/1000, LR 0.000266
Train loss: 0.0018;  Loss pred: 0.0018; Loss self: 0.0000; time: 0.16s
Val loss: 0.3717 score: 0.8409 time: 0.05s
Test loss: 0.7449 score: 0.7209 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 87/1000, LR 0.000266
Train loss: 0.0019;  Loss pred: 0.0019; Loss self: 0.0000; time: 0.17s
Val loss: 0.3652 score: 0.8409 time: 0.05s
Test loss: 0.7415 score: 0.7674 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 88/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.18s
Val loss: 0.3612 score: 0.8409 time: 0.08s
Test loss: 0.7422 score: 0.7674 time: 0.06s
     INFO: Early stopping counter 18 of 20
Epoch 89/1000, LR 0.000266
Train loss: 0.0024;  Loss pred: 0.0024; Loss self: 0.0000; time: 0.29s
Val loss: 0.3589 score: 0.8409 time: 0.05s
Test loss: 0.7431 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 90/1000, LR 0.000266
Train loss: 0.0016;  Loss pred: 0.0016; Loss self: 0.0000; time: 0.26s
Val loss: 0.3606 score: 0.8409 time: 0.07s
Test loss: 0.7504 score: 0.7674 time: 0.05s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 069,   Train_Loss: 0.0048,   Val_Loss: 0.3558,   Val_Precision: 0.7308,   Val_Recall: 0.8636,   Val_accuracy: 0.7917,   Val_Score: 0.7727,   Val_Loss: 0.3558,   Test_Precision: 0.6296,   Test_Recall: 0.8095,   Test_accuracy: 0.7083,   Test_Score: 0.6744,   Test_loss: 0.5396


[0.06113242998253554, 0.11343773291446269, 0.14132661803159863, 0.05262730398681015, 0.10299825191032141, 0.09975217399187386, 0.16877738700713962, 0.1624851309461519, 0.11636502202600241, 0.09409018000587821, 0.06207356904633343, 0.10940970794763416, 0.10664870403707027, 0.1647164709866047, 0.11684316699393094, 0.05691265605855733, 0.09337871090974659, 0.06773503904696554, 0.08559224603231996, 0.08530414593406022, 0.08964969904627651, 0.05612020299304277, 0.13096557394601405, 0.08523298997897655, 0.0574891259893775, 0.096315000904724, 0.06727673800196499, 0.09511876595206559, 0.1354906519409269, 0.0956640390213579, 0.10692029097117484, 0.057214832049794495, 0.10857600998133421, 0.30633669695816934, 0.061056191101670265, 0.11729220196139067, 0.08619520300999284, 0.12362447008490562, 0.06313893501646817, 0.06579352100379765, 0.05526144301984459, 0.05524656700436026, 0.06239294190891087, 0.055379385012201965, 0.08134239399805665, 0.055422416888177395, 0.06589120195712894, 0.06261756492312998, 0.05979219090659171, 0.20870858302805573, 0.059032420977018774, 0.05577976198401302, 0.05597905605100095, 0.0723285130225122, 0.061976026045158505, 0.07805083098355681, 0.05573054892010987, 0.07030142203439027, 0.05925756797660142, 0.05513758200686425, 0.06310411402955651, 0.07644447998609394, 0.06898134900256991, 0.05919210298452526, 0.059863141970708966, 0.06278133403975517, 0.06279321492183954, 0.11637154500931501, 0.06062521191779524, 0.061195839080028236, 0.11759852699469775, 0.06806300801690668, 0.09276503999717534, 0.15575748099945486, 0.054418081999756396, 0.06446138094179332, 0.08846866199746728, 0.1539271930232644, 0.09635936201084405, 0.10416059393901378, 0.05648687807843089, 0.053203944000415504, 0.0888657970353961, 0.09133656194899231, 0.17951862304471433, 0.08701515302527696, 0.16244034201372415, 0.09165172302164137, 0.09208151802886277, 0.09559025801718235, 0.05495428992435336, 0.05273962905630469, 0.07859981199726462, 0.12235939293168485, 0.0570620991056785, 0.0812391460640356, 0.09362184989731759, 0.11074552789796144, 0.05395229393616319, 0.0577835509320721, 0.0899269999936223, 0.10386388609185815, 0.05707719502970576, 0.08095760096330196, 0.05673734494484961, 0.097058444051072, 0.12110627302899957, 0.11447730707004666, 0.09695881605148315, 0.05275801301468164, 0.08394310297444463, 0.12030472105834633, 0.16850648797117174, 0.09002174204215407, 0.08977780095301569, 0.05611190898343921, 0.07043686090037227, 0.10328210995066911, 0.05592938605695963, 0.07764384103938937, 0.07489345490466803, 0.08740920294076204, 0.05328547803219408, 0.10173921391833574, 0.1093389040324837, 0.05466619797516614, 0.0903754070168361, 0.16284860100131482, 0.053605639026500285, 0.06198699993547052, 0.12054554501082748, 0.053718492039479315, 0.053461486008018255, 0.08319251099601388, 0.10241719195619226, 0.052195356925949454, 0.10092775395605713, 0.09966422605793923, 0.05276789504569024, 0.09382707590702921, 0.0946229750989005, 0.052990154013969004, 0.07170598593074828, 0.09099068003706634, 0.22758207994047552, 0.06472716899588704, 0.07495637296233326, 0.05416708497796208, 0.14103591500315815, 0.08776511310134083, 0.08189427002798766, 0.11666148994117975, 0.051865482004359365, 0.05471009400207549, 0.06215855199843645, 0.09492775506805629, 0.07289848395157605, 0.06817008799407631, 0.05613096302840859, 0.054642326082102954, 0.076380446087569, 0.06685269204899669, 0.10498324502259493, 0.06330618599895388, 0.18480368796736002, 0.05459345004055649, 0.056064141914248466, 0.060348146012984216, 0.05774252291303128, 0.07316461100708693, 0.05923729296773672, 0.058143839007243514, 0.10445100802462548, 0.0964065189473331, 0.04975049092900008, 0.10600805992726237, 0.10016625304706395, 0.05248830805066973, 0.073904337012209, 0.1581226799171418, 0.05475709598977119, 0.10836132802069187, 0.08020609593950212, 0.05732075893320143, 0.09034238802269101, 0.14701237890403718, 0.05684423400089145, 0.09242329595144838, 0.12153555999975652, 0.04953298706095666, 0.10350690607447177, 0.056616154965013266, 0.1413025869987905, 0.08404149196576327, 0.05935637804213911, 0.11151351407170296, 0.05215432506520301, 0.08426434302236885, 0.11589859996456653, 0.05081855796743184, 0.08024448400828987, 0.05771419208031148, 0.05082803301047534, 0.05217485490720719, 0.09687861904967576, 0.16476012207567692, 0.05309417610988021, 0.08042312099132687, 0.09393952996470034, 0.0547488359734416, 0.10797428910154849, 0.10435703699477017, 0.05256807093974203, 0.09348076197784394, 0.09401438699569553, 0.05255389295052737, 0.09268936200533062, 0.19794064399320632, 0.0512009309604764, 0.052133581950329244, 0.11973676295019686, 0.15588907897472382, 0.05498743592761457, 0.1125195570057258, 0.04937679704744369, 0.054042703937739134, 0.09374155197292566, 0.10024664702359587, 0.05340496904682368, 0.09454967093188316, 0.20704217394813895, 0.05252710392232984, 0.0853484789840877, 0.13577304093632847, 0.05514857603702694, 0.1130943720927462, 0.05198014003690332, 0.05272294697351754, 0.058167216018773615, 0.09611310693435371, 0.0517123460303992, 0.08691842400003225, 0.09776253497693688, 0.05271652399096638, 0.05688222695607692, 0.061511787003837526, 0.052868782076984644, 0.057839445071294904, 0.07824694202281535, 0.0689397769747302, 0.053556442027911544, 0.052401817054487765, 0.0594359670067206, 0.05559030896984041, 0.13135732500813901, 0.06764979101717472, 0.05192926898598671, 0.08228416100610048, 0.06101885193493217, 0.054986506002023816, 0.05559359397739172]
[0.0013893734086939896, 0.0025781302935105155, 0.0032119685916272415, 0.0011960750906093215, 0.002340869361598214, 0.0022670948634516785, 0.0038358497047077185, 0.0036928438851398164, 0.002644659591500055, 0.0021384131819517775, 0.0014107629328712144, 0.00248658427153714, 0.002423834182660688, 0.0037435561587864704, 0.0026555265225893395, 0.001293469455876303, 0.002122243429766968, 0.0015394327056128532, 0.0019452783189163629, 0.0019387305894104595, 0.002037493160142648, 0.0012754591589327902, 0.0029764903169548647, 0.0019371134086131033, 0.001306571045213125, 0.002188977293289182, 0.0015290167727719315, 0.002161790135274218, 0.00307933299865743, 0.002174182705030861, 0.0024300066129812462, 0.001300337092040784, 0.0024676365904848685, 0.006962197658140212, 0.0013876407068561423, 0.002665731862758879, 0.0019589818865907464, 0.0028096470473842187, 0.001434975795828822, 0.0014953072955408557, 0.0012559418868146497, 0.0012556037955536422, 0.0014180214070207016, 0.0012586223866409537, 0.0018486907726831057, 0.0012596003838222134, 0.001497527317207476, 0.0014231264755256814, 0.0013589134296952661, 0.004743376887001266, 0.0013416459312958812, 0.0012677218632730232, 0.0012722512738863852, 0.0016438298414207318, 0.001408546046480875, 0.001773882522353564, 0.0012666033845479515, 0.001597759591690688, 0.0013467629085591232, 0.0012531268637923693, 0.001434184409762648, 0.0017373745451384987, 0.001567757931876589, 0.0013452750678301197, 0.0013605259538797493, 0.0014268485009035264, 0.0014271185209508985, 0.0026448078411207957, 0.0013778457254044372, 0.0013908145245460962, 0.00267269379533404, 0.0015468865458387882, 0.0021082963635721667, 0.0035399427499876106, 0.0012367745909035545, 0.0014650313850407574, 0.002010651409033347, 0.003498345295983282, 0.0021899855002464556, 0.0023672862258866767, 0.001283792683600702, 0.0012091805454639887, 0.0020196772053499112, 0.002075830953386189, 0.00407996870556169, 0.00197761711421084, 0.003691825954857367, 0.002082993705037304, 0.002092761773383245, 0.0021725058640268717, 0.0012489611346443946, 0.0011986279330978339, 0.001786359363574196, 0.0027808952939019286, 0.0012968658887654203, 0.0018463442287280816, 0.0021772523231934323, 0.0025754773929758472, 0.0012547045101433299, 0.0013438035100481885, 0.0020913255812470303, 0.0024154392114385617, 0.0013273766285978085, 0.0018827349061233012, 0.0013194731382523165, 0.0022571731174667905, 0.0028164249541627806, 0.0026622629551173644, 0.002254856187243794, 0.0012269305352251543, 0.0019521651854522008, 0.002797784210659217, 0.003918755534213296, 0.0020935288847012574, 0.002087855836116644, 0.001304928115893935, 0.001638066532566797, 0.002401909533736491, 0.0013006833966734798, 0.0018056707218462645, 0.001741708253596931, 0.002032772161413071, 0.0012391971635393971, 0.0023660282306589707, 0.0025427652100577605, 0.0012713069296550266, 0.002101753651554328, 0.0037871767674724378, 0.0012466427680581462, 0.0014415581380341981, 0.0028033847676936625, 0.001249267256732077, 0.0012432903722794943, 0.0019347095580468344, 0.002381795161771913, 0.0012138455099058012, 0.002347157068745515, 0.0023177726990218426, 0.001227160349899773, 0.0021820250210937025, 0.002200534304625593, 0.0012323291631155583, 0.001667581068156937, 0.0021160623264434033, 0.0052926065102436165, 0.0015052829999043498, 0.0017431714642403085, 0.0012596996506502808, 0.003279905000073445, 0.0020410491418916474, 0.00190451790762762, 0.0027130579056088315, 0.0012061740001013807, 0.0012723277674901278, 0.001445547720893871, 0.0022076222108850297, 0.0016953135802692107, 0.00158535088358317, 0.0013053712332188043, 0.0012707517693512315, 0.0017762894438969534, 0.0015547137685813183, 0.002441470814478952, 0.0014722368836966018, 0.004297760185287442, 0.001269615117222244, 0.0013038172538197318, 0.001403445256115912, 0.001342849370070495, 0.0017015025815601613, 0.0013776114643659703, 0.0013521823024940352, 0.0024290932098750113, 0.00224201206854263, 0.0011569881611395367, 0.00246530371923866, 0.002329447745280557, 0.001220658326759761, 0.0017187055119118372, 0.003677271625980042, 0.0012734208369714229, 0.0025200308842021363, 0.0018652580451047004, 0.001333040905423289, 0.0021009857679695583, 0.0034188925326520273, 0.0013219589302532894, 0.0021493789756150787, 0.002826408372087361, 0.001151929931650155, 0.002407137350569111, 0.0013166547666282156, 0.0032861066743904766, 0.0019544533015293784, 0.0013803808847009096, 0.0025933375365512316, 0.0012128912805861166, 0.001959635884241136, 0.0026953162782457334, 0.0011818269294751591, 0.001866150790890462, 0.0013421905134956157, 0.00118204727931338, 0.0012133687187722603, 0.0022529911406901342, 0.0038316307459459746, 0.0012347482816251211, 0.001870305139333183, 0.0021846402317372174, 0.0012732287435684093, 0.002511029979105779, 0.0024269078370876785, 0.0012225132776684192, 0.002173971208787068, 0.002186381092923152, 0.0012221835569890087, 0.002155566558263503, 0.004603270790539682, 0.0011907193246622418, 0.0012124088825657964, 0.002784575882562718, 0.0036253274180168328, 0.0012787775797119668, 0.0026167338838540884, 0.0011482976057545044, 0.0012568070683195147, 0.00218003609239362, 0.0023313173726417645, 0.0012419760243447368, 0.002198829556555422, 0.004814934277863696, 0.0012215605563332523, 0.0019848483484671557, 0.003157512579914616, 0.0012825250241169056, 0.002630101676575493, 0.0012088404659744958, 0.0012261150458957567, 0.0013527259539249678, 0.002235188533357063, 0.0012026126983813769, 0.0020213586976751685, 0.0022735473250450437, 0.0012259656742085205, 0.001322842487350626, 0.0014305066745078495, 0.0012295065599298755, 0.0013451033737510443, 0.0018196963261119848, 0.0016032506273193068, 0.0012454986518118964, 0.0012186469082439014, 0.0013822317908539674, 0.0012927978830195445, 0.0030548215118171864, 0.0015732509538877843, 0.0012076574182787606, 0.0019135851396767552, 0.0014190430682542365, 0.0012787559535354376, 0.0012928742785439935]
[719.7489125259707, 387.87799147200906, 311.3355474915718, 836.0679089893644, 427.1917162080571, 441.09314352972785, 260.69843111233087, 270.79400892738755, 378.1204973275213, 467.63647383022476, 708.8363159391911, 402.158097534264, 412.56947655647025, 267.12568413135944, 376.5731546996278, 773.1145064593098, 471.19947974573523, 649.5899407320288, 514.0652575396308, 515.8014246342942, 490.7991936179008, 784.0313764626741, 335.96615258707186, 516.2320365723763, 765.3621314077737, 456.8343413454914, 654.015062363976, 462.579592571391, 324.74565122901413, 459.94294669260813, 411.52151383372285, 769.0313581923385, 405.2460576472117, 143.63280807329542, 720.6476395936911, 375.1315028980663, 510.4692426433402, 355.91659134943654, 696.8758657161976, 668.7588584514315, 796.2151835991585, 796.4295771812818, 705.2079715080077, 794.5194767024824, 540.9233468227061, 793.902584378019, 667.7674513909748, 702.6782349970794, 735.8820496933702, 210.82027083709002, 745.3531342908861, 788.8165606122664, 786.008252084723, 608.3354704984054, 709.9519412222338, 563.7351895621663, 789.5131279448603, 625.8763866607982, 742.5211918480006, 798.0038006476653, 697.2604033295106, 575.5811277413891, 637.8535739908591, 743.3424017981424, 735.009866697762, 700.8452539752943, 700.7126495238065, 378.09930250971576, 725.7706589077462, 719.0031325897737, 374.1543463548983, 646.4598213036737, 474.3166175677797, 282.490444232043, 808.554774131822, 682.5792335992719, 497.3512541792443, 285.8494274845243, 456.6240278245963, 422.42462658922705, 778.9419684144499, 827.0063587702516, 495.1286261740766, 481.7347955857171, 245.09991918242667, 505.6590544318008, 270.8686737207347, 480.0782631179825, 477.83747424980857, 460.2979520370266, 800.6654268587156, 834.2872482668718, 559.7977766350289, 359.5964228472912, 771.0897546638162, 541.610813650326, 459.29449212081863, 388.2775297221869, 797.0004028165691, 744.1564131382127, 478.16562326164103, 414.00338094388667, 753.3656826972787, 531.1422212164103, 757.87825535011, 443.0320351866909, 355.06005530946703, 375.6202962888448, 443.48726347037774, 815.0420674113304, 512.2517333328836, 357.425707168595, 255.18305269857908, 477.66238493656994, 478.9602724007867, 766.325736889311, 610.4758140885966, 416.3354139505689, 768.8266049659104, 553.8108293507239, 574.1489700900399, 491.9390470719824, 806.9740872742141, 422.6492258384789, 393.27264508911713, 786.5921097994435, 475.79315456902447, 264.0489370839165, 802.1544147387681, 693.6938397529111, 356.7116478351623, 800.4692307520104, 804.3173359145082, 516.8734479244209, 419.85138606800257, 823.8280669486544, 426.04732905006233, 431.44869228204504, 814.8894315903166, 458.2898868404209, 454.43508783206346, 811.4715044735395, 599.6709959685687, 472.57587241334323, 188.94282015195012, 664.3269073413724, 573.667031909457, 793.8400232815663, 304.8868793387636, 489.94410740801595, 525.0672603260838, 368.5877835237697, 829.0677795375698, 785.9609964912278, 691.7793065881204, 452.97605499226364, 589.8613752868087, 630.7751869667018, 766.0656022993434, 786.935752614013, 562.9713127192418, 643.2052125662356, 409.58916816436147, 679.2385186608867, 232.6793392109937, 787.640274942435, 766.9786521618327, 712.5322456591845, 744.6851614842724, 587.715828842921, 725.8940752647108, 739.5452507813096, 411.67625677544703, 446.0279291226241, 864.3130790682279, 405.6295344854394, 429.2862984482018, 819.230064693452, 581.833241977347, 271.9407489332493, 785.2863491524924, 396.82053353747233, 536.1188510214242, 750.1645267835675, 475.9670509174478, 292.4923759520169, 756.4531522990645, 465.2506660505667, 353.8059149115382, 868.1083567014243, 415.4312174016882, 759.5005352548658, 304.31148440593, 511.6520303746784, 724.4377338771047, 385.6034881328472, 824.4762049214838, 510.29888156352445, 371.0139726722006, 846.1475830848539, 535.8623777250256, 745.0507136990478, 845.989849560733, 824.1517887586915, 443.8543862598949, 260.9854827629311, 809.8816697147731, 534.6721125711757, 457.7412726693237, 785.4048261566528, 398.2429554091255, 412.0469614536386, 817.9870257991821, 459.987692549955, 457.3768055517797, 818.2077023386048, 463.915157788302, 217.23683995630446, 839.8284795484099, 824.8042507604532, 359.12111652697143, 275.83715474367585, 781.996819357156, 382.15578824054427, 870.8543804225183, 795.6670719055605, 458.70800189460505, 428.94202725681924, 805.1685220957444, 454.7874104287331, 207.687154650776, 818.624991463126, 503.8168285109906, 316.70499315224953, 779.7118817923722, 380.2134377185157, 827.2390180071106, 815.5841520315372, 739.2480325364315, 447.38955353268796, 831.5229012182577, 494.71674728000187, 439.84129513564795, 815.6835228242396, 755.9478997403453, 699.0530123489563, 813.3344160904962, 743.4372848321195, 549.5422426535469, 623.7327982038818, 802.8912745471416, 820.5822320109303, 723.467660501559, 773.5161181300311, 327.3513677089244, 635.6265016263434, 828.0493994938327, 522.5793089974147, 704.7002465050197, 782.0100443992087, 773.4704113119011]
Elapsed: 0.08533217143615301~0.03652801594975806
Time per graph: 0.001967083573781657~0.0008392211826107685
Speed: 580.6631935933699~186.8148592083791
Total Time: 0.0567
best val loss: 0.35582178831100464 test_score: 0.6744

Testing...
Test loss: 0.7449 score: 0.7209 time: 0.05s
test Score 0.7209
Epoch Time List: [0.5694545840378851, 0.34612133994232863, 0.44936126994434744, 0.4647056310204789, 0.3055782449664548, 0.6012817961163819, 0.541969764046371, 0.4025109460344538, 0.4073727980721742, 0.337119824020192, 0.3988798060454428, 0.3387120591942221, 0.37164870102424175, 0.43003343301825225, 0.32545030303299427, 0.33141688897740096, 0.3431315190391615, 0.36248998704832047, 0.31434848392382264, 0.2873671550769359, 0.503582744859159, 0.35211948305368423, 0.3659928230335936, 0.3890576738631353, 0.38311899604741484, 0.30673852493055165, 0.30307944503147155, 0.32784671301487833, 0.3789958319393918, 0.3064443301409483, 0.3732401089509949, 0.28758417605422437, 0.4500546180643141, 0.5530716290231794, 0.41215661086607724, 0.3556946530006826, 0.31965220405254513, 0.4961317889392376, 0.2536585567286238, 0.25344654999207705, 0.2467510470887646, 0.28990242211148143, 0.2841140360105783, 0.2483561870176345, 0.33516570506617427, 0.25033975986298174, 0.2616872328799218, 0.26027403702028096, 0.24595566501375288, 0.4098585620522499, 0.3639565600315109, 0.2472788579761982, 0.24270789406728, 0.2644021598389372, 0.26380844390951097, 0.3152856188826263, 0.30708154710009694, 0.2546051051467657, 0.2475301680387929, 0.24750416784081608, 0.26056074199732393, 0.2797216458711773, 0.2746688270708546, 0.2918405419914052, 0.24385933799203485, 0.25820080505218357, 0.2529100519604981, 0.3152186118531972, 0.2702978750457987, 0.2583225261187181, 0.3574946220032871, 0.5026628320338205, 0.3142301777843386, 0.4062447580508888, 0.3992301808902994, 0.26715681108180434, 0.29537670500576496, 0.5225820419145748, 0.3902483769925311, 0.37906606611795723, 0.30269589193630964, 0.5089736121008173, 0.2865167971467599, 0.2953018631087616, 0.5171952330274507, 0.3021846718620509, 0.40847639611456543, 0.3624878420960158, 0.4020374509273097, 0.37277960195206106, 0.29150991793721914, 0.34071803803090006, 0.27902834699489176, 0.3785099539672956, 0.31327784003224224, 0.5444416018435732, 0.3061554579762742, 0.350044006947428, 0.28620257193688303, 0.3364454668480903, 0.3212006529793143, 0.38306200492661446, 0.48262620996683836, 0.3936747530242428, 0.3107836061390117, 0.33660195709671825, 0.3459550269180909, 0.34748315007891506, 0.42119870614260435, 0.3886190268676728, 0.2999012280488387, 0.3697442429838702, 0.4657567589310929, 0.31952746014576405, 0.34604463493451476, 0.4678365859435871, 0.4616390719311312, 0.39703974698204547, 0.39165430911816657, 0.281270264997147, 0.2881344669731334, 0.2936276299878955, 0.42879173904657364, 0.3032997730188072, 0.3133124361047521, 0.39189289696514606, 0.44846686406526715, 0.41652619594242424, 0.37954779900610447, 0.3124366579577327, 0.45399242802523077, 0.29697712091729045, 0.35521138284821063, 0.3154456769116223, 0.30212898296304047, 0.3633291310397908, 0.47378651495091617, 0.3159602879313752, 0.3689145991811529, 0.4776769569143653, 0.354720102972351, 0.4724950570380315, 0.27734893816523254, 0.44006995810195804, 0.4594946230063215, 0.41355666099116206, 0.4147341769421473, 0.2436891309916973, 0.3625479128677398, 0.32985865499358624, 0.3269039060687646, 0.3362277818378061, 0.35177848604507744, 0.22764932096470147, 0.2678069288376719, 0.34930602088570595, 0.29953924915753305, 0.3260038511361927, 0.7099362469743937, 0.24429349100682884, 0.26529812696389854, 0.2649286249652505, 0.3122087309602648, 0.43176960991695523, 0.6294775152346119, 0.2642007569083944, 0.2952941640978679, 0.2597958849510178, 0.279815916903317, 0.290477231843397, 0.2864235130837187, 0.4012304157949984, 0.3664941129973158, 0.4419219788396731, 0.38281431095674634, 0.33046044886577874, 0.3316393119748682, 0.5517885530134663, 0.33684490295127034, 0.36653395113535225, 0.5323495399206877, 0.36283771391026676, 0.3961706270929426, 0.36921196582261473, 0.3538868739269674, 0.40587824198883027, 0.36081479908898473, 0.4134715630207211, 0.33971665403805673, 0.40335558203514665, 0.37803207500837743, 0.446712345816195, 0.547666177037172, 0.3839527419768274, 0.6130406589945778, 0.35689400299452245, 0.43040253198705614, 0.46419880201574415, 0.3917818780755624, 0.4718864760361612, 0.29718995501752943, 0.39038100105244666, 0.2999669590499252, 0.3655545200454071, 0.5315724979154766, 0.43630982004106045, 0.37156028393656015, 0.43294939608313143, 0.36855068209115416, 0.3688139340374619, 0.3457278369460255, 0.35905936104245484, 0.39883147284854203, 0.34588726703077555, 0.33788328303489834, 0.4013811171753332, 0.3565280110342428, 0.4096894069807604, 0.4398262659087777, 0.3805768348975107, 0.3863109789090231, 0.4325135559774935, 0.3465852600056678, 0.35908713412936777, 0.4300661259330809, 0.35728929995093495, 0.33289817394688725, 0.4417676869779825, 0.36218487995211035, 0.3118505651364103, 0.46259548200760037, 0.49442164006177336, 0.47351142892148346, 0.39199292892590165, 0.44245639408472925, 0.3712395220063627, 0.4240395900560543, 0.35912352497689426, 0.2850026680389419, 0.35028565011452883, 0.45814847911242396, 0.29604877589736134, 0.35817718296311796, 0.7498415129957721, 0.3064652280882001, 0.2670700679300353, 0.283424528199248, 0.2668871378991753, 0.28120177995879203, 0.2719877279596403, 0.25925195298623294, 0.266154354903847, 0.2616573399864137, 0.2721398819703609, 0.393840204924345, 0.2722392671275884, 0.2605177400400862, 0.2979339560260996, 0.30937495606485754, 0.390437112073414, 0.37905235192738473]
Total Epoch List: [96, 75, 90]
Total Time List: [0.0818010080838576, 0.05995860602706671, 0.05666112597100437]
========================training times:2========================
========================k_idx:0========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdd16f20>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7872;  Loss pred: 0.7872; Loss self: 0.0000; time: 0.13s
Val loss: 1.3063 score: 0.5116 time: 0.06s
Test loss: 1.2331 score: 0.4773 time: 0.05s
Epoch 2/1000, LR 0.000000
Train loss: 0.7218;  Loss pred: 0.7218; Loss self: 0.0000; time: 0.13s
Val loss: 1.1641 score: 0.4884 time: 0.05s
Test loss: 1.1094 score: 0.4773 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.7386;  Loss pred: 0.7386; Loss self: 0.0000; time: 0.13s
Val loss: 1.0937 score: 0.4884 time: 0.06s
Test loss: 1.0506 score: 0.4773 time: 0.06s
Epoch 4/1000, LR 0.000060
Train loss: 0.7465;  Loss pred: 0.7465; Loss self: 0.0000; time: 0.15s
Val loss: 1.0502 score: 0.4884 time: 0.07s
Test loss: 1.0100 score: 0.4773 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.7291;  Loss pred: 0.7291; Loss self: 0.0000; time: 0.14s
Val loss: 1.0186 score: 0.4884 time: 0.06s
Test loss: 0.9787 score: 0.4773 time: 0.06s
Epoch 6/1000, LR 0.000120
Train loss: 0.6953;  Loss pred: 0.6953; Loss self: 0.0000; time: 0.15s
Val loss: 0.9933 score: 0.4884 time: 0.12s
Test loss: 0.9553 score: 0.4545 time: 0.13s
Epoch 7/1000, LR 0.000150
Train loss: 0.7588;  Loss pred: 0.7588; Loss self: 0.0000; time: 0.19s
Val loss: 0.9792 score: 0.4884 time: 0.18s
Test loss: 0.9418 score: 0.4545 time: 0.07s
Epoch 8/1000, LR 0.000180
Train loss: 0.6793;  Loss pred: 0.6793; Loss self: 0.0000; time: 0.20s
Val loss: 0.9699 score: 0.4884 time: 0.06s
Test loss: 0.9264 score: 0.4318 time: 0.06s
Epoch 9/1000, LR 0.000210
Train loss: 0.6665;  Loss pred: 0.6665; Loss self: 0.0000; time: 0.18s
Val loss: 0.9561 score: 0.4884 time: 0.06s
Test loss: 0.9104 score: 0.4545 time: 0.09s
Epoch 10/1000, LR 0.000240
Train loss: 0.6122;  Loss pred: 0.6122; Loss self: 0.0000; time: 0.33s
Val loss: 0.9380 score: 0.4651 time: 0.08s
Test loss: 0.8989 score: 0.4545 time: 0.05s
Epoch 11/1000, LR 0.000270
Train loss: 0.5949;  Loss pred: 0.5949; Loss self: 0.0000; time: 0.13s
Val loss: 0.9206 score: 0.4419 time: 0.09s
Test loss: 0.8856 score: 0.4318 time: 0.07s
Epoch 12/1000, LR 0.000270
Train loss: 0.5527;  Loss pred: 0.5527; Loss self: 0.0000; time: 0.20s
Val loss: 0.9047 score: 0.4651 time: 0.08s
Test loss: 0.8760 score: 0.4545 time: 0.08s
Epoch 13/1000, LR 0.000270
Train loss: 0.5398;  Loss pred: 0.5398; Loss self: 0.0000; time: 0.22s
Val loss: 0.8976 score: 0.4651 time: 0.06s
Test loss: 0.8725 score: 0.4545 time: 0.14s
Epoch 14/1000, LR 0.000270
Train loss: 0.5591;  Loss pred: 0.5591; Loss self: 0.0000; time: 0.21s
Val loss: 0.8731 score: 0.4651 time: 0.16s
Test loss: 0.8530 score: 0.4318 time: 0.05s
Epoch 15/1000, LR 0.000270
Train loss: 0.5250;  Loss pred: 0.5250; Loss self: 0.0000; time: 0.17s
Val loss: 0.8408 score: 0.4651 time: 0.06s
Test loss: 0.8247 score: 0.4318 time: 0.13s
Epoch 16/1000, LR 0.000270
Train loss: 0.4632;  Loss pred: 0.4632; Loss self: 0.0000; time: 0.20s
Val loss: 0.8027 score: 0.4419 time: 0.14s
Test loss: 0.7924 score: 0.4545 time: 0.10s
Epoch 17/1000, LR 0.000270
Train loss: 0.4508;  Loss pred: 0.4508; Loss self: 0.0000; time: 0.13s
Val loss: 0.7747 score: 0.4419 time: 0.16s
Test loss: 0.7713 score: 0.4773 time: 0.11s
Epoch 18/1000, LR 0.000270
Train loss: 0.4825;  Loss pred: 0.4825; Loss self: 0.0000; time: 0.16s
Val loss: 0.7468 score: 0.4884 time: 0.05s
Test loss: 0.7482 score: 0.4773 time: 0.09s
Epoch 19/1000, LR 0.000270
Train loss: 0.4315;  Loss pred: 0.4315; Loss self: 0.0000; time: 0.17s
Val loss: 0.7218 score: 0.5116 time: 0.09s
Test loss: 0.7251 score: 0.5000 time: 0.14s
Epoch 20/1000, LR 0.000270
Train loss: 0.3892;  Loss pred: 0.3892; Loss self: 0.0000; time: 0.14s
Val loss: 0.6954 score: 0.5581 time: 0.16s
Test loss: 0.7038 score: 0.5455 time: 0.05s
Epoch 21/1000, LR 0.000270
Train loss: 0.3908;  Loss pred: 0.3908; Loss self: 0.0000; time: 0.18s
Val loss: 0.6671 score: 0.5116 time: 0.05s
Test loss: 0.6804 score: 0.5455 time: 0.10s
Epoch 22/1000, LR 0.000270
Train loss: 0.3723;  Loss pred: 0.3723; Loss self: 0.0000; time: 0.16s
Val loss: 0.6472 score: 0.5349 time: 0.09s
Test loss: 0.6635 score: 0.5455 time: 0.14s
Epoch 23/1000, LR 0.000270
Train loss: 0.3384;  Loss pred: 0.3384; Loss self: 0.0000; time: 0.14s
Val loss: 0.6329 score: 0.5814 time: 0.32s
Test loss: 0.6530 score: 0.5227 time: 0.10s
Epoch 24/1000, LR 0.000270
Train loss: 0.3149;  Loss pred: 0.3149; Loss self: 0.0000; time: 0.18s
Val loss: 0.6248 score: 0.6047 time: 0.07s
Test loss: 0.6475 score: 0.5227 time: 0.07s
Epoch 25/1000, LR 0.000270
Train loss: 0.2813;  Loss pred: 0.2813; Loss self: 0.0000; time: 0.19s
Val loss: 0.6201 score: 0.6279 time: 0.11s
Test loss: 0.6441 score: 0.5000 time: 0.04s
Epoch 26/1000, LR 0.000270
Train loss: 0.3168;  Loss pred: 0.3168; Loss self: 0.0000; time: 0.29s
Val loss: 0.6180 score: 0.6279 time: 0.07s
Test loss: 0.6430 score: 0.5227 time: 0.06s
Epoch 27/1000, LR 0.000270
Train loss: 0.2593;  Loss pred: 0.2593; Loss self: 0.0000; time: 0.15s
Val loss: 0.6171 score: 0.6047 time: 0.07s
Test loss: 0.6429 score: 0.5227 time: 0.07s
Epoch 28/1000, LR 0.000270
Train loss: 0.2473;  Loss pred: 0.2473; Loss self: 0.0000; time: 0.14s
Val loss: 0.6178 score: 0.5814 time: 0.07s
Test loss: 0.6442 score: 0.5227 time: 0.14s
     INFO: Early stopping counter 1 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2202;  Loss pred: 0.2202; Loss self: 0.0000; time: 0.13s
Val loss: 0.6213 score: 0.5814 time: 0.08s
Test loss: 0.6474 score: 0.5000 time: 0.10s
     INFO: Early stopping counter 2 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2068;  Loss pred: 0.2068; Loss self: 0.0000; time: 0.14s
Val loss: 0.6284 score: 0.5814 time: 0.06s
Test loss: 0.6529 score: 0.5227 time: 0.08s
     INFO: Early stopping counter 3 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.1764;  Loss pred: 0.1764; Loss self: 0.0000; time: 0.19s
Val loss: 0.6441 score: 0.5814 time: 0.05s
Test loss: 0.6655 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2061;  Loss pred: 0.2061; Loss self: 0.0000; time: 0.27s
Val loss: 0.6593 score: 0.5581 time: 0.05s
Test loss: 0.6786 score: 0.5227 time: 0.17s
     INFO: Early stopping counter 5 of 20
Epoch 33/1000, LR 0.000270
Train loss: 0.1868;  Loss pred: 0.1868; Loss self: 0.0000; time: 0.13s
Val loss: 0.6676 score: 0.5581 time: 0.06s
Test loss: 0.6889 score: 0.5227 time: 0.10s
     INFO: Early stopping counter 6 of 20
Epoch 34/1000, LR 0.000270
Train loss: 0.1560;  Loss pred: 0.1560; Loss self: 0.0000; time: 0.14s
Val loss: 0.6703 score: 0.5581 time: 0.08s
Test loss: 0.6950 score: 0.5227 time: 0.10s
     INFO: Early stopping counter 7 of 20
Epoch 35/1000, LR 0.000270
Train loss: 0.1620;  Loss pred: 0.1620; Loss self: 0.0000; time: 0.18s
Val loss: 0.6708 score: 0.5581 time: 0.18s
Test loss: 0.6973 score: 0.5227 time: 0.18s
     INFO: Early stopping counter 8 of 20
Epoch 36/1000, LR 0.000270
Train loss: 0.1543;  Loss pred: 0.1543; Loss self: 0.0000; time: 0.16s
Val loss: 0.6712 score: 0.5581 time: 0.17s
Test loss: 0.6999 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 37/1000, LR 0.000270
Train loss: 0.1203;  Loss pred: 0.1203; Loss self: 0.0000; time: 0.43s
Val loss: 0.6669 score: 0.5814 time: 0.05s
Test loss: 0.6993 score: 0.5227 time: 0.08s
     INFO: Early stopping counter 10 of 20
Epoch 38/1000, LR 0.000270
Train loss: 0.1462;  Loss pred: 0.1462; Loss self: 0.0000; time: 0.18s
Val loss: 0.6649 score: 0.5814 time: 0.11s
Test loss: 0.7039 score: 0.5227 time: 0.09s
     INFO: Early stopping counter 11 of 20
Epoch 39/1000, LR 0.000269
Train loss: 0.1233;  Loss pred: 0.1233; Loss self: 0.0000; time: 0.20s
Val loss: 0.6470 score: 0.5814 time: 0.10s
Test loss: 0.6917 score: 0.5227 time: 0.10s
     INFO: Early stopping counter 12 of 20
Epoch 40/1000, LR 0.000269
Train loss: 0.1145;  Loss pred: 0.1145; Loss self: 0.0000; time: 0.15s
Val loss: 0.6318 score: 0.5814 time: 0.18s
Test loss: 0.6798 score: 0.5000 time: 0.19s
     INFO: Early stopping counter 13 of 20
Epoch 41/1000, LR 0.000269
Train loss: 0.1035;  Loss pred: 0.1035; Loss self: 0.0000; time: 0.18s
Val loss: 0.6354 score: 0.5814 time: 0.09s
Test loss: 0.6835 score: 0.5000 time: 0.09s
     INFO: Early stopping counter 14 of 20
Epoch 42/1000, LR 0.000269
Train loss: 0.0879;  Loss pred: 0.0879; Loss self: 0.0000; time: 0.17s
Val loss: 0.6392 score: 0.5814 time: 0.07s
Test loss: 0.6900 score: 0.5227 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 43/1000, LR 0.000269
Train loss: 0.0830;  Loss pred: 0.0830; Loss self: 0.0000; time: 0.21s
Val loss: 0.6585 score: 0.5581 time: 0.13s
Test loss: 0.7120 score: 0.5455 time: 0.05s
     INFO: Early stopping counter 16 of 20
Epoch 44/1000, LR 0.000269
Train loss: 0.0808;  Loss pred: 0.0808; Loss self: 0.0000; time: 0.40s
Val loss: 0.6880 score: 0.5349 time: 0.07s
Test loss: 0.7425 score: 0.5227 time: 0.11s
     INFO: Early stopping counter 17 of 20
Epoch 45/1000, LR 0.000269
Train loss: 0.0894;  Loss pred: 0.0894; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7145 score: 0.5116 time: 0.07s
Test loss: 0.7678 score: 0.5227 time: 0.27s
     INFO: Early stopping counter 18 of 20
Epoch 46/1000, LR 0.000269
Train loss: 0.0701;  Loss pred: 0.0701; Loss self: 0.0000; time: 0.40s
Val loss: 0.7394 score: 0.5349 time: 0.08s
Test loss: 0.7910 score: 0.5227 time: 0.08s
     INFO: Early stopping counter 19 of 20
Epoch 47/1000, LR 0.000269
Train loss: 0.0662;  Loss pred: 0.0662; Loss self: 0.0000; time: 0.19s
Val loss: 0.7679 score: 0.5349 time: 0.12s
Test loss: 0.8162 score: 0.5227 time: 0.40s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 026,   Train_Loss: 0.2593,   Val_Loss: 0.6171,   Val_Precision: 0.5641,   Val_Recall: 1.0000,   Val_accuracy: 0.7213,   Val_Score: 0.6047,   Val_Loss: 0.6171,   Test_Precision: 0.5122,   Test_Recall: 0.9545,   Test_accuracy: 0.6667,   Test_Score: 0.5227,   Test_loss: 0.6429


[0.05353200400713831, 0.059153871960006654, 0.06937694095540792, 0.05328389594797045, 0.06225390895269811, 0.13080618402455002, 0.07276171899866313, 0.06624936906155199, 0.09080701495986432, 0.05329299496952444, 0.07520483201369643, 0.08036009303759784, 0.145346435951069, 0.055992089910432696, 0.13503225601743907, 0.10310205596033484, 0.11431060696486384, 0.09671122196596116, 0.14925312995910645, 0.053615333046764135, 0.10720451199449599, 0.1452262990642339, 0.1085815189871937, 0.07624030497390777, 0.048731882008723915, 0.06563475797884166, 0.0720192709704861, 0.14826493698637933, 0.10679565998725593, 0.08226602198556066, 0.055725147016346455, 0.17142809403594583, 0.10352800297550857, 0.10277499002404511, 0.18024046102073044, 0.05653903807979077, 0.08022790809627622, 0.09481806703843176, 0.10328404407482594, 0.19769049098249525, 0.09811706305481493, 0.058980197994969785, 0.055670263012871146, 0.11786673299502581, 0.27495466207619756, 0.08976135298144072, 0.4020521939964965]
[0.001216636454707689, 0.001344406180909242, 0.0015767486580774528, 0.0012109976351811465, 0.0014148615671067753, 0.002972867818739773, 0.0016536754317877983, 0.001505667478671636, 0.002063795794542371, 0.0012112044311255556, 0.0017092007275840099, 0.0018263657508544964, 0.0033033280897970226, 0.0012725474979643795, 0.0030689149094872514, 0.0023432285445530647, 0.0025979683401105417, 0.002197982317408208, 0.003392116589979692, 0.0012185302965173667, 0.0024364661816930907, 0.003300597706005316, 0.0024677617951634934, 0.0017327342039524492, 0.0011075427729255434, 0.001491699044973674, 0.0016368016129655932, 0.0033696576587813483, 0.002427174090619453, 0.0018696823178536513, 0.001266480614007874, 0.0038960930462714964, 0.0023529091585342858, 0.002335795227819207, 0.00409637411410751, 0.0012849781381770629, 0.0018233615476426414, 0.0021549560690552676, 0.002347364638064226, 0.004492965704147619, 0.002229933251245794, 0.0013404590453402225, 0.001265233250292526, 0.0026787893862505866, 0.006248969592640854, 0.002040030749578198, 0.009137549863556738]
[821.9382183811529, 743.822822447666, 634.2164902929495, 825.7654440839725, 706.7829272123646, 336.37553398654285, 604.7135857360438, 664.1572685638681, 484.5440632471787, 825.6244563691976, 585.0687890903957, 547.5354536910983, 302.72500121580305, 785.8252847926243, 325.8480699183276, 426.7616158588299, 384.9161610481561, 454.9627137943351, 294.8011878347574, 820.66076063768, 410.4304863797059, 302.97542720233275, 405.22549703130824, 577.1225602397369, 902.8996662210415, 670.3765101744422, 610.9475895421302, 296.7660520035304, 412.00176116942, 534.8502205165929, 789.5896620441936, 256.6673814315049, 425.00578331844184, 428.119720466096, 244.11832809803633, 778.2233567169105, 548.4375829318459, 464.0465828328463, 426.0096551614829, 222.57013871191222, 448.4439161761148, 746.0130941532717, 790.3680999284494, 373.30295734808294, 160.0263827779955, 490.1886896590958, 109.43852727833496]
Elapsed: 0.10478871985229644~0.06269380241412052
Time per graph: 0.0023815618148249193~0.0014248591457754662
Speed: 519.1747122918681~203.16438033115008
Total Time: 0.4024
best val loss: 0.6170969009399414 test_score: 0.5227

Testing...
Test loss: 0.6441 score: 0.5000 time: 0.05s
test Score 0.5000
Epoch Time List: [0.2386474049417302, 0.23698948917444795, 0.2569511700421572, 0.2686661221086979, 0.2525211760075763, 0.3975912630558014, 0.43671675689984113, 0.3269686341518536, 0.32356250600423664, 0.4534768069861457, 0.29003395314794034, 0.35084644285961986, 0.41240445303265005, 0.4167550130514428, 0.3636886980384588, 0.44057594507467, 0.40148504986427724, 0.3057047779439017, 0.41003145882859826, 0.3485990351764485, 0.3338054360356182, 0.3903276158962399, 0.5638366589555517, 0.32137978298123926, 0.3449831899488345, 0.41877988108899444, 0.28091792785562575, 0.35331367212347686, 0.3039102910552174, 0.2750011889729649, 0.28809488203842193, 0.4906964801484719, 0.28928981500212103, 0.31078528019133955, 0.5406650858931243, 0.38794472499284893, 0.5624508240725845, 0.38144517107866704, 0.4059472121298313, 0.5222920010564849, 0.36814069910906255, 0.2907579409657046, 0.3837342010810971, 0.582721235929057, 0.5109960209811106, 0.5656269750325009, 0.7095537649001926]
Total Epoch List: [47]
Total Time List: [0.4024314070120454]
========================k_idx:1========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdd14100>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.6671;  Loss pred: 0.6671; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9321 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8598 score: 0.5116 time: 0.12s
Epoch 2/1000, LR 0.000000
Train loss: 0.6715;  Loss pred: 0.6715; Loss self: 0.0000; time: 0.22s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8694 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8044 score: 0.5116 time: 0.05s
Epoch 3/1000, LR 0.000030
Train loss: 0.6850;  Loss pred: 0.6850; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8211 score: 0.5000 time: 0.11s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7660 score: 0.5116 time: 0.09s
Epoch 4/1000, LR 0.000060
Train loss: 0.6709;  Loss pred: 0.6709; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7727 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7398 score: 0.5116 time: 0.05s
Epoch 5/1000, LR 0.000090
Train loss: 0.6258;  Loss pred: 0.6258; Loss self: 0.0000; time: 0.17s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7325 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7108 score: 0.5116 time: 0.10s
Epoch 6/1000, LR 0.000120
Train loss: 0.6636;  Loss pred: 0.6636; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7098 score: 0.5000 time: 0.26s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6863 score: 0.5116 time: 0.06s
Epoch 7/1000, LR 0.000150
Train loss: 0.6699;  Loss pred: 0.6699; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7047 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6698 score: 0.5116 time: 0.05s
Epoch 8/1000, LR 0.000180
Train loss: 0.5862;  Loss pred: 0.5862; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7129 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6665 score: 0.5116 time: 0.11s
     INFO: Early stopping counter 1 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.5532;  Loss pred: 0.5532; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7284 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6673 score: 0.5116 time: 0.15s
     INFO: Early stopping counter 2 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.5375;  Loss pred: 0.5375; Loss self: 0.0000; time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7428 score: 0.5000 time: 0.12s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6717 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.5122;  Loss pred: 0.5122; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7545 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6814 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 4 of 20
Epoch 12/1000, LR 0.000270
Train loss: 0.4903;  Loss pred: 0.4903; Loss self: 0.0000; time: 0.19s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7674 score: 0.5000 time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.6972 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 5 of 20
Epoch 13/1000, LR 0.000270
Train loss: 0.4738;  Loss pred: 0.4738; Loss self: 0.0000; time: 0.28s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7806 score: 0.5000 time: 0.07s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7108 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 6 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.4592;  Loss pred: 0.4592; Loss self: 0.0000; time: 0.14s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7914 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7217 score: 0.5116 time: 0.09s
     INFO: Early stopping counter 7 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.4504;  Loss pred: 0.4504; Loss self: 0.0000; time: 0.31s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8090 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7388 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.4198;  Loss pred: 0.4198; Loss self: 0.0000; time: 0.20s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8272 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7567 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 9 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.3905;  Loss pred: 0.3905; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8535 score: 0.5000 time: 0.05s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7818 score: 0.5116 time: 0.10s
     INFO: Early stopping counter 10 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.3820;  Loss pred: 0.3820; Loss self: 0.0000; time: 0.24s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.8807 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8071 score: 0.5116 time: 0.05s
     INFO: Early stopping counter 11 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.3779;  Loss pred: 0.3779; Loss self: 0.0000; time: 0.27s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9075 score: 0.5000 time: 0.29s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8331 score: 0.5116 time: 0.16s
     INFO: Early stopping counter 12 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.3653;  Loss pred: 0.3653; Loss self: 0.0000; time: 0.18s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.9266 score: 0.5000 time: 0.06s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.8535 score: 0.5116 time: 0.08s
     INFO: Early stopping counter 13 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.3396;  Loss pred: 0.3396; Loss self: 0.0000; time: 0.19s
Val loss: 0.9385 score: 0.5227 time: 0.14s
Test loss: 0.8665 score: 0.5349 time: 0.06s
     INFO: Early stopping counter 14 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.3176;  Loss pred: 0.3176; Loss self: 0.0000; time: 0.16s
Val loss: 0.9415 score: 0.5227 time: 0.06s
Test loss: 0.8715 score: 0.5349 time: 0.06s
     INFO: Early stopping counter 15 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3054;  Loss pred: 0.3054; Loss self: 0.0000; time: 0.20s
Val loss: 0.9352 score: 0.5455 time: 0.05s
Test loss: 0.8684 score: 0.5581 time: 0.12s
     INFO: Early stopping counter 16 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.2833;  Loss pred: 0.2833; Loss self: 0.0000; time: 0.18s
Val loss: 0.9229 score: 0.5682 time: 0.05s
Test loss: 0.8609 score: 0.5814 time: 0.05s
     INFO: Early stopping counter 17 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.2791;  Loss pred: 0.2791; Loss self: 0.0000; time: 0.29s
Val loss: 0.9070 score: 0.5682 time: 0.06s
Test loss: 0.8496 score: 0.5581 time: 0.08s
     INFO: Early stopping counter 18 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.2629;  Loss pred: 0.2629; Loss self: 0.0000; time: 0.18s
Val loss: 0.8895 score: 0.5682 time: 0.08s
Test loss: 0.8364 score: 0.5581 time: 0.13s
     INFO: Early stopping counter 19 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.2427;  Loss pred: 0.2427; Loss self: 0.0000; time: 0.22s
Val loss: 0.8661 score: 0.5455 time: 0.16s
Test loss: 0.8173 score: 0.5581 time: 0.06s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 006,   Train_Loss: 0.6699,   Val_Loss: 0.7047,   Val_Precision: 0.5000,   Val_Recall: 1.0000,   Val_accuracy: 0.6667,   Val_Score: 0.5000,   Val_Loss: 0.7047,   Test_Precision: 0.5116,   Test_Recall: 1.0000,   Test_accuracy: 0.6769,   Test_Score: 0.5116,   Test_loss: 0.6698


[0.05353200400713831, 0.059153871960006654, 0.06937694095540792, 0.05328389594797045, 0.06225390895269811, 0.13080618402455002, 0.07276171899866313, 0.06624936906155199, 0.09080701495986432, 0.05329299496952444, 0.07520483201369643, 0.08036009303759784, 0.145346435951069, 0.055992089910432696, 0.13503225601743907, 0.10310205596033484, 0.11431060696486384, 0.09671122196596116, 0.14925312995910645, 0.053615333046764135, 0.10720451199449599, 0.1452262990642339, 0.1085815189871937, 0.07624030497390777, 0.048731882008723915, 0.06563475797884166, 0.0720192709704861, 0.14826493698637933, 0.10679565998725593, 0.08226602198556066, 0.055725147016346455, 0.17142809403594583, 0.10352800297550857, 0.10277499002404511, 0.18024046102073044, 0.05653903807979077, 0.08022790809627622, 0.09481806703843176, 0.10328404407482594, 0.19769049098249525, 0.09811706305481493, 0.058980197994969785, 0.055670263012871146, 0.11786673299502581, 0.27495466207619756, 0.08976135298144072, 0.4020521939964965, 0.12175445107277483, 0.054704347043298185, 0.09348124195821583, 0.05543793994002044, 0.1058585720602423, 0.0652026500320062, 0.05311605799943209, 0.11143118399195373, 0.15787936095148325, 0.05511111707892269, 0.10688687092624605, 0.05138289905153215, 0.08475723199080676, 0.09280869306530803, 0.05018429993651807, 0.055127328960224986, 0.10892023099586368, 0.05886524205561727, 0.16354113502893597, 0.08691515203099698, 0.06443851604126394, 0.061888000927865505, 0.12258999003097415, 0.052391245029866695, 0.08719716698396951, 0.1333409920334816, 0.06570514698978513]
[0.001216636454707689, 0.001344406180909242, 0.0015767486580774528, 0.0012109976351811465, 0.0014148615671067753, 0.002972867818739773, 0.0016536754317877983, 0.001505667478671636, 0.002063795794542371, 0.0012112044311255556, 0.0017092007275840099, 0.0018263657508544964, 0.0033033280897970226, 0.0012725474979643795, 0.0030689149094872514, 0.0023432285445530647, 0.0025979683401105417, 0.002197982317408208, 0.003392116589979692, 0.0012185302965173667, 0.0024364661816930907, 0.003300597706005316, 0.0024677617951634934, 0.0017327342039524492, 0.0011075427729255434, 0.001491699044973674, 0.0016368016129655932, 0.0033696576587813483, 0.002427174090619453, 0.0018696823178536513, 0.001266480614007874, 0.0038960930462714964, 0.0023529091585342858, 0.002335795227819207, 0.00409637411410751, 0.0012849781381770629, 0.0018233615476426414, 0.0021549560690552676, 0.002347364638064226, 0.004492965704147619, 0.002229933251245794, 0.0013404590453402225, 0.001265233250292526, 0.0026787893862505866, 0.006248969592640854, 0.002040030749578198, 0.009137549863556738, 0.002831498862157554, 0.0012721941172860044, 0.0021739823711212983, 0.0012892544172097777, 0.002461827257214937, 0.001516340698418749, 0.0012352571627774903, 0.0025914228835338077, 0.0036716130453833314, 0.0012816538855563416, 0.0024857411843313033, 0.0011949511407333057, 0.0019710984183908546, 0.00215834169919321, 0.0011670767427097227, 0.0012820309060517438, 0.002533028627810783, 0.0013689591175724948, 0.003803282209975255, 0.0020212826053720226, 0.0014985701404945102, 0.001439255835531756, 0.002850930000720329, 0.0012184010472062023, 0.0020278410926504536, 0.003100953303104223, 0.0015280266741810496]
[821.9382183811529, 743.822822447666, 634.2164902929495, 825.7654440839725, 706.7829272123646, 336.37553398654285, 604.7135857360438, 664.1572685638681, 484.5440632471787, 825.6244563691976, 585.0687890903957, 547.5354536910983, 302.72500121580305, 785.8252847926243, 325.8480699183276, 426.7616158588299, 384.9161610481561, 454.9627137943351, 294.8011878347574, 820.66076063768, 410.4304863797059, 302.97542720233275, 405.22549703130824, 577.1225602397369, 902.8996662210415, 670.3765101744422, 610.9475895421302, 296.7660520035304, 412.00176116942, 534.8502205165929, 789.5896620441936, 256.6673814315049, 425.00578331844184, 428.119720466096, 244.11832809803633, 778.2233567169105, 548.4375829318459, 464.0465828328463, 426.0096551614829, 222.57013871191222, 448.4439161761148, 746.0130941532717, 790.3680999284494, 373.30295734808294, 160.0263827779955, 490.1886896590958, 109.43852727833496, 353.1698399617286, 786.0435655317435, 459.9853307385465, 775.6420972085664, 406.2023430235715, 659.4823980143824, 809.5480278385824, 385.8883883267808, 272.35985591057727, 780.2418509938968, 402.2944972322261, 836.8542996547372, 507.33133904920373, 463.3186674629883, 856.8416826456473, 780.0123969551473, 394.784326170158, 730.4820042933414, 262.9307910354899, 494.73537116594696, 667.3027661354656, 694.8035056120054, 350.7627334755099, 820.7478172257021, 493.1352873873207, 322.4814765830061, 654.438837290555]
Elapsed: 0.0979187418549397~0.054537351830285574
Time per graph: 0.002242002983019726~0.0012399232151958652
Speed: 538.1490942519005~201.38093223587907
Total Time: 0.0662
best val loss: 0.7047157883644104 test_score: 0.5116

Testing...
Test loss: 0.8609 score: 0.5814 time: 0.07s
test Score 0.5814
Epoch Time List: [0.2386474049417302, 0.23698948917444795, 0.2569511700421572, 0.2686661221086979, 0.2525211760075763, 0.3975912630558014, 0.43671675689984113, 0.3269686341518536, 0.32356250600423664, 0.4534768069861457, 0.29003395314794034, 0.35084644285961986, 0.41240445303265005, 0.4167550130514428, 0.3636886980384588, 0.44057594507467, 0.40148504986427724, 0.3057047779439017, 0.41003145882859826, 0.3485990351764485, 0.3338054360356182, 0.3903276158962399, 0.5638366589555517, 0.32137978298123926, 0.3449831899488345, 0.41877988108899444, 0.28091792785562575, 0.35331367212347686, 0.3039102910552174, 0.2750011889729649, 0.28809488203842193, 0.4906964801484719, 0.28928981500212103, 0.31078528019133955, 0.5406650858931243, 0.38794472499284893, 0.5624508240725845, 0.38144517107866704, 0.4059472121298313, 0.5222920010564849, 0.36814069910906255, 0.2907579409657046, 0.3837342010810971, 0.582721235929057, 0.5109960209811106, 0.5656269750325009, 0.7095537649001926, 0.3262799320509657, 0.3187370849773288, 0.38603113999124616, 0.3908177597913891, 0.32616335107013583, 0.5068218598607928, 0.385054815094918, 0.351725340122357, 0.3881031950004399, 0.315941845998168, 0.3325333909597248, 0.4117599379969761, 0.42318257107399404, 0.28832262801006436, 0.4990479819243774, 0.29954879404976964, 0.33921474614180624, 0.4478818558854982, 0.7157840210711583, 0.3167876440566033, 0.38868778105825186, 0.2798960090149194, 0.3628319570561871, 0.2745707860449329, 0.42562442587222904, 0.38732264610007405, 0.4395047699799761]
Total Epoch List: [47, 27]
Total Time List: [0.4024314070120454, 0.06624253699555993]
========================k_idx:2========================
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
Extracting 1-hop subgraphs...
Done!
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead
  warnings.warn(out)
Model: GPSPerformer
GPS(
  (x_lin): Linear(in_features=14887, out_features=64, bias=True)
  (pe_lin): Linear(in_features=20, out_features=8, bias=True)
  (pe_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (convs): ModuleList(
    (0-1): 2 x GPSConv(64, conv=GINEConv(nn=Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )), heads=1, attn_type=performer)
  )
  (classifier): Sequential(
    (0): Linear(in_features=64, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=16, bias=True)
    (3): ReLU()
    (4): Linear(in_features=16, out_features=2, bias=True)
  )
)
Total number of parameters: 1039522
Extracting 1-hop subgraphs...
Done!
<torch_geometric.deprecation.DataLoader object at 0x742ecdd17880>
Training...
Epoch 1/1000, LR 0.000300
Train loss: 0.7245;  Loss pred: 0.7245; Loss self: 0.0000; time: 0.16s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Val loss: 0.7343 score: 0.5000 time: 0.15s
/opt/conda/envs/SAMamba/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
Test loss: 0.7645 score: 0.4884 time: 0.04s
Epoch 2/1000, LR 0.000000
Train loss: 0.8393;  Loss pred: 0.8393; Loss self: 0.0000; time: 0.19s
Val loss: 0.7290 score: 0.4545 time: 0.10s
Test loss: 0.7288 score: 0.4651 time: 0.06s
Epoch 3/1000, LR 0.000030
Train loss: 0.7500;  Loss pred: 0.7500; Loss self: 0.0000; time: 0.22s
Val loss: 0.7417 score: 0.3409 time: 0.05s
Test loss: 0.7206 score: 0.3023 time: 0.08s
     INFO: Early stopping counter 1 of 20
Epoch 4/1000, LR 0.000060
Train loss: 0.7558;  Loss pred: 0.7558; Loss self: 0.0000; time: 0.19s
Val loss: 0.7495 score: 0.4773 time: 0.08s
Test loss: 0.7220 score: 0.4419 time: 0.11s
     INFO: Early stopping counter 2 of 20
Epoch 5/1000, LR 0.000090
Train loss: 0.7355;  Loss pred: 0.7355; Loss self: 0.0000; time: 0.14s
Val loss: 0.7556 score: 0.4318 time: 0.15s
Test loss: 0.7186 score: 0.4419 time: 0.05s
     INFO: Early stopping counter 3 of 20
Epoch 6/1000, LR 0.000120
Train loss: 0.6778;  Loss pred: 0.6778; Loss self: 0.0000; time: 0.20s
Val loss: 0.7570 score: 0.4773 time: 0.05s
Test loss: 0.7166 score: 0.4651 time: 0.11s
     INFO: Early stopping counter 4 of 20
Epoch 7/1000, LR 0.000150
Train loss: 0.6721;  Loss pred: 0.6721; Loss self: 0.0000; time: 0.15s
Val loss: 0.7532 score: 0.4318 time: 0.05s
Test loss: 0.7156 score: 0.4419 time: 0.06s
     INFO: Early stopping counter 5 of 20
Epoch 8/1000, LR 0.000180
Train loss: 0.6627;  Loss pred: 0.6627; Loss self: 0.0000; time: 0.16s
Val loss: 0.7484 score: 0.5000 time: 0.12s
Test loss: 0.7133 score: 0.4419 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 9/1000, LR 0.000210
Train loss: 0.7005;  Loss pred: 0.7005; Loss self: 0.0000; time: 0.29s
Val loss: 0.7422 score: 0.5227 time: 0.05s
Test loss: 0.7072 score: 0.4419 time: 0.08s
     INFO: Early stopping counter 7 of 20
Epoch 10/1000, LR 0.000240
Train loss: 0.6139;  Loss pred: 0.6139; Loss self: 0.0000; time: 0.32s
Val loss: 0.7321 score: 0.4773 time: 0.16s
Test loss: 0.7001 score: 0.4186 time: 0.05s
     INFO: Early stopping counter 8 of 20
Epoch 11/1000, LR 0.000270
Train loss: 0.6005;  Loss pred: 0.6005; Loss self: 0.0000; time: 0.34s
Val loss: 0.7228 score: 0.4773 time: 0.04s
Test loss: 0.6943 score: 0.4186 time: 0.10s
Epoch 12/1000, LR 0.000270
Train loss: 0.5849;  Loss pred: 0.5849; Loss self: 0.0000; time: 0.19s
Val loss: 0.7215 score: 0.4091 time: 0.06s
Test loss: 0.6916 score: 0.4419 time: 0.31s
Epoch 13/1000, LR 0.000270
Train loss: 0.5990;  Loss pred: 0.5990; Loss self: 0.0000; time: 0.30s
Val loss: 0.7225 score: 0.3409 time: 0.07s
Test loss: 0.6900 score: 0.4419 time: 0.06s
     INFO: Early stopping counter 1 of 20
Epoch 14/1000, LR 0.000270
Train loss: 0.5420;  Loss pred: 0.5420; Loss self: 0.0000; time: 0.17s
Val loss: 0.7307 score: 0.3182 time: 0.06s
Test loss: 0.6905 score: 0.4186 time: 0.08s
     INFO: Early stopping counter 2 of 20
Epoch 15/1000, LR 0.000270
Train loss: 0.5724;  Loss pred: 0.5724; Loss self: 0.0000; time: 0.20s
Val loss: 0.7450 score: 0.3409 time: 0.08s
Test loss: 0.6936 score: 0.3953 time: 0.06s
     INFO: Early stopping counter 3 of 20
Epoch 16/1000, LR 0.000270
Train loss: 0.5280;  Loss pred: 0.5280; Loss self: 0.0000; time: 0.21s
Val loss: 0.7607 score: 0.3636 time: 0.08s
Test loss: 0.6970 score: 0.3953 time: 0.05s
     INFO: Early stopping counter 4 of 20
Epoch 17/1000, LR 0.000270
Train loss: 0.4712;  Loss pred: 0.4712; Loss self: 0.0000; time: 0.19s
Val loss: 0.7745 score: 0.3636 time: 0.05s
Test loss: 0.6996 score: 0.3953 time: 0.10s
     INFO: Early stopping counter 5 of 20
Epoch 18/1000, LR 0.000270
Train loss: 0.4539;  Loss pred: 0.4539; Loss self: 0.0000; time: 0.18s
Val loss: 0.7886 score: 0.3636 time: 0.15s
Test loss: 0.7006 score: 0.3953 time: 0.04s
     INFO: Early stopping counter 6 of 20
Epoch 19/1000, LR 0.000270
Train loss: 0.4483;  Loss pred: 0.4483; Loss self: 0.0000; time: 0.15s
Val loss: 0.8015 score: 0.3636 time: 0.15s
Test loss: 0.7009 score: 0.3953 time: 0.05s
     INFO: Early stopping counter 7 of 20
Epoch 20/1000, LR 0.000270
Train loss: 0.4409;  Loss pred: 0.4409; Loss self: 0.0000; time: 0.24s
Val loss: 0.8095 score: 0.3636 time: 0.08s
Test loss: 0.6999 score: 0.4186 time: 0.08s
     INFO: Early stopping counter 8 of 20
Epoch 21/1000, LR 0.000270
Train loss: 0.4061;  Loss pred: 0.4061; Loss self: 0.0000; time: 0.15s
Val loss: 0.8175 score: 0.3864 time: 0.06s
Test loss: 0.7000 score: 0.4186 time: 0.13s
     INFO: Early stopping counter 9 of 20
Epoch 22/1000, LR 0.000270
Train loss: 0.4029;  Loss pred: 0.4029; Loss self: 0.0000; time: 0.15s
Val loss: 0.8295 score: 0.3864 time: 0.15s
Test loss: 0.7022 score: 0.4186 time: 0.05s
     INFO: Early stopping counter 10 of 20
Epoch 23/1000, LR 0.000270
Train loss: 0.3550;  Loss pred: 0.3550; Loss self: 0.0000; time: 0.18s
Val loss: 0.8380 score: 0.4545 time: 0.05s
Test loss: 0.7039 score: 0.4651 time: 0.11s
     INFO: Early stopping counter 11 of 20
Epoch 24/1000, LR 0.000270
Train loss: 0.3369;  Loss pred: 0.3369; Loss self: 0.0000; time: 0.16s
Val loss: 0.8496 score: 0.4545 time: 0.08s
Test loss: 0.7068 score: 0.4651 time: 0.15s
     INFO: Early stopping counter 12 of 20
Epoch 25/1000, LR 0.000270
Train loss: 0.3182;  Loss pred: 0.3182; Loss self: 0.0000; time: 0.15s
Val loss: 0.8700 score: 0.4773 time: 0.16s
Test loss: 0.7149 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 13 of 20
Epoch 26/1000, LR 0.000270
Train loss: 0.3265;  Loss pred: 0.3265; Loss self: 0.0000; time: 0.35s
Val loss: 0.8845 score: 0.4773 time: 0.06s
Test loss: 0.7208 score: 0.4651 time: 0.10s
     INFO: Early stopping counter 14 of 20
Epoch 27/1000, LR 0.000270
Train loss: 0.3091;  Loss pred: 0.3091; Loss self: 0.0000; time: 0.22s
Val loss: 0.8903 score: 0.4545 time: 0.14s
Test loss: 0.7216 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 15 of 20
Epoch 28/1000, LR 0.000270
Train loss: 0.2751;  Loss pred: 0.2751; Loss self: 0.0000; time: 0.29s
Val loss: 0.8915 score: 0.4545 time: 0.06s
Test loss: 0.7177 score: 0.4651 time: 0.10s
     INFO: Early stopping counter 16 of 20
Epoch 29/1000, LR 0.000270
Train loss: 0.2633;  Loss pred: 0.2633; Loss self: 0.0000; time: 0.19s
Val loss: 0.8920 score: 0.4545 time: 0.05s
Test loss: 0.7120 score: 0.4651 time: 0.08s
     INFO: Early stopping counter 17 of 20
Epoch 30/1000, LR 0.000270
Train loss: 0.2521;  Loss pred: 0.2521; Loss self: 0.0000; time: 0.26s
Val loss: 0.8860 score: 0.4773 time: 0.13s
Test loss: 0.7016 score: 0.4651 time: 0.07s
     INFO: Early stopping counter 18 of 20
Epoch 31/1000, LR 0.000270
Train loss: 0.2225;  Loss pred: 0.2225; Loss self: 0.0000; time: 0.17s
Val loss: 0.8818 score: 0.4773 time: 0.04s
Test loss: 0.6916 score: 0.4651 time: 0.05s
     INFO: Early stopping counter 19 of 20
Epoch 32/1000, LR 0.000270
Train loss: 0.2374;  Loss pred: 0.2374; Loss self: 0.0000; time: 0.20s
Val loss: 0.8822 score: 0.4773 time: 0.07s
Test loss: 0.6846 score: 0.5349 time: 0.10s
     INFO: Early stopping counter 20 of 20
     INFO: Early stopping

=====final results=====
Exp: 1,  Epoch: 011,   Train_Loss: 0.5849,   Val_Loss: 0.7215,   Val_Precision: 0.3571,   Val_Recall: 0.2273,   Val_accuracy: 0.2778,   Val_Score: 0.4091,   Val_Loss: 0.7215,   Test_Precision: 0.3846,   Test_Recall: 0.2381,   Test_accuracy: 0.2941,   Test_Score: 0.4419,   Test_loss: 0.6916


[0.05353200400713831, 0.059153871960006654, 0.06937694095540792, 0.05328389594797045, 0.06225390895269811, 0.13080618402455002, 0.07276171899866313, 0.06624936906155199, 0.09080701495986432, 0.05329299496952444, 0.07520483201369643, 0.08036009303759784, 0.145346435951069, 0.055992089910432696, 0.13503225601743907, 0.10310205596033484, 0.11431060696486384, 0.09671122196596116, 0.14925312995910645, 0.053615333046764135, 0.10720451199449599, 0.1452262990642339, 0.1085815189871937, 0.07624030497390777, 0.048731882008723915, 0.06563475797884166, 0.0720192709704861, 0.14826493698637933, 0.10679565998725593, 0.08226602198556066, 0.055725147016346455, 0.17142809403594583, 0.10352800297550857, 0.10277499002404511, 0.18024046102073044, 0.05653903807979077, 0.08022790809627622, 0.09481806703843176, 0.10328404407482594, 0.19769049098249525, 0.09811706305481493, 0.058980197994969785, 0.055670263012871146, 0.11786673299502581, 0.27495466207619756, 0.08976135298144072, 0.4020521939964965, 0.12175445107277483, 0.054704347043298185, 0.09348124195821583, 0.05543793994002044, 0.1058585720602423, 0.0652026500320062, 0.05311605799943209, 0.11143118399195373, 0.15787936095148325, 0.05511111707892269, 0.10688687092624605, 0.05138289905153215, 0.08475723199080676, 0.09280869306530803, 0.05018429993651807, 0.055127328960224986, 0.10892023099586368, 0.05886524205561727, 0.16354113502893597, 0.08691515203099698, 0.06443851604126394, 0.061888000927865505, 0.12258999003097415, 0.052391245029866695, 0.08719716698396951, 0.1333409920334816, 0.06570514698978513, 0.04613744595553726, 0.06596454104874283, 0.08999015903100371, 0.11568122589960694, 0.0504794679582119, 0.11090264702215791, 0.06799895304720849, 0.049329782952554524, 0.08087419904768467, 0.0536450109211728, 0.10564466693904251, 0.3112287539988756, 0.06414128001779318, 0.08722631796263158, 0.060985834104940295, 0.05952533695381135, 0.10374302708078176, 0.047769455006346107, 0.05085731705185026, 0.08086087403353304, 0.13770235201809555, 0.05876236909534782, 0.11208326695486903, 0.15488428005483001, 0.051193080958910286, 0.10134175990242511, 0.05095313198398799, 0.10103078093379736, 0.08384851098526269, 0.07402159203775227, 0.05553653999231756, 0.109593739034608]
[0.001216636454707689, 0.001344406180909242, 0.0015767486580774528, 0.0012109976351811465, 0.0014148615671067753, 0.002972867818739773, 0.0016536754317877983, 0.001505667478671636, 0.002063795794542371, 0.0012112044311255556, 0.0017092007275840099, 0.0018263657508544964, 0.0033033280897970226, 0.0012725474979643795, 0.0030689149094872514, 0.0023432285445530647, 0.0025979683401105417, 0.002197982317408208, 0.003392116589979692, 0.0012185302965173667, 0.0024364661816930907, 0.003300597706005316, 0.0024677617951634934, 0.0017327342039524492, 0.0011075427729255434, 0.001491699044973674, 0.0016368016129655932, 0.0033696576587813483, 0.002427174090619453, 0.0018696823178536513, 0.001266480614007874, 0.0038960930462714964, 0.0023529091585342858, 0.002335795227819207, 0.00409637411410751, 0.0012849781381770629, 0.0018233615476426414, 0.0021549560690552676, 0.002347364638064226, 0.004492965704147619, 0.002229933251245794, 0.0013404590453402225, 0.001265233250292526, 0.0026787893862505866, 0.006248969592640854, 0.002040030749578198, 0.009137549863556738, 0.002831498862157554, 0.0012721941172860044, 0.0021739823711212983, 0.0012892544172097777, 0.002461827257214937, 0.001516340698418749, 0.0012352571627774903, 0.0025914228835338077, 0.0036716130453833314, 0.0012816538855563416, 0.0024857411843313033, 0.0011949511407333057, 0.0019710984183908546, 0.00215834169919321, 0.0011670767427097227, 0.0012820309060517438, 0.002533028627810783, 0.0013689591175724948, 0.003803282209975255, 0.0020212826053720226, 0.0014985701404945102, 0.001439255835531756, 0.002850930000720329, 0.0012184010472062023, 0.0020278410926504536, 0.003100953303104223, 0.0015280266741810496, 0.0010729638594310991, 0.0015340590941568101, 0.002092794396069854, 0.0026902610674327198, 0.0011739411153072535, 0.0025791313260966954, 0.0015813710010978718, 0.0011472042547105703, 0.0018807953266903412, 0.0012475583935156465, 0.0024568527195126164, 0.007237877999973851, 0.0014916576748323996, 0.002028519022386781, 0.0014182752117427976, 0.0013843101617165429, 0.0024126285367623663, 0.0011109175582871187, 0.0011827283035314015, 0.0018804854426403032, 0.003202380279490594, 0.0013665667231476239, 0.0026065876036016054, 0.0036019600012751167, 0.0011905367664862858, 0.0023567851140098863, 0.0011849565577671626, 0.0023495530449720316, 0.0019499653717502952, 0.001721432372970983, 0.0012915474416818036, 0.002548691605456]
[821.9382183811529, 743.822822447666, 634.2164902929495, 825.7654440839725, 706.7829272123646, 336.37553398654285, 604.7135857360438, 664.1572685638681, 484.5440632471787, 825.6244563691976, 585.0687890903957, 547.5354536910983, 302.72500121580305, 785.8252847926243, 325.8480699183276, 426.7616158588299, 384.9161610481561, 454.9627137943351, 294.8011878347574, 820.66076063768, 410.4304863797059, 302.97542720233275, 405.22549703130824, 577.1225602397369, 902.8996662210415, 670.3765101744422, 610.9475895421302, 296.7660520035304, 412.00176116942, 534.8502205165929, 789.5896620441936, 256.6673814315049, 425.00578331844184, 428.119720466096, 244.11832809803633, 778.2233567169105, 548.4375829318459, 464.0465828328463, 426.0096551614829, 222.57013871191222, 448.4439161761148, 746.0130941532717, 790.3680999284494, 373.30295734808294, 160.0263827779955, 490.1886896590958, 109.43852727833496, 353.1698399617286, 786.0435655317435, 459.9853307385465, 775.6420972085664, 406.2023430235715, 659.4823980143824, 809.5480278385824, 385.8883883267808, 272.35985591057727, 780.2418509938968, 402.2944972322261, 836.8542996547372, 507.33133904920373, 463.3186674629883, 856.8416826456473, 780.0123969551473, 394.784326170158, 730.4820042933414, 262.9307910354899, 494.73537116594696, 667.3027661354656, 694.8035056120054, 350.7627334755099, 820.7478172257021, 493.1352873873207, 322.4814765830061, 654.438837290555, 931.9978405705244, 651.8653706424825, 477.83002567186816, 371.71113692482146, 851.8314819719657, 387.7274452377027, 632.3626772627972, 871.6843542846616, 531.6899642449199, 801.5656863820045, 407.02480537717264, 138.16204141650533, 670.3951026245741, 492.9704818954014, 705.0817723671453, 722.3814630963925, 414.4856884358803, 900.1568051024972, 845.5027219811942, 531.7775811100914, 312.2677236068513, 731.7608303066917, 383.64334987946233, 277.62662540561075, 839.9572597420656, 424.3068212097529, 843.9127944777318, 425.61286374868956, 512.8296196882701, 580.9115802058036, 774.2650155365862, 392.35818011849443]
Elapsed: 0.09471626978538895~0.05314645732532693
Time per graph: 0.002178146378226077~0.001214291220735128
Speed: 553.4027366525212~205.02228803747747
Total Time: 0.1101
best val loss: 0.7215011119842529 test_score: 0.4419

Testing...
Test loss: 0.7072 score: 0.4419 time: 0.13s
test Score 0.4419
Epoch Time List: [0.2386474049417302, 0.23698948917444795, 0.2569511700421572, 0.2686661221086979, 0.2525211760075763, 0.3975912630558014, 0.43671675689984113, 0.3269686341518536, 0.32356250600423664, 0.4534768069861457, 0.29003395314794034, 0.35084644285961986, 0.41240445303265005, 0.4167550130514428, 0.3636886980384588, 0.44057594507467, 0.40148504986427724, 0.3057047779439017, 0.41003145882859826, 0.3485990351764485, 0.3338054360356182, 0.3903276158962399, 0.5638366589555517, 0.32137978298123926, 0.3449831899488345, 0.41877988108899444, 0.28091792785562575, 0.35331367212347686, 0.3039102910552174, 0.2750011889729649, 0.28809488203842193, 0.4906964801484719, 0.28928981500212103, 0.31078528019133955, 0.5406650858931243, 0.38794472499284893, 0.5624508240725845, 0.38144517107866704, 0.4059472121298313, 0.5222920010564849, 0.36814069910906255, 0.2907579409657046, 0.3837342010810971, 0.582721235929057, 0.5109960209811106, 0.5656269750325009, 0.7095537649001926, 0.3262799320509657, 0.3187370849773288, 0.38603113999124616, 0.3908177597913891, 0.32616335107013583, 0.5068218598607928, 0.385054815094918, 0.351725340122357, 0.3881031950004399, 0.315941845998168, 0.3325333909597248, 0.4117599379969761, 0.42318257107399404, 0.28832262801006436, 0.4990479819243774, 0.29954879404976964, 0.33921474614180624, 0.4478818558854982, 0.7157840210711583, 0.3167876440566033, 0.38868778105825186, 0.2798960090149194, 0.3628319570561871, 0.2745707860449329, 0.42562442587222904, 0.38732264610007405, 0.4395047699799761, 0.3567477948963642, 0.3470265679061413, 0.35237589688040316, 0.37896698201075196, 0.33107519592158496, 0.35291016404516995, 0.2641228340798989, 0.3208490270189941, 0.4159618300618604, 0.5273245458956808, 0.4798945450456813, 0.5541122440481558, 0.42506784200668335, 0.3099296538857743, 0.3278340391116217, 0.34620114194694906, 0.3378306641243398, 0.3799270330928266, 0.3460476720938459, 0.3975710639497265, 0.347363268956542, 0.3514039098517969, 0.33833520603366196, 0.39230026793666184, 0.3564189028693363, 0.4989159309770912, 0.40820931701455265, 0.44154111901298165, 0.3247501478763297, 0.4577583718346432, 0.26577926496975124, 0.3699527920689434]
Total Epoch List: [47, 27, 32]
Total Time List: [0.4024314070120454, 0.06624253699555993, 0.11009874800220132]
T-times Epoch Time: 0.3752933498449594 ~ 0.009315073727615465
T-times Total Epoch: 58.555555555555564 ~ 21.413622063456316
T-times Total Time: 0.1192414121210782 ~ 0.053766457834615776
T-times Inference Elapsed: 0.09240567096725773 ~ 0.005100930820825148
T-times Time Per Graph: 0.0021228840946650386 ~ 0.00011170744447856846
T-times Speed: 567.8442287430145 ~ 11.188016629105425
T-times cross validation test micro f1 score:0.6107292797190494 ~ 0.07525692573851574
T-times cross validation test precision:0.5783076015464265 ~ 0.09396613011545736
T-times cross validation test recall:0.7101972101972102 ~ 0.056136073990032995
T-times cross validation test f1_score:0.6107292797190494 ~ 0.04594433357430997
